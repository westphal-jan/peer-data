{"id": "1511.07053", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2015", "title": "ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation", "abstract": "We propose a structured prediction architecture for images centered around deep recurrent neural networks. The proposed network, called ReSeg, is based on the recently introduced ReNet model for object classification. We modify and extend it to perform object segmentation, noting that the avoidance of pooling can greatly simplify pixel-wise tasks for images. The ReSeg layer is composed of four recurrent neural networks that sweep the image horizontally and vertically in both directions, along with a final layer that expands the prediction back to the original image size. ReSeg combines multiple ReSeg layers with several possible input layers as well as a final layer which expands the prediction back to the original image size, making it suitable for a variety of structured prediction tasks. We evaluate ReSeg on the specific task of object segmentation with three widely-used image segmentation datasets, namely Weizmann Horse, Fashionista and Oxford Flower. The results suggest that ReSeg can challenge the state of the art in object segmentation, and may have further applications in structured prediction at large.", "histories": [["v1", "Sun, 22 Nov 2015 19:25:27 GMT  (3227kb,D)", "http://arxiv.org/abs/1511.07053v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Mon, 11 Jan 2016 14:41:56 GMT  (3272kb,D)", "http://arxiv.org/abs/1511.07053v2", "Under review as a conference paper at ICLR 2016"], ["v3", "Tue, 24 May 2016 15:55:41 GMT  (2488kb,D)", "http://arxiv.org/abs/1511.07053v3", "In CVPR Deep Vision Workshop, 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["francesco visin", "marco ciccone", "adriana romero", "kyle kastner", "kyunghyun cho", "yoshua bengio", "matteo matteucci", "aaron courville"], "accepted": false, "id": "1511.07053"}, "pdf": {"name": "1511.07053.pdf", "metadata": {"source": "CRF", "title": "RESEG: A RECURRENT NEURAL NETWORK FOR OBJECT SEGMENTATION", "authors": ["Francesco Visin", "Kyle Kastner", "Aaron Courville", "Yoshua Bengio", "Matteo Matteucci", "Kyunghyun Cho"], "emails": ["matteo.matteucci}@polimi.it"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, Convolutionary Neural Networks (CNN, Fukushima, 1980; LeCun et al., 1989) have become the de facto standard in many computer vision tasks. Object classification from an image is almost always performed using very deep Convolutionary Neural Networks (see, for example, Lin et al., 2014; Simonyan & Zisserman, 2015; Szegedy et al., 2014) by directly educating them in a superordinate manner. Furthermore, Convolutionary Neural Networks have been found to extract good, generic image representations when trained on a large number of images (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al.) These representations from the Convolutionary Neural Network have been used in a variety of computer vision tasks ranging from image capture generation (see, e.g., Vinyals et al., 2015)."}, {"heading": "2 MODEL DESCRIPTION", "text": "As shown in Figure 2, the ReSeg model consists of several recursive neural networks coupled to capture the spatial relationship of the input data, and an expansion layer followed by a Softmax operation. In the following section, we will define each component in more detail."}, {"heading": "2.1 RESEG", "text": "We take as our input an image (or the characteristic map of the previous layer) X of the elements {xi, j}, c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c \"c, c, c\" c, c, c \"c, c, c, c\" c, c, c \"c, c, c\" c, c, c \"c, c, c\" c, c \"c, c\" c, c \"c, c\" c, c \"c, c, c\" c, c, c \"c, c\" c \"c, c, c\" c, c \"c, c, c\" c \"c, c, c, c\" c, c \"c, c, c, c\" c, c, c \"c, c, c\" c \"c, c, c, c, c\" c, c, c, c, c, c, c \"c, c, c, c, c, c, c, c, c, c\" c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c \", c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c,"}, {"heading": "2.2 GATED RECURRENT UNITS", "text": "Recurring units with additional memory and gating, such as gated recurrent units (GRU, Cho et al., 2014) and long-time memory units (LSTM, Hochreiter & Schmidhuber, 1997), have been key components for many successful applications using relapsing neural networks (including Cho et al., 2014; Sutskever et al., 2014; Xu et al., 2015). Previous work on ReNet has shown that the ReNet model can work well without caring for the specific relapsing unit, but for this particular set of experiments we have decided to use the GRU unit. The hidden state of the GRU at the time t is calculated by ht = (1 \u2212 ut) ht \u2212 1 + ut h, where h = tanh (Wxt + U (rt ht \u2212 1) + b) and [ut; rt] = \u043c (Wgxt + Ught \u2212 1 + bg).For an in-depth comparison of the trade similarities between STU and multiple STU (rt + 1) and GRU \u2212 (1)."}, {"heading": "2.3 UPSAMPLING LAYER", "text": "Since each ReSeg layer handles non-overlapping patches, the size of the last composite feature map will be smaller than the size of the original input X. We therefore need to add one or more layers to expand them back to the size of the image in order to calculate the appropriate segmentation mask. To achieve this, we have examined several different architectures, which we will discuss in more detail in this section."}, {"heading": "2.3.1 LINEAR FULLY-CONNECTED UPSAMPLING", "text": "The simplest way to zoom in on the last ReSeg composite function board before inserting it into a Softmax classifier is to first use a linear, fully connected plane to obtain an advanced function board with c \u00b7 e characteristics: E = H \u00b7 W + b (3), where W-R2d \u00b7 c \u00b7 u and b-Rc \u00b7 u, with c the number of classes and u the upsampling factor, are calculated as u = uw \u00b7 uh, where uw and uh are the factors extending across both axes, calculated over each axis a as: ua = l layersa (l) p (4) The function card E can then be rearranged in such a way that each of its elements, ei, j R1 \u00b7 c \u00b7 u, is mapped to a patch on the output function card oi, j Ruw \u00d7 uh \u00b7 c. Subsequently, a Softmax can be applied to the output card O to obtain the probabilities for the class."}, {"heading": "2.3.2 CONVOLUTIONAL FULLY-CONNECTED UPSAMPLING", "text": "One problem with linear fully connected upsampling is that each patch on the pre-softmax Feature Map O only depends on one element of the last ReSeg composite feature map. To have a larger context about the hidden state of the RNN, it is possible to insert an equal fold before the linear fully connected upsampling layer, so that the upsampling layer performs a transformation hi: i + cw, j: j + ch \u2192 ok: k + uw, l: l + uh, where cw and ch are the width or height of the core of the fold, respectively."}, {"heading": "2.3.3 FULL CONVOLUTION", "text": "We will not go into the details of this method as it is widely accepted and very well documented. However, we would like to stress that the main disadvantage of using a full convolution is that many full wave layers and / or very large cores are required to achieve a high upsampling rate. For most of the models examined in this paper, as we will explain in Section 3, this alternative has proved too expensive to be considered practicable in terms of both memory usage and computing time."}, {"heading": "2.3.4 GRADIENT-BASED UPSAMPLING", "text": "A final alternative we have studied to increase the composite characteristic card of the ReSeg is to exploit the gradient of a folding by using it in forward gear instead of using it in the conventional way for reverse gear. Specifically, it is possible to imagine the last ReSeg characteristic card H as the result of merging a core K with the desired pre-sigmoid characteristic card O: H = K \u00b2 O, using \u0445 = folding operator (5) In order to reverse this function and calculate the contribution of each element of the composite characteristic card Hi, j to each element of the pre-sigmoid characteristic card Ok, l, it is then possible to use the gradient of this fictive folding."}, {"heading": "2.4 INTERMEDIATE PREDICTIONS", "text": "In some experiments, after some of the ReSeg substrata, we introduced an intermediate classification path to give the network a stronger learning signal (see Figure 3), justified by previous work such as Bengio (2014) and Simonyan & Zisserman (2015). It is important to note that the prediction emanating from this path is only used to calculate the gradient and is not taken into account to determine the actual performance of the network."}, {"heading": "3 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 DATASETS", "text": "We have evaluated the proposed ReSeg using several widely used benchmark data sets; in this section we describe each data set in detail."}, {"heading": "3.1.1 WEIZMANN HORSE", "text": "The Weizmann Horse dataset introduced in Borenstein (2004) is an image segmentation dataset consisting of 329 images of different sizes in RGB and grayscale format that match an equal number of images for basic truth segmentation that are the same size as the corresponding image. The basic truth segmentations contain a focussed horse foreground / background mask that is encoded as a real value between 0 and 255. To convert this into a Boolean mask, we set all smaller values to 0 in the middle of the range and all larger ones to 1. This dataset is one of the primary small-scale benchmarks found in the existing image segmentation literature."}, {"heading": "3.1.2 FASHIONISTA", "text": "The Fashionista dataset from Yamaguchi (2012) contains 685 RGB images of models with different clothing. Each image and associated mask are 400 pixels wide and 600 pixels high, with encoded values for 53 garments. In this thesis, we focus on foreground / background segmentation and create matching masks from the more complex maps provided by the dataset by creating a new map that maintains the background class as 0 and sets pixels belonging to all other classes to 1. This seems to be the same procedure used in Yang et al. (2015) to create a foreground / background task for this dataset."}, {"heading": "3.1.3 OXFORD FLOWERS 17", "text": "The Oxford Flowers 17 dataset by Nilsback & Zisserman (2006) contains 1363 RGB images of varying sizes with 848 segmentation maps associated with a subset of RGB images. There are 8 unique segmentation classes defined across all maps, including flowers, sky and grass. To create a foreground / background mask, we take the original segmentation maps and set any pixel that does not belong to class 38 (flower class) to 0 and set the pixels of the flower class to 1. This binary segmentation task for Oxford Flowers 17 is further described in Wu & Kashino (2014). A larger Oxford Flowers 102 dataset is available from the same authors."}, {"heading": "3.2 PREPROCESSING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 DATA AUGMENTATION", "text": "With this in mind, we decided to use several methods of data enlargement: rotate, move, color change, and resize. For each sample, there was a 50% chance of mirroring the image horizontally, reflecting the intuition that images that are inverted horizontally generally appear like the same scene. Moving was as follows: We either moved 2 pixels to the left with a 25% chance, 2 pixels to the right with a 25% chance, or we did not shift. After this step, we moved down with a 25% chance, or left the image as it was before this step."}, {"heading": "3.3 MODEL ARCHITECTURES", "text": "The parameters that describe the core of the proposed ReSeg model are the number of features (dRE) of each sublayer and the size of the patches they read (wp \u00d7 hp), the number of ReSeg layers used (nl), the type of upsampling strategy applied (see 2.3) and their parameters (e.g. the number of turns nc, their grain size wck \u00d7 hck and stride wcs \u00d7 hcs and their activation ac) if an intermediate prediction path is added to the topology and at what point in the network. We have tried many combinations of hyperparameters, but in the reported experiments the number of features dRE was between 50 and 300, the patch size wp \u00d7 hp was always 2, the number of layers 2, and the upsampling strategy we chose either linearly fully connected or expiring."}, {"heading": "3.4 TRAINING", "text": "The first component of neural network training is initialization. In this work, we used the fan-in plus fan-out initialization described in Glorot et al. (2010) for all feedback and convolutional initializations. Recurring weight matrices were initialized as orthonormal, following the procedure defined in Saxe et al. (2014). An adaptive learning algorithm known as Adam (Kingma & Ba, 2014) was a key factor for stable learning, although others such as (Zeiler, 2012) were useful during model development. In addition, we also used the gradient standard to deal with the problems described in Bengio et al. (2013). Regularization proved to be another important component of our process, especially on the smaller Weizmann Horse Dataset."}, {"heading": "4 RESULTS AND ANALYSIS", "text": "In Table 1, we present the results on three sets of data, along with previously reported results. There are a number of difficulties in directly comparing our performance to reported values, some of which are typical of the use of neural models for segmentation, while others are specific to the assessment data sets."}, {"heading": "4.1 DIFFICULTIES IN NEURAL MODEL EVALUATION", "text": "The general approach to developing neural network models is to take the data set and divide it into three parts: training, validation and testing. To learn model parameters, an update rule is used based on iterations of the training, and the best model is usually selected by evaluating a secondary stop criterion on a pre-set validation set. Once this is complete, the best model is evaluated on the basis of the never-before-seen test set. It is not always clear. This general procedure is very different from a leave-one-out or k-fold cross validation scheme, as is common in much of the segmentation literature, and the \"loss\" of data to a validation set is particularly painful in limited data sets such as the three reported experiments. Nevertheless, we report our results in line with approaches that other training programs use, as we think the performance indicators should continue to improve depending on the practicality of the methodology."}, {"heading": "4.2 DATASET SPECIFIC ISSUES", "text": "In the data sets of Weizmann Horse and Oxford Flowers, there appears to be a small number of partially corrupt images or masks in the download. This corruption was present across multiple machines in different networks, so it seems to be more of a problem in the data set itself than transferring to our machines. In the case of larger data sets, this kind of corruption might not make much difference in overall performance accuracy, but with small assessment sets such as Weizmann Horse (128 images) and Oxford Flowers (197 images), corruption of one or two images could lead to large performance shifts. It is not clear whether this corruption is new, but we have contacted several of the cited authors to find out whether the problems we found during their analysis were also present and if so to know how the reported results were collected. It is also important to note that this problem has far less impact if we apply a folding or omit one of the approaches that would normally not have a workable solution for large neural models.In our case, the 2 corrupt images of Oxford Flowers were not replaced by a mask of Oxford size."}, {"heading": "5 CONCLUSION", "text": "The ReSeg model proposed in this paper works competitively on several small to medium-sized benchmarks for object segmentation, which is not typically a strong suit of neural models with a large number of parameters. The success of a ReSeg-based architecture on this task has shown that recurring neural networks can be a viable alternative to energy-based, part-based, or application-specific models, even when combined with the power of a deeply preformed CNN. We hope that these results will encourage further work in applying neural models to segmentation tasks and plan to approach larger and more modern datasets for segmentation with similar techniques."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank all the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012), Pascal Lamblin, Arnaud Bergeron and Fre'de'ric Bastien for their technical support and commitment, especially in the late evening hours, Ce'sar Laurent for their insightful discussions on batch normalization and moral support, and Vincent Dumoulin for his brilliant suggestions for solving the upsampling problem. We would also like to thank the following research funding and computer support organizations: NSERC, IBM Watson Group, IBM Research, NVIDIA, Samsung, Calcul Que'bec, Compute Canada, the Canada Research Chairs and CIFAR."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representations (ICLR 2015),", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Frederic", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "Submited to the Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "How auto-encoders could provide credit assignment in deep networks via target", "author": ["Bengio", "Yoshua"], "venue": "propagation. CoRR,", "citeRegEx": "Bengio and Yoshua.,? \\Q2014\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2014}, {"title": "Advances in optimizing recurrent networks", "author": ["Bengio", "Yoshua", "Boulanger-Lewandowski", "Nicolas", "Pascanu", "Razvan"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Kernelized structural svm learning for supervised object segmentation", "author": ["Bertelli", "Luca", "Yu", "Tianli", "Vu", "Diem", "Gokturk", "Burak"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Bertelli et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bertelli et al\\.", "year": 2011}, {"title": "On the statistical analysis of dirty pictures", "author": ["Besag", "Julian"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Besag and Julian.,? \\Q1986\\E", "shortCiteRegEx": "Besag and Julian.", "year": 1986}, {"title": "Visual Reconstruction", "author": ["Blake", "Andrew", "Zisserman"], "venue": null, "citeRegEx": "Blake et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Blake et al\\.", "year": 1987}, {"title": "Combining top-down and bottom-up segmentation", "author": ["Borenstein", "Eran"], "venue": "Proceedings IEEE workshop on Perceptual Organization in Computer Vision, CVPR, pp", "citeRegEx": "Borenstein and Eran.,? \\Q2004\\E", "shortCiteRegEx": "Borenstein and Eran.", "year": 2004}, {"title": "Graph cuts and efficient nd image segmentation", "author": ["Boykov", "Yuri", "Funka-Lea", "Gareth"], "venue": "International journal of computer vision,", "citeRegEx": "Boykov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2006}, {"title": "Fast and robust fuzzy c-means clustering algorithms incorporating local information for image segmentation", "author": ["Cai", "Weiling", "Chen", "Songcan", "Zhang", "Daoqiang"], "venue": "Pattern Recognition,", "citeRegEx": "Cai et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2007}, {"title": "Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform", "author": ["Chen", "Liang-Chieh", "Barron", "Jonathan T", "Papandreou", "George", "Murphy", "Kevin", "Yuille", "Alan L"], "venue": "arXiv preprint arXiv:1511.03328,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "Gulcehre", "Caglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "First results", "citeRegEx": "Chorowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Efficient graph-based image segmentation", "author": ["Felzenszwalb", "Pedro F", "Huttenlocher", "Daniel P"], "venue": "Int. J. Comput. Vision,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2004}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics,", "citeRegEx": "Fukushima,? \\Q1980\\E", "shortCiteRegEx": "Fukushima", "year": 1980}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["Geman", "Stuart", "Donald"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Geman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1984}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Practical variational inference for neural networks", "author": ["Graves", "Alex"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Graves and Alex.,? \\Q2011\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2011}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In ICML\u20192014,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2008}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Grid long short-term memory", "author": ["Kalchbrenner", "Nal", "Danihelka", "Ivo", "Graves", "Alex"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "[cs.LG],", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A simple weight decay can improve generalization", "author": ["Krogh", "Anders", "Hertz", "John A"], "venue": "In ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS", "citeRegEx": "Krogh et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Krogh et al\\.", "year": 1992}, {"title": "Figure-ground segmentation by transferring window masks", "author": ["Kuettel", "Daniel", "Ferrari", "Vittorio"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kuettel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kuettel et al\\.", "year": 2012}, {"title": "URL http://arxiv.org/ abs/1510.01378", "author": ["Laurent", "C\u00e9sar", "Pereyra", "Gabriel", "Brakel", "Philemon", "Zhang", "Ying", "Bengio", "Yoshua"], "venue": "Batch normalized recurrent neural networks. CoRR,", "citeRegEx": "Laurent et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Laurent et al\\.", "year": 2015}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["LeCun", "Yann", "Boser", "Bernhard", "Denker", "John S", "Henderson", "Donnie", "Howard", "Richard E", "Hubbard", "Wayne", "Jackel", "Lawrence D"], "venue": "Neural Computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Learning to combine bottom-up and top-down segmentation", "author": ["Levin", "Anat", "Weiss", "Yair"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Levin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2009}, {"title": "Network in network", "author": ["Lin", "Min", "Chen", "Qiang", "Yan", "Shuicheng"], "venue": "In Proceedings of the Second International Conference on Learning Representations (ICLR", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Crf learning with cnn features for image segmentation", "author": ["Liu", "Fayao", "Lin", "Guosheng", "Shen", "Chunhua"], "venue": "Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Statistical Language Models based on Neural Networks", "author": ["Mikolov", "Tomas"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "Mikolov and Tomas.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tomas.", "year": 2012}, {"title": "A visual vocabulary for flower classification", "author": ["Nilsback", "M-E", "A. Zisserman"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Nilsback et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nilsback et al\\.", "year": 2006}, {"title": "Grabcut: Interactive foreground extraction using iterated graph cuts", "author": ["Rother", "Carsten", "Kolmogorov", "Vladimir", "Blake", "Andrew"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "Rother et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rother et al\\.", "year": 2004}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "In Proceedings of the Second International Conference on Learning Representations (ICLR", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "Normalized cuts and image segmentation", "author": ["Shi", "Jianbo", "Malik", "Jitendra"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Shi et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2000}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In NIPS\u20192014,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Show and tell: a neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv 1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Renet: A recurrent neural network based alternative to convolutional networks", "author": ["Visin", "Francesco", "Kastner", "Kyle", "Cho", "Kyunghyun", "Matteucci", "Matteo", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1505.00393,", "citeRegEx": "Visin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Visin et al\\.", "year": 2015}, {"title": "Tri-map self-validation based on least gibbs energy for foreground segmentation", "author": ["Wu", "Xiaomeng", "Kashino", "Kunio"], "venue": "In Proceedings of the British Machine Vision Conference. BMVA Press,", "citeRegEx": "Wu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy Lei", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Parsing clothing in fashion photographs", "author": ["Yamaguchi", "Kota"], "venue": "In Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Yamaguchi and Kota.,? \\Q2012\\E", "shortCiteRegEx": "Yamaguchi and Kota.", "year": 2012}, {"title": "Patchcut: Data-driven object segmentation via local shape transfer", "author": ["Yang", "Jimei", "Price", "Brian", "Cohen", "Scott", "Lin", "Zhe", "Ming-Hsuan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Video description generation incorporating spatio-temporal features and a soft-attention mechanism", "author": ["Yao", "Li", "Torabi", "Atousa", "Cho", "Kyunghyun", "Ballas", "Nicolas", "Pal", "Christopher", "Larochelle", "Hugo", "Courville", "Aaron"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 29, "context": "In recent years, convolutional neural networks (CNN, Fukushima, 1980; LeCun et al., 1989) have become the de facto standard in many computer vision tasks.", "startOffset": 47, "endOffset": 89}, {"referenceID": 42, "context": "Object classification from an image is almost always done with very deep convolutional neural networks (see, e.g., Lin et al., 2014; Simonyan & Zisserman, 2015; Szegedy et al., 2014) by directly training them in a supervised manner.", "startOffset": 103, "endOffset": 182}, {"referenceID": 25, "context": "Furthermore, the convolutional neural networks have been found to extract good, generic image representations, when they were trained on a large set of images (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2014).", "startOffset": 159, "endOffset": 234}, {"referenceID": 42, "context": "Furthermore, the convolutional neural networks have been found to extract good, generic image representations, when they were trained on a large set of images (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2014).", "startOffset": 159, "endOffset": 234}, {"referenceID": 46, "context": "These representations from the convolutional neural network have been used in a wide variety of computer vision tasks, ranging from image caption generation (see, e.g., Vinyals et al., 2014; Xu et al., 2015), video description generation (see, e.", "startOffset": 157, "endOffset": 207}, {"referenceID": 11, "context": ", 2014) to object segmentation (Chen et al., 2015).", "startOffset": 31, "endOffset": 50}, {"referenceID": 41, "context": ", Mikolov, 2012), and machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015).", "startOffset": 42, "endOffset": 107}, {"referenceID": 12, "context": ", Mikolov, 2012), and machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015).", "startOffset": 42, "endOffset": 107}, {"referenceID": 0, "context": ", Mikolov, 2012), and machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015).", "startOffset": 42, "endOffset": 107}, {"referenceID": 23, "context": "More recently, recurrent neural networks have begun to be employed in a few computation vision tasks (see, e.g., Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015; Graves & Schmidhuber, 2009).", "startOffset": 101, "endOffset": 206}, {"referenceID": 11, "context": "More recently, recurrent neural networks have begun to be employed in a few computation vision tasks (see, e.g., Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015; Graves & Schmidhuber, 2009).", "startOffset": 101, "endOffset": 206}, {"referenceID": 10, "context": ", Shi & Malik, 2000; Felzenszwalb & Huttenlocher, 2004; Boykov & Funka-Lea, 2006) that represent the pixels of the image as nodes of the graph, greedy approaches (such as, e.g., Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.", "startOffset": 162, "endOffset": 208}, {"referenceID": 44, "context": "Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015).", "startOffset": 71, "endOffset": 176}, {"referenceID": 23, "context": "Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015).", "startOffset": 71, "endOffset": 176}, {"referenceID": 11, "context": "Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015).", "startOffset": 71, "endOffset": 176}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification.", "startOffset": 15, "endOffset": 373}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN.", "startOffset": 15, "endOffset": 1231}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d).", "startOffset": 15, "endOffset": 1381}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d). A multidimensional RNN, on the other hand, requires an exponential number of RNNs at each layer (2). Furthermore, the proposed variant is more easily parallelizable, as each RNN is dependent only along a horizontal or vertical sequence of patches. This architectural distinction results in our model being much more amenable to distributed computing than that of Graves & Schmidhuber (2009). Kalchbrenner et al.", "startOffset": 15, "endOffset": 1897}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d). A multidimensional RNN, on the other hand, requires an exponential number of RNNs at each layer (2). Furthermore, the proposed variant is more easily parallelizable, as each RNN is dependent only along a horizontal or vertical sequence of patches. This architectural distinction results in our model being much more amenable to distributed computing than that of Graves & Schmidhuber (2009). Kalchbrenner et al. (2015) has further extended many of the concepts from the multidimensional RNN paper of Graves & Schmidhuber (2009), and bears some similarity to the ReNet approach.", "startOffset": 15, "endOffset": 1925}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d). A multidimensional RNN, on the other hand, requires an exponential number of RNNs at each layer (2). Furthermore, the proposed variant is more easily parallelizable, as each RNN is dependent only along a horizontal or vertical sequence of patches. This architectural distinction results in our model being much more amenable to distributed computing than that of Graves & Schmidhuber (2009). Kalchbrenner et al. (2015) has further extended many of the concepts from the multidimensional RNN paper of Graves & Schmidhuber (2009), and bears some similarity to the ReNet approach.", "startOffset": 15, "endOffset": 2034}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d). A multidimensional RNN, on the other hand, requires an exponential number of RNNs at each layer (2). Furthermore, the proposed variant is more easily parallelizable, as each RNN is dependent only along a horizontal or vertical sequence of patches. This architectural distinction results in our model being much more amenable to distributed computing than that of Graves & Schmidhuber (2009). Kalchbrenner et al. (2015) has further extended many of the concepts from the multidimensional RNN paper of Graves & Schmidhuber (2009), and bears some similarity to the ReNet approach. Grid LSTM inherently uses three dimensional blocks, and modulates information passed over depth, while ReNet simply stacks hidden layers and requires less recurrent passes over the data. The authors of Kalchbrenner et al. (2015) show promising results over a number of tasks, including MNIST recognition, but do not have results for image segmentation or larger image datasets as of this writing.", "startOffset": 15, "endOffset": 2313}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d). A multidimensional RNN, on the other hand, requires an exponential number of RNNs at each layer (2). Furthermore, the proposed variant is more easily parallelizable, as each RNN is dependent only along a horizontal or vertical sequence of patches. This architectural distinction results in our model being much more amenable to distributed computing than that of Graves & Schmidhuber (2009). Kalchbrenner et al. (2015) has further extended many of the concepts from the multidimensional RNN paper of Graves & Schmidhuber (2009), and bears some similarity to the ReNet approach. Grid LSTM inherently uses three dimensional blocks, and modulates information passed over depth, while ReNet simply stacks hidden layers and requires less recurrent passes over the data. The authors of Kalchbrenner et al. (2015) show promising results over a number of tasks, including MNIST recognition, but do not have results for image segmentation or larger image datasets as of this writing. In this work we extend the results of Visin et al. (2015) modifying and extending the ReNet model to the more ambitious task of object segmentation.", "startOffset": 15, "endOffset": 2539}, {"referenceID": 10, "context": ", Besag, 1986; Cai et al., 2007) and continuation methods (such as, e.g., Blake & Zisserman, 1987; Geman & Geman, 1984) are usually more popular. Recently however a few approaches in this direction have been explored (see, e.g., Graves & Schmidhuber, 2009; Visin et al., 2015; Kalchbrenner et al., 2015; Chen et al., 2015). The architecture proposed in Visin et al. (2015) is related and inspired by this earlier work, but relies on multiple uni-dimensional RNNs coupled in a novel way to address the problem of Object Classification. In the proposed model, the image is first swept by two horizontal RNNs in both directions (left to right and right to left) and then their concatenated hidden state is swept by a second couple of RNNs vertically (top to bottom and bottom to top). The output activation of the ReNet layer is the concatenation of the hidden states of these last two RNNs, which encodes the local features of the image in each position with respect to the whole input image. Finally, a number of fully connected layers and a softmax layer are exploited to predict the class of the object in the image. A similar approach to offline Arabic handwriting recognition was previously shown in Graves & Schmidhuber (2009), but was built on the more complex multi-dimensional RNN. One important consequence of the adoption of the usual sequence ones in Visin et al. (2015) is that the number of RNNs at each layer scales linearly with respect to the number of dimensions d of the input image (2d). A multidimensional RNN, on the other hand, requires an exponential number of RNNs at each layer (2). Furthermore, the proposed variant is more easily parallelizable, as each RNN is dependent only along a horizontal or vertical sequence of patches. This architectural distinction results in our model being much more amenable to distributed computing than that of Graves & Schmidhuber (2009). Kalchbrenner et al. (2015) has further extended many of the concepts from the multidimensional RNN paper of Graves & Schmidhuber (2009), and bears some similarity to the ReNet approach. Grid LSTM inherently uses three dimensional blocks, and modulates information passed over depth, while ReNet simply stacks hidden layers and requires less recurrent passes over the data. The authors of Kalchbrenner et al. (2015) show promising results over a number of tasks, including MNIST recognition, but do not have results for image segmentation or larger image datasets as of this writing. In this work we extend the results of Visin et al. (2015) modifying and extending the ReNet model to the more ambitious task of object segmentation. We test the performances of the model in the object segmentation domain on one of the historically most used datasets in this field, the Weizmann Horse dataset (Borenstein, 2004), the Oxford Flowers 17 dataset (Nilsback & Zisserman, 2006) and the more recent Fashionista dataset (Yamaguchi, 2012). Our experiments show that the proposed adaptation of the ReSeg for pixel-level object segmentation can challenge the state of the art. The proposed architecture can be easily merged with that proposed in Visin et al. (2015) into a joint network to perform both object classification and object segmentation at the same time, sharing most of the computation.", "startOffset": 15, "endOffset": 3152}, {"referenceID": 12, "context": "Note also that fF * and f F * represent a generic Recurrent Neural Network that can be implemented as a vanilla tanh RNN layer, as a Gated Recurrent Unit (GRU) layer (Cho et al., 2014) or as a Long Short-Term Memory (LSTM) layer (Hochreiter & Schmidhuber, 1997).", "startOffset": 166, "endOffset": 184}, {"referenceID": 41, "context": ", 2014) and long short-term memory units (LSTM, Hochreiter & Schmidhuber, 1997), have been key components to many successful applications using recurrent neural networks (including Cho et al., 2014; Sutskever et al., 2014; Xu et al., 2015).", "startOffset": 170, "endOffset": 239}, {"referenceID": 46, "context": ", 2014) and long short-term memory units (LSTM, Hochreiter & Schmidhuber, 1997), have been key components to many successful applications using recurrent neural networks (including Cho et al., 2014; Sutskever et al., 2014; Xu et al., 2015).", "startOffset": 170, "endOffset": 239}, {"referenceID": 14, "context": "For an in-depth comparison of the similarities and trade-offs between GRU and LSTM, there are several resources including (Chung et al., 2015).", "startOffset": 122, "endOffset": 142}, {"referenceID": 48, "context": "This appears to be the same procedure undertaken in Yang et al. (2015) to create a foreground/background task for this dataset.", "startOffset": 52, "endOffset": 71}, {"referenceID": 3, "context": "In addition, we also utilized gradient norm rescaling to help with the problems described in (Bengio et al., 2013).", "startOffset": 93, "endOffset": 114}, {"referenceID": 40, "context": "Dropout (Srivastava et al., 2014) on each forward connection with drop probability of 0.", "startOffset": 8, "endOffset": 33}, {"referenceID": 34, "context": "The recurrent weight matrices were initialized to be orthonormal, following the procedure defined in Saxe et al. (2014). An adaptive learning rate algorithm known as Adam (Kingma & Ba, 2014) was a key ingredient to stable learning, though others such as (Zeiler, 2012) were useful during model development.", "startOffset": 101, "endOffset": 120}, {"referenceID": 3, "context": "In addition, we also utilized gradient norm rescaling to help with the problems described in (Bengio et al., 2013). Regularization proved to be another important part of our process, especially on the smaller Weizmann Horse dataset. Weight noise, as described in Graves (2011), with a scale 0.", "startOffset": 94, "endOffset": 277}, {"referenceID": 3, "context": "In addition, we also utilized gradient norm rescaling to help with the problems described in (Bengio et al., 2013). Regularization proved to be another important part of our process, especially on the smaller Weizmann Horse dataset. Weight noise, as described in Graves (2011), with a scale 0.075 was applied to all weight matrices before each forward pass during some experiments. Dropout (Srivastava et al., 2014) on each forward connection with drop probability of 0.2 on the input, and/or with drop probability of 0.5 on the hidden projections was applied during some experiments. Nearly all experiments used L2 regularization (Krogh & Hertz, 1992), also known as weight decay, set to 0.0005 to avoid instability at the end of training. The effect of Batch Normalization in Recurrent Neural Networks has been recently in the focus of attention in Laurent et al. (2015) and since it does not seem to provide a reliable improvement in performances we decided not to adopt it.", "startOffset": 94, "endOffset": 873}, {"referenceID": 48, "context": "03% (Yang et al., 2015) 95.", "startOffset": 4, "endOffset": 23}, {"referenceID": 32, "context": "0% (Liu et al., 2015) 95.", "startOffset": 3, "endOffset": 21}, {"referenceID": 5, "context": "08% (Bertelli et al., 2011) 74.", "startOffset": 4, "endOffset": 27}, {"referenceID": 48, "context": "33% (Yang et al., 2015) 94.", "startOffset": 4, "endOffset": 23}, {"referenceID": 35, "context": "23% (Rother et al., 2004) 77.", "startOffset": 4, "endOffset": 25}, {"referenceID": 35, "context": "3% (Rother et al., 2004) 86.", "startOffset": 3, "endOffset": 24}, {"referenceID": 48, "context": "? as reported by (Yang et al., 2015)", "startOffset": 17, "endOffset": 36}, {"referenceID": 4, "context": "We would like to thank all the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 52, "endOffset": 97}, {"referenceID": 1, "context": "We would like to thank all the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 52, "endOffset": 97}], "year": 2017, "abstractText": "We propose a structured prediction architecture for images centered around deep recurrent neural networks. The proposed network, called ReSeg, is based on the recently introduced ReNet model for object classification. We modify and extend it to perform object segmentation, noting that the avoidance of pooling can greatly simplify pixel-wise tasks for images. The ReSeg layer is composed of four recurrent neural networks that sweep the image horizontally and vertically in both directions, along with a final layer that expands the prediction back to the original image size. ReSeg combines multiple ReSeg layers with several possible input layers as well as a final layer which expands the prediction back to the original image size, making it suitable for a variety of structured prediction tasks. We evaluate ReSeg on the specific task of object segmentation with three widely-used image segmentation datasets, namely Weizmann Horse, Fashionista and Oxford Flower. The results suggest that ReSeg can challenge the state of the art in object segmentation, and may have further applications in structured prediction at large.", "creator": "LaTeX with hyperref package"}}}