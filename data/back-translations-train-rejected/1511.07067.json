{"id": "1511.07067", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2015", "title": "Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes", "abstract": "We propose a model to learn visually grounded word embeddings (vis-w2v) to capture visual notions of semantic relatedness. While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world. For instance, visual grounding can help us realize that concepts like eating and staring at are related, since when people are eating something, they also tend to stare at the food. Grounding a rich variety of relations like eating and stare at in vision is a challenging task, despite recent progress in vision. We realize the visual grounding for words depends on the semantics of our visual world, and not the literal pixels. We thus use abstract scenes created from clipart to provide the visual grounding. We find that the embeddings we learn capture fine-grained visually grounded notions of semantic relatedness. We show improvements over text only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval. Our code and datasets will be available online.", "histories": [["v1", "Sun, 22 Nov 2015 20:46:42 GMT  (3974kb,D)", "http://arxiv.org/abs/1511.07067v1", "15 pages, 11 figures"], ["v2", "Wed, 29 Jun 2016 18:15:25 GMT  (4864kb,D)", "http://arxiv.org/abs/1511.07067v2", "15 pages, 11 figures"]], "COMMENTS": "15 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["satwik kottur", "ramakrishna vedantam", "jos\\'e m f moura", "devi parikh"], "accepted": false, "id": "1511.07067"}, "pdf": {"name": "1511.07067.pdf", "metadata": {"source": "CRF", "title": "Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes", "authors": ["Satwik Kottur", "Ramakrishna Vedantam", "Jos\u00e9 M. F. Moura", "Devi Parikh"], "emails": ["moura}@andrew.cmu.edu", "parikh}@vt.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. Related Work", "text": "In fact, most of them will be able to put themselves in a situation where they are able to survive on their own, and in which they are not able to survive on their own."}, {"heading": "3. Approach", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4. Applications", "text": "We examine the benefits of using vis-w2v over w2v in classifying common sense, visual paraphrasing, and text-based image retrieval claims. All of these tasks are ostensibly in the text, but could benefit from visual thinking."}, {"heading": "4.1. Common Sense Assertion Classification", "text": "We examine the common sense classification task (CS) introduced by Vedantam et al. [33]. Given the tuples of form (primary object or P, relation or R, secondary object or S), e.g. (boy, eats, cake), the task is to classify them as plausible or not. The CS dataset contains 14,332 TEST claims (comprising 203 relationships), 37% of which are plausible, as human annotations show. These TEST claims are taken from the MSCOCO dataset [22], which contains real images and captions. Therefore, the TEST dataset contains fictitious claims about the real world. [33] approach the task by constructing a multimodal similarity function between TEST claims, the plausibility of which must be evaluated, and TRAIN claims, which are plausible as real-world claims. [26] We construct the task by constructing a multimodal similarity function between TEST claims, the plausibility of which must be evaluated, and TRAIN claims, which consist of each of two pairs of abstract data sets, each of which are known as the abstract scenes, the abstract TRAIN-213."}, {"heading": "4.2. Visual Paraphrasing", "text": "Visual paraphrasing is the problem of determining whether a description pair describes the same scene or two different scenes. Each description contains three sets; the data set contains 10,020 abstract scenes and descriptions from Zitnick et al. [42]; the data set contains 30,600 description pairs, a third of which are positive (describe the same scene) and the rest negative; the TRAIN data set contains 24,000 VP pairs, while the TEST data set contains 6,060 VP pairs. We use the features provided by [41] to perform image clusters; we retain a set of 1,000 pairs (333 positive and 667 negative) from TRAIN to form a VAL set to select the number of clusters for training vis-w2v. Thus, our VP-TRAIN set includes 23,000 pairs."}, {"heading": "4.3. Text-based Image Retrieval", "text": "To verify that our model has really learned the visual grounding of concepts, we examine the task of text-based image recovery. In the face of a query, the task is to determine the image of interest by matching the query and earth-truth tuples using Word embedding. We extend the Common Sense (CS) dataset [33] (Par. 4.1) to collect three query tuples for each of the original 4260 CS TRAIN scenes. Each scene in the CS TRAIN dataset contains annotations for which objects in the scene are the primary and secondary objects in the earth-truth tuples. We highlight the primary and secondary objects in the scene, and ask the staff on AMT to name the primary, secondary objects and the relationship represented by the interaction between the primary and secondary objects. Some sample data may be more likely to attract others during interaction 3."}, {"heading": "5. Experimental Setup", "text": "We will now explain some experimental details. First, we will explain how to integrate our vis-w2v and w2v models into the text-based methods examined for common sense (CS), visual paraphrasing (VP) and text-based image analysis tasks. We will also provide evaluation details. Then, we will explain the methods and baselines we compare with for each task and discuss some design decisions. For text tokenization, we will use the NLTK toolkit. We implement vis-w2v in the same framework as the Google C implementation of word2vec3."}, {"heading": "5.1. Common Sense Assertion Classification", "text": "The task in common sense is to calculate the plausibility of a test claim based on its similarity to a series of training stages known to be plausible. Starting from a tuple (primary object tP, relation tR, secondary object tS) and a training instance, the plausibility values are calculated as follows: h (t \u2032 P) = WP (t \u2032 P) T \u00b7 WP (tiP) + WR (t \u2032 R) T \u00b7 WR (tiR) + WS (t \u2032 S) T \u00b7 WS (tiS) (3), with WP, WR, WS representing the corresponding w2v embedding spaces. The final text score is given as follows: f (t \u2032 R) = 1 | I | IDE (t \u2032 I max (h \u2032 S) T \u00b7 WS (tiS) (3), with WP, WR, WS representing the corresponding text embedding spaces."}, {"heading": "5.2. Visual Paraphrasing", "text": "For the visual paraphrasing task, we receive a description pair at test date. We must assign each pair a score indicating how likely it is that they are paraphrases, i.e. that it is the same scene. Following [23], we compute the word2vec (w2v) embedding of the sentences and insert them into their text-based evaluation function. This evaluation function combines term frequency, word cooccurence statistics and averaged w2v values to determine the final paraphrasing score. Results are evaluated as a metric based on average accuracy (AP). While we train w2v for the task, we attach the sentences from the train set of [23] to the w2v training corpus to counteract problems with excessive vocabulary."}, {"heading": "5.3. Text-based Image Retrieval", "text": "The task is to retrieve the target image from an image database for a query pair. Each image in the database has an associated truth item that describes it. We use this to classify images by calculating the similarity to the query item 4. Given the tuples of the form (primary object, relation, secondary object) or (P, R, S), we first calculate the cosinal similarity between query and basic truth for P, R, S separately, and on average the three similarities in P, R, S. In the common model, we then examine separate and common models, just as we did for common sense classification. In the separate model, we first calculate the cosinal similarity between the query and the basic truth for P, R, S, and calculate the three similarities on average."}, {"heading": "5.4. Methods and Baselines", "text": "In general, we look at two types of w2v models: those we learned from generic text, such as Wikipedia (w2v-wiki), and those we learned from visual text, such as MSCOCO (w2v-coco). Visual text is text that describes images, such as captions from the MSCOCO dataset. Embedding we learned from visual text typically contains more visual information [33]. For all applications (Sec. 4), we train vis-w2v on the corresponding datasets. vis-w2v-wiki are vis-w2v embedding we learned from w2v-wiki as a starting point, while vis-w2v-coco are the vis-w2v embedding we learned best from w2v-coco. In all settings, we4One could consider our embedding as a hash table to view them."}, {"heading": "6. Results", "text": "We present results that demonstrate the advantage of w2v over w2v in three areas of application: common sense classification, visual paraphrasing, and text-based image retrieval. We compare our approach to different baselines, as explained in paragraph 5 for each application. Finally, we train our model using real images instead of abstract scenes, and analyze differences."}, {"heading": "6.1. Common Sense Assertion Classification", "text": "As I said, common sense requires us to classify the secondary tuples as plausible or not. Here, we fix the size of the hidden units (NH) to 200 when comparing them."}, {"heading": "6.2. Visual Paraphrasing", "text": "Next, we describe our results on the Visual Paraphrasing (VP) task (Sec. 4.2), which is to determine whether a description pair describes the same scene. Each description has three sentences. Table 2 summarizes our results and compares the performance with w2v. However, we vary the size of the context window Sw and check the performance on the VAL set. We get the best results with the entire description as context window Sw. We found that applying the Principal Component Analysis (PCA) to the image characteristics before the conclusion brings marginal improvements. Our vis-w2v models give an improvement of 0.7% on both w2v-wiki and w2v-coco. Compared to the w2v-wiki approach reported in [23], we get an overall gain of 1.2% with our vis-w2v-coco model, which is embedded on a visual level. [23] We set ourselves the visual text corresponding to the scene before solving the task."}, {"heading": "6.3. Text-based Image Retrieval", "text": "Next, we present the results of the text-based image replication task (Fig. 4.3). This task requires a visual grounding, as the query and the ground truth tuples can often be very different due to the text similarity, but can describe the same scene (Fig. 3). We are investigating the generalization of the embedding learned in the common sense experiments for this task. Table 3 presents our results. Note that vis-w2v refers here to the embedding learned using the CS dataset. We find that the most powerful models vis-w2v-wiki shared and vis-w2v-coco are separate. Both give Recall @ 10 values \u2248 0.495, whereas the baseline w2v-wiki embedding gives values of 0.454 and 0.476, respectively."}, {"heading": "6.4. Real Image Experiment", "text": "Finally, we test our vis-w2v approach with real images on the CS task. That is, instead of semantic features from abstract scenes, we get surrogate labels by clustering real images from the MSCOCO dataset using fc7 features from the VGG-16 [37] CNN. We cross over to find the best number of clusters and hidden units. We perform real image experiments in two settings: 1) We use the entire MSCOCO dataset after we have removed the images whose tuples are in the CS-TEST set of [33]. This gives us a collection of \u2248 78K images to learn vis-w2v. MSCOCO dataset has a collection of 5 captions for each image. We use all of these five captions with set context5 windows to learn vis-w2v80K. 2) We create a real dataset by substituting their respective CO20 images from each MCO and S2ki."}, {"heading": "7. Discussion", "text": "Antol et al. [2] have studied the generalization of classification models that we have learned on abstract scenes, on real images. The idea is to transfer fine-grained concepts that are easier to transfer in the fully commented abstract domain to tasks in the real domain. While the abstract and real domains differ in pixels, they have a common underlying semantics. Vis-w2v embeddings can therefore form the bridge between the abstract and real domains, such as the classification of common sense. While the abstract and real domains differ in pixels, they have common semantics. Vis-w2v embeddings form the bridge between the abstract and real domains. Next, we will discuss some considerations in the design of the model. One possible design choice when learning embeddings could be a triplet-loss function where the similarity between a tuple and a visual pair exists."}, {"heading": "8. Conclusion", "text": "We learn visually grounded word embeddings called vis-w2v from abstract scenes and associated text. Abstract scenes that are trivially commented in full give us access to a rich semantic feature space. We use this feature space to learn visually grounded terms of semantic kinship between words that would be difficult to grasp with text alone and using real images. We demonstrate the visual grounding that is captured by our model in three applications, but that benefit from visual cues: 1) common sense, 2) visual paraphrasing, and 3) text-based image repetition. Our method outperforms the w2v baselines in all tasks. Our method can be considered a modality to transfer knowledge from the abstract scene to the real domain via text. All of our datasets, codes, and vis-w2v embeddings are made available for public use."}, {"heading": "A. Common Sense Assertion Classification", "text": "This year, it has come to the point where it only takes one year for it to come to a conclusion."}, {"heading": "C. Text-based Image Retrieval", "text": "Let's remember that in Text-based Image Retrieval (section 4.3 in the main work) we highlight the primary object (P) and secondary object (S) and ask Amazon Mechanical Turk (AMT) workers to describe the relationship illustrated by the tuples scene. An illustration of our tuples collection interface can be found in fig. 10. Each tupel entered into the text fields is treated as a query for text-based image retrieval. Some qualitative examples of vis-w2v-wiki success and failure cases in relation to w2v-wiki are in fig. 11. We see that vis-w2v-wiki captures terms such as the relationship between hold and open better than w2v-wiki."}, {"heading": "D. Real Image Experiments", "text": "We're the first ones we're able to implement, \"he said.\" We're the first ones we're able to implement, \"he said.\" We're the first ones we're able to reform, \"he said.\" We're the first ones we're able to reform, \"he said.\" We're the first ones we're able to reform, \"he said.\" We're the first ones we're able to reform, \"he said.\" We're the first ones we're able to reform. \""}], "references": [{"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Zero-shot learning via visual abstraction", "author": ["S. Antol", "C.L. Zitnick", "D. Parikh"], "venue": "In Computer Vision - ECCV 2014 - 13th European Conference,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Technical report,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "CoRR, abs/1411.5654,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1411.4389,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Discriminative unsupervised feature learning with  convolutional neural networks", "author": ["A. Dosovitskiy", "J.T. Springenberg", "M. Riedmiller", "T. Brox"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Predicting object dynamics in scenes", "author": ["D.F. Fouhey", "C.L. Zitnick"], "venue": "In CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "A. Yuille"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Framing image description as a ranking task: Data, models and evaluation", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "metrics. J. Artif. Intell. Res. (JAIR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Image Specificity", "author": ["M. Jas", "D. Parikh"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S.M. Katz"], "venue": "In IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1987}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "page 13,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "CoRR, abs/1411.2539,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "In Proceedings of the 24th CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Microsoft COCO: Common objects in context", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Don\u2019t just listen, use your imagination: Leveraging visual common sense for non-visual tasks", "author": ["X. Lin", "D. Parikh"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR (to appear),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Action recognition from a distributed representation of pose and appearance", "author": ["S. Maji", "L. Bourdev", "J. Malik"], "venue": "In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "CoRR, abs/1410.0210,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "CoRR, abs/1505.01121,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "CoRR, abs/1410.1090,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Neural network based language models for highly inflective languages", "author": ["T. Mikolov", "J. Kopecky", "L. Burget", "O. Glembek", "J. Cernocky"], "venue": "In 2009 IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Midge: Generating descriptions of images", "author": ["M. Mitchell", "X. Han", "J. Hayes"], "venue": "In Proceedings of the Seventh International Natural Language Generation Conference,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Learning common sense through visual abstraction", "author": ["Xiao Lin"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["M. Ren", "R. Kiros", "R.S. Zemel"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Viske: Visual knowledge extraction and question answering by visual verification of relation phrases", "author": ["F. Sadeghi", "S.K. Divvala", "A. Farhadi"], "venue": "In CVPR,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Improving word representations via global visual context", "author": ["R. Xu", "J. Lu", "C. Xiong", "Z. Yang", "J.J. Corso"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Adopting abstract images for semantic scene understanding", "author": ["C. Zitnick", "R. Vedantam", "D. Parikh"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C.L. Zitnick", "D. Parikh"], "venue": "In CVPR,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Learning the visual interpretation of sentences", "author": ["C.L. Zitnick", "D. Parikh", "L. Vanderwende"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}], "referenceMentions": [{"referenceID": 16, "context": "There is a rich history of works which do n-gram based language modeling [17, 4].", "startOffset": 73, "endOffset": 80}, {"referenceID": 3, "context": "There is a rich history of works which do n-gram based language modeling [17, 4].", "startOffset": 73, "endOffset": 80}, {"referenceID": 2, "context": "However, in recent years, neural language models [3, 30] have been growing in popularity.", "startOffset": 49, "endOffset": 56}, {"referenceID": 29, "context": "However, in recent years, neural language models [3, 30] have been growing in popularity.", "startOffset": 49, "endOffset": 56}, {"referenceID": 30, "context": "One popular choice for this vector space is the word2vec [31, 29] embedding.", "startOffset": 57, "endOffset": 65}, {"referenceID": 28, "context": "One popular choice for this vector space is the word2vec [31, 29] embedding.", "startOffset": 57, "endOffset": 65}, {"referenceID": 30, "context": "This embedding captures rich notions of semantic relatedness and compositionality between words [31].", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "While recent years have seen tremendous progress in tasks like image classification [20], detection [13], semantic segmentation [24], action recognition [25], etc.", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "While recent years have seen tremendous progress in tasks like image classification [20], detection [13], semantic segmentation [24], action recognition [25], etc.", "startOffset": 100, "endOffset": 104}, {"referenceID": 23, "context": "While recent years have seen tremendous progress in tasks like image classification [20], detection [13], semantic segmentation [24], action recognition [25], etc.", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "While recent years have seen tremendous progress in tasks like image classification [20], detection [13], semantic segmentation [24], action recognition [25], etc.", "startOffset": 153, "endOffset": 157}, {"referenceID": 39, "context": "of objects are readily accessible [40, 41].", "startOffset": 34, "endOffset": 42}, {"referenceID": 40, "context": "of objects are readily accessible [40, 41].", "startOffset": 34, "endOffset": 42}, {"referenceID": 30, "context": "Since we predict a single context output given a set of input words, our model can be viewed as a multi-modal extension of the CBOW (Continuous Bag Of Words) [31] word2vec model.", "startOffset": 158, "endOffset": 162}, {"referenceID": 22, "context": "We find that vis-w2v is useful for a number of tasks which are ostensibly in text, but can benefit from visual grounding, such as visual paraphrasing [23], common sense assertion classification [33], textbased image retrieval [15] etc.", "startOffset": 150, "endOffset": 154}, {"referenceID": 32, "context": "We find that vis-w2v is useful for a number of tasks which are ostensibly in text, but can benefit from visual grounding, such as visual paraphrasing [23], common sense assertion classification [33], textbased image retrieval [15] etc.", "startOffset": 194, "endOffset": 198}, {"referenceID": 14, "context": "We find that vis-w2v is useful for a number of tasks which are ostensibly in text, but can benefit from visual grounding, such as visual paraphrasing [23], common sense assertion classification [33], textbased image retrieval [15] etc.", "startOffset": 226, "endOffset": 230}, {"referenceID": 22, "context": "Visual paraphrasing [23] is the problem of determining whether two sentences describe the same underlying scene or not.", "startOffset": 20, "endOffset": 24}, {"referenceID": 32, "context": "Common sense assertion classification [33] is the task of modeling the plausibility of common sense assertions of the form (boy, eats, cake).", "startOffset": 38, "endOffset": 42}, {"referenceID": 30, "context": "Word Embeddings: Neural network based word embeddings [31, 6] have gained a lot of popularity in recent times.", "startOffset": 54, "endOffset": 61}, {"referenceID": 5, "context": "Word Embeddings: Neural network based word embeddings [31, 6] have gained a lot of popularity in recent times.", "startOffset": 54, "endOffset": 61}, {"referenceID": 2, "context": "These embeddings are learnt offline and then typically fed into a multi-layer Neural Network Language Model [3, 30].", "startOffset": 108, "endOffset": 115}, {"referenceID": 29, "context": "These embeddings are learnt offline and then typically fed into a multi-layer Neural Network Language Model [3, 30].", "startOffset": 108, "endOffset": 115}, {"referenceID": 38, "context": "[39] use visual cues to improve the w2v representation by predicting global visual context using fc7 features from real images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Other works use word embeddings as parts of larger systems for various tasks such as image retrieval [18], image captioning [38, 18], etc.", "startOffset": 101, "endOffset": 105}, {"referenceID": 37, "context": "Other works use word embeddings as parts of larger systems for various tasks such as image retrieval [18], image captioning [38, 18], etc.", "startOffset": 124, "endOffset": 132}, {"referenceID": 17, "context": "Other works use word embeddings as parts of larger systems for various tasks such as image retrieval [18], image captioning [38, 18], etc.", "startOffset": 124, "endOffset": 132}, {"referenceID": 8, "context": "Previous works have used surrogate labels to learn image features [9, 7].", "startOffset": 66, "endOffset": 72}, {"referenceID": 6, "context": "Previous works have used surrogate labels to learn image features [9, 7].", "startOffset": 66, "endOffset": 72}, {"referenceID": 8, "context": "Previous works have created surrogate labels using data transformations [9] or sampling [7].", "startOffset": 72, "endOffset": 75}, {"referenceID": 6, "context": "Previous works have created surrogate labels using data transformations [9] or sampling [7].", "startOffset": 88, "endOffset": 91}, {"referenceID": 40, "context": "[41, 40] learn the importance of various visual features (occurrence, co-occurrence, expression, gaze, etc.", "startOffset": 0, "endOffset": 8}, {"referenceID": 39, "context": "[41, 40] learn the importance of various visual features (occurrence, co-occurrence, expression, gaze, etc.", "startOffset": 0, "endOffset": 8}, {"referenceID": 41, "context": "[42] and [10] learn the visual interpretation of sentences and the dynamics of objects in temporal abstract scenes respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[42] and [10] learn the visual interpretation of sentences and the dynamics of objects in temporal abstract scenes respectively.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "[2] learn models of fine-grained interactions between pairs of people using visual abstractions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Lin and Parikh [23] \u201cimagine\u201d abstract scenes corresponding to text, and use the common sense depicted in these imagined scenes to solve textual tasks such as fill-in-the-blanks and paraphrasing.", "startOffset": 15, "endOffset": 19}, {"referenceID": 32, "context": "[33] classify common sense assertions as plausible or not by using textual and visual cues.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "In this work, we experiment with the tasks of [23] and [33], which are two tasks in text that could benefit from visual grounding.", "startOffset": 46, "endOffset": 50}, {"referenceID": 32, "context": "In this work, we experiment with the tasks of [23] and [33], which are two tasks in text that could benefit from visual grounding.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 4, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 7, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 37, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 27, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 18, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 13, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 31, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 20, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 68, "endOffset": 102}, {"referenceID": 7, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 122, "endOffset": 129}, {"referenceID": 34, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 122, "endOffset": 129}, {"referenceID": 0, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 157, "endOffset": 180}, {"referenceID": 10, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 157, "endOffset": 180}, {"referenceID": 26, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 157, "endOffset": 180}, {"referenceID": 33, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 157, "endOffset": 180}, {"referenceID": 25, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 157, "endOffset": 180}, {"referenceID": 11, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 157, "endOffset": 180}, {"referenceID": 15, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 207, "endOffset": 215}, {"referenceID": 17, "context": "There have been recent breakthroughs in tasks like image captioning [16, 5, 8, 38, 28, 19, 14, 32, 21], video description [8, 35], visual question answering [1, 11, 27, 34, 26, 12], aligning text and vision [16, 18], etc.", "startOffset": 207, "endOffset": 215}, {"referenceID": 35, "context": "[36, 33] study one such problem of assessing the plausibility of common sense assertions.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "[36, 33] study one such problem of assessing the plausibility of common sense assertions.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "We evaluate our vis-w2v model on the common sense assertion classification task of [33].", "startOffset": 83, "endOffset": 87}, {"referenceID": 32, "context": "We thus use the features from abstract scenes [33, 23] that are trivially fully annotated with a range of semantics.", "startOffset": 46, "endOffset": 54}, {"referenceID": 22, "context": "We thus use the features from abstract scenes [33, 23] that are trivially fully annotated with a range of semantics.", "startOffset": 46, "endOffset": 54}, {"referenceID": 32, "context": "[33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "These TEST assertions are extracted from the MSCOCO dataset [22], which contains real images and captions.", "startOffset": 60, "endOffset": 64}, {"referenceID": 32, "context": "[33] approach the task by constructing a multi-modal similarity function between TEST assertions whose plausibility is to be evaluated, and TRAIN assertions that are known to be plausible.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": ", are obtained from authors [33].", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "The visual paraphrasing (VP) task was introduced in [23].", "startOffset": 52, "endOffset": 56}, {"referenceID": 41, "context": "[42].", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "We use the features provided by [41] to perform clustering of images.", "startOffset": 32, "endOffset": 36}, {"referenceID": 32, "context": "We augment the common sense (CS) dataset [33] (Sec.", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "This is related to the notion of Image Specificity [15].", "startOffset": 51, "endOffset": 55}, {"referenceID": 32, "context": "The task in common sense assertion classification [33] is to compute the plausibility of a test assertion based on its similarity to a set of training tuples (\u03a9) known to be plausible.", "startOffset": 50, "endOffset": 54}, {"referenceID": 32, "context": "We use the values of \u03b4 used by [33] for our experiments.", "startOffset": 31, "endOffset": 35}, {"referenceID": 32, "context": "[33] share embedding parameters across P, R, S in their text based model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "The approach of [33] also has a visual similarity function that combines text and abstract scenes that is used along with this text-based similarity.", "startOffset": 16, "endOffset": 20}, {"referenceID": 32, "context": "In line with [33], we also evaluate our results using average precision (AP) as a performance metric.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "Following [23] we average the word2vec (w2v) embeddings for the sentences and plug them into their text-based evalaution scoring function.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "While training w2v for the task, we append the sentences from the train set of [23] to the w2v training corpora in order to counter outof-vocabulary issues.", "startOffset": 79, "endOffset": 83}, {"referenceID": 32, "context": "Embeddings learnt from visual text typically contain more visual information [33].", "startOffset": 77, "endOffset": 81}, {"referenceID": 32, "context": "w2v-coco (from [33]) 72.", "startOffset": 15, "endOffset": 19}, {"referenceID": 32, "context": "2 w2v-wiki (from [33]) 68.", "startOffset": 17, "endOffset": 21}, {"referenceID": 32, "context": "1 w2v-coco + vision (from [33]) 73.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "Table 1: Performance on the common sense task of [33]", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "1, the common sense task [33] requires us to classify common sense tuples as plausible or not.", "startOffset": 25, "endOffset": 29}, {"referenceID": 32, "context": "Here, we fix the size of the hidden units (NH ) to 200 when comparing to [33].", "startOffset": 73, "endOffset": 77}, {"referenceID": 32, "context": "6% with our best vis-w2v-coco model over the w2v-coco model used in [33].", "startOffset": 68, "endOffset": 72}, {"referenceID": 32, "context": "Performance Comparison: Our model outperforms the joint text + vision model from [33] that reasons about visual features for a given test tuple, which we do not.", "startOffset": 81, "endOffset": 85}, {"referenceID": 32, "context": "3% over the w2v-coco model of [33], whose textual models are all shared.", "startOffset": 30, "endOffset": 34}, {"referenceID": 32, "context": "4, our visual features in this experiment focus on interactions between primary and secondary objects in the scene [33], and capture semantics of location, pose, relative location, expression, gaze etc.", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": "In comparison to the w2v-wiki approach reported in [23], we get an overall gain of 1.", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "[23] imagine the visual scene corresponding to text to solve the task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "w2v-wiki (from [23]) 94.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "Table 2: Performance on visual paraphrasing task of [23].", "startOffset": 52, "endOffset": 56}, {"referenceID": 36, "context": "That is, instead of semantic features from abstract scenes, we obtain surrogate labels by clustering real images from the MSCOCO dataset using fc7 features from the VGG-16 [37] CNN.", "startOffset": 172, "endOffset": 176}, {"referenceID": 32, "context": "We perform real image experiments in two settings: 1) We use all of the MSCOCO dataset after removing the images whose tuples are in the CS TEST set of [33].", "startOffset": 152, "endOffset": 156}, {"referenceID": 1, "context": "[2] have studied generalization of classification models learnt on abstract scenes to real images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "Recall that the common sense assertion classification task [33] is to determine if a tuple of the form (primary object or P, relation or R, secondary object or S) is plausible or not.", "startOffset": 59, "endOffset": 63}, {"referenceID": 32, "context": "In this section, we first describe the abstract visual features used by [33].", "startOffset": 72, "endOffset": 76}, {"referenceID": 32, "context": "Our visual features are essentially the same as those used by [33]: a) Features corresponding to primary and secondary object, i.", "startOffset": 62, "endOffset": 66}, {"referenceID": 32, "context": "The number of hidden units NH is kept fixed to 200 to be comparable to the text-only baseline reported in [33].", "startOffset": 106, "endOffset": 110}, {"referenceID": 32, "context": "Note that as [33] fix the number of hidden units to 200 in their evaluation, we cannot directly compare the performance to their baseline.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "The Visual Paraphrasing (VP) task [23] is to classify whether a pair of textual descriptions are paraphrases of each other.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "Observations: From Table 4, we see improvements over the text baseline [23].", "startOffset": 71, "endOffset": 75}, {"referenceID": 36, "context": "We now present the results when training vis-w2vwith real images from MSCOCO dataset by clustering using fc7 features from the VGG-16 [37] CNN.", "startOffset": 134, "endOffset": 138}, {"referenceID": 32, "context": "Table 5: Performance on the common sense task of [33] using 78k real images with text baseline at 72.", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "Table 6: Performance on the common sense task of [33] using 4k real images with with text baseline at 68.", "startOffset": 49, "endOffset": 53}], "year": 2017, "abstractText": "We propose a model to learn visually grounded word embeddings (vis-w2v) to capture visual notions of semantic relatedness. While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world. For instance, visual grounding can help us realize that concepts like eating and staring at are related, since when people are eating something, they also tend to stare at the food. Grounding a rich variety of relations like eating and stare at in vision is a challenging task, despite recent progress in vision. We realize the visual grounding for words depends on the semantics of our visual world, and not the literal pixels. We thus use abstract scenes created from clipart to provide the visual grounding. We find that the embeddings we learn capture fine-grained visually grounded notions of semantic relatedness. We show improvements over text only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval. Our code and datasets will be available online.", "creator": "LaTeX with hyperref package"}}}