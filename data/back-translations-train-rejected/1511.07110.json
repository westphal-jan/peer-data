{"id": "1511.07110", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "On the Generalization Error Bounds of Neural Networks under Diversity-Inducing Mutual Angular Regularization", "abstract": "Recently diversity-inducing regularization methods for latent variable models (LVMs), which encourage the components in LVMs to be diverse, have been studied to address several issues involved in latent variable modeling: (1) how to capture long-tail patterns underlying data; (2) how to reduce model complexity without sacrificing expressivity; (3) how to improve the interpretability of learned patterns. While the effectiveness of diversity-inducing regularizers such as the mutual angular regularizer has been demonstrated empirically, a rigorous theoretical analysis of them is still missing. In this paper, we aim to bridge this gap and analyze how the mutual angular regularizer (MAR) affects the generalization performance of supervised LVMs. We use neural network (NN) as a model instance to carry out the study and the analysis shows that increasing the diversity of hidden units in NN would reduce estimation error and increase approximation error. In addition to theoretical analysis, we also present empirical study which demonstrates that the MAR can greatly improve the performance of NN and the empirical observations are in accordance with the theoretical analysis.", "histories": [["v1", "Mon, 23 Nov 2015 04:51:49 GMT  (368kb,D)", "http://arxiv.org/abs/1511.07110v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pengtao xie", "yuntian deng", "eric xing"], "accepted": false, "id": "1511.07110"}, "pdf": {"name": "1511.07110.pdf", "metadata": {"source": "CRF", "title": "On the Generalization Error Bounds of Neural Networks under Diversity-Inducing Mutual Angular Regularization", "authors": ["Pengtao Xie", "Yuntian Deng", "Eric Xing"], "emails": [], "sections": [{"heading": null, "text": "More recently, diversity-inducing regulation methods for latent variable models (LVMs) have been studied, which encourage the components in LVMs to be diverse in order to solve various problems related to latent variable modelling: (1) how to capture long tail patterns, (2) how to reduce the complexity of the model without sacrificing expressivity, (3) how to improve the interpretability of learned patterns. While the effectiveness of diversity-inducing regulators such as the mutual angle regularizer [1] has been empirically demonstrated, a rigorous theoretical analysis of these patterns is still lacking. In this paper, we want to close this gap and analyze how the mutual angle adjuster (MAR) affects the generalization performance of monitored LVMs. We use neural networks (NN) as a model example for conducting the study, and the analysis shows that increasing the diversity of theoretical analysis would significantly increase NAR, in addition to NR estimation errors, and increase NR error."}, {"heading": "1 Introduction", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2 Diversify Neural Networks with Mutual Angular Regularizer", "text": "In this section we review diversity-regulated latent variable models and propose diversified neural networks with mutual angle regulation."}, {"heading": "2.1 Diversity-Promoting Regularization of", "text": "The latently variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] fit elegantly into this task. The knowledge and structures hidden behind the data usually consist of multiple patterns. The semantics underlying the documents contain a number of themes [28, 10], such as politics, economics, and education. Accordingly, latently variable models are parameterized by multiple components, in which each component aims to capture a pattern in knowledge and is represented by a parameter vector. The components in Latent Dirichlet Allocation [10] are called themes, and each topic is addressed by a multinomic vector. To address the above three challenges in latently variable modeling: the outlined distribution of patterns, the conflicts between model complexity, and the lack of interpretative ability of the learned components."}, {"heading": "2.2 Neural Network with Mutual Angular Regularization", "text": "Recently, neural networks (NNs) have shown great success in many applications, such as speech recognition [19], image classification [29], machine translation [30], etc. Neural networks are nonlinear models with large capacity and rich expressiveness. If trained correctly, they can capture the complex patterns underlying the data and achieve remarkable performance in many machine learning tasks. NNNs consist of several layers of computing units and units in adjacent layers connected to weighted edges. NNNs are a typical type of LVM where each hidden unit is a component aimed at capturing the latent underlying data, and is characterized by a vector of weights that connects to units in the lower layer."}, {"heading": "3 Generalization Error Analysis", "text": "In this section we will analyze how the mutual alignment mechanism affects the generalization error of neural networks. Let L (f) = E (x, y) \u0445 p \u043c ['(x, y, f) \u2212 \u03b8 \u2212 Estimation error of hypothesis f, where p * is the distribution of the input-output pair (x, y) and \"(\u00b7) the loss function. Let f * Argminf * FL (f) be the expected risk minimizer. Let L * (f) = 1n x * (i), y (i), f) minimize the training error and f * argminf * the ability to empirically minimize risk."}, {"heading": "3.1 Setup", "text": "For an easier representation, we first consider a simple neural network whose structure is described below. Later, we extend the analysis to more complex neural networks. \u2022 Network structure: an input layer, a hidden layer, and an output level \u2022 Activation function: Lipschitz continuous function h (t) with constant L. Example: rectified linear h (t) = max (0, t), L = 1; tanh h h (t) = tanh (t), L = 1; sigmoid h (t) = sigmoid (t), L = 0.25. \u2022 Task: univariate regression \u2022 Let x-Rd use the input vector with x-2 \u2264 C1 \u2022 Let y use the response value with | y-C2 \u2022 Let wj-Rd be the weights associated with the jth hidden unit, j = 1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 m, let x-Rd use the input vector with x-C3 \u2264 C1 \u2022 Let y use the response value with jy-C2 \u2022 Let us wj-Rd be the weights associated with the jj-Rd-hidden unit (t), j-p-p-p-jp-jp-jp-jp-jp, let us assume p-jp-p-p-p-p-p-jump p-jump p-jump p-jump p-jump-p, p-jump-p-jump-p-p-jump-p-jump-p-p-jump-p-jump-p-jump-p-p-jump-p-p-jump-p-p-jump-p-p-p-jump-p-p-p-jump-p-p-jump-p-jump-p-p-p-jump-p-p-jump-p-p-jump-p-jump-p-p-p-jump-p-p-jump-p-p-jump-p-p-jump-p-jump-p-p-p-p-jump-p-p-p-p-jump-p-p-p-p-p-p-jump-p-p-p-jump-p-p-p-p-p-p-p"}, {"heading": "3.2 Estimation Error", "text": "First, we analyze the estimation error caused by MARNN and are interested in how the upper limit relates to the diversity (measured by \u03b8) of the hidden units. The main result is represented in Theorem 1.Theorem 1. theorem 1. theorem 1. Probably at least (1 \u2212 \u03b4) \u03c4L (f) \u2212 L (f \u0445) \u2264 8 (\u221a J + C2) (2LC1C3C4 + C4 (0) |) \u221a m \u221a n + (\u221a J + C2) 2 \u221a 2 log (2 / \u03b4) n (5), where J = mC24h2 (0) + L2C21C23C24 (((((m \u2212 1) cos \u03b8 + 1) + 2 \u221a mC1C3C 2 4L | h (0) | \u221a (m \u2212 1) cos terminal + 1.Note that the right side is a decreasing function w.r.t. Larger units (labeling of the hidden units are more diverse) would trigger a lower estimation margin."}, {"heading": "3.2.1 Proof", "text": "A well-established result in the learning theory is that the estimation error can be limited by the Rademacher complexity. (We proceed from the Rademacher complexity and show how the diversity of the hidden units influences this upper limit. (6) The Rademacher complexity Rn (A) of the loss function set A is defined as Rn (A) = E [sup'A 1n] n (f'i), y (i))] (6), where the order of the order of the order of the estimation error is uniform and [x) (i) (i)), y (i)} ni = 1 are i.d samples drawn from p '( i). The Rademacher complexity can be applied to the upper limit of the estimation error, as in Lemma 2.Lemma 2.Lemma 2.Lemma 2.Lemma 2.Lemma (2, 33, 34] With the probability at least 1 L'f's \u2212 f'i (\u2212 f'i)."}, {"heading": "3.2.2 Extensions", "text": "In the above analysis, we will consider a simple neural network described in Section 3.1. (In this section, we will present how to extend the analysis to more complicated cases, such as neural networks with multiple hidden layers, other loss functions and multiple outputs.Multiple hidden layers Analysis can be extended to multiple hidden layers by recursively applying the composition property of the wheel-maker complexity to the hypothesis. (We define the hypothesis that p for neural networks with P-hidden layers recursively: F0 = {f0 (x) = w0 \u00b7 x} F1 = F = {f1 (x) F1 = {f1 (x) = 1 wj m0 j = 1 wj (f0j), f0j) Fp = {fp (x)."}, {"heading": "3.3 Approximation Error", "text": "Now we assume that the target function g belongs to a function class whose smoothness is expressed at the first moment of its Fourier representation: We define the function class \u0441C as the set of functions g that satisfies the functionality g. We use the function f in F = {f | f (x) = x x x = x = x = x = x = x = j = 1 \u03b1jh (w T j x)}, which is the NN function class defined in Section 3.1, to approximate the diversity g (x). Let us take the following conditions of F:"}, {"heading": "3.4 Proof", "text": "Before we assume Theorem 3 = torrential proportions, we need the following problem: Lemma 10 = torrential proportions (torrential proportions), for all three types of proportions (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions), (torrential proportions)."}, {"heading": "4 Experiments", "text": "In this section, we present the experimental results on MAR-NN. Specifically, we are interested in how the performance of neural networks varies as the compromise parameter \u03bb in MAR-NN increases. A larger \u03bb would lead to a stronger regularization, resulting in a larger angle with lower limits. We apply MAR-NN for the phoneme classification [36] on the TIMIT1 language dataset. The inputs are MFCC characteristics extracted with context windows, and the outputs are class names generated by the HMM-GMM model by forced alignment. The feature dimension is 360 and the number of classes is 2001. In total, there are 1.1 million data instances. We use 70% data for training and 30% for testing. The activation function is sigmoid and the loss function is cross-entropy. The networks are trained with stochastic gradient lineage and the minibatch size is 100."}, {"heading": "5 Related Works", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Diversity-Promoting Regularization", "text": "Diversity-enhancing regularization approaches that encourage parameter vectors in machine learning1https: / / catalog.ldc.upenn.edu / LDC93S1Models to differentiate from each other have been extensively studied and found many applications. Early work [37, 38, 39, 40, 41, 42, 43] explored how a diverse subset of basic classifiers or regressors can be selected in group learning with the aim of improving generalization errors and reducing computational complexity. Recently, [27, 1, 25] investigated the diversity regulation of latent variable models with the aim of gathering long-term knowledge and reducing model complexity. In a multi-level classification problem, [44] proposed using the determinant of the covariance matrix to encourage classifiers to differentiate from each other."}, {"heading": "5.2 Regularization of Neural Networks", "text": "Among the extensive research on neural networks, we focused on the regulation of parameter learning of NNs [45, 46] in order to limit the complexity of models, prevent overadaptation, and achieve a better generalization of invisible data. L1 [47], L2 regulators [45, 2], Early Stop [2], Dropout [46], and DropConnect [48] are among the regulators extensively studied and applied. In this paper, we examine a new type of NN regulatory approach: diversity-enhancing regulation that has new properties and functionalities that complement existing regulators."}, {"heading": "5.3 Generalization Performance of Neural Networks", "text": "The generalization performance of neural networks, in particular the approximation errors and the estimation errors, has been extensively investigated in recent decades. [49] For the approximation error, it was shown that finite linear combinations of compositions of a fixed, univariate function and a series of affinity functions can uniformly approximate any continuous function. [50] showed that neural networks with a single hidden layer, sufficient number of hidden units and a non-constant activation function are universal approximators. [52] showed that the target function is formed in the hypothesis formed by neural networks with a hidden layer of m-units, then the approximation error rate O (1 / 2 m-dimension) is derived on the basis of different assumptions of the target function. [35] showed that the target function can be formed in the hypothesis formed by a hidden layer of m-units, then the approximation error rate O (1 / 2 m-dimension) can be achieved by a non-neural function with a signatory movement."}, {"heading": "6 Conclusions", "text": "In this paper, we analyze theoretically why diversity-enhancing regulators can lead to better latent variable modeling. Using neural networks as an example, we analyze how the generalization performance of monitored latent variable models is influenced by the mutual angle regulator. Our analysis shows that increasing the diversity of hidden units leads to a reduction in estimation errors and an increase in approximation errors. Overall, a low generalization error can be achieved if the diversity level is adequately determined. Empirical experiments show that the performance of reciprocal angle-regulated neural networks can be significantly improved and that empirical observations are consistent with the theoretical implications."}], "references": [{"title": "Diversifying restricted boltzmann machine for document modeling", "author": ["Pengtao Xie", "Yuntian Deng", "Eric P. Xing"], "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Pattern recognition and machine learning", "author": ["Christopher M Bishop"], "venue": "springer,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Data mining: concepts and techniques: concepts and techniques", "author": ["Jiawei Han", "Micheline Kamber", "Jian Pei"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Introduction to statistical pattern recognition", "author": ["Keinosuke Fukunaga"], "venue": "Academic press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Machine learning: Trends, perspectives, and prospects", "author": ["MI Jordan", "TM Mitchell"], "venue": "Science, 349(6245):255\u2013260,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Lawrence R Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Latent variable models. In Learning in graphical models, pages 371\u2013403", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Latent variable models and factor analysis", "author": ["Martin Knott", "David J Bartholomew"], "venue": "Number 7. Edward Arnold,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Mixed membership stochastic blockmodels", "author": ["Edoardo M Airoldi", "David M Blei", "Stephen E Fienberg", "Eric P Xing"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Build, compute, critique, repeat: Data analysis with latent variable models", "author": ["David M Blei"], "venue": "Annual Review of Statistics and Its Application,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Learning internal representations by error propagation", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Technical report, DTIC Document,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1985}, {"title": "Indexing by latent semantic analysis", "author": ["Scott C. Deerwester", "Susan T Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "author": ["Bruno A Olshausen", "David J Field"], "venue": "Vision research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["Daniel D Lee", "H Sebastian Seung"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["Eric P Xing", "Michael I Jordan", "Stuart Russell", "Andrew Y Ng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Bayesian haplotype inference via the dirichlet process", "author": ["Eric P Xing", "Michael I Jordan", "Roded Sharan"], "venue": "Journal of Computational Biology,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Keller: estimating time-varying interactions between genes", "author": ["Le Song", "Mladen Kolar", "Eric P Xing"], "venue": "Bioinformatics, 25(12):i128\u2013i136,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Tied boltzmann machines for cold start recommendations", "author": ["Asela Gunawardana", "Christopher Meek"], "venue": "In Proceedings of the 2008 ACM conference on Recommender systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky"], "venue": "IEEE Computer,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Peacock: Learning longtail topic features for industrial applications", "author": ["Yi Wang", "Xuemin Zhao", "Zhenlong Sun", "Hao Yan", "Lifeng Wang", "Zhihui Jin", "Liubin Wang", "Yang Gao", "Ching Law", "Jia Zeng"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Learning compact and effective distance metrics with diversity regularization", "author": ["Pengtao Xie"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Rubik: Knowledge guided tensor factorization and completion for health data analytics", "author": ["Yichen Wang", "Robert Chen", "Joydeep Ghosh", "Joshua C Denny", "Abel Kho", "You Chen", "Bradley A Malin", "Jimeng Sun"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Priors for diversity in generative latent variable models", "author": ["James Y. Zou", "Ryan P. Adams"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Probabilistic latent semantic analysis", "author": ["Thomas Hofmann"], "venue": "In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "All of statistics: a concise course in statistical inference", "author": ["Larry Wasserman"], "venue": "Springer Science & Business Media,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Neural network learning: Theoretical foundations", "author": ["Martin Anthony", "Peter L Bartlett"], "venue": "cambridge university press,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1999}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "Lecture notes of statistical learning theory", "author": ["Percy Liang"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["Andrew R Barron"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1993}, {"title": "Deep belief networks using discriminative features for phone recognition", "author": ["Abdel-rahman Mohamed", "Tara N Sainath", "George Dahl", "Bhuvana Ramabhadran", "Geoffrey E Hinton", "Michael Picheny"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Neural network ensembles, cross validation, and active learning", "author": ["Anders Krogh", "Jesper Vedelsby"], "venue": "Advances in neural information processing systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1995}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy", "author": ["Ludmila I Kuncheva", "Christopher J Whitaker"], "venue": "Machine learning,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "Diversity creation methods: a survey and categorisation", "author": ["Gavin Brown", "Jeremy Wyatt", "Rachel Harris", "Xin Yao"], "venue": "Information Fusion,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Ensemble diversity measures and their application to thinning", "author": ["Robert E Banfield", "Lawrence O Hall", "Kevin W Bowyer", "W Philip Kegelmeyer"], "venue": "Information Fusion,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2005}, {"title": "An analysis of diversity measures", "author": ["E Ke Tang", "Ponnuthurai N Suganthan", "Xin Yao"], "venue": "Machine Learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Focused ensemble selection: A diversitybased method for greedy ensemble selection", "author": ["Ioannis Partalas", "Grigorios Tsoumakas", "Ioannis P Vlahavas"], "venue": "In European Conference on Artificial Intelligence,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2008}, {"title": "Diversity regularized machine", "author": ["Yang Yu", "Yu-Feng Li", "Zhi-Hua Zhou"], "venue": "In IJCAI Proceedings- International Joint Conference on Artificial Intelligence. Citeseer,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "Ratio semi-definite classifiers", "author": ["Jonathan Malkin", "Jeff Bilmes"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "Generalization performance of regularized neural network models", "author": ["Jan Larsen", "Lars Kai Hansen"], "venue": "In Neural Networks for Signal Processing", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1994}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}, {"title": "Breaking the curse of dimensionality with convex neural networks", "author": ["Francis Bach"], "venue": "arXiv preprint arXiv:1412.8690,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko"], "venue": "Mathematics of control, signals and systems,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1989}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["Kurt Hornik"], "venue": "Neural networks,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1991}, {"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function", "author": ["Moshe Leshno", "Vladimir Ya Lin", "Allan Pinkus", "Shimon Schocken"], "venue": "Neural networks,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1993}, {"title": "A simple lemma on greedy approximation in hilbert space and convergence rates for projection pursuit regression and neural network training", "author": ["Lee K Jones"], "venue": "The annals of Statistics,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1992}, {"title": "Uniform approximation by neural networks", "author": ["Y Makovoz"], "venue": "Journal of Approximation Theory,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "While the effectiveness of diversityinducing regularizers such as the mutual angular regularizer [1] has been demonstrated empirically, a rigorous theoretical analysis of them is still missing.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6].", "startOffset": 95, "endOffset": 104}, {"referenceID": 2, "context": "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6].", "startOffset": 95, "endOffset": 104}, {"referenceID": 3, "context": "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6].", "startOffset": 95, "endOffset": 104}, {"referenceID": 4, "context": "One central task in machine learning (ML) is to extract underlying patterns from observed data [2, 3, 4], which is essential for making effective use of big data for many applications [5, 6].", "startOffset": 184, "endOffset": 190}, {"referenceID": 5, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 6, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 7, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 8, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 9, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 10, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 11, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 105, "endOffset": 130}, {"referenceID": 12, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 161, "endOffset": 181}, {"referenceID": 13, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 161, "endOffset": 181}, {"referenceID": 14, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 161, "endOffset": 181}, {"referenceID": 15, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 161, "endOffset": 181}, {"referenceID": 16, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 161, "endOffset": 181}, {"referenceID": 13, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 333, "endOffset": 341}, {"referenceID": 8, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 333, "endOffset": 341}, {"referenceID": 14, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 359, "endOffset": 367}, {"referenceID": 15, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 359, "endOffset": 367}, {"referenceID": 5, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 388, "endOffset": 395}, {"referenceID": 17, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 388, "endOffset": 395}, {"referenceID": 18, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 419, "endOffset": 427}, {"referenceID": 19, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 419, "endOffset": 427}, {"referenceID": 20, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 452, "endOffset": 460}, {"referenceID": 21, "context": "Among the various ML models and algorithms designed for pattern discovery, latent variable models (LVMs) [7, 8, 9, 10, 11, 12, 13] or latent space models (LSMs) [14, 15, 16, 17, 18] are a large family of models providing a principled and effective way to uncover knowledge hidden behind data and have been widely used in text mining [15, 10], computer vision [16, 17], speech recognition [7, 19], computational biology [20, 21] and recommender systems [22, 23].", "startOffset": 452, "endOffset": 460}, {"referenceID": 22, "context": "Although LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1].", "startOffset": 357, "endOffset": 364}, {"referenceID": 0, "context": "Although LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1].", "startOffset": 357, "endOffset": 364}, {"referenceID": 22, "context": "Although LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1].", "startOffset": 474, "endOffset": 481}, {"referenceID": 0, "context": "Although LVMs have now been widely used, several new challenges have emerged due to the dramatic growth of volume and complexity of data: (1) In the event that the popularity of patterns behind big data is distributed in a power-law fashion, where a few dominant patterns occur frequently whereas most patterns in the long-tail region are of low popularity [24, 1], standard LVMs are inadequate to capture the longtail patterns, which can incur significant information loss [24, 1].", "startOffset": 474, "endOffset": 481}, {"referenceID": 23, "context": "(2) To cope with the rapidly growing complexity of patterns present in big data, ML practitioners typically increase the size and capacity of LVMs, which incurs great challenges for model training, inference, storage and maintenance [25].", "startOffset": 233, "endOffset": 237}, {"referenceID": 24, "context": "(3) There exist substantial redundancy and overlapping amongst patterns discovered by existing LVMs from massive data, making them hard to interpret [26].", "startOffset": 149, "endOffset": 153}, {"referenceID": 25, "context": "To address these challenges, several recent works [27, 1, 25] have investigated a diversity-promoting regularization technique for LVMs, which controls the geometry of the latent space during learning to encourage the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually \u201ddifferent\u201d from each other.", "startOffset": 50, "endOffset": 61}, {"referenceID": 0, "context": "To address these challenges, several recent works [27, 1, 25] have investigated a diversity-promoting regularization technique for LVMs, which controls the geometry of the latent space during learning to encourage the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually \u201ddifferent\u201d from each other.", "startOffset": 50, "endOffset": 61}, {"referenceID": 23, "context": "To address these challenges, several recent works [27, 1, 25] have investigated a diversity-promoting regularization technique for LVMs, which controls the geometry of the latent space during learning to encourage the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually \u201ddifferent\u201d from each other.", "startOffset": 50, "endOffset": 61}, {"referenceID": 25, "context": ", clusters, topics) from data: if the model components are biased to be far apart from each other, then one would expect that such components will tend to be less overlapping and less aggregated over dominant patterns (as one often experiences in standard clustering algorithms [27]), and therefore more likely to capture the long-tail patterns.", "startOffset": 278, "endOffset": 282}, {"referenceID": 25, "context": "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 25, "context": "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].", "startOffset": 220, "endOffset": 224}, {"referenceID": 25, "context": "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].", "startOffset": 254, "endOffset": 258}, {"referenceID": 0, "context": "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].", "startOffset": 289, "endOffset": 292}, {"referenceID": 0, "context": "Several diversity-inducing regularizers such as Determinantal Point Process [27], mutual angular regularizer [1] have been proposed to promote diversity in various latent variable models including Gaussian Mixture Model [27], Latent Dirichlet Allocation [27], Restricted Boltzmann Machine [1], Distance Metric Learning [1].", "startOffset": 319, "endOffset": 322}, {"referenceID": 25, "context": "While the empirical effectiveness of diversity-inducing regularizers has been demonstrated in [27, 1, 25], their theoretical behaviors are still unclear.", "startOffset": 94, "endOffset": 105}, {"referenceID": 0, "context": "While the empirical effectiveness of diversity-inducing regularizers has been demonstrated in [27, 1, 25], their theoretical behaviors are still unclear.", "startOffset": 94, "endOffset": 105}, {"referenceID": 23, "context": "While the empirical effectiveness of diversity-inducing regularizers has been demonstrated in [27, 1, 25], their theoretical behaviors are still unclear.", "startOffset": 94, "endOffset": 105}, {"referenceID": 0, "context": "We focus on the mutual angular regularizer proposed in [1] and analyze how it affects the generalization performance of supervised latent variable models.", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "Uncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6].", "startOffset": 83, "endOffset": 98}, {"referenceID": 2, "context": "Uncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6].", "startOffset": 83, "endOffset": 98}, {"referenceID": 3, "context": "Uncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6].", "startOffset": 83, "endOffset": 98}, {"referenceID": 4, "context": "Uncover latent patterns from observed data is a central task in big data analytics [2, 3, 5, 4, 6].", "startOffset": 83, "endOffset": 98}, {"referenceID": 12, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 5, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 13, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 14, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 6, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 7, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 15, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 16, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 8, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 9, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 10, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 11, "context": "Latent variable models [14, 7, 15, 16, 8, 9, 17, 18, 10, 11, 12, 13] elegantly fit into this task.", "startOffset": 23, "endOffset": 68}, {"referenceID": 26, "context": "For instance, the semantics underlying documents contains a set of themes [28, 10], such as politics, economics and education.", "startOffset": 74, "endOffset": 82}, {"referenceID": 8, "context": "For instance, the semantics underlying documents contains a set of themes [28, 10], such as politics, economics and education.", "startOffset": 74, "endOffset": 82}, {"referenceID": 8, "context": "For instance, the components in Latent Dirichlet Allocation [10] are called topics and each topic is parametrized by a multinomial vector.", "startOffset": 60, "endOffset": 64}, {"referenceID": 25, "context": "To address the aforementioned three challenges in latent variable modeling: the skewed distribution of pattern popularity, the conflicts between model complexity and expressivity and the poor interpretability of learned patterns, recent works [27, 1, 25] propose to diversify the components in LVMs, by solving a regularized problem:", "startOffset": 243, "endOffset": 254}, {"referenceID": 0, "context": "To address the aforementioned three challenges in latent variable modeling: the skewed distribution of pattern popularity, the conflicts between model complexity and expressivity and the poor interpretability of learned patterns, recent works [27, 1, 25] propose to diversify the components in LVMs, by solving a regularized problem:", "startOffset": 243, "endOffset": 254}, {"referenceID": 23, "context": "To address the aforementioned three challenges in latent variable modeling: the skewed distribution of pattern popularity, the conflicts between model complexity and expressivity and the poor interpretability of learned patterns, recent works [27, 1, 25] propose to diversify the components in LVMs, by solving a regularized problem:", "startOffset": 243, "endOffset": 254}, {"referenceID": 25, "context": "Several regularizers have been proposed to induce diversity, such as Determinantal Point Process [27], mutual angular regularizer [1].", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "Several regularizers have been proposed to induce diversity, such as Determinantal Point Process [27], mutual angular regularizer [1].", "startOffset": 130, "endOffset": 133}, {"referenceID": 0, "context": "Here we present a detailed review of the mutual angular regularizer [1] as our theoretical analysis is based on it.", "startOffset": 68, "endOffset": 71}, {"referenceID": 17, "context": "Recently, neural networks (NNs) have shown great success in many applications, such as speech recognition [19], image classification [29], machine translation [30], etc.", "startOffset": 106, "endOffset": 110}, {"referenceID": 27, "context": "Recently, neural networks (NNs) have shown great success in many applications, such as speech recognition [19], image classification [29], machine translation [30], etc.", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "Recently, neural networks (NNs) have shown great success in many applications, such as speech recognition [19], image classification [29], machine translation [30], etc.", "startOffset": 159, "endOffset": 163}, {"referenceID": 29, "context": "According to Chebyshev inequality [31],", "startOffset": 34, "endOffset": 38}, {"referenceID": 30, "context": "[32, 33, 34] With probability at least 1\u2212 \u03b4", "startOffset": 0, "endOffset": 12}, {"referenceID": 31, "context": "[32, 33, 34] With probability at least 1\u2212 \u03b4", "startOffset": 0, "endOffset": 12}, {"referenceID": 32, "context": "[32, 33, 34] With probability at least 1\u2212 \u03b4", "startOffset": 0, "endOffset": 12}, {"referenceID": 31, "context": "literature such as [33].", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "R||(F \u2032) = R||(h \u25e6 g) = R||(h \u25e6 g + h(0)) \u2264 R||(h \u25e6 g) + 2|h(0)| n (Theorem 12 in [33]) \u2264 2LR||(g) + 2|h(0)| n (Theorem 12 in [33]) (11)", "startOffset": 82, "endOffset": 86}, {"referenceID": 31, "context": "R||(F \u2032) = R||(h \u25e6 g) = R||(h \u25e6 g + h(0)) \u2264 R||(h \u25e6 g) + 2|h(0)| n (Theorem 12 in [33]) \u2264 2LR||(g) + 2|h(0)| n (Theorem 12 in [33]) (11)", "startOffset": 126, "endOffset": 130}, {"referenceID": 33, "context": "For the ease of analysis, following [35], we assume the target function g belongs to a function class with smoothness expressed in the first moment of its Fourier representation: we define function class \u0393C as the set of functions g satisfying \u222b", "startOffset": 36, "endOffset": 40}, {"referenceID": 33, "context": "Please refer to Theorem 3 in [35] for the proof.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "minj\u2208{1,\u00b7\u00b7\u00b7 ,l} \u03c6(e, wj) > \u03b8, then as Ej is a connected set, there is a path q : t \u2208 [0, 1] \u2192 Ej connecting e to w1, and when t = 0, the path starts at q(0) = e; when t = 1, the path ends at q(1) = w1.", "startOffset": 85, "endOffset": 91}, {"referenceID": 0, "context": "We define functions rj(t) = \u03c6(q(t), wj) for t \u2208 [0, 1] and j = 1, \u00b7 \u00b7 \u00b7 , l.", "startOffset": 48, "endOffset": 54}, {"referenceID": 34, "context": "We apply MAR-NN for phoneme classification [36] on the TIMIT speech dataset.", "startOffset": 43, "endOffset": 47}, {"referenceID": 34, "context": "The inputs are MFCC features extracted with context windows and the outputs are class labels generated by the HMM-GMM model through forced alignment [36].", "startOffset": 149, "endOffset": 153}, {"referenceID": 35, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 36, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 37, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 38, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 39, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 40, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 41, "context": "Early works [37, 38, 39, 40, 41, 42, 43] explored how to select a diverse subset of base classifiers or regressors in ensemble learning, with the aim to improve generalization error and reduce computational complexity.", "startOffset": 12, "endOffset": 40}, {"referenceID": 25, "context": "Recently, [27, 1, 25] studied the diversity regularization of latent variable models, with the goal to capture long-tail knowledge and reduce model complexity.", "startOffset": 10, "endOffset": 21}, {"referenceID": 0, "context": "Recently, [27, 1, 25] studied the diversity regularization of latent variable models, with the goal to capture long-tail knowledge and reduce model complexity.", "startOffset": 10, "endOffset": 21}, {"referenceID": 23, "context": "Recently, [27, 1, 25] studied the diversity regularization of latent variable models, with the goal to capture long-tail knowledge and reduce model complexity.", "startOffset": 10, "endOffset": 21}, {"referenceID": 42, "context": "In a multi-class classification problem, [44] proposed to use the determinant of the covariance matrix to encourage classifiers to be different from each other.", "startOffset": 41, "endOffset": 45}, {"referenceID": 43, "context": "Among the vast amount of neural network research, a large body of works have been devoted to regularizing the parameter learning of NNs [45, 46], to restrict model complexity, prevent overfitting and achieve better generalization on unseen data.", "startOffset": 136, "endOffset": 144}, {"referenceID": 44, "context": "Among the vast amount of neural network research, a large body of works have been devoted to regularizing the parameter learning of NNs [45, 46], to restrict model complexity, prevent overfitting and achieve better generalization on unseen data.", "startOffset": 136, "endOffset": 144}, {"referenceID": 45, "context": "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].", "startOffset": 51, "endOffset": 55}, {"referenceID": 43, "context": "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].", "startOffset": 73, "endOffset": 80}, {"referenceID": 1, "context": "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].", "startOffset": 73, "endOffset": 80}, {"referenceID": 1, "context": "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].", "startOffset": 97, "endOffset": 100}, {"referenceID": 44, "context": "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].", "startOffset": 110, "endOffset": 114}, {"referenceID": 46, "context": "Widely studied and applied regularizers include L1 [47], L2 regularizers [45, 2], early stopping [2], dropout [46] and DropConnect [48].", "startOffset": 131, "endOffset": 135}, {"referenceID": 47, "context": "For the approximation error, [49] demonstrated that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function.", "startOffset": 29, "endOffset": 33}, {"referenceID": 48, "context": "[50] showed that neural networks with a single hidden layer, sufficiently many hidden units and arbitrary bounded and nonconstant activation function are universal approximators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[51] proved that multi-", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[52] showed that if the target function is in the hypothesis set formed by neural networks with one hidden layer of m units, then the approximation error rate is O(1/ \u221a m).", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] showed that neural networks with one layer of m hidden units and sigmoid activation function can achieve approximation error of order O(1/ \u221a m), where the target function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[53] proved that if the target function is of the form f(x) = \u222b", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "As for the estimation error, please refer to [32] for an extensive review, which introduces various estimation error bounds based on VC-dimension, flat-shattering dimension, pseudo dimension and so on.", "startOffset": 45, "endOffset": 49}], "year": 2015, "abstractText": "Recently diversity-inducing regularization methods for latent variable models (LVMs), which encourage the components in LVMs to be diverse, have been studied to address several issues involved in latent variable modeling: (1) how to capture long-tail patterns underlying data; (2) how to reduce model complexity without sacrificing expressivity; (3) how to improve the interpretability of learned patterns. While the effectiveness of diversityinducing regularizers such as the mutual angular regularizer [1] has been demonstrated empirically, a rigorous theoretical analysis of them is still missing. In this paper, we aim to bridge this gap and analyze how the mutual angular regularizer (MAR) affects the generalization performance of supervised LVMs. We use neural network (NN) as a model instance to carry out the study and the analysis shows that increasing the diversity of hidden units in NN would reduce estimation error and increase approximation error. In addition to theoretical analysis, we also present empirical study which demonstrates that the MAR can greatly improve the performance of NN and the empirical observations are in accordance with the theoretical analysis.", "creator": "LaTeX with hyperref package"}}}