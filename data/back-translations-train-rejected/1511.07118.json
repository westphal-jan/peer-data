{"id": "1511.07118", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "Cascading Denoising Auto-Encoder as a Deep Directed Generative Model", "abstract": "Recent work (Bengio et al., 2013) has shown howDenoising Auto-Encoders(DAE) become gener-ative models as a density estimator. However,in practice, the framework suffers from a mixingproblem in the MCMC sampling process and nodirect method to estimate the test log-likelihood.We consider a directed model with an stochas-tic identity mapping (simple corruption pro-cess) as an inference model and a DAE as agenerative model. By cascading these mod-els, we propose Cascading Denoising Auto-Encoders(CDAE) which can generate samples ofdata distribution from tractable prior distributionunder the assumption that probabilistic distribu-tion of corrupted data approaches tractable priordistribution as the level of corruption increases.This work tries to answer two questions. On theone hand, can deep directed models be success-fully trained without intractable posterior infer-ence and difficult optimization of very deep neu-ral networks in inference and generative mod-els? These are unavoidable when recent suc-cessful directed model like VAE (Kingma &amp;Welling, 2014) is trained on complex dataset likereal images. On the other hand, can DAEs getclean samples of data distribution from heavilycorrupted samples which can be considered oftractable prior distribution far from data mani-fold? so-called global denoising scheme.Our results show positive responses of thesequestions and this work can provide fairly simpleframework for generative models of very com-plex dataset.", "histories": [["v1", "Mon, 23 Nov 2015 06:32:57 GMT  (39kb)", "http://arxiv.org/abs/1511.07118v1", null], ["v2", "Fri, 27 Jan 2017 19:09:52 GMT  (0kb,I)", "http://arxiv.org/abs/1511.07118v2", "not completed"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dong-hyun lee"], "accepted": false, "id": "1511.07118"}, "pdf": {"name": "1511.07118.pdf", "metadata": {"source": "META", "title": "Cascading Denoising Auto-Encoder as a Deep Directed Generative Model", "authors": ["Dong-Hyun Lee"], "emails": ["DONGHYUN.LEE.DL@GMAIL.COM"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.07 118v 1 [cs.L G] 23 Nov 2We consider a steered model with a stochastic identity mapping (simple corruption process) as an inference model and a PCS as a generative model. By cascading these models, we propose Cascading Denoising AutoEncoders (CDAE), which can generate samples of the data distribution from a traceable previous distribution on the assumption that the likely distribution of corrupt data is approaching a traceable previous distribution as a level of corruption. This work tries to answer two questions: First, can deep-level models be successfully trained without intractable subordinate conclusions and difficult optimization of very deep neural networks in inference and generative models? These are inevitable when recently successfully guided models like VAE (Kingma & Welling, 2014) are trained on complex datasets such as real images."}, {"heading": "1. Introduction", "text": "rE \"s rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green.\" rE \"s rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green"}, {"heading": "2. Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Denoising Auto-Encoder as a Directed Generative Model", "text": "In general, characteristics derived from the encoder in AutoEncoder have no probable interpretation without a sampling process q (q) being performed on it (for consistency with previous distribution). Furthermore, in the generalized version of DAE (Bengio et al., 2013), encoder and decoder are not distinguished from each other. As the sampling process as corruption is based on the observed input variable, we can consider the corrupt input x (x, z) as a latent variable z with an inference model of stochastic identity mapping.We have the inference model q (x, z) and the generative model p (x, z) as the following, q (z) = q (x) = x (z) = x (z) is the distribution variable z, c (z = x) is a corruption process, pctul (x) is a parametric part (x) in which we are a fixed construction variable x (x)."}, {"heading": "2.2. Cascading DAE with gradually increasing corruption", "text": "Inspired by (Sohl-Dickstein et al., 2015), we are introducing Cascading q (q), (q) Denoising Auto-Encoders (CDAE). In the first step, we do not need to recover clean data samples at once, but only have to make a transition from the previous distribution to a more likely distribution of the data. Eventually, through several steps, we can obtain clean data samples. First, we have corrupt samples with gradually increasing corruption levels x (1),... x (k) of corruption x (0).q (x (0),..., z (k) = x (k) = q (x) = q (0) k (i), (3), p (0), p (0), p (0),..., z = x (k), k), k (x), k)."}, {"heading": "2.3. Gaussian CDAE for continuous data", "text": "For continuous data, isotropic Gauss is the simplest and most typical choice, as it can show how this framework relates to the typical educational objective of PCS, the square reconstruction error associated with Gaussian corruption. (p\u03b8 (x (i) | x (i + 1))) = N (r\u03b8 (x (i + 1), \u03c3 (i) r 2) q (x (i) | x (0) = N (x (0), \u03c3 (i) c 2) (5) The negative lower limit of variations is \u2212 L\u03b8 (x) = k \u2211 i = 1E x (i) 12\u03c3 (i \u2212 1) r2 \u0445 x (0) \u2212 r\u03b8 (i) - r\u0442 (i)."}, {"heading": "2.4. Estimating test loglikelihood", "text": "Due to the lack of a direct method for estimating the test log probability of generative PCS (Bengio et al., 2013), they use parzen window estimators. It is a practical and unique method for generative models where the exact log probability is not traceable, but the estimator has several disadvantages. (...) The more the number of steps, the greater the gap between the exact value and the limit. The gap is DKL (q (x (1.. k) | x (0)) | p (x (1.. k) | x (0))) (10) and this value may be greater than the number of steps k grows. In RAISE (Burda et al., 2015) and AIS (Neal, 2001) they estimate the exact log probability by equation (0) (10) and this value may be greater than the number of steps k grows (1)."}, {"heading": "Acknowledgments", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Generalized denoising auto-encoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "In NIPS\u20192013,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Accurate and conservative estimates of mrf loglikelihood using reverse annealing", "author": ["Burda", "Yuri", "Grosse", "Roger B", "Salakhutdinov", "Ruslan"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "In NIPS\u20192014,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Durk P", "Welling", "Max"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Annealed importance sampling", "author": ["Neal", "Radford M"], "venue": "Statistics and Computing,", "citeRegEx": "Neal and M.,? \\Q2001\\E", "shortCiteRegEx": "Neal and M.", "year": 2001}, {"title": "Deep unsupervised learning using nonequilibrium thermodynamics", "author": ["Sohl-Dickstein", "Jascha", "Weiss", "Eric A", "Maheswaranathan", "Niru", "Ganguli", "Surya"], "venue": "In Proceedings of the 31th International Conference on Machine Learning", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2015}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "J. Machine Learning Res.,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Abstract Recent work (Bengio et al., 2013) has shown how Denoising Auto-Encoders(DAE) become generative models as a density estimator.", "startOffset": 21, "endOffset": 42}, {"referenceID": 6, "context": "Introduction Denoising Auto-Encoders were originally designed for feature learning algorithm or layer-wise pre-training of deep neural network (Vincent et al., 2010).", "startOffset": 143, "endOffset": 165}, {"referenceID": 0, "context": "Recent work (Bengio et al., 2013) has shown that DAEs can also be probabilistic generative models in the context of pure unsupervised learning.", "startOffset": 12, "endOffset": 33}, {"referenceID": 0, "context": "(Bengio et al., 2013) suggested the walkback training to avoid it.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Recently, directed generative models have been the focus of attention, as for example, Variational AutoEncoder (Kingma & Welling, 2014) which showed great success both as generative models and in the context of semi-supervised learning (Kingma et al., 2014).", "startOffset": 236, "endOffset": 257}, {"referenceID": 0, "context": "Moreover, encoder and decoder are not distinguished in generalized version of DAE (Bengio et al., 2013).", "startOffset": 82, "endOffset": 103}, {"referenceID": 5, "context": "Cascading DAE with gradually increasing corruption Inspired by (Sohl-Dickstein et al., 2015), we introduce Cascading Denoising Auto-Encoders(CDAE).", "startOffset": 63, "endOffset": 92}, {"referenceID": 5, "context": "Different from diffusion process in (Sohl-Dickstein et al., 2015), we use the same DAE in all p\u03b8(x|x)\u2019s (that is, we use shared weights for all steps like Recurrent Neural Networks), and conditionally independent intermediate corruption distribution given the data sample.", "startOffset": 36, "endOffset": 65}, {"referenceID": 0, "context": "Estimating test loglikelihood Due to lack of a direct method to estimate test loglikelihood of generative DAE (Bengio et al., 2013), they use parzen window estimator.", "startOffset": 110, "endOffset": 131}, {"referenceID": 1, "context": "In RAISE (Burda et al., 2015) and AIS (Neal, 2001), they estimate the exact loglikelihood through importance weights.", "startOffset": 9, "endOffset": 29}], "year": 2017, "abstractText": "Recent work (Bengio et al., 2013) has shown how Denoising Auto-Encoders(DAE) become generative models as a density estimator. However, in practice, the framework suffers from a mixing problem in the MCMC sampling process and no direct method to estimate the test loglikelihood. We consider a directed model with an stochastic identity mapping (simple corruption process) as an inference model and a DAE as a generative model. By cascading these models, we propose Cascading Denoising AutoEncoders(CDAE) which can generate samples of data distribution from tractable prior distribution under the assumption that probabilistic distribution of corrupted data approaches tractable prior distribution as the level of corruption increases. This work tries to answer two questions. On the one hand, can deep directed models be successfully trained without intractable posterior inference and difficult optimization of very deep neural networks in inference and generative models? These are unavoidable when recent successful directed model like VAE (Kingma & Welling, 2014) is trained on complex dataset like real images. On the other hand, can DAEs get clean samples of data distribution from heavily corrupted samples which can be considered of tractable prior distribution far from data manifold? so-called global denoising scheme. Our results show positive responses of these questions and this work can provide fairly simple framework for generative models of very complex dataset. Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).", "creator": "LaTeX with hyperref package"}}}