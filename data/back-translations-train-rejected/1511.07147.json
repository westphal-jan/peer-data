{"id": "1511.07147", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "A PAC Approach to Application-Specific Algorithm Selection", "abstract": "The best algorithm for a computational problem generally depends on the \"relevant inputs,\" a concept that depends on the application domain and often defies formal articulation. While there is a large literature on empirical approaches to selecting the best algorithm for a given application domain, there has been surprisingly little theoretical analysis of the problem.", "histories": [["v1", "Mon, 23 Nov 2015 09:30:19 GMT  (507kb,D)", "http://arxiv.org/abs/1511.07147v1", "28 pages, 2 figures"], ["v2", "Fri, 2 Sep 2016 21:06:20 GMT  (520kb,D)", "http://arxiv.org/abs/1511.07147v2", "28 pages, 2 figures"]], "COMMENTS": "28 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["rishi gupta", "tim roughgarden"], "accepted": false, "id": "1511.07147"}, "pdf": {"name": "1511.07147.pdf", "metadata": {"source": "CRF", "title": "A PAC Approach to Application-Specific Algorithm Selection\u2217", "authors": ["Rishi Gupta", "Tim Roughgarden"], "emails": ["rishig@cs.stanford.edu", "tim@cs.stanford.edu"], "sections": [{"heading": null, "text": "This paper adapts concepts from statistical and online learning theory to reason via application-specific algorithm selection. Our models capture several state-of-the-art empirical and theoretical approaches to this problem, ranging from self-improving algorithms to empirical performance models, and our results identify conditions under which these approaches are guaranteed to work well. We present a framework that models algorithm selection as a statistical learning problem, and our work shows here that dimensional terms from statistical learning theory, traditionally used to measure the complexity of classes of binary and real-value functions, are relevant in a much broader algorithmic context. We also examine the online version of the algorithm selection problem and indicate possibilities and impossibilities for the existence of no-regret learning algorithms."}, {"heading": "1 Introduction", "text": "In this context, it should be noted that the case is an accident."}, {"heading": "2 Motivating Scenarios", "text": "Our Learning Framework highlights several well-known approaches covering different fields of application, right up to the problem of learning a good algorithm from data. To motivate and interpret our results, we describe some of them in detail. [Lon01] describes the performance of randomized rounding of packaging and linear programs through the pseudo-dimension of a sentence derived from the constraint matrix, and [MM14, MR15] uses dimension terms from learning theory to tie the sample complexity of learning roughly to the revenue maximization of true auctions."}, {"heading": "2.1 Example #1: Greedy Heuristic Selection", "text": "Therefore, one of the most common and challenging motivations for algorithm selection is represented by mathematically difficult optimization problems. If the available computing resources are insufficient to solve such a problem accurately, heuristic algorithms must be used. For most hard problems, our understanding of when different heuristics work well remains primitive. To be more specific, we describe a current and high-risk example of this problem, which is also in line with our model and the results in Section 3.3. Computational and Operational Research literature has many similar examples. In 2016, the FCC is scheduled to conduct a novel dual auction to buy back licenses for certain television stations and resell them to telecommunications companies for wireless broadband use. The auction is expected to generate over $20 billion for the U.S. government [CBO14]. The \"reverse\" (i.e., redeemable) phase of the auction must determine which stations to buy (and what they should pay)."}, {"heading": "2.2 Example #2: Self-Improving Algorithms", "text": "The domain of self-improving algorithms was initiated by [ACCL06], which considered sorting and clustering problems. Subsequent work [CS08, CMS10, CMS12] examined several problems in low-dimensional geometry, including maxima and convex hull problems. For a given problem, the goal is to design an algorithm that approaches the optimal algorithm for this distribution in the face of a sequence of i.e. samples from an unknown distribution across instances. Furthermore, the algorithm should use only a small amount of auxiliary space. For example, in [ACCL06] the algorithm solves each instance (on n numbers) in O (n log n) time, uses space O (n1 + c) (where c > 0 is an arbitrarily small constant), and after a polynomic number of samples optimizes runtime within a constant factor of these information rites."}, {"heading": "2.3 Example #3: Parameter Tuning in Optimization and Machine Learning", "text": "Many of the \"algorithms\" used in practice are really meta-algorithms, with a large number of free parameters that need to be instantiated by the user. For example, implementation of even the most basic version of gradient parentage requires the choice of a step size and error tolerance. An analogous problem in machine learning is \"hyperparameter optimization,\" where the goal is to adjust the parameters of a learning algorithm so that it learns (from training data) a model with high accuracy on test data, and in particular a model that does not exceed the training data. A simple example is regulated regression, such as crest regression, where a single parameter trades between the accuracy of the learned model and its \"complexity.\""}, {"heading": "2.4 Example #4: Empirical Performance Models for SAT Algorithms", "text": "The examples above are already motivating for selecting an algorithm for a problem based on the properties of the application domain. A more ambitious and sophisticated approach is to select one algorithm per instance (rather than per domain), and while it is impossible to remember the best algorithm for each possible instance, one might hope to use rough features of a problem instance as a guide where the algorithm is likely to work well. [XHHL08], for example, applied this idea to the problem of satisfaction (SAT).Their algorithm portfolio consisted of a small number (exactly, 7) of modern SAT solvers with unparalleled and widely varying runtimes across different instances. EgEg3 The authors identified a number of instance characteristics, ranging from simple features such as input size and clause / variable ratio to complex features such as Knuth's estimate of the size of the search tree and the speed of prediction of search probes Egbes.3 The next step included the creation of an \"Instorilla PM,\" whereby each of the instances will run."}, {"heading": "3 PAC Learning an Application-Specific Algorithm", "text": "This section raises the problem of selecting the best algorithm for a poorly understood application domain to learn the optimal algorithm with respect to an unknown instance distribution. Section 3.1 formally defines the basic model, Section 3.2 reviews relevant preliminary work from statistical learning theory, Section 3.3 limits the pseudo-dimension of many classes of greedy and local search heuristics, Section 3.4 reinterprets the theory of self-improving algorithms through our framework, Section 3.5 expands the basic model to capture empirical performance models and function-based algorithm selection, and Section 3.6 examines step size selection in gradient descent."}, {"heading": "3.1 The Basic Model", "text": "Our basic model consists of the following components: 1. A fixed calculation or optimization problem; 2. An unknown distribution D across instances x. 3. A set A of algorithms for A; see Sections 3.3 and 3.4 for concrete examples.4. A performance measure cost: A x x \u2192 [0, H] Indication of the performance of a given algorithm on a given instance. Two common choices for cost are the runtime of an algorithm and, for optimization problems, the objective functional value of the solution generated by an algorithm. The \"application-specific information\" is encoded by the unknown input distribution D, and the corresponding \"application-specific optimal algorithm\" AD is the algorithm that minimizes or maximizes the algorithm (depending on appropriateness)."}, {"heading": "3.2 Pseudo-Dimension and Uniform Convergence", "text": "The pseudo-dimension of H is the cardinality of the largest subset of H (or + 2 if arbitrarily large subsets of H).The pseudo-dimension of H is the largest subset of H (or + 2 if arbitrarily large subsets of H).The pseudo-dimension of H is the largest subset of H (or + 2 if arbitrarily large subsets of H).The pseudo-dimension of H is the largest subset of H (or + 2 if arbitrarily large subsets of H).The pseudo-dimension of H is the largest subset of H (or + 2 if arbitrarily large subsets of H).The pseudo-dimension is a natural extension of the UK dimension of H (or).The pseudo-dimension of H is the largest subset of H (or + 2 if arbitrarily large subsets of H).The pseudo-dimension is a natural extension of the VK dimension of H."}, {"heading": "3.3 Application: Greedy Heuristics and Extensions", "text": "The goal of this section is to bind the pseudo-dimension of many classes of greedy heuristics, including, as a special case, the family of heuristics described for the FCC double auction in Section 2.1. It will be obvious that analog calculations are possible for many other heuristics classes, and we provide several extensions to Section 3.3.4 to illustrate this point. In this section, the performance measurement costs are the objective functional value of the solution generated by heuristics in a case where we assume a maximization goal without losing universality."}, {"heading": "3.3.1 Definitions and Examples", "text": "\"Why?,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \"\", \"\", \"\", \"\", \"\", \"\", \",\" \",\", \"\", \"\", \",\" \",\", \"\", \",\" \",\" \",\" \",\" \",\" \",\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\" \",\", \"\", \"\", \",\", \"\", \"\" \",\", \",\" \"\", \",\" \",\" \"\", \"\", \"\" \",\", \"\" \",\" \",\" \"\", \",\", \"\", \"\", \"\" \"\", \",\", \",\", \"\" \"\", \",\" \",\", \"\" \",\", \"\" \"\" \"\", \",\", \",\", \"\" \",\" \"\" \",\", \"\", \"\", \",\" \",\", \"\" \",\", \"\", \"\" \",\" \"\", \"\" \"\" \",\", \",\", \"\" \"\", \",\", \",\" \",\", \"\" \"\", \",\", \"\", \"\" \"\", \",\" \"\", \",\" \"\", \"\", \"\" \",\" \",\" \"\" \",\", \"\", \"\", \",\" \"\" \",\", \",\" \",\" \",\", \"\" \"\" \",\", \",\", \"\", \",\""}, {"heading": "3.3.2 Upper Bound on Pseudo-Dimension", "text": "Next, we show that each (each, \u03b2) -single parameter family of greedy heuristics has a small pseudo-dimension. (This result applies to all the concrete examples mentioned above, and it is easy to come up with other examples (for the problems already discussed and for additional problems).Theorem 3.5 (pseudo-dimension of greedy algorithms).In particular, all our running examples are classes of heuristics with pseudo-dimensions O (log n).Proof: Recall from the definitions (Section 3.2) that we need to determine the size of each set of greedy heuristics in A. For us, a set is a fixed set of s (each with n objects) S = x1."}, {"heading": "3.3.3 Computational Considerations", "text": "The evidence for Theorem 3.5 also demonstrates the existence of an efficient ERM algorithm: the relevant O (sn\u03b2) 2 comparisons are easy to identify, the corresponding of each induced subinterval is easy to calculate (under mild assumptions about the scoring rule), and the brute force search can be used to select the best of the resulting O (sn\u03b2) 2\u043c algorithms (an arbitrary one from each subinterval).This algorithm runs in polynomial time as long as \u03b2 and \u0445 are polynomial in n, and each algorithm of A runs in polynomial time.For the family of snapsack scoring rules described above, for example, the implementation of this ERM algorithm on comparing the results of O (n2m) reduces various greedy heuristics (on each of the m sampled inputs), whereby the optimal dimension of H (log n) cannot be considered an optimal value."}, {"heading": "3.3.4 Extensions: Multiple Algorithms, Multiple Parameters, and Local Search", "text": "It is not as if this is a real problem, but a pseudo-dimensioning based on the way in which the individual algorithms adapt to the class of all such algorithms - for a second example, in which the number of interconnected components of the difference between two scoring functions (with which we determine the difference between two scoring functions)..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3.4 Application: Self-Improving Algorithms Revisited", "text": "In fact, most of them are able to trump themselves, and they are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...)"}, {"heading": "3.5 Application: Feature-Based Algorithm Selection", "text": "Previous sections examined the problem of selecting a single algorithm for use in an application domain - the use of training data to make a sound commitment to a single algorithm from a class A. As in the scenario in Section 2.4, this approach assumes that it is possible to quickly calculate some features of an instance and then select an algorithm as a function of those characteristics. During this section, we will add: 5. A set of F possible instance characteristics and a map f: X \u2192 F that calculates the characteristics of a given instance. 21For example, if X is the set of SAT instances, f (x) could encode the clause / variable ratio of instance x, nuths estimation of the search tree size [3.75], and section 3.1 describing the basic features of this model, as described in Section 3.1."}, {"heading": "3.5.1 The Case of Few Features: Estimating Selection Maps", "text": "If the quantity of possible instance values is limited, the guarantees for the basic model could be greater than an optimal factor 21. \"This is the greatest challenge for us.\" \"This is the greatest challenge for the world.\" \"This is the greatest challenge for the world.\" \"This is the greatest challenge for the world.\" \"The greatest challenge for the world.\" \"The greatest challenge for the world.\" \"\" The greatest challenge for the world. \"\" \"The greatest challenge for the world.\" \"\" The greatest challenge for the world. \"\" \"\" The greatest challenge for the world. \"\" \"\" \"The greatest challenge for the world.\" \"\" \"\" \"The greatest challenge for the world.\" \"\" \"\" \"\" \"\" The greatest challenge for the world. \"\" \"\" \"The greatest challenge for the world.\" \"\" \"The greatest challenge for the world.\" \"\" The greatest challenge for the world. \"\" \"The greatest challenge for the world.........\" \"\" The greatest challenge for the world. \"\" \""}, {"heading": "3.5.2 Feature-Based Performance Prediction", "text": "In fact, it is so that most of us are able to abide by the rules that we have set ourselves in order to fulfil them. (...) In fact, it is so that we are able to abide by the rules. (...) It is not so that we are able to change the rules. (...) \"It is not so that we are able to understand the rules. (...)\" \"It is not so that we are able to see the rules. (...)\" (...) \"(...)\" (...) \"(...)\" (() \"(()\" (() \"() () (() () () () () () () () () () () () () () () () () () () () () ()) (() ()) (() () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () ()) () () () () () () () () ()) () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () (() () () (() () () (() () () (() () () () () () () () (() () (() () () (() () () (() () () (() () () ((() () () () ("}, {"heading": "3.6 Application: Choosing the Step Size in Gradient Descent", "text": "In our last PAC example, we give sample complexity results for the problem of choosing the best step size for the gradient descent. When the gradient descent is used in practice, the step size is generally taken much larger than the ceilings suggested by theoretical guarantees and often converges in much less iterations than the step size suggested by theory. This motivates the problem of learning the step size using examples. We consider this to be a small step toward a more general reflection on the problem of learning good parameters for machine learning. In this section, we will consider an environment in which the approximation quality for all algorithms is determined and the cost of performance measurement is the runtime of the algorithm. Unlike the applications we have seen so far, the parameter space here will actually satisfy a Lipschitz-like condition, and we will be able to follow the discretization approach proposed in Remark 3.6."}, {"heading": "3.6.1 Gradient Descent Preliminaries", "text": "Let us remember the basic gradient algorithm for minimizing a function f in the face of a starting point z0 via Rn: 1. Initialize z: = z0.2. While focusing on the more interesting parameter, the step size. Larger values of this level have the potential to make further progress in each step, but run the risk of exceeding a minimum of f. Let us adopt the basic model (Section 3.1) to examine the problem of learning the best step size. There is an unknown distribution D over instances in which an instance x value consists of a function f and a starting point z0. Each algorithm of A is the basic gradient algorithm from which we proceed, with some selection criteria of a step size drawn from any fixed interval."}, {"heading": "3.6.2 A Lipschitz-like Bound on cost(A\u03c1, x) as a Function of \u03c1.", "text": "This will be the largest part of the argument. Our first problem shows that the gradient descend g is a Lipschitz function of z, even if it is greater than 2 / L. One might hope that the guaranteed progression condition would be sufficient to show that (say) g is a contraction, but the lipschitzness of g actually comes from the L smoothness. (It is not too difficult to come up with non-smooth functions that make a guaranteed progress, and where g is arbitrarily non-Lipschitz.) Lemma 3.16% g (w,) \u2212 g (y,) \u2212 g (y,)."}, {"heading": "3.6.3 Learning the Best Step Size", "text": "We can now apply the discretization approach suggested by Remark 3.6. Let K = \u03bdc 2 LZD (\u03c1u) \u2212 H. Note that, since D is an increasing function, K is less or equal to the \u03bdc 2LZD (\u03c1) \u2212 H of Lemma 3.18 for each \u03c1. Let N be a minimal K mesh. Note that we tie everything together in the following theorem. Theorem 3.19 (Learning ability of step size in gradient descent) 28 There is a learning algorithm that (1,1, \u03b4) -learns the optimal algorithm in A using m = O (log (Z / \u03bd) c) 3 samples from D.29Proof: The pseudo-dimension of AN = {A\u03c1: The pseudo-dimension of N} is at most log | N}, since AN is a finite quantity. Since AN is a finite quantity, it also trivially allows an ERM algorithm to LN."}, {"heading": "4 Online Learning of Application-Specific Algorithms", "text": "This section examines the problem of learning the best application-specific algorithm online, with the instances arriving individually. 30 The goal is to select an algorithm at each step before seeing the next instance, so that the average performance comes close to that of the best fixed algorithm, in contrast to the statistical (or \"batch\" learning setup used in Section 3), where the goal was to identify a single algorithm from a series of training instances that can be generalized well to future instances from the same distribution. For many of the motivating examples in Section 2, both the statistical and the online learning approaches are relevant. The non-distributed online learning formalism of this section may be particularly appropriate when instances cannot be modeled as i.i.d."}, {"heading": "4.1 The Online Learning Model", "text": "Our online learning model shares with the basic model of Section 3.1 a calculation or optimization problem (e.g. MWIS), a set of algorithms (e.g. a one-parameter family of greedy heuristics), and a performance metric (e.g. the total weight of the returned solution).31 Instead of modeling the specifics of an application domain over an unknown distribution D over instances, however, we use an unknown instance sequence x1,.., xT.3228You can also directly obtain a pseudo-dimensional boundary, as in the previous examples, by using a generalization known as the crushing dimension D. In particular, A has a 1.01-fat fragmentation of the dimension origin protocol | N | = Log (Z / \u03bd) c.29We again use O-dimensional boundaries (\u00b7) to suppress logarithmic factors. 30The online model is obviously relevant if the training data we can remove over time with large amounts of data even."}, {"heading": "4.2 An Impossibility Result for Worst-Case Instances", "text": "This section proves an impossibility result for no-contrition online learning algorithms for the problem of application-specific algorithm selection. We show this with the example of the running example in Section 3.3: Maximum Weight Independent Sentence (MWIS) heuristics 34, which processes vertices in the order of the non-increasing value of wv / (1 + deg (v)) for some parameters \u03c1. Let A denote the set of all these MWIS algorithms. Note that A is an infinite set and therefore the standard no-contrition results (for a finite number of actions) do not immediately apply. In online learning, infinite sets of options are usually controlled by a Lipschitz condition, which states that \"nearby\" actions always provide approximately the same performance; our group A does not have such Lipschitz property (Remark 3.6). The next section shows that these methods are purely technical problems to avoid A - there are not enough to prevent the complexity."}, {"heading": "4.2.1 A Hard Example for MWIS", "text": "We show a distribution over sequences of MWIS instances, for which each algorithm (randomized or not) has a sum of 33m2. (1) By o (1) here (and in the rest of this section) we mean a function that is constant in T, but which tends to define 0 as the number of wells n, tends to infinity. (1) Let us remember these costs (1) is the total weight of the returned wells of these costs, and we try to maximize this quantity.The key construction is the following: Lemma 4.1 For all constants 0 < r < s < s < s < s there is an MWIS instance x on most n wells of these costs (A) x) = 1, if they (r, s), and costs (A) = o (1), if we (1), if we have them < r or n > s.Proof: Let A, B, and sets of m2, m2 and 3, and 3 respectively, that we have each, mbe and 3 and 3 wells."}, {"heading": "4.3 A Smoothed Analysis", "text": "Despite the negative result above, we can show a \"low-regret\" learning algorithm for MWIS with a slight limitation on the way the instances are selected. By low-regret, we mean that regret can be made polynomically small as a function of the number of wells. Although it is not a true no-regret algorithm that would require regret, it tends to tilt 0 as a function of T. Note the enormous improvement between this and the almost complete regret of the worst case of the lower limit. As in the previous section, we use o (1) to denote the class of functions that is constant in T but tends to tilt 0 as n to infinity. We take the approach suggested by smoothed analysis [ST09], described below. Fix a parameter 0 < \u03c3 < 1. We leave each xt an arbitrary graph on n wells, but we would replace each weight distribution wv a maximum probability distribution, we would support a density of 65, Dv and D1]."}, {"heading": "4.3.1 A Low-Regret Algorithm for \u03c3-Smooth MWIS", "text": "Let's start with some definitions. For a fixed x, let's be (x) the set of transition points, namely that there are no (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x), (x)."}, {"heading": "5 Conclusions and Future Directions", "text": "Empirical work on application-based algorithm selection has far outstripped theoretical analysis of the problem, and this work has taken a first step toward eliminating this imbalance. We formulated the problem as one of learning the best algorithm or algorithm sequence from a class in terms of an unknown input distribution or input sequence. Many state-of-the-art empirical approaches to algorithm selection naturally map instances of our learning framework. We have shown that many well-studied classes of algorithms have a small pseudo-dimension, making it possible to learn a near-optimal algorithm from a relatively modest amount of data. We have demonstrated that the worst-case scenario provides guarantees that online learning algorithms are impossible without regret, but that good online learning algorithms exist in a naturally smoothed model. Our work suggests numerous widely open research directions that are worth further investigation."}, {"heading": "Acknowledgements", "text": "We are grateful for the comments of the anonymous ITCS reviewers."}, {"heading": "A A Bad Example for Gradient Descent", "text": "For each member f-F, f: R2 \u2192 R, we parameterise fI-F by finite subsets I-0, 1. We now plot an aerial view of fI. The \"squiggle\" s (I) cuts the relevant axis at exactly I (to be precise, let s (I) be the monic polynomial with roots at I. We set the starting point z0 for all instances at the end of the arrow and fix f \"and \u03c1u so that the first step of the gradient descent z0 leads from the red inclination to the middle of the black-blue range. Let xI be the instance that fI corresponds to the starting point z0."}], "references": [{"title": "Neural Network Learning: Theoretical Foundations", "author": ["M. Anthony", "P.L. Bartlett"], "venue": null, "citeRegEx": "Anthony and Bartlett.,? \\Q1999\\E", "shortCiteRegEx": "Anthony and Bartlett.", "year": 1999}, {"title": "Self-improving algorithms", "author": ["N. Ailon", "B. Chazelle", "S. Comandur", "D. Liu"], "venue": "In Symposium on Discrete Algorithms (SODA), pages 261\u2013270,", "citeRegEx": "Ailon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2006}, {"title": "Local search heuristics for k-median and facility location problems", "author": ["Vijay Arya", "Naveen Garg", "Rohit Khandekar", "Adam Meyerson", "Kamesh Munagala", "Vinayaka Pandit"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Arya et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Arya et al\\.", "year": 2004}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bergstra and Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "Convex optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Congressional Budget Office: The budget and economic outlook: 2015 to 2025", "author": ["S. U"], "venue": null, "citeRegEx": "U.,? \\Q2014\\E", "shortCiteRegEx": "U.", "year": 2014}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Self-improving algorithms for convex hulls", "author": ["Kenneth L. Clarkson", "Wolfgang Mulzer", "C. Seshadhri"], "venue": "In Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Clarkson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Clarkson et al\\.", "year": 2010}, {"title": "Self-improving algorithms for coordinate-wise maxima", "author": ["Kenneth L. Clarkson", "Wolfgang Mulzer", "C. Seshadhri"], "venue": "In Symposium on Computational Geometry (SoCG),", "citeRegEx": "Clarkson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Clarkson et al\\.", "year": 2012}, {"title": "Self-improving algorithms for Delaunay triangulations", "author": ["K.L. Clarkson", "C. Seshadhri"], "venue": "In Symposium on Computational Geometry (SoCG),", "citeRegEx": "Clarkson and Seshadhri.,? \\Q2008\\E", "shortCiteRegEx": "Clarkson and Seshadhri.", "year": 2008}, {"title": "How to solve it automatically: Selection among problem solving methods", "author": ["Eugene Fink"], "venue": "In International Conference on Artificial Intelligence Planning Systems,", "citeRegEx": "Fink.,? \\Q1998\\E", "shortCiteRegEx": "Fink.", "year": 1998}, {"title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications", "author": ["D. Haussler"], "venue": "Information and Computation,", "citeRegEx": "Haussler.,? \\Q1992\\E", "shortCiteRegEx": "Haussler.", "year": 1992}, {"title": "Predicting execution time of computer programs using sparse polynomial regression", "author": ["Ling Huang", "Jinzhu Jia", "Bin Yu", "Byung-Gon Chun", "Petros Maniatis", "Mayur Naik"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "A Bayesian approach to tackling hard computational problems", "author": ["Eric Horvitz", "Yongshao Ruan", "Carla P. Gomes", "Henry A. Kautz", "Bart Selman", "David Maxwell Chickering"], "venue": "In Conference in Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Horvitz et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Horvitz et al\\.", "year": 2001}, {"title": "Algorithm runtime prediction: Methods & evaluation", "author": ["Frank Hutter", "Lin Xu", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "Artificial Intelligence,", "citeRegEx": "Hutter et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2014}, {"title": "Counting zeros of generalized polynomials: Descartes rule of signs and Laguerres extensions", "author": ["G.J.O. Jameson"], "venue": "Mathematical Gazette,", "citeRegEx": "Jameson.,? \\Q2006\\E", "shortCiteRegEx": "Jameson.", "year": 2006}, {"title": "The traveling salesman problem: A case study in local optimization", "author": ["David S Johnson", "Lyle A McGeoch"], "venue": "Local search in combinatorial optimization,", "citeRegEx": "Johnson and McGeoch.,? \\Q1997\\E", "shortCiteRegEx": "Johnson and McGeoch.", "year": 1997}, {"title": "An evaluation of machine learning in algorithm selection for search problems", "author": ["Lars Kotthoff", "Ian P. Gent", "Ian Miguel"], "venue": "AI Communications,", "citeRegEx": "Kotthoff et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kotthoff et al\\.", "year": 2012}, {"title": "Estimating the efficiency of backtrack programs", "author": ["D.E. Knuth"], "venue": "Mathematics of Computation,", "citeRegEx": "Knuth.,? \\Q1975\\E", "shortCiteRegEx": "Knuth.", "year": 1975}, {"title": "Empirical hardness models: Methodology and a case study on combinatorial auctions", "author": ["Kevin Leyton-Brown", "Eugene Nudelman", "Yoav Shoham"], "venue": "Journal of the ACM,", "citeRegEx": "Leyton.Brown et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Leyton.Brown et al\\.", "year": 2009}, {"title": "Using the pseudo-dimension to analyze approximation algorithms for integer programming", "author": ["Philip M. Long"], "venue": "In International Workshop on Algorithms and Data Structures (WADS),", "citeRegEx": "Long.,? \\Q2001\\E", "shortCiteRegEx": "Long.", "year": 2001}, {"title": "Truth revelation in approximately efficient combinatorial auctions", "author": ["Daniel Lehmann", "Liadan Ita O\u0107allaghan", "Yoav Shoham"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Lehmann et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 2002}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K Warmuth"], "venue": "Information and computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Learning theory and algorithms for revenue optimization in second price auctions with reserve", "author": ["Mehryar Mohri", "Andres Munoz Medina"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Mohri and Medina.,? \\Q2014\\E", "shortCiteRegEx": "Mohri and Medina.", "year": 2014}, {"title": "The pseudo-dimension of near-optimal auctions", "author": ["Jamie Morgenstern", "Tim Roughgarden"], "venue": "CoRR, abs/1506.03684,", "citeRegEx": "Morgenstern and Roughgarden.,? \\Q2015\\E", "shortCiteRegEx": "Morgenstern and Roughgarden.", "year": 2015}, {"title": "Deferred-acceptance auctions and radio spectrum reallocation", "author": ["P. Milgrom", "I. Segal"], "venue": null, "citeRegEx": "Milgrom and Segal.,? \\Q2014\\E", "shortCiteRegEx": "Milgrom and Segal.", "year": 2014}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["S. Shalev-Shwartz", "S. Ben-David"], "venue": null, "citeRegEx": "Shalev.Shwartz and Ben.David.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Ben.David.", "year": 2014}, {"title": "Smoothed analysis: an attempt to explain the behavior of algorithms in practice", "author": ["Daniel A Spielman", "Shang-Hua Teng"], "venue": "Communications of the ACM,", "citeRegEx": "Spielman and Teng.,? \\Q2009\\E", "shortCiteRegEx": "Spielman and Teng.", "year": 2009}, {"title": "A note on greedy algorithms for the maximum weighted independent set problem", "author": ["Shuichi Sakai", "Mitsunori Togasaki", "Koichi Yamazaki"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "Sakai et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sakai et al\\.", "year": 2003}, {"title": "SATzilla: Portfoliobased algorithm selection for SAT", "author": ["Lin Xu", "Frank Hutter", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "J. Artificial Intelligence Research (JAIR),", "citeRegEx": "Xu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2008}, {"title": "Hydra-mip: Automated algorithm configuration and selection for mixed integer programming. In RCRA workshop on experimental evaluation of algorithms for solving problems with combinatorial explosion at the international joint conference on artificial intelligence", "author": ["Lin Xu", "Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2011}, {"title": "SATzilla2012: Improved algorithm selection based on cost-sensitive classification models", "author": ["Lin Xu", "Frank Hutter", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "In International Conference on Theory and Applications of Satisfiability Testing (SAT),", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}], "referenceMentions": [], "year": 2017, "abstractText": "The best algorithm for a computational problem generally depends on the \u201crelevant inputs,\u201d a concept that depends on the application domain and often defies formal articulation. While there is a large literature on empirical approaches to selecting the best algorithm for a given application domain, there has been surprisingly little theoretical analysis of the problem. This paper adapts concepts from statistical and online learning theory to reason about application-specific algorithm selection. Our models capture several state-of-the-art empirical and theoretical approaches to the problem, ranging from self-improving algorithms to empirical performance models, and our results identify conditions under which these approaches are guaranteed to perform well. We present one framework that models algorithm selection as a statistical learning problem, and our work here shows that dimension notions from statistical learning theory, historically used to measure the complexity of classes of binaryand real-valued functions, are relevant in a much broader algorithmic context. We also study the online version of the algorithm selection problem, and give possibility and impossibility results for the existence of no-regret learning algorithms.", "creator": "LaTeX with hyperref package"}}}