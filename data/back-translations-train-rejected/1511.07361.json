{"id": "1511.07361", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "Interpretable Two-level Boolean Rule Learning for Classification", "abstract": "This paper proposes algorithms for learning two-level Boolean rules in Conjunctive Normal Form (CNF, i.e. AND-of-ORs) or Disjunctive Normal Form (DNF, i.e. OR-of-ANDs) as a type of human-interpretable classification model, aiming for a favorable trade-off between the classification accuracy and the simplicity of the rule. Two formulations are proposed. The first is an integer program whose objective function is a combination of the total number of errors and the total number of features used in the rule. We generalize a previously proposed linear programming (LP) relaxation from one-level to two-level rules. The second formulation replaces the 0-1 classification error with the Hamming distance from the current two-level rule to the closest rule that correctly classifies a sample. Based on this second formulation, block coordinate descent and alternating minimization algorithms are developed. Experiments show that the two-level rules can yield noticeably better performance than one-level rules due to their dramatically larger modeling capacity, and the two algorithms based on the Hamming distance formulation are generally superior to the other two-level rule learning methods in our comparison. A proposed approach to binarize any fractional values in the optimal solutions of LP relaxations is also shown to be effective.", "histories": [["v1", "Mon, 23 Nov 2015 18:52:21 GMT  (64kb)", "http://arxiv.org/abs/1511.07361v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["guolong su", "dennis wei", "kush r varshney", "dmitry m malioutov"], "accepted": false, "id": "1511.07361"}, "pdf": {"name": "1511.07361.pdf", "metadata": {"source": "CRF", "title": "Interpretable Two-level Boolean Rule Learning for Classification", "authors": ["Guolong Su", "Dennis Wei", "Kush R. Varshney", "Dmitry M. Malioutov"], "emails": ["guolong@mit.edu)", "dmalioutov@us.ibm.com)"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.07 361v 1 [cs.L G] 23 Nov 201 5In this thesis, algorithms for learning two-stage Boolean rules in the conjunctive normal form (CNF, i.e. AND-of-ORs) or disjunctive normal form (DNF, i.e. OR-of-ANDs) are proposed as a kind of human interpretable classification model aiming at a beneficial balance between classification accuracy and the simplicity of the rule. Two formulations are proposed: the first is an integer program whose objective function is a combination of the total number of errors and the total number of characteristics normally used. We generalize a previously proposed linear programming process (LP) from single-stage to two-stage rules. The second formulation replaces the 0-1 classification error with the hamming removal from the current two-stage rule, which correctly classifies a worm number."}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2 Review of Existing Work", "text": "The two Boolean rules in this paper are examples of sparse decision rules that are well understood as characteristics of general training. [4] Decision trees form a different class that can represent the same Boolean function and can be converted into decision rules. [5] The single-step rule learning method in [10] forms a building block in the current work environment. [6] A single-step rule interpretation in [10] a standard binary superior class work is considered. [7] The single-step rule learning method in [10] forms a building block in the current work environment. [8] A single-step rule interpretation in [10] A standard binary superior classification problem is considered. [10] We have a training dataset with n labeled examples; the ith sample has a binary label yi (0, 1) and in the totality of binary features."}, {"heading": "3 Problem Formulation", "text": "\"The goal is to learn a two-step Boolean rule, for which we can find a two-step solution.\" (1) \"The goal is to learn a two-step Boolean rule.\" (2) \"The goal is to find a two-step solution.\" (2) \"The goal is to find a two-step solution.\" (3) \"The goal is to find a two-step solution.\" (3) \"The goal is to find a two-step solution.\" (4) \"The goal is to find a two-step solution.\" (4) \"The goal is to find a two-step solution.\" (4) \"The goal is to find a two-step solution.\""}, {"heading": "4 Optimization Approaches", "text": "Since \"AND\" and \"OR\" are defined only on two levels, there are different interdisciplinary approaches on both levels. Based on the formulation in Section 3.2, we propose the block flow problem if the result of the LP is non-binary and the alternative minimization algorithms in Section 4.3 for the target (3.12). Since all algorithms use the LP relativizations, Section 4.4 looks at the binarization problem if the result of the LP is non-binary. 4.1 This approach looks at the 0-1 formulation and directly generalizes the idea of replacing binary operations \"AND\" and \"OR\" with linear algebraic operations as used on one level."}, {"heading": "5 Numerical Evaluation", "text": "This section evaluates the algorithms using UCI repository data sets [9], including the connectionist benchmark Sonar (Sonar), BUPA liver disease (Leber), Pima Indian Diabetes (Pima) and Parkinsons (Parkin). However, the continuously evaluated properties in these data sets are converted to binary using quantitative thresholds. The aim is to learn a DNF rule (OR-of-ANDs) from each dataset. We use Profiled 10-fold cross-validation and then consider the average test and training error rates over these 10 folds. All LPs are dissolved by CPLEX version 12. The economical parameter profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile profile"}, {"heading": "5.3 Pareto Fronts with Different Numbers of", "text": "Clauses The Pareto fronts with different number of clauses are shown in Fig. 1, where we vary R from 1 to 5. Fig. 1 (a) and (b) show the average test and training error rates of the alterating minimization algorithm on the Pima dataset, while Fig. 1 (c) and (d) show the error rates of the block coordinate descent algorithm on the Liver dataset. Each point in the figure corresponds to the pair of the average error rate and the average number of features in the learned DNF rule, which is reached at one of the 18 values of \u03b8, and the Pareto fronts are discent algorithms are denoted by lines for ease of visualization.The following observations are implied by Fig. 1. A comparison of the Pareto fronts of R = 1 and R > 1 suggests that two-level rules have more trade off between accuracy and simplicity."}, {"heading": "6 Conclusion", "text": "This paper has provided two optimization-based formulations for two-stage Boolean rule learning, the first based on 0-1 classification errors and the second based on hamming distances. Three algorithms have been developed, namely two-stage LP relaxation, block-coordinate descent and alternating minimization. A redundancy-conscious binarization method has been introduced. Numerical results show that two-stage Boolean rules have a significantly lower error rate and more flexible accuracy and simplicity compromises than single-stage rules for certain complex datasets. However, too many clauses can lead to revision, and the optimal number of clauses may depend on the complexity of the datasets. The block-coordinate-descent algorithm and alternating minimization algorithms can work with noisy datasets and generally surpass the other two-level rule learning methods in our comparison."}, {"heading": "Acknowledgment", "text": "The authors thank V. S. Iyengar, A. Mojsilovic, K. N. Ramamurthy and E. van den Berg for talks and support. The authors are grateful for the help with experiments using data sets from [9]."}], "references": [{"title": "An implementation of logical analysis of data", "author": ["E. Boros", "P.L. Hammer", "T. Ibaraki", "A. Kogan", "E. Mayoraz", "I. Muchnik"], "venue": "IEEE Trans. Knowl. Data Eng., 12 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Minimization of Boolean complexity in human concept learning", "author": ["J. Feldman"], "venue": "Nature, 407 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Comprehensible classification models \u2013 a position paper", "author": ["A.A. Freitas"], "venue": "ACM SIGKDD Explor., 15 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Proof theory of many-valued logic\u2013linear optimization\u2013logic design: Connections and interactions", "author": ["R. H\u00e4hnle"], "venue": "Soft Comput., 1 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Computer-aided auditing of prescription drug claims", "author": ["V.S. Iyengar", "K.B. Hermiz", "R. Natarajan"], "venue": "Health Care Manag. Sci., 17 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "On the learnability of Boolean formulae", "author": ["M. Kearns", "M. Li", "L. Pitt", "L. Valiant"], "venue": "Proc. Annu. ACM Symp. on Theory of Comput.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1987}, {"title": "Building interpretable classifiers with rules using Bayesian analysis", "author": ["B. Letham", "C. Rudin", "T.H. McCormick", "D. Madigan"], "venue": "Department of Stat. Tech. Report tr609, Univ. of Washington, ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "http://archive.ics.uci.edu/ml, Univ. of Calif., Irvine, School of Information and Computer Sciences", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Exact rule learning via Boolean compressed sensing", "author": ["D.M. Malioutov", "K.R. Varshney"], "venue": "Proc. Int. Conf. Mach. Learn.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "The set covering machine", "author": ["M. Marchand", "J. Shawe-Taylor"], "venue": "J. Mach. Learn. Res., 3 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "ESPRESSO- SIGNATURE: A new exact minimizer for logic functions", "author": ["P.C. McGeer", "J.V. Sanghavi", "R.K. Brayton", "A.L. Sangiovanni-Vincentelli"], "venue": "IEEE Trans. VLSI Syst., 1 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "Binary rule generation via Hamming Clustering", "author": ["M. Muselli", "D. Liberati"], "venue": "IEEE Trans. Knowl. Data Eng., 14 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Simplifying decision trees", "author": ["J.R. Quinlan"], "venue": "Int. J. Man- Mach. Studies, 27 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1987}, {"title": "Learning decision lists", "author": ["R.L. Rivest"], "venue": "Mach. Learn., 2 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1987}, {"title": "Bayesian Or\u2019s of And\u2019s for interpretable classification with application to context aware recommender systems", "author": ["T. Wang", "C. Rudin", "F. Doshi-Velez", "Y. Liu", "E. Klampfl", "P. MacNeille"], "venue": "tech. report, MIT", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "As an example, a Boolean rule in [8] for the prediction of 10 year coronary heart disease (CHD) risk for a 45", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "An advantage of Boolean rules is high human interpretability [3, 4].", "startOffset": 61, "endOffset": 67}, {"referenceID": 2, "context": "An advantage of Boolean rules is high human interpretability [3, 4].", "startOffset": 61, "endOffset": 67}, {"referenceID": 2, "context": "Human interpretability has high importance in a wide range of applications such as medicine and business [4, 8], where results from prediction models are generally presented to a human decision maker/agent who makes the final decision.", "startOffset": 105, "endOffset": 111}, {"referenceID": 6, "context": "Human interpretability has high importance in a wide range of applications such as medicine and business [4, 8], where results from prediction models are generally presented to a human decision maker/agent who makes the final decision.", "startOffset": 105, "endOffset": 111}, {"referenceID": 6, "context": "As an example, medical diagnosis models [8] may predict a high risk of certain diseases for a patient; a doctor then needs to know the underlying factors to compare with his/her domain knowledge, take the correct action, and communicate with the patient.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "Another application requiring interpretability is fraud detection [6], where convincing reasons are needed to justify further auditing.", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "sparsity) [3].", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "2 Review of Existing Work The two-level Boolean rules in this work are examples of sparse decision rule lists, one of the major classes of interpretable models [4].", "startOffset": 160, "endOffset": 163}, {"referenceID": 12, "context": "Decision trees constitute another class that can represent the same Boolean functions and be converted to decision rule lists [14], although they may differ in the representation complexity depending on the dataset [4].", "startOffset": 126, "endOffset": 130}, {"referenceID": 2, "context": "Decision trees constitute another class that can represent the same Boolean functions and be converted to decision rule lists [14], although they may differ in the representation complexity depending on the dataset [4].", "startOffset": 215, "endOffset": 218}, {"referenceID": 8, "context": "The one-level rule learning method in [10] forms a building block in the current work.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "1 One-level Rule Learning in [10] A standard binary supervised classification problem is considered in [10].", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "1 One-level Rule Learning in [10] A standard binary supervised classification problem is considered in [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "The class of classifiers considered in [10] consists of one-level Boolean rules, which take only a conjunction (or disjunction) of selected features.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "Due to this equivalence, algorithms in [10] focus on the disjunctive rule", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "Replacing binary operators with linear-algebraic expressions, a mixed integer program is formulated for the one-level rule learning problem [10]:", "startOffset": 140, "endOffset": 144}, {"referenceID": 8, "context": "4) into 0 \u2264 wj \u2264 1 yields a linear program that is efficiently solvable [10].", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "Sufficient conditions for the relaxation to be exact are discussed in [10].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "In fact, if we include the negations of input features, then two-level rules can represent any Boolean function of the input features [12, 13], which does not hold for one-level rules.", "startOffset": 134, "endOffset": 142}, {"referenceID": 11, "context": "In fact, if we include the negations of input features, then two-level rules can represent any Boolean function of the input features [12, 13], which does not hold for one-level rules.", "startOffset": 134, "endOffset": 142}, {"referenceID": 8, "context": "Two algorithms are proposed in [10] for rule set learning, based on the one-level learning algorithm.", "startOffset": 31, "endOffset": 35}, {"referenceID": 9, "context": "The first algorithm uses the set covering approach [11] and obtains a two-level rule.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "The second algorithm for rule sets in [10] applies boosting, in which the predictor is a weighted combination of rules rather than a two-level rule and thus hinders interpretability.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "Another algorithm for DNF learning is the Hamming Clustering (HC) approach [13], which uses greedy methods to iteratively cluster samples in the same category and with features close to each other in Hamming", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "Experiments in [13] seem to imply HC produces a high number of clauses, which hinders interpretability.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "First, Bayesian approaches in [8, 16] typically utilize approximate inference algorithms to obtain the MAP solution or to produce posterior distribution over decision lists.", "startOffset": 30, "endOffset": 37}, {"referenceID": 14, "context": "First, Bayesian approaches in [8, 16] typically utilize approximate inference algorithms to obtain the MAP solution or to produce posterior distribution over decision lists.", "startOffset": 30, "endOffset": 37}, {"referenceID": 0, "context": "Second, Logical Analysis of Data (LAD) [2] learns patterns for both positive and negative samples by techniques such as set covering [11], and typically builds a classifier by a weighted combination of the patterns, i.", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "Second, Logical Analysis of Data (LAD) [2] learns patterns for both positive and negative samples by techniques such as set covering [11], and typically builds a classifier by a weighted combination of the patterns, i.", "startOffset": 133, "endOffset": 137}, {"referenceID": 5, "context": "Third, learnability of Boolean formulae is considered in [7] from the perspective of probably approximately correct (PAC) learning.", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "Different from our problem, the setup of [7] and related work typically assumes positive or negative samples can be generated on demand and without noise.", "startOffset": 41, "endOffset": 44}, {"referenceID": 10, "context": "Fourth, two-level logic optimization in circuit design [12] considers simplifying two-level rules that exactly match a given truth table.", "startOffset": 55, "endOffset": 59}, {"referenceID": 8, "context": "The second motivation for this formulation is to avoid identical clauses by training each clause with a different subset of samples, as done in [10, 11, 15].", "startOffset": 144, "endOffset": 156}, {"referenceID": 9, "context": "The second motivation for this formulation is to avoid identical clauses by training each clause with a different subset of samples, as done in [10, 11, 15].", "startOffset": 144, "endOffset": 156}, {"referenceID": 13, "context": "The second motivation for this formulation is to avoid identical clauses by training each clause with a different subset of samples, as done in [10, 11, 15].", "startOffset": 144, "endOffset": 156}, {"referenceID": 8, "context": "1) in one-level rule learning [10].", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "7) and directly generalizes the idea of replacing binary operations \u201cAND\u201d and \u201cOR\u201d with linear-algebraic operations, as used in one-level rule learning [10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 3, "context": "The \u201cOR\u201d function has the following interpolations [5]", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "The logical \u201cAND\u201d operator also has the tightest convex and concave interpolations as [5]", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "Each iteration updates a single clause with all the other (R \u2212 1) clauses fixed, using the one-level rule learning algorithm [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 8, "context": "To update the r 0 clause, we remove all samples that have label yi = 0 and are already predicted as 0 by at least one of the other (R \u2212 1) clauses, and then update the r 0 clause with the remaining samples using the one-level rule learning algorithm [10].", "startOffset": 250, "endOffset": 254}, {"referenceID": 8, "context": "For example, one option is the set covering method [10], as is used in our experiments.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "Explicitly, the update of the r clause will first remove all the samples with vi,r = DC, and then utilize the one-level rule learning algorithm [10].", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "9), tie breaking is achieved by a \u201cclustering\u201d approach similar to the spirit of [13].", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "The set covering approach [10] is used in our experiments.", "startOffset": 26, "endOffset": 30}, {"referenceID": 8, "context": "Although there are conditions under which the optimal solution to the LP relaxation for one-level rule learning is guaranteed to be binary [10], we are not aware of similar guarantees in two-level rule learning; in addition, these conditions are unlikely to always hold with a real-world and noisy dataset.", "startOffset": 139, "endOffset": 143}, {"referenceID": 8, "context": "A straight-forward binarization method is to compare each wj,r from LP with a specified threshold, as done in [10].", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "1 Setup This section evaluates the algorithms with UCI repository datasets [9], including connectionist bench sonar (Sonar), BUPA liver disorders (Liver), Pima Indian diabetes (Pima), and Parkinsons (Parkin).", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "Algorithms in comparison and their abbreviations are: two-level LP relaxation (TLP), block coordinate descent (BCD), alternating minimization (AM), set covering [10, 11] (SCS: simple binarization with threshold at 0.", "startOffset": 161, "endOffset": 169}, {"referenceID": 9, "context": "Algorithms in comparison and their abbreviations are: two-level LP relaxation (TLP), block coordinate descent (BCD), alternating minimization (AM), set covering [10, 11] (SCS: simple binarization with threshold at 0.", "startOffset": 161, "endOffset": 169}, {"referenceID": 13, "context": "2, SCN: new redundancy-aware binarization), decision list [15] in IBM SPSS (DL), and decision trees [14] (C5.", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "2, SCN: new redundancy-aware binarization), decision list [15] in IBM SPSS (DL), and decision trees [14] (C5.", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "0, and CART are cited from [10].", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "We refer the reader to [10] for results from other classifiers that are generally not interpretable; the accuracy of our algorithms is generally quite competitive with them.", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "As a preliminary comparison with the Hamming Clustering approach [13], we consider \u201cPima\u201d which is the only dataset shared by this work, [10], and [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "As a preliminary comparison with the Hamming Clustering approach [13], we consider \u201cPima\u201d which is the only dataset shared by this work, [10], and [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "As a preliminary comparison with the Hamming Clustering approach [13], we consider \u201cPima\u201d which is the only dataset shared by this work, [10], and [13].", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "0% test error rate with an average of 85 features used in the rule as reported in [13], while block coordinate descent and alternating minimization algorithms have lower minimal error rates of 24.", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "There are two differences in setup: HC uses 12-fold cross validation [13], while we use 10-fold; the parameters to convert continuous features into binary may potentially be different.", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "The authors are thankful for the assistance in experiments by using datasets from [9].", "startOffset": 82, "endOffset": 85}], "year": 2015, "abstractText": "This paper proposes algorithms for learning two-level Boolean rules in Conjunctive Normal Form (CNF, i.e. AND-of-ORs) or Disjunctive Normal Form (DNF, i.e. OR-of-ANDs) as a type of human-interpretable classification model, aiming for a favorable trade-off between the classification accuracy and the simplicity of the rule. Two formulations are proposed. The first is an integer program whose objective function is a combination of the total number of errors and the total number of features used in the rule. We generalize a previously proposed linear programming (LP) relaxation from onelevel to two-level rules. The second formulation replaces the 0-1 classification error with the Hamming distance from the current two-level rule to the closest rule that correctly classifies a sample. Based on this second formulation, block coordinate descent and alternating minimization algorithms are developed. Experiments show that the two-level rules can yield noticeably better performance than one-level rules due to their dramatically larger modeling capacity, and the two algorithms based on the Hamming distance formulation are generally superior to the other two-level rule learning methods in our comparison. A proposed approach to binarize any fractional values in the optimal solutions of LP relaxations is also shown to be effective.", "creator": "LaTeX with hyperref package"}}}