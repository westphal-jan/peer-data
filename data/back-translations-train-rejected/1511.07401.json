{"id": "1511.07401", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "MazeBase: A Sandbox for Learning from Games", "abstract": "This paper introduces an environment for simple 2D maze games, designed as a sandbox for machine learning approaches to reasoning and planning. Within it, we create 10 simple games based on algorithmic tasks (e.g. embodying simple if-then statements). We deploy a range of neural models (fully connected, convolutional network, memory network) on these games, with and without a procedurally generated curriculum. We show that these architectures can be trained with reinforcement to respectable performance on these tasks, but are still far from optimal, despite their simplicity. We also apply these models to games involving combat, including StarCraft, demonstrating their ability to learn non-trivial tactics which enable them to consistently beat the in-game AI.", "histories": [["v1", "Mon, 23 Nov 2015 20:23:53 GMT  (434kb,D)", "https://arxiv.org/abs/1511.07401v1", null], ["v2", "Thu, 7 Jan 2016 18:41:14 GMT  (166kb,D)", "http://arxiv.org/abs/1511.07401v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["sainbayar sukhbaatar", "arthur szlam", "gabriel synnaeve", "soumith chintala", "rob fergus"], "accepted": false, "id": "1511.07401"}, "pdf": {"name": "1511.07401.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sainbayar Sukhbaatar"], "emails": ["sainbar@cs.nyu.edu", "aszlam@fb.com", "gab@fb.com", "soumith@fb.com", "robfergus@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, they have served as a touchstone for machine learning approaches (Perez et al., 2014). GVG AI competition (Perez et al., 2014) uses a range of 2D arcade games to compare planning and search methods (Mnih et al., 2014). In this paper, we present the MazeBase game environment, which complements existing frameworks in several key areas. \u2022 The focus is on learning to understand the environment, not testing algorithms for search and planning."}, {"heading": "1.1 RELATED WORK", "text": "The MazeBase environment can be thought of as a small practical step toward some of the ideas detailed in Mikolov et al. (2015). In particular, linking with the agent and the environment in (quasi) natural language was inspired by discussions with the authors of this work. However, our ambitions are more local, limiting ourselves to the limits within which current models fail (but almost succeed) rather than aiming at a global view of a path toward AI. For example, we specifically avoid algorithmic tasks that require unrestricted repetitions or loops, as we find there are many difficulties in learning. Furthermore, we are responsible for the examples described below, such as the noise from amplification through discrete actions. In non-game environments, there have been many difficulties in learning recently."}, {"heading": "2 ENVIRONMENT AND TASKS", "text": "In the specific examples below, the dimensions range from 3 to 10 on each side, but these can of course be set as the user likes. \u2022 Any place in the net may be empty, or may contain one or more elements. \u2022 Water: The agent can move into a grid position with water, but there will be additional costs (fixed at \u2212 0.2 in the games below) for doing so. \u2022 Switch: an insoluble obstacle that does not bring the agent into that grid position. \u2022 Water: The agent can move into a grid position with water, but there will be additional costs (fixed at \u2212 0.2 in the games below)."}, {"heading": "2.1 TASKS", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "3 MODELS", "text": "We examine several different types of models: (i) simple linear, (ii) multi-layer neural nets, (iii) convolutional nets and (iv) end-to-end memory networks (Weston et al., 2015b; Sukhbaatar et al., 2015) While the input format for each approach (detailed below) is the same, the outputs are the same: a probability distribution via discrete actions {N, S, E, W, toggle switch, pushN, push-E, push-W}; and a continuous baseline value that predicts the expected reward. We consider models that return in state order such as RNNNs or LSTMs because these tasks are discussed above, Markovian.Linear: For a simple baseline, we take the existence of any word-location pair on the largest network surface we consider."}, {"heading": "4 TRAINING PROCEDURES", "text": "First of all, we play the game by feeding the current state xt into the model and sampling the next actions based on its results. At the end of the game, we update the model parameters. Instead of using a single baseline b value for each state, we let the model output a specific baseline value for the current state. This is achieved by adding an additional header for the output of the baseline value to the models. In addition to maximizing the expected reward from the policy gradient, the models are also trained to minimize the distance between the baseline value and the actual reward."}, {"heading": "4.1 CURRICULUM", "text": "A key feature of our environment is the ability to programmatically vary all the features of a particular game (Bengio et al., 2009).As we show, this is very important to avoid local minimums and to learn superior models.Each game has many variables that influence the difficulty: generic are: maze dimensions (height / width) and the fraction of blocks & water.For switching games (switches, light keys), the number of switches and colors can be varied.For target-based games (multi-targets, mandatory targets, exclusion), the variables are the number of goals (and active targets).For the fighting game Kiting (see section 6), we vary the number of agents and enemies and their initial health."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "In fact, the fact is that most of them will be able to play by the rules that they have shown in recent years, and they will be able to play by the rules that they have set themselves."}, {"heading": "6 COMBAT GAMES", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is not a country, but a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which"}, {"heading": "7 DISCUSSION", "text": "The MazeBase environment enables the easy creation of games and the precise control of their behavior. This enabled us to quickly develop a set of 10 simple games with algorithmic components and to evaluate them using a series of neural models.The flexibility of the environment enabled the creation of syllabuses for each game that supported the training of the models and resulted in superior test performance.Even with the help of a syllabus, the models in most cases fell short of optimum performance.The memory networks were able to solve some tasks that the fully networked models and convex networks could not solve, although the overall performance was similar. This suggests that existing neural models lack some basic skills required to solve algorithmic thinking. Potential candidates include: the ability to plan or predict the outcome of actions and a more complex memory (it is noteworthy that the MemNN exceeded the others in tasks by incorporating large information elements to be able in the game environment).We also demonstrated how to use the MazeBase environment to directly in order to develop the target architectures that are able to be used in the game environment."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Computer Go: an AI oriented survey", "author": ["Bouzy", "Bruno", "Cazenave", "Tristan"], "venue": "Artificial Intelligence,", "citeRegEx": "Bouzy et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bouzy et al\\.", "year": 2001}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "In arXiv preprint:", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Deep learning for real-time atari game play using offline monte-carlo tree search planning", "author": ["Guo", "Xiaoxiao", "Singh", "Satinder", "Lee", "Honglak", "Lewis", "Richard L", "Wang", "Xiaoshi"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": null, "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "A roadmap towards machine intelligence", "author": ["Mikolov", "Tomas", "Joulin", "Armand", "Baroni", "Marco"], "venue": "CoRR, abs/1511.08130,", "citeRegEx": "Mikolov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "Playing atari with deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Graves", "Alex", "Antonoglou", "Ioannis", "Wierstra", "Daan", "Riedmiller", "Martin"], "venue": "In NIPS Deep Learning Workshop", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg", "Petersen", "Stig", "Beattie", "Charles", "Sadik", "Amir", "Antonoglou", "Ioannis", "King", "Helen", "Kumaran", "Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Monte-carlo tree search in ms. pac-man", "author": ["Ikehata", "T. Ito"], "venue": "In Proceedings of IEEE Conference on Computational Intelligence and Games,", "citeRegEx": "N. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "N. et al\\.", "year": 2011}, {"title": "The GVG-AI competition", "author": ["D. Perez", "S. Samothrakis", "J. Togelius", "T. Schaul", "S. Lucas"], "venue": "In http://www.gvgai.net/index.php,", "citeRegEx": "Perez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Perez et al\\.", "year": 2014}, {"title": "Adaapt: A deep architecture for adaptive policy transfer from multiple sources", "author": ["J. Rajendran", "P. Prasanna", "B. Ravindran", "M. Khapra"], "venue": "In arXiv:1510.02879v2,", "citeRegEx": "Rajendran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rajendran et al\\.", "year": 2015}, {"title": "The 2010 mario AI championship: Level generation track", "author": ["N. Shaker", "J. Togelius", "G. Yannakakis", "B. Weber", "T. Shimizu", "N. Hashiyama", "P. Soreson", "P Pasquier", "G. Mawhorter", "G. Takahashi", "R. Smith", "R. Baumgarten"], "venue": "In In special Issue of IEEE Transactions on Procedural Content Generation,", "citeRegEx": "Shaker et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shaker et al\\.", "year": 2010}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "In arXiv preprint:", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "In Machine Learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "In arXiv preprint:", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Learning simple algorithms from examples", "author": ["Zaremba", "Wojciech", "Mikolov", "Tomas", "Joulin", "Armand", "Fergus", "Rob"], "venue": "CoRR, abs/1511.07275,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "More recently, they have served as a test-bed for machine learning approaches (Perez et al., 2014).", "startOffset": 78, "endOffset": 98}, {"referenceID": 0, "context": "For example, Atari games (Bellemare et al., 2013) have been investigated using neural models with reinforcement learning (Mnih et al.", "startOffset": 25, "endOffset": 49}, {"referenceID": 6, "context": ", 2013) have been investigated using neural models with reinforcement learning (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015).", "startOffset": 79, "endOffset": 135}, {"referenceID": 3, "context": ", 2013) have been investigated using neural models with reinforcement learning (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015).", "startOffset": 79, "endOffset": 135}, {"referenceID": 7, "context": ", 2013) have been investigated using neural models with reinforcement learning (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015).", "startOffset": 79, "endOffset": 135}, {"referenceID": 9, "context": "The GVG-AI competition (Perez et al., 2014) uses a suite of 2D arcade games to compare planning and search methods.", "startOffset": 23, "endOffset": 43}, {"referenceID": 2, "context": "It also differs from the recent surge of work on learning simple algorithms (Zaremba & Sutskever, 2015; Graves et al., 2014; Zaremba et al., 2015), which lack grounding.", "startOffset": 76, "endOffset": 146}, {"referenceID": 16, "context": "It also differs from the recent surge of work on learning simple algorithms (Zaremba & Sutskever, 2015; Graves et al., 2014; Zaremba et al., 2015), which lack grounding.", "startOffset": 76, "endOffset": 146}, {"referenceID": 5, "context": "See Mikolov et al. (2015) for further discussion.", "startOffset": 4, "endOffset": 26}, {"referenceID": 12, "context": "We also combine the recent MemN2N model (Sukhbaatar et al., 2015) with a reinforcement learning and evaluate it on the games.", "startOffset": 40, "endOffset": 65}, {"referenceID": 5, "context": "The MazeBase environment can be thought of as a small practical step towards of some of the ideas discussed at length in Mikolov et al. (2015). In particular, interfacing the agent and the environment in (quasi-)natural language was inspired by discussions with the authors of that work.", "startOffset": 121, "endOffset": 143}, {"referenceID": 2, "context": "(Graves et al., 2014; Vinyals et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) demonstrating tasks such as sorting and reversal of inputs.", "startOffset": 0, "endOffset": 94}, {"referenceID": 6, "context": "In some of these approaches (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) the models were trained with reinforcement learning or using discrete search, allowing possibly delayed rewards with discrete action spaces.", "startOffset": 28, "endOffset": 135}, {"referenceID": 3, "context": "In some of these approaches (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) the models were trained with reinforcement learning or using discrete search, allowing possibly delayed rewards with discrete action spaces.", "startOffset": 28, "endOffset": 135}, {"referenceID": 7, "context": "In some of these approaches (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) the models were trained with reinforcement learning or using discrete search, allowing possibly delayed rewards with discrete action spaces.", "startOffset": 28, "endOffset": 135}, {"referenceID": 12, "context": "Several works have also demonstrated the ability of neural models to learn to answer questions in simple natural language within a restricted environment (Weston et al., 2015b; Sukhbaatar et al., 2015).", "startOffset": 154, "endOffset": 201}, {"referenceID": 2, "context": "(Graves et al., 2014; Vinyals et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) demonstrating tasks such as sorting and reversal of inputs. The algorithms instantiated in our games are even simpler, e.g. conditional statements or navigation to a location, but involve interaction with an environment. In some of these approaches (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) the models were trained with reinforcement learning or using discrete search, allowing possibly delayed rewards with discrete action spaces. Our games also involve discrete actions, and these works inform our choice of the reinforcement learning techniques. Several works have also demonstrated the ability of neural models to learn to answer questions in simple natural language within a restricted environment (Weston et al., 2015b; Sukhbaatar et al., 2015). The tasks we present here share many features with those in Weston et al. (2015a), and the input-output format our games use is inter-operable with their stories.", "startOffset": 1, "endOffset": 995}, {"referenceID": 2, "context": "(Graves et al., 2014; Vinyals et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) demonstrating tasks such as sorting and reversal of inputs. The algorithms instantiated in our games are even simpler, e.g. conditional statements or navigation to a location, but involve interaction with an environment. In some of these approaches (Mnih et al., 2013; Guo et al., 2014; Mnih et al., 2015; Joulin & Mikolov, 2015; Zaremba & Sutskever, 2015) the models were trained with reinforcement learning or using discrete search, allowing possibly delayed rewards with discrete action spaces. Our games also involve discrete actions, and these works inform our choice of the reinforcement learning techniques. Several works have also demonstrated the ability of neural models to learn to answer questions in simple natural language within a restricted environment (Weston et al., 2015b; Sukhbaatar et al., 2015). The tasks we present here share many features with those in Weston et al. (2015a), and the input-output format our games use is inter-operable with their stories. However, during training and testing, the environment in Weston et al. (2015a) is static, unlike the game worlds we consider.", "startOffset": 1, "endOffset": 1155}, {"referenceID": 9, "context": "The GVG-AI competition (Perez et al., 2014) is similar in overall intent to MazeBase, but differs in that it is more appropriate for testing search-based methods, since a simulator (and game rules) are provided.", "startOffset": 23, "endOffset": 43}, {"referenceID": 11, "context": "Similarly, competitions have been organized around Super Mario (Shaker et al., 2010), and Pacman (N.", "startOffset": 63, "endOffset": 84}, {"referenceID": 11, "context": "Furthermore (Shaker et al., 2010), (N.", "startOffset": 12, "endOffset": 33}, {"referenceID": 0, "context": "Also note that given the sandbox nature of MazeBase, in principle it could be used to recreate any of these games, including those in the ACE benchmark (Bellemare et al., 2013).", "startOffset": 152, "endOffset": 176}, {"referenceID": 4, "context": "Our work is similar to Mnih et al. (2013); Guo et al.", "startOffset": 23, "endOffset": 42}, {"referenceID": 2, "context": "(2013); Guo et al. (2014); Mnih et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 2, "context": "(2013); Guo et al. (2014); Mnih et al. (2015) in that we use reinforcement and neural models when training on games.", "startOffset": 8, "endOffset": 46}, {"referenceID": 10, "context": "the classical Puddle World [Rajendran et al. (2015)], for example the basic 2-d grid structure and water obstacles, but is richer, and the agent is not expected to memorize any given world, as they are regenerated at each new game, and agents are tested on unseen worlds.", "startOffset": 28, "endOffset": 52}, {"referenceID": 14, "context": "\u201d Such representation is compatible with the format of the bAbI tasks, introduced in Weston et al. (2015a). However, note that we use egocentric spatial coordinates (e.", "startOffset": 85, "endOffset": 107}, {"referenceID": 12, "context": "\u2020This is consistent with Sukhbaatar et al. (2015), where the \u201cagent\u201d answering the questions was also given them in egocentric temporal coordinates.", "startOffset": 25, "endOffset": 50}, {"referenceID": 12, "context": "We investigate several different types of model: (i) simple linear, (ii) multi-layer neural nets, (iii) convolutional nets and (iv) end-to-end memory networks (Weston et al., 2015b; Sukhbaatar et al., 2015).", "startOffset": 159, "endOffset": 206}, {"referenceID": 12, "context": "Otherwise, we use the architecture from (Sukhbaatar et al., 2015) with 3 hops and tanh nonlinearities.", "startOffset": 40, "endOffset": 65}], "year": 2016, "abstractText": "This paper introduces MazeBase: an environment for simple 2D games, designed as a sandbox for machine learning approaches to reasoning and planning. Within it, we create 10 simple games embodying a range of algorithmic tasks (e.g. if-then statements or set negation). A variety of neural models (fully connected, convolutional network, memory network) are deployed via reinforcement learning on these games, with and without a procedurally generated curriculum. Despite the tasks\u2019 simplicity, the performance of the models is far from optimal, suggesting directions for future development. We also demonstrate the versatility of MazeBase by using it to emulate small combat scenarios from StarCraft. Models trained on the MazeBase version can be directly applied to StarCraft, where they consistently beat the in-game AI.", "creator": "LaTeX with hyperref package"}}}