{"id": "1511.07471", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "Weak Convergence Properties of Constrained Emphatic Temporal-difference Learning with Constant and Slowly Diminishing Stepsize", "abstract": "We consider the emphatic temporal-difference (TD) algorithm, ETD($\\lambda$), for learning the value functions of stationary policies in a discounted, finite state and action Markov decision process. The ETD($\\lambda$) algorithm was recently proposed by Sutton, Mahmood, and White to solve a long-standing divergence problem of the standard TD algorithm when it is applied to off-policy training, where data from an exploratory policy are used to evaluate other policies of interest. The almost sure convergence of ETD($\\lambda$) has been proved in our recent work under general off-policy training conditions, but for a narrow range of diminishing stepsize. In this paper we present convergence results for constrained versions of ETD($\\lambda$) with constant stepsize and with diminishing stepsize from a broad range. Our results characterize the asymptotic behavior of the trajectory of iterates produced by those algorithms, and are derived by combining key properties of ETD($\\lambda$) with powerful convergence theorems from the weak convergence methods in stochastic approximation theory. For the case of constant stepsize, in addition to analyzing the behavior of the algorithms in the limit as the stepsize parameter approaches zero, we also analyze their behavior for a fixed stepsize and bound the deviations of their averaged iterates from the desired solution. These results are obtained by exploiting the weak Feller property of the Markov chains associated with the algorithms, and by using ergodic theorems for weak Feller Markov chains, in conjunction with the convergence results we get from the weak convergence methods. Besides ETD($\\lambda$), our analysis also applies to the off-policy TD($\\lambda$) algorithm, when the divergence issue is avoided by setting $\\lambda$ sufficiently large.", "histories": [["v1", "Mon, 23 Nov 2015 21:29:43 GMT  (60kb)", "https://arxiv.org/abs/1511.07471v1", "53 pages"], ["v2", "Tue, 10 May 2016 18:38:32 GMT  (61kb)", "http://arxiv.org/abs/1511.07471v2", "minor changes from the first version (updated some references and corrected some typos); 53 pages"], ["v3", "Fri, 20 Jan 2017 18:35:27 GMT  (61kb)", "http://arxiv.org/abs/1511.07471v3", "Minor edits; 53 pages. Longer and more proof details than the journal version"]], "COMMENTS": "53 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["huizhen yu"], "accepted": false, "id": "1511.07471"}, "pdf": {"name": "1511.07471.pdf", "metadata": {"source": "CRF", "title": "Weak Convergence Properties of Constrained Emphatic Temporal-difference Learning with Constant and Slowly Diminishing Stepsize\u2217", "authors": ["Huizhen Yu"], "emails": ["(janey.hzyu@gmail.com)"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.07 471v 3 [cs.L G] 20 YES Keywords: Markov decision-making; approximate policy assessment; reinforcement learning; time difference methods; importance testing; stochastic approach; convergence \u043a This research was supported by a grant from Alberta Innovates - Technology Futures. \u2020 RLAI Lab, Department of Computing Science, University of Alberta, Canada (janey.hzyu @ gmail.com) 1"}, {"heading": "2 Weak Convergence Properties of Constrained ETD Learning", "text": ""}, {"heading": "1 Introduction 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Preliminaries 5", "text": "........................................................................................................................"}, {"heading": "3 Convergence Results for Constrained ETD(\u03bb) 12", "text": "3.1 Main results............................................................. 12 3.2 Two variants of a restricted ETD (\u03bb) with distortions......................................................................................................"}, {"heading": "4 Proofs for Section 3 19", "text": "The proofs for theorems 3.1 and 3.2. The proofs for theorems 3.1 and 3.2. The proofs for theorems 3.1 and 3.2. The proofs for theorems 3.3 and 3.4. Proofs. 194.1.1. Conditions for Verify."}, {"heading": "5 Discussion 45", "text": "5.1 The case without adoption 2.2.........................................................................................................................................................................................................................................................................................................."}, {"heading": "1 Introduction", "text": "We look at Discounted Finite State and Action Markov Decision Processes (MDPs) and the problem of learning an approximate value function for a given policy from non-political data, that is, from data due to a different policy. The first policy is referred to as target policy and the second as behavioural policy. The case of on-the-spot learning, where the goal and behavioural policies are the same, has been well studied and widely applied (see e.g., [41, 48] and the books [4, 44]). On-the-field learning provides additional flexibility and is useful in many contexts. For example, one may want to avoid executing the target policy before assessing the potential risk for safety concerns, or one may want to learn value functions for many target policies in an exploratory behaviour. Furthermore, these require value functions (in terms of different reward / cost allocations) to reflect statistical properties of future outcomes, autonomous learning can be used from an experience-based model of artificial intelligence 43 applications in the world."}, {"heading": "4 Weak Convergence Properties of Constrained ETD Learning", "text": "In fact, most of them will be able to address the countries of origin in order to address the countries of origin."}, {"heading": "2 Preliminaries", "text": "In this section, we describe the problem of policy evaluation in the non-political case, the ETD (\u03bb) algorithm and its limited version, and we also review the results of our previous work [52] required in this paper."}, {"heading": "2.1 Off-policy Policy Evaluation", "text": "Let S = {1,.., N} be a finite set of states, and let A be a finite set of actions. Without loss of universality, we assume that for all states any action can be applied in A. When an action is applied in a state S, the system moves into a state s \"with the probability p (s) | s, a) and delivers a random reward with the mean r (s, a, s) and a limited variance according to a probability distribution q (\u00b7 s, a, s\"). These are the parameters of the MDP model we are looking at; they are unknown to the learning algorithms to introduce. A stationary policy is a time-invariant decision rule that determines the probability of taking action in each state."}, {"heading": "6 Weak Convergence Properties of Constrained ETD Learning", "text": "(St, At) times t \u2265 0 form a (time-homogeneous) Markov chain on the space S \u00b7 A, with the marginal state process {St} also being a visual chain.Let \u03c0 and \u03c0o be two given stationary policies, with \u03c0 (a | s) and \u03c0o (a | s), which the probability of taking one at state s under \u03c0 and \u03c0o, respectively. While the system evolves under the policy \u03c0o, generating a stream of state transitions and rewards, we want to evaluate the performance of the policy \u03c0, with a discounted reward criterion, the definition of which will be given shortly."}, {"heading": "2.2 The ETD(\u03bb) Algorithm", "text": "As the ETD algorithm (\u03bb) -Vector (46) approaches the value function v\u03c0 through a function of the form v (s) =?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "2.3 Associated Bellman Equations and Approximation and Convergence Properties of ETD(\u03bb)", "text": "Let us call the diagonal matrix with diagonal entries \u03bb (s), s \"S.\" Associated with ETD (\u03bb) is a generalized multi-stage Bellman equation, whose unique solution is v\u03c0 [42]: 6v = r\u03bb\u03c0, \u03b3 + P \u03bb \u03c0, \u03b3 v. (2,6) 5The definition (2,5) we use here differs slightly from the original definition of et in [46], but the two are equivalent and (2,5) seem more convenient for our analyses.6For the details of this Bellman equation, we refer readers to the early work [42, 44] and recent work [46]. We note that similar to Bellman's single-stage standard equation, which represents a recursive relationship, the v\u03c0 in relation to the expected single-stage reward and the expected total future reward given by vxion itself can be used to satisfy other recursive relationships, whereby the expected reward is replaced by the expected one."}, {"heading": "8 Weak Convergence Properties of Constrained ETD Learning", "text": "Here P\u03bb\u03c0, \u03b3 a N \u00b7 N substochastic matrix, is \"immanent\" = > immanent (> immanent) is a vector of the expected discounted total rewards, which are achieved up to a certain random time depending on the function \u03bb, and they can be expressed in the terms \"immanent\" and \"immanent.\" (> immanent) ETD aims to solve a projected version of the Bellman equation (2,6) [immanent] [immanent] [immanent] [immanent] [immanent] [immanent] [immanent] [immanent], \"immanent\" [immanent] [immanent] [immanent] [immanent] is \"immanent,\" which is \"immanent\" the following forms of approximate value functions and \"immanent.\" (immanent)."}, {"heading": "2.4 Constrained ETD(\u03bb), Averaged Processes and Mean ODE", "text": "First, we consider a limited version of ETD (\u03bb), which simply scales the number of individual steps to keep them limited if necessary. (http: / / www.ETD-Iterate = > 48), (http: / / www.ETD-Iterate = > 48), (2.11), where \"B\" is the Euclidean projection on a closed sphere (greater than the threshold given in Lemma 2.1 below), from any given initial case (e0, F0, 0), the algorithm (2.11), almost certainly converts to a smaller step size I (1 / t). Our interest in this work is (2.11), with a much larger step size, which we almost certainly convert to a smaller step size I, for a smaller step size I (1 / t)."}, {"heading": "10 Weak Convergence Properties of Constrained ETD Learning", "text": "To prepare ourselves for the analysis, we will consider several results from [52] that will be necessary. First, we will discuss the \"mean ODE\" that we want to associate with (2,11). It is the projected ODE x x = h (x) = h (x) = h (x) = h (x) = h (x) = h (x) is the normal constant of B at x, i.e., NB (x) is the left side of the equation Cx + b = 0 that we want to solve: h (x) = Cx + b; (x) is the normal constant of B at x, i.e., NB (x) = {0} for x inside B, NB (x) = {ax | a) for x at the boundary of B; and z is the boundary that cancels the component of h (x) in NB (x)."}, {"heading": "12 Weak Convergence Properties of Constrained ETD Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Convergence Results for Constrained ETD(\u03bb)", "text": "In this section we present the convergence properties of the restricted ETD (\u03bb) algorithm (2,11) and several variants thereof, for constant step size and for step size, which is slowly decreasing. We will briefly explain how the results are achieved, leaving the detailed analysis to Section 4. The first results about the algorithm (2,11) are given first in Section 3.1, followed by similar results in Section 3.2 for two variant algorithms, which have distortions but are better able to mitigate the problem of variance in non-political learning. These results are obtained by applying two general convergence theorems from [19], which concern weak convergence of stochastic approximation algorithms for reduction and constant step size. Finally, the case of constant step size will be further analyzed in Section 3.3 to refine some of the results of Sections 3.1-3.2, so that the asymptotic behavior of the algorithms for a fixed step size cannot be determined."}, {"heading": "3.1 Main Results", "text": "Let us first consider the algorithm (2,11) for decreasing the step size. Let the step size change slowly flow into the following senses (2,1). Assumption 3,1 (condition on decreased step size). The (deterministic) non-negative sequence (2,11) fulfills that the (actual) size of the step size (3,1) is the condition (3,1) that the condition (3,1) is the condition A.8.2.8 in [19, Chap. 8] and enables the step sizes much larger than O (1 / t). We can have the condition (0, 1), and even larger step sizes are possible."}, {"heading": "14 Weak Convergence Properties of Constrained ETD Learning", "text": "The theorems of [19] that we will apply are based on the weak convergence methods. While it is beyond the scope of this work to explain these powerful methods, here we will cite some basic facts about them to explain the origin of the convergence theorems that we have outlined above. (Often, one also shifts a trajectory of iterations to bring the \"asymptotic part\" of the trajectory closer to the origin of the perpetual timeline.) In the case of our problem, for example, to reduce the properties of the iterations, one shifts a trajectory of iterations to bring the \"asymptotic part\" of the trajectory closer to the origin of the perpetual timeline."}, {"heading": "3.2 Two Variants of Constrained ETD(\u03bb) with Biases", "text": "2.11, 2.11, 2.11, 2.11, 2.11, 2.11, 2.11, 2.11, 2.11, 2.11, 2.11, (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), 2.11), (2.11), 2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), 2.11), (2.11), (2.11), (2.11), (2.11), (2.11), 2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), (2.11), ("}, {"heading": "16 Weak Convergence Properties of Constrained ETD Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3 More about the Constant-stepsize Case", "text": "In the case of the constant step size specified in theorems 3.2 and 3.4, we can see several similarities to their counterparts for the decreasing step size specified in theorems 3.1 and 3.3. However, they characterize the behavior of the iterates in the boundary as the step size parameter approaches 0, and deal only with a finite segment of iterates for each step size (although in their part (ii) both the length of the segment and its starting position as Alpha \u2192 0). Unlike in the case of decreasing step size, these results do not explicitly tell us about the behavior of the respective step size as we take it to a fixed step size. The purpose of the present subsection is to further analyze the case of a fixed step size that we have just mentioned. We observe that for a fixed step size that we form a weak step size together with Zt = (St, et, Ft)."}, {"heading": "18 Weak Convergence Properties of Constrained ETD Learning", "text": "Finally, we consider a simple modification of the previous algorithms, for which the conclusions of theorems 3.5 (ii) and 3.6 (ii) can be substantiated. This is our motivation for introducing the modification, but we will postpone the discussion until Remark 3.2 at the end of this subsection. For each of the algorithms (2.11), (3.3), or (3.4), if the original recursion can be written under a constant step size, until Remark 3.2 at the end of this subsection, it is clear that we are now modifying this recursion formula by adding a perturbation term that does not say: \"The original recursion is no more than a constant step size.\""}, {"heading": "4 Proofs for Section 3", "text": "We now prove the theorems from the previous section."}, {"heading": "4.1 Proofs for Theorems 3.1 and 3.2", "text": "In this section, we will demonstrate theorems 3.1 and 3.2 on the convergence properties of the limited ETD (\u03bb) algorithm (2.11). We will apply two theorems from [19], theorems 8.2.2 and 8.2.3, which relate to the weak convergence of stochastic approximation algorithms for constant and decreasing step variables, respectively, which requires us to show that the conditions of these theorems are met by our algorithm. The most important conditions relate to the uniformity, density and convergence in the mean of certain sequences of random variables involved in the algorithm. Our evidence will be based on many properties of the ETD iterates that we have found in [52] when analyzing the near-certain convergence of the algorithm."}, {"heading": "4.1.1 Conditions to Verify", "text": "For some index sets K {Xk} k \u00b2 K should be a set of random variables with values in a metric space X (in our context X will be Rm or Rm). The set {Xk} k \u00b2 K should be narrow or limited in probability if for each CC > 0 there is a compact set of DCC \u00b2 X of such type that inf k \u00b2 KP (Xk \u00b2 D\u03b4 a) \u2265 1 \u2212 \u03b4.For Rm-weighted Xk the set {Xk} k \u00b2 K should be uniformly integrable (among others) iflim a \u00b2 s \u00b2 s \u00b2 sup k \u00b2 KE [\u0644Xk \u00b2 a) = 0.In order to analyze the restricted ETD (\u03bb) algorithm given by (2,11), it is generated by a byte number + 1 = \u0421B (\u03b1t + \u03b1gebt), with Yt (Rt + \u041a\u03b4) and YT-1 (T)."}, {"heading": "20 Weak Convergence Properties of Constrained ETD Learning", "text": "In other words: that the condition (empirical) is the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition, that the condition that the condition, that the condition, that the condition, that the condition, that the condition that the condition, that the condition, that the condition, that the condition, that the condition that the condition, that the condition, that the condition, that the condition, that the condition that the condition, that that that that the condition that the condition that that the condition that that that the condition, that that the condition, that the condition, that the condition, that that that the condition, that the condition, that that that that the condition that that that the condition that that that that the condition that the condition that that the condition that that that that"}, {"heading": "4.1.2 Proofs", "text": "Condition (ii) is clearly met. In what follows, we prove that the rest of the conditions are also met. (iii) We begin with the condition conditions (iii) and (iii) as directly implied by a property of the trace iterates that we already know. (i) We then proceed with the same condition conditions (i), (i), (iv) and (iv) before tackling the convergence of the condition bases of the ETD iterates required in (v) and (v), which we set in [52] and in Section 2.4 and Appendix A.First, we show that the condition conditions (iii) and (iii) are satisfied. This is implied by the following property of the tracks: for each given initial state (e0, F0), supt."}, {"heading": "22 Weak Convergence Properties of Constrained ETD Learning", "text": "Under the premise of (ik), (k), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K)."}, {"heading": "24 Weak Convergence Properties of Constrained ETD Learning", "text": "The two conditions are identical in this case, as they each relate to a specified simplicity, whereas the sequential function required in the condition is not affected by the function h, which is associated with the desired mean ODE (2,12). Let us now examine the required convergence in averaging by examining the properties of the sequential iterate and the convergence results given in Theorem 2,3 and Cor 2,1.Proposition 4,3. Let us hold the assumption 2,1. For each individual arrangement B and each compact arrangement D (lim k) we provide the required convergence in the mean iterate and the convergence results that we specify."}, {"heading": "26 Weak Convergence Properties of Constrained ETD Learning", "text": "And accordingly, we look at the following three sequences of variables, as they are usually applied in practice: K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K + K +"}, {"heading": "4.2 Proofs for Theorems 3.3 and 3.4", "text": "In this subsection, we will prove theorems 3.3-3.4 for the two variants of the restricted ETD (\u03bb) algorithm specified in (3.3) and (3.4). As in the previous subsection, we will apply [19, theorems 8.2,2, 8.2,3] and show separately for each variant that the required conditions are met. On the basis of the properties of the mean ODEs of the variant algorithms, we will then specialize the conclusions of these theorems in order to achieve the desired results.13 The details for this statement are as follows: Since the h limit is on B and the limit reflection term z (\u00b7) 0 is below our assumptions (Lemma 2.1), a solution x (\u00b7) of the (2.12) Lipschitz solution is continuous on [0, \u02da). We calculate V-shaped on the Lyapunov function V (\u03bd) = | x-shaped on the negative definition of (2.12) and on the negative definition of (2.12) Lipschitz is continuous on [2.0, \u02da)."}, {"heading": "28 Weak Convergence Properties of Constrained ETD Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Proofs for the First Variant", "text": "Consider the first variant of the algorithm (3,3): the first variant of the algorithm (3,3): the second variant (3,3): the second variant (3,3): the third variant (4,4): the third variant (4,4): the fourth variant (4,4): the fourth variant (4,4): the fourth variant (4,4): the fourth variant (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth variant (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4): the fourth (4,4)."}, {"heading": "30 Weak Convergence Properties of Constrained ETD Learning", "text": "(< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< \"< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) (< K) < K) (< K) (< K) (< K) (< K) < K) (< K) (< K) (< K) (< K) (<"}, {"heading": "4.2.2 Proofs for the Second Variant", "text": "We now analyze the second variant of the algorithm (3,4) that we use in relation to the hK function (4,4) that we use in relation to the hK function (4,4) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the hK function (4,0) that we use in relation to the function (4,0) that we use in relation to the hK (4,0) that we use in relation to the function (4,0) that we use in relation to the hK (4,0) that we use in relation to the hK (4,0) that we use in relation to the hK function (4,0) that we use in relation to the function (4,0) that we use in relation to the hK (4,0) that we use in relation to the function (4,0) that we use in relation to the hK, we use in relation to the function (4,0) that we use in relation to use in relation to the hK, we use in relation to the hK"}, {"heading": "32 Weak Convergence Properties of Constrained ETD Learning", "text": "& # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 222; K (8222) K & # 222; K & # 222; K # 222; K # 222; K # 222; K # 222; K # 222; K (822); K & # 22; K # 22; K # 22; K # 22; K (822) 222; K; K & # 22; K # 22; K (822); K & # 22; K # 22; K (822); K & # 22; K # 22; K; K & # 22; K # 22; K (822); K # 22; K # 22; K (822); K & # 22; K # 22; K (822); K # 22; K; K # 22; K; K (822)"}, {"heading": "4.3 Further Analysis of the Constant-stepsize Case", "text": "We will now look again at the case of the constant increment and prove the theorems 3.5-3.8 from Section 3.3. The proofs will be based on the combination of the results obtained previously through the stochastic approximation theory with the ergodic theorems of weak Feller-Markov chains. As before, the proofs will also be based on the key properties of the ETD iterates."}, {"heading": "4.3.1 Weak Feller Markov Chains", "text": "We focus on Markov chains on fully separable metric spaces. For such a Markov chain {Xt} with state points X, we let P (\u00b7, \u00b7) denote its transition core, i.e. P: X \u00b7 B (X) \u2192 [0, 1], P (x, D) = Px (X1), {x}, where B (X) denotes the capability of the Borel sigma algebra on X, and Px denotes the probability distribution of {Xt}, conditioned on X0 = x. Multi-stage transition cores will also be needed. For t [Xp], the t-step transition core P t (\u00b7 \u00b7): X \u00b7 B (X, 1] becomes the probability distribution of {Xt} properties on X0 = x."}, {"heading": "34 Weak Convergence Properties of Constrained ETD Learning", "text": "If the chain {Xt} starts from X0 = x and each \u00b5x, t is a random variable, the values in the space of probabilities are set to X. Leave \"Px-a.s\" for \"almost certainly in relation to Px.\" The next problem concerns the convergence of the probabilities of a weak Feller-Markov chain. It is a result of Meyn [29] and will be needed in our proofs of theorems 3.7-3.8.Lemma 4.4 ([29, Prop. 4.2]. Leave {Xt} a weak Feller-Markov chain with the transition core P (\u00b7) on a fully separable metric space X. Suppose that (i) {Xt} has a unique invariant probability dimension; (ii) for each compact quantity E-X-x, the quantity {P-k (x, \u00b7) | x-K-Convergence for each individual eimability density is x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x."}, {"heading": "4.3.2 Proofs of Theorems 3.5 and 3.6", "text": "In this subsection we show that the conclusions of theorems 3.5-3.6 are the same for the disturbed version (3,7) of these algorithms as well as for a constant step quantity. We begin with a preliminary analysis given in the next two lemmas. Recall Zt = (St, at, Ft) and {Zt} is a weak Feller Markov chain on Z: = S \u00b7 A \u00b7 Rn + 1 [52, sec. 3.1], and its development is not influenced by the development of the Markov chain."}, {"heading": "38 Weak Convergence Properties of Constrained ETD Learning", "text": "For the disturbed version (3,7) of the algorithm (2,11), the only difference to (2,11) under current assumptions is the disturbance variables (3,7) involved in each iteration, but by definition these variables have a conditional zero value: E\u03b1t [\u03b1 \u03b1, t] = 0, so the only condition in which they appear is the uniform integrability condition (i): {Y \u03b1t | t \u00b2 0, \u03b1 > 0} is u.i., where Y \u03b1t = \u03b1 (\u03b1 \u03b1, t) + and the only condition in which they occur is the uniform integrability condition (i). By definition, this is not the case, t \u00b2 \u00b2 \u00b2 \u00b2 is the condition for all other conditions and t have a limited variance, and hence \u00b2 \u00b2 \u00b2 is the condition in which Y \u03b1t = h (\u03b1) + \u03b1 (\u03b1, \u03b1) \u03b1)."}, {"heading": "40 Weak Convergence Properties of Constrained ETD Learning", "text": "the convergence property of the averaged probability measures of the m-step version of {(Zt, \u03b8\u03b1t)}, which have all been proved for the algorithms (3,3) and (3,4) and their disturbed version (3,7) in this paragraph. The preceding arguments also show that the first part of theorem 3.8 applies; that is, the conclusions of theorem 3.4 and theorem 3.6 also apply to the disturbed version (3,7) of the algorithm (3,3) or (3,4)."}, {"heading": "4.3.3 Proofs of Theorems 3.7 and 3.8", "text": "In this subsection, we will establish the complete theorems 3.7 and 3.8 in relation to the disturbed version (\u03b2 7) of the algorithms (2.11), (3.3) and (3.4). We have already described the first part of these two phenomena in the previous subsection. In the following, we will discuss their second part, which, we remember, is stronger than the corresponding part of theorems 3.5 and 3.6 in relation to a specified step width \u03b1, the deviation of the averaged iterations from the boundary is now characterized as t \u00b2 not in an expected sense, but for almost all sample paths. To simplify the presentation, unless otherwise known, the deviation of the averaged iterations in this subsection is taken for granted, which is generated by the perverted version (3.7) of the three algorithms (2.11), (3.3) and (3.4)."}, {"heading": "42 Weak Convergence Properties of Constrained ETD Learning", "text": "We now define: (Zt), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z), (Z)."}, {"heading": "44 Weak Convergence Properties of Constrained ETD Learning", "text": "Since we can apply the conclusions of our Lemma 4.4, we will (4,53) - (4,55), that (4,55), that (4,55), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (4,00), that (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), (0,00), 0,00, (0,00), (0,00), 0,00), (0,00), (0,00), (0,00), (0,00), 0,00), (0,00, 0,00), (0,00), 0,00), (0,00), (0,00), (0,00), (0,00), 0,00), (0,00), (0,00), (0,00), 0,00), (0,00), (0,00), (0,00), 0,00), (0,00), 0,00), (0,00), (0,00), (0,00), (0,00), (0,00), 0,00), (0,00), (0,00), 0,00), (0,00), (0,00), 0,00), (0,00), (0,00), 0,00), (0,00), 0,00), (0,00), 0,00), (0,00), 0,00)"}, {"heading": "5 Discussion", "text": "In this section, we will discuss the direct application of our convergence results to ETD (\u03bb) under relaxed conditions and to two other algorithms, the non-political TD (\u03bb) algorithm and the ETD (\u03bb, \u03b2) algorithm proposed by Hallak et al. [15]. Afterwards, we will discuss some open questions to conclude the paper."}, {"heading": "46 Weak Convergence Properties of Constrained ETD Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 The Case without Assumption 2.2", "text": "In this paper, we have focused on the case where assumption 2.2 holds and C is defined negatively (theorem 2.1). If assumption 2.2 is not fully utilized, then there are either less than n stressed states (i.e., states s with M-ss > 0), or the characteristic vectors of the stressed states are not rich enough to contain n linear independent vectors. In both cases, the functional alignment capacity is not fully utilized. Therefore, it is desirable to fulfill assumption 2.2 by adding other states with positive interest weights."}, {"heading": "C = \u2212\u03a6\u22a41 G\u0302\u03a61, where G\u0302 = M\u0302(I \u2212 Q\u0302) is positive definite.", "text": "Let us define the range (A) as the range of a matrix A. By the positive definition of the matrix G (1), the negative semidefinitive matrix C is able to possess the following properties (we ignore the simple proof): (1) Absence of the matrix G (2), and leave i (s) > 0 for at least one state s (s). Then, the matrix C fulfills the following properties (C) = Absence (s) = Absence of the matrix J1); and (ii) there is such a state that for all x (s) the negative properties of the matrix J1 (s) exist."}, {"heading": "48 Weak Convergence Properties of Constrained ETD Learning", "text": "In the general off-policy situation, depending on how fast e (0) t shrinks exponentially, it can be shown that the convergence to a point depends on the sample path. In the general off-policy case, depending on how fast e (0) t shrinks exponentially, it can be shown that the convergence to a point depends on the sample path."}, {"heading": "5.2 Off-policy TD(\u03bb)", "text": "The application of TD (\u03bb) to non-political learning processes by using sampling techniques was first proposed in [36, 35], and the emphasis was on episodic data. The analysis we have given in this paper refers directly to the (non-episodic) non-political TD (\u03bb) algorithm, which was examined in [5, 51, 10] if its divergence problem is avoided by being sufficiently large. Specifically, we are looking at constant results [0, 1) and constant results [0, 1] and an infinitely long orbit that is still generated by behavioural policy.The algorithm is the same as TD (\u03bb) except for the inclusion of meaning, sampling weighting (0 \u2212 1) and constant results (Rt + forecasts) properties that are still generated by behavioural policy.The algorithm is the same as TD (\u03bb), except for the inclusion of the meaning, sampling weighting = 16 and vice versa."}, {"heading": "5.3 Open Issues", "text": "A major difficulty in applying TD learning outside politics, especially with \u03bb > 0, are the high deviations of iterations. For ETD (\u03bb), TD (\u03bb) outside politics and their versions with the smallest squares, the deviations of the products from the importance of scanning weights \u03c1t\u03c1t + 1 \u00b7 \u00b7 \u00b7 along a trajectory and due to the increased effects these weights can have on the tracks, can grow indefinitely over time, severely affecting the behavior of the algorithms in practice. (The problem of growing deviations in the application of importance tests for simulating Markov systems was also known before and discussed in previous work; see e.g. [14, 38]. The two distorted algorithms discussed in this paper were motivated by the need to mitigate the problem of policy deviations, and their robust behavior was observed in our experiments [28, 53]."}, {"heading": "Acknowledgement", "text": "I would like to thank Professors Richard Sutton and Csaba Szepesva'ri for helpful discussions and two anonymous reviewers for their helpful feedback. This research was supported by a grant from Alberta Innovates - Technology Futures."}, {"heading": "50 Weak Convergence Properties of Constrained ETD Learning", "text": "[2] Antos, A., Szepesv \u0301 ari, C., and Munos, R. (2008). Learning near-optimal policies with Bellman residual minimization based suited policy iteration and a single sample path. Machine Learning, 71: 89-129. [3] Baird, L. C. (1995). Residual algorithms: Reinforcement learning with function approximation. [5] Bertsekas, D. P. and Yu, H. and Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific, Belmont, MA. [5] Bertsekas, D. P. and Yu, H. Projected equation methods for approximate solution of large linear systems. Journal of Computational and Applied Mathematics, 227 (1). Billingsley, P. (1968)."}, {"heading": "52 Weak Convergence Properties of Constrained ETD Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix A: Key Properties of Trace Iterates", "text": "In this appendix we list four key properties of trace iterates {(et, Ft)} generated by the ETD (\u03bb) algorithms, three of which were derived in [52, Appendix A] and are used in the convergence analysis of ETD (\u03bb) in both [52] and the present study (Section 3.2, {(et, Ft)}. However, as the statement below shows, the statement below is limited in a bostochastic sense. Proposition A.1. Assuming 2.1, given a bounded set E-Rn + 1, there is a constant L < so that if the initial position (e0, Ft) is in the E, then supt. 0 E (et, Ft)."}], "references": [{"title": "Adaptive importance sampling technique for Markov chains using stochastic approximation. Operations Research, 54:489\u2013504", "author": ["T.P. Ahamed", "V.S. Borkar", "S. Juneja"], "venue": "Weak Convergence Properties of Constrained ETD Learning", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Learning near-optimal policies with Bellman residual minimization based fitted policy iteration and a single sample path", "author": ["A. Antos", "C. Szepesv\u0301ari", "R. Munos"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L.C. Baird"], "venue": "In The 12th International Conference on Machine Learning (ICML)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Projected equation methods for approximate solution of large linear systems", "author": ["D.P. Bertsekas", "H. Yu"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Convergence of Probability Measures", "author": ["P. Billingsley"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1968}, {"title": "Stochastic Approximation: A Dynamic Viewpoint", "author": ["V.S. Borkar"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Least-squares temporal difference learning", "author": ["J.A. Boyan"], "venue": "In The 16th International Conference on Machine Learning (ICML)", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": "Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Policy evaluation with temporal differences: A survey and comparison", "author": ["C. Dann", "G. Neumann", "J. Peters"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Stochastic Processes", "author": ["J.L. Doob"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1953}, {"title": "Real Analysis and Probability", "author": ["R.M. Dudley"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Off-policy learning with eligibility traces: A survey", "author": ["M. Geist", "B. Scherrer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Importance sampling for stochastic simulations", "author": ["P.W. Glynn", "D.L. Iglehart"], "venue": "Management Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1989}, {"title": "Generalized emphatic temporal difference learning: Bias-variance analysis", "author": ["A. Hallak", "A. Tamar", "R. Munos", "S. Mannor"], "venue": "In The 30th AAAI Conference on Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Actor-Critic Algorithms", "author": ["V.R. Konda"], "venue": "PhD thesis,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Stochastic Approximation Methods for Constrained and Unconstrained Systems", "author": ["H.J. Kushner", "D.S. Clark"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1978}, {"title": "Weak convergence and asymptotic properties of adaptive filters with constant gains", "author": ["H.J. Kushner", "A. Shwartz"], "venue": "IEEE Transactions on Information", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1984}, {"title": "Stochastic Approximation and Recursive Algorithms and Applications", "author": ["H.J. Kushner", "G.G. Yin"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Finite-sample analysis of least-squares policy iteration", "author": ["A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Finite-sample analysis of proximal gradient TD algorithms", "author": ["B. Liu", "J. Liu", "M. Ghavamzadeh", "S. Mahadevan", "M. Petrik"], "venue": "In The 31st Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Regularized off-policy TD-learning", "author": ["B. Liu", "S. Mahadevan", "J. Liu"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of Alberta", "author": ["H.R. Maei"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Sparse Q-learning with mirror descent", "author": ["S. Mahadevan", "B. Liu"], "venue": "In The 28th Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Proximal reinforcement learning: A new theory of sequential decision making in primal-dual spaces. arXiv:1405.6757", "author": ["S. Mahadevan", "B. Liu", "P. Thomas", "W. Dabney", "S. Giguere", "N. Jacek", "I. Gemp", "J. Liu"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Off-policy learning based on weighted importance sampling with linear computational complexity", "author": ["A.R. Mahmood", "R.S. Sutton"], "venue": "In The 31st Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Weighted importance sampling for offpolicy learning with linear function approximation", "author": ["A.R. Mahmood", "H. van Hasselt", "R.S. Sutton"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Emphatic temporal-difference learning", "author": ["A.R. Mahmood", "H. Yu", "M. White", "R.S. Sutton"], "venue": "In European Workshops on Reinforcement Learning (EWRL)", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Ergodic theorems for discrete time stochastic systems using a stochastic Lyapunov function", "author": ["S. Meyn"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1989}, {"title": "Markov Chains and Stochastic Stability", "author": ["S. Meyn", "R.L. Tweedie"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Finite time bounds for fitted value iteration", "author": ["R. Munos", "C. Szepesv\u0301ari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Discrete-Parameter Martingales", "author": ["J. Neveu"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1975}, {"title": "Statistical linear estimation with penalized estimators: An application to reinforcement learning", "author": ["B.A. Pires", "C. Szepesv\u0301ari"], "venue": "In The 29th International Conference on Machine Learning (ICML)", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1992}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["D. Precup", "R.S. Sutton", "S. Dasgupta"], "venue": "In The 18th International Conference on Machine Learning (ICML)", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "In The 17th International Conference on Machine Learning (ICML)", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1994}, {"title": "Combining importance sampling and temporal difference control variates to simulate Markov chains", "author": ["R.S. Randhawa", "S. Juneja"], "venue": "ACM Transactions on Modeling and Computer Simulation,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "Iterative Methods for Sparse Linear Systems", "author": ["Y. Saad"], "venue": "SIAM, Philadelphia,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2003}, {"title": "Should one compute the temporal difference fix point or minimize the Bellman residual? The unified oblique projection view", "author": ["B. Scherrer"], "venue": "In The 27th International Conference on Machine Learning (ICML)", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1988}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["R.S. Sutton"], "venue": "In The 12th International Conference on Machine Learning (ICML)", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1995}, {"title": "The grand challenge of predictive empirical abstract knowledge", "author": ["R.S. Sutton"], "venue": "In IJCAI Workshop on Grand Challenges for Reasoning from Experiences", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1998}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesv\u00e1ri", "E. Wiewiora"], "venue": "In The 26th International Conference on Machine Learning (ICML)", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["R.S. Sutton", "A.R. Mahmood", "M. White"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "A convergent O(n) algorithm for off-policy temporaldifference learning with linear function approximation", "author": ["R.S. Sutton", "C. Szepesv\u00e1ri", "H. Maei"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2008}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1997}, {"title": "Generalized TD learning", "author": ["T. Ueno", "S. Maeda", "M. Kawanabe", "S. Ishii"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2011}, {"title": "Preconditioned temporal difference learning", "author": ["H.S. Yao", "Z.Q. Liu"], "venue": "In The 25th International Conference on Machine Learning (ICML)", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2008}, {"title": "Least squares temporal difference methods: An analysis under general conditions", "author": ["H. Yu"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "On convergence of emphatic temporal-difference learning. http://arxiv.org/abs/1506.02582; a shorter version appeared in The 28th", "author": ["H. Yu"], "venue": "Annual Conference on Learning Theory (COLT),", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2015}, {"title": "Some simulation results for emphatic temporal-difference learning algorithms. http://arxiv.org/abs/1605.02099", "author": ["H. Yu"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2016}, {"title": "Error bounds for approximations from projected linear equations", "author": ["H. Yu", "D.P. Bertsekas"], "venue": "Mathematics of Operations Research,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2010}], "referenceMentions": [{"referenceID": 44, "context": "The ETD(\u03bb) algorithm was recently proposed by Sutton, Mahmood, and White [46] to solve a long-standing divergence problem of the standard TD algorithm when it is applied to off-policy training, where data from an exploratory policy are used to evaluate other policies of interest.", "startOffset": 73, "endOffset": 77}, {"referenceID": 39, "context": ", [41, 48] and the books [4, 44]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 46, "context": ", [41, 48] and the books [4, 44]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 42, "context": ", [41, 48] and the books [4, 44]).", "startOffset": 25, "endOffset": 32}, {"referenceID": 41, "context": "In addition, insofar as value functions (with respect to different reward/cost assignments) reflect statistical properties of future outcomes, off-policy learning can be used by an autonomous agent to build an experience-based internal model of the world in artificial intelligence applications [43].", "startOffset": 295, "endOffset": 299}, {"referenceID": 44, "context": "In this paper we focus on a new off-policy learning algorithm proposed recently by Sutton, Mahmood, and White [46]: the emphatic temporal-difference (TD) learning algorithm, or ETD(\u03bb).", "startOffset": 110, "endOffset": 114}, {"referenceID": 39, "context": "The algorithm is similar to the standard TD(\u03bb) algorithm with linear function approximation [41], but uses a novel scheme to resolve a long-standing divergence problem in TD(\u03bb) when applied to off-policy data.", "startOffset": 92, "endOffset": 96}, {"referenceID": 46, "context": "Regarding the divergence problem, while TD(\u03bb) was proved to converge for the on-policy case [48], it was known quite early that the algorithm can diverge in other cases [3, 48] (for related discussions, see also the books [4, 44] and the recent works [46, 28]).", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "Regarding the divergence problem, while TD(\u03bb) was proved to converge for the on-policy case [48], it was known quite early that the algorithm can diverge in other cases [3, 48] (for related discussions, see also the books [4, 44] and the recent works [46, 28]).", "startOffset": 169, "endOffset": 176}, {"referenceID": 46, "context": "Regarding the divergence problem, while TD(\u03bb) was proved to converge for the on-policy case [48], it was known quite early that the algorithm can diverge in other cases [3, 48] (for related discussions, see also the books [4, 44] and the recent works [46, 28]).", "startOffset": 169, "endOffset": 176}, {"referenceID": 42, "context": "Regarding the divergence problem, while TD(\u03bb) was proved to converge for the on-policy case [48], it was known quite early that the algorithm can diverge in other cases [3, 48] (for related discussions, see also the books [4, 44] and the recent works [46, 28]).", "startOffset": 222, "endOffset": 229}, {"referenceID": 44, "context": "Regarding the divergence problem, while TD(\u03bb) was proved to converge for the on-policy case [48], it was known quite early that the algorithm can diverge in other cases [3, 48] (for related discussions, see also the books [4, 44] and the recent works [46, 28]).", "startOffset": 251, "endOffset": 259}, {"referenceID": 26, "context": "Regarding the divergence problem, while TD(\u03bb) was proved to converge for the on-policy case [48], it was known quite early that the algorithm can diverge in other cases [3, 48] (for related discussions, see also the books [4, 44] and the recent works [46, 28]).", "startOffset": 251, "endOffset": 259}, {"referenceID": 3, "context": "In particular, in the off-policy LSTD(\u03bb) algorithm [5, 51] (an extension of the on-policy least-squares version of TD(\u03bb), called LSTD(\u03bb) [9, 8]), with higher computational complexity than TD(\u03bb), the linear equation associated with TD(\u03bb) is estimated from data and then solved.", "startOffset": 51, "endOffset": 58}, {"referenceID": 49, "context": "In particular, in the off-policy LSTD(\u03bb) algorithm [5, 51] (an extension of the on-policy least-squares version of TD(\u03bb), called LSTD(\u03bb) [9, 8]), with higher computational complexity than TD(\u03bb), the linear equation associated with TD(\u03bb) is estimated from data and then solved.", "startOffset": 51, "endOffset": 58}, {"referenceID": 7, "context": "In particular, in the off-policy LSTD(\u03bb) algorithm [5, 51] (an extension of the on-policy least-squares version of TD(\u03bb), called LSTD(\u03bb) [9, 8]), with higher computational complexity than TD(\u03bb), the linear equation associated with TD(\u03bb) is estimated from data and then solved.", "startOffset": 137, "endOffset": 143}, {"referenceID": 6, "context": "In particular, in the off-policy LSTD(\u03bb) algorithm [5, 51] (an extension of the on-policy least-squares version of TD(\u03bb), called LSTD(\u03bb) [9, 8]), with higher computational complexity than TD(\u03bb), the linear equation associated with TD(\u03bb) is estimated from data and then solved.", "startOffset": 137, "endOffset": 143}, {"referenceID": 45, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 30, "endOffset": 42}, {"referenceID": 43, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 30, "endOffset": 42}, {"referenceID": 21, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 30, "endOffset": 42}, {"referenceID": 20, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 83, "endOffset": 91}, {"referenceID": 22, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 83, "endOffset": 91}, {"referenceID": 19, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 102, "endOffset": 110}, {"referenceID": 23, "context": "In the gradient-TD algorithms [47, 45, 23] and the proximal gradient-TD algorithms [22, 24] (see also [21, 25]), the difficulty in TD(\u03bb) is overcome by reformulating the approximate policy evaluation problem TD(\u03bb) attempts to solve as optimization problems and then tackle them with optimization techniques.", "startOffset": 102, "endOffset": 110}, {"referenceID": 11, "context": "(See the surveys [13, 10] for other algorithm examples.", "startOffset": 17, "endOffset": 25}, {"referenceID": 8, "context": "(See the surveys [13, 10] for other algorithm examples.", "startOffset": 17, "endOffset": 25}, {"referenceID": 44, "context": "An important result of this weighting scheme is that under natural conditions on the function approximation architecture, the average dynamics of ETD(\u03bb) can be described by an affine function involving a negative definite matrix [46, 52], which provides a desired stability property, similar to the case of convergent on-policy TD algorithms.", "startOffset": 229, "endOffset": 237}, {"referenceID": 50, "context": "An important result of this weighting scheme is that under natural conditions on the function approximation architecture, the average dynamics of ETD(\u03bb) can be described by an affine function involving a negative definite matrix [46, 52], which provides a desired stability property, similar to the case of convergent on-policy TD algorithms.", "startOffset": 229, "endOffset": 237}, {"referenceID": 50, "context": "The almost sure convergence of ETD(\u03bb), under general off-policy training conditions, has been shown in our recent work [52] for diminishing stepsize.", "startOffset": 119, "endOffset": 123}, {"referenceID": 48, "context": "An efficient algorithm for solving the estimated equations is the one given in [50] based on the line search method.", "startOffset": 79, "endOffset": 83}, {"referenceID": 31, "context": "It can also be applied to finding approximate solutions under additional penalty terms suggested by [33].", "startOffset": 100, "endOffset": 104}, {"referenceID": 44, "context": "The papers [46, 28] work with the negation of the matrix that we associate with ETD(\u03bb) in this paper.", "startOffset": 11, "endOffset": 19}, {"referenceID": 26, "context": "The papers [46, 28] work with the negation of the matrix that we associate with ETD(\u03bb) in this paper.", "startOffset": 11, "endOffset": 19}, {"referenceID": 44, "context": "The negative definiteness property we discuss here corresponds to the positive definiteness property discussed in [46, 28].", "startOffset": 114, "endOffset": 122}, {"referenceID": 26, "context": "The negative definiteness property we discuss here corresponds to the positive definiteness property discussed in [46, 28].", "startOffset": 114, "endOffset": 122}, {"referenceID": 15, "context": "The first group of results are derived by combining key properties of ETD(\u03bb) with powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19].", "startOffset": 181, "endOffset": 193}, {"referenceID": 16, "context": "The first group of results are derived by combining key properties of ETD(\u03bb) with powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19].", "startOffset": 181, "endOffset": 193}, {"referenceID": 17, "context": "The first group of results are derived by combining key properties of ETD(\u03bb) with powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19].", "startOffset": 181, "endOffset": 193}, {"referenceID": 15, "context": "To derive the first group of results, we use powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19].", "startOffset": 144, "endOffset": 156}, {"referenceID": 16, "context": "To derive the first group of results, we use powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19].", "startOffset": 144, "endOffset": 156}, {"referenceID": 17, "context": "To derive the first group of results, we use powerful convergence theorems from the weak convergence methods in stochastic approximation theory [17, 18, 19].", "startOffset": 144, "endOffset": 156}, {"referenceID": 50, "context": "Some of these properties were established earlier in our work [52] when analyzing the almost sure convergence of ETD(\u03bb).", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "We use ergodic theorems for weak Feller Markov chains [29, 30], together with the properties of ETD(\u03bb) iterates and the convergence results we get from the weak convergence methods, in this second part of our analysis.", "startOffset": 54, "endOffset": 62}, {"referenceID": 28, "context": "We use ergodic theorems for weak Feller Markov chains [29, 30], together with the properties of ETD(\u03bb) iterates and the convergence results we get from the weak convergence methods, in this second part of our analysis.", "startOffset": 54, "endOffset": 62}, {"referenceID": 13, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "(This is not a surprise, for the biased algorithms are in fact defined by using a well-known robustifying approach from stochastic approximation theory [19].", "startOffset": 152, "endOffset": 156}, {"referenceID": 26, "context": ") Their behavior is demonstrated by experiments in [28, 53].", "startOffset": 51, "endOffset": 59}, {"referenceID": 51, "context": ") Their behavior is demonstrated by experiments in [28, 53].", "startOffset": 51, "endOffset": 59}, {"referenceID": 51, "context": "In particular, [53] is our companion note for this paper and includes several simulation results to illustrate some of the theorems we give here regarding the behavior of multiple consecutive iterates of the biased algorithms.", "startOffset": 15, "endOffset": 19}, {"referenceID": 50, "context": "2 Preliminaries In this section we describe the policy evaluation problem in the off-policy case, the ETD(\u03bb) algorithm and its constrained version, and we also review the results from our prior work [52] that are needed in this paper.", "startOffset": 199, "endOffset": 203}, {"referenceID": 0, "context": "Let \u03b3(s) \u2208 [0, 1], s \u2208 S, be state-dependent discount factors, with \u03b3(s) < 1 for at least one state.", "startOffset": 11, "endOffset": 17}, {"referenceID": 39, "context": "2 The ETD(\u03bb) Algorithm Like the standard TD(\u03bb) algorithm [41, 48], the ETD(\u03bb) algorithm [46] approximates the value function v\u03c0 by a function of the form v(s) = \u03c6(s) \u03b8, s \u2208 S, using a parameter vector \u03b8 \u2208 R and n-dimensional feature representations \u03c6(s) for the states.", "startOffset": 57, "endOffset": 65}, {"referenceID": 46, "context": "2 The ETD(\u03bb) Algorithm Like the standard TD(\u03bb) algorithm [41, 48], the ETD(\u03bb) algorithm [46] approximates the value function v\u03c0 by a function of the form v(s) = \u03c6(s) \u03b8, s \u2208 S, using a parameter vector \u03b8 \u2208 R and n-dimensional feature representations \u03c6(s) for the states.", "startOffset": 57, "endOffset": 65}, {"referenceID": 44, "context": "2 The ETD(\u03bb) Algorithm Like the standard TD(\u03bb) algorithm [41, 48], the ETD(\u03bb) algorithm [46] approximates the value function v\u03c0 by a function of the form v(s) = \u03c6(s) \u03b8, s \u2208 S, using a parameter vector \u03b8 \u2208 R and n-dimensional feature representations \u03c6(s) for the states.", "startOffset": 88, "endOffset": 92}, {"referenceID": 35, "context": ", [37]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "We focus on a general form of the ETD(\u03bb) algorithm, which uses state-dependent \u03bb values specified by a function \u03bb : S \u2192 [0, 1].", "startOffset": 120, "endOffset": 126}, {"referenceID": 0, "context": "The algorithm can access the following functions, in addition to the features \u03c6(s): (i) the state-dependent discount factor \u03b3(s) that defines v\u03c0 , as described earlier; (ii) \u03bb : S \u2192 [0, 1], which determines the single or multi-step Bellman equation for the algorithm (cf.", "startOffset": 182, "endOffset": 188}, {"referenceID": 40, "context": "Associated with ETD(\u03bb) is a generalized multistep Bellman equation of which v\u03c0 is the unique solution [42]: 6 v = r \u03c0,\u03b3 + P \u03bb \u03c0,\u03b3 v.", "startOffset": 102, "endOffset": 106}, {"referenceID": 44, "context": "5) we use here differs slightly from the original definition of et in [46], but the two are equivalent and (2.", "startOffset": 70, "endOffset": 74}, {"referenceID": 40, "context": "For the details of this Bellman equation, we refer the readers to the early work [42, 44] and the recent work [46].", "startOffset": 81, "endOffset": 89}, {"referenceID": 42, "context": "For the details of this Bellman equation, we refer the readers to the early work [42, 44] and the recent work [46].", "startOffset": 81, "endOffset": 89}, {"referenceID": 44, "context": "For the details of this Bellman equation, we refer the readers to the early work [42, 44] and the recent work [46].", "startOffset": 110, "endOffset": 114}, {"referenceID": 40, "context": "Earlier works on using such equations in TD learning include [42] and [4, Chap.", "startOffset": 61, "endOffset": 65}, {"referenceID": 47, "context": "The recent work [49] considers an even broader class of Bellman equations using the concept", "startOffset": 16, "endOffset": 20}, {"referenceID": 44, "context": "6) [46], which takes the following forms in the space of approximate value functions and in the space of the \u03b8-parameters, respectively: v = \u03a0 ( r \u03c0,\u03b3 + P \u03bb \u03c0,\u03b3 v ) , v \u2208 column-space(\u03a6), \u21d0\u21d2 C\u03b8 + b = 0, \u03b8 \u2208 R.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "9), the diagonal matrix M\u0304 is determined by the steady state probabilities of the states under the target policy \u03c0 under an ergodicity assumption [48], and for off-policy TD(\u03bb), it is determined by the steady state probabilities d\u03c0o(s) under the behavior policy \u03c0 .", "startOffset": 146, "endOffset": 150}, {"referenceID": 44, "context": "A salient property of ETD(\u03bb) is that the matrix C is always negative semidefinite [46], and under natural and mild conditions, C is negative definite.", "startOffset": 82, "endOffset": 86}, {"referenceID": 50, "context": "This is proved in [52] and summarized below.", "startOffset": 18, "endOffset": 22}, {"referenceID": 38, "context": "The relation between the approximate value function v = \u03a6\u03b8 and the desired value function v\u03c0, in particular, the approximation error, can be characterized by using the oblique projection viewpoint [40] for projected Bellman equations.", "startOffset": 197, "endOffset": 201}, {"referenceID": 38, "context": "Briefly speaking, [40] showed that the solutions of projected Bellman equations are oblique projections of v\u03c0 on the approximation subspace.", "startOffset": 18, "endOffset": 22}, {"referenceID": 37, "context": "An oblique projection is defined by two nonorthogonal subspaces of equal dimensions and is the projection onto the first subspace orthogonally to the second [39].", "startOffset": 157, "endOffset": 161}, {"referenceID": 13, "context": ") Recently, for the case of constant \u03bb, i and \u03b3, [15] derived bounds on the approximation bias that are based on contraction arguments and are comparable to the bound for on-policy TD(\u03bb) [48].", "startOffset": 49, "endOffset": 53}, {"referenceID": 46, "context": ") Recently, for the case of constant \u03bb, i and \u03b3, [15] derived bounds on the approximation bias that are based on contraction arguments and are comparable to the bound for on-policy TD(\u03bb) [48].", "startOffset": 187, "endOffset": 191}, {"referenceID": 52, "context": "[54] and [55, Sec.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "To prepare for the analysis, in the rest of this section, we review several results from [52] that will be needed.", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "Then applying powerful convergence theorems from the stochastic approximation theory [19], we can assert that the iterates \u03b8t will eventually \u201cfollow closely\u201d a solution of the mean ODE.", "startOffset": 85, "endOffset": 89}, {"referenceID": 50, "context": "It is shown in [52] that under Assumption 2.", "startOffset": 15, "endOffset": 19}, {"referenceID": 50, "context": "It was proved in [52] as a special case of the convergence of averaged sequences for a larger set of functions including h(\u03b8, \u00b7).", "startOffset": 17, "endOffset": 21}, {"referenceID": 50, "context": "The convergence of the averaged sequence 1t \u2211t\u22121 k=0 g(\u03bek) is given in the theorem below; the part on convergence in mean will be used frequently later in this paper (and was actually also needed in [52] to prove the ergodicity of {Zt} given earlier).", "startOffset": 199, "endOffset": 203}, {"referenceID": 17, "context": "These results are obtained through applying two general convergence theorems from [19], which concern weak convergence of stochastic approximation algorithms for diminishing and constant stepsize.", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "(This is the power of the weak convergence methods [17, 18, 19], by which our conclusion is obtained.", "startOffset": 51, "endOffset": 63}, {"referenceID": 16, "context": "(This is the power of the weak convergence methods [17, 18, 19], by which our conclusion is obtained.", "startOffset": 51, "endOffset": 63}, {"referenceID": 17, "context": "(This is the power of the weak convergence methods [17, 18, 19], by which our conclusion is obtained.", "startOffset": 51, "endOffset": 63}, {"referenceID": 17, "context": "The theorems from [19] which we will apply are based on the weak convergence methods.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "In the framework of [19], one studies a trajectory of iterates produced by an algorithm by working with continuous-time processes that are piecewise constant or linear interpolations of the iterates.", "startOffset": 20, "endOffset": 24}, {"referenceID": 27, "context": "Thus we can apply several ergodic theorems for weak Feller Markov chains (Meyn [29], Meyn and Tweedie [30]) to analyze the constant-stepsize case and combine the implications from these theorems with the results we obtained previously using stochastic approximation theory.", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "Thus we can apply several ergodic theorems for weak Feller Markov chains (Meyn [29], Meyn and Tweedie [30]) to analyze the constant-stepsize case and combine the implications from these theorems with the results we obtained previously using stochastic approximation theory.", "startOffset": 102, "endOffset": 106}, {"referenceID": 32, "context": "This iterative averaging is also known as \u201cPolyak-averaging\u201d when it is applied to accelerate the convergence of the \u03b8-iterates (see [34], [19, Chap.", "startOffset": 133, "endOffset": 137}, {"referenceID": 27, "context": "The uniqueness allows us to invoke a result of Meyn [29] on the convergence We adopt these conditions for simplicity.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "We will apply two theorems from [19], Theorems 8.", "startOffset": 32, "endOffset": 36}, {"referenceID": 50, "context": "Our proofs will rely on many properties of the ETD iterates that we have established in [52] when analyzing the almost sure convergence of the algorithm.", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "This algorithm belongs to the class of stochastic approximation algorithms with \u201cexogenous noises\u201d studied in the book [19]\u2014the term \u201cexogenous noises\u201d reflects the fact that the evolution of {\u03bet} is not driven by the \u03b8-iterates.", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "1 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "3 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "4 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "5 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "7 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "6 of [19] for the case of constant stepsize), the following conditions are required.", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "1 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "7 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "8 of [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "9 of [19], and it is in fact stronger than the latter condition but is satisfied by our algorithms as we will show.", "startOffset": 5, "endOffset": 9}, {"referenceID": 50, "context": "The proofs build upon several key properties of the ETD iterates we have established in [52] and recounted in Section 2.", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "For such a Markov chain {Xt} with state space X, let P (\u00b7, \u00b7) denote its transition kernel, that is, P : X\u00d7 B(X) \u2192 [0, 1], P (x,D) = Px(X1 \u2208 D), \u2200x \u2208 X, D \u2208 B(X), where B(X) denotes the Borel sigma-algebra on X, and Px denotes the probability distribution of {Xt} conditioned on X0 = x.", "startOffset": 115, "endOffset": 121}, {"referenceID": 0, "context": "For t \u2265 1, the t-step transition kernel P t(\u00b7, \u00b7) : X\u00d7 B(X) \u2192 [0, 1] is given by P (x,D) = Px(Xt \u2208 D), \u2200x \u2208 X, D \u2208 B(X), and for t = 0, P 0 is defined as P (x, \u00b7) = \u03b4x, the Dirac measure that assigns probability 1 to the point x, for each x \u2208 X.", "startOffset": 62, "endOffset": 68}, {"referenceID": 27, "context": "1 in [29].", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "It is a result of Meyn [29] and will be needed in our proofs of Theorems 3.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "For two topological spaces X and Y, a function Q : B(X) \u00d7Y \u2192 [0, 1] is a (Borel measurable) stochastic kernel on X given Y, if for each y \u2208 Y, Q(\u00b7 | y) is a probability measure on B(X) and for each D \u2208 B(X), Q(D | y) is a Borel measurable function on Y.", "startOffset": 61, "endOffset": 67}, {"referenceID": 13, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "2 does not hold (in which case C is negative semidefinite [46]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 50, "context": "2 in [52] that G has a block-diagonal structure with respect to the partition {J1,J0}, G = [", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "2 when applying the two general convergence theorems from [19], and we used the negative definiteness of C implied by this assumption only near the end of our proofs to get the solution properties of the mean ODE associated with each algorithm.", "startOffset": 58, "endOffset": 62}, {"referenceID": 38, "context": "Then for the emphasized states, the relation between the approximate value function \u03a61\u03b8 and v\u03c0 on J1, in particular the approximation error, can again be characterized using the oblique projection viewpoint [40], similar to the case with Assumption 2.", "startOffset": 207, "endOffset": 211}, {"referenceID": 46, "context": "This is very similar to the case of TD(\u03bb) with possibly linearly dependent features discussed in [48].", "startOffset": 97, "endOffset": 101}, {"referenceID": 34, "context": "2 Off-policy TD(\u03bb) Applying TD(\u03bb) to off-policy learning by using importance sampling techniques was first proposed in [36, 35], and the focus there was on episodic data.", "startOffset": 119, "endOffset": 127}, {"referenceID": 33, "context": "2 Off-policy TD(\u03bb) Applying TD(\u03bb) to off-policy learning by using importance sampling techniques was first proposed in [36, 35], and the focus there was on episodic data.", "startOffset": 119, "endOffset": 127}, {"referenceID": 3, "context": "The analysis we gave in this paper applies directly to the (non-episodic) off-policy TD(\u03bb) algorithm studied in [5, 51, 10], when its divergence issue is avoided by setting \u03bb sufficiently large.", "startOffset": 112, "endOffset": 123}, {"referenceID": 49, "context": "The analysis we gave in this paper applies directly to the (non-episodic) off-policy TD(\u03bb) algorithm studied in [5, 51, 10], when its divergence issue is avoided by setting \u03bb sufficiently large.", "startOffset": 112, "endOffset": 123}, {"referenceID": 8, "context": "The analysis we gave in this paper applies directly to the (non-episodic) off-policy TD(\u03bb) algorithm studied in [5, 51, 10], when its divergence issue is avoided by setting \u03bb sufficiently large.", "startOffset": 112, "endOffset": 123}, {"referenceID": 0, "context": "Specifically, we consider constant \u03b3 \u2208 [0, 1) and constant \u03bb \u2208 [0, 1], and an infinitely long trajectory generated by the behavior policy as before.", "startOffset": 63, "endOffset": 69}, {"referenceID": 3, "context": ", It is not necessary to multiply the term \u03c6(St)\u03b8t by \u03c1t, and that version of the algorithm was the one given in [5, 51].", "startOffset": 113, "endOffset": 120}, {"referenceID": 49, "context": ", It is not necessary to multiply the term \u03c6(St)\u03b8t by \u03c1t, and that version of the algorithm was the one given in [5, 51].", "startOffset": 113, "endOffset": 120}, {"referenceID": 8, "context": "The experimental results in [10] suggest to us that each version can have less variance than the other in some occasions, however.", "startOffset": 28, "endOffset": 32}, {"referenceID": 49, "context": "As far as convergence analysis is concerned, the two versions are essentially the same and the analyses given in [51, 52] and this paper indeed apply simultaneously to both versions of the algorithm.", "startOffset": 113, "endOffset": 121}, {"referenceID": 50, "context": "As far as convergence analysis is concerned, the two versions are essentially the same and the analyses given in [51, 52] and this paper indeed apply simultaneously to both versions of the algorithm.", "startOffset": 113, "endOffset": 121}, {"referenceID": 46, "context": "1(ii), the associated projected Bellman equation is the same as that for onpolicy TD(\u03bb) [48] except that the projection norm is the weighted Euclidean norm with weights given by the steady state probabilities d\u03c0o(s), s \u2208 S.", "startOffset": 88, "endOffset": 92}, {"referenceID": 3, "context": "Assuming \u03a6 has full column rank, the corresponding equation in the \u03b8-space, C\u03b8+ b = 0, has the desired property that the matrix C is negative definite, if \u03bb is sufficiently large (in particular if \u03bb = 1) [5].", "startOffset": 204, "endOffset": 207}, {"referenceID": 49, "context": "(In fact, some of these properties were first derived for off-policy LSTD(\u03bb) and TD(\u03bb) in [51] and extended later in [52] to ETD(\u03bb).", "startOffset": 90, "endOffset": 94}, {"referenceID": 50, "context": "(In fact, some of these properties were first derived for off-policy LSTD(\u03bb) and TD(\u03bb) in [51] and extended later in [52] to ETD(\u03bb).", "startOffset": 117, "endOffset": 121}, {"referenceID": 50, "context": ") For the same reason, the convergence analyses we gave in [52] and this paper for ETD also apply to a variation of the ETD algorithm, ETD(\u03bb, \u03b2), proposed recently by Hallak et al.", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "[15], when the parameter \u03b2 is set in an appropriate range.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": ", [14, 38].", "startOffset": 2, "endOffset": 10}, {"referenceID": 36, "context": ", [14, 38].", "startOffset": 2, "endOffset": 10}, {"referenceID": 26, "context": ") The two biased constrained algorithms discussed in this paper were motivated by the need to mitigate the variance problem, and their robust behavior has been observed in our experiments [28, 53].", "startOffset": 188, "endOffset": 196}, {"referenceID": 51, "context": ") The two biased constrained algorithms discussed in this paper were motivated by the need to mitigate the variance problem, and their robust behavior has been observed in our experiments [28, 53].", "startOffset": 188, "endOffset": 196}, {"referenceID": 36, "context": "However, beyond simply constraining the iterates, more variance reduction techniques are needed, such as control variates [38, 1] and weighted importance sampling [36, 35, 27, 26].", "startOffset": 122, "endOffset": 129}, {"referenceID": 0, "context": "However, beyond simply constraining the iterates, more variance reduction techniques are needed, such as control variates [38, 1] and weighted importance sampling [36, 35, 27, 26].", "startOffset": 122, "endOffset": 129}, {"referenceID": 34, "context": "However, beyond simply constraining the iterates, more variance reduction techniques are needed, such as control variates [38, 1] and weighted importance sampling [36, 35, 27, 26].", "startOffset": 163, "endOffset": 179}, {"referenceID": 33, "context": "However, beyond simply constraining the iterates, more variance reduction techniques are needed, such as control variates [38, 1] and weighted importance sampling [36, 35, 27, 26].", "startOffset": 163, "endOffset": 179}, {"referenceID": 25, "context": "However, beyond simply constraining the iterates, more variance reduction techniques are needed, such as control variates [38, 1] and weighted importance sampling [36, 35, 27, 26].", "startOffset": 163, "endOffset": 179}, {"referenceID": 24, "context": "However, beyond simply constraining the iterates, more variance reduction techniques are needed, such as control variates [38, 1] and weighted importance sampling [36, 35, 27, 26].", "startOffset": 163, "endOffset": 179}, {"referenceID": 50, "context": "Regarding convergence analysis of ETD(\u03bb), the results we gave in [52] and this paper concern only the convergence properties and not the rates of convergence.", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "Finally, besides asymptotic behavior of the algorithms, their finite-time or finite-sample properties (such as those considered by [31, 2, 20, 21]), and their large deviations properties are also worth studying.", "startOffset": 131, "endOffset": 146}, {"referenceID": 1, "context": "Finally, besides asymptotic behavior of the algorithms, their finite-time or finite-sample properties (such as those considered by [31, 2, 20, 21]), and their large deviations properties are also worth studying.", "startOffset": 131, "endOffset": 146}, {"referenceID": 18, "context": "Finally, besides asymptotic behavior of the algorithms, their finite-time or finite-sample properties (such as those considered by [31, 2, 20, 21]), and their large deviations properties are also worth studying.", "startOffset": 131, "endOffset": 146}, {"referenceID": 19, "context": "Finally, besides asymptotic behavior of the algorithms, their finite-time or finite-sample properties (such as those considered by [31, 2, 20, 21]), and their large deviations properties are also worth studying.", "startOffset": 131, "endOffset": 146}], "year": 2017, "abstractText": "We consider the emphatic temporal-difference (TD) algorithm, ETD(\u03bb), for learning the value functions of stationary policies in a discounted, finite state and action Markov decision process. The ETD(\u03bb) algorithm was recently proposed by Sutton, Mahmood, and White [46] to solve a long-standing divergence problem of the standard TD algorithm when it is applied to off-policy training, where data from an exploratory policy are used to evaluate other policies of interest. The almost sure convergence of ETD(\u03bb) has been proved in our recent work under general off-policy training conditions, but for a narrow range of diminishing stepsize. In this paper we present convergence results for constrained versions of ETD(\u03bb) with constant stepsize and with diminishing stepsize from a broad range. Our results characterize the asymptotic behavior of the trajectory of iterates produced by those algorithms, and are derived by combining key properties of ETD(\u03bb) with powerful convergence theorems from the weak convergence methods in stochastic approximation theory. For the case of constant stepsize, in addition to analyzing the behavior of the algorithms in the limit as the stepsize parameter approaches zero, we also analyze their behavior for a fixed stepsize and bound the deviations of their averaged iterates from the desired solution. These results are obtained by exploiting the weak Feller property of the Markov chains associated with the algorithms, and by using ergodic theorems for weak Feller Markov chains, in conjunction with the convergence results we get from the weak convergence methods. Besides ETD(\u03bb), our analysis also applies to the off-policy TD(\u03bb) algorithm, when the divergence issue is avoided by setting \u03bb sufficiently large. It yields, for that case, new results on the asymptotic convergence properties of constrained off-policy TD(\u03bb) with constant or slowly diminishing stepsize.", "creator": "LaTeX with hyperref package"}}}