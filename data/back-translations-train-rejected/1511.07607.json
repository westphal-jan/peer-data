{"id": "1511.07607", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2015", "title": "Fine-Grain Annotation of Cricket Videos", "abstract": "The recognition of human activities is one of the key problems in video understanding. Action recognition is challenging even for specific categories of videos, such as sports, that contain only a small set of actions. Interestingly, sports videos are accompanied by detailed commentaries available online, which could be used to perform action annotation in a weakly-supervised setting. For the specific case of Cricket videos, we address the challenge of temporal segmentation and annotation of ctions with semantic descriptions. Our solution consists of two stages. In the first stage, the video is segmented into \"scenes\", by utilizing the scene category information extracted from text-commentary. The second stage consists of classifying video-shots as well as the phrases in the textual description into various categories. The relevant phrases are then suitably mapped to the video-shots. The novel aspect of this work is the fine temporal scale at which semantic information is assigned to the video. As a result of our approach, we enable retrieval of specific actions that last only a few seconds, from several hours of video. This solution yields a large number of labeled exemplars, with no manual effort, that could be used by machine learning algorithms to learn complex actions.", "histories": [["v1", "Tue, 24 Nov 2015 08:34:20 GMT  (2853kb,D)", "http://arxiv.org/abs/1511.07607v1", null], ["v2", "Wed, 27 Sep 2017 10:48:11 GMT  (2935kb,D)", "http://arxiv.org/abs/1511.07607v2", "ACPR 2015"]], "reviews": [], "SUBJECTS": "cs.MM cs.CL cs.CV", "authors": ["rahul anand sharma", "pramod sankar k", "cv jawahar"], "accepted": false, "id": "1511.07607"}, "pdf": {"name": "1511.07607.pdf", "metadata": {"source": "CRF", "title": "Fine-Grain Annotation of Cricket Videos", "authors": ["Rahul Anand Sharma", "Pramod Sankar", "C. V. Jawahar"], "emails": ["rahul.anand@research.iiit.ac.in", "pramod.kompalli@xerox.com", "jawahar@iiit.ac.in"], "sections": [{"heading": "1. Introduction", "text": "Identifying human actions in videos is a difficult problem for computer vision systems. There are three difficult tasks that need to be solved to perform action detection: 1) Identifying the sequence of images that contain an action performed, 2) locating the person performing the action, and 3) detecting pixel information to assign a semantic label. While each of these tasks could be solved independently, there are few robust solutions for their common conclusion in generic videos.Certain categories of videos, such as movies, news feeds, sports videos, etc. contain domain-specific terms that could be exploited toward a better understanding of the virtual content. For example, the appearance of a basketball court [?] could help track players and their movements. However, current visual detection solutions have seen limited success toward fine-grain-action classification. For example, it is difficult to distinguish an \"advance\" to a \"semi-volleyball\" in tennis."}, {"heading": "1.1. Our Solution", "text": "We present a two-step solution to the problem of fine-grained segmentation and annotation of cricket videos. In the first stage, the goal is to align the two modalities on a \"scene level,\" which consists of a common synchronization and segmentation of the video with the text commentary. Each scene is a significant event that lasts a few minutes (Figure 2) and is described by a small number of sentences in the comment (Figure 3). The solution for this stage is inspired by the approach proposed in [10] and in Section 2. Given the scene segmentation and the description for each scene, the next step is to align the individual descriptions with their corresponding visuals. At this stage, the alignment between the video recordings and the phrases of the text comment is achieved by classifying video recordings and phrases into a known group of categories, making them easily mapped across the modalities, as in Section 3. As a result of this extensive step, it is possible for us to create the first step detailed actions in this one."}, {"heading": "2. Scene Segmentation", "text": "A typical scene in a cricket match follows the sequence of events shown in Figure 2. A scene always begins with the bowler (similar to a pitcher in baseball) walking up to the batsman and throwing the ball, who then delivers his punch. Events that follow always vary according to the action. Each such scene is described in the text comments as shown in Figure 3. The comment consists of the event number, which is not a timestamp; the player names, which are difficult to recognize; and detailed descriptions that are conditioned in [10] the visual-temporal patterns of the scenes that are conditioned to the outcome of the event. In other words, a 1-run result is visually different from a 4-run result, which is difficult to interpret. This can be observed from the status charts displayed in Figure 4."}, {"heading": "3. Shot/Phrase Alignment", "text": "After segmenting the scene, we get an alignment between minute-long clips with a section of text. In order to make a fine-grained annotation of the video, we need to segment both the video clip and the description text. Firstly, the video segments at the scene level are excessively segmented into recordings to ensure that it is unlikely that multiple actions are depicted in one setting. In the case of the text, the action descriptions must be identified more finely grained than the sentence level, since the comment is free-flowing. Therefore, we opt for the phrase level by segmenting sentences at all punctuation marks. Both the video recordings and the phrases are divided into one of these categories: {Bowler Action, Batsman Action, Other} by learning suitable classifiers for each modality. Afterwards, the individual phrases could be assigned to the video recordings that belong to the same category."}, {"heading": "3.1. Video-Shot Recognition", "text": "To ensure that the video recordings are atomic to an action / activity, we perform an over-segmentation of the video. We use a window-based shot detection scheme that works as follows. For each Frame Fi, we calculate its difference from any other Frame Fj where j [i \u2212 w / 2, i + w / 2], for a selected window size w, centered on Fi. If the maximum frame difference within that window is greater than a certain threshold, we declare Fi as shotboundary. We select a small value for over-segmenting the scenarios. Each recording is now represented with the classic Bag of Visual Words (BoW) approach. \u2212 The SIFT features are initially calculated independently for each frame, which are then clustered using the K-Mean Clustering algorithm to build a visual vocabulary (where each cluster center corresponds to a visual word)."}, {"heading": "3.2. Text Classification", "text": "The Phrase Classifier is learned fully automatically, starting by crawling the web for comments from about 300 games and segmenting the text into phrases. It has been observed that the name of the bowler or batsman is sometimes included in the description, for example: \"Sachin hooks the ball to the square leg.\" These phrases can accordingly be described as belonging to the actions of the bowler or batsman. From the 300 game comments we now get about 1500 phrases for bowler actions and about 6000 phrases for the batsman shot. We remove the names of the respective players and present each phrase as a histogram of their constituent word occurrences. A linear SVMis now learns for the bowler and batsman categories via this bag-of-word representation. In the face of a test expression, the SVM gives certainty that it belongs to one of the two classes."}, {"heading": "4. Experiments", "text": "Record: Our record is taken from the YouTube channel of the Indian Premier League (IPL) Tournament. The format for this tournament is 20 overs per side, resulting in approximately 120 scenes or events per team. The record consists of 16 matches, resulting in approximately 64 hours of footage. Four matches were manually grounded at scene and shot level, two of which are used for training and the other two for testing."}, {"heading": "4.1. Scene Segmentation", "text": "For the text-driven scene segmentation module, we use the following mid-level characteristics: {Pitch, Ground, Sky, PlayerCloseup, Scorecard} These characteristics are modeled using bundled color histograms. Each image is then represented by the fraction of pixels that contain each of these concepts, so the scene is a spatial and temporal model in which these points are accumulation.The limitation of DP formulation is the amount of memory available to store the DP score and the indicator matrices. With our machines, we are limited to running the DP over 100K frames, which accounts for 60 scenes or 10 transitions of the match.The accuracy of scene segmentation is measured as the number of video recordings available in the correct scene segment. We get a segmentation accuracy of 83.45%. Example segmentation results for two scenes are represented in Scene 5, which is very close to the truth you can tell."}, {"heading": "4.2. Shot Recognition", "text": "The accuracy of shot detection based on various characteristics and SVM cores is given in Table 1. We observe that the 1000-size vocabulary works better than 300. The linear core seems to be sufficient to learn the decision limit, with a best pass accuracy of 82.25%. Refining SVM predictions with the CRF-based method results in an improved accuracy of 86.54. Specifically, the accuracy of the batsman / bowler shot categories is 89.68%."}, {"heading": "4.3. Shot Annotation Results", "text": "Since the segmentation of the scene may contain errors, we perform a search in a shot neighborhood centered on the inferred scene boundary. We evaluate the accuracy of finding the correct bowler and batsman shots within a neighborhood region R of the scene boundary specified in Table 2. It was observed that 90% of the bowler and batsman shots were correctly identified by searching within a window of 10 shots on either side of the inferred boundary. Once the shots are identified, the corresponding text comments for bowler and batsman actions are assigned to these video segments. Some shots that were correctly commented are shown in Figure 6."}, {"heading": "5. Conclusions", "text": "Our approach bypasses technical challenges in visual recognition by leveraging information from online text comments. We obtain high annotation accuracy, which is evaluated through a large collection of videos. Commented videos are to be made available to the community for benchmarking, such a rich data set is not yet publicly available. In the future, the obtained marked data sets could be used to learn classifiers for detecting and understanding fine-grained activities."}], "references": [{"title": "Event based indexing of broadcast sports video by intermodal collaboration", "author": ["N. Babaguchi", "Y. Kawai", "T. Kitahashi"], "venue": "IEEE Trans. Multimedia,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Segmentation of intentional human gestures for sports video annotation", "author": ["G.S. Chambers", "S. Venkatesh", "G.A.W. West", "H.H. Bui"], "venue": "In Proc. MMM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Hello! My name is.", "author": ["M. Everingham", "J. Sivic", "A. Zisserman"], "venue": "Buffy\u201d \u2013 automatic naming of characters in TV video. In Proc. BMVC,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Understanding videos, constructing plots: Learning a visually grounded storyline model from annotated videos", "author": ["A. Gupta", "P. Srinivasan", "J. Shi", "L. Davis"], "venue": "In Proc. CVPR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "A hierarchical framework for generic sports video classification", "author": ["M.H. Kolekar", "S. Sengupta"], "venue": "In Proc. ACCV,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Learning realistic human actions from movies", "author": ["I. Laptev", "M. Marszalek", "C. Schmid", "B. Rozenfeld"], "venue": "In Proc. CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Learning to track and identify players from broadcast sports videos", "author": ["W.-L. Lu", "J.-A. Ting", "J. Little", "K. Murphy"], "venue": "IEEE PAMI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Text driven temporal segmentation of cricket videos", "author": ["Pramod Sankar K", "S. Pandey", "C.V. Jawahar"], "venue": "In Proc. ICVGIP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Automatically extracting highlights for tv baseball programs", "author": ["Y. Rui", "A. Gupta", "A. Acero"], "venue": "In ACM Multimedia,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Video Google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "In Proc. ICCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Live sports event detection based on broadcast video and web-casting text", "author": ["C. Xu", "J. Wang", "K. Wan", "Y. Li", "L. Duan"], "venue": "In Proc. ACM Multimedia,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Personalized retrieval of sports video", "author": ["Y. Zhang", "X. Zhang", "C. Xu", "H. Lu"], "venue": "In Proc. Intl. Workshop on Multimedia Information Retrieval,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}], "referenceMentions": [{"referenceID": 2, "context": "Instead of addressing the problem using visual analysis alone, several researchers proposed to utilize relevant parallel information to build better solutions [5].", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "For example, the scripts available for movies provide a weak supervision to perform person [5] and action recognition [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "For example, the scripts available for movies provide a weak supervision to perform person [5] and action recognition [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "Similar parallel text in sports was previously used to detect events in soccer videos, and index them for efficient retrieval [13, 14].", "startOffset": 126, "endOffset": 134}, {"referenceID": 11, "context": "Similar parallel text in sports was previously used to detect events in soccer videos, and index them for efficient retrieval [13, 14].", "startOffset": 126, "endOffset": 134}, {"referenceID": 3, "context": "[6] learn a graphical model from annotated baseball videos that could then be used to generate captions automatically.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9] show that the weak supervision from parallel text results in superior player identification and tracking in Basketball videos.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "The solution for this stage is inspired from the approach proposed in [10], and presented in Section 2.", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "It was observed in [10] that the visual-temporal patterns of the scenes are conditioned on the outcome of the event.", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "Each shot is now represented with the classical Bag of Visual Words (BoW) approach [12].", "startOffset": 83, "endOffset": 87}], "year": 2015, "abstractText": "The recognition of human activities is one of the key problems in video understanding. Action recognition is challenging even for specific categories of videos, such as sports, that contain only a small set of actions. Interestingly, sports videos are accompanied by detailed commentaries available online, which could be used to perform action annotation in a weakly-supervised setting. For the specific case of Cricket videos, we address the challenge of temporal segmentation and annotation of actions with semantic descriptions. Our solution consists of two stages. In the first stage, the video is segmented into \u201cscenes\u201d, by utilizing the scene category information extracted from textcommentary. The second stage consists of classifying videoshots as well as the phrases in the textual description into various categories. The relevant phrases are then suitably mapped to the video-shots. The novel aspect of this work is the fine temporal scale at which semantic information is assigned to the video. As a result of our approach, we enable retrieval of specific actions that last only a few seconds, from several hours of video. This solution yields a large number of labelled exemplars, with no manual effort, that could be used by machine learning algorithms to learn complex actions.", "creator": "LaTeX with hyperref package"}}}