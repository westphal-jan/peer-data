{"id": "1511.07938", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Temporal Convolutional Neural Networks for Diagnosis from Lab Tests", "abstract": "Early diagnosis of treatable diseases is essential for improving healthcare, and many diseases' onsets are predictable from annual lab tests and their temporal trends. We introduce a multi-resolution convolutional neural network for early detection of multiple diseases from irregularly measured sparse lab values. Our novel architecture takes as input both an imputed version of the data and a binary observation matrix. For imputing the temporal sparse observations, we develop a flexible, fast to train method for differentiable multivariate kernel regression. Our experiments on data from 300K individuals over 8 years, 18 common lab measurements, and 170 diseases show that the temporal signatures learned via convolution are significantly more predictive than baselines commonly used for early disease diagnosis.", "histories": [["v1", "Wed, 25 Nov 2015 02:56:33 GMT  (2479kb,D)", "https://arxiv.org/abs/1511.07938v1", null], ["v2", "Thu, 7 Jan 2016 05:27:22 GMT  (2428kb,D)", "http://arxiv.org/abs/1511.07938v2", null], ["v3", "Tue, 19 Jan 2016 22:19:40 GMT  (2432kb,D)", "http://arxiv.org/abs/1511.07938v3", null], ["v4", "Fri, 11 Mar 2016 00:00:50 GMT  (2432kb,D)", "http://arxiv.org/abs/1511.07938v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["narges razavian", "david sontag"], "accepted": false, "id": "1511.07938"}, "pdf": {"name": "1511.07938.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["LAB TESTS", "Narges Razavian", "David Sontag"], "emails": ["razavian@cs.nyu.edu", "dsontag@cs.nyu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "2 TEMPORAL CONVOLUTIONAL NETWORK", "text": "Each individual has a variable length of history of laboratory observations (X) and diagnostic data sets (Y). X is continuously evaluated, and Y is binary. We use a sliding window to deal with variable length. At each point in time when t for each person X, the model considers a 36-month reverse window of all input D biomarkers, X1: Dt \u2212 36: t, to predict the outcome, is a binary vector Y of size M, corresponding to M disease outbreaks, each occurring within the following months from t + 3 to t + 24. In this paper, we look at commonly measured biomarkers, and predict 171 common diseases (i.e. D = 18, M = 171). To maintain the clinical validity for our early detection task, we have carefully discussed our experimental settings, outcome criteria, and exclusion criteria, which are discussed in section 2.We will discuss in Exclusion Time."}, {"heading": "2.1 RELATED WORK", "text": "Traditional methods of feature engineering dominate in the medical field. Only recently, attempts to learn the patterns have begun to gain some attention. (Lasko et al., 2013) investigated a method based on sparse auto-encoders to learn temporal variation characteristics from 30-day uric acid observations in order to distinguish between gout and leukemia. (Che et al., 2015) developed a training that allows earlier domain knowledge to regulate the deeper layers of the feed-forward network for the task of classifying multiple diseases when data sets are small. To our knowledge, a comprehensive study of Convolutionary Neural Networks for the task of detecting disease patterns is not yet among the first to show significant increases in speech recognition tasks on a large scale. In contrast to the language domain, where input is fully observed, we have sparse and asynchronously measured observations. Alternative models, however, would be the neural results currently available as the curative networks."}, {"heading": "3 IMPUTATION VIA DIFFERENTIABLE KERNEL REGRESSION", "text": "In order to learn biological disease signatures, we now present our imputation model, which we apply to the input before learning the variation patterns. Our model is based on non-parametric regression, which we formulate as differentiable functions of nuclei and input (univariate and multivariate), and using back propagation (Rumelhart et al., 1988) we show how to learn the entire form of the core function instead of crossing within a limited group of parametric families (such as Gauss or Laplace)."}, {"heading": "3.1 RELATED WORK ON UNIVARIATE KERNEL LEARNING", "text": "In the field of non-parametric methods, most existing work monitors only a few known core functions such as radial base (Gaussian), laplace or other simple nuclei and does not take into account the entire space of legal nuclei. At best, experiments such as (Duvenaud et al., 2013), (Go \ufffd nen & Alpayd\u0131n, 2011) learn a composition or combination of nuclear families. Algorithms are slow, and in practice, the search algorithm is not comprehensive enough to guarantee the restoration of the correct kernel."}, {"heading": "3.2 UNIVARIATE KERNEL REGRESSION: LEARNING THE KERNEL", "text": "Imagine that the input is derived from the D series (x-x), each of which is irregular. We call the samples x1t11, x1 t12,..., x1t1n1,..., xD tD2,..., xDtDnD, where xd refers to the time series d and td1,... t d, which refers to the time points at which the time series d is sampled. Kernel regression assumes the following: x = f (t) + N, 2) Given the observed samples xt1, xtn from the series, general functional regression with additive noise, we can estimate the value of x at a new point in time."}, {"heading": "3.3 MULTIVARIATE KERNEL REGRESSION: LEARNING THE TEMPORAL KERNEL AND DEPENDENCY STRUCTURE", "text": "Suppose we now have D time series corresponding to each of the laboratories; we could try to model the complete common distribution of time series so that observations of related laboratories could be used at close times to derive the values of missing laboratories; various multi-output extensions of Gaussian processes have been proposed before; the main problem with the models is that they are scalable only under sparse structural assumptions (Boyle & Frean, 2004; Alvarez et al., 2010; Alvarez & Lawrence, 2009) and highly correlated in biological domains due to many unobserved latent variables (Alvarez et al., 2011; Byron et al., 2009)."}, {"heading": "3.4 MULTIVARIATE KERNEL REGRESSION", "text": "We extend the kernel K to a predictive matrix of the size D \u00d7 (2M + 1) and learn the kernel magnitude (r, j, s), which corresponds to the kernel value between the assigned series at the time r and the series j at the time s. Multivariate kernel regression becomes a 2D convolution of this kernel matrix, where the observed points of all time series are normalized by the 2D convolution of the kernel matrix with a binary matrix encoding, which series exhibits an unequal observation at which time."}, {"heading": "4 EXPERIMENTS AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 DATA", "text": "Our original dataset consisted of laboratory measurement and diagnostic information for 298,000 people. Laboratory measurements had a resolution of one month, and we used a 36-month reverse window for each prediction. We limited the input of this paper to a comprehensive laboratory panel plus cholesterol and bilirubin (a total of 18 laboratory types) currently recommended annually and covered by insurance companies. Name and code of the laboratories used in our analysis are in Table 1. Each laboratory value was normalized by subtracting the mean and dividing it by standard deviation across the entire dataset. We randomly divided the individuals into a 100K training set, a 100K validation set and a 98K test set. Validation set was used to select the best epochs / parameters for models, and predictive results are presented on the test set that were not visible during training and validation. Output we corresponded with each disease diagnosis anew in our CD9."}, {"heading": "4.2 PREDICTION TASK SETUP", "text": "Our goal is the early diagnosis of diseases for people who do not already have the disease. We requested a 3-month period between the end of the reverse window (i.e. t) and the beginning of the early detection window. The purpose of the 3-month period was to ensure that the clinical tests conducted immediately prior to the diagnosis of a disease would not allow our system to cheat in predicting this disease. Each issue was defined as positive if the diagnosis of the disease was observed in at least two different months between 3 and 3 + 24 months after T. Using 24 months helps alleviate the loud labeling problem. Requiring at least 2 observations of noise also reduced the noise generated by up-coding physicians (physicians who indicate their incorrect suspected diagnosis as a diagnosis). For each disease, we excluded people who already have the disease after the time t + 3. Our exclusion required only 1 diagnostic spot, but also led to a more interesting 2."}, {"heading": "4.3 PREDICTION MODEL ARCHITECTURE DETAILS", "text": "The specific architectural choices for the common part of the prediction network are as follows: We have set the number of filters to 8 for all folding modules, with the core length of 3 (months) and the step size of 1. Each max pooling module has the horizontal length of 3 and the vertical length of 1, with a step size of 3 in the horizontal direction (i.e. no overlap); each folding module is followed by a batch normalization module (Ioffe & Szegedy, 2015) and then a ReLU nonlinearity (Nair & Hinton, 2010); we also have two fully connected layers (100 nodes each) after the concatenation of the results of all folding layers; each of the fully connected layers is followed by a batch normalization layer and a ReLu nonlinearity layer (Nair & Hinton, 2010); we also add a dropout module (Srivaropal, each of which connects 0,000,000 layers before each layer was connected)."}, {"heading": "4.4 DATA AUGMENTATION", "text": "During the training of kernel regression imputation, we randomly disrupted each time series by adding Gaussian noise with a standard deviation of 0.01 to each laboratory observation, and also randomly disrupted the time of each observation by a random jump from a Gaussian distribution in both directions with a standard deviation of 2 (we use the bottom of the continuous value to determine the integer number of months to be deferred). We found this step particularly important to learn robust imputation cores."}, {"heading": "4.5 IMPUTATION RESULTS", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "4.6 PREDICTION RESULTS", "text": "In fact, it is such that most of them are able to go in search of a suitable candidate to trump themselves, \"he told the German Press Agency.\" We have it not easy, \"he said,\" but we have it in hand, \"in fact:\" We have it in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand in hand, in hand, in hand, in hand, in hand in hand, in hand, in hand, in hand, in hand, in hand in hand, in hand, in hand, in hand, in hand in hand, in hand, in hand, in hand in hand, in hand, in hand, in hand, in hand, in hand in hand, in hand, in hand in hand, in hand, in hand, in hand in hand, in hand, in hand in hand, in hand in hand, in hand, in hand, in hand, in hand in hand, in hand in hand, in hand, in hand, in hand in hand in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand in hand in hand, in hand, in hand, in hand, in hand in hand, in hand, in hand, in hand in hand, in hand in hand, in hand, in hand in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand, in hand in hand, in hand, in hand, in hand in hand, in hand, in hand, in hand, in hand, in hand, in hand, in"}, {"heading": "5 CONCLUSION", "text": "In this paper, we presented the first large-scale application of Convolutionary Neural Networks to detect early disease signatures for disease prediction. We presented a new approach to non-parametric imputation, which is critical for learning biologically valid disease signatures. Our results show a significant improvement in the quality of early detection compared to methods currently used in most medical and clinical settings, with only 18 laboratory measurements being used in the last three years.Our findings suggest that the onset of many diseases, including severe heart, kidney and liver disease, prostate cancer and diabetes, is highly predictable in advance. In many of these diseases, early detection even by a few months can lead to significant increases in the effectiveness of treatment, the quality of life of patients and their families, as well as a reduction in the financial burden on health systems. Our method also allows large-scale interventions to better target the population with the highest risk, and the highest availability."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors thank Independence Blue Cross for their support. The Tesla K40 used for this research was donated by NVIDIA Corporation."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["Abdel-Hamid", "Ossama", "Mohamed", "Abdel-rahman", "Jiang", "Hui", "Penn", "Gerald"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Abdel.Hamid et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abdel.Hamid et al\\.", "year": 2012}, {"title": "Sparse convolved gaussian processes for multi-output regression. In Advances in neural information processing", "author": ["Alvarez", "Mauricio", "Lawrence", "Neil D"], "venue": null, "citeRegEx": "Alvarez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Alvarez et al\\.", "year": 2009}, {"title": "Efficient multioutput gaussian processes through variational inducing kernels", "author": ["Alvarez", "Mauricio A", "Luengo", "David", "Titsias", "Michalis K", "Lawrence", "Neil D"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Alvarez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Alvarez et al\\.", "year": 2010}, {"title": "Kernels for vector-valued functions: A review", "author": ["Alvarez", "Mauricio A", "Rosasco", "Lorenzo", "Lawrence", "Neil D"], "venue": "Machine Learning,", "citeRegEx": "Alvarez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Alvarez et al\\.", "year": 2011}, {"title": "Visit-to-visit low-density lipoprotein cholesterol variability and risk of cardiovascular outcomes: Insights from the tnt trial", "author": ["Bangalore", "Sripal", "Breazna", "Andrei", "DeMicco", "David A", "Wun", "Chuan-Chuan", "Messerli", "Franz H"], "venue": "Journal of the American College of Cardiology,", "citeRegEx": "Bangalore et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bangalore et al\\.", "year": 2015}, {"title": "Dependent gaussian processes", "author": ["Boyle", "Phillip", "Frean", "Marcus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Boyle et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boyle et al\\.", "year": 2004}, {"title": "Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity", "author": ["Byron", "M Yu", "Cunningham", "John P", "Santhanam", "Gopal", "Ryu", "Stephen I", "Shenoy", "Krishna V", "Sahani", "Maneesh"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Byron et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Byron et al\\.", "year": 2009}, {"title": "Deep computational phenotyping", "author": ["Che", "Zhengping", "Kale", "David", "Li", "Wenzhe", "Bahadori", "Mohammad Taha", "Liu", "Yan"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Che et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Che et al\\.", "year": 2015}, {"title": "A recurrent latent variable model for sequential data", "author": ["Chung", "Junyoung", "Kastner", "Kyle", "Dinh", "Laurent", "Goel", "Kratarth", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Structure discovery in nonparametric regression through compositional kernel search", "author": ["Duvenaud", "David", "Lloyd", "James Robert", "Grosse", "Roger", "Tenenbaum", "Joshua B", "Ghahramani", "Zoubin"], "venue": "arXiv preprint arXiv:1302.4922,", "citeRegEx": "Duvenaud et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2013}, {"title": "Kernel measures of conditional dependence", "author": ["Fukumizu", "Kenji", "Gretton", "Arthur", "Sun", "Xiaohai", "Sch\u00f6lkopf", "Bernhard"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Fukumizu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2007}, {"title": "Multiple kernel learning algorithms", "author": ["G\u00f6nen", "Mehmet", "Alpayd\u0131n", "Ethem"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "G\u00f6nen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "G\u00f6nen et al\\.", "year": 2011}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Effects of visit-to-visit variability in systolic blood pressure on macrovascular and microvascular complications in patient with type 2 diabetes: the advance trial", "author": ["Hata", "Jun", "Arima", "Hisatomi", "Rothwell", "Peter M", "Woodward", "Mark", "Zoungas", "Sophia", "Anderson", "Craig", "Patel", "Anushka", "Neal", "Bruce", "Glasziou", "Paul", "Hamet", "Pavel"], "venue": "Circulation, pp. CIRCULATIONAHA\u2013113,", "citeRegEx": "Hata et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hata et al\\.", "year": 2013}, {"title": "Trends in electronic health record system use among office-based physicians: United states, 2007-2012", "author": ["Hsiao", "Chun-Ju", "Hing", "Esther", "Ashman", "Jill"], "venue": "Natl Health Stat Report.,", "citeRegEx": "Hsiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hsiao et al\\.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Relating mean blood glucose and glucose variability to the risk of multiple episodes of hypoglycaemia in type 1 diabetes", "author": ["ES Kilpatrick", "AS Rigby", "K Goode", "Atkin", "SL"], "venue": "Diabetologia,", "citeRegEx": "Kilpatrick et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kilpatrick et al\\.", "year": 2007}, {"title": "Normalized and differential convolution", "author": ["Knutsson", "Hans", "Westin", "Carl-Fredrik"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Knutsson et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Knutsson et al\\.", "year": 1993}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Computational phenotype discovery using unsupervised feature learning over noisy, sparse, and irregular clinical data. volume 8, pp. e66341", "author": ["Lasko", "Thomas A", "Denny", "Joshua C", "Levy", "Mia A"], "venue": "Public Library of Science,", "citeRegEx": "Lasko et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lasko et al\\.", "year": 2013}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Le Cun", "B Boser", "Denker", "John S", "D Henderson", "Howard", "Richard E", "W Hubbard", "Jackel", "Lawrence D"], "venue": "In Advances in neural information processing systems. Citeseer,", "citeRegEx": "Cun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Cun et al\\.", "year": 1990}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "On estimating regression", "author": ["Nadaraya", "Elizbar A"], "venue": "Theory of Probability & Its Applications,", "citeRegEx": "Nadaraya and A.,? \\Q1964\\E", "shortCiteRegEx": "Nadaraya and A.", "year": 1964}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Gaussian processes for machine learning", "author": ["Rasmussen", "Carl Edward"], "venue": null, "citeRegEx": "Rasmussen and Edward.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Edward.", "year": 2006}, {"title": "Variability of repeated serum prostate-specific antigen (psa) measurements within less than 90 days in a well-defined patient", "author": ["Roehrborn", "Claus G", "Pickens", "G John", "Carmody", "Thomas"], "venue": "population. Urology,", "citeRegEx": "Roehrborn et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Roehrborn et al\\.", "year": 1996}, {"title": "Learning representations by back-propagating errors", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Deep convolutional neural networks for lvcsr", "author": ["Sainath", "Tara N", "Mohamed", "Abdel-rahman", "Kingsbury", "Brian", "Ramabhadran", "Bhuvana"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "A hilbert space embedding for distributions", "author": ["Smola", "Alex", "Gretton", "Arthur", "Song", "Le", "Sch\u00f6lkopf", "Bernhard"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Kernel embeddings of latent tree graphical models", "author": ["Song", "Le", "Xing", "Eric P", "Parikh", "Ankur P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "The recurrent temporal restricted boltzmann machine", "author": ["Sutskever", "Ilya", "Hinton", "Geoffrey E", "Taylor", "Graham W"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Learning generative models with visual attention", "author": ["Tang", "Yichuan", "Srivastava", "Nitish", "Salakhutdinov", "Ruslan R"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Joint training of a convolutional network and a graphical model for human pose estimation", "author": ["Tompson", "Jonathan J", "Jain", "Arjun", "LeCun", "Yann", "Bregler", "Christoph"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tompson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tompson et al\\.", "year": 2014}, {"title": "Smooth regression analysis", "author": ["Watson", "Geoffrey S"], "venue": "Sankhya\u0304: The Indian Journal of Statistics,", "citeRegEx": "Watson and S.,? \\Q1964\\E", "shortCiteRegEx": "Watson and S.", "year": 1964}, {"title": "Gaussian process regression networks", "author": ["Wilson", "Andrew", "Ghahramani", "Zoubin", "Knowles", "David A"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Wilson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 19, "context": "Representation learning and unsupervised feature discovery via deep learning has led to ground breaking advances in domains such as image processing (Krizhevsky et al., 2012), speech recognition (Graves & Schmidhuber, 2005), natural language processing (Mikolov et al.", "startOffset": 149, "endOffset": 174}, {"referenceID": 23, "context": ", 2012), speech recognition (Graves & Schmidhuber, 2005), natural language processing (Mikolov et al., 2013), surpassing methods based on hand-engineered features in all benchmarks tested.", "startOffset": 86, "endOffset": 108}, {"referenceID": 15, "context": "Following recent availability of large electronic medical record datasets and other biological signals (Hsiao et al., 2014), discovery of early temporal disease signatures within lab values has become a possibility.", "startOffset": 103, "endOffset": 123}, {"referenceID": 22, "context": "This last characteristic has inspired us to train a temporal convolution model (Le Cun et al., 1990; LeCun et al., 1998; Tompson et al., 2014; Krizhevsky et al., 2012) to learn variation patterns of labs as biological representations of healthy and diseased states.", "startOffset": 79, "endOffset": 167}, {"referenceID": 37, "context": "This last characteristic has inspired us to train a temporal convolution model (Le Cun et al., 1990; LeCun et al., 1998; Tompson et al., 2014; Krizhevsky et al., 2012) to learn variation patterns of labs as biological representations of healthy and diseased states.", "startOffset": 79, "endOffset": 167}, {"referenceID": 19, "context": "This last characteristic has inspired us to train a temporal convolution model (Le Cun et al., 1990; LeCun et al., 1998; Tompson et al., 2014; Krizhevsky et al., 2012) to learn variation patterns of labs as biological representations of healthy and diseased states.", "startOffset": 79, "endOffset": 167}, {"referenceID": 19, "context": ", 2014; Krizhevsky et al., 2012) to learn variation patterns of labs as biological representations of healthy and diseased states. In the clinical domain, each biomarker varies with a different natural speed of change in the body. Therefore, in this paper we focus on multi-resolution deep convolutional architectures, inspired by Mnih et al. (2014).", "startOffset": 8, "endOffset": 350}, {"referenceID": 17, "context": "Additionally, medical community has actively studied the causal effect of variations on different signals, such as glucose (Kilpatrick et al., 2007), cholesterol (Bangalore et al.", "startOffset": 123, "endOffset": 148}, {"referenceID": 4, "context": ", 2007), cholesterol (Bangalore et al., 2015), blood pressure(Hata et al.", "startOffset": 21, "endOffset": 45}, {"referenceID": 14, "context": ", 2015), blood pressure(Hata et al., 2013), and prostate-specific antigen (Roehrborn et al.", "startOffset": 23, "endOffset": 42}, {"referenceID": 28, "context": ", 2013), and prostate-specific antigen (Roehrborn et al., 1996) on different disease onsets.", "startOffset": 39, "endOffset": 63}, {"referenceID": 35, "context": "Examples of such models include Gaussian processes and generative models based on recurrent neural networks (Sutskever et al., 2011; 2009; Tang et al., 2014; Chung et al., 2015).", "startOffset": 108, "endOffset": 177}, {"referenceID": 36, "context": "Examples of such models include Gaussian processes and generative models based on recurrent neural networks (Sutskever et al., 2011; 2009; Tang et al., 2014; Chung et al., 2015).", "startOffset": 108, "endOffset": 177}, {"referenceID": 8, "context": "Examples of such models include Gaussian processes and generative models based on recurrent neural networks (Sutskever et al., 2011; 2009; Tang et al., 2014; Chung et al., 2015).", "startOffset": 108, "endOffset": 177}, {"referenceID": 33, "context": "Each of the hidden layers are subject to Dropout(Srivastava et al., 2014) regularization (with probability 0.", "startOffset": 48, "endOffset": 73}, {"referenceID": 20, "context": "(Lasko et al., 2013) studied a method based on sparse auto-encoders to learn temporal variation features from 30-day uric acid observations, to distinguish between gout and leukemia.", "startOffset": 0, "endOffset": 20}, {"referenceID": 7, "context": "(Che et al., 2015) developed a training which allows prior domain knowledge to regularize the deeper layers of feed-forward network, for the task of multiple disease classification when datasets are small.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "Within the domain of temporal convolutional networks, (Abdel-Hamid et al., 2012; Sainath et al., 2013) were among the first to show significant gains in speech recognition tasks in large scale.", "startOffset": 54, "endOffset": 102}, {"referenceID": 30, "context": "Within the domain of temporal convolutional networks, (Abdel-Hamid et al., 2012; Sainath et al., 2013) were among the first to show significant gains in speech recognition tasks in large scale.", "startOffset": 54, "endOffset": 102}, {"referenceID": 29, "context": "Using back-propagation (Rumelhart et al., 1988), we then show how one can learn the entire form of the kernel function instead of cross validating within a limited set of parametric family (such as Gaussian or Laplace).", "startOffset": 23, "endOffset": 47}, {"referenceID": 10, "context": "In best case, attempts such as (Duvenaud et al., 2013), (G\u00f6nen & Alpayd\u0131n, 2011) learn a composition or combination of kernel families.", "startOffset": 31, "endOffset": 54}, {"referenceID": 2, "context": "The dominant approaches rely on Bayesian formalization and process convolution (Boyle & Frean, 2004; Alvarez et al., 2010; Alvarez & Lawrence, 2009), and require known dependency structure on the multiple outputs.", "startOffset": 79, "endOffset": 148}, {"referenceID": 3, "context": "The main problem with the models is that they are only scalable under the sparse structure assumptions (Alvarez et al., 2011; Byron et al., 2009).", "startOffset": 103, "endOffset": 145}, {"referenceID": 6, "context": "The main problem with the models is that they are only scalable under the sparse structure assumptions (Alvarez et al., 2011; Byron et al., 2009).", "startOffset": 103, "endOffset": 145}, {"referenceID": 39, "context": "One solution for unconstrained structure was proposed in (Wilson et al., 2012), however inference required Monte Carlo sampling or variational inference, which were inefficient.", "startOffset": 57, "endOffset": 78}, {"referenceID": 11, "context": "Alternatively, one could model the full joint distribution for a window of interest using a nonparametric graphical model (Fukumizu et al., 2007; Smola et al., 2007; Song et al., 2011).", "startOffset": 122, "endOffset": 184}, {"referenceID": 31, "context": "Alternatively, one could model the full joint distribution for a window of interest using a nonparametric graphical model (Fukumizu et al., 2007; Smola et al., 2007; Song et al., 2011).", "startOffset": 122, "endOffset": 184}, {"referenceID": 32, "context": "Alternatively, one could model the full joint distribution for a window of interest using a nonparametric graphical model (Fukumizu et al., 2007; Smola et al., 2007; Song et al., 2011).", "startOffset": 122, "endOffset": 184}, {"referenceID": 33, "context": "We also add one Dropout module (Srivastava et al., 2014) (0.", "startOffset": 31, "endOffset": 56}, {"referenceID": 9, "context": "We implemented the architecture using Torch (Collobert et al., 2011).", "startOffset": 44, "endOffset": 68}], "year": 2016, "abstractText": "Early diagnosis of treatable diseases is essential for improving healthcare, and many diseases\u2019 onsets are predictable from annual lab tests and their temporal trends. We introduce a multi-resolution convolutional neural network for early detection of multiple diseases from irregularly measured sparse lab values. Our novel architecture takes as input both an imputed version of the data and a binary observation matrix. For imputing the temporal sparse observations, we develop a flexible, fast to train method for differentiable multivariate kernel regression. Our experiments on data from 298K individuals over 8 years, 18 common lab measurements, and 171 diseases show that the temporal signatures learned via convolution are significantly more predictive than baselines commonly used for early disease diagnosis.", "creator": "LaTeX with hyperref package"}}}