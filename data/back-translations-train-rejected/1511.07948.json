{"id": "1511.07948", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Learning Halfspaces and Neural Networks with Random Initialization", "abstract": "We study non-convex empirical risk minimization for learning halfspaces and neural networks. For loss functions that are $L$-Lipschitz continuous, we present algorithms to learn halfspaces and multi-layer neural networks that achieve arbitrarily small excess risk $\\epsilon&gt;0$. The time complexity is polynomial in the input dimension $d$ and the sample size $n$, but exponential in the quantity $(L/\\epsilon^2)\\log(L/\\epsilon)$. These algorithms run multiple rounds of random initialization followed by arbitrary optimization steps. We further show that if the data is separable by some neural network with constant margin $\\gamma&gt;0$, then there is a polynomial-time algorithm for learning a neural network that separates the training data with margin $\\Omega(\\gamma)$. As a consequence, the algorithm achieves arbitrary generalization error $\\epsilon&gt;0$ with ${\\rm poly}(d,1/\\epsilon)$ sample and time complexity. We establish the same learnability result when the labels are randomly flipped with probability $\\eta&lt;1/2$.", "histories": [["v1", "Wed, 25 Nov 2015 04:41:20 GMT  (62kb,D)", "http://arxiv.org/abs/1511.07948v1", "31 pages"]], "COMMENTS": "31 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuchen zhang", "jason d lee", "martin j wainwright", "michael i jordan"], "accepted": false, "id": "1511.07948"}, "pdf": {"name": "1511.07948.pdf", "metadata": {"source": "CRF", "title": "Learning Halfspaces and Neural Networks with Random Initialization", "authors": ["Yuchen Zhang", "Jason D. Lee", "Martin J. Wainwright", "Michael I. Jordan"], "emails": ["yuczhang@eecs.berkeley.edu", "jasondlee88@eecs.berkeley.edu", "wainwrig@eecs.berkeley.edu", "jordan@eecs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "Learning a half-space is the core problem solved by many machine learning methods, including the Perceptron problem (Rosenblatt, 1958), the support vector machine (Vapnik, 1998), and AdaBoost (Freund and Schapire, 1997). Formally, for a given entrance space X-Rd, a half-space is defined by a linear mapping f (x) = < w, x > from X to the real line. The character of the function value f (x) determines whether x is on the positive side or the negative side of the half-space. A designated data point consists of a pair (x, y) consisting of a pair (x, y), x-x {\u2212 1}, and in the face of such pairs {(xi, yi)} ni = 1, the empirical prediction error is given by \"(f): = 1n n n (n)."}, {"heading": "1.1 Our contributions", "text": "The first contribution of this paper is to present two poly (n, d) temporal methods - algorithm 1 and algorithm 2 - to minimize the cost function (2) for any L-Lipschitz function h. We prove that these algorithms achieve excessive risk for each given tolerance > 0 by going through several rounds of random initialization followed by a constant number of optimization rounds (e.g. the use of an algorithm such as stochastic gradient descent).The first algorithm is based on the selection of the initial vector uniformly randomly from the Euclidean sphere; despite the simplicity of this scheme, it still does not have trivial guarantees. The second algorithm uses a better initialization achieved by solving a minimum quadratic problem, which leads to a stronger theoretical guarantee."}, {"heading": "1.2 Related Work", "text": "This section is dedicated to discussing some related work to put our contributions in a broader context."}, {"heading": "1.2.1 Learning halfspaces", "text": "The problem of learning half-spaces is an important problem in theoretical computer science. It is known that for any constant approximation in 2001, the problem of approximating zero-one loss is mathematically difficult (Guruswami and Raghavendra, 2009; Daniely et al., 2014). However, half-spaces can be learned efficiently if the data comes from certain special distributions or if the label is corrupted by certain forms of noise. In fact, Blum et al. (1998) and Servedio and Valiant (2001) show that if the labels are corrupted by random noise, the half-space can be learned in polynomial time. The same conclusion was reached by Awasthi et al al al al al al al al al. (2015) when the labels are corrupted by mass noise, and the covariants are drawn from the uniform distribution on a single sphere. If the label-noise space can be learned, the half-space is inherent."}, {"heading": "1.2.2 Learning neural networks", "text": "It is known that any smooth function can be approximated by a neural network with only one hidden layer (Barron, 1993), but that the formation of such a network is NP-hard (Blum and Rivest, 1992). In practice, people use optimization algorithms such as stochastic gradients (SG) to train neural networks. Although strong theoretical results are available for the setting of convex objective functions, there are few such results in the nonconvex setting of neural networks. Several recent papers address the challenge of establishing polynomial learning outcomes for neural networks. Arora et al. (2013) investigate the restoration of denoizing auto-encoders. They assume that the top layer values of the network are randomly generated and all network weights randomly from {\u2212 1}."}, {"heading": "2 Preliminaries", "text": "In this section, we formalize the problem and present several preliminary lemmas that are useful for theoretical analysis. We first establish some notation to define a general empirical risk minimization problem. Let D be a dataset containing n dots (xi, yi) ni = 1, in which xi, X, Rd and yi, 1). The goal is to define a function f: X, R so that f (xi) is as close as possible to yi. We can write the loss function as \"(f): = n, i = 1, II = 1, II = (xi). (4), where hi: R \u2192 R is a continuous function that depends on yi, and {\u03b11,.) n} are not negative importance, which we refer to 1. As concrete examples, the function h can be the piecewise linear function defined by Equation (3) or the sigmoid function h."}, {"heading": "3 Learning Halfspaces", "text": "In this section, we assume that function f is a linear mapping f (x): = < w, x >, so that our cost function can be written as \"(w): = n \u2211 i = 1 \u03b1ih (\u2212 yi < w, xi >). (8) In this section, we present two polynomial time algorithms to approximately minimize this cost function over certain types of\" p-balls. \"Both algorithms perform several rounds of random initialization followed by any optimization steps. The first algorithm initializes the weight vector by uniformly pulling from a sphere. Next, we present and analyze a second algorithm that follows the initialization from solving a least quadratic problem. It achieves the stronger theoretical guarantees promised in the introduction."}, {"heading": "3.1 Initialization by uniform sampling", "text": "First, we analyze a very simple optimization method based on uniform sampling values. Let's minimize the objective function (8) by assuming, as necessary, that the number of points within the Eulcidean unit 1 - that is, the number of points we create in the Eulcidean sphere to minimize the loss function starting with the randomly drawn vector w. On an intuitive level, this algorithm will find the global optimum when the initial weighting is pulled near w, reducing the iterative optimization method to the global minimum."}, {"heading": "3.2 Initialization by solving a least-square problem", "text": "We turn to a more general constellation in which w * and xi are limited by a general \"q standard\" for some q functions (> q functions for some q functions [2]. The line p * [1, 2] denotes the associated dual exponent (i.e., so that 1 / p + 1 / q = 1), we assume that \"w * p * p\" and \"p * p\" for each i * [n]. Note that this generalizes our previous constellation, which is applied to the case p = q = 2. In this constellation, algorithm 2 is a method that outputs an approximate minimizer of the loss function. In each constellation, it extracts k n points from the data collection D, and then constructs a random least squared problem based on these samples. The solution to this problem is used to initialize an optimization step. The success of algorithm 2 is based on the following Lemma observation: If we then imply 1."}, {"heading": "3.3 Hardness result", "text": "In Theorem 2, time complexity has an exponential dependence on L /. Shalev-Shwartz et al. (2011) show that the time complexity in L cannot be polynomial even with improper learning. It is natural to wonder whether algorithm 2 can be improved to have a polynomial dependence on (n, d, 1 /) because L = 1. In this section, we show that this is probably not the case. To prove the hardness problem, we reduce the MAX-2-SAT problem, which is known to be NP-hard. In particular, we show that if there is an algorithm that solves the minimization problem (8), it also solves the MAX-2-SAT problem. Let us recall the MAX-2-SAT problem, which is known to be NP-hard."}, {"heading": "4 Learning Neural Networks", "text": "We must now turn to the case in which function f represents a neural network."}, {"heading": "4.1 Agnostic learning", "text": "The agnostic setting does not assume that there is a neural network that separates the data. Rather, our goal is to calculate a neural network that minimizes the loss function over the space of all given networks."}, {"heading": "4.2 Learning with separable data", "text": "In this section, we assume that the activation function of Nm is an odd function (i.e., there is a network f + 1 that is executed in such a way that yif (xi) gives a certain row for each i value. (Given a distribution P over space X + {\u2212 1, 1}, we say that it is divisible if there is a network f + 1 that is executed in such a way that yf (x) is almost certainly (in relation to P). Algorithm 4 learns a neural network on the divisible data. It uses the AdaBoost approach (Friend and Schapire, 1997) to construct the network, and we refer to it as the BoostNet algorithm."}, {"heading": "4.3 Hardness result for \u03b3-separable problems", "text": "Finally, we present a hardness result that shows that the dependence on 1 / \u03b3 is difficult to improve. Our proof is based on the hardness of the standard (non-agnostic) PAC doctrine of the intersection of hemispheres given in Klivans et al. (2006). Specifically, we look at the family of hemisphere indicator functions given by X = {\u2212 1, 1} d to {\u2212 1, 1} from H = {x \u2192 characters (wTx \u2212 b \u2212 1 / 2): x {\u2212 1} d, b \u00b2 N, w \u00b2 Nd, | b \u00b2 w \u00b2 Poly (d)}. Faced with a T tuple of functions {h1,., hT} belonging to H, we define the intersection functions (x) = {1 if h1 (x) = \u00b7 \u00b7 hT (x) = hT (x) (x) = hT (x) = 1, \u2212 1 otherwise, which represents the intersection of T hemispheres."}, {"heading": "5 Simulation", "text": "In this section, we compare the BoostNet algorithm with the classical method of reverse propagation for the formation of two-layer neural networks. The aim is to learn parity functions from noisy data - a challenging problem in mathematical learning theory (see e.g. Blum et al., 2003). We construct a synthetic dataset with n = 50,000 points. Each point (x, y) is generated as follows: First, the vector x is uniformly drawn from {\u2212 1, 1} d and linked to a constant 1 as (d + 1) -th coordinate. The label is generated as follows: for an unknown subset of p indices 1 \u2264 i1 < \u00b7 < ip \u2264 d, we sety = {xi1xi2.."}, {"heading": "6 Conclusion", "text": "In this paper, we have proposed algorithms to learn semi-spaces and neural networks with non-convex loss functions. We show that the temporal complexity in the input dimension and sample size is polynomial, but exponentially in excess risk. A hardness result in terms of the need for exponential dependence is also presented. The algorithms perform a randomized initialization followed by optimization steps. This idea is consistent with the widely used heuristics in practice, but the theoretical analysis suggests that careful treatment of the initialization step is necessary. We proposed the BoostNet algorithm and showed that it can be used to learn a neural network in polynomic time when the data can be separated at a constant distance. We suspect that the theoretical results of this work are likely to be conservative, as the algorithms applied to real data may be much more efficient than the boundaries suggest."}, {"heading": "A Proof of Lemma 1", "text": "The following inequality is always valid: sup f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f & # 160; f # 160; f & # 160; f & # 160; f; f; f; f; f; 160; f; f; f; f; f; 160; f; f; f; f; f; f; and # 160; f; f; f; f; f; f; f; f; f; and f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; f; r; f; f; f; 160; f; f; f; f; f; f; f; f; f; f; f; f; f; and 160; f; f; f; f; r; f; f; f; f; f; f; f; f; f; f; f; f; f; f; and 160; f; f; f; f; f; f; f; f; f; f; f; f; f; r; f; f; f; f; f; f; f; and"}, {"heading": "B Proof of Theorem 1", "text": "Consider a single iteration of algorithm 1. Let us remember that u > inequality is sampled evenly from the unit sphere of Rd. Alternatively, it can be viewed using the following procedure: If we first draw a random s-dimensional sub-space of Rd, then we draw u evenly from the unit sphere of S. If we look at the n + 2 fixed points {0, w \u00b2, x1,. \u2212 If we see a random s-dimensional sub-space of Rd, then we will have an operator that projects the vector w \u00b2 Rd into sub-space S and scales it to r. If we choose s, 12 log (n + 2) 2 and r: \u2212 xi \u2212 s, then Lemma 2 implies that we have at least 1 / (n + 2) an operator that projects the vector w \u00b2 Rd into sub-space S and scales it to l."}, {"heading": "C Proof of Theorem 2", "text": "s examine a single iteration of algorithm 2. \u2212 k The condition for any (x-q, y-j) kj = 1, we define the function G (w): = 1k-k = 1 h (\u2212 y-j < w, x-j >) \u2212 h (\u2212 y-j >). Since h is L-Lipschitz, we have the function G (v) \u2212 w (w-w) = 1 k k-j = 1 h (\u2212 y-j < w, x-j >) \u2212 h (\u2212 y-j < w-j < w >) \u2264 L k-p (v-w)."}, {"heading": "D Proof of Proposition 1", "text": "We verify the lower limit using a data set in which we analyze the problem."}, {"heading": "E Proof of Theorem 3", "text": "Taking into account the definition (6) of the Rademacher complexity, the following problem is the complexity of Nm.Lemma 6. The Rademacher complexity of Nm is limited as Rk (Nm)."}, {"heading": "F Proof of Theorem 4", "text": "The proof of the part (a) First of all, we prove that f = a = a = a = a = b = a = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b = b."}, {"heading": "G Proof of Corollary 1", "text": "The first step is to use the incomplete learning algorithm (Zhang et al., 2015, algorithm 1) to learn a predictor g that minimizes the following risk function: \"(g): = E [\u03c6 (\u2212 y \u2264 g (x))], where? (t): = {\u2212 2\u03b71 \u2212 2\u03b7 + \u03b7 (t + \u03b3) (1 \u2212 2\u03b7), if t \u2264 \u2212 \u03b3, \u2212 2\u03b71 \u2212 2\u03b7 + t (1 \u2212 2\u03b7), if t > \u2212 \u03b3.Since \u03b7 < 1 / 2, the function \u03c6 (t) is convex and Lipschitz is continuous. The activation function erf (x) \u2212 \u03b3 fulfills the condition of (Zhang et al., 2015, theorem 1). Thus, the example complexity is polarity (1 / \u0440, log (1 / \u043c), the x) and the time complexity that x implies."}, {"heading": "H Proof of Proposition 2", "text": "We reduce the PAC learning of the activation function to the problem of learning a neural mesh (=). We assume that T = area mesh (= > area mesh) for any number of pairs taking the form (x, h, x) that there is a neural network f (\u00b7 \u00b7 N2) that separates all pairs with a margin of n, and also that the margin is limited by x. To prove the assertion, it must be remembered that h (x) = 1 if and only if h1 (x) = hT (x) = 1 for any h1,. hT (x) for any hT (x) H. For each ht, the definition of H implies that there is one (wt, bt) such a pair if ht (x) = 1 then w T t \u2212 bt \u2212 1 / 2)."}], "references": [{"title": "Provable bounds for learning some deep representations", "author": ["S. Arora", "A. Bhaskara", "R. Ge", "T. Ma"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "The power of localization for efficiently learning linear separators with noise", "author": ["P. Awasthi", "M.F. Balcan", "P.M. Long"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Awasthi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2014}, {"title": "Efficient learning of linear separators under bounded noise", "author": ["P. Awasthi", "M.-F. Balcan", "N. Haghtalab", "R. Urner"], "venue": null, "citeRegEx": "Awasthi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2015}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["A.R. Barron"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Barron.,? \\Q1993\\E", "shortCiteRegEx": "Barron.", "year": 1993}, {"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network", "author": ["P.L. Bartlett"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Bartlett.,? \\Q1998\\E", "shortCiteRegEx": "Bartlett.", "year": 1998}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2003\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2003}, {"title": "Efficient learning of linear perceptrons", "author": ["S. Ben-David", "H.U. Simon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ben.David and Simon.,? \\Q2001\\E", "shortCiteRegEx": "Ben.David and Simon.", "year": 2001}, {"title": "Learning halfspaces with the zero-one loss: time-accuracy tradeoffs", "author": ["A. Birnbaum", "S.S. Shwartz"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Birnbaum and Shwartz.,? \\Q2012\\E", "shortCiteRegEx": "Birnbaum and Shwartz.", "year": 2012}, {"title": "Training a 3-node neural network is NP-complete", "author": ["A. Blum", "R.L. Rivest"], "venue": "Neural Networks,", "citeRegEx": "Blum and Rivest.,? \\Q1992\\E", "shortCiteRegEx": "Blum and Rivest.", "year": 1992}, {"title": "A polynomial-time algorithm for learning noisy linear threshold functions", "author": ["A. Blum", "A. Frieze", "R. Kannan", "S. Vempala"], "venue": null, "citeRegEx": "Blum et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1998}, {"title": "Noise-tolerant learning, the parity problem, and the statistical query model", "author": ["A. Blum", "A. Kalai", "H. Wasserman"], "venue": "Journal of the ACM,", "citeRegEx": "Blum et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2003}, {"title": "From average case complexity to improper learning complexity", "author": ["A. Daniely", "N. Linial", "S. Shalev-Shwartz"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Daniely et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2014}, {"title": "An elementary proof of the Johnson-Lindenstrauss lemma", "author": ["S. Dasgupta", "A. Gupta"], "venue": "International Computer Science Institute,", "citeRegEx": "Dasgupta and Gupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta and Gupta.", "year": 1999}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Hardness of learning halfspaces with noise", "author": ["V. Guruswami", "P. Raghavendra"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Guruswami and Raghavendra.,? \\Q2009\\E", "shortCiteRegEx": "Guruswami and Raghavendra.", "year": 2009}, {"title": "Training a sigmoidal node is hard", "author": ["D.R. Hush"], "venue": "Neural Computation,", "citeRegEx": "Hush.,? \\Q1999\\E", "shortCiteRegEx": "Hush.", "year": 1999}, {"title": "Generalization bounds for neural networks through tensor factorization", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": null, "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "Lecture note: Rademacher composition and linear prediction", "author": ["S. Kakade", "A. Tewari"], "venue": null, "citeRegEx": "Kakade and Tewari.,? \\Q2008\\E", "shortCiteRegEx": "Kakade and Tewari.", "year": 2008}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["S.M. Kakade", "K. Sridharan", "A. Tewari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kakade et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2009}, {"title": "Agnostically learning halfspaces", "author": ["A.T. Kalai", "A.R. Klivans", "Y. Mansour", "R.A. Servedio"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Kalai et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2008}, {"title": "Embedding hard learning problems into gaussian space. Approximation, Randomization, and Combinatorial Optimization", "author": ["A. Klivans", "P. Kothari"], "venue": "Algorithms and Techniques,", "citeRegEx": "Klivans and Kothari.,? \\Q2014\\E", "shortCiteRegEx": "Klivans and Kothari.", "year": 2014}, {"title": "Cryptographic hardness for learning intersections of halfspaces", "author": ["A.R. Klivans", "A. Sherstov"], "venue": "In 47th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Klivans and Sherstov,? \\Q2006\\E", "shortCiteRegEx": "Klivans and Sherstov", "year": 2006}, {"title": "Learning halfspaces with malicious noise", "author": ["A.R. Klivans", "P.M. Long", "R.A. Servedio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Klivans et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Klivans et al\\.", "year": 2009}, {"title": "Empirical margin distributions and bounding the generalization error of combined classifiers", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": "Annals of Statistics,", "citeRegEx": "Koltchinskii and Panchenko.,? \\Q2002\\E", "shortCiteRegEx": "Koltchinskii and Panchenko.", "year": 2002}, {"title": "Probability in Banach Spaces: isoperimetry and processes, volume 23", "author": ["M. Ledoux", "M. Talagrand"], "venue": "Springer Science & Business Media,", "citeRegEx": "Ledoux and Talagrand.,? \\Q2013\\E", "shortCiteRegEx": "Ledoux and Talagrand.", "year": 2013}, {"title": "Norm-based capacity control in neural networks", "author": ["B. Neyshabur", "R. Tomioka", "N. Srebro"], "venue": null, "citeRegEx": "Neyshabur et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 1993}, {"title": "On the equivalence of weak learnability and linear separability", "author": ["versity", "MA Cambridge", "2001. S. Shalev-Shwartz", "Y. Singer"], "venue": null, "citeRegEx": "versity et al\\.,? \\Q2001\\E", "shortCiteRegEx": "versity et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 13, "context": "The learning of a halfspace is the core problem solved by many machine learning methods, including the Perceptron (Rosenblatt, 1958), the Support Vector Machine (Vapnik, 1998) and AdaBoost (Freund and Schapire, 1997).", "startOffset": 189, "endOffset": 216}, {"referenceID": 14, "context": "In particular, Guruswami and Raghavendra (2009) show that, for any \u2208 (0, 1/2], given a set of", "startOffset": 15, "endOffset": 48}, {"referenceID": 14, "context": "In fact, the result of Guruswami and Raghavendra (2009) shows that the approximation ratio of such procedures could be arbitrarily large.", "startOffset": 23, "endOffset": 56}, {"referenceID": 13, "context": "This so-called BoostNet algorithm uses the AdaBoost approach (Freund and Schapire, 1997) to construct a m-layer neural network by taking an (m \u2212 1)-layer network as a weak classifier.", "startOffset": 61, "endOffset": 88}, {"referenceID": 14, "context": "It is known that for any constant approximation ratio, the problem of approximately minimizing the zero-one loss is computationally hard (Guruswami and Raghavendra, 2009; Daniely et al., 2014).", "startOffset": 137, "endOffset": 192}, {"referenceID": 11, "context": "It is known that for any constant approximation ratio, the problem of approximately minimizing the zero-one loss is computationally hard (Guruswami and Raghavendra, 2009; Daniely et al., 2014).", "startOffset": 137, "endOffset": 192}, {"referenceID": 19, "context": "When the label noise is adversarial, the halfspace can be learned if the data distribution is isotropic log-concave and the fraction of labels being corrupted is bounded by a small quantity (Kalai et al., 2008; Klivans et al., 2009; Awasthi et al., 2014).", "startOffset": 190, "endOffset": 254}, {"referenceID": 22, "context": "When the label noise is adversarial, the halfspace can be learned if the data distribution is isotropic log-concave and the fraction of labels being corrupted is bounded by a small quantity (Kalai et al., 2008; Klivans et al., 2009; Awasthi et al., 2014).", "startOffset": 190, "endOffset": 254}, {"referenceID": 1, "context": "When the label noise is adversarial, the halfspace can be learned if the data distribution is isotropic log-concave and the fraction of labels being corrupted is bounded by a small quantity (Kalai et al., 2008; Klivans et al., 2009; Awasthi et al., 2014).", "startOffset": 190, "endOffset": 254}, {"referenceID": 7, "context": "Indeed, Blum et al. (1998) and Servedio and Valiant (2001) show that if the labels are corrupted by random noise, then the halfspace can be learned in polynomial time.", "startOffset": 8, "endOffset": 27}, {"referenceID": 7, "context": "Indeed, Blum et al. (1998) and Servedio and Valiant (2001) show that if the labels are corrupted by random noise, then the halfspace can be learned in polynomial time.", "startOffset": 8, "endOffset": 59}, {"referenceID": 1, "context": "The same conclusion was established by Awasthi et al. (2015) when the labels are corrupted by Massart noise, and the covariates are drawn from the uniform distribution on a unit sphere.", "startOffset": 39, "endOffset": 61}, {"referenceID": 17, "context": "on the noise, Kalai et al. (2008) show that if the data are drawn from the uniform distribution on a unit sphere, then there is an algorithm whose time complexity is polynomial in the input dimension, but exponential in 1/ (where is the additive error).", "startOffset": 14, "endOffset": 34}, {"referenceID": 17, "context": "on the noise, Kalai et al. (2008) show that if the data are drawn from the uniform distribution on a unit sphere, then there is an algorithm whose time complexity is polynomial in the input dimension, but exponential in 1/ (where is the additive error). In this same setting, Klivans and Kothari (2014) prove that the exponential dependence on 1/ is unavoidable.", "startOffset": 14, "endOffset": 303}, {"referenceID": 6, "context": "Ben-David and Simon (2001) suggest comparing the zero-one loss of the learned halfspace to the optimal \u03bc-margin loss.", "startOffset": 0, "endOffset": 27}, {"referenceID": 6, "context": "Ben-David and Simon (2001) suggest comparing the zero-one loss of the learned halfspace to the optimal \u03bc-margin loss. The \u03bc-margin loss asserts that all points whose classification margins are smaller than \u03bc should be marked as misclassified. Under this metric, it was shown by Ben-David and Simon (2001); Birnbaum and Shwartz (2012) that the optimal \u03bc-margin loss can be achieved in polynomial time if \u03bc is a positive constant.", "startOffset": 0, "endOffset": 305}, {"referenceID": 6, "context": "Ben-David and Simon (2001) suggest comparing the zero-one loss of the learned halfspace to the optimal \u03bc-margin loss. The \u03bc-margin loss asserts that all points whose classification margins are smaller than \u03bc should be marked as misclassified. Under this metric, it was shown by Ben-David and Simon (2001); Birnbaum and Shwartz (2012) that the optimal \u03bc-margin loss can be achieved in polynomial time if \u03bc is a positive constant.", "startOffset": 0, "endOffset": 334}, {"referenceID": 6, "context": "Ben-David and Simon (2001) suggest comparing the zero-one loss of the learned halfspace to the optimal \u03bc-margin loss. The \u03bc-margin loss asserts that all points whose classification margins are smaller than \u03bc should be marked as misclassified. Under this metric, it was shown by Ben-David and Simon (2001); Birnbaum and Shwartz (2012) that the optimal \u03bc-margin loss can be achieved in polynomial time if \u03bc is a positive constant. Shalev-Shwartz et al. (2011) study the minimization of a continuous approximation to the zero-one loss, which is similar to our setup.", "startOffset": 0, "endOffset": 458}, {"referenceID": 3, "context": "2 Learning neural networks It is known that any smooth function can be approximated by a neural network with just one hidden layer (Barron, 1993), but that training such a network is NP-hard (Blum and Rivest, 1992).", "startOffset": 131, "endOffset": 145}, {"referenceID": 8, "context": "2 Learning neural networks It is known that any smooth function can be approximated by a neural network with just one hidden layer (Barron, 1993), but that training such a network is NP-hard (Blum and Rivest, 1992).", "startOffset": 191, "endOffset": 214}, {"referenceID": 0, "context": "Arora et al. (2013) study the recovery of denoising auto-encoders.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Arora et al. (2013) study the recovery of denoising auto-encoders. They assume that the top-layer values of the network are randomly generated and all network weights are randomly drawn from {\u22121, 1}. As a consequence, the bottom layer generates a sequence of random observations from which the algorithm can recover the network weights. The algorithm has polynomial-time complexity and is capable of learning random networks that are drawn from a specific distribution. However, in practice people want to learn deterministic networks that encode data-dependent representations. Sedghi and Anandkumar (2014) study the supervised learning of neural networks under the assumption that the score function of the data distribution is known.", "startOffset": 0, "endOffset": 608}, {"referenceID": 0, "context": "Arora et al. (2013) study the recovery of denoising auto-encoders. They assume that the top-layer values of the network are randomly generated and all network weights are randomly drawn from {\u22121, 1}. As a consequence, the bottom layer generates a sequence of random observations from which the algorithm can recover the network weights. The algorithm has polynomial-time complexity and is capable of learning random networks that are drawn from a specific distribution. However, in practice people want to learn deterministic networks that encode data-dependent representations. Sedghi and Anandkumar (2014) study the supervised learning of neural networks under the assumption that the score function of the data distribution is known. They show that if the input dimension is large enough and the network is sparse enough, then the first network layer can be learned by a polynomial-time algorithm. More recently, Janzamin et al. (2015) propose another algorithm relying on the score function that removes the restrictions of Sedghi and Anandkumar (2014).", "startOffset": 0, "endOffset": 939}, {"referenceID": 0, "context": "Arora et al. (2013) study the recovery of denoising auto-encoders. They assume that the top-layer values of the network are randomly generated and all network weights are randomly drawn from {\u22121, 1}. As a consequence, the bottom layer generates a sequence of random observations from which the algorithm can recover the network weights. The algorithm has polynomial-time complexity and is capable of learning random networks that are drawn from a specific distribution. However, in practice people want to learn deterministic networks that encode data-dependent representations. Sedghi and Anandkumar (2014) study the supervised learning of neural networks under the assumption that the score function of the data distribution is known. They show that if the input dimension is large enough and the network is sparse enough, then the first network layer can be learned by a polynomial-time algorithm. More recently, Janzamin et al. (2015) propose another algorithm relying on the score function that removes the restrictions of Sedghi and Anandkumar (2014). The assumption in this case is that the network weights satisfy a non-degeneracy condition; however, the algorithm is only capable of learning neural networks with one hidden layer.", "startOffset": 0, "endOffset": 1057}, {"referenceID": 0, "context": "Arora et al. (2013) study the recovery of denoising auto-encoders. They assume that the top-layer values of the network are randomly generated and all network weights are randomly drawn from {\u22121, 1}. As a consequence, the bottom layer generates a sequence of random observations from which the algorithm can recover the network weights. The algorithm has polynomial-time complexity and is capable of learning random networks that are drawn from a specific distribution. However, in practice people want to learn deterministic networks that encode data-dependent representations. Sedghi and Anandkumar (2014) study the supervised learning of neural networks under the assumption that the score function of the data distribution is known. They show that if the input dimension is large enough and the network is sparse enough, then the first network layer can be learned by a polynomial-time algorithm. More recently, Janzamin et al. (2015) propose another algorithm relying on the score function that removes the restrictions of Sedghi and Anandkumar (2014). The assumption in this case is that the network weights satisfy a non-degeneracy condition; however, the algorithm is only capable of learning neural networks with one hidden layer. Our algorithm does not impose any assumption on the data distribution, and is able to learn multilayer neural networks. Another approach to the problem is via the improper learning framework. The goal in this case is to find a predictor that is not a neural network, but performs as well as the best possible neural network in terms of the generalization error. Livni et al. (2014) propose a polynomial-time algorithm to learn networks whose activation function is quadratic.", "startOffset": 0, "endOffset": 1622}, {"referenceID": 0, "context": "Arora et al. (2013) study the recovery of denoising auto-encoders. They assume that the top-layer values of the network are randomly generated and all network weights are randomly drawn from {\u22121, 1}. As a consequence, the bottom layer generates a sequence of random observations from which the algorithm can recover the network weights. The algorithm has polynomial-time complexity and is capable of learning random networks that are drawn from a specific distribution. However, in practice people want to learn deterministic networks that encode data-dependent representations. Sedghi and Anandkumar (2014) study the supervised learning of neural networks under the assumption that the score function of the data distribution is known. They show that if the input dimension is large enough and the network is sparse enough, then the first network layer can be learned by a polynomial-time algorithm. More recently, Janzamin et al. (2015) propose another algorithm relying on the score function that removes the restrictions of Sedghi and Anandkumar (2014). The assumption in this case is that the network weights satisfy a non-degeneracy condition; however, the algorithm is only capable of learning neural networks with one hidden layer. Our algorithm does not impose any assumption on the data distribution, and is able to learn multilayer neural networks. Another approach to the problem is via the improper learning framework. The goal in this case is to find a predictor that is not a neural network, but performs as well as the best possible neural network in terms of the generalization error. Livni et al. (2014) propose a polynomial-time algorithm to learn networks whose activation function is quadratic. Zhang et al. (2015) propose an algorithm for improper learning of sigmoidal neural networks.", "startOffset": 0, "endOffset": 1736}, {"referenceID": 24, "context": "This lemma is based on a slight sharpening of the usual Ledoux-Talagrand contraction for Rademacher variables (Ledoux and Talagrand, 2013); see Appendix A for the proof.", "startOffset": 110, "endOffset": 138}, {"referenceID": 12, "context": "See the paper by Dasgupta and Gupta (1999) for a simple proof.", "startOffset": 17, "endOffset": 43}, {"referenceID": 3, "context": "Maurey-Barron-Jones lemma: Letting G be a subset of any Hilbert space H, the MaureyBarron-Jones lemma guarantees that any point v in the convex hull of G can be approximated by a convex combination of a small number of points of G. More precisely, we have: Lemma 3. Consider any subset G of any Hilbert space such that \u2016g\u2016H \u2264 b for all g \u2208 G. Then for any point v is in the convex hull of G, there is a point vs in the convex hull of s points of G such that \u2016v \u2212 vs\u2016H \u2264 b2/s. See the paper by Pisier (1980) for a proof.", "startOffset": 7, "endOffset": 507}, {"referenceID": 15, "context": "We note that Hush (1999) proved a similar hardness result, but without the unit-norm constraint on w\u2217 and {xi}i=1.", "startOffset": 13, "endOffset": 25}, {"referenceID": 4, "context": "This regularization scheme has been studied by Bartlett (1998); Koltchinskii and Panchenko (2002); Bartlett and Mendelson (2003); Neyshabur et al.", "startOffset": 47, "endOffset": 63}, {"referenceID": 4, "context": "This regularization scheme has been studied by Bartlett (1998); Koltchinskii and Panchenko (2002); Bartlett and Mendelson (2003); Neyshabur et al.", "startOffset": 47, "endOffset": 98}, {"referenceID": 4, "context": "This regularization scheme has been studied by Bartlett (1998); Koltchinskii and Panchenko (2002); Bartlett and Mendelson (2003); Neyshabur et al.", "startOffset": 47, "endOffset": 129}, {"referenceID": 4, "context": "This regularization scheme has been studied by Bartlett (1998); Koltchinskii and Panchenko (2002); Bartlett and Mendelson (2003); Neyshabur et al. (2015). Assuming a constant `1-norm bound might be restrictive for some applications, but without this norm constraint, the neural network class activated by any sigmoid-like or ReLU-like function is not efficiently learnable (Zhang et al.", "startOffset": 47, "endOffset": 154}, {"referenceID": 13, "context": "It uses the AdaBoost approach (Freund and Schapire, 1997) to construct the network, and we refer to it as the BoostNet algorithm.", "startOffset": 30, "endOffset": 57}, {"referenceID": 22, "context": "Our proof relies on the hardness of standard (nonagnostic) PAC learning of the intersection of halfspaces given in Klivans et al. (2006). More precisely, consider the family of halfspace indicator functions mapping X = {\u22121, 1}d to {\u22121, 1} given by H = {x\u2192 sign(wx\u2212 b\u2212 1/2) : x \u2208 {\u22121, 1}, b \u2208 N, w \u2208 N, |b|+ \u2016w\u20161 \u2264 poly(d)}.", "startOffset": 115, "endOffset": 137}, {"referenceID": 22, "context": "Klivans et al. (2006) show that if T = \u0398(d\u03c1) then HT is not efficiently learnable under a certain cryptographic assumption.", "startOffset": 0, "endOffset": 22}], "year": 2015, "abstractText": "We study non-convex empirical risk minimization for learning halfspaces and neural networks. For loss functions that are L-Lipschitz continuous, we present algorithms to learn halfspaces and multi-layer neural networks that achieve arbitrarily small excess risk > 0. The time complexity is polynomial in the input dimension d and the sample size n, but exponential in the quantity (L/ ) log(L/ ). These algorithms run multiple rounds of random initialization followed by arbitrary optimization steps. We further show that if the data is separable by some neural network with constant margin \u03b3 > 0, then there is a polynomial-time algorithm for learning a neural network that separates the training data with margin \u03a9(\u03b3). As a consequence, the algorithm achieves arbitrary generalization error > 0 with poly(d, 1/ ) sample and time complexity. We establish the same learnability result when the labels are randomly flipped with probability \u03b7 < 1/2.", "creator": "LaTeX with hyperref package"}}}