{"id": "1511.07953", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Exploring Correlation between Labels to improve Multi-Label Classification", "abstract": "This paper attempts multi-label classification by extending the idea of independent binary classification models for each output label, and exploring how the inherent correlation between output labels can be used to improve predictions. Logistic Regression, Naive Bayes, Random Forest, and SVM models were constructed, with SVM giving the best results: an improvement of 12.9\\% over binary models was achieved for hold out cross validation by augmenting with pairwise correlation probabilities of the labels.", "histories": [["v1", "Wed, 25 Nov 2015 05:21:53 GMT  (580kb,D)", "http://arxiv.org/abs/1511.07953v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.SI", "authors": ["amit garg", "jonathan noyola", "romil verma", "ashutosh saxena", "aditya jami"], "accepted": false, "id": "1511.07953"}, "pdf": {"name": "1511.07953.pdf", "metadata": {"source": "CRF", "title": "Exploring Correlation between Labels to improve Multi-Label Classification", "authors": ["Amit Garg", "Jonathan Noyola", "Romil Verma", "Ashutosh Saxena", "Aditya Jami"], "emails": [], "sections": [{"heading": null, "text": "Investigation of the correlation between labels to improve the multi-label classification Amit Garg, Jonathan Noyola, Romil Verma, Ashutosh Saxena, Aditya JamiStanford University"}, {"heading": "I. Abstract", "text": "This paper attempts multi-label classification by extending the idea of independent binary classification models for each label and examining how the inherent correlation between output labels can be used to improve predictions. Logistical regression, naive bayes, random forest and SVM models were constructed, with SVM providing the best results: a 12.9% improvement over binary models was achieved to sustain cross-validation by adding paired correlation probabilities of labels. II. The introduction of multi-label classification is a series of classification problems where the output vector has a variable length. The average number of labels per review varies between datasets and is generally a function of text semantics rather than syntax. The learning algorithm needs to estimate the number of labels and make the correct predictions. Previously, problems of multi-label adaptation problems were solved by using multi-label conversion problems with transverted form labels [1]."}, {"heading": "III. Task Definition", "text": "Faced with large datasets of Amazon and Twitter product ratings and manually labeled multi-label classifications for each (Ground Truth), the algorithm aims to predict all classifications for a rating [2]. Our goal is to make the algorithm portable across different datasets, i.e. to train a model for Amazon ratings and thus classify Twitter tweets."}, {"heading": "IV. Dataset", "text": "The Amazon dataset contains ratings for 400,000 products in 40,000 different categories. Among them, we focused on books and book subcategories. The Twitter dataset includes 100,000 tweets. Both datasets have the same scheme, and each review and each tweet has a unique ID, pruned content, and a tree of all product labels from review to book category at the root."}, {"heading": "V. Theory", "text": "Our approach extends the independent binary classification model. In addition to the individual probabilities, this algorithm also considers the probability of common occurrence of labels. Therefore, the inference algorithm models the formula -P (y (m) | x (m)) = female labels i (y (m) i (x (m)) i j, k P (y (m) j, y (m) k) \u03b1, where yi = {0, 1}, j (m) i | yi = 1}, 0 \u2264 \u03b1 \u2264 1P (y (m) i | x (m)) represents the probability of the independent binary model for labels i classified as 0or 1 given labels x."}, {"heading": "VI. Methodology", "text": "After parsing, the entire algorithm is divided into 3 steps - pre-processing, training independent classifiers and including the probability of applause to create the final label set. Hyperparameters must be adjusted for each data set. LIBLINEAR works well for larger data sets, while LIBSVM works well for custom and smaller data sets. LIBLINEAR only supports a linear kernel, but is very fast compared to LIBSVM and is therefore used in learning algorithms [4]."}, {"heading": "Parsing", "text": "The labels are structured in a hierarchy - for books there are 31 top-level labels and for each Xiv: 151 1.07 953v 1 [cs.L G] 25 Nov 201 52 labels there are several sub-labels. For example, a top-level label is \"Literature and Fiction\" and only a few associated sub-labels are \"Folklore,\" \"Mysteries\" and \"Classics.\" Reviews are labelled with these sub-level labels, which we assign to one of the 31 top-level labels and use for multi-label classifications."}, {"heading": "Preprocessing", "text": "When parsing a review, its content is truncated and separated, and then a tf-idf-based vector is created, which is used as input tag x. The labels corresponding to this review are used as output marks y.The correlation matrix is constructed by analyzing 100,000 book reviews and generating pairs of different labels - a 31 x 31 line normalized matrix with Laplace Smoothing. After normalization, the matrix is not symmetrical, so the geometric mean of the cells (i, j) and (j, i) is treated as the actual correlation probability for two labels. Fig. 1. draws the line covariance matrix for the Amazon dataset. The existence of large peaks is the motivation behind the algorithm."}, {"heading": "Algorithm", "text": "\u2022 The inputs to the learning algorithm are tf-idf-based input functions, their manually assigned labels and the previously generated correlation matrix \u2022 31 independent binary models are trained using L2 regulated SVM. This serves as a baseline. \u2022 For verification, the J labels with the highest probabilities are selected from the independent models. \u2022 Probabilities of paired combinations of these labels are calculated as P (a) P (b) P (a, b) \u03b1 \u2022 K (a, b) pairs with the highest possible probability product and different labels form the predicted quantity. The algorithm does not exactly calculate the theoretical model, but is an approximation. It is similar to beam search."}, {"heading": "Hyperparameters", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "VII. Error Metric", "text": "The general prediction error can be considered a combination of two different types of errors - false positives and false negatives. False positives are the labels contained in the predicted label set but not in the basic truth, and false negatives are the labels that are present in the basic truth but not in the predicted label setup. We do not weigh the two identically, but attach slightly higher importance to false negatives. The reason for this is that over-predictions can be further controlled using additional heuristics, but the labels that are missed can never be recovered without reviewing the entire label. Error = 1 \u2212 (total labels correctly predicted number of labels in the basic truth) \u00b7 (Total labels correctly predicted Total labels) \u03b3 is a hyperparameter that cannot be adjusted as it would set it to a value of 1, and this results in zero and J, i.e., all labels will output K independently."}, {"heading": "VIII. Results", "text": "Cross-validation with 70% / 30% graduation"}, {"heading": "1. Train on Amazon and Test on Amazon Dataset", "text": "Figure 2. Shows the performance of our algorithm with the \"31 independent models\" as the size of the data set is varied. As is obvious, the algorithm achieves an improvement of about 10% over the baseline just by looking at the correlation matrix after adjusting the hyperparameters. This confirms our assumption regarding the benefit of the correlation matrix for the multi-mark classification 3 with variable number of outputs."}, {"heading": "2. Train and Test on Twitter Dataset", "text": "Fig. 3 is the performance of the algorithm when the Twitter data set is used for both training and testing after the hyperparameters have been adjusted accordingly; the gain is only about 0.1%, due to the scarcity of content per Twitter review (Twitter's record size for the same number of reviews was about 12% of Amazon's). Verifying the output labels predicted by our algorithm showed that the overpredictions were the main cause of the error, as opposed to Amazon, where mispredictions were the main cause of the error."}, {"heading": "3. Train on Amazon and Test on Twitter Dataset", "text": "In this case, we train with 100% data from Amazon and test with 100% data from Twitter. Figure 4 shows the performance of our algorithm when the training was performed with Amazon's dataset and the tests were performed with Twitter's dataset. Hyperparameters used are the same as in Figure 2. The observed improvement is therefore much less since the reason is the same, i.e. the scarcity of content per tweet."}, {"heading": "10% K-fold Cross Validation", "text": "While our algorithm had observed a 10% improvement over the capture of independent binary models for hold-out cross-validation, this did not apply to the results of the K-fold cross-validation. Training and testing on Amazon showed no improvement, but a 2% deterioration in performance. Also, the results for Twitter data sets changed significantly, but because both algorithms experienced similar shifts, the actual improvements were still marginal. We attribute the poor performance of the correlation alignment frame to two causes: First, the number of labels per evaluation of the Amazon data set has an average of 3.06 and a standard deviation of 1.3. Since the standard deviation is relatively high, it can be expected that the average number of labels per check for a given partition can be quite different, resulting in the algorithm producing many false positives if it is based on some fixed deviations from that number, with a correlation much lower than the average."}, {"heading": "Comparing Learning Algorithms", "text": "We compare the baseline results of three different models: SVM, Naive Bayes and Random Forest. Naive Bayes did not scale well, but converged quickly. Additionally, Naive Bayes in Scikit Learn [5] provided binary probabilities that are not well suited for multi-label classification, where each check has a variable number of labels. However, SVM showed a noticeable trend and scaled best with increasing data sets. Subsequently, each model was extended with the correlation matrix. SVM provided the best results and was then optimized by adjusting hyperparameters."}, {"heading": "Other Approaches Considered", "text": "Another approach was the formation of 31C2 = 465 (per label pair) separate correlation models, which conditioned the correlation probability on the train data instead of being fixed for a dataset, which increased the error marginally and the computation time considerably. This method did not perform so well because in the new correlation models the error of matching an input vector with a LabelgotFIG mixed with the correlation error. 7: Basic error for SVM, Naive Bayes and Random ForestFIG. 8: Correlation algorithm error for SVM, Naive Bayes and Random Forestmixed with the correlation error. As a solution, the algorithm weighed both the correlation probability by \u03b1 and the independent probabilities by 1 \u2212 \u03b1. This further reduced the error count slightly."}, {"heading": "IX. Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Precision, Recall and F1 Score", "text": "Train and test on 3000 Twitter reviews, \u03b2 = 1 / 3 Train and test on 3000 Amazon, Twitter reviews The correlation algorithm weighs false positives and false negatives differently, so the F\u03b2 score a5 Predicted Correlated BaselineActual TP 1124 FN 658 TP 1124 FN 658 FP 433 TN 25716 FP 382 TN 25767TABLE VI: Confusion Matrix for Train Amazon, Test TwitterPrecision Recall F1 F\u03b2 Baseline 0.0869 0.3712 0.1409 0.2797 Correlation 0.1334 0.3299 0.1900 0.2876TABLE VII: Scores for Training Amazon, Test Twittermore accurate of the algorithm performance. F\u03b2 for the correlation algorithm test on Amazon is 30.19% better than the baseline test, 2.82% better than the baseline test, but 0.3% worse than the baseline test."}, {"heading": "Bias and Variance", "text": "Because the feature vectors were based on word occurrences in the text, they were significantly larger than the dataset size - the observed feature size for 2500 dataset size of Amazon reviews was 12,000. Due to computational limitations, a simple PCA check was not feasible.Sparse notation was used to represent feature vectors, from Python's Numpy library. Then Sparse SVD (Singular Value Decomposition) found the smallest subspace to which the feature matrix was assigned, with all singular values greater than 1. This significantly reduced the dataset size from Python's Numpy library, which was now reduced to a feature vector of size 2300. While the number of features was still comparable to the number of data points, it was significantly smaller than the size of the general vector."}, {"heading": "X. Conclusion", "text": "However, the results show that extending an independent model with a second-order correlation probability has only marginal improvements in performing a multi-label classification with a sufficiently large set of characteristics; the model has been modified from the baseline by adding a single characteristic: second-order correlation. This is the greedy method of selecting characteristics, as opposed to studying the correlation of all combinations of labels. In such a case, where we have not examined the correlation of higher order, we might have overlooked, for example, that the labels A, B, and C never occur together - information that could have proved crucial to representing the true data of the model."}, {"heading": "XI. Challenges & Future Work", "text": "The correlation matrix has been specifically constructed for each dataset, so that a generalized correlation framework is needed to apply the learning algorithms to cross-datasets. Currently, hyperparameter K is optimized to select the number of predicted labels using monitored techniques.The number of labels for each check could be increased by using unattended techniques such as k-mean clustering on label probabilities. The datasets have different inherent structures that cannot be generalized well when using a common learning algorithm. To the Twitter dataset, each feature vector is about 12% the size of a corresponding feature vector in the Amazon dataset. The features should be supplemented with options such as semantic relationships in the text to make classifications and user history contexts.The ratings have associated sublevel labels that should be rolled up to first-level labels, so that the classifications should be fixed."}, {"heading": "XII. Acknowledgments", "text": "Many thanks to Professor Ashutosh Saxena and Aditya Jami for advising the research team and providing the marked datasets of product reviews from Amazon and Twitter. [1] Tsoumakas, Grigorios; Katakis, Ioannis (2007). \"Multilabel classification: an overview.\" International Journal of Data Warehousing and Mining 3 (3): 1-13 doi: 104018 / jdwm.20070101 [2] cs.stanford.edu / people / adityaj / cs229 _ fall2014 _ dataset.html [3] miscsnapnets, author = Jure Leskovec and Andrej Krevl, title = SNAP Datasets: Stanford Large Network DatasetCollection, howpublished = http: / / snap.stanford.edu / data, month = jun, year = 2014 [4] D. Albanese, R. Visintainer, S. Merler, S. Ricadonna G. Pethelman, 20155."}], "references": [{"title": "Multilabel classification: an overview", "author": ["Grigorios Tsoumakas", "Ioannis Katakis"], "venue": "Journal of Data Warehousing and Mining", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Krevl, title = SNAP Datasets: Stanford Large Network Dataset  Collection, howpublished = http://snap.stanford.edu/ data, month = jun, year", "author": ["miscsnapnets", "author = Jure Leskovec", "Andrej"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "C", "author": ["D. Albanese", "R. Visintainer", "S. Merler", "S. Riccadonna", "G. Jurman"], "venue": "Furlanello. mlpy: Machine Learning Python", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Previously multi-label classification problems were solved using problem transformation techniques (converting the problem into binary classification problems per output label), or by adapting the algorithm to directly perform multi-label classification [1].", "startOffset": 254, "endOffset": 257}, {"referenceID": 2, "context": "LIBLINEAR only supports a linear Kernel but is very fast relative to LIBSVM and hence used in the learning algorithms [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "[1] Tsoumakas, Grigorios; Katakis, Ioannis (2007).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "html [3] miscsnapnets, author = Jure Leskovec and Andrej Krevl, title = SNAP Datasets: Stanford Large Network Dataset Collection, howpublished = http://snap.", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "edu/ data, month = jun, year = 2014 [4] D.", "startOffset": 36, "endOffset": 39}], "year": 2015, "abstractText": "I. Abstract This paper attempts multi-label classification by extending the idea of independent binary classification models for each output label, and exploring how the inherent correlation between output labels can be used to improve predictions. Logistic Regression, Naive Bayes, Random Forest, and SVM models were constructed, with SVM giving the best results: an improvement of 12.9% over binary models was achieved for hold out cross validation by augmenting with pairwise correlation probabilities of the labels.", "creator": "LaTeX with hyperref package"}}}