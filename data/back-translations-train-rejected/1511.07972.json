{"id": "1511.07972", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Learning with Memory Embeddings", "abstract": "Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. In recent publications the embedding models were extended to also consider temporal evolutions, temporal patterns and subsymbolic representations. These extended models were used successfully to predict clinical events like procedures, lab measurements, and diagnoses. In this paper, we attempt to map these embedding models, which were developed purely as solutions to technical problems, to various cognitive memory functions, in particular to semantic and concept memory, episodic memory and sensory memory. We also make an analogy between a predictive model, which uses entity representations derived in memory models, to working memory. Cognitive memory functions are typically classified as long-term or short-term memory, where long-term memory has the subcategories declarative memory and non-declarative memory and the short term memory has the subcategories sensory memory and working memory. There is evidence that these main cognitive categories are partially dissociated from one another in the brain, as expressed in their differential sensitivity to brain damage. However, there is also evidence indicating that the different memory functions are not mutually independent. A hypothesis that arises out off this work is that mutual information exchange can be achieved by sharing or coupling of distributed latent representations of entities across different memory functions.", "histories": [["v1", "Wed, 25 Nov 2015 07:06:09 GMT  (19kb)", "https://arxiv.org/abs/1511.07972v1", "7 pages"], ["v2", "Tue, 1 Dec 2015 05:53:38 GMT  (22kb)", "http://arxiv.org/abs/1511.07972v2", "7 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v3", "Wed, 16 Dec 2015 23:38:03 GMT  (376kb,D)", "http://arxiv.org/abs/1511.07972v3", "14 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v4", "Mon, 21 Dec 2015 22:35:39 GMT  (218kb,D)", "http://arxiv.org/abs/1511.07972v4", "14 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v5", "Mon, 25 Jan 2016 20:02:39 GMT  (285kb,D)", "http://arxiv.org/abs/1511.07972v5", "14 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v6", "Wed, 13 Apr 2016 17:23:42 GMT  (439kb,D)", "http://arxiv.org/abs/1511.07972v6", "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v7", "Thu, 21 Apr 2016 04:40:58 GMT  (293kb,D)", "http://arxiv.org/abs/1511.07972v7", "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v8", "Thu, 5 May 2016 14:57:41 GMT  (297kb,D)", "http://arxiv.org/abs/1511.07972v8", "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v9", "Sat, 7 May 2016 09:06:15 GMT  (291kb,D)", "http://arxiv.org/abs/1511.07972v9", "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["volker tresp", "crist\\'obal esteban", "yinchong yang", "stephan baier", "denis krompa{\\ss}"], "accepted": false, "id": "1511.07972"}, "pdf": {"name": "1511.07972.pdf", "metadata": {"source": "CRF", "title": "Learning with Memory Embeddings", "authors": ["Volker Tresp", "Crist\u00f3bal Esteban", "Yinchong Yang", "Stephan Baier"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this is an attempt to expand knowledge to include a tensory representation, the results of which are based on latent representations; the latent variability is well suited to deal with the high dimensionality and the low number of typical knowledge. In recent publications, the embedding of models has also been taken into account, both in terms of temporal development and in terms of subjective representation."}, {"heading": "2 Memories and Their Tensor Embeddings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Unique-Representation Hypothesis", "text": "In this section, we will discuss how the various memory functions can be encoded as tensors, and how conclusions and generalizations are stored by coupled tensor decomposition. We will begin with the consideration of declarative memories. The prime example of a declarative memory is semantic memory, which stores general knowledge of the world about entities. Second, there is conceptual memory, which stores information about the concepts in the world and their hierarchical organization. In contrast to the general setting in machine learning, in this paper entities are the main focus and concepts of secondary interest. Finally, episodic memory stores information about general and personal events [139, 140, 141, 54] whereas semantic memory concerns information that we \"know,\" episodic memory concerns information that we \"remember.\" [57] The part of episodic memory that concerns the life of an individual that includes personal experiences is called autographical memories, 149, 1454, 141."}, {"heading": "2.2 A Semantic Knowledge Graph Model", "text": "\"We assume that it is a generalization.\" \"We assume that it is a generalization.\" \"We assume that it is a generalization.\" \"We assume that it is a generalization.\" \"We assume that it is a generalization.\" \"We assume that it is a generalization.\" \"We assume that it is a generalization.\" \"We assume that it is a generalization.\" \"We assume that it is a generalization.\" \"We assume that it is a generalization.\" \"We assume that it is a generalization.\" \"We assume that it is a generalization.\" \"We assume that it is a generalization.\""}, {"heading": "2.3 An Event Model for Episodic Memory", "text": "While a semantic KG model reflects the state of the world, for example, of a clinic and its patients, observations and actions describe factual knowledge of discrete events, which in our approach are represented by an episodic event tensor. In a clinical setting, events could be a prescription for a cholesterol-lowering drug, the decision to measure cholesterol levels and the result of cholesterol levels; thus, events can be e.g. actions, decisions, and metrics. The episodic event tensor is a four-way tensor Z, where the tensor element zs, p, o, t is the associated value of the quadrangle (es, eo, et). The indicator mapping function is then an episode, p, o, f episodicpaes, aep, aeo, aetqwhere we have added a representation for the time of an event."}, {"heading": "2.4 Autobiographical Event Tensor", "text": "The autobiographical event tensor is simply the subtensor Z, which concerns only the events of the individual. We then receive a personal time it \"i, t with latent representation aes\" i. While aet is a latent representation for all events for all patients at the time t, aes \"i, t, a latent representation for all events for patients is i at the time t [48, 49]. The autobiographical event tensor would correspond to autobiographical memory, which stores autobiographical events of an individual on a semantic abstraction level [33, 54]. The autobiographical event tensor can be associated with Baddeley's episodic buffer and is a part and is regarded as part of a temporary memory, contrary to Tulving's concept of episodic memory [33, 54]."}, {"heading": "2.5 A Sensory Buffer", "text": "We assume that the sensor input consists of Q channels and that at each step t a buffer is formed from N samples of the Q channels. \u03b3 \"0,., N specifies the time position within the buffer (see also Figure 2). In contrast to the event buffer, the sensory buffer operates at a subsymbolic level. Technically, it could represent measurements such as temperature and pressure, and in a cognitive model, it could represent input channels from the senses. The sensory buffer could be related to the mini-batches in Spark Streaming, in which data is captured in buffers holding seconds to minutes of the input currents [149]. The sensory buffer is described by a three-way tensor U in which the tensor element uq, ig, t is the corresponding value of the triple (eq, e\u03b3, et)."}, {"heading": "2.6 Comment", "text": "The various memories and their tensor representations and models are summarized in Figure 2. Under the Unique Representation Hypothesis adopted in this paper, the latent representations of generalized entities are central to query and predict: Memory does not have to store all facts and relationships about an entity. Furthermore, there is no need to explicitly store the semantic graph. At any time, an approximation to the graph can be reconstructed from the latent representations. See also the discussion in Section 7."}, {"heading": "2.7 Cost Functions", "text": "Each memory function generates a term in the cost function (see appendix), and all terms can be taken into account in the training to adjust all latent representations and all parameters in the various functional mappings. Note that this is a global optimization step that includes all available data. 3 In general, we assume a unique representation for a unit, for example, we assume that aes is the same in the predictive model and in the semantic model. Sometimes, it makes sense to relax this assumption and assume only some form of coupling. Technically, there are a number of possibilities: for example, the predictive model could be trained to its own cost function by using the latent representations from the knowledge diagram as initialization; alternatively, different weights can be used for the different terms of the cost function. Some researchers suggest that only some dimensions of the latent representations should be shared [3, 1]. 4 [89, 19, 17] contain extensive discussions about transferring predilatations."}, {"heading": "3 Modelling the Indicator Mapping Function", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Using General Function Approximators", "text": "Consider the semantic Kg. Here, the indicator mapping function f semanticp \u00a8 q can be modeled as a general function approximator, as in Figure 4. With this model, it would be easy to query the plausibility of a triple pes, ep, eo; Valueq, but other queries would be more difficult to handle. An alternative model is represented at the bottom of Figure 4 with inputs it and ep, and where a function approximator predicts a latent representation vector J.pject with component shobjectr \"f semantic, objecktr paes, aepq r\" 1,., r \"semantic\" approximator."}, {"heading": "3.2 Tensor Decompositions", "text": "Tensor decompositions have also proven themselves in the modelling of KGs [106]."}, {"heading": "4 Querying Memories", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Function Approximator Models", "text": "In many applications, one is interested in retrieving triples with a high probability, due to some information, so we are essentially facing an optimization problem. To answer a query of the form pJack, likes,? q, we must max aeof semanticpJack, likes, aeoq. Of course, one is often interested in a number of probable answers. We propose to approach the query using a simulated annealing approach. We define an energy function Eps, p, oq \"f semanticpaes, aep, aeoq, and define a Boltzmann distribution asP, p, oq\" 1 Zp\u03b2q exp\u00df f semanticpaes, aep, aeoq. Here, Zp\u03b2q is the partition function that normalizes the distribution, and \u03b2 \u0430 0 is an inverse temperature. Note that we have now created a probability distribution where subject, predicate, and object are the random variables."}, {"heading": "4.2 Tensor Models", "text": "By forcing the nonnegativity of the factors and the core tensor entries, we can create a probability model for a Tucker model with Eps, p, oq \"log f semanticpaes, aep, aeoq asP, p, oq9\" 1r \"1\" r3 \"1 aes, r1 aep, r2 aeo, r3 gpr1, r2, r3q \u03b2.\" (4) An attractive feature of tensor models is that marginals and conditionals are easy to obtain. Here, we look at the Tucker model. For P po | s, pq, we can use Equation 4 with appropriate normalization. For P pp | sqwe use the same equation where we replace aeo with an \"object\" and conditionals. For P psq, we use the same equation where we replace P numbers with an answer."}, {"heading": "5 From Sensory Memory to Semantic Decoding", "text": "We will now consider the situation that a new sensor input is available for time. With all other latent representations and functional mappings, the challenge of calculating a new latent representation htimet is solved. Since for a new sensory input currently t the only information available is the sensory buffer u:,: t there is a clear information propagation of sensory input to the episodic memory. We assume that a nonlinear map of the formhtimet \"fM pvecpu::, tqq (5) where fM p \u00a8 q is a function that needs to be learned [147] and where vecpu:, tq are vectorized representations from the part of the sensory tensors that are associated with the individual time t. Depending on the application, the fM p \u00a8 q may be a simple linear map, or it may be a second to last level in a deep neural network."}, {"heading": "6 Predictions with Memory Embeddings and Working Memory", "text": "In this section, we will focus on working memory, which orchestrates the various memory functions, such as prediction and decision-making. In a way, working memory represents intelligence above memory functions, and links have been made to complex decision-making and consciousness. Here, we focus on the limited but important task of predicting. For example, to ensure non-negativity, we could model htimet \"exp fM pvecpu:,:, tqq. 9This equation describes a special form of conditional random field, and with proper local normalization, it becomes a conditional multinomial probability mixing model."}, {"heading": "6.1 ARX Model for Predicting Latent Representations of Time", "text": "Here we assume that it is a deterministic function of the sensory input via Eq.5, but not of the latent representations of past times. There may be time dependencies in the sensory input; due to the high dimensionality of the input, it is easier to model the dependencies between the latent representations, since h-timet \"f predipaet '1, aet' 2,..., aet 'W, aeindiviualq. But note that this model is only used for predicting htimet, and once the sensory input is available, it overrides the prediction with Eq.5! The model is also suitable for novelty detection: if h-timet differs from htimet, then the sensory scene could be new. Note that we also include the latent representation of the individual aeinudivisional input, which can be interpreted as a representation of the state of individuality. The model can be used as an authoritative model based on the latency of the external representation of time, the ART and the size of the representation."}, {"heading": "6.2 Recurrent Model", "text": "Here we extend the model of Equation 7 with earlier information about the latent representation ashtimet \"fRNN pvecpu:,:, tq, aet '1, aeindiviualq. (7) Note that this is the structure of a recurrent neural network and the assumption is that the latent state depends on both the sensory input and the previous latent state. The architecture is shown in Figure 9 below. Both models make sense for different purposes and assume different assumptions. In fact, both models could play a role in human cognition. Alternatively, networks with additional memory buffers and attention mechanisms could be used [71, 143, 60, 86, 58]."}, {"heading": "7 Hypotheses on Human Memory", "text": "This section deals with the relevance of the presented models for the functions of human memory. In particular, we present several concrete hypotheses. Figure 10 shows the overall model and explains the flow of sensory inputs into long-term memory and semantic decoding."}, {"heading": "7.1 Triple Hypothesis", "text": "A main assumption, of course, is that semantic memory is described by triples, and episodic memory is described by triples in time, that is, by quadruples. In a way, this is the perspective from which this work was written. Arguments for this narrative are that relationships of higher order can always be reduced to triples, and that triples have great practical significance and have been used in large KGs."}, {"heading": "7.2 Unique-representation Hypothesis for Entities and Predicates", "text": "Universal theory holds that each generalized entity is represented by an index neuron and a unique (rather high-dimensional) latent representation stored as weight patterns that connect the index neurons to neurons in the representation layer. Although the weight vectors are very sparse and not negative in some models, they are the basis for episodic memory and semantic memory. Latent representations integrate everything known about a generalized entity and can be instrumented for predicting and assisting decision-making in working memory. Among other advantages, a common representation would explain why background information about an entity appears to be effortlessly integrated into human understanding of the sense scene and decision-making support, at least for entities familiar to the individual."}, {"heading": "7.3 Representation of Concepts", "text": "In contrast, machine learning is typically about assigning entities to concepts. Concepts entail a certain order: one can, for example, imply certain characteristics by knowing that Cloe is a cat. Concept learning is not the main focus of this paper and we only want to describe a simple insight. Consider that we simply treat a concept as another entity with its own latent representation, as in [105]. We can introduce the type of relationship that links entities to their concepts. Inductive conclusions during model learning can then materialize that Cloe is also a mammal and a living being, and that it has typical cat characteristics by default."}, {"heading": "7.4 Spatial Representations", "text": "An example would be pMary, observedIn, TownHall, LastFridayq. To model that the individual was at the town hall last Friday, a triple would suffice, such as pmeLocation, TownHall, LastFridayq, and the spatial decoding of an individual could be done by a special circuit separated from the semantic decoding."}, {"heading": "7.5 Sensory Input is Transformed into a Latent Representation for Time", "text": "In our model, we assume that every sensory impression is htimet \"aet byM -map fM \u00a8 q decoded into a temporally latent representation, which could actually be implemented as a series of modules responsible for various aspects of sensory input.10 Training fM p \u00a8 q to refine its functioning would correspond to cognitive learning. In the brain, fM p \u00a8 q would probably be implemented through the various sensory pathways, such as the visual and auditory pathways, and could contain internal feedback loops."}, {"heading": "7.6 New Representations are formed in the Hippocampus and are then Transferred to Long-Term Episodic and Semantic Memories", "text": "In fact, it is the case that you will be able to put yourself in a situation in which you see yourself, in which you are able to assert yourself, and in which you are in a situation in which you are in."}, {"heading": "7.7 Tensor Memory Hypothesis", "text": "The hypothesis is that semantic memory and episodic memory are implemented as functions applied to the latent representations involved in generalized units, which include entities, predicates, and time. Therefore, neither the knowledge graph nor the tensors ever need to be explicitly stored! Due to the similarity with the decomposition of tensors, we call this the tensor memory hypothesis."}, {"heading": "7.8 The Semantic Decoding Hypothesis and Association", "text": "It is therefore possible that the way in which we have discussed this generational process in sections 5 and 4 suggests that the decoding by the generation of ps, p, oq, oq, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o"}, {"heading": "7.9 Semantic Memory and Episodic Memory", "text": "In fact, it is such that we are in a position to put ourselves in a situation in which we are in a position to survive ourselves, in which we are in a position in which we are in, in which we are in, in which we are in a situation in which we are in a situation in which we are in, in which we are in a situation in which we are in, in which we are in a situation in which we are in, in which we are in, in which we are in, in which we are in, in which we are in which we are in, in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which are in which we are in which we are in which we are in which we are in which are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which are in which we are in which we are in which we are in which are in which we are in which we are in which we are in"}, {"heading": "7.10 Online Learning and the Semantic-Attractor Learning Hypothesis", "text": "An interesting feature of the proposed model is that no learning or adjustment is necessary in operation as long as sensory information can be described by the already known entities and predicates. However, the only structural adjustment that is happening online is the formation of the index neuron and its representation pattern.13 One can also easily consider a mechanism for introducing new index neurons with new latent representations for entities and predictors that are not yet stored in memory. If deciphering is not successful, for example if the decrypted triples have a low probability, one could consider a mechanism for introducing new index neurons with new latent representations for entities and predictors that are not stored in memory. Therefore, the available resources (entities and predicates) are insufficient to explain the sensory data to introduce new index neurons for entities and predicates."}, {"heading": "7.11 Working Memory Exploits the Memory Representations for Tasks like Prediction and Decision Making", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "8 Conclusions and Discussion", "text": "An important assumption is that a knowledge diagram does not need to be explicitly stored, but only latent representations of generalized units from which the knowledge diagram can be reconstructed and inductive conclusions drawn (tensor memory hypothesis).In contrast to the knowledge diagram, in which a unit is represented by a single node in a diagram and its associations, a unit has a distributed representation in the form of a latent vector, i.e. in the form of several latent components. Clear representations lead to a global propagation of information in all memory functions during learning [104].We suggested that the latent representation is a time that summarizes all sensory information available at the time."}, {"heading": "9 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Cost Functions", "text": "The tilde notation X represents subsets corresponding to the facts known in the training. If only positive facts with the value \"True\" are known, negative facts can be generated e.g. using local closed world assumptions [106]. We use negative log likelihood cost terms. For a Bernoulli probability \"logP px | \u03b8q\" logr1 'exptp1' 2xq\u03b8us (cross entropy) and for a Gaussian probability \"logP px | \u03b8q\" const '12\u03c32 px'\u03b8q 2."}, {"heading": "9.1.1 Semantic KG Model", "text": "The cost concept for the semantic Kg model is inexpensive \"'xs, p, oPX, logP pxs, p, o | \u03b8semantics, p, o pA, W qqwhere A stands for the latent representations and W for the parameters in the functional mapping. 9.1.2 Episodic event model\"' costepisodic \"'zs, p, o, tPZ, logP pzs, p, o, t | \u03b8episodics, p, o, t pA, W qq9.1.3 Sensory buffer cost sensory\" \"\u00ffuq, \u03b3, tPU, logP puq, \u03b3, t | \u03b8sensoryq, \u03b3, t pA, W qq"}, {"heading": "9.1.4 Future-Prediction Model", "text": "The cost function of the ARX prediction model is the cost forecast \"'tlogP paet | f predictpaet' 1, aet '2,..., aet' W, aeindiviual, A, W q"}, {"heading": "9.1.5 Regularizer", "text": "To regulate the solution, we add \u03bbA} A} 2F'\u03bbW} W} 2F, where \"F\" is the frobenious norm and where \"A\" 0 and \"W\" 0 are regularization parameters. If we use M mappings, we regulate M instead of A and we include \"M\" 2F."}, {"heading": "9.2 Sampling using Function Approximators", "text": "Figure 11 shows how samples for the semantic kg can be generated using function approximations (e.g. an NN), and Figure 12 shows the semantic decoding. C: D: B:"}], "references": [{"title": "Data fusion in metabolomics using coupled matrix and tensor factorizations", "author": ["Evrim Acar", "Rasmus Bro", "Age K Smilde"], "venue": "Proceedings of the IEEE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Mixed Membership Stochastic Blockmodels", "author": ["Edoardo M. Airoldi", "David M. Blei", "Stephen E. Fienberg", "Eric P. Xing"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Generalized singular value decomposition for comparative analysis of genome-scale expression data sets of two different organisms", "author": ["Orly Alter", "Patrick O Brown", "David Botstein"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "The architecture of cognition", "author": ["John R Anderson"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1983}, {"title": "ACT-R: A theory of higher level cognition and its relation to visual attention", "author": ["John R Anderson", "Michael Matessa", "Christian Lebiere"], "venue": "Human-Computer Interaction,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Human memory: A proposed system and its control processes", "author": ["Richard C Atkinson", "Richard M Shiffrin"], "venue": "The psychology of learning and motivation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1968}, {"title": "DBpedia: A Nucleus for a Web of Open Data. In The Semantic Web, Lecture Notes in Computer Science", "author": ["S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Cognitive psychology and human memory", "author": ["Alan Baddeley"], "venue": "Trends in neurosciences,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1988}, {"title": "The episodic buffer: a new component of working memory", "author": ["Alan Baddeley"], "venue": "Trends in cognitive sciences,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Working memory: theories, models, and controversies", "author": ["Alan Baddeley"], "venue": "Annual review of psychology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Simulation, situated conceptualization, and prediction", "author": ["Lawrence W Barsalou"], "venue": "Philosophical Transactions of the Royal Society of London B: Biological Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Remembering: A study in experimental and social psychology, volume 14", "author": ["Frederic C Bartlett"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Novelty or surprise", "author": ["Andrew Barto", "Marco Mirolli", "Gianluca Baldassarre"], "venue": "Frontiers in psychology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "A computational principle for hippocampal learning and neurogenesis. Hippocampus", "author": ["Suzanna Becker"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Yoshua Bengio"], "venue": "Unsupervised and Transfer Learning Challenges in Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pierre Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In AAAI\u201911,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Translating Embeddings for Modeling Multi-relational Data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["Herv\u00e9 Bourlard", "Yves Kamp"], "venue": "Biological cybernetics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1988}, {"title": "Computational models of working memory: putting longterm memory into context", "author": ["Neil Burgess", "Graham Hitch"], "venue": "Trends in cognitive sciences,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "The amygdala and emotional memory", "author": ["Larry Cahill", "Ralf Babinsky", "Hans J Markowitsch", "James L McGaugh"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1995}, {"title": "Toward an architecture for never-ending language", "author": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R Hruschka Jr.", "Tom M Mitchell"], "venue": "learning. AAAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Neural network models for pattern recognition and associative memory", "author": ["Gail A Carpenter"], "venue": "Neural networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1989}, {"title": "Whatever next? predictive brains, situated agents, and the future of cognitive science", "author": ["Andy Clark"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "A spreading-activation theory of semantic processing", "author": ["Allan M Collins", "Elizabeth F Loftus"], "venue": "Psychological review,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1975}, {"title": "Iconic memory and visible persistence", "author": ["Max Coltheart"], "venue": "Perception & psychophysics,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1980}, {"title": "The construction of autobiographical memories in the self-memory system", "author": ["Martin A Conway", "Christopher W Pleydell-Pearce"], "venue": "Psychological review,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2000}, {"title": "Attention and memory", "author": ["Nelson Cowan"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1997}, {"title": "What are the differences between long-term, short-term, and working memory", "author": ["Nelson Cowan"], "venue": "Progress in brain research,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "The helmholtz machine", "author": ["Peter Dayan", "Geoffrey E Hinton", "Radford M Neal", "Richard S Zemel"], "venue": "Neural computation,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1995}, {"title": "Learning and selective attention", "author": ["Peter Dayan", "Sham Kakade", "P Read Montague"], "venue": "nature neuroscience,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2000}, {"title": "Indexing by latent semantic analysis", "author": ["Scott C. Deerwester", "Susan T. Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman"], "venue": "JASIS, 41(6):391\u2013407,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1990}, {"title": "Imaging recollection and familiarity in the medial temporal lobe: a three-component model", "author": ["Rachel A Diana", "Andrew P Yonelinas", "Charan Ranganath"], "venue": "Trends in cognitive sciences,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion", "author": ["Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Neurocomputational models of working memory", "author": ["Daniel Durstewitz", "Jeremy K Seamans", "Terrence J Sejnowski"], "venue": "Nature neuroscience,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2000}, {"title": "Bringing the grandmother back into the picture: A memory-based view of object recognition", "author": ["Shimon Edelman", "Tomaso Poggio"], "venue": "International journal of pattern recognition and artificial intelligence,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1992}, {"title": "Time cells in the hippocampus: a new dimension for mapping memories", "author": ["Howard Eichenbaum"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "The medial temporal lobe and recognition memory", "author": ["Howard Eichenbaum", "AR Yonelinas", "Charan Ranganath"], "venue": "Annual review of neuroscience,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2007}, {"title": "Towards a functional organization of episodic memory in the medial temporal lobe", "author": ["Howard Eichenbaum", "Magdalena Sauvage", "Norbert Fortin", "Robert Komorowski", "Paul Lipton"], "venue": "Neuroscience & Biobehavioral Reviews,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}, {"title": "Long-term working memory", "author": ["K Anders Ericsson", "Walter Kintsch"], "venue": "Psychological review,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1995}, {"title": "Predicting sequences of clinical events by using a personalized temporal latent embedding model", "author": ["Crist\u00f3bal Esteban", "Danilo Schmidt", "Denis Krompa\u00df", "Volker Tresp"], "venue": "In Proceedings of the IEEE International Conference on Healthcare Informatics,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Predicting the co-evolution of event and knowledge graphs", "author": ["Crist\u00f3bal Esteban", "Volker Tresp", "Yinchong Yang", "Stephan Baier", "Denis Krompa\u00df"], "venue": "arXiv preprint,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2015}, {"title": "Interactions between frontal cortex and basal ganglia in working memory: a computational model. Cognitive, Affective", "author": ["Michael J Frank", "Bryan Loughry", "Randall C OReilly"], "venue": "Behavioral Neuroscience,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2001}, {"title": "The organization of recent and remote memories", "author": ["Paul W Frankland", "Bruno Bontempi"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2005}, {"title": "\u03b1camkii-dependent plasticity in the cortex is required for permanent", "author": ["Paul W Frankland", "Cara O\u2019Brien", "Masuo Ohno", "Alfredo Kirkwood", "Alcino J Silva"], "venue": "memory. Nature,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2001}, {"title": "The free-energy principle: a unified brain theory", "author": ["Karl Friston"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2010}, {"title": "Cognitive Neuroscience: The biology of the mind", "author": ["Michael S Gazzaniga", "Richard B Ivry", "George Ronald Mangun"], "venue": "New York: WW Norton, fourth edition edition,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2013}, {"title": "Towards a mathematical theory of cortical micro-circuits", "author": ["Dileep George", "Jeff Hawkins"], "venue": "PLoS Comput Biol,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "Computational models of the hippocampal region: linking incremental learning and episodic memory", "author": ["Mark A Gluck", "Martijn Meeter", "Catherine E Myers"], "venue": "Trends in cognitive sciences,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2003}, {"title": "Learning and memory: From brain to behavior", "author": ["Mark A Gluck", "Eduardo Mercado", "Catherine E Myers"], "venue": "Palgrave Macmillan,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2013}, {"title": "Deep learning. Book in preparation for", "author": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2015}, {"title": "Neurogenesis in the neocortex of adult", "author": ["Elizabeth Gould", "Alison J Reeves", "Michael SA Graziano", "Charles G Gross"], "venue": "primates. Science,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1999}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2014}, {"title": "Interdependence of episodic and semantic memory: evidence from neuropsychology", "author": ["Daniel L Greenberg", "Mieke Verfaellie"], "venue": "Journal of the International Neuropsychological society,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2010}, {"title": "Google and the mind predicting fluency with pagerank", "author": ["Thomas L Griffiths", "Mark Steyvers", "Alana Firl"], "venue": "Psychological Science,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2007}, {"title": "Topics in semantic representation", "author": ["Thomas L Griffiths", "Mark Steyvers", "Joshua B Tenenbaum"], "venue": "Psychological review,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2007}, {"title": "Bayesian models of cognition. In The Cambridge Handbook of Computational Psychology", "author": ["Thomas L Griffiths", "Charles Kemp", "Joshua B Tenenbaum"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2008}, {"title": "Deconstructing episodic memory with construction", "author": ["Demis Hassabis", "Eleanor A Maguire"], "venue": "Trends in cognitive sciences,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2007}, {"title": "A practical guide to training restricted boltzmann machines. Momentum", "author": ["Geoffrey Hinton"], "venue": null, "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2010}, {"title": "Implementing semantic networks in parallel hardware. In Parallel models of associative memory, pages 161\u2013187", "author": ["Geoffrey E Hinton"], "venue": null, "citeRegEx": "67", "shortCiteRegEx": "67", "year": 1981}, {"title": "Parallel Models of Associative Memory: Updated Edition", "author": ["Geoffrey E Hinton", "James A Anderson"], "venue": null, "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2006}, {"title": "Autoencoders, minimum description length, and helmholtz free energy", "author": ["Geoffrey E Hinton", "Richard S Zemel"], "venue": "Advances in neural information processing systems,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1994}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 1997}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 1999}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["John J Hopfield"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 1982}, {"title": "Natural speech reveals the semantic maps that tile human cerebral cortex", "author": ["Alexander G. Huth", "Wendy A. de Heer", "Thomas L. Griffiths", "Fr\u00e9d\u00e9ric E. Theunissen", "Jack L. Gallant"], "venue": null, "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2016}, {"title": "Bayesian surprise attracts human attention", "author": ["Laurent Itti", "Pierre F Baldi"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2005}, {"title": "The mind and brain of short-term memory", "author": ["John Jonides", "Richard L Lewis", "Derek Evan Nee", "Cindy A Lustig", "Marc G Berman", "Katherine Sledge Moore"], "venue": "Annual review of psychology,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2008}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2015}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["Charles Kemp", "Joshua B. Tenenbaum", "Thomas L. Griffiths", "Takeshi Yamada", "Naonori Ueda"], "venue": "In Proceedings of the Twenty-First National Conference on Artificial Intelligence,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2006}, {"title": "Entorhinal\u2013 hippocampal neuronal circuits bridge temporally discontiguous events", "author": ["Takashi Kitamura", "Christopher J Macdonald", "Susumu Tonegawa"], "venue": "Learning & memory (Cold Spring Harbor, NY),", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2015}, {"title": "Entorhinal cortical ocean cells encode specific contexts and drive context-specific fear", "author": ["Takashi Kitamura", "Chen Sun", "Jared Martin", "Lacey J Kitch", "Mark J Schnitzer", "Susumu Tonegawa"], "venue": "memory. Neuron,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2015}, {"title": "The bayesian brain: the role of uncertainty in neural coding and computation", "author": ["David C Knill", "Alexandre Pouget"], "venue": "Trends in Neurosciences,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2004}, {"title": "Self-organization and associative memory, volume", "author": ["Teuvo Kohonen"], "venue": null, "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2012}, {"title": "Bayesian integration in force estimation", "author": ["Konrad P K\u00f6rding", "Shih-pi Ku", "Daniel M Wolpert"], "venue": "Journal of Neurophysiology,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2004}, {"title": "Probabilistic Latent- Factor Database Models", "author": ["Denis Krompa\u00df", "Xueyan Jiang", "Maximilian Nickel", "Volker Tresp"], "venue": "In Proceedings of the 1st Workshop on Linked Data for Knowledge Discovery (ECML PKDD),", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2014}, {"title": "Type-constrained representation learning in knowledge graphs. In The Semantic Web\u2013ISWC 2015, pages 640\u2013655", "author": ["Denis Krompa\u00df", "Stephan Baier", "Volker Tresp"], "venue": null, "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2015}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T Dumais"], "venue": "Psychological review,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 1997}, {"title": "An introduction to latent semantic analysis", "author": ["Thomas K Landauer", "Peter W Foltz", "Darrell Laham"], "venue": "Discourse processes,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 1998}, {"title": "A specific role of the human hippocampus in recall of temporal sequences", "author": ["Hanne Lehn", "Hill-Aina Steffenach", "Niels M van Strien", "Dick J Veltman", "Menno P Witter", "Asta K H\u00e5berg"], "venue": "The Journal of Neuroscience,", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2009}, {"title": "The myth of repressed memory: False memories and allegations of sexual abuse", "author": ["Elizabeth Loftus", "Katherine Ketcham"], "venue": null, "citeRegEx": "92", "shortCiteRegEx": "92", "year": 1996}, {"title": "Semantic and associative priming in highdimensional semantic space", "author": ["Kevin Lund", "Curt Burgess", "Ruth Ann Atchley"], "venue": "Proceedings of the 17th annual conference of the Cognitive Science Society,", "citeRegEx": "93", "shortCiteRegEx": "93", "year": 1995}, {"title": "On the computational power of winner-take-all", "author": ["Wolfgang Maass"], "venue": "Neural computation,", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2000}, {"title": "Simple memory: A theory for archicortex", "author": ["D Marr"], "venue": "Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 1971}, {"title": "Distributed memory and the representation of general and specific information", "author": ["James L McClelland", "David E Rumelhart"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 1985}, {"title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory", "author": ["James L McClelland", "Bruce L McNaughton", "Randall C O\u2019Reilly"], "venue": "Psychological review,", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 1995}, {"title": "Interactions between episodic and semantic memory", "author": ["Neal W Morton"], "venue": "Technical report, Vanderbilt Computational Memory Lab,", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2013}, {"title": "Place cells, grid cells, and memory", "author": ["May-Britt Moser", "David C Rowland", "Edvard I Moser"], "venue": "Cold Spring Harbor perspectives in biology,", "citeRegEx": "99", "shortCiteRegEx": "99", "year": 2015}, {"title": "Neural net architectures for temporal sequence processing", "author": ["Michael C Mozer"], "venue": "Santa Fe Institute Studies in the Sciences of Complexity,", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 1993}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2011}, {"title": "Tensor factorization for relational learning", "author": ["Maximilian Nickel"], "venue": "PhD thesis, Ludwig Maximilian University of Munich,", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2013}, {"title": "Learning Taxonomies from Multi-Relational Data via Hierarchical Link-Based Clustering", "author": ["Maximilian Nickel", "Volker Tresp"], "venue": "In Learning Semantics. Workshop at NIPS\u201911,", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2011}, {"title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2011}, {"title": "Factorizing YAGO: scalable machine learning for linked data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 21st International Conference on World Wide Web, WWW", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2012}, {"title": "A review of relational machine learning for knowledge graphs: From multi-relational link prediction to automated knowledge graph construction", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": "Proceedings of the IEEE,", "citeRegEx": "106", "shortCiteRegEx": "106", "year": 2015}, {"title": "Holographic embeddings of knowledge graphs", "author": ["Maximilian Nickel", "Lorenzo Rosasco", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1510.04935,", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2015}, {"title": "Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia", "author": ["Randall C O\u2019Reilly", "Michael J Frank"], "venue": "Neural computation,", "citeRegEx": "108", "shortCiteRegEx": "108", "year": 2006}, {"title": "11 a biologically based computational model of working memory. Models of working memory: Mechanisms of active maintenance and executive control, page", "author": ["Randall C O\u2019Reilly", "Todd S Braver", "Jonathan D Cohen"], "venue": null, "citeRegEx": "109", "shortCiteRegEx": "109", "year": 1999}, {"title": "Learning distributed representations of concepts using linear relational embedding", "author": ["Alberto Paccanaro", "Geoffrey E Hinton"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "110", "shortCiteRegEx": "110", "year": 2001}, {"title": "Sum-product networks: A new deep architecture", "author": ["Hoifung Poon", "Pedro Domingos"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "111", "shortCiteRegEx": "111", "year": 2011}, {"title": "Invariant visual representation by single neurons in the human brain", "author": ["R Quian Quiroga", "Leila Reddy", "Gabriel Kreiman", "Christof Koch", "Itzhak Fried"], "venue": null, "citeRegEx": "112", "shortCiteRegEx": "112", "year": 2005}, {"title": "Concept cells: the building blocks of declarative memory functions", "author": ["Rodrigo Quian Quiroga"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "113", "shortCiteRegEx": "113", "year": 2012}, {"title": "SAM: A theory of probabilistic search of associative memory. The psychology of learning and motivation", "author": ["Jeroen GW Raaijmakers", "Richard M Shiffrin"], "venue": "Advances in research and theory,", "citeRegEx": "114", "shortCiteRegEx": "114", "year": 1981}, {"title": "Binding items and contexts the cognitive neuroscience of episodic memory", "author": ["Charan Ranganath"], "venue": "Current Directions in Psychological Science,", "citeRegEx": "115", "shortCiteRegEx": "115", "year": 2010}, {"title": "Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects", "author": ["Rajesh PN Rao", "Dana H Ballard"], "venue": "Nature neuroscience,", "citeRegEx": "116", "shortCiteRegEx": "116", "year": 1999}, {"title": "Creating false memories: Remembering words not presented in lists", "author": ["Henry L Roediger", "Kathleen B McDermott"], "venue": "Journal of experimental psychology: Learning, Memory, and Cognition,", "citeRegEx": "117", "shortCiteRegEx": "117", "year": 1995}, {"title": "A computational theory of episodic memory formation in the hippocampus", "author": ["Edmund T Rolls"], "venue": "Behavioural brain research,", "citeRegEx": "118", "shortCiteRegEx": "118", "year": 2010}, {"title": "The noisy brain. Stochastic dynamics as a principle of brain function.(Oxford", "author": ["ET Rolls", "G Deco"], "venue": null, "citeRegEx": "119", "shortCiteRegEx": "119", "year": 2010}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "arXiv preprint arXiv:1507.01127,", "citeRegEx": "120", "shortCiteRegEx": "120", "year": 2015}, {"title": "Implicit memory: History and current status", "author": ["Daniel L Schacter"], "venue": "Journal of experimental psychology: learning, memory, and cognition,", "citeRegEx": "121", "shortCiteRegEx": "121", "year": 1987}, {"title": "The future of memory: remembering", "author": ["Daniel L Schacter", "Donna Rose Addis", "Demis Hassabis", "Victoria C Martin", "R Nathan Spreng", "Karl K Szpunar"], "venue": "imagining, and the brain. Neuron,", "citeRegEx": "122", "shortCiteRegEx": "122", "year": 2012}, {"title": "Driven by compression progress: A simple principle explains essential aspects of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes", "author": ["J\u00fcrgen Schmidhuber"], "venue": "In Anticipatory Behavior in Adaptive Learning Systems,", "citeRegEx": "123", "shortCiteRegEx": "123", "year": 2009}, {"title": "Introducing the Knowledge Graph: things, not strings, May 2012.  URL http://googleblog.blogspot.com/2012/05/ introducing-knowledge-graph-things-not.html", "author": ["Amit Singhal"], "venue": null, "citeRegEx": "126", "shortCiteRegEx": "126", "year": 2012}, {"title": "Cognitive Psychology: Pearson New International Edition: Mind and Brain", "author": ["Edward E Smith", "Stephen M Kosslyn"], "venue": "Pearson Higher Ed,", "citeRegEx": "127", "shortCiteRegEx": "127", "year": 2013}, {"title": "Harmony theory: Problem solving, parallel cognitive models, and thermal physics", "author": ["Paul Smolensky", "Mary S Riley"], "venue": "Technical report, DTIC Document,", "citeRegEx": "128", "shortCiteRegEx": "128", "year": 1984}, {"title": "A bayesian analysis of dynamics in free recall", "author": ["Richard Socher", "Samuel Gershman", "Per Sederberg", "Kenneth Norman", "Adler J Perotte", "David M Blei"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "129", "shortCiteRegEx": "129", "year": 2009}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "130", "shortCiteRegEx": "130", "year": 2013}, {"title": "Memory and brain", "author": ["Larry R Squire"], "venue": null, "citeRegEx": "131", "shortCiteRegEx": "131", "year": 1987}, {"title": "Retrograde amnesia and memory consolidation: a neurobiological perspective", "author": ["Larry R Squire", "Pablo Alvarez"], "venue": "Current opinion in neurobiology,", "citeRegEx": "132", "shortCiteRegEx": "132", "year": 1995}, {"title": "Word association spaces for predicting semantic similarity effects in episodic memory. Experimental cognitive psychology and its applications: Festschrift in honor of Lyle", "author": ["Mark Steyvers", "Richard M Shiffrin", "Douglas L Nelson"], "venue": null, "citeRegEx": "133", "shortCiteRegEx": "133", "year": 2004}, {"title": "Yago: A Core of Semantic Knowledge", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th International Conference on World Wide Web,", "citeRegEx": "134", "shortCiteRegEx": "134", "year": 2007}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "135", "shortCiteRegEx": "135", "year": 2014}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lars Wolf"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "136", "shortCiteRegEx": "136", "year": 2014}, {"title": "Theory-based bayesian models of inductive learning and reasoning", "author": ["Joshua B Tenenbaum", "Thomas L Griffiths", "Charles Kemp"], "venue": "Trends in cognitive sciences,", "citeRegEx": "137", "shortCiteRegEx": "137", "year": 2006}, {"title": "Materializing and querying learned knowledge", "author": ["Volker Tresp", "Yi Huang", "Markus Bundschus", "Achim Rettinger"], "venue": "Proc. of IRMLeS,", "citeRegEx": "138", "shortCiteRegEx": "138", "year": 2009}, {"title": "Episodic and semantic memory 1", "author": ["Endel Tulving"], "venue": "Organization of Memory. London: Academic,", "citeRegEx": "139", "shortCiteRegEx": "139", "year": 1972}, {"title": "Elements of episodic memory", "author": ["Endel Tulving"], "venue": null, "citeRegEx": "140", "shortCiteRegEx": "140", "year": 1985}, {"title": "Episodic memory: from mind to brain", "author": ["Endel Tulving"], "venue": "Annual review of psychology,", "citeRegEx": "141", "shortCiteRegEx": "141", "year": 2002}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "142", "shortCiteRegEx": "142", "year": 2015}, {"title": "The unreasonable effectiveness of mathematics in the natural sciences. richard courant lecture in mathematical sciences delivered at new york university", "author": ["Eugene P Wigner"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "144", "shortCiteRegEx": "144", "year": 1959}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "145", "shortCiteRegEx": "145", "year": 2015}, {"title": "Infinite Hidden Relational Models", "author": ["Zhao Xu", "Volker Tresp", "Kai Yu", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 22nd International Conference on Uncertainity in Artificial Intelligence,", "citeRegEx": "146", "shortCiteRegEx": "146", "year": 2006}, {"title": "Embedding mapping approaches for tensor factorization and knowledge graph modelling", "author": ["Yinchong Yang", "Crist\u00f3bal", "Volker Tresp"], "venue": "In ESWC,", "citeRegEx": "147", "shortCiteRegEx": "147", "year": 2016}, {"title": "The Cognitive Neuroscience of Semantic Memory", "author": ["Eiling Yee", "Evangelia G Chrysikou", "Sharon L Thompson-Schill"], "venue": "Oxford Handbook of Cognitive Neuroscience,", "citeRegEx": "148", "shortCiteRegEx": "148", "year": 2014}, {"title": "Discretized streams: an efficient and fault-tolerant model for stream processing on large clusters", "author": ["Matei Zaharia", "Tathagata Das", "Haoyuan Li", "Scott Shenker", "Ion Stoica"], "venue": "In Presented as part of the,", "citeRegEx": "149", "shortCiteRegEx": "149", "year": 2012}], "referenceMentions": [{"referenceID": 15, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 113, "endOffset": 138}, {"referenceID": 14, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 113, "endOffset": 138}, {"referenceID": 16, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 113, "endOffset": 138}, {"referenceID": 53, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 113, "endOffset": 138}, {"referenceID": 103, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 129, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 97, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 19, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 20, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 121, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 36, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 99, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 100, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 43, "context": "In recent publications the embedding models were extended to also consider temporal evolutions, time patterns and subsymbolic representations [48, 49].", "startOffset": 142, "endOffset": 150}, {"referenceID": 44, "context": "In recent publications the embedding models were extended to also consider temporal evolutions, time patterns and subsymbolic representations [48, 49].", "startOffset": 142, "endOffset": 150}, {"referenceID": 113, "context": "These extended models were used successfully to predict clinical Some authors make a distinction between latent representations, which are application specific, and embeddings, which are identical across applications and might represent universal properties of entities [120, 124].", "startOffset": 270, "endOffset": 280}, {"referenceID": 49, "context": "Figure 1: Organization of human memory [54, 57].", "startOffset": 39, "endOffset": 47}, {"referenceID": 52, "context": "Figure 1: Organization of human memory [54, 57].", "startOffset": 39, "endOffset": 47}, {"referenceID": 83, "context": "Our approach follows the tradition of latent semantic analysis (LSA), which is a classical representation learning approach that on the one hand has found a number of technical applications and on the other hand could be related to cognitive semantic memories [88, 87, 38].", "startOffset": 260, "endOffset": 272}, {"referenceID": 82, "context": "Our approach follows the tradition of latent semantic analysis (LSA), which is a classical representation learning approach that on the one hand has found a number of technical applications and on the other hand could be related to cognitive semantic memories [88, 87, 38].", "startOffset": 260, "endOffset": 272}, {"referenceID": 34, "context": "Our approach follows the tradition of latent semantic analysis (LSA), which is a classical representation learning approach that on the one hand has found a number of technical applications and on the other hand could be related to cognitive semantic memories [88, 87, 38].", "startOffset": 260, "endOffset": 272}, {"referenceID": 5, "context": "Cognitive memory functions are typically classified as long-term, short-term, and sensory memory, where long-term memory has the subcategories declarative memory and non-declarative memory [42, 6, 131, 14, 35, 54, 57].", "startOffset": 189, "endOffset": 217}, {"referenceID": 122, "context": "Cognitive memory functions are typically classified as long-term, short-term, and sensory memory, where long-term memory has the subcategories declarative memory and non-declarative memory [42, 6, 131, 14, 35, 54, 57].", "startOffset": 189, "endOffset": 217}, {"referenceID": 11, "context": "Cognitive memory functions are typically classified as long-term, short-term, and sensory memory, where long-term memory has the subcategories declarative memory and non-declarative memory [42, 6, 131, 14, 35, 54, 57].", "startOffset": 189, "endOffset": 217}, {"referenceID": 31, "context": "Cognitive memory functions are typically classified as long-term, short-term, and sensory memory, where long-term memory has the subcategories declarative memory and non-declarative memory [42, 6, 131, 14, 35, 54, 57].", "startOffset": 189, "endOffset": 217}, {"referenceID": 49, "context": "Cognitive memory functions are typically classified as long-term, short-term, and sensory memory, where long-term memory has the subcategories declarative memory and non-declarative memory [42, 6, 131, 14, 35, 54, 57].", "startOffset": 189, "endOffset": 217}, {"referenceID": 52, "context": "Cognitive memory functions are typically classified as long-term, short-term, and sensory memory, where long-term memory has the subcategories declarative memory and non-declarative memory [42, 6, 131, 14, 35, 54, 57].", "startOffset": 189, "endOffset": 217}, {"referenceID": 49, "context": "There is evidence that these main cognitive categories are partially dissociated from one another in the brain, as expressed in their differential sensitivity to brain damage [54].", "startOffset": 175, "endOffset": 179}, {"referenceID": 71, "context": "However, there is also evidence indicating that the different memory functions are not mutually independent and support each other [76, 61].", "startOffset": 131, "endOffset": 139}, {"referenceID": 56, "context": "However, there is also evidence indicating that the different memory functions are not mutually independent and support each other [76, 61].", "startOffset": 131, "endOffset": 139}, {"referenceID": 130, "context": "Finally, episodic memory stores information of general and personal events [139, 140, 141, 54].", "startOffset": 75, "endOffset": 94}, {"referenceID": 131, "context": "Finally, episodic memory stores information of general and personal events [139, 140, 141, 54].", "startOffset": 75, "endOffset": 94}, {"referenceID": 132, "context": "Finally, episodic memory stores information of general and personal events [139, 140, 141, 54].", "startOffset": 75, "endOffset": 94}, {"referenceID": 49, "context": "Finally, episodic memory stores information of general and personal events [139, 140, 141, 54].", "startOffset": 75, "endOffset": 94}, {"referenceID": 52, "context": "Whereas semantic memory concerns information we \u201cknow\u201d, episodic memory concerns information we \u201cremember\u201d [57].", "startOffset": 107, "endOffset": 111}, {"referenceID": 49, "context": "It is the ability to retain impressions of sensory information after the original stimuli have ended [54].", "startOffset": 101, "endOffset": 105}, {"referenceID": 6, "context": "Popular large-scale KGs are DBpedia [7], YAGO [134], Freebase [21], NELL [27], and the Google Knowledge Graph [126].", "startOffset": 36, "endOffset": 39}, {"referenceID": 125, "context": "Popular large-scale KGs are DBpedia [7], YAGO [134], Freebase [21], NELL [27], and the Google Knowledge Graph [126].", "startOffset": 46, "endOffset": 51}, {"referenceID": 18, "context": "Popular large-scale KGs are DBpedia [7], YAGO [134], Freebase [21], NELL [27], and the Google Knowledge Graph [126].", "startOffset": 62, "endOffset": 66}, {"referenceID": 24, "context": "Popular large-scale KGs are DBpedia [7], YAGO [134], Freebase [21], NELL [27], and the Google Knowledge Graph [126].", "startOffset": 73, "endOffset": 77}, {"referenceID": 117, "context": "Popular large-scale KGs are DBpedia [7], YAGO [134], Freebase [21], NELL [27], and the Google Knowledge Graph [126].", "startOffset": 110, "endOffset": 115}, {"referenceID": 97, "context": "For example if a writer was born in Munich, the model can infer that the writer is also born in Germany and probably writes in the German language [104, 105].", "startOffset": 147, "endOffset": 157}, {"referenceID": 98, "context": "For example if a writer was born in Munich, the model can infer that the writer is also born in Germany and probably writes in the German language [104, 105].", "startOffset": 147, "endOffset": 157}, {"referenceID": 99, "context": "Stochastic gradient descent (SGD) is typically being used as an iterative approach for finding both optimal latent representations and optimal parameters in f semanticp \u0308q [106, 85].", "startOffset": 172, "endOffset": 181}, {"referenceID": 80, "context": "Stochastic gradient descent (SGD) is typically being used as an iterative approach for finding both optimal latent representations and optimal parameters in f semanticp \u0308q [106, 85].", "startOffset": 172, "endOffset": 181}, {"referenceID": 99, "context": "For a recent review, please consult [106].", "startOffset": 36, "endOffset": 41}, {"referenceID": 79, "context": "More complex queries on semantic models involving existential quantifier are discussed in [84].", "startOffset": 90, "endOffset": 94}, {"referenceID": 96, "context": "In [103, 102] such a structure was learned from the latent representations by hierarchical clustering.", "startOffset": 3, "endOffset": 13}, {"referenceID": 95, "context": "In [103, 102] such a structure was learned from the latent representations by hierarchical clustering.", "startOffset": 3, "endOffset": 13}, {"referenceID": 82, "context": ", in latent semantic analysis [87] which is restricted to attribute-based representations.", "startOffset": 30, "endOffset": 34}, {"referenceID": 67, "context": "Generalizations towards probabilistic models are probabilistic latent semantic indexing [72] and latent Dirichlet allocation [20].", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "Generalizations towards probabilistic models are probabilistic latent semantic indexing [72] and latent Dirichlet allocation [20].", "startOffset": 125, "endOffset": 129}, {"referenceID": 73, "context": "Latent clustering and topic models [78, 146, 2] are extensions toward multi-relational domains and use discrete latent representations.", "startOffset": 35, "endOffset": 47}, {"referenceID": 136, "context": "Latent clustering and topic models [78, 146, 2] are extensions toward multi-relational domains and use discrete latent representations.", "startOffset": 35, "endOffset": 47}, {"referenceID": 1, "context": "Latent clustering and topic models [78, 146, 2] are extensions toward multi-relational domains and use discrete latent representations.", "startOffset": 35, "endOffset": 47}, {"referenceID": 86, "context": "See also [93, 62, 63].", "startOffset": 9, "endOffset": 21}, {"referenceID": 57, "context": "See also [93, 62, 63].", "startOffset": 9, "endOffset": 21}, {"referenceID": 58, "context": "See also [93, 62, 63].", "startOffset": 9, "endOffset": 21}, {"referenceID": 27, "context": "of semantic memory [30].", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "Associate models are the symbolic ACT-R [4, 5] and SAM [114].", "startOffset": 40, "endOffset": 46}, {"referenceID": 4, "context": "Associate models are the symbolic ACT-R [4, 5] and SAM [114].", "startOffset": 40, "endOffset": 46}, {"referenceID": 107, "context": "Associate models are the symbolic ACT-R [4, 5] and SAM [114].", "startOffset": 55, "endOffset": 60}, {"referenceID": 100, "context": "[107] explores holographic embeddings with representation learning to model associative memories.", "startOffset": 0, "endOffset": 5}, {"referenceID": 68, "context": "Connectionists memory models are described in [73, 96, 28, 82, 67, 68].", "startOffset": 46, "endOffset": 70}, {"referenceID": 89, "context": "Connectionists memory models are described in [73, 96, 28, 82, 67, 68].", "startOffset": 46, "endOffset": 70}, {"referenceID": 25, "context": "Connectionists memory models are described in [73, 96, 28, 82, 67, 68].", "startOffset": 46, "endOffset": 70}, {"referenceID": 77, "context": "Connectionists memory models are described in [73, 96, 28, 82, 67, 68].", "startOffset": 46, "endOffset": 70}, {"referenceID": 62, "context": "Connectionists memory models are described in [73, 96, 28, 82, 67, 68].", "startOffset": 46, "endOffset": 70}, {"referenceID": 63, "context": "Connectionists memory models are described in [73, 96, 28, 82, 67, 68].", "startOffset": 46, "endOffset": 70}, {"referenceID": 118, "context": "Episodic memory represents our memory of experiences and specific events in time in a serial form (a \u201cmental time travel\u201d), from which we can reconstruct the actual events that took place at any given point in our lives [127]2.", "startOffset": 220, "endOffset": 225}, {"referenceID": 131, "context": "In contrast to semantic memory, it requires recollection of a prior experience [140].", "startOffset": 79, "endOffset": 84}, {"referenceID": 43, "context": "Some of the elements of this triple graph will affect changes in the KG [48, 49] (see also the discussion in Section 7).", "startOffset": 72, "endOffset": 80}, {"referenceID": 44, "context": "Some of the elements of this triple graph will affect changes in the KG [48, 49] (see also the discussion in Section 7).", "startOffset": 72, "endOffset": 80}, {"referenceID": 43, "context": "Whereas aet is a latent representation for all events for all patients at time t, aes\u201ci,t is a latent representation for all events for patients i at time t [48, 49].", "startOffset": 157, "endOffset": 165}, {"referenceID": 44, "context": "Whereas aet is a latent representation for all events for all patients at time t, aes\u201ci,t is a latent representation for all events for patients i at time t [48, 49].", "startOffset": 157, "endOffset": 165}, {"referenceID": 29, "context": "The autobiographical event tensor would correspond to the autobiographical memory, which stores autobiographical events of an individual on a semantic abstraction level [33, 54].", "startOffset": 169, "endOffset": 177}, {"referenceID": 49, "context": "The autobiographical event tensor would correspond to the autobiographical memory, which stores autobiographical events of an individual on a semantic abstraction level [33, 54].", "startOffset": 169, "endOffset": 177}, {"referenceID": 8, "context": "The autobiographical event tensor can be related to Baddeley\u2019s episodic buffer and, in contrast to Tulving\u2019s concept of episodic memory, is a temporary store and is considered to be a part of working memory [10, 76, 11].", "startOffset": 207, "endOffset": 219}, {"referenceID": 71, "context": "The autobiographical event tensor can be related to Baddeley\u2019s episodic buffer and, in contrast to Tulving\u2019s concept of episodic memory, is a temporary store and is considered to be a part of working memory [10, 76, 11].", "startOffset": 207, "endOffset": 219}, {"referenceID": 9, "context": "The autobiographical event tensor can be related to Baddeley\u2019s episodic buffer and, in contrast to Tulving\u2019s concept of episodic memory, is a temporary store and is considered to be a part of working memory [10, 76, 11].", "startOffset": 207, "endOffset": 219}, {"referenceID": 139, "context": "The sensory buffer might be related to the mini-batches in Spark Streaming where data is captured in buffers that hold seconds to minutes of the input streams [149].", "startOffset": 159, "endOffset": 164}, {"referenceID": 44, "context": "In a technical application [49], the sensors measure, e.", "startOffset": 27, "endOffset": 31}, {"referenceID": 130, "context": "In human cognition, sensory memory (milliseconds to a second) represents the ability to retain impressions of sensory information after the original stimuli have ended [139, 31, 54].", "startOffset": 168, "endOffset": 181}, {"referenceID": 28, "context": "In human cognition, sensory memory (milliseconds to a second) represents the ability to retain impressions of sensory information after the original stimuli have ended [139, 31, 54].", "startOffset": 168, "endOffset": 181}, {"referenceID": 49, "context": "In human cognition, sensory memory (milliseconds to a second) represents the ability to retain impressions of sensory information after the original stimuli have ended [139, 31, 54].", "startOffset": 168, "endOffset": 181}, {"referenceID": 5, "context": ", the autobiographical episodic buffer) is the first step in some memory models, in particular in the modal theory of Atkinson and Shiffrin [6].", "startOffset": 140, "endOffset": 143}, {"referenceID": 49, "context": "New evidence suggests that short-term memory is not the sole gateway to long-term memory [54].", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "Some investigators propose that only some dimensions of the latent representations should be shared [3, 1].", "startOffset": 100, "endOffset": 106}, {"referenceID": 0, "context": "Some investigators propose that only some dimensions of the latent representations should be shared [3, 1].", "startOffset": 100, "endOffset": 106}, {"referenceID": 16, "context": "4 [89, 19, 17] contain extensive discussions on the transfer of latent representations.", "startOffset": 2, "endOffset": 14}, {"referenceID": 14, "context": "4 [89, 19, 17] contain extensive discussions on the transfer of latent representations.", "startOffset": 2, "endOffset": 14}, {"referenceID": 43, "context": "In the technical solutions [48, 49], we got best results by focussing on the cost function that corresponded to the problem to solve.", "startOffset": 27, "endOffset": 35}, {"referenceID": 44, "context": "In the technical solutions [48, 49], we got best results by focussing on the cost function that corresponded to the problem to solve.", "startOffset": 27, "endOffset": 35}, {"referenceID": 99, "context": "Tensor decompositions have also shown excellent performance in modelling KGs [106].", "startOffset": 77, "endOffset": 82}, {"referenceID": 97, "context": "Finally, the RESCAL model [104] is a Tucker2 model with", "startOffset": 26, "endOffset": 31}, {"referenceID": 104, "context": "By repeating this process we can obtain independent samples from P ps, p, oq! Note that there is a certain equivalence between tensor models and sum-product networks, where similar operations for marginals and conditionals can be defined [111].", "startOffset": 238, "endOffset": 243}, {"referenceID": 137, "context": "where f p \u0308q is a function to be learned [147] (see Figure 8) and where vecpu:,:,tq are vectorized representations from the portion of the sensory tensor associated with the individual at time t.", "startOffset": 41, "endOffset": 46}, {"referenceID": 127, "context": "Depending on the application, f p \u0308q can be a simple linear map, or it can be a second to last layer in a deep neural network as in the face recognition application DeepFace [136, 101].", "startOffset": 174, "endOffset": 184}, {"referenceID": 94, "context": "Depending on the application, f p \u0308q can be a simple linear map, or it can be a second to last layer in a deep neural network as in the face recognition application DeepFace [136, 101].", "startOffset": 174, "endOffset": 184}, {"referenceID": 66, "context": "Alternatively one might use networks with additional memory buffers and attention mechanisms [71, 143, 60, 86, 58].", "startOffset": 93, "endOffset": 114}, {"referenceID": 55, "context": "Alternatively one might use networks with additional memory buffers and attention mechanisms [71, 143, 60, 86, 58].", "startOffset": 93, "endOffset": 114}, {"referenceID": 81, "context": "Alternatively one might use networks with additional memory buffers and attention mechanisms [71, 143, 60, 86, 58].", "startOffset": 93, "endOffset": 114}, {"referenceID": 53, "context": "Alternatively one might use networks with additional memory buffers and attention mechanisms [71, 143, 60, 86, 58].", "startOffset": 93, "endOffset": 114}, {"referenceID": 106, "context": "Researchers have reported on a remarkable subset of medial temporal lobe (MTL) neurons that are selectively activated by strikingly different pictures of given individuals, landmarks or objects and in some cases even by letter strings with their names [113, 112].", "startOffset": 252, "endOffset": 262}, {"referenceID": 105, "context": "Researchers have reported on a remarkable subset of medial temporal lobe (MTL) neurons that are selectively activated by strikingly different pictures of given individuals, landmarks or objects and in some cases even by letter strings with their names [113, 112].", "startOffset": 252, "endOffset": 262}, {"referenceID": 89, "context": "Our hypothesis supports both locality and globality of encoding [96, 43], since index neurons are local representations of generalized entities, whereas the representation layers would be highdimensional and non-local.", "startOffset": 64, "endOffset": 72}, {"referenceID": 38, "context": "Our hypothesis supports both locality and globality of encoding [96, 43], since index neurons are local representations of generalized entities, whereas the representation layers would be highdimensional and non-local.", "startOffset": 64, "endOffset": 72}, {"referenceID": 52, "context": "Often neurons with similar receptive fields are clustered together in sensory cortices and form a topographic map [57].", "startOffset": 114, "endOffset": 118}, {"referenceID": 69, "context": "A detailed atlas of semantic categories has been established in extensive fMRI studies showing the involvement of the lateral temporal cortex (LTC), the ventral temporal cortex (VTC), the lateral parietal cortex (LPC), the medial parietal cortex (MPC), the medial prefrontal cortex, the superior prefrontal cortex (SPFC) and the inferior prefrontal cortex (IPFC) [74].", "startOffset": 363, "endOffset": 367}, {"referenceID": 98, "context": ", in [105].", "startOffset": 5, "endOffset": 10}, {"referenceID": 41, "context": "Evidence for time cells have recently been found [46, 44, 80, 79].", "startOffset": 49, "endOffset": 65}, {"referenceID": 39, "context": "Evidence for time cells have recently been found [46, 44, 80, 79].", "startOffset": 49, "endOffset": 65}, {"referenceID": 75, "context": "Evidence for time cells have recently been found [46, 44, 80, 79].", "startOffset": 49, "endOffset": 65}, {"referenceID": 74, "context": "Evidence for time cells have recently been found [46, 44, 80, 79].", "startOffset": 49, "endOffset": 65}, {"referenceID": 84, "context": "It has been observed that the hippocampus becomes activated when the temporal order of events is being processed [91, 119, 118].", "startOffset": 113, "endOffset": 127}, {"referenceID": 112, "context": "It has been observed that the hippocampus becomes activated when the temporal order of events is being processed [91, 119, 118].", "startOffset": 113, "endOffset": 127}, {"referenceID": 111, "context": "It has been observed that the hippocampus becomes activated when the temporal order of events is being processed [91, 119, 118].", "startOffset": 113, "endOffset": 127}, {"referenceID": 52, "context": "In fact, it has been observed that the adult macaque monkey forms a few thousand new neurons daily [57, 59], possibly to encode new information [16].", "startOffset": 99, "endOffset": 107}, {"referenceID": 54, "context": "In fact, it has been observed that the adult macaque monkey forms a few thousand new neurons daily [57, 59], possibly to encode new information [16].", "startOffset": 99, "endOffset": 107}, {"referenceID": 13, "context": "In fact, it has been observed that the adult macaque monkey forms a few thousand new neurons daily [57, 59], possibly to encode new information [16].", "startOffset": 144, "endOffset": 148}, {"referenceID": 92, "context": "There are multiple, functionally specialized, cell types of the hippocampalentorhinal circuit, such as place, grid, and border cells [99].", "startOffset": 133, "endOffset": 137}, {"referenceID": 90, "context": "Spatial memories, as other memories, are thought to be slowly induced in the neocortex by a gradual recruitment of neocortical memory circuits in long-term storage of hippocampal memories [97, 132, 52, 99].", "startOffset": 188, "endOffset": 205}, {"referenceID": 123, "context": "Spatial memories, as other memories, are thought to be slowly induced in the neocortex by a gradual recruitment of neocortical memory circuits in long-term storage of hippocampal memories [97, 132, 52, 99].", "startOffset": 188, "endOffset": 205}, {"referenceID": 47, "context": "Spatial memories, as other memories, are thought to be slowly induced in the neocortex by a gradual recruitment of neocortical memory circuits in long-term storage of hippocampal memories [97, 132, 52, 99].", "startOffset": 188, "endOffset": 205}, {"referenceID": 92, "context": "Spatial memories, as other memories, are thought to be slowly induced in the neocortex by a gradual recruitment of neocortical memory circuits in long-term storage of hippocampal memories [97, 132, 52, 99].", "startOffset": 188, "endOffset": 205}, {"referenceID": 49, "context": "The perirhinal and parahippocampal cortices also project back to the association areas of the cortex [54].", "startOffset": 101, "endOffset": 105}, {"referenceID": 44, "context": "This is the case in the medical application described in [49, 48] where u:,:,t describes procedures and diagnosis and one can think of f p \u0308q as being an encoder system and f p \u0308q as being a decoder and the complex as being an autoencoder [24, 70].", "startOffset": 57, "endOffset": 65}, {"referenceID": 43, "context": "This is the case in the medical application described in [49, 48] where u:,:,t describes procedures and diagnosis and one can think of f p \u0308q as being an encoder system and f p \u0308q as being a decoder and the complex as being an autoencoder [24, 70].", "startOffset": 57, "endOffset": 65}, {"referenceID": 21, "context": "This is the case in the medical application described in [49, 48] where u:,:,t describes procedures and diagnosis and one can think of f p \u0308q as being an encoder system and f p \u0308q as being a decoder and the complex as being an autoencoder [24, 70].", "startOffset": 239, "endOffset": 247}, {"referenceID": 65, "context": "This is the case in the medical application described in [49, 48] where u:,:,t describes procedures and diagnosis and one can think of f p \u0308q as being an encoder system and f p \u0308q as being a decoder and the complex as being an autoencoder [24, 70].", "startOffset": 239, "endOffset": 247}, {"referenceID": 123, "context": "According to the standard model of memory consolidation [132, 51] memory is retained in the hippocampus for up to one week after initial learning, representing the hippocampus-dependent stage.", "startOffset": 56, "endOffset": 65}, {"referenceID": 46, "context": "According to the standard model of memory consolidation [132, 51] memory is retained in the hippocampus for up to one week after initial learning, representing the hippocampus-dependent stage.", "startOffset": 56, "endOffset": 65}, {"referenceID": 52, "context": "The frontal cortex, associated with higher functionalities, plays a role in which new information gets encoded as episodic and semantic memory and what gets forgotten [57].", "startOffset": 167, "endOffset": 171}, {"referenceID": 23, "context": "The amygdala belongs to the MTL and consists of several nuclei but is not considered to be a part of memory itself [26].", "startOffset": 115, "endOffset": 119}, {"referenceID": 111, "context": "The amygdala and the orbitofrontal cortex might also provide reward-related information to the hippocampus [118].", "startOffset": 107, "endOffset": 112}, {"referenceID": 135, "context": ", in the decoding of complex scenes [145, 142, 77].", "startOffset": 36, "endOffset": 50}, {"referenceID": 133, "context": ", in the decoding of complex scenes [145, 142, 77].", "startOffset": 36, "endOffset": 50}, {"referenceID": 72, "context": ", in the decoding of complex scenes [145, 142, 77].", "startOffset": 36, "endOffset": 50}, {"referenceID": 126, "context": "The proposed model can be related to encoder-decoder networks [135] which produce text sequences, whereas we produce a set of likely triples.", "startOffset": 62, "endOffset": 67}, {"referenceID": 87, "context": "In the past, a number of neural winner-takes-all networks have been proposed where the neuron with the largest activation wins over all other neurons, which are driven to inactivity [94, 69].", "startOffset": 182, "endOffset": 190}, {"referenceID": 64, "context": "In the past, a number of neural winner-takes-all networks have been proposed where the neuron with the largest activation wins over all other neurons, which are driven to inactivity [94, 69].", "startOffset": 182, "endOffset": 190}, {"referenceID": 88, "context": "It is known that CA3 contains many feedback connections, essential for winner-takes-all computations [95, 56, 118].", "startOffset": 101, "endOffset": 114}, {"referenceID": 51, "context": "It is known that CA3 contains many feedback connections, essential for winner-takes-all computations [95, 56, 118].", "startOffset": 101, "endOffset": 114}, {"referenceID": 111, "context": "It is known that CA3 contains many feedback connections, essential for winner-takes-all computations [95, 56, 118].", "startOffset": 101, "endOffset": 114}, {"referenceID": 111, "context": "CA3 is sometimes modelled as a continuous attractor neural network (CANN) with excitatory recurrent colateral connections and global inhibition [118].", "startOffset": 144, "endOffset": 149}, {"referenceID": 119, "context": "The restricted Boltzmann machine (RBM) might be an interesting option for supporting the decoding process [128, 66].", "startOffset": 106, "endOffset": 115}, {"referenceID": 61, "context": "The restricted Boltzmann machine (RBM) might be an interesting option for supporting the decoding process [128, 66].", "startOffset": 106, "endOffset": 115}, {"referenceID": 49, "context": "There is growing evidence that the hippocampus plays an important role not just in encoding but also in decoding of memory and is involved in the retrieval of information from long-term memory [54].", "startOffset": 193, "endOffset": 197}, {"referenceID": 40, "context": "Both types of information then pass through the entorhinal cortex but only converge within the hippocampus where it enables a full recognition of an episodic event [45, 39, 115, 54].", "startOffset": 164, "endOffset": 181}, {"referenceID": 35, "context": "Both types of information then pass through the entorhinal cortex but only converge within the hippocampus where it enables a full recognition of an episodic event [45, 39, 115, 54].", "startOffset": 164, "endOffset": 181}, {"referenceID": 108, "context": "Both types of information then pass through the entorhinal cortex but only converge within the hippocampus where it enables a full recognition of an episodic event [45, 39, 115, 54].", "startOffset": 164, "endOffset": 181}, {"referenceID": 49, "context": "Both types of information then pass through the entorhinal cortex but only converge within the hippocampus where it enables a full recognition of an episodic event [45, 39, 115, 54].", "startOffset": 164, "endOffset": 181}, {"referenceID": 134, "context": "The physicist Eugene Wigner has speculated on the \u201cThe Unreasonable Effectiveness of Mathematics in the Natural Sciences\u201d [144]; in other words mathematics is the right code for the natural sciences.", "startOffset": 122, "endOffset": 127}, {"referenceID": 52, "context": "13 This form of a semantic memory is very attractive since it requires no additional modelling effort and can use the same structures that are needed for episodic memory! It has been argued that semantic memory is information we have encountered repeatedly, so often that the actual learning episodes are blurred [32, 57].", "startOffset": 313, "endOffset": 321}, {"referenceID": 56, "context": "Without doubt, semantic and episodic memories support one another [61].", "startOffset": 66, "endOffset": 70}, {"referenceID": 122, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 7, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 124, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 120, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 90, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 138, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 81, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 91, "context": "[98] is a recent overview on the topic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 132, "context": ", the representations of entities and predicates [141, 57].", "startOffset": 49, "endOffset": 58}, {"referenceID": 52, "context": ", the representations of entities and predicates [141, 57].", "startOffset": 49, "endOffset": 58}, {"referenceID": 49, "context": "But note that studies have also found an independent formation of semantic memories, in case that the episodic memory is dysfunctional, as in certain amnesic patients: Amnesic patients might learn new facts without remembering the episodes during which they have learned the information [54].", "startOffset": 287, "endOffset": 291}, {"referenceID": 110, "context": "In fact in many studies it has been shown that individuals produce false memories but are personally absolutely convinced of their truthfulness [117, 92].", "startOffset": 144, "endOffset": 153}, {"referenceID": 85, "context": "In fact in many studies it has been shown that individuals produce false memories but are personally absolutely convinced of their truthfulness [117, 92].", "startOffset": 144, "endOffset": 153}, {"referenceID": 68, "context": "The biological plausibility of symmetric weights has been discussed intensely in computational neuroscience and many biologically oriented models have that property [73, 69].", "startOffset": 165, "endOffset": 173}, {"referenceID": 64, "context": "The biological plausibility of symmetric weights has been discussed intensely in computational neuroscience and many biologically oriented models have that property [73, 69].", "startOffset": 165, "endOffset": 173}, {"referenceID": 65, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 32, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 109, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 76, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 78, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 128, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 59, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 50, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 48, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 33, "context": "As discussed before, novelty might be an important factor that determines which sensory information is stored in episodic memory, as speculated by other models and supported by cognitive studies [37, 75, 123, 53, 15].", "startOffset": 195, "endOffset": 216}, {"referenceID": 70, "context": "As discussed before, novelty might be an important factor that determines which sensory information is stored in episodic memory, as speculated by other models and supported by cognitive studies [37, 75, 123, 53, 15].", "startOffset": 195, "endOffset": 216}, {"referenceID": 116, "context": "As discussed before, novelty might be an important factor that determines which sensory information is stored in episodic memory, as speculated by other models and supported by cognitive studies [37, 75, 123, 53, 15].", "startOffset": 195, "endOffset": 216}, {"referenceID": 48, "context": "As discussed before, novelty might be an important factor that determines which sensory information is stored in episodic memory, as speculated by other models and supported by cognitive studies [37, 75, 123, 53, 15].", "startOffset": 195, "endOffset": 216}, {"referenceID": 12, "context": "As discussed before, novelty might be an important factor that determines which sensory information is stored in episodic memory, as speculated by other models and supported by cognitive studies [37, 75, 123, 53, 15].", "startOffset": 195, "endOffset": 216}, {"referenceID": 10, "context": "Mental imagery can be viewed as the conscious and explicit manipulation of simulations in working memory to predict future events [13].", "startOffset": 130, "endOffset": 134}, {"referenceID": 115, "context": "The link between episodic memory and mental imagery has been studied in [122] and [65].", "startOffset": 72, "endOffset": 77}, {"referenceID": 60, "context": "The link between episodic memory and mental imagery has been studied in [122] and [65].", "startOffset": 82, "endOffset": 86}, {"referenceID": 101, "context": "Prediction of events and actions on a semantic level is sometimes considered to be one of the important functions of a cognitive working memory [108].", "startOffset": 144, "endOffset": 149}, {"referenceID": 49, "context": "As in our prediction model, the contents of working memory could either originate from sensory input, the episodic buffer, or from semantic memory [54].", "startOffset": 147, "endOffset": 151}, {"referenceID": 9, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 52, "endOffset": 71}, {"referenceID": 30, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 52, "endOffset": 71}, {"referenceID": 42, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 52, "endOffset": 71}, {"referenceID": 93, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 114, "endOffset": 140}, {"referenceID": 37, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 114, "endOffset": 140}, {"referenceID": 45, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 114, "endOffset": 140}, {"referenceID": 22, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 114, "endOffset": 140}, {"referenceID": 71, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 114, "endOffset": 140}, {"referenceID": 101, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 114, "endOffset": 140}, {"referenceID": 26, "context": "The terms \u201cpredictive brain\u201d and \u201canticipating brain\u201d emphasize the importance of \u201clooking into the future\u201d, namely prediction, preparation, anticipation, prospection or expectations in various cognitive domains [29].", "startOffset": 212, "endOffset": 216}, {"referenceID": 65, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 32, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 109, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 76, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 78, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 128, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 59, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 50, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 48, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 49, "context": "The cerebellum is involved in trial-and-error learning based on predictive error signals [54].", "startOffset": 89, "endOffset": 93}, {"referenceID": 49, "context": "Reward prediction is a task of the basal ganglia where dopamine neurons encode both present rewards and future rewards, as a basis for reinforcement learning [54, 57].", "startOffset": 158, "endOffset": 166}, {"referenceID": 52, "context": "Reward prediction is a task of the basal ganglia where dopamine neurons encode both present rewards and future rewards, as a basis for reinforcement learning [54, 57].", "startOffset": 158, "endOffset": 166}, {"referenceID": 52, "context": "There is evidence that a strong working memory is associated with general intelligence [57].", "startOffset": 87, "endOffset": 91}, {"referenceID": 102, "context": "There is an emerging consensus that functions of working memory are located in the prefrontal cortex and that a number of other brain areas are recruited [109, 54].", "startOffset": 154, "endOffset": 163}, {"referenceID": 49, "context": "There is an emerging consensus that functions of working memory are located in the prefrontal cortex and that a number of other brain areas are recruited [109, 54].", "startOffset": 154, "endOffset": 163}, {"referenceID": 52, "context": "More precisely, the central executive is attributed to the dorsolateral prefrontal cortex, the phonological loop with the left ventrolateral prefrontal cortex (the semantic information is anterior to the phonological information) and the visuospatial sketchpad in the right ventrolateral prefrontal cortex [57].", "startOffset": 306, "endOffset": 310}, {"referenceID": 52, "context": "The function of the frontal lobe, in particular of the orbitofrontal cortex, includes the ability to project future consequences (predictions) resulting from current actions [57].", "startOffset": 174, "endOffset": 178}, {"referenceID": 97, "context": "Unique representations lead to a global propagation of information across all memory functions during learning [104].", "startOffset": 111, "endOffset": 116}, {"referenceID": 114, "context": "One can make a link between those parameters and implicit skill memory [121].", "startOffset": 71, "endOffset": 76}, {"referenceID": 43, "context": "More details on concrete technical solutions can be found in [48, 49] where we also present successful applications to clinical decision modeling, sensor network modeling and recommendation engines.", "startOffset": 61, "endOffset": 69}, {"referenceID": 44, "context": "More details on concrete technical solutions can be found in [48, 49] where we also present successful applications to clinical decision modeling, sensor network modeling and recommendation engines.", "startOffset": 61, "endOffset": 69}], "year": 2016, "abstractText": "Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Latent variable models are well suited to deal with the high dimensionality and sparsity of typical knowledge graphs. In recent publications the embedding models were extended to also consider time evolutions, time patterns and subsymbolic representations. In this paper we map embedding models, which were developed purely as solutions to technical problems for modelling temporal knowledge graphs, to various cognitive memory functions, in particular to semantic and concept memory, episodic memory, sensory memory, short-term memory, and working memory. We discuss learning, query answering, the path from sensory input to semantic decoding, and the relationship between episodic memory and semantic memory. We introduce a number of hypotheses on human memory that can be derived from the developed mathematical models. There are four main hypotheses. The first one is that semantic memory is described as triples and that episodic memory is described as triples in time. A second main hypothesis is that generalized entities have unique latent representations which are shared across memory functions and that are the basis for prediction, decision support and other functionalities executed by working memory (tensor memory hypothesis). A third main hypothesis is that the latent representation for a time t, which summarizes all sensory information available at time t, is the basis for episodic memory. Finally, our proposed model suggests that semantic memory and episodic memory depend on each other: Episodic decoding depends on semantic memory and semantic memory is developed as a long term store of episodic memory. On the other hand there is also a certain independence: the pure storage of episodic memory does not depend on semantic memory and semantic memory can be acquired even without a functioning episodic memory. The same relationships between semantic and episodic memories can be found in the human brain.", "creator": "LaTeX with hyperref package"}}}