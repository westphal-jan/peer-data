{"id": "1511.08032", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Learning to detect video events from zero or very few video examples", "abstract": "In this work we deal with the problem of high-level event detection in video. Specifically, we study the challenging problems of i) learning to detect video events from solely a textual description of the event, without using any positive video examples, and ii) additionally exploiting very few positive training samples together with a small number of ``related'' videos. For learning only from an event's textual description, we first identify a general learning framework and then study the impact of different design choices for various stages of this framework. For additionally learning from example videos, when true positive training samples are scarce, we employ an extension of the Support Vector Machine that allows us to exploit ``related'' event videos by automatically introducing different weights for subsets of the videos in the overall training set. Experimental evaluations performed on the large-scale TRECVID MED 2014 video dataset provide insight on the effectiveness of the proposed methods.", "histories": [["v1", "Wed, 25 Nov 2015 12:17:50 GMT  (1057kb,D)", "http://arxiv.org/abs/1511.08032v1", "Image and Vision Computing Journal, Elsevier, 2015, accepted for publication"]], "COMMENTS": "Image and Vision Computing Journal, Elsevier, 2015, accepted for publication", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["christos tzelepis", "damianos galanopoulos", "vasileios mezaris", "ioannis patras"], "accepted": false, "id": "1511.08032"}, "pdf": {"name": "1511.08032.pdf", "metadata": {"source": "CRF", "title": "Learning to detect video events from zero or very few video examples", "authors": ["Christos Tzelepis", "Damianos Galanopoulos", "Vasileios Mezaris", "Ioannis Patras"], "emails": ["tzelepis@iti.gr).", "dgalanop@iti.gr).", "bmezaris@iti.gr).", "i.patras@qmul.ac.uk)."], "sections": [{"heading": null, "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "II. RELATED WORK", "text": "In the first category, one can distinguish between i) methods that assume that positive video samples are generally limited in practice. Regardless of the assumptions about the number of positive training samples, it is assumed that negative samples can be found without much effort, and thus the number of negative video samples is not a restrictive parameter in the learning process. The above training conditions are typically simulated by the 100Ex and 10Ex MED subtasks of TRECVID [3], where 100 and 10 positive video samples are available."}, {"heading": "A. Learning from zero positive examples", "text": "In fact, most of them will be able to put themselves in a situation in which they are able to put themselves in a situation in which they are able, in which they are able, in which they are able to move, in which they are able, and in which they are able to unfold, and in which they are able to unfold."}, {"heading": "B. Learning from a few positive examples", "text": "A limited number of studies considered the problem of learning video event detectors from very few (e.g. 10) positive training examples [18], [19], [20], [21]. In [18] visual statics (e.g. SIFT [22], Transformed Color Histogram [23]) and motion (e.g. MoSIFT [24], Improved Dense Trajectories [18]) descriptors are used, along with the Fisher Vector Encoding Scheme [25], [27]. ASR and OCR techniques are also used for the use of audio and text information in video streams, as well as audio features and visual features trained in DCNNs [28]. DCNN-based functions usually include one or more hidden layers of the network [29], which provide high discriminatory power [5]."}, {"heading": "III. DETECTING VIDEO EVENTS USING AN EVENT\u2019S TEXTUAL DESCRIPTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. General architecture", "text": "We assume that the only knowledge available in relation to each event class is a textual description of the same, consisting of a title, a free-form text and a list of possible visual and audiovisual keywords, as in [14], [18], we use a pool of Nc concepts along with their titles and in some cases a limited number of subtleties. To link this textual information to the visual content of the videos we want to examine, we use a pool of Nc concepts."}, {"heading": "B. Language Models", "text": "We examine the construction of three different types of ELM, depending on the text information they use (Fig. 1). The first type of ELM, hereinafter referred to as \"title,\" is based on the automatic extraction of word terms exclusively from the title of an event class. The second type of ELM, which is referred to as \"visual,\" is constructed by using only the visual keywords provided along with the textual description of each event class. Both titles and visual keywords consist only of words and individual phrases, such as the attempt at a bicycle trick, cycling, cycling on a bicycle, etc. These words can be automatically extracted from the textual description without human experts intervening. Finally, the third type of ELM is achieved by the automatic extraction of words based on the visual and audio keywords, as well as on the short, free text of an event class, and it is referred to as \"baid.\""}, {"heading": "C. Building an event detector", "text": "The constructed language models represent the given event class and each of the available concepts as rankings of words. Thus, we can calculate the similarities between them by calculating the semantic similarity between each word in the ELM and each word belonging to the CLMs. To this end, we use the semantic measure of affinity of ESA [37], which calculates the similarity distance between two terms by calculating the cosinal similarity between their weighted vectors from Wikipedia articles. In this way, a N \u00d7 M matrix of results is calculated for each pair of event concepts. Let S denote the above similarity matrix; that is, its (i, j) ter entry si, j denotes the similarity distances between the word wi in the ELM and the word wj in the respective CLM. Then we can arrive at a single score expressing the relationship between the above-mentioned ELM and CLM, by evaluating an Exconcept, we evaluate the highest class > S-S event in each case."}, {"heading": "D. Applying the event detector to a video dataset", "text": "The DCNN-based detectors for the Nc concepts in our concept pool are applied to the videos of the dataset, whereby these videos are presented as vectors of the concept detector output scores (hereinafter referred to as model vectors). The output scores that correspond to the K concepts of the event detector are selected and compared with the corresponding values of the event detector. For this comparison, as shown in Table I (row C4), various choices of a distance function are possible; that is, we examine the use of the following distance functions: euclidean, histogram intersection, chiquare, cullback Leibler, and cosine distance. For a selected distance function, we obtain a ranking list of the videos most closely related to the event in descending order."}, {"heading": "E. Using multiple event detectors as pseudo-positive training samples", "text": "Figure 2c shows an alternative approach to using event detectors as shown in Section III-C. Since multiple event detectors can be obtained for a specific event class by varying the design selection C1 to C3 of Figure 2a (as shown in Table I), one set of pseudo-positive samples can be obtained for each event class. Negative samples for the given event class can also be obtained in two ways. First, samples that are pseudo-positive for other event classes can be considered pseudo-negative for a specific event class. Second, real videos can be selected from the web, analogous to how images are often selected as negative samples for the formation of concept detectors from web data, e.g. [38], [39]. These pseudo-positive and (pseudo) negative videos can be used to create a new machine learning concept for SVM detection mode (using any model for the evaluation of event detectors)."}, {"heading": "IV. LEARNING A VIDEO EVENT DETECTOR FROM A FEW POSITIVE AND RELATED VIDEO EXAMPLES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Exploiting related videos for learning from a few examples", "text": "As in Section II, where the problem of video event detection is discussed, it is not uncommon to be provided, or to be able to find, which relevant samples are available, i.e. videos that are closely related to a given complex event, but do not meet the exact requirements to be a true positive event instance. (RDSVM) It can be particularly interesting if only a few true positive samples are available, because conversely, when a plethora of positive samples are available, one can effectively learn from them. (RDSVM), proposed in [21] for handling \"related\" training samples, is applied in such a way that related samples are taken as weighted negative or weighted positive examples. (RDSVM extends the standard SVM algorithms so that each training sample with a trustworthy in (0, 1) the degree of relevance of each training sample with the class relateded.X (xi)."}, {"heading": "B. Treating pseudo-positive training samples as related ones", "text": "In Section III-E we discussed how to use the results of processing the textual description of an event as pseudo-positive event samples to train a standard SVM. However, if we do have some real positive videos available for training, one way to merge the two positive / pseudo-positive sets and use their association for SVM training would be to use them. However, since the pseudo-positive samples are artificially created and can therefore be corrupted by error / noise, a better option would be to treat them as \"related\" samples rather than real positives. This can be easily achieved through the RDSVM methodology presented in the previous section. The evaluation of this approach is presented in Section V-D."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Datasets and experimental setup", "text": "This year, there will be a significant reduction in greenhouse gas emissions in the first half of the year."}, {"heading": "C. Learning from a few positive and related video examples", "text": "In this section, we validate the performance of the proposed framework for processing video sequences from a few positive samples. To this end, we compare the following approaches to investigate whether and in what way it is advantageous to use these samples in training: a) with no related samples, b) with the pure negative samples as they are usually used, c) with the pure positive samples, c) with the pure positive samples, c) with the pure positive samples, c) with the labels of the related samples, c) with the pure positive samples, c) with the pure positive samples, c) with the pure positive samples."}, {"heading": "D. Combining textual event detectors with learning from a few positive and related samples", "text": "This year, it will be in a position to achieve the objectives I have mentioned."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we proposed a framework for learning video event detectors consisting solely of a text description of an event class or very few positive and related training samples. We identified a general learning framework and investigated the impact of different design decisions at different stages of that framework. To use related video samples, we used an SVM extension (RDSVM) so that related samples are automatically treated as weighted negative or positive samples. Experimental evaluation of the proposed approaches and their combination on the sophisticated, large-scale video data set TRECVID MED 2014 confirmed the applicability of the proposed methods in cases where positive samples are unavailable or scarce, and provided useful insights on how video event detectors can be trained under such conditions. 11"}, {"heading": "VII. ACKNOWLEDGMENT", "text": "This work was supported by the European Commission under the contracts FP7-600826 ForgetIT and FP7-287911 LinkedTV."}], "references": [{"title": "High-level event recognition in unconstrained videos", "author": ["Y.-G. Jiang", "S. Bhattacharya", "S.-F. Chang", "M. Shah"], "venue": "International Journal of Multimedia Information Retrieval, pp. 1\u201329, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "An overview of the goals, tasks, data, evaluation mechanisms and metrics", "author": ["P. Over", "G. Awad", "M. Michel", "J. Fiscus", "G. Sanders", "W. Kraaij", "A.F. Smeaton", "G. Quenot"], "venue": "Proc. of TRECVID 2013. NIST, USA, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "An overview of the goals, tasks, data, evaluation mechanisms and metrics", "author": ["\u2014\u2014"], "venue": "Proc. of TRECVID 2014. NIST, USA, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Event detection in baseball video using superimposed caption recognition", "author": ["D. Zhang", "S.-F. Chang"], "venue": "Proc. of the 10th ACM Int. Conf. on Multimedia. ACM, 2002, pp. 315\u2013318.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "A large-scale benchmark dataset for event recognition in surveillance video", "author": ["S. Oh", "A. Hoogs", "A. Perera", "N. Cuntoor", "C.-C. Chen", "J.T. Lee", "S. Mukherjee", "J. Aggarwal", "H. Lee", "L. Davis"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on. IEEE, 2011, pp. 3153\u20133160.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "On the prevalence of event clusters in autobiographical memory", "author": ["N.R. Brown"], "venue": "Social Cognition, vol. 23, pp. 35\u201369, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Video event detection using a subclass recoding errorcorrecting output codes framework", "author": ["N. Gkalelis", "V. Mezaris", "M. Dimopoulos", "I. Kompatsiaris", "T. Stathaki"], "venue": "Multimedia and Expo (ICME), IEEE Int. Conf. on. IEEE, 2013, pp. 1\u20136.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Video event detection using generalized subclass discriminant analysis and linear support vector machines", "author": ["N. Gkalelis", "V. Mezaris"], "venue": "Proc. of Int. Conf. on multimedia retrieval. ACM, 2014, p. 25.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "COSTA: Co-occurrence statistics for zero-shot classification", "author": ["T. Mensink", "E. Gavves", "C.G. Snoek"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on. IEEE, 2014, pp. 2441\u20132448.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Zeroshot learning with semantic output codes", "author": ["M. Palatucci", "D. Pomerleau", "G.E. Hinton", "T.M. Mitchell"], "venue": "Advances in Neural Information Processing Systems 22, Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, Eds. Curran Associates, Inc., 2009, pp. 1410\u20131418.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Write a classifier: Zero-shot learning using purely textual descriptions", "author": ["M. Elhoseiny", "B. Saleh", "A. Elgammal"], "venue": "Computer Vision (ICCV), IEEE Int. Conf. on. IEEE, 2013, pp. 2584\u20132591.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Videostory: A new multimedia embedding for few-example recognition and translation of events", "author": ["A. Habibian", "T. Mensink", "C.G. Snoek"], "venue": "Proc. of the ACM Int. Conf. on Multimedia. ACM, 2014, pp. 17\u201326.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "BBN VISER TRECVID 2014 multimedia event detection and multimedia event recounting systems", "author": ["Y. Guangnan", "L. Dong", "C. Shih-Fu", "S. Ruslan", "M. Vlad", "D. Larry", "G. Abhinav", "H. Ismail", "G. Sadiye", "M. Ashutosh"], "venue": "Proc. TRECVID Workshop, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Zeroshot event detection using multi-modal fusion of weakly supervised concepts", "author": ["S. Wu", "S. Bondugula", "F. Luisier", "X. Zhuang", "P. Natarajan"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on. IEEE, 2014, pp. 2665\u20132672.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Composite concept discovery for zero-shot video event detection", "author": ["A. Habibian", "T. Mensink", "C.G. Snoek"], "venue": "Proc. of Int. Conf. on Multimedia Retrieval. ACM, 2014, p. 17.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Zero-example event search using multimodal pseudo relevance feedback", "author": ["L. Jiang", "T. Mitamura", "S.-I. Yu", "A.G. Hauptmann"], "venue": "Proc. of Int. Conf. on Multimedia Retrieval. ACM, 2014, p. 297.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Bridging the ultimate semantic gap: A semantic search engine for internet videos", "author": ["L. Jiang", "S.-I. Yu", "D. Meng", "T. Mitamura", "A.G. Hauptmann"], "venue": "Int. Conf. on Multimedia Retrieval, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Informedia at TRECVID 2014 MED and MER", "author": ["S.-I. Yu", "L. Jiang", "Z. Mao", "X. Chang", "X. Du", "C. Gan", "Z. Lan", "Z. Xu", "X. Li", "Y. Cai"], "venue": "NIST TRECVID Video Retrieval Evaluation Workshop, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The 2014 SESAME multimedia event detection and recounting system", "author": ["R. Bolles", "B. Burns", "J. Herson"], "venue": "Proc. TRECVID Workshop, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "ITI-CERTH participation to TRECVID 2014", "author": ["N. Gkalelis", "F. Markatopoulou", "A. Moumtzidou", "D. Galanopoulos", "K. Avgerinakis", "N. Pittaras", "S. Vrochidis", "V. Mezaris", "I. Kompatsiaris", "I. Patras"], "venue": "Proc. TRECVID Workshop, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving event detection using related videos and relevance degree support vector machines", "author": ["C. Tzelepis", "N. Gkalelis", "V. Mezaris", "I. Kompatsiaris"], "venue": "Proceedings of the 21st ACM Int. Conf. on Multimedia. ACM, 2013, pp. 673\u2013676.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision, vol. 60, no. 2, pp. 91\u2013110, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Evaluating color descriptors for object and scene recognition", "author": ["K.E. Van De Sande", "T. Gevers", "C.G. Snoek"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 32, no. 9, pp. 1582\u2013 1596, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "MoSIFT: Recognizing human actions in surveillance videos", "author": ["M.-y. Chen", "A. Hauptmann"], "venue": "Technical Report.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 0}, {"title": "Image classification with the fisher vector: Theory and practice", "author": ["J. S\u00e1nchez", "F. Perronnin", "T. Mensink", "J. Verbeek"], "venue": "International Journal of Computer Vision, vol. 105, no. 3, pp. 222\u2013245, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving the fisher kernel for large-scale image classification", "author": ["F. Perronnin", "J. S\u00e1nchez", "T. Mensink"], "venue": "Computer Vision\u2013ECCV 2010. Springer, 2010, pp. 143\u2013156.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "The devil is in the details: an evaluation of recent feature encoding methods", "author": ["K. Chatfield", "V. Lempitsky", "A. Vedaldi", "A. Zisserman"], "venue": "British Machine Vision Conference, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition (CVPR), IEEE Conf. on. IEEE, 2009, pp. 248\u2013255.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "SRI-Sarnoff AURORA system at TRECVID 2014 multimedia event detection and recounting", "author": ["H. Cheng", "J. Liu", "I. Chakraborty", "G. Chen", "Q. Liu", "M. Elhoseiny", "G. Gan", "A. Divakaran", "H. Sawhney", "J. Allan", "J. Foley", "M. Shah", "A. Dehghan", "M. Witbrock", "J. Curtis"], "venue": "Proc. TRECVID Workshop, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "The INRIA-LIM-VocR and AXES submissions to TRECVID 2014 multimedia event detection", "author": ["M. Douze", "D. Oneata", "M. Paulin", "C. Leray", "N. Chesneau", "D. Potapov", "J. Verbeek", "K. Alahari", "Z. Harchaoui", "L. Lamel", "J.-L. Gauvain", "C.A. Schmidt", "C. Schmid"], "venue": "2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal knowledge-based analysis in multimedia event detection", "author": ["E. Younessian", "T. Mitamura", "A. Hauptmann"], "venue": "Proc. of the 2nd ACM Int. Conf. on Multimedia Retrieval. ACM, 2012, pp. 51:1\u201351:8.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Computing semantic relatedness using wikipedia-based explicit semantic analysis.", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "in IJCAI,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Introduction to modern information", "author": ["G. Salton", "M.J. McGill"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1986}, {"title": "Mining of massive datasets", "author": ["J. Leskovec", "A. Rajaraman", "J.D. Ullman"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "EasyESA: A low-effort infrastructure for explicit semantic analysis", "author": ["D. Carvalho", "C. Call\u0131", "A. Freitas", "E. Curry"], "venue": "Proc. of the 13th Int. Semantic Web Conf. (ISWC), 2014, pp. 177\u2013180.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Negative pseudo-relevance feedback in content-based video retrieval", "author": ["R. Yan", "A.G. Hauptmann", "R. Jin"], "venue": "Proc. of the 11th ACM Int. Conf. on Multimedia. ACM, 2003, pp. 343\u2013346.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "Bootstrapping visual categorization with relevant negatives", "author": ["X. Li", "C.G. Snoek", "M. Worring", "D. Koelma", "A.W. Smeulders"], "venue": "IEEE Transactions on Multimedia, vol. 15, no. 4, pp. 933\u2013945, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Incorporating prior knowledge with weighted margin support vector machines", "author": ["X. Wu", "R. Srihari"], "venue": "Proc. 10th ACM SIGKDD Int. Conf. on Knowledge discovery and data mining. ACM, 2004, pp. 326\u2013333.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2004}, {"title": "A study of smoothing methods for language models applied to information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "ACM Transactions on Information Systems (TOIS), vol. 22, no. 2, pp. 179\u2013214, 2004.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Typically, an event is defined as an interaction among humans or between humans and physical objects [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "Some examples of complex events are those defined in the Multimedia Event Detection (MED) task of the TRECVID benchmarking activity [2], [3].", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "Some examples of complex events are those defined in the Multimedia Event Detection (MED) task of the TRECVID benchmarking activity [2], [3].", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "For instance, in MED 2014 [2], the defined complex events include Attempting a bike trick, Cleaning an appliance, or Beekeeping, to name a few.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "significant attention in a wide range of applications, including video annotation and retrieval [1], video organization and summarization [4], or surveillance applications [5].", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "significant attention in a wide range of applications, including video annotation and retrieval [1], video organization and summarization [4], or surveillance applications [5].", "startOffset": 138, "endOffset": 141}, {"referenceID": 4, "context": "significant attention in a wide range of applications, including video annotation and retrieval [1], video organization and summarization [4], or surveillance applications [5].", "startOffset": 172, "endOffset": 175}, {"referenceID": 5, "context": "In [6], Brown studies how high-level events play a substantial role in the mechanism of structuring memories and recalling past experiences.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "[7], [8]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7], [8]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "The above training conditions are typically simulated by the 100Ex and 10Ex MED subtasks of TRECVID [3], where 100 and 10 positive video samples are available for training video event detectors, respectively.", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "Training based solely on a textual description of each event class is reported in a few works, mostly in the context of the TRECVID MED 0Ex and Semantic Query (SQ) subtasks [3], where no positive video examples are provided.", "startOffset": 173, "endOffset": 176}, {"referenceID": 8, "context": "For instance, due to the rapidly increasing number of images on the Web, extensive research efforts have been devoted in multi-label, zero-example (or few-example) classification in images [9].", "startOffset": 189, "endOffset": 192}, {"referenceID": 9, "context": "Similarly, a method for zero-example classification of fMRI data was proposed in [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "In [11], a method for predicting unseen image classes from a textual description, using knowledge transfer from textual to visual features, was proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "In [12] this problem is addressed by transforming both the event\u2019s textual description and the visual content of un-classified videos in a high dimensional concept-based representation, using a large pool of concept detectors; then relevant videos are retrieved by computing the similarities between these representations.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "Similarly, in [13], each event", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "In [14], multiple low-level representations", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "In [15], logical operators are used to discover different types of composite concepts, which leads to better event detection performance.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Moreover, in [16], a relevance feedback approach is used in order to improve event detection results in the zero-example problem using features computed from several modalities.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "In [17], E-Lamp was proposed, which is a zero-example event detection system made of four subsystems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "10) positive training examples [18], [19], [20], [21].", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "10) positive training examples [18], [19], [20], [21].", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "10) positive training examples [18], [19], [20], [21].", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "10) positive training examples [18], [19], [20], [21].", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "In [18], visual static (e.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "SIFT [22], Transformed Color Histogram [23]), and motion (e.", "startOffset": 5, "endOffset": 9}, {"referenceID": 22, "context": "SIFT [22], Transformed Color Histogram [23]), and motion (e.", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": "MoSIFT [24], Improved Dense Trajectories [18]) descriptors are used, along with the Fisher Vector encoding scheme [25], [26], [27].", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "MoSIFT [24], Improved Dense Trajectories [18]) descriptors are used, along with the Fisher Vector encoding scheme [25], [26], [27].", "startOffset": 41, "endOffset": 45}, {"referenceID": 24, "context": "MoSIFT [24], Improved Dense Trajectories [18]) descriptors are used, along with the Fisher Vector encoding scheme [25], [26], [27].", "startOffset": 114, "endOffset": 118}, {"referenceID": 25, "context": "MoSIFT [24], Improved Dense Trajectories [18]) descriptors are used, along with the Fisher Vector encoding scheme [25], [26], [27].", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "MoSIFT [24], Improved Dense Trajectories [18]) descriptors are used, along with the Fisher Vector encoding scheme [25], [26], [27].", "startOffset": 126, "endOffset": 130}, {"referenceID": 27, "context": "ASR and OCR techniques are also used for exploiting audio and textual information in video streams, as well as audio features and visual features based in DCNNs trained in ImageNet [28].", "startOffset": 181, "endOffset": 185}, {"referenceID": 28, "context": "layers [29], providing high discriminative power [5].", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "layers [29], providing high discriminative power [5].", "startOffset": 49, "endOffset": 52}, {"referenceID": 18, "context": "In SESAME [19], the authors also use DCNN classification scores as video features,", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "In [13], [30], [20], a similar training framework for learning from a few positive samples is", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "In [13], [30], [20], a similar training framework for learning from a few positive samples is", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "In [13], [30], [20], a similar training framework for learning from a few positive samples is", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "This is simulated in the TRECVID MED task [3] by the \u201cnear-miss\u201d video examples provided for each target event class.", "startOffset": 42, "endOffset": 45}, {"referenceID": 30, "context": "Differently from our method, none of the above works takes full advantage of these related videos for learning from few positive samples; instead, the \u201crelated\u201d samples are either excluded from the training procedure, or they are treated as true positive or true negative instances [32].", "startOffset": 282, "endOffset": 286}, {"referenceID": 31, "context": "We assume that the only knowledge available, with respect to each event class, is a textual description of it, which consists of a title, a free-form text, and a list of possible visual and audio cues, as in [33], [16].", "startOffset": 208, "endOffset": 212}, {"referenceID": 15, "context": "We assume that the only knowledge available, with respect to each event class, is a textual description of it, which consists of a title, a free-form text, and a list of possible visual and audio cues, as in [33], [16].", "startOffset": 214, "endOffset": 218}, {"referenceID": 13, "context": "content of the videos that we want to examine, similarly to [14], [18], we a) use a pool of Nc concepts along with their titles and in some cases a limited number of subtitles (e.", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "content of the videos that we want to examine, similarly to [14], [18], we a) use a pool of Nc concepts along with their titles and in some cases a limited number of subtitles (e.", "startOffset": 66, "endOffset": 70}, {"referenceID": 32, "context": "Subsequently, for each word in ELM and each word in each one of CLMs we calculate the Explicit Semantic Analysis (ESA) distance [34] between them.", "startOffset": 128, "endOffset": 132}, {"referenceID": 33, "context": "Tf-Idf) adopted for transforming this textual information in a Bag of Words (BoW) [35] representation.", "startOffset": 82, "endOffset": 86}, {"referenceID": 34, "context": "[36], this resulting in six different types of CLMs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "To this end, we use the ESA semantic relatedness measure [37], which calculates the similarity distance between two terms by computing the cosine similarity between their weighted vectors of Wikipedia articles.", "startOffset": 57, "endOffset": 61}, {"referenceID": 36, "context": "[38], [39].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38], [39].", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "In this section, Relevance Degree Support Vector Machine (RDSVM), proposed in [21] for handling \u201crelated\u201d training samples, is employed such that related samples are taken into", "startOffset": 78, "endOffset": 82}, {"referenceID": 20, "context": "For the exploitation of the related observations, as proposed in [21], [40], each training sample xi is associated with a adjustable confidence value vi, and a monotonically increasing function g(vi) (called slack normalization function) is used to weight each slack variable \u03bei denoting the loss introduced by a misclassified sample.", "startOffset": 65, "endOffset": 69}, {"referenceID": 38, "context": "For the exploitation of the related observations, as proposed in [21], [40], each training sample xi is associated with a adjustable confidence value vi, and a monotonically increasing function g(vi) (called slack normalization function) is used to weight each slack variable \u03bei denoting the loss introduced by a misclassified sample.", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "In [21], this function is modified so that only related class observations are associated with a confidence value ci \u2208 (0, 1] (called hereafter relevance degree) indicating the degree of relevance of the i-th observation with the class it is related.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In this study, we follow the approach proposed in [21] for handling the related samples as a subclass of the positive or negative class, for which a global relevance degree, c = ci \u2200i, is assigned to all related samples.", "startOffset": 50, "endOffset": 54}, {"referenceID": 28, "context": "More specifically, a 16-layer pre-trained deep ConvNet network provided in [29] was used.", "startOffset": 75, "endOffset": 79}, {"referenceID": 27, "context": "the ImageNet data [28] and provides scores for 1000 concepts.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "We compare the proposed method with the E-Lamp framework, a state-of-the-art system that participated in the TRECVID 2014 MED task [18] and is described in detail in [17].", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "We compare the proposed method with the E-Lamp framework, a state-of-the-art system that participated in the TRECVID 2014 MED task [18] and is described in detail in [17].", "startOffset": 166, "endOffset": 170}, {"referenceID": 31, "context": "Concerning the MS stage, we compared with the following retrieval methods: the Vector Space Model [33], the", "startOffset": 98, "endOffset": 102}, {"referenceID": 39, "context": "Okapi BM25, and two unigram language models with JelinekMercer smoothing (LM-JM) and Dirichlet smoothing (LMDL) respectively [41].", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "the event detection creation methods of [17], the exact word seems to perform considerably better than the others (but much worse than the proposed method).", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "It should be noted that the number of visual concepts used in [17] is significantly greater than the 1000 concepts used throughout our experiments, and other modalities (e.", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "audio) are also exploited in [17]; this explains", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "Event detector creation Similarity measures [17] Similarity measures (proposed) VSM BM25 LM-JM LM-DL Cosine Histogram Intersection WordNet - Wu 0.", "startOffset": 44, "endOffset": 48}, {"referenceID": 30, "context": "To this end, we compare the following approaches in order to investigate whether and in what way it is beneficial to use related samples in training: a) using no related samples, b) related samples are used as pure negative ones, as in [32], c) related samples are used as pure positive ones, again as in [32], and d) related samples are used as weighted negative or positive ones under the RD-SVM framework of Section IV-A, which involves an automatic procedure for selecting both the labels of the related samples and their weights.", "startOffset": 236, "endOffset": 240}, {"referenceID": 30, "context": "To this end, we compare the following approaches in order to investigate whether and in what way it is beneficial to use related samples in training: a) using no related samples, b) related samples are used as pure negative ones, as in [32], c) related samples are used as pure positive ones, again as in [32], and d) related samples are used as weighted negative or positive ones under the RD-SVM framework of Section IV-A, which involves an automatic procedure for selecting both the labels of the related samples and their weights.", "startOffset": 305, "endOffset": 309}, {"referenceID": 20, "context": "For the approaches that use related samples, 10 such samples were chosen from the pool of 25 related samples that are available for each event class; these, as suggested in [21], were selected as the 10 that are the nearest to the median of all 25 related samples in the employed feature space.", "startOffset": 173, "endOffset": 177}, {"referenceID": 30, "context": "We observe that when related samples are used as pure negative or pure positive ones as in [32], the overall detection performance is lower than the baseline approach which does not use these samples at all (P10).", "startOffset": 91, "endOffset": 95}, {"referenceID": 20, "context": "Automatically selecting to use related samples as weighted negatives or weighted positives using RD-SVM [21], on the other hand, is better than excluding them from the annotation process for 19 out of the 30 event classes.", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "The above confirm our hypothesis that treating related samples as weighted negatives/positives using RD-SVM [21] can lead to better event detection performance.", "startOffset": 108, "endOffset": 112}, {"referenceID": 30, "context": "Experimental Scenario P10 (baseline) Related as negatives (as in [32]) Related as positives (as in [32]) R10 (proposed)", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "Experimental Scenario P10 (baseline) Related as negatives (as in [32]) Related as positives (as in [32]) R10 (proposed)", "startOffset": 99, "endOffset": 103}], "year": 2015, "abstractText": "In this work we deal with the problem of high-level event detection in video. Specifically, we study the challenging problems of i) learning to detect video events from solely a textual description of the event, without using any positive video examples, and ii) additionally exploiting very few positive training samples together with a small number of \u201crelated\u201d videos. For learning only from an event\u2019s textual description, we first identify a general learning framework and then study the impact of different design choices for various stages of this framework. For additionally learning from example videos, when true positive training samples are scarce, we employ an extension of the Support Vector Machine that allows us to exploit \u201crelated\u201d event videos by automatically introducing different weights for subsets of the videos in the overall training set. Experimental evaluations performed on the large-scale TRECVID MED 2014 video dataset provide insight on the effectiveness of the proposed methods.", "creator": "LaTeX with hyperref package"}}}