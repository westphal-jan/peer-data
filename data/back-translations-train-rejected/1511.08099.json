{"id": "1511.08099", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Strategic Dialogue Management via Deep Reinforcement Learning", "abstract": "Artificially intelligent agents equipped with strategic skills that can negotiate during their interactions with other natural or artificial agents are still underdeveloped. This paper describes a successful application of Deep Reinforcement Learning (DRL) for training intelligent agents with strategic conversational skills, in a situated dialogue setting. Previous studies have modelled the behaviour of strategic agents using supervised learning and traditional reinforcement learning techniques, the latter using tabular representations or learning with linear function approximation. In this study, we apply DRL with a high-dimensional state space to the strategic board game of Settlers of Catan---where players can offer resources in exchange for others and they can also reply to offers made by other players. Our experimental results report that the DRL-based learnt policies significantly outperformed several baselines including random, rule-based, and supervised-based behaviours. The DRL-based policy has a 53% win rate versus 3 automated players (`bots'), whereas a supervised player trained on a dialogue corpus in this setting achieved only 27%, versus the same 3 bots. This result supports the claim that DRL is a promising framework for training dialogue systems, and strategic agents with negotiation abilities.", "histories": [["v1", "Wed, 25 Nov 2015 15:48:59 GMT  (712kb,D)", "http://arxiv.org/abs/1511.08099v1", "NIPS'15 Workshop on Deep Reinforcement Learning"]], "COMMENTS": "NIPS'15 Workshop on Deep Reinforcement Learning", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["heriberto cuay\\'ahuitl", "simon keizer", "oliver lemon"], "accepted": false, "id": "1511.08099"}, "pdf": {"name": "1511.08099.pdf", "metadata": {"source": "CRF", "title": "Strategic Dialogue Management via Deep Reinforcement Learning", "authors": ["Heriberto Cuay\u00e1huitl", "Simon Keizer"], "emails": ["hc213@hw.ac.uk", "s.keizer@hw.ac.uk", "o.lemon@hw.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own, \"he said in an interview with the\" New York Times, \"in which he sided with the\" New York Times. \""}, {"heading": "2 Background", "text": "A Reinforcement Learning (RL) agent learns its behavior from interacting with an environment and the physical or virtual Agent Q = Q-Q-rt function contained therein, which maps situations by maximizing a long-term reward signal [29, 30]. An RL agent is typically characterized by: (i) a finite or infinite series of states S = {si}; (ii) a finite or infinite series of actions A = {aj}; (iii) a stochastic state transition function T (s, s), which specifies the next state s \u2032 in light of the current state s \u2032 and the current action a; (iv) a reward function R (a, s \u2032), which gives the agent the reward for selecting an action when the environment makes a transition from state s \u2032 to state s \u2032; and (v) a policy between s \u2032 s \u2032, S \u2192 A, which defines a mapping of states and action a."}, {"heading": "3 Policy Learning for Strategic Interaction", "text": "Our approach to strategic interaction optimizes two tasks in common: learning to offer offers and learning to respond to offers. Furthermore, our approach learns from limited search spaces and not from unlimited ones, which leads to faster learning and also to learning only from legal (allowed) decisions."}, {"heading": "3.1 Learning to offer and to reply", "text": "A strategic agent must offer a trade to his opposing agents (or players). In the case of the Settlers of Catan game, an example of a trade offer is that I will give each one a sheep for clay. Several things can be observed from this simple example. Firstly, it should be noted that this offer may include several toxic and challenging resources; secondly, it should be noted that the offer is aimed at all opponents (as opposed to an opponent, which could also be possible); thirdly, not all offers are allowed at a certain point in the game - they depend on the state of the game and the resources available to the player for trading; the agent's goal is to make legal offers that will result in the largest payout in the long term; and a strategic agent must also respond to trade offers made by an opponent. In the case of the Settlers of Catan game, the answers to these answers may be limited to (a) accepting the offer, (b) rejecting a counter offer, or (c) responding to a counter offer."}, {"heading": "3.2 Deep Learning from constrained action sets", "text": "While the behavior of a strategic agent can be trained as described above, using deep learning with large action sets can be prohibitively expensive in terms of computation time. Our solution to this limitation is to learn from limited action sets, rather than whole and static action sets. We distinguish between two action sets, an Ar action set, which contains reactions to trade negotiations and remains static, and an Ao action set, which contains those trade negotiations that are valid at a given time in the game (i.e. the player cannot produce because of the resources he has). We refer to the latter action set as A-o, which contains a dynamic set of trade negotiations that are available according to game status and available resources (i.e. the player would not offer a specific resource if he does not have it). Therefore, we reformulate the goal of a strategic learning agent so that an optimal policy is established, so that the action selection is determined depending on the function."}, {"heading": "4 Experiments and Results", "text": "In this section, we apply the above approach to interlocutors who learn to offer and respond to Settlers of Catan in the game."}, {"heading": "4.1 Experimental Setting", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Integrated learning environment", "text": "Figure 1 (left) shows our integrated learning environment. On the left, the JSettlers Benchmark Framework [33] receives an action (trade offer or response) and issues the next game status and numerical reward. On the right, a Deep Reinforcement Learning (DRL) agent receives the state and reward, updates its policies during learning, and issues an action that follows its learned policy. Our integrated system is based on a multi-threaded implementation in which each player uses a synchronized thread. In addition, this system runs under a client-server architecture in which the learning agent acts as a \"server\" and the game as a \"client.\" They communicate by exchanging messages in which the server informs the client of the action to be executed, and the client informs the server of the game status and the observed reward. Our DRL agents are based on the ConvNetJS tool [15], which implements the \"Deep with Learning\" algorithm proposed by Q-play [22]."}, {"heading": "4.1.2 Characterisation of the learning agent", "text": "The state space S = {si} of our learner includes 160 non-binary characteristics describing the game board and available resources. Table 1 describes the state variables representing the entry nodes that we normalize into the range [0.. 1] These characteristics represent a high-dimensional state space - which can only be enhanced by enhancing learning with the approximation.2The code of this substantial expansion with an illustrative dialogue system is available at the following link: https: / github.com / cuayahuitl / SimpleDSThe action space includes 70 actions for offering trade negotiations 3 and 3 actions 4 for responding to offers from opponents."}, {"heading": "4.2 Experimental Results", "text": "This year it is more than ever before in the history of the city."}, {"heading": "5 Related Work", "text": "In fact, most of them will be able to move to a different world in which they are able than to another world in which they are able, in which they live, in which they live and in which they live."}, {"heading": "6 Concluding Remarks", "text": "The contribution of this paper is the first application of Deep Reinforcement Learning (DRL) to optimize the behavior of strategic interlocutors. Our learning agents are capable of: (i) figuring out what trade negotiations have to offer; (ii) figuring out when to accept, reject or reject an offer; (iii) detecting strategic behavior based on limited approaches - i.e., selecting actions from legal actions rather than from all of them; and (iv) learning highly competitive behavior against different types of adversaries, all supported by a comprehensive evaluation of three DRL agents trained on three baselines (random, heuristic, and monitored) analyzed from a cross-evaluation perspective. Our experimental results show that all DRL agents significantly outperform all baseline agents; our results are evidence that DRL can provide a promising framework for training in the behavior of complex strategic, active, interactive agents similar to, for example, the other strategic agents above."}, {"heading": "Acknowledgments", "text": "Funding from the European Research Council's (ERC) STAC: Strategic Conversation project No 269427 is gratefully accepted, see http: / / www.irit.fr / STAC /. Funding from ESPRC's EP / M01553X / 1 \"BABBLE\" project is gratefully accepted, see https: / / sites. google.com / site / hwinteractionlab / babble."}], "references": [{"title": "Commitments, beliefs and intentions in dialogue", "author": ["N. Asher", "A. Lascarides"], "venue": "Proc. of SemDial,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Strategic conversation", "author": ["N. Asher", "A. Lascarides"], "venue": "Semantics and Pragmatics, 6(2):1\u201362,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning", "author": ["A. Criminisi", "J. Shotton", "E. Konukoglu"], "venue": "Foundations and Trends in Computer Graphics and Vision, 7(2-3),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to trade in strategic board games", "author": ["H. Cuay\u00e1huitl", "S. Keizer", "O. Lemon"], "venue": "IJCAI Workshop on Computer Games (IJCAI-CGW),", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning trading negotiations using manually and automatically labelled data", "author": ["H. Cuay\u00e1huitl", "S. Keizer", "O. Lemon"], "venue": "International Conference on Tools with Artificial Intelligence (ICTAI),", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine learning for interactive systems and robots: A brief introduction. In Proceedings of the 2nd Workshop on Machine Learning for Interactive Systems: Bridging the Gap Between Perception, Action and Communication, MLIS", "author": ["H. Cuay\u00e1huitl", "M. van Otterlo", "N. Dethlefs", "L. Frommberger"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Hierarchical reinforcement learning for situated natural language generation", "author": ["N. Dethlefs", "H. Cuay\u00e1huitl"], "venue": "Natural Language Engineering, 21, 5", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Online learning and mining human play in complex games", "author": ["M.S. Dobre", "A. Lascarides"], "venue": "IEEE Conference on Computational Intelligence and Games, CIG,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to manage risk in non-cooperative dialogues", "author": ["I. Efstathiou", "O. Lemon"], "venue": "SemDial,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning non-cooperative dialogue behaviours", "author": ["I. Efstathiou", "O. Lemon"], "venue": "SIGDIAL,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Reinforcement learning of argumentation dialogue policies in negotiation", "author": ["K. Georgila", "D. Traum"], "venue": "Proc. of INTERSPEECH,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Game strategies for The Settlers of Catan", "author": ["M. Guhe", "A. Lascarides"], "venue": "2014 IEEE Conference on Computational Intelligence and Games, CIG,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "The elements of statistical learning: data mining, inference and prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Springer, 2 edition,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "ConvNetJS: Javascript library for deep learning", "author": ["A. Karpathy"], "venue": "http://cs.stanford.edu/people/karpathy/convnetjs/,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Trade Negotiation Policies in Strategic Conversation", "author": ["S. Keizer", "H. Cuay\u00e1huitl", "O. Lemon"], "venue": "Workshop on the Semantics and Pragmatics of Dialogue: goDIAL,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive Natural Language Generation in Dialogue using Reinforcement Learning", "author": ["O. Lemon"], "venue": "Proc. SEMDIAL,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Can automated agents proficiently negotiate with humans? Commun", "author": ["R. Lin", "S. Kraus"], "venue": "ACM, 53(1), Jan.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Move evaluation in go using deep convolutional neural networks", "author": ["C.J. Maddison", "A. Huang", "I. Sutskever", "D. Silver"], "venue": "CoRR, abs/1412.6564,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "10 great board games for traders", "author": ["M. McFarlin"], "venue": "Futures Magazine,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533, 02", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML), pages 807\u2013814,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, September", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement Learning of Multi-Issue Negotiation Dialogue Policies", "author": ["A. Papangelis", "K. Georgila"], "venue": "Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGdial),", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning of strategies for settlers of catan", "author": ["M. Pfeiffer"], "venue": "International Conference on Computer Games: Artificial Intelligence, Design and Education,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Machine learning for interactive systems: Challenges and future trends", "author": ["O. Pietquin", "M. Lopez"], "venue": "Proceedings of the Workshop Affect, Compagnon Artificiel (WACAI),", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "A Taxonomy of Robot Deception and its Benefits in HRI", "author": ["J. Shim", "R. Arkin"], "venue": "Proc. IEEE Systems, Man, and Cybernetics Conference,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["C. Szepesv\u00e1ri"], "venue": "Morgan and Claypool Publishers,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Monte-carlo tree search in settlers of catan", "author": ["I. Szita", "G. Chaslot", "P. Spronck"], "venue": "Proceedings of the 12th International Conference on Advances in Computer Games, ACG\u201909,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Temporal difference learning and TD-Gammon", "author": ["G. Tesauro"], "venue": "Commun. ACM, 38(3),", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1995}, {"title": "Java Settlers: a research environment for studying multi-agent negotiation", "author": ["R. Thomas", "K.J. Hammond"], "venue": "Intelligent User Interfaces (IUI), pages 240\u2013240,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Real-time decision making for adversarial environments using a plan-based heuristic", "author": ["R.S. Thomas"], "venue": "PhD thesis, Northwestern University,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "Extended abstract: Computational models of non-cooperative dialogue", "author": ["D. Traum"], "venue": "Proc. of SIGdial Workshop on Discourse and Dialogue,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "While typical conversations of artificial agents assume cooperative behaviour from partner conversants, strategic conversation does not assume full cooperation during the interaction between agents [2].", "startOffset": 198, "endOffset": 201}, {"referenceID": 19, "context": "Popular board games of this kind include Last Will, Settlers of Catan, and Power Grid, among others [20].", "startOffset": 100, "endOffset": 104}, {"referenceID": 32, "context": "The trading behaviours of AI agents in computer games are usually based on carefully tuned rules [33], search algorithms such", "startOffset": 97, "endOffset": 101}, {"referenceID": 30, "context": "as Monte-Carlo tree search [31, 9], and reinforcement learning with tabular representations [12, 11] or linear function approximation [26, 25].", "startOffset": 27, "endOffset": 34}, {"referenceID": 8, "context": "as Monte-Carlo tree search [31, 9], and reinforcement learning with tabular representations [12, 11] or linear function approximation [26, 25].", "startOffset": 27, "endOffset": 34}, {"referenceID": 11, "context": "as Monte-Carlo tree search [31, 9], and reinforcement learning with tabular representations [12, 11] or linear function approximation [26, 25].", "startOffset": 92, "endOffset": 100}, {"referenceID": 10, "context": "as Monte-Carlo tree search [31, 9], and reinforcement learning with tabular representations [12, 11] or linear function approximation [26, 25].", "startOffset": 92, "endOffset": 100}, {"referenceID": 25, "context": "as Monte-Carlo tree search [31, 9], and reinforcement learning with tabular representations [12, 11] or linear function approximation [26, 25].", "startOffset": 134, "endOffset": 142}, {"referenceID": 24, "context": "as Monte-Carlo tree search [31, 9], and reinforcement learning with tabular representations [12, 11] or linear function approximation [26, 25].", "startOffset": 134, "endOffset": 142}, {"referenceID": 28, "context": "A Reinforcement Learning (RL) agent learns its behaviour from interaction with an environment and the physical or virtual agents within it, where situations are mapped to actions by maximizing a long-term reward signal [29, 30].", "startOffset": 219, "endOffset": 227}, {"referenceID": 29, "context": "A Reinforcement Learning (RL) agent learns its behaviour from interaction with an environment and the physical or virtual agents within it, where situations are mapped to actions by maximizing a long-term reward signal [29, 30].", "startOffset": 219, "endOffset": 227}, {"referenceID": 21, "context": "To induce the Q function above we use Deep Reinforcement Learning as in [22], which approximates Q\u2217 using a multilayer convolutional neural network.", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "This process is implemented in the learning algorithm Deep Q-Learning with Experience Replay described in [22].", "startOffset": 106, "endOffset": 110}, {"referenceID": 32, "context": "(left) GUI of the board game \u201cSettlers of Catan\u201d [33].", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "On the left-hand side, the JSettlers benchmark framework [33] receives an action (trading offer or response) and outputs the next game state and numerical reward.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "Our DRL agents are based on the ConvNetJS tool [15], which implements the algorithm \u2018Deep Q-Learning with experience replay\u2019 proposed by [22].", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "Our DRL agents are based on the ConvNetJS tool [15], which implements the algorithm \u2018Deep Q-Learning with experience replay\u2019 proposed by [22].", "startOffset": 137, "endOffset": 141}, {"referenceID": 32, "context": "The state transition function of our agents is based on the game itself using the JSettlers framework [33].", "startOffset": 102, "endOffset": 106}, {"referenceID": 22, "context": "The hidden layers use RELU (Rectified Linear Units) activation functions to normalise their weights, see [23] for details.", "startOffset": 105, "endOffset": 109}, {"referenceID": 33, "context": "\u2022 Heu: This agent chooses trading negotiation offers and replies to offers from opponents as dictated by the heuristic bots included in the JSettlers framework5, see [34, 13] for details.", "startOffset": 166, "endOffset": 174}, {"referenceID": 12, "context": "\u2022 Heu: This agent chooses trading negotiation offers and replies to offers from opponents as dictated by the heuristic bots included in the JSettlers framework5, see [34, 13] for details.", "startOffset": 166, "endOffset": 174}, {"referenceID": 2, "context": "\u2022 Sup: This agent chooses trading negotiation offers using a random forest classifier [3, 14], and replies to offers from opponents using the heuristic behaviour above.", "startOffset": 86, "endOffset": 93}, {"referenceID": 13, "context": "\u2022 Sup: This agent chooses trading negotiation offers using a random forest classifier [3, 14], and replies to offers from opponents using the heuristic behaviour above.", "startOffset": 86, "endOffset": 93}, {"referenceID": 3, "context": ") is the posterior distribution of the bth tree, and Z is a normalisation constant [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "7%\u2014according to a 10-fold cross-validation evaluation [5, 6].", "startOffset": 54, "endOffset": 60}, {"referenceID": 5, "context": "7%\u2014according to a 10-fold cross-validation evaluation [5, 6].", "startOffset": 54, "endOffset": 60}, {"referenceID": 12, "context": "The baseline trading agent referred to as \u2018heuristic\u2019 included the following parameters, see [13]: TRY N BEST BUILD PLANS:0, FAVOUR DEV CARDS:-5.", "startOffset": 93, "endOffset": 97}, {"referenceID": 31, "context": "[32] proposes reinforcement learning with multilayer neural networks for training an agent to play the game of Backgammon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] proposes hierarchical reinforcement learning for automatic decision making on object-placing and trading actions in the game of Settlers of Catan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] used reinforcement learning in non-cooperative dialogue, and focus on a small 2-player trading problem with 3 resource types, but without using any real human dialogue data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "they believe such statements) but also against adversaries who can detect manipulation and can punish the player for being manipulative [10].", "startOffset": 136, "endOffset": 140}, {"referenceID": 15, "context": "More recently, [16] designed an MDP model for selecting trade offers, trained and evaluated within the full jSettlers environment (4 players, 5 resource types).", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "Other related work has been carried out in the context of automated non-cooperative dialogue systems, where an agent may act to satisfy its own goals rather than those of other participants [12].", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "The game-theoretic underpinnings of non-cooperative behaviour have also been investigated [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 11, "context": "Such automated agents are of interest when trying to persuade, argue, or debate, or in the area of believable characters in video games and educational simulations [12, 28].", "startOffset": 164, "endOffset": 172}, {"referenceID": 27, "context": "Such automated agents are of interest when trying to persuade, argue, or debate, or in the area of believable characters in video games and educational simulations [12, 28].", "startOffset": 164, "endOffset": 172}, {"referenceID": 34, "context": "Another arena in which strategic conversational behaviour has been investigated is negotiation [35], where hiding information (and even outright lying) can be advantageous.", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": "[19] train a deep convolutional network for the game of Go, but it is trained in a supervised fashion rather than trained to maximise a long-term reward as in this work.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "A closely related work to ours is a DRL agent for text-based games [24].", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "Another closely related work to ours is DRL agents trained to play ATARI games [21].", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "To our knowledge, our results report the highest winning rates reported to date in the game of Settlers of Catan, see [13, 16, 9].", "startOffset": 118, "endOffset": 129}, {"referenceID": 15, "context": "To our knowledge, our results report the highest winning rates reported to date in the game of Settlers of Catan, see [13, 16, 9].", "startOffset": 118, "endOffset": 129}, {"referenceID": 8, "context": "To our knowledge, our results report the highest winning rates reported to date in the game of Settlers of Catan, see [13, 16, 9].", "startOffset": 118, "endOffset": 129}, {"referenceID": 17, "context": "Future work can for example carry out similar evaluations as above in other strategic environments, and can also extend the abilities of the agents with other strategic features [18] and forms of learning [7, 27].", "startOffset": 178, "endOffset": 182}, {"referenceID": 6, "context": "Future work can for example carry out similar evaluations as above in other strategic environments, and can also extend the abilities of the agents with other strategic features [18] and forms of learning [7, 27].", "startOffset": 205, "endOffset": 212}, {"referenceID": 26, "context": "Future work can for example carry out similar evaluations as above in other strategic environments, and can also extend the abilities of the agents with other strategic features [18] and forms of learning [7, 27].", "startOffset": 205, "endOffset": 212}, {"referenceID": 16, "context": "Last but not least, given that our learning agents trade at the semantic level, they can be extended with language understanding/generation abilities to communicate verbally [17, 8].", "startOffset": 174, "endOffset": 181}, {"referenceID": 7, "context": "Last but not least, given that our learning agents trade at the semantic level, they can be extended with language understanding/generation abilities to communicate verbally [17, 8].", "startOffset": 174, "endOffset": 181}], "year": 2015, "abstractText": "Artificially intelligent agents equipped with strategic skills that can negotiate during their interactions with other natural or artificial agents are still underdeveloped. This paper describes a successful application of Deep Reinforcement Learning (DRL) for training intelligent agents with strategic conversational skills, in a situated dialogue setting. Previous studies have modelled the behaviour of strategic agents using supervised learning and traditional reinforcement learning techniques, the latter using tabular representations or learning with linear function approximation. In this study, we apply DRL with a high-dimensional state space to the strategic board game of Settlers of Catan\u2014where players can offer resources in exchange for others and they can also reply to offers made by other players. Our experimental results report that the DRL-based learnt policies significantly outperformed several baselines including random, rule-based, and supervised-based behaviours. The DRL-based policy has a 53% win rate versus 3 automated players (\u2018bots\u2019), whereas a supervised player trained on a dialogue corpus in this setting achieved only 27%, versus the same 3 bots. This result supports the claim that DRL is a promising framework for training dialogue systems, and strategic agents with negotiation abilities.", "creator": "LaTeX with hyperref package"}}}