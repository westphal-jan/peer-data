{"id": "1511.08299", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2015", "title": "Hierarchical classification of e-commerce related social media", "abstract": "In this paper, we attempt to classify tweets into root categories of the Amazon browse node hierarchy using a set of tweets with browse node ID labels, a much larger set of tweets without labels, and a set of Amazon reviews. Examining twitter data presents unique challenges in that the samples are short (under 140 characters) and often contain misspellings or abbreviations that are trivial for a human to decipher but difficult for a computer to parse. A variety of query and document expansion techniques are implemented in an effort to improve information retrieval to modest success.", "histories": [["v1", "Thu, 26 Nov 2015 06:57:06 GMT  (463kb,D)", "http://arxiv.org/abs/1511.08299v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.CL cs.IR cs.LG", "authors": ["matthew long", "aditya jami", "ashutosh saxena"], "accepted": false, "id": "1511.08299"}, "pdf": {"name": "1511.08299.pdf", "metadata": {"source": "CRF", "title": "Hierarchical classification of e-commerce related social media", "authors": ["Matthew Long", "Aditya Jami", "Ashutosh Saxena"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Internet users post information on a topic on a number of different websites, but companies and organizations typically train their classification algorithms only on the basis of information published on their own platform. Obviously, data from competitors is often difficult to collect, but in cases where it is freely available, cross-platform analysis can only be beneficial because data from other sources can only be used if it improves performance. To make this data valuable, it must be correctly classified according to what it relates to. The goal of this project is to find a likely product category within the Amazon Browse Node hierarchy for a particular tweet. Twitter data consisted of a training data set of 58,000 tweets labeled with Amazon Browse IDs, and a much larger set of 15,000,000 tweets without labels that can be used for augmentation. Amazon data consisted of 1,900,000 reviews for products labeled with their browse node ID, and a much larger set of 15,000,000 tweets with no labeling that can be used for a particular tweet. Amazon data consisted of 1,900,000 reviews for products labeled with their browse node ID, each of which were text labeled with multiple categories, all of which were metadata sets in the original category, as well as ON categories."}, {"heading": "2 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Preprocessing", "text": "Because the data consists of text strings, a bag-of-words model was used to render the tweets. In order to reduce the feature size and trim unhelpful data, all tweets were converted to lowercase letters and all punctuation except hashtags were removed. In addition, URLs and stop words were removed from both a list within the Natural Language Toolkit and a list we developed specifically for Twitter, and words were stamped with the WordNet Lemmatizer [1]. With 5x cross-validation, which corresponded to an 80 / 20 training / test split, the unprocessed tweets consisted of 48,000 unique words that were truncated to 22,213 words after pre-processing. Text was then converted into a sparse matrix representation of TF-IDF features to be acceptable to downstream estimators. This weighting scheme was chosen because it is weighted against words that often appear in all documents and thus implicit in a document 2."}, {"heading": "2.2 Baseline Models", "text": "To evaluate the impact of our tests, we compared the performance of different learning algorithms when trained on the preprocessed dataset with all characteristics. To ensure that there were both training and testing examples for each category, a layered 5x cross-validation was used to divide the dataset into training and testing sets. Metrics associated with each classifier indicate the unweighted mean of the metrics for each category. Due to the unbalanced nature of the marked dataset, we opt for such an assessment of model quality. Vectorization of the corpus and training of the models took place using the Scikit Learn package [6]. Class weights provide a way to correct the imbalance by weighting for or against certain classes, but would be difficult to adjust for each technique we study [9]. For this reason, an unweighted linear SVM is used as the baseline on which the effectiveness of our model will be measured, although the effectiveness of our model is based."}, {"heading": "2.3 Feature Selection", "text": "Characteristics were graded according to their Anova F values and models were graded if they were trained to the top n percent of characteristics [8]. We trained models for Uniram characteristics and Uniram and Bigram characteristics. Figure 2 shows that precision and recall stabilize in the test set after about 20% of the characteristics were used in both Uniram and Uniram and Bigram cases. Since the F1 value was roughly similar in both cases and the absolute number of characteristics is much lower for a certain percentage, we decided to use 25% of the Unigram characteristics for our models."}, {"heading": "2.4 Expansion", "text": "Since tweets are shorter than typical documents, it makes sense to broaden them as they improve the vocabulary of the model [3]. To improve classification accuracy, we have considered query enhancement, where terms are added to tweets to the tests, and document enhancement, where terms are added to training tweets. Both topics are areas of research in the field of information retrieval (IR), although query enhancement is the more promising and therefore better studied field [4]."}, {"heading": "2.4.1 Document", "text": "Tweets from the training set were expanded based on the hashtags they contained and the root category to which they belonged. To perform a hashtag extension, a thesaurus was added to each tweet that contained that hashtag from the most common words in tweets that contained a specific hashtag. n randomly selected words from the top 2n words from each hashtag were then added to each tweet that contained that hashtag. No words were added from the stop lists, nor was the hashtag word. However, for the root category extension, a thesaurus was created for each category that used the words from the training set portion of the labeled tweets, and another was created for the ratings in the Amazon set. However, when building the thesaurus to expand the root category using Twitter, the top words for each category were selected using a TF-IDF weighting scheme, as the corpus was much smaller than the thesaurus that was built on."}, {"heading": "2.4.2 Query", "text": "Since the hashtag thesaurus was built from an external data set, the hashtag extension could also be used on tweets from the test set of the labeled tweets. An identical procedure for documenting the hashtag extension was used."}, {"heading": "3 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Expansion", "text": "Expanding the tweets gave mixed results in categorization, with the hashtag extension showing a slight improvement in both training and test performance, while the hashtag extension only worsened performance in each set. Amazon's node extension achieved similar results to the base case model, while expanding the Twitter nodes significantly reduced performance."}, {"heading": "3.2 Overall", "text": "In the final model, we used both hashtag documents and query extensions, and added class weights to the linear SVM classifier. The added class weighting scheme was primarily aimed at reducing the impact of the imbalance on the book category, so that this category was assigned a weight of 0.1, while other categories were weighted to 1 [9]. Additionally, the C parameter of the SVM estimator was adjusted using Scikit-Learn's GridSearch function, and a value of 5 was selected. Table 4 shows the results of our final model."}, {"heading": "4 Discussion", "text": "The model achieved an average F1 score of 0.61 in all categories with an average accuracy of 81% and an average recall rate of 54%. Categories with more tweets tended to be classified more accurately than tweets with few samples. This makes intuitive sense because the vocabulary of the samples is limited in the small categories and therefore there is a high likelihood that the samples do not contain the same words as in the training samples. This is representative of the fact that the boundary of the generalization error decreases as the sample size increases, so naturally larger categories are able to improve the accuracy of the tests. Figure 5 shows this rough trend. Query expansion is typically considered more effective than document expansion, and the only thing we have expanded in the test set were hashtags [4]. Many tweets do not contain hashtags, so the effects of query expansion were only achieved by a fraction of the test set. It is clear that this performance can be significantly improved."}, {"heading": "5 Future Work", "text": "The next step would be to build a thesaurus on single words from both Amazon and unrecognisable Twitter data to expand testing and training for word expansion. Building these thesauruses will be space-consuming, as each word will need to store the frequency of all the other words it has appeared in a tweet or review. This step is promising as it could be used for both query and document expansion and could be used for all tweets. A full word thesaurus could also explore selective expansion, expanding only certain categories. There are already existing thesauruses that can be downloaded, such as WordNet, but the frequent use of abbreviations and slang on Twitter makes building a thesaurus from a corpus of tweets potentially useful [5]. Another step that would provide immediate benefits would be to build a larger tweetus of unrepresented tweets to enhance the use of large subcategories."}], "references": [{"title": "Natural Language Processing with Python, OReilly", "author": ["S. Bird", "E. Loper", "E Klein"], "venue": "Media Inc,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Schtze"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Document Expansion Based on WordNet for Robust IR", "author": ["E. Agirre", "X. Arregi", "A. Otegi"], "venue": "Association for Computational Linguistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Document Expansion versus Query Expansion for Ad-hoc Retrieval", "author": ["B. Billerbeck", "J. Zobel"], "venue": "Proceedings of the Tenth Australasian Document Computing Symposium,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Term-weighting approaches in automatic text retrieval,Information", "author": ["G. Salton", "C. Buckley"], "venue": "Processing and Management,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1988}, {"title": "An overview of classification algorithms for imbalanced datasets, International", "author": ["V. Ganganwar"], "venue": "Journal of Emerging Technology and Advanced Engineering,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Additionally, URLs and stop words from both a list within the Natural Language Toolkit and a list we developed specifically for Twitter were removed and words were stemmed with the WordNet Lemmatizer [1][5].", "startOffset": 200, "endOffset": 203}, {"referenceID": 1, "context": "This weighting scheme was chosen because it weights against words that show up frequently across all documents and thus implicitly reflects the importance of a word in a document [2][7].", "startOffset": 179, "endOffset": 182}, {"referenceID": 4, "context": "This weighting scheme was chosen because it weights against words that show up frequently across all documents and thus implicitly reflects the importance of a word in a document [2][7].", "startOffset": 182, "endOffset": 185}, {"referenceID": 5, "context": "Class weights provide a way of correcting for imbalance by weighting for or against certain classes but would be difficult to tune for each technique we will explore[9].", "startOffset": 165, "endOffset": 168}, {"referenceID": 2, "context": "As tweets are shorter than typical documents, expanding them seems reasonable as it improves the vocabulary of the model[3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "Both topics are areas of research in Information Retrieval (IR), although query expansion is the more promising, and thus more studied field[4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "1 was applied to that category, while other categories weighted by 1[9].", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "Query expansion is typically regarded to be more effective than document expansion and the only thing we expanded in the test set were hashtags[4].", "startOffset": 143, "endOffset": 146}], "year": 2015, "abstractText": "In this paper, we attempt to classify tweets into root categories of the Amazon browse node hierarchy using a set of tweets with browse node ID labels, a much larger set of tweets without labels, and a set of Amazon reviews. Examining twitter data presents unique challenges in that the samples are short (under 140 characters) and often contain misspellings or abbreviations that are trivial for a human to decipher but difficult for a computer to parse. A variety of query and document expansion techniques are implemented in an effort to improve information retrieval to modest success.", "creator": "LaTeX with hyperref package"}}}