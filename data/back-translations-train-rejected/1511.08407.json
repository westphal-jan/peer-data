{"id": "1511.08407", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2015", "title": "The Mechanism of Additive Composition", "abstract": "We prove an upper bound for the bias of additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010), a widely used method for computing meanings of phrases by averaging the vector representations of their constituent words. The result endorses additive composition as a reasonable operation for calculating meanings of phrases, which is the first theoretical analysis on compositional frameworks from a machine learning point of view. The theory also suggests ways to improve additive compositionality, including: transforming entries of distributional word vectors by a function that meets a specific condition, constructing a novel type of vector representations to make additive composition sensitive to word order, and utilizing singular value decomposition to train word vectors.", "histories": [["v1", "Thu, 26 Nov 2015 14:58:17 GMT  (1936kb,D)", "http://arxiv.org/abs/1511.08407v1", "Submitted to Journal of Machine Learning Research; Under Review"], ["v2", "Fri, 18 Dec 2015 23:34:40 GMT  (1942kb,D)", "http://arxiv.org/abs/1511.08407v2", "Submitted to Journal of Machine Learning Research; Under Review"], ["v3", "Mon, 6 Jun 2016 04:28:21 GMT  (1939kb,D)", "http://arxiv.org/abs/1511.08407v3", "In this version we have revised the mathematical proof. Now the theory is built on more fundamental assumptions -- an assumption we call it \"Generalized Zipf's Law\" and linked to a Hierarchical Pitman-Yor Process"], ["v4", "Tue, 7 Mar 2017 02:39:58 GMT  (2002kb,D)", "http://arxiv.org/abs/1511.08407v4", "More explanations on theory and additional experiments added. Accepted by Machine Learning Journal"]], "COMMENTS": "Submitted to Journal of Machine Learning Research; Under Review", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["ran tian", "naoaki okazaki", "kentaro inui"], "accepted": false, "id": "1511.08407"}, "pdf": {"name": "1511.08407.pdf", "metadata": {"source": "CRF", "title": "The Mechanism of Additive Composition", "authors": ["Ran Tian", "Naoaki Okazaki", "Kentaro Inui"], "emails": ["tianran@ecei.tohoku.ac.jp", "okazaki@ecei.tohoku.ac.jp", "inui@ecei.tohoku.ac.jp"], "sections": [{"heading": null, "text": "Keywords: compositional distribution semantics, bias and variance, approximation error limits, natural language data"}, {"heading": "1. Introduction", "text": "The decomposition of generalization errors in the surrounding contexts of words, based on the hypothesis that something similar has happened, is one of the most fundamental insights into learning theory. Bias is caused by the low capacity of models when the training samples are assumed to be infinite, whereas variance is caused by over-adaptation to finite keywords. In this paper, we apply the analysis to a new set of problems in compositional composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010). Calculations of meanings are fundamental problems in Natural Language Processing (NLP). In recent years, vector representations have seen great success in communicating the meanings of individual words."}, {"heading": "2. Theoretical Results", "text": "A vector representation in the distribution paradigm is constructed by context functions, so that each dimension is a function of interaction with a context word. Specifically, we consider that each target is associated with its context, which is usually defined as a window of context words surrounding the target. In Table 1, for example, we show the contexts of the target word in a corpus, and then we get the distribution of context words that coincide with the target word tokens. This distribution determines the probability P (c) of a word c co-occurring with t that is used to construct the vector word in a corpus. In this paper, we use the notation t to rename the target word interactively."}, {"heading": "2.1 The Choice of Function F", "text": "This condition suggests that the asymptotic behavior of F (p) at boundary p \u2192 0 may actually be a determining factor in the additive composition of vector representations oriented toward the additive composition of vector representations. In particular, since F (p): = ln p satisfies (4) with F (p): = p or F (p): = p (p): = p (p): = p = 0.5, we expect the bias in the additive composition of vector representations using vector representations using F (p): = p or F (p): = p ln p, since these defaults are not satisfactory (4). Therefore, vector representations using F (p): = p or F (p): = p ln p are probably less compatible with the additive composition than F (p)."}, {"heading": "2.2 Handling Word Order in Additive Composition", "text": "\"We will have to handle the changes of meanings in other words that we consider inappropriate for these purposes, because we always have to handle the changes of meanings in other words.\" \"We will always have to handle the changes of meanings in other words.\" \"We will always have to handle the changes of meanings in other words.\" \"We will always prove inappropriate for this purpose, because we always have wt2 + wt2 + wt2 + wt2 + wt2 + wt2 + wt2.\" \"We will have to handle the changes of meanings in other words.\" \"We will always prove inappropriate for this purpose, because we always have wt2 + wt2 + wt2 + wt2 + wt2 + wt2 + wt2 + wt2 + wt2 + wt2 + wt2\" \"We will have to handle the changes of meanings in other words.\" \"\" We will always have wt2 + wt2. \""}, {"heading": "2.3 Dimension Reduction", "text": "In practice, people mainly use low-dimensional vectors or \"word embeddings\" to represent meanings of words, but many of these embeddings, including SGNS and GloVe, can be used as matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix (s) matrix matrix (s) matrix matrix matrix matrix matrix matrix matrix matrix matrix (s) matrix matrix matrix matrix matrix matrix matrix matrix matrix (s) matrix matrix matrix matrix matrix matrix matrix matrix (s) matrix matrix matrix matrix matrix matrix matrix matrix matrix (s) matrix matrix matrix matrix matrix) matrix matrix matrix matrix matrix matrix matrix (s) matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix (s) matrix matrix matrix matrix matrix matrix matrix matrix) matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix mat"}, {"heading": "3. Related Work", "text": "The classical approach of approaching the semantics of phrases and / or sentences is fed into a profound question of network analysis (Foltz et al., 1998; Landauer and Dutnais, 1997).Compared to other compositional processes, the vector addition / average is either a strong baseline (Mitchell and Lapata, 2008) or one of the most competitive methods until recently (Banea et al., 2014).The additive composition has been successfully integrated into several NLP systems that also relate to death. For example, Tian et al. (2014) Use of vector additives to evaluate similarities between paraphrase candidates in a logically-based Textual recognition method (e.g. the similarity between the two languages blamed for death) and \"cause for the loss of life\" is characterized by the cosmic similarity between sums of the word vblame + vblame + vcause + vcause + vcause + vcause + vloss + vcause + vloss [vcause + vloss] and]."}, {"heading": "4. Proof of the Bias Bound", "text": "In order to deduce the bias, we should study statistics of vector representation as a whole, rather than delving into any specific co-event probabilities. Our idea is to consider vector representation as a random sample drawn from a probability space, and P (\u00b7), P (\u00b7]], and S (\u00b7) as random variables (i.e., the functions of E | C | R). For brevity, we refer to P (\u00b7) of P (\u00b7) and s (\u00b7) of P (\u00b7). Then, by the law of large numbers, 1 | C | E2t1t2: [St1t2 \u2212 2 \u2212 2) 2 (St1 + St2) 2], (8) where the word E | C | S (\u00b7] denotes the expectation of a random variable on the Etc | C | C, and \"denotes equality at the boundary of an infinitely large corpus.\" We assume that we are converting space into."}, {"heading": "4.2 Zipf\u2019s Law", "text": "A prerequisite for our calculation is Zipf's law (Zipf, 1935), which states that the frequency of a word is inversely proportional to its rank in the frequency table. In other words, we have f1 / fn = n. Let us now consider the probability Pr (Freq \u2265 fn). By definition, there are altogether n words whose frequency is greater than fn. Meanwhile, there are altogether | C | fn = f | C | fn. (9) If we replace fn in (9) with x, we obtain the cumulative distribution function of Freq: Pr (Freq \u2265 fn) = f | C | \u00b7 fn = f | C | fn."}, {"heading": "4.3 An Assumption on P\u03c4", "text": "The Zipnian law specifies the distribution of the random variables Freq in (11), which in turn determines the distribution of P via Equation (12). We need assumptions that also indicate the distribution of the random variables Pushing, especially if Freq (c) is small, because in Zipnian law there are many context words whose frequency is small but intuitively is only a small number of c) x x x x x x x x, which are semantically related to the target variables and will have a drastic deviation from P (c)."}, {"heading": "4.4 Effects of the Function F", "text": "We characterize the function F (p) by its asymptotic behavior at p \u2192 0, which is natural because in the construction of vector representations p is replaced by small probability values. \u2212 \u2212 \u2212 \u2212 \u2212 We define an index \u03b1 associated with F to be such a behavior (p), its index corresponds to the performance \u03b1 (p) when F (p): If F (p): = ln p, we have p = 0. The index \u03b1 F is a decisive factor in determining the properties of vector representations, as we explain below. We consider the difference between F (P) and F (P). The intuition is that F (p) when F asp) is smaller has a greater inclination near 0, so that the differences between small Pig and P are amplified."}, {"heading": "4.5 An Upper Bound for the Bias of Additive Composition", "text": "The proof of our bias requires a second assumption based on the intuition that the contexts of the limited target words tk (Ptk) and tl (tk) are \"irrelevant\" because the targets are different words (i.e.) that do not occur together. Therefore, we expect F (Ptk) and F (Ptl) \u2212 P (P) to both occur with the word tk (resp. tl) as random variables. Therefore, we expect F (Ptktl) \u2212 F (P) and F (Ptk\\ tl) tktl (F) to occur somehow with the word tk (resp. tl)."}, {"heading": "5. Experimental Verification", "text": "The corpus contains about 100 million word marks, including written texts and utterances in British English. To construct vector representations, we use lemmatized words commented on in the corpus, and count the co-occurrences within context windows that do not exceed sentence boundaries. The size of the context windows is 5 on each page for a target word and 4 for a target bigram. We extract all unigrams, ordered bigrams and disordered bigrams that occur more than 200 times, resulting in 16,210 unigrams, 45,793 ordered bigrams and 45,398 disordered bigrams as targets. For the C lexicon of context words, we use the same amount of unigrams.The co-occurrences probability P (ci) is estimated by P (ci)."}, {"heading": "5.1 The Choice of Function F", "text": "In this section, we review the bias (5) for bigrams observed in the BNC and confirm the effects of function F. We normalize a (\u03c4), b (ci), and r as in section 4.1, and for each bigram tktl that occurs more than 200 times in the BNC, we draw the charts \u221a 1 2 (\u03c02k\\ l + 2 l\\ k + \u03c0k\\ l\u03c0l\\ k) as x and for each bigram tktl \u2212 1 (wtk + wtl) as y.The charts are shown in Figure 6. We demonstrate various choices for F as shown above in each chart. It confirms that if F is selected such that \u03b1 \u2264 0.5 (i.e. F (p): = ln p in (a) and F (p): = p (b), the theoretical margin of error y p (red solid lines) fits sharply on the graph."}, {"heading": "5.2 Handling Word Order in Additive Composition", "text": "For vector representations constructed from the near-distant contexts (Section 2.2), we have given a similar bias as in 3 to approximate ordered bigrams. Formalization and proof are simply performed by redefining C as the lexicon of N-F context words, with Zipf's law for this lexicon and in Adoption I., II. and Theorem 5 replacing the additive composition of Nearfar context vectors, and tl\\ tk by tk tl, (tk\\ tl) L and (tl\\ tk) R, according to the meaning. In this section, we experimentally verify the bias bound for additive composition of Nearfar context vectors, and qualitatively show that the composition can be used to evaluate similarities between ordered bigrams. In Figure 7 and Figure 8, we show that we have the default 12 (k\\ l) L + (k\\ l) LDP (l)\\ DP (2)\\ DP (LDP)\\ LDP (l)\\ LDP (l)."}, {"heading": "5.3 Dimension Reduction", "text": "In this section, we verify the prediction in Section 2.3 that word vectors trained by SVD are better able to maintain the bias (5) in the additive composition than GloVe and SGNS. In Figure 9, we use normalized word vectors constructed from ordinary contexts in the BNC and reduced to 200 dimensions by different reduction methods. We use F (p): = ln p and F (p): = \u221a p in (a) and (b), respectively, both trained by SVD. GloVe model and SGNS use F (p): = ln p and are represented in (c) and (d), respectively."}, {"heading": "6. Extrinsic Evaluations of Additive Compositionality", "text": "In this section, we test the additive composition of human annotated data sets to determine whether our theoretical predictions correlate with human judgments. We perform a phrase similarity task and a word analogy task."}, {"heading": "6.1 Phrase Similarity", "text": "In a dataset created by Mitchell and Lapata, we can compare the similarity of phrases with the cosmic similarities between two vector values. Each instance in the data is a (Phrase1, Phrase2, Similarity) function. Each instance in the data is a (Phrase1, Phrase2, Similarity) function, and each phrase consists of two words. The similarity is commented by humans, from 1 to 7, which shows how similar the meanings of the two phrases are. Phrases are divided into three categories: verb object, compound noun, and adjective noun. Each category has 108 pairs of phrases, and they are annotated by 18 human participants (i.e., 1,944 instances in each category)."}, {"heading": "6.2 Word Analogy", "text": "The task of analogy is to solve questions of form \"a is to b as c is to?,\" and an elegant approach is to find the word vector that is most similar to vb \u2212 va + vc (Mikolov et al., 2013a). For example, to answer the question \"man is king as woman is to?,\" one must calculate the key to solving analogy questions and find the most similar word vector that can probably turn out to be vqueen, which is the correct answer queen.As pointed out by Levy and Goldberg (2014a), the key to solving analogy questions is the ability to \"add\" (resp. \"subtract\") some aspects to (or from) a concept that is a concept of human qualities that has the aspects of being royal and masculine. If we can add \"the aspect of male of king and female,\" then we probably get the concept of queen."}, {"heading": "7. Conclusion", "text": "In this paper, we have developed a theory of additive composition with respect to its bias, which has explained why and how additive composition works, making useful suggestions on how additive composition can be improved, including the choice of a transformation function, word order awareness, and dimensional reduction methods. Predictions made by the theory are empirically verified and show positive correlations with human judgments. In short, we have uncovered the mechanism of additive composition; however, we note that our theory is not \"proof\" that additive composition is a \"good\" compositional framework. Since a generalizing error usually lies in machine learning theory, our limit to bias does not show whether additive composition is \"good\"; rather, it specifies some factors that can influence the errors. If we have generalization-related error limits for other compositional operations, a comparison between such boundaries can provide useful insights into compositional theory decisions."}, {"heading": "Appendix A. Proofs of Lemmas", "text": "In this section we prove all lemmas in section 4. The evidence depends on some preliminary propositions. Proposition 6 (Chebyshev's Inequality) Leave Yi (i = 1,.., k) are not random variables, and assume that the second moments E [Y 2i] exist. Then, in all cases > 0, we will havePr (k i = 1 {Y 2i] and 0 otherwise. Then, note that 2Y 2i) can be the random variable that is 1 when Y 2i."}, {"heading": "Appendix B. The Loss Function of SGNS", "text": "In this section we discuss the loss function of SGNS. The model is originally proposed as an ad hoc objective function using the negative sampling technique (Mikolov et al., 2013b), without an explicit explanation of what is optimized and what is the loss. It is later shown that SGNS is a factorization of the shifted PMI matrix (Levy and Goldberg, 2014b), but the loss function for this factorization remains unspecified. Here, we give a re-explanation of the SGNS model, with the loss function explicitly state.B.1 Noise Contrastive EstimationThe original objective function of SGNS is proposed as an adaptation of the noise contrastive method, but in fact SGNS NCE is used without any adaptation. NCE (Gutmann and Hyva, 2012) is a method for solving the classic problem that provides a pattern (xi) N (1)."}], "references": [{"title": "Simcompass: Using deep learning word embeddings to assess cross-level similarity", "author": ["Carmen Banea", "Di Chen", "Rada Mihalcea", "Claire Cardie", "Janyce Wiebe"], "venue": "In Proceedings of SemEval,", "citeRegEx": "Banea et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Banea et al\\.", "year": 2014}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Baroni and Zamparelli.,? \\Q2010\\E", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Blacoe and Lapata.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "Probabilistic topic models", "author": ["David M. Blei"], "venue": "Commun. ACM,", "citeRegEx": "Blei.,? \\Q2012\\E", "shortCiteRegEx": "Blei.", "year": 2012}, {"title": "Stochastic gradient descent tricks", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q2012\\E", "shortCiteRegEx": "Bottou.", "year": 2012}, {"title": "Error bounds for approximation with neural networks", "author": ["Martin Burger", "Andreas Neubauer"], "venue": "Journal of Approximation Theory,", "citeRegEx": "Burger and Neubauer.,? \\Q2001\\E", "shortCiteRegEx": "Burger and Neubauer.", "year": 2001}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Kenneth Ward Church", "Patrick Hanks"], "venue": "Comput. Linguist.,", "citeRegEx": "Church and Hanks.,? \\Q1990\\E", "shortCiteRegEx": "Church and Hanks.", "year": 1990}, {"title": "A context-theoretic framework for compositionality in distributional semantics", "author": ["Daoud Clarke"], "venue": "Comput. Linguist.,", "citeRegEx": "Clarke.,? \\Q2012\\E", "shortCiteRegEx": "Clarke.", "year": 2012}, {"title": "Power-law distributions in empirical data", "author": ["Aaron Clauset", "Cosma Rohilla Shalizi", "M.E.J. Newman"], "venue": "SIAM Rev.,", "citeRegEx": "Clauset et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Clauset et al\\.", "year": 2009}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark"], "venue": "Linguistic Analysis,", "citeRegEx": "Coecke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Similarity-based estimation of word cooccurrence probabilities", "author": ["Ido Dagan", "Fernando Pereira", "Lillian Lee"], "venue": "In Proceedings of ACL,", "citeRegEx": "Dagan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1994}, {"title": "General estimation and evaluation of compositional distributional semantic models", "author": ["Georgiana Dinu", "Nghia The Pham", "Marco Baroni"], "venue": "In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,", "citeRegEx": "Dinu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "The measurement of textual coherence with latent semantic analysis", "author": ["Peter W. Foltz", "Walter Kintsch", "Thomas K. Landauer"], "venue": "Discourse Process,", "citeRegEx": "Foltz et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Foltz et al\\.", "year": 1998}, {"title": "Neural networks and the bias/variance dilemma", "author": ["Stuart Geman", "Elie Bienenstock", "Ren\u00e9 Doursat"], "venue": "Neural Comput.,", "citeRegEx": "Geman et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1992}, {"title": "Approximation error bounds via rademachers complexity", "author": ["Giorgio Gnecco", "Marcello Sanguineti"], "venue": "Applied Mathematical Sciences,", "citeRegEx": "Gnecco and Sanguineti.,? \\Q2008\\E", "shortCiteRegEx": "Gnecco and Sanguineti.", "year": 2008}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Grefenstette and Sadrzadeh.,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "A regression model of adjective-noun compositionality in distributional semantics", "author": ["Emiliano Guevara"], "venue": "In Proceedings of the Workshop on GEometrical Models of Natural Language Semantics,", "citeRegEx": "Guevara.,? \\Q2010\\E", "shortCiteRegEx": "Guevara.", "year": 2010}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Michael U. Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2012}, {"title": "Extension of zipf\u2019s law to words and phrases", "author": ["Le Quan Ha", "E.I. Sicilia-Garcia", "Ji Ming", "F.J. Smith"], "venue": "In Proceedings of Coling,", "citeRegEx": "Ha et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ha et al\\.", "year": 2002}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.G. Martinsson", "J.A. Tropp"], "venue": "SIAM Rev.,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Proceedings of ACL,", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Perplexity on reduced corpora", "author": ["Hayato Kobayashi"], "venue": "In Proceedings of ACL,", "citeRegEx": "Kobayashi.,? \\Q2014\\E", "shortCiteRegEx": "Kobayashi.", "year": 2014}, {"title": "On the computational basis of learning and cognition: Arguments from LSA", "author": ["Thomas K. Landauer"], "venue": "The psychology of learning and motivation,", "citeRegEx": "Landauer.,? \\Q2002\\E", "shortCiteRegEx": "Landauer.", "year": 2002}, {"title": "A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T. Dutnais"], "venue": "Psychological review,", "citeRegEx": "Landauer and Dutnais.,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dutnais.", "year": 1997}, {"title": "How well can passage meaning be derived without using word order? a comparison of latent semantic analysis and humans", "author": ["Thomas K. Landauer", "Darrell Laham", "Bob Rehder", "M.E. Schreiner"], "venue": "In Proceedings of Annual Conference of the Cognitive Science Society,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Word embeddings through Hellinger PCA", "author": ["R\u00e9mi Lebret", "Ronan Collobert"], "venue": "In Proceedings of EACL,", "citeRegEx": "Lebret and Collobert.,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Advances in NIPS,", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Trans. ACL,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Advances in NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "In Proceedings of ACL-HLT,", "citeRegEx": "Mitchell and Lapata.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell and Lapata.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Beyond the zipf-mandelbrot law in quantitative linguistics", "author": ["M.A. Montemurro"], "venue": "Physica A,", "citeRegEx": "Montemurro.,? \\Q2001\\E", "shortCiteRegEx": "Montemurro.", "year": 2001}, {"title": "Finding the best model among representative compositional models", "author": ["Masayasu Muraoka", "Sonse Shimaoka", "Kazeto Yamamoto", "Yotaro Watanabe", "Naoaki Okazaki", "Kentaro Inui"], "venue": "In Proceedings of PACLIC,", "citeRegEx": "Muraoka et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Muraoka et al\\.", "year": 2014}, {"title": "Generalization bounds for function approximation from scattered noisy data", "author": ["Partha Niyogi", "Federico Girosi"], "venue": "Advances in Computational Mathematics,", "citeRegEx": "Niyogi and Girosi.,? \\Q1999\\E", "shortCiteRegEx": "Niyogi and Girosi.", "year": 1999}, {"title": "A note on the delta method", "author": ["Gary W. Oehlert"], "venue": "The American Statistician,", "citeRegEx": "Oehlert.,? \\Q1992\\E", "shortCiteRegEx": "Oehlert.", "year": 1992}, {"title": "A practical and linguisticallymotivated approach to compositional distributional semantics", "author": ["Denis Paperno", "Nghia The Pham", "Marco Baroni"], "venue": "In Proceedings of ACL,", "citeRegEx": "Paperno et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Rothe and Sch\u00fctze.,? \\Q2015\\E", "shortCiteRegEx": "Rothe and Sch\u00fctze.", "year": 2015}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["Karl Stratos", "Michael Collins", "Daniel Hsu"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Stratos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stratos et al\\.", "year": 2015}, {"title": "A hierarchical bayesian language model based on pitman-yor processes", "author": ["Yee Whye Teh"], "venue": "In Proceedings of ACL,", "citeRegEx": "Teh.,? \\Q2006\\E", "shortCiteRegEx": "Teh.", "year": 2006}, {"title": "Logical inference on dependency-based compositional semantics", "author": ["Ran Tian", "Yusuke Miyao", "Takuya Matsuzaki"], "venue": "In Proceedings of ACL,", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Mining the web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["Peter D. Turney"], "venue": "In Proceedings of EMCL,", "citeRegEx": "Turney.,? \\Q2001\\E", "shortCiteRegEx": "Turney.", "year": 2001}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Turney and Pantel.,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Estimating linear models for compositional distributional semantics", "author": ["Fabio Massimo Zanzotto", "Ioannis Korkontzelos", "Francesca Fallucchi", "Suresh Manandhar"], "venue": "In Proceedings of Coling,", "citeRegEx": "Zanzotto et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2010}, {"title": "The Psychobiology of Language: An Introduction to Dynamic Philology", "author": ["George K. Zipf"], "venue": null, "citeRegEx": "Zipf.,? \\Q1935\\E", "shortCiteRegEx": "Zipf.", "year": 1935}], "referenceMentions": [{"referenceID": 14, "context": "Abstract We prove an upper bound for the bias of additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010), a widely used method for computing meanings of phrases by averaging the vector representations of their constituent words.", "startOffset": 70, "endOffset": 145}, {"referenceID": 25, "context": "Abstract We prove an upper bound for the bias of additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010), a widely used method for computing meanings of phrases by averaging the vector representations of their constituent words.", "startOffset": 70, "endOffset": 145}, {"referenceID": 34, "context": "Abstract We prove an upper bound for the bias of additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010), a widely used method for computing meanings of phrases by averaging the vector representations of their constituent words.", "startOffset": 70, "endOffset": 145}, {"referenceID": 15, "context": "Introduction The decomposition of generalization errors into bias and variance (Geman et al., 1992) is one of the most profound insights of learning theory.", "startOffset": 79, "endOffset": 99}, {"referenceID": 14, "context": "We prove an upper bound for the bias of a widely used compositional framework, the additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010).", "startOffset": 104, "endOffset": 179}, {"referenceID": 25, "context": "We prove an upper bound for the bias of a widely used compositional framework, the additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010).", "startOffset": 104, "endOffset": 179}, {"referenceID": 34, "context": "We prove an upper bound for the bias of a widely used compositional framework, the additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010).", "startOffset": 104, "endOffset": 179}, {"referenceID": 30, "context": "It has been shown that the cosine similarities of such word vectors can capture similarities between words considerably well (Levy et al., 2015), thus the next natural question is to extend the vector representations to phrases and even sentences.", "startOffset": 125, "endOffset": 144}, {"referenceID": 1, "context": "Some are complicated methods that rely on the underlying linguistic structures of phrases (Baroni and Zamparelli, 2010; Socher et al., 2012; Paperno et al., 2014), which are rationalized by linguistic intuitions (Coecke et al.", "startOffset": 90, "endOffset": 162}, {"referenceID": 42, "context": "Some are complicated methods that rely on the underlying linguistic structures of phrases (Baroni and Zamparelli, 2010; Socher et al., 2012; Paperno et al., 2014), which are rationalized by linguistic intuitions (Coecke et al.", "startOffset": 90, "endOffset": 162}, {"referenceID": 39, "context": "Some are complicated methods that rely on the underlying linguistic structures of phrases (Baroni and Zamparelli, 2010; Socher et al., 2012; Paperno et al., 2014), which are rationalized by linguistic intuitions (Coecke et al.", "startOffset": 90, "endOffset": 162}, {"referenceID": 9, "context": ", 2014), which are rationalized by linguistic intuitions (Coecke et al., 2010).", "startOffset": 57, "endOffset": 78}, {"referenceID": 34, "context": "Others have sought supports from cognitive science by comparing with human judgments (Mitchell and Lapata, 2010).", "startOffset": 85, "endOffset": 112}, {"referenceID": 14, "context": "The most widely used framework is the additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997), in which the meanings of phrases are calculated by averaging the word vectors.", "startOffset": 59, "endOffset": 107}, {"referenceID": 25, "context": "The most widely used framework is the additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997), in which the meanings of phrases are calculated by averaging the word vectors.", "startOffset": 59, "endOffset": 107}, {"referenceID": 12, "context": "Such theories would be important for the pursuit of better compositional frameworks, given that the empirical evaluation of compositional frameworks is already considerably complicated, due to different choices of the word vectors, the composition operations, and the methods for estimating parameters (Dinu et al., 2013).", "startOffset": 302, "endOffset": 321}, {"referenceID": 24, "context": "It has been shown that the cosine similarities of such word vectors can capture similarities between words considerably well (Levy et al., 2015), thus the next natural question is to extend the vector representations to phrases and even sentences. Based on the success of distributional hypothesis, it is expected that at least for short phrases, the meanings can still be represented by vectors constructed from surrounding contexts. However, a main obstacle here is that phrases are far more sparse than individual words. For example, in the British National Corpus (BNC) (The BNC Consortium, 2007), which contains about 100 million word tokens, there are about 16000 lemmatized words which occur more than 200 times, but only about 46000 such bigrams, far less than the 160002 possible two-word combinations. In other words, there are too few training samples for even two-word phrases. Therefore, a direct estimation of the surrounding contexts of a phrase can have large sampling error. Alternatively, Mitchell and Lapata (2010) propose to construct vector representations of phrases by combining word vectors, based on the linguistic intuition that meanings of phrases are \u201ccomposed\u201d from the meanings of their constituent words.", "startOffset": 126, "endOffset": 1034}, {"referenceID": 40, "context": ", 2013b), the GloVe model (Pennington et al., 2014), the Hellinger PCA (Lebret and Collobert, 2014) and a CCA model (Stratos et al.", "startOffset": 26, "endOffset": 51}, {"referenceID": 27, "context": ", 2014), the Hellinger PCA (Lebret and Collobert, 2014) and a CCA model (Stratos et al.", "startOffset": 27, "endOffset": 55}, {"referenceID": 10, "context": "Though there are other methods for constructing word vectors, such as neural networks (Collobert et al., 2011), we consider this formalization as general enough to cover a wide range of distributional word vectors in the literature, including some popular models such as SGNS and GloVe.", "startOffset": 86, "endOffset": 110}, {"referenceID": 6, "context": "Indeed, if we set F (p) := ln p and b(ci) := lnP (ci), then s(ci, \u03c4) becomes the Point-wise Mutual Information (PMI) function which has long been widely adopted in NLP (Church and Hanks, 1990; Dagan et al., 1994; Turney, 2001; Turney and Pantel, 2010).", "startOffset": 168, "endOffset": 251}, {"referenceID": 11, "context": "Indeed, if we set F (p) := ln p and b(ci) := lnP (ci), then s(ci, \u03c4) becomes the Point-wise Mutual Information (PMI) function which has long been widely adopted in NLP (Church and Hanks, 1990; Dagan et al., 1994; Turney, 2001; Turney and Pantel, 2010).", "startOffset": 168, "endOffset": 251}, {"referenceID": 47, "context": "Indeed, if we set F (p) := ln p and b(ci) := lnP (ci), then s(ci, \u03c4) becomes the Point-wise Mutual Information (PMI) function which has long been widely adopted in NLP (Church and Hanks, 1990; Dagan et al., 1994; Turney, 2001; Turney and Pantel, 2010).", "startOffset": 168, "endOffset": 251}, {"referenceID": 48, "context": "Indeed, if we set F (p) := ln p and b(ci) := lnP (ci), then s(ci, \u03c4) becomes the Point-wise Mutual Information (PMI) function which has long been widely adopted in NLP (Church and Hanks, 1990; Dagan et al., 1994; Turney, 2001; Turney and Pantel, 2010).", "startOffset": 168, "endOffset": 251}, {"referenceID": 40, "context": "The SGNS model is a matrix factorization of PMI (Levy and Goldberg, 2014b), and the more general form of shift terms a(t) and b(ci) are explicitly introduced by GloVe (Pennington et al., 2014).", "startOffset": 167, "endOffset": 192}, {"referenceID": 27, "context": "As for other forms of F , it has been reported that one can achieve better empirical results by setting F (p) := \u221a p instead of F (p) := p (Lebret and Collobert, 2014; Stratos et al., 2015) (see Section 2.", "startOffset": 139, "endOffset": 189}, {"referenceID": 43, "context": "As for other forms of F , it has been reported that one can achieve better empirical results by setting F (p) := \u221a p instead of F (p) := p (Lebret and Collobert, 2014; Stratos et al., 2015) (see Section 2.", "startOffset": 139, "endOffset": 189}, {"referenceID": 12, "context": "If COMP has parameters, it is a widely adopted approach to learn the parameters by minimizing this error Et1t2 for bigrams observed in a corpus (Dinu et al., 2013; Baroni and Zamparelli, 2010; Guevara, 2010).", "startOffset": 144, "endOffset": 207}, {"referenceID": 1, "context": "If COMP has parameters, it is a widely adopted approach to learn the parameters by minimizing this error Et1t2 for bigrams observed in a corpus (Dinu et al., 2013; Baroni and Zamparelli, 2010; Guevara, 2010).", "startOffset": 144, "endOffset": 207}, {"referenceID": 18, "context": "If COMP has parameters, it is a widely adopted approach to learn the parameters by minimizing this error Et1t2 for bigrams observed in a corpus (Dinu et al., 2013; Baroni and Zamparelli, 2010; Guevara, 2010).", "startOffset": 144, "endOffset": 207}, {"referenceID": 39, "context": "In Pennington et al. (2014), the authors noted that logarithm is a homomorphism from multiplication to addition, and used this property to justify F (p) := ln p for training semantically additive word vectors, based but on the unverified hypothesis that multiplications of co-occurrence probabilities have specialties in semantics.", "startOffset": 3, "endOffset": 28}, {"referenceID": 27, "context": "On the other hand, Lebret and Collobert (2014) proposed to use F (p) := \u221a p, which is motivated by the Hellinger distance between two probability distributions, and reported its being better than F (p) := p.", "startOffset": 19, "endOffset": 47}, {"referenceID": 27, "context": "On the other hand, Lebret and Collobert (2014) proposed to use F (p) := \u221a p, which is motivated by the Hellinger distance between two probability distributions, and reported its being better than F (p) := p. Stratos et al. (2015) proposed a similar but more general and better-motivated", "startOffset": 19, "endOffset": 230}, {"referenceID": 26, "context": "As the following famous example (Landauer et al., 1997) shows, meanings of sentences can differ greatly as the word order changes.", "startOffset": 32, "endOffset": 55}, {"referenceID": 27, "context": "Such word vectors have been used in Lebret and Collobert (2014), Stratos et al.", "startOffset": 36, "endOffset": 64}, {"referenceID": 27, "context": "Such word vectors have been used in Lebret and Collobert (2014), Stratos et al. (2015) and Levy et al.", "startOffset": 36, "endOffset": 87}, {"referenceID": 27, "context": "Such word vectors have been used in Lebret and Collobert (2014), Stratos et al. (2015) and Levy et al. (2015). For L2-loss, we can assume that \u2016Avt1 \u2212wt1\u2016 \u2264 \u03b51, \u2016Avt2 \u2212wt2\u2016 \u2264 \u03b52 and \u2016Avt1t2 \u2212wt1t2\u2016 \u2264 \u03b53, where \u03b51, \u03b52 and \u03b53 are minimized.", "startOffset": 36, "endOffset": 110}, {"referenceID": 40, "context": "GloVe The GloVe model (Pennington et al., 2014) is a dimension reduction of vector representations in which F (p) := ln p.", "startOffset": 22, "endOffset": 47}, {"referenceID": 13, "context": "To minimize this loss function, GloVe uses stochastic gradient descent methods such as AdaGrad (Duchi et al., 2011).", "startOffset": 95, "endOffset": 115}, {"referenceID": 19, "context": "The training of SGNS is based on the Noise Contrastive Estimation (NCE) (Gutmann and Hyv\u00e4rinen, 2012), therefore two inherited parameters can affect the loss function, namely the number k of noise samples per data point, and the distribution Pnoise(\u00b7) of noise.", "startOffset": 72, "endOffset": 101}, {"referenceID": 14, "context": "Additive composition is the classical approach to approximating the semantics of phrases and/or sentences (Foltz et al., 1998; Landauer and Dutnais, 1997).", "startOffset": 106, "endOffset": 154}, {"referenceID": 25, "context": "Additive composition is the classical approach to approximating the semantics of phrases and/or sentences (Foltz et al., 1998; Landauer and Dutnais, 1997).", "startOffset": 106, "endOffset": 154}, {"referenceID": 33, "context": "Compared to other composition operations, vector addition/average has either served as a strong baseline (Mitchell and Lapata, 2008), or remained one of the most competitive methods until recently (Banea et al.", "startOffset": 105, "endOffset": 132}, {"referenceID": 0, "context": "Compared to other composition operations, vector addition/average has either served as a strong baseline (Mitchell and Lapata, 2008), or remained one of the most competitive methods until recently (Banea et al., 2014).", "startOffset": 197, "endOffset": 217}, {"referenceID": 24, "context": "Word-order dependent syntactic effects on meaning have been considered as the most important lack in additive composition (Landauer, 2002).", "startOffset": 122, "endOffset": 138}, {"referenceID": 0, "context": "Compared to other composition operations, vector addition/average has either served as a strong baseline (Mitchell and Lapata, 2008), or remained one of the most competitive methods until recently (Banea et al., 2014). Additive composition has been successfully integrated into several NLP systems as well. For example, Tian et al. (2014) use vector additions for assessing meaning similarities between paraphrase candidates in a logic-based textual entailment recognition system (e.", "startOffset": 198, "endOffset": 339}, {"referenceID": 0, "context": "Compared to other composition operations, vector addition/average has either served as a strong baseline (Mitchell and Lapata, 2008), or remained one of the most competitive methods until recently (Banea et al., 2014). Additive composition has been successfully integrated into several NLP systems as well. For example, Tian et al. (2014) use vector additions for assessing meaning similarities between paraphrase candidates in a logic-based textual entailment recognition system (e.g. the similarity between \u201cblamed for death\u201d and \u201ccause loss of life\u201d is calculated by the cosine similarity between sums of word vectors vblame+vdeath and vcause+vloss+vlife); in Iyyer et al. (2015), the average of word vectors in a whole sentence/document is fed into a deep neural network for sentiment analysis and question answering, which achieves near state-of-the-art accuracies with minimum training time.", "startOffset": 198, "endOffset": 683}, {"referenceID": 33, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 49, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 1, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 9, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 17, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 42, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 39, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 33, "context": "For example, in a two-word phrase, one can first transform the two word vectors by different matrices before adding them, in which the two matrices are parameters (Mitchell and Lapata, 2008); or, regarding different syntactic roles, one can assign matrices to adjectives for modifying vectors of nouns (Baroni and Zamparelli, 2010); further, one can insert neural network layers between parents and their children in a syntactic tree (Socher et al.", "startOffset": 163, "endOffset": 190}, {"referenceID": 1, "context": "For example, in a two-word phrase, one can first transform the two word vectors by different matrices before adding them, in which the two matrices are parameters (Mitchell and Lapata, 2008); or, regarding different syntactic roles, one can assign matrices to adjectives for modifying vectors of nouns (Baroni and Zamparelli, 2010); further, one can insert neural network layers between parents and their children in a syntactic tree (Socher et al.", "startOffset": 302, "endOffset": 331}, {"referenceID": 42, "context": "For example, in a two-word phrase, one can first transform the two word vectors by different matrices before adding them, in which the two matrices are parameters (Mitchell and Lapata, 2008); or, regarding different syntactic roles, one can assign matrices to adjectives for modifying vectors of nouns (Baroni and Zamparelli, 2010); further, one can insert neural network layers between parents and their children in a syntactic tree (Socher et al., 2012).", "startOffset": 434, "endOffset": 455}, {"referenceID": 16, "context": "Error bounds in approximation schemes have been extensively studied in statistical learning theory (Vapnik, 1995; Gnecco and Sanguineti, 2008), and especially for neural networks (Niyogi and Girosi, 1999; Burger and Neubauer, 2001).", "startOffset": 99, "endOffset": 142}, {"referenceID": 37, "context": "Error bounds in approximation schemes have been extensively studied in statistical learning theory (Vapnik, 1995; Gnecco and Sanguineti, 2008), and especially for neural networks (Niyogi and Girosi, 1999; Burger and Neubauer, 2001).", "startOffset": 179, "endOffset": 231}, {"referenceID": 5, "context": "Error bounds in approximation schemes have been extensively studied in statistical learning theory (Vapnik, 1995; Gnecco and Sanguineti, 2008), and especially for neural networks (Niyogi and Girosi, 1999; Burger and Neubauer, 2001).", "startOffset": 179, "endOffset": 231}, {"referenceID": 50, "context": "Zipf\u2019s law (Zipf, 1935) is a classical observation and still finds new applications in recent results (Kobayashi, 2014).", "startOffset": 11, "endOffset": 23}, {"referenceID": 23, "context": "Zipf\u2019s law (Zipf, 1935) is a classical observation and still finds new applications in recent results (Kobayashi, 2014).", "startOffset": 102, "endOffset": 119}, {"referenceID": 44, "context": "Advanced Bayesian language models such as the hierarchical Pitman-Yor process (Teh, 2006) and the topic model (Blei, 2012) have been proposed, which we expect could further refine our theory, for example, by considering the additive compositionality of topics.", "startOffset": 78, "endOffset": 89}, {"referenceID": 3, "context": "Advanced Bayesian language models such as the hierarchical Pitman-Yor process (Teh, 2006) and the topic model (Blei, 2012) have been proposed, which we expect could further refine our theory, for example, by considering the additive compositionality of topics.", "startOffset": 110, "endOffset": 122}, {"referenceID": 1, "context": ", 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014). The ordinary approach is by introducing new parameters to represent differences in word positions or syntactic roles. For example, in a two-word phrase, one can first transform the two word vectors by different matrices before adding them, in which the two matrices are parameters (Mitchell and Lapata, 2008); or, regarding different syntactic roles, one can assign matrices to adjectives for modifying vectors of nouns (Baroni and Zamparelli, 2010); further, one can insert neural network layers between parents and their children in a syntactic tree (Socher et al., 2012). An empirical comparison of a broad range of compositional models, with an accessible introduction to the literature can be found in Blacoe and Lapata (2012). One theoretical issue of these methods, however, is that they lack learning guarantee.", "startOffset": 8, "endOffset": 868}, {"referenceID": 50, "context": "2 Zipf\u2019s Law An assumption essential to our calculation is Zipf\u2019s law (Zipf, 1935), which states that the frequency of any word is inversely proportional to its rank in the frequency table.", "startOffset": 70, "endOffset": 82}, {"referenceID": 33, "context": "We refer to Montemurro (2001), Ha et al.", "startOffset": 12, "endOffset": 30}, {"referenceID": 19, "context": "We refer to Montemurro (2001), Ha et al. (2002), and Clauset et al.", "startOffset": 31, "endOffset": 48}, {"referenceID": 8, "context": "(2002), and Clauset et al. (2009) for detailed analyses and empirical tests.", "startOffset": 12, "endOffset": 34}, {"referenceID": 38, "context": "The approach, called the \u201cdelta method\u201d (Oehlert, 1992), is as follows.", "startOffset": 40, "endOffset": 55}, {"referenceID": 33, "context": "1 Phrase Similarity In a data set1 created by Mitchell and Lapata (2010), phrase pairs are annotated with similarity scores.", "startOffset": 46, "endOffset": 73}, {"referenceID": 21, "context": "For training word vectors, we use the random projection algorithm (Halko et al., 2011) for SVD, and Stochastic Gradient Descent (SGD) (Bottou, 2012) for SGNS and GloVe.", "startOffset": 66, "endOffset": 86}, {"referenceID": 4, "context": ", 2011) for SVD, and Stochastic Gradient Descent (SGD) (Bottou, 2012) for SGNS and GloVe.", "startOffset": 55, "endOffset": 69}, {"referenceID": 4, "context": ", 2011) for SVD, and Stochastic Gradient Descent (SGD) (Bottou, 2012) for SGNS and GloVe. Since these are randomized algorithms, we run each test 20 times and report the mean performance with standard deviation. We tune SGD learning rates by checking convergence of the objectives, and get slightly better results than the default training parameters set in the software of SGNS2 and GloVe3. As pointed out by Levy et al. (2015), there are other detailed settings that can vary in SGNS and GloVe.", "startOffset": 56, "endOffset": 429}, {"referenceID": 35, "context": "Furthermore, in \u201cMuraoka et al.\u201d, we cite the best results reported by Muraoka et al. (2014), which has experimented on many different composition methods.", "startOffset": 17, "endOffset": 93}, {"referenceID": 28, "context": "As pointed out by Levy and Goldberg (2014a), the key to solving analogy questions is the ability to \u201cadd\u201d (resp.", "startOffset": 18, "endOffset": 44}, {"referenceID": 31, "context": "Thus, the vector-based solution proposed by Mikolov et al. (2013a) as above is essentially assuming that \u201cadding\u201d and \u201csubtracting\u201d aspects can be realized by adding and subtracting word vectors.", "startOffset": 44, "endOffset": 67}, {"referenceID": 7, "context": "In computational linguistics, the idea of treating semantics and semantic relations by algebraic operations on distributional context vectors is relatively new (Clarke, 2012).", "startOffset": 160, "endOffset": 174}, {"referenceID": 19, "context": "NCE (Gutmann and Hyv\u00e4rinen, 2012) is a method for solving the classical problem that, given a sample (xi) N i=1 (wherein xi \u2208 X ) drawn from an unknown probability distribution Pdata, and a function family f(\u00b7; \u03b8) : X \u2192 R\u22650 parameterized by \u03b8, to find the optimal \u03b8\u2217 such that f(x; \u03b8\u2217) approximates the distribution Pdata(x) best.", "startOffset": 4, "endOffset": 33}, {"referenceID": 31, "context": "which is exactly the objective function of SGNS proposed in Mikolov et al. (2013b).", "startOffset": 60, "endOffset": 83}], "year": 2017, "abstractText": "We prove an upper bound for the bias of additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010), a widely used method for computing meanings of phrases by averaging the vector representations of their constituent words. The result endorses additive composition as a reasonable operation for calculating meanings of phrases, which is the first theoretical analysis on compositional frameworks from a machine learning point of view. The theory also suggests ways to improve additive compositionality, including: transforming entries of distributional word vectors by a function that meets a specific condition, constructing a novel type of vector representations to make additive composition sensitive to word order, and utilizing singular value decomposition to train word vectors.", "creator": "TeX"}}}