{"id": "1511.08458", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2015", "title": "An Introduction to Convolutional Neural Networks", "abstract": "This document provides a brief introduction to Convolutional Neural Networks (CNNs), discussing recently published papers and newly form techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the workings of neural networks and have some background in Artificial Intelligence.", "histories": [["v1", "Thu, 26 Nov 2015 17:45:01 GMT  (92kb,D)", "https://arxiv.org/abs/1511.08458v1", "8 pages, 3 figures"], ["v2", "Wed, 2 Dec 2015 18:06:03 GMT  (92kb,D)", "http://arxiv.org/abs/1511.08458v2", "10 pages, 5 figures"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["keiron o'shea", "ryan nash"], "accepted": false, "id": "1511.08458"}, "pdf": {"name": "1511.08458.pdf", "metadata": {"source": "CRF", "title": "An Introduction to Convolutional Neural Networks", "authors": ["Keiron O\u2019Shea", "Ryan Nash"], "emails": ["keo7@aber.ac.uk", "nashrd@live.lancs.ac.uk"], "sections": [{"heading": null, "text": "This document provides a brief introduction to CNNs and discusses recently published essays and newly developed techniques for developing these brilliantly fantastic image recognition models. This introduction requires you to be familiar with the basics of ANNs and machine learning. Keywords: pattern recognition, artificial neural networks, machine learning, image analysis"}, {"heading": "1 Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "1.1 Overfitting", "text": "The simple answer to this question is no. This is for two reasons, one is the simple problem that we do not have unlimited computing power and time to train these huge ANNs. The second reason is to stop or reduce the effects of overfitting. Overfitting is basically when a network cannot effectively learn for a number of reasons. It is an important concept of most, if not all, machine learning algorithms, and it is important that every precaution is taken to minimize its impact.If our models show signs of overfitting, then we can detect a decreased ability to overfit generalized features not only for our training data sets, but also for our testing and prediction levels. This is the main reason for reducing the complexity of our ANNs. The fewer parameters required to train, the less likely the network will overfit - and, of course, improve the prediction performance of the model."}, {"heading": "2 CNN architecture", "text": "As mentioned above, CNNs focus primarily on the basis that the input consists of images, which concentrates the architecture that needs to be designed to best meet the need to deal with the specific type of data. One of the main differences is that the neurons that comprise the layers within the CNN consist of neurons organized into three dimensions, the spatial dimensionality of the input (height and width), and the depth. Depth refers not to the total number of layers within the ANN, but to the third dimension of an activation volume. Unlike standard ANNS, the neurons within a given layer connect only to a small region of the previous layer. In practice, this would mean that the input \"volume\" for the example given above will have a dimensionality of 64 x 64 x 3 (height, width, and depth), resulting in a final output layer that consists of a dimensionality of 1 x possible class (where n would represent the number of classes as sensed)."}, {"heading": "2.1 Overall architecture", "text": "When these layers are stacked, a CNN architecture is formed. A simplified CNN architecture for MNIST classification is shown in Figure 2. However, the basic functionality of the example above, CNN, can be divided into four key areas. 1. As in other forms of ANN, the input layer will hold the pixel values of the image. 2. The revolutionary layer will determine the output of neurons connected to local regions of input by calculating the scalar product between their weights and the region associated with the input volume. 2. The rectified linear unit (usually shortened to ReLu) aims to determine the activation function of neurons connected to the local regions of input, e.g. sigmoid to the output of the activation generated by the previous layer. 3. The pooling layer will then simply perform a reduction in time."}, {"heading": "2.2 Convolutional layer", "text": "Dre eeisrteeGsrrtee\u00fccnlhsrtee\u00fccnlhsrtee\u00fccnlhsrtee\u00fccnlhsrteee\u00fccsrlhsrtee\u00fccsrrtee\u00fccsrrrrrrsrrrrrrsrrrrrsrrrrrrrrrrrrrrrrrrllrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrllllrrrrrrrrrrrrrrrrrllllrrrlllllrrrrrlllllceteecrrrrrrrrlrrrrrrrrrrrrrrlrrlrrlrlrlrlrrrrrrlllrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrroioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioioetrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrlrlrlrlrlrlrlrlrlrlrlrlrllrlrlllrlrlrlllrlrlllllrlrlllllllllllllrlrllllrlrllllrlllllllllllllll"}, {"heading": "2.3 Pooling layer", "text": "Pooling layers aim to gradually decrease the dimensionality of the representation, thus further reducing the number of parameters and the computational complexity of the model. However, the pooling layer operates over each activation card in the input and scales its dimensionality using the \"MAX\" function, but in most CNNs these arrive in the form of layers with cores of a dimensionality of 2 x 2 that are plotted with a step of 2 along the spatial dimensions of the input. This scales the activation card down to 25% of the original size - while maintaining the depth volume to its default size. Due to the destructive nature of the pool layer, there are only two commonly observed methods of max pooling. Usually, the steps and filters of the pooling layers are both set to 2 x 2, thereby extending the layer over the whole of the pool layer."}, {"heading": "2.4 Fully-connected layer", "text": "The fully connected layer contains neurons that are directly connected to the neurons in the two adjacent layers without being connected to any layers within these layers, which is similar to the way neurons are arranged in traditional forms of ANN. (Figure 1)"}, {"heading": "3 Recipes", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to make them a reality."}, {"heading": "4 Conclusion", "text": "Convolutionary Neural Networks differ from other forms of Artificial Neural Networks in that knowledge of the specific type of input is used instead of focusing on the totality of the problem area, which in turn allows for the establishment of a much simpler network architecture. In this paper, the basic concepts of Convolutionary Neural Networks are outlined, the required layers explained, and detailed how the network can best be structured in most image analysis tasks. Research in the field of image analysis using neural networks has slowed down somewhat recently, in part due to the mistaken belief surrounding the degree of complexity and knowledge required to begin modeling these super-powerful machine learning algorithms, and the authors hope that this paper has in some way reduced this confusion and made the field more accessible to beginners."}, {"heading": "Acknowledgements", "text": "The authors thank Dr. Chuan Lu and Nicholas Dimonaco for their useful discussions and suggestions."}], "references": [{"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 3642\u20133649. IEEE", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Mitosis detection in breast cancer histology images with deep neural networks", "author": ["D.C. Cire\u015fan", "A. Giusti", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2013, pp. 411\u2013418. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L. Maria Gambardella", "J. Schmidhuber"], "venue": "IJCAI Proceedings-International Joint Conference on Artificial Intelligence. vol. 22, p. 1237", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional neural network committees for handwritten character classification", "author": ["D.C. Cire\u015fan", "U. Meier", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Document Analysis and Recognition (ICDAR), 2011 International Conference on. pp. 1135\u20131139. IEEE", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Image processing with neural networksa review", "author": ["M. Egmont-Petersen", "D. de Ridder", "H. Handels"], "venue": "Pattern recognition 35(10), 2279\u20132301", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Hardware accelerated convolutional neural networks for synthetic vision systems", "author": ["C. Farabet", "B. Martini", "P. Akselrod", "S. Talay", "Y. LeCun", "E. Culurciello"], "venue": "Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp. 257\u2013260. IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G. Hinton"], "venue": "Momentum 9(1), 926", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "3d convolutional neural networks for human action recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1), 221\u2013231", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Largescale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725\u20131732. IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems. pp. 1097\u20131105", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation 1(4), 541\u2013551", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11), 2278\u20132324", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Evaluation of convolutional neural networks for visual recognition", "author": ["C. Nebauer"], "venue": "Neural Networks, IEEE Transactions on 9(4), 685\u2013696", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "null. p. 958. IEEE", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Improving neural networks with dropout", "author": ["N. Srivastava"], "venue": "Ph.D. thesis, University of Toronto", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Pedestrian detection with convolutional neural networks", "author": ["M. Szarvas", "A. Yoshizawa", "M. Yamamoto", "J. Ogata"], "venue": "Intelligent Vehicles Symposium, 2005. Proceedings. IEEE. pp. 224\u2013229. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Deep neural networks for object detection", "author": ["C. Szegedy", "A. Toshev", "D. Erhan"], "venue": "Advances in Neural Information Processing Systems. pp. 2553\u20132561", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "A new class of convolutional neural networks (siconnets) and their application of face detection", "author": ["F.H.C. Tivive", "A. Bouzerdoum"], "venue": "Neural Networks, 2003. Proceedings of the International Joint Conference on. vol. 3, pp. 2157\u20132162. IEEE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1301.3557", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Computer Vision\u2013ECCV 2014, pp. 818\u2013833. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.", "creator": "LaTeX with hyperref package"}}}