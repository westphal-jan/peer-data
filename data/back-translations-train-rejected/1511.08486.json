{"id": "1511.08486", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2015", "title": "Distributed Machine Learning via Sufficient Factor Broadcasting", "abstract": "Matrix-parametrized models, including multiclass logistic regression and sparse coding, are used in machine learning (ML) applications ranging from computer vision to computational biology. When these models are applied to large-scale ML problems starting at millions of samples and tens of thousands of classes, their parameter matrix can grow at an unexpected rate, resulting in high parameter synchronization costs that greatly slow down distributed learning. To address this issue, we propose a Sufficient Factor Broadcasting (SFB) computation model for efficient distributed learning of a large family of matrix-parameterized models, which share the following property: the parameter update computed on each data sample is a rank-1 matrix, i.e., the outer product of two \"sufficient factors\" (SFs). By broadcasting the SFs among worker machines and reconstructing the update matrices locally at each worker, SFB improves communication efficiency --- communication costs are linear in the parameter matrix's dimensions, rather than quadratic --- without affecting computational correctness. We present a theoretical convergence analysis of SFB, and empirically corroborate its efficiency on four different matrix-parametrized ML models.", "histories": [["v1", "Thu, 26 Nov 2015 19:42:39 GMT  (8336kb,D)", "http://arxiv.org/abs/1511.08486v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["pengtao xie", "jin kyu kim", "yi zhou", "qirong ho", "abhimanu kumar", "yaoliang yu", "eric xing"], "accepted": false, "id": "1511.08486"}, "pdf": {"name": "1511.08486.pdf", "metadata": {"source": "CRF", "title": "Distributed Machine Learning via Sufficient Factor Broadcasting", "authors": ["Pengtao Xie", "Jin Kyu Kim", "Yi Zhou", "Qirong Ho", "Abhimanu Kumar", "Eric Xing"], "emails": ["pengtaox@cs.cmu.edu", "jinkyuk@andrew.cmu.edu", "yzhou35@syr.edu", "hoqirong@gmail.com", "abhimanyu.kumar@gmail.com", "epxing@cs.cmu.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2. SUFFICIENT FACTOR PROPERTY OF MATRIX-PARAMETRIZED MODELS", "text": "The core objective of Sufficient Factor Broadcasting (SFB) is to reduce network communication costs for matrix parameterized models, especially those that follow an optimization formula (P) min W1 N \u2211 i = 1 fi (Wai) + h (W) (1), where the model is parameterized by a matrix W, RJ \u00b7 D. The loss function fi (\u00b7) is typically defined by a series of training samples {(ai, bi)} Ni = 1, suppressing the dependence on bi. We allow fi (\u00b7) to be either convex or nonconvex, smooth or not smooth (with subgradients everywhere); examples include \"2 loss and multi-class logistic loss, among others. The more regularized h (W) is assumed to allow an efficient proximal operator Proxh (\u00b7) [3]. For example, h () could be an indicator function of convex."}, {"heading": "2.1 Optimization via proximal SGD and SDCA", "text": "To solve the optimization problem (P), it is common to use either (proximal) or (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and wave-stabilized parallel optimization techniques. Proximal SGD: In proximal SGD, a stochastic estimate of the gradient, 4W, can first be calculated using a data sample (or a mini batch of samples) to update W."}, {"heading": "3. SUFFICIENT FACTOR BROADCASTING", "text": "It is indeed the case that most of us are able to establish ourselves in the world, \"he said in an interview with the\" world. \"\" But it is not the case that the world is in order. \"\" It is not the case that the world is in order. \"\" It is not the case that the world is in order. \"\" It is not the case that the world is in order. \"\" It is not the case that the world is in order. \"\" It is the case that the world is in order. \"\" It is not the case that the world is in order. \"\" It is not the case that the world is in order. \"\" It is not the case that the world is in order. \"\" It is not the case that the world is in order. \"\" It is not the case that the world is in order. \"\" It is not the case that the world is in order. \""}, {"heading": "4. SUFFICIENT FACTOR BROADCASTER:", "text": "AN IMPLEMENTATION In this section, we introduce Sufficient Factor Broadcaster (SFBcaster) - an implementation of SFB - including consistency models, programming interface, and implementation details. We emphasize that SFB does not require a specific system; it can be implemented on existing distributed frameworks using any suitable communication topology - such as star3, ring, tree, full-connected, and halton sequence [21]."}, {"heading": "4.1 Flexible Consistency Models", "text": "Our CRC implementation supports three consistency models: Bulk Synchronous Parallel (BSP-SFB) > q, Asynchronous Parallel (ASP-SFB) and Stale Synchronous Parallel (SSPSFB), and we offer theoretical convergence guarantees for BSP-SFB and SSP-SFB in the next section. BSP-SFB: Under BSP [11, 23, 38] a global barrier at the end of the iteration ensures that all workers have completed their work and synchronized their parameter copies before moving on to the next iteration. BSP-SFB is a strong consistency model p that guarantees the same computational output (and thus the convergence of algorithms) of each worker. 3For example, each worker sends the CRC to a hoist, which passes it on to all other workers. ASP-SFB: ASP-SFSP can be sensitive to tighter workers."}, {"heading": "4.2 Programming Interface", "text": "To send out SF pairs (u, v), the user adds them to a buffer object list signaled by commit (). Finally, to choose between BSP, ASP, and SSP consistency, the user simply sets the shelf life to an appropriate value (0 for BSP, for ASP, for all other values for SSP). SFBcaster automatically updates the local parameter matrix of each worker using all SF pairs - including both locally compressed SF pairs added to the sv list, as well as SF pairs received by other workers. Figure 2 shows that SFBxi pseudoxi code for multicultural proximal-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-pseudoxi code for multiculary proximal-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-pseudoxi-principles-principles-principles-pseudoxi-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-pseudoxes-principles-principles-principles-principles-principles-principles-principles-principles-principles-pseudoxes-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-principles-pseudoxes-principles-principles-principles-principles-principles-principles-principles-pseudoxes-principles-principles-principles-principles-principles-principles-principles-principles"}, {"heading": "4.3 Halton Sequence Broadcast", "text": "In the previous sections, we assume that the sufficient factors generated by each worker are transmitted to all other workers. Such a complete transmission pattern incurs communication costs O (P 2), which grow quadratically with the number of workers P. In data center clusters, P can reach several thousand [10, 22], and a complete transmission scheme would be too costly. To solve this problem, we borrow the idea of the halo sequence proposed in [21] [21], in which each machine is connected to a subset of Q machines and messages are sent to them, rather than to all other machines. Q is located on logP scale, which can significantly reduce communication costs. The basic idea of the halon sequence Broadcast (HSB) [21] works as follows: Given a constructed halon sequence 44http: / / en.wikipedia.org / wiki / Halton _ sequence {P / 2, P / 4, 3P / 4, 8 \u00b7 P, each machine can send Ip to sufficiently \u00b7 P \u00b7 4p)."}, {"heading": "4.4 Implementation Details", "text": "Each worker receives three threads: SF Computing Thread, Parameter Update Thread and Communication Thread. Each worker keeps a local copy of the parameter matrix and a partition of the training data. In addition, he maintains an SF Input Queue, in which the sufficient factors calculated locally and remotely received are stored, and an SF Queue Output, in which SFs are stored that are sent to other workers. In each iteration, the SF Computing Thread checks the consistency policy detailed in Section 4 of the main paper. If permitted, this thread randomly selects a minibatch of samples from the training data, calculates the SFs and pushes them into the SF Input and Output Queue. The Parameter Update Thread retrieves SFs from the SF Input Queue and uses them to update the parameter Matrix."}, {"heading": "5. COST ANALYSIS AND THEORY", "text": "We now examine the costs and convergence behavior of the CRC under synchronous and bounded-async (e.g. SSP [5, 15, 8]) consistency and show that the CRC is preferable to full matrix synchronization / communication."}, {"heading": "5.1 Cost Analysis", "text": "It is not possible to compare the cost of communication, space and time (to apply updates to W), the cost of peer-to-peer CRCs, against the full matrix synchronization (FMS). (CRCs) It is not possible to calculate the cost of processing data. (CRCs) It is not possible to calculate the cost of processing data. (CRCs) It is not possible to calculate the cost of processing data. (CRCs) It is not possible to capture the cost of processing data. (CRCs) Each worker communicates with a Halton sequential transmission scheme, each worker communicates SF pairs with Q = O (P) peers, hence the communication costs can be reduced to O. (J + D) Because SF pairs cannot be aggregated prior to transmission, the cost has a linear dependence on K. In contrast, the communication costs are reduced with Q = O (P) peers, hence the communication costs are reduced to O (P) in communication costs."}, {"heading": "6. EXPERIMENTS", "text": "We show how four popular models can be efficiently learned with the help of SFB: (1) logistic multiclass regression (MLR) and distance metric learning (DML) 5 based on SGD; (2) sparse coding (SC) based on proximal SGD; (3) '2 regulated logistic multiclass regression (L2-MLR) based on SDCA. For baselines, we compare with (a) Spark [38] for MLR and L2-MLR and (b) complete matrix synchronization (FMS) implemented on open source parameter servers [15, 22] for all four models. In FMS, the workers send update matrices to the central server, which then sends current parameter5For DML, we use the parameterization proposed in [34], in which a linear projection matrix is sufficient to use the interface RML-12 and the GHG-1 factor."}, {"heading": "6.1 Datasets and Experimental Setup", "text": "We used two datasets for our experiments: (1) ImageNet [12] ILSFRC2012 dataset, which contains 1.2 million images from 1000 categories; the images are represented with LLC functions [33], whose dimensionality is 172k; (2) Wikipedia [26] dataset, which contains 2.4 million documents from 325k categories; documents are represented with tf-idf, with a dimensionality of 20k. We kept MLR, DML, SC, L2-MLR on Wikipedia, ImageNet, ImageNet, Wikipedia datasets and / or the parameter matrices contained up to 6.5b, 8.6b, 8.6b, 6.5b entries (the largest latent for DML and the largest dictionary size for SC were both 50k); the compromise parameters in SC and L2-MLR were set to 0.001 and 0.5b."}, {"heading": "6.2 Convergence Speed and Quality", "text": "6This has the same communication complexity as [7], which sends SFs from clients to servers, but sends complete matrices from servers to clients (which dominates the total cost).Figure 7 shows the time it takes to reach a fixed target for different model variables that use BSP consistency. Figure 8 shows the results under SSP with Stalness = 20. SFB converts faster than FMS, as does Spark v1.3.17 This is because SFB has lower communication costs, hence a greater share of runtime is spent on the calculation than network services.This is shown in Figure 9, the data samples processed per second (throughput) and algorithm progress per sample for MLR, consistency and variation."}, {"heading": "6.3 Scalability", "text": "In all the following experiments, we set the number of (L2) MLR classes, the latent dimension of the DML, and the size of the SC dictionary to 325k, 50k, and 50k, respectively. Figure 11 shows that SFB scalability is about twice slower than PS [15, 22] -based C + + implementation of the FMS due to JVM and RDD overheads. 8We use samples per second instead of iterations so that different mini-battery sizes can be compared with different machines under BSP, for MLR, DML, SC, L2MLR. Figure 12 shows how SFB scales with number of machines, under SSP with shelf life = 20. In general, we observed near-linear (ideal) acceleration with a slight decline in 12 machines."}, {"heading": "6.4 Computation Time vs Network Waiting Time", "text": "In the figure, the horizontal axis corresponds to different stality values; for all stality values, CRCs require far less network maintenance time (because CRCs are much smaller than complete matrices in FMS); the computing time for CRCs is slightly longer than FMS, because (1) update matrices need to be reconstructed on each CRCs worker; and (2) CRCs require far less network maintenance time (because CRCs are much smaller than complete matrices in FMS); the computing time for CRCs is slightly longer than the computing time for stalation procedures; if the stalation time needs to be reconstructed on each CRCs worker, the stalation time is longer than the CRCs waiting for convergence (2)."}, {"heading": "6.5 Communication Cost", "text": "Figure 14 shows the communication volume of four models below BSP. As shown in the figure, the communication volume of CRCs is significantly lower than FMS and Spark. Under the BSP consistency model, CRCs and FMS share the same iteration quality and therefore require the same number of iterations to converge. Within each iteration, CRCs communicate vectors while FMS transmits matrices. As a result, the communication volume of CRCs is much lower than FMS."}, {"heading": "6.6 Halton Sequence Broadcasting", "text": "Figures 15 and 16 show the convergence time of MLR and L2-MLR in comparison to different Q at BSP and SSP respectively (stalness = 20). As observed in these two figures, a smaller Q leads to a longer convergence time. This is because a smaller Q tends not to synchronize the parameter copies for different workers and the iteration quality deteriorates. However, as long as Q is not too small, the convergence speed of the HSB is comparable to a complete transmission scheme. As shown in the figures, the convergence time of the HSB at Q \u2265 4 \u2248 log2 12 is very close to full transmission (where Q = 12). This shows that by using HSB we can reduce the communication costs from O (P 2) to O (P logP) \u2248 O (P logP) with slight abandonment of the convergence speed."}, {"heading": "7. RELATED WORKS", "text": "On the system side [10], it was proposed to reduce the communication effort by reducing the frequency of the exchange of parameters and gradients between employees and the central server. [22] Using filters to select some of the \"important\" parameters / updates for transmission to reduce the number of data entries to be transmitted. On the algorithm side, [32] and [36] investigated the trade-offs between communication and calculation in distributed dual averaging or distributed stochastic dual coordinate distribution. [28] proposed an approximate Newton-like method for achieving communication efficiency in distributed optimization. CRC is orthogonal to these existing approaches and could be combined with them to further reduce communication costs. Peer-to-peer, decentralized architectures were studied in other distributed ML frameworks [6, 9, 25, 21]."}, {"heading": "8. CONCLUSIONS AND FUTURE WORKS", "text": "In this paper, we identify the sufficient factor property of a large set of matrix-parameterized models: If these models are optimized with stochastic gradient descent or stochastic dual coordinate ascent, the update matrices are of low rank. Based on this property, we propose a computational model with a sufficient factor to efficiently handle learning these models at low communication costs. We analyze the cost and convergence characteristics of SFB and offer efficient implementation and empirical evaluations. For very large models, the size of the local parameter matrix W may exceed the storage capacity of each machine - to solve this problem, we would like to investigate partitioning W over a small number of nearby machines or the use of out-core (disk-based) storage to keep W in future work."}, {"heading": "9. ADDITIONAL AUTHORS", "text": "Other authors: Yaoliang Yu (Machine Learning Department, Carnegie Mellon University yaoliang @ cs.cmu.edu)"}, {"heading": "10. REFERENCES", "text": "In Conference on Neural Information Processing Systems, 2011. [2] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, and A. J. Smola H. Scalable inference in latent variable models. In Conference on Web Search and Data Mining, 2012. [3] A. Beck and M. Teboulle. A fast iterative shrinkage-threshold algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2009. [4] D. P. Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999. [5] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Prentice-Hall, 1989. [6] K. Bhaduri, R. Wolff, C. Giannella, and H. Karpguta. Distributed decision-tree systems peer-based data-based systems."}, {"heading": "A. PROOF OF THEOREM 1", "text": "Proof Fc (Wcp): p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p = 1, p =, p = p, p, p, p = 1, p, p, p, p (p, p, p = 1, p, p, p, p = 1, p, p, p, p = 1, p, p, p, p = 1, p, p, p = 1, p, p, p = 1, p, p, p = 1, p, p, p = 1, p, p, p = 1, p, p, p = 1, p, p, p = 1, p, p, p = 1, p, p, p = 1, p, p, p = 1, p, p, p, p = 1, p, p, p = 1, p, p, p = 1, p, p = 1, p, p, p, p = 1, p, p, p = 1, p, p = 1, p, p, p, p = 1, p, p = 1, p, p, p, p = 1, p, p, p, p = 1, p, p, p, p, p = 1, p, p, p = 1, p, p = 1, p, p, p, p, p = 1, p, p, p, p = 1, p, p, p, p, p, p, p = 1, p, p, p, p, p, p, p = 1, p, p, p, p, p, p, p, p = 1, p, p, p, p, p, p, p"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Matrix-parametrized models, including multiclass logistic<lb>regression and sparse coding, are used in machine learning<lb>(ML) applications ranging from computer vision to compu-<lb>tational biology. When these models are applied to large-<lb>scale ML problems starting at millions of samples and tens of<lb>thousands of classes, their parameter matrix can grow at an<lb>unexpected rate, resulting in high parameter synchroniza-<lb>tion costs that greatly slow down distributed learning. To<lb>address this issue, we propose a Sufficient Factor Broadcast-<lb>ing (SFB) computation model for efficient distributed learn-<lb>ing of a large family of matrix-parameterized models, which<lb>share the following property: the parameter update com-<lb>puted on each data sample is a rank-1 matrix, i.e. the outer<lb>product of two \u201csufficient factors\u201d (SFs). By broadcast-<lb>ing the SFs among worker machines and reconstructing the<lb>update matrices locally at each worker, SFB improves com-<lb>munication efficiency \u2014 communication costs are linear in<lb>the parameter matrix\u2019s dimensions, rather than quadratic \u2014<lb>without affecting computational correctness. We present a<lb>theoretical convergence analysis of SFB, and empirically cor-<lb>roborate its efficiency on four different matrix-parametrized<lb>ML models.", "creator": "LaTeX with hyperref package"}}}