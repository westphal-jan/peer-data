{"id": "1511.08552", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "Simultaneous Private Learning of Multiple Concepts", "abstract": "We investigate the direct-sum problem in the context of differentially private PAC learning: What is the sample complexity of solving $k$ learning tasks simultaneously under differential privacy, and how does this cost compare to that of solving $k$ learning tasks without privacy? In our setting, an individual example consists of a domain element $x$ labeled by $k$ unknown concepts $(c_1,\\ldots,c_k)$. The goal of a multi-learner is to output $k$ hypotheses $(h_1,\\ldots,h_k)$ that generalize the input examples.", "histories": [["v1", "Fri, 27 Nov 2015 03:57:22 GMT  (35kb,D)", "http://arxiv.org/abs/1511.08552v1", "29 pages. To appear in ITCS '16"]], "COMMENTS": "29 pages. To appear in ITCS '16", "reviews": [], "SUBJECTS": "cs.DS cs.CR cs.LG", "authors": ["mark bun", "kobbi nissim", "uri stemmer"], "accepted": false, "id": "1511.08552"}, "pdf": {"name": "1511.08552.pdf", "metadata": {"source": "CRF", "title": "Simultaneous Private Learning of Multiple Concepts", "authors": ["Mark Bun", "Kobbi Nissim", "Uri Stemmer"], "emails": ["mbun@seas.harvard.edu", "kobbi@cs.bgu.ac.il,", "kobbi@seas.harvard.edu", "stemmer@cs.bgu.ac.il"], "sections": [{"heading": null, "text": "Excluding privacy, the complexity of the sample required to learn k concepts at the same time is essentially the same as that required to learn a single concept. In the context of differential privacy, the basic strategy of learning each hypothesis independently leads to sample complexity that grows polynomically with k. In some concept courses, we give multi-learners who need fewer samples than the basic strategy. Unfortunately, however, we also set lower limits that show that even with very simple concept courses, the sample costs of private multi-learning must grow polynomically in k keywords: differential privacy, PAC learning, agnostic learning, direct sum cost-cost-cost-cost-cost-standard-cost-cost-cost-cost-on. Supported by an NDSEG scholarship and NSF Ben Ben for 12235 grants, the research was conducted partially during the author's studies. \u2020 Yu @."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "1.1 Our results", "text": "The basics of private privacy (depending on whether the learners are right or wrong, and whether the learning is done with pure or approximate differential privacy, but in this case we limit our attention to uniformly selected examples. We analyze the example complexity of multi-learning of these simple concept classes as well as general concept classes. We also look at the class arguments of the parity functions, but in this case we limit our attention to uniformly and inappropriate PAC and agnostic learning under pure and approximate different privacy. For the simplicity of reference we refer to tables with our results in Section 1.3, where we hide the dependence on privacy and accuracy."}, {"heading": "1.2 Related work", "text": "Most of our work relates to the work on private learning and its sample complexity [9, 26, 15, 19, 4, 6, 7, 22, 8, 13] and the early work on disinfection [10]. That many \"natural\" learning tasks can be performed privately was demonstrated in the early work of Blum et al. [9] and Kasiviswanathan et al. [26]. A characterization of the sample complexity of purely private learners took place in [6], in terms of a new combinatorial measure - the representational dimension, that is, in the face of a classC is the number of C needed for private learning and sufficient C (RepDim (C)). Building on [6] Feldman and Xiao [22] showed an equivalence between the representational dimension of a C concept and the randomized one-way communication complexity of the problem."}, {"heading": "1.3 Tables of results", "text": "The following tables summarize the results of this work. In the tables below C is a class of concepts (i.e., predicates) defined by domain X. | Sample complexity upper and lower bounds is given in terms of | C | and | X |. Note that for POINTX, THRESHX, and PARd we have | C | = sample complexity upper and lower bounds is given in the setting of agnostic learning and lower bounds are for the (potentially easier) setting of PAC learning. Similarly, where not explicitly mentioned, upper bounds are for proper learning and lower bounds for the (less restrictive) setting of improper learning. For simplicity, these tables hide constant and logarithmic factors as well as dependencies on learning and privacy parameters."}, {"heading": "2 Preliminaries", "text": "We remember and extend standard definitions from learning theory and differential privacy."}, {"heading": "2.1 Multi-learners", "text": "A concept (similar, hypothesis) over domainX is a predicate defined over X. A concept class (similar, hypothesis class) is a set of concepts. Definition 2.1 (generalization error). Let P (X \u00b7 {0, 1}) is a probability distribution over X \u00b7 {0, 1}. The generalization error of a hypothesis h: X \u2192 {0, 1} w.r.t. P is defined as errorP (h) = Pr (x, y).P (x).Let D (X) is a probability distribution over X and let c: x. The generalization error of the hypothesis h: X \u2192 {0, 1} w.r.t. c andD is defined as errorD (c, h) = Prx."}, {"heading": "2.2 The Sample Complexity of Multi-Learning", "text": "Excluding privacy, the sample complexities of PAC and agnostic learning are essentially characterized by a combinatorial size called the Vapnik-Chervonenkis dimension (VC)."}, {"heading": "2.2.1 The Vapnik-Chervonenkis Dimension", "text": "The Vapnik-Chervonenkis (VC) dimension of C, denotes VC (C), is the magnitude of the largest theorem, that of C. The Vapnik-Chervonenkis (VC) dimension is an important combinatorial measure of a concept class. Classical results in statistical learning theory show that the generalization error of a hypothesis h and its empirical error rate (observed on a sufficiently large sample) is similar. Definition 2.6 (Empirical Error). Let S = (xi, yi) ni = 1 (X, 1} n."}, {"heading": "2.3 Differential privacy", "text": "Two k-marked databases S, S \u00b2 (X \u00b7 {0, 1} k) n are called adjacent if they differ in a single (multi-marked) entry, i.e., | {i: (xi, y1, i,..., yk, i) 6 = (x \u2032 i, y \u2032 1, i,.., y \u2032 k, i)} | = 1.Definition 2.15 (Differential Privacy [18]). Letter A: (X \u00b7 {0, 1} k) n \u2192 (2X) k should be an algorithm that works on a k-marked database and returns k hypotheses. Let's [A (S).0 Algorithm A] & lt.- < < < < < < < < < <"}, {"heading": "2.4 Differentially Private Sanitization", "text": "A basic task in the field of differential privacy is the problem of data cleanup. In view of a database that captures the statistical properties of D (x1,.), we are primarily interested in cleansing boolean-rated functions (equivalent to queries). In view of a function c: X (0, 1) and a database D = (x1,., xn), we are primarily interested in cleansing boolean-rated functions (equivalent to queries). A function c: X (0, 1) is a database D = (x1,.) -precise sanitizer for a concept class C, if for every D-Xn that produces a database."}, {"heading": "2.5 Private learners and multi-learners", "text": "Generalizing on the concept of private learners [26], we say that an algorithm A (\u03b1, \u03b2, \u03b4) -private PAC is k learner for C if A (\u03b1, \u03b2) -PAC is k learner for C with H, and A (, \u03b4) -differentiated is private (similar to agnostic private PAC k learners). We omit the parameter k if k = 1 and the parameter \u03b4 if \u03b4 = 0.In case k = 1 we have a generic construction with sample complexity proportional to log | C |: Theorem 2.22 ([26]). Let C be a concept class, and \u03b1, \u03b2, > 0. There is a (\u03b1, \u03b2) -private agnostic correct learner for C with sample complexity O (log | C | + log 1 / \u03b2)."}, {"heading": "2.6 Private PAC learning vs. Empirical Learning", "text": "We saw in Theorem 2.11 that if an empirical k-learner A is empirically exact for a concept class C = empirically accurate, it is also an (agnostic) PAC-learner. In particular, if an empirical sample k-learner A is differentiated private, then it also serves as a differentiated private (agnostic) PAC-learner database. Generalized to a result of [13], the next theorem shows that the inversion is also true: a differentiated private (agnostic) PAC-learner database yields a private empirical k-learner database with only a constant factor increase in sample complexity. Theorem 2.24. Let's leave A is a (,) -differentiated private (\u03b1, \u03b2) -accurate (agnostic) PAC-learner database with identical factor."}, {"heading": "3 Upper Bounds on the Sample Complexity of Private Multi-Learners", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Generic Construction", "text": "In this section, we present the following general limits on the sample complexity of private k-\u03b2 \u03b2 learners 3.1. Let C be a finite concept class and let k \u2265 1. There is a proper sample (\u03b1, \u03b2,) -private PAC k learners for C with sample complexity. In addition, there is a proper sample for C with sample complexity. (k) Log learner for C with sample complexity. (k) Log learner for C with sample complexity. (k) Log learner for C with sample complexity. (k) Log learner for C with sample complexity. (\u03b2) Log learner for C with sample complexity. (\u03b2) Log learner for C with sample complexity."}, {"heading": "3.2 Upper Bounds for Approximate Private Multi-Learners", "text": "In this section, we give two examples of cases where the sample complexity of private k-learning is of the same order of magnitude as that of non-private k-learning (sample complexity does not depend on k). Our algorithms are (, \u03b4) -differentiated private and rely on stability arguments: The identity of the best k concepts as a whole vector is unlikely to change on nearby k-marked databases. Therefore, it can be published privately. The most important technical tool we use is the Adist algorithm of Smith and Thakurta [31]. Our discussion follows the treatment of [7]. Remember that a quality function q: X-X-F \u2192 N sets an optimization problem over domain X and a finite solution gap F: Given a database S-X, we find f-F that maximizes (approximately) q-database (S, f)."}, {"heading": "3.2.1 Learning Parities under the Uniform Distribution", "text": "There are one (2), (3), (3), (4), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (6), (6), (6), (5), (5), (5), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (7), (8), (8), (8, (8), 8, 8, 8, (8), (8, 8, 8, 8, (8), 8, (8), 8, 8, 8, (8), 8, 8, 8, (8), 8, 8, 8, 8, (8), 8, 8, 8, 8, 8, 8, 8, 8 (8), 8, 8, 8, 8, 8, 8, 8, 8 (8), 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, (8, 8), 8, 8, 8, 8, (8, (8), (8), (8), (8), (5, (5,"}, {"heading": "3.2.2 Learning Points", "text": "Next, we show that the class of POINTX elements (non-agnostic) k-learned (non-agnostic) k-learned with constant complexity corresponding to the non-private complexity (non-agnostic).For each domain X and each k-N, there is a (non-agnostic) database (non-agnostic) clearer for the non-private complexity O (non-agnostic).The proof is about the construction of algorithm 5. The algorithm begins with private identification (by sanitizing) of a set of O (1 / \u03b1) \"heavy\" elements in the input database that appear. Labels of such a heavy element can be identified privately (since their duplication in the database is large).The labels of a \"non-heavy\" element can be set to 0, as a target concept can be evaluated to at most one such non-heavy element, in which case the error disappears."}, {"heading": "4 Approximate Privacy Lower Bounds from Fingerprinting Codes", "text": "In this section, we will show how the users who contributed to it can still be brought up to the assumptions without making any assumptions. The link between fingerprinting codes and differential privacy was highlighted by Bun, Ullman, and Vadhan, where users have a number of other different private analyses [3, 20, 32, 13].A (full-collusion-resistant) fingerprinting code is a scheme for distributing code words.., wn to n users that can be clearly traced back to each user. Furthermore, if any group of users include their code words in a pirate codeword, then the pirate codeword can be traced back."}, {"heading": "4.1 Fingerprinting Codes", "text": "A fingerprint code (n, k) consists of a pair of random algorithms (gene, trace). The parameter n is the number of users supported by the fingerprint code, and k is the length of the code. The code book generator Gen generates a code book W: 0, 1} n \u00b7 k. Each line wi: 0, 1} k of W is the code word of the user i. For a subset T: [n] (or if it accuses no user).We define the set of code words belonging to the users in T. The accusation algorithm Trace takes as input a pirate code word w: and accuses some i: [n] (or if it accuses no user).We define the feasible set of pirate codewords belonging to the users in T."}, {"heading": "4.2 Lower Bound for Improper PAC Learning", "text": "Our lower limits for multi-learning follow from constructions of fingerprinting codes with additional structural properties. Definition 4.2. Let C be a concept class over a domain. An (n, k) fingerprinting code (gene, trace) is compatible with concept class C if it is x1,., xn, xn, so for each codebookW in support of gene, k) fingerprinting code (gene, trace) for each i = 1,.,., n and j,.,. Theorem 4.3. Suppose there is a (n, k) fingerprinting code that is certainly compatible with a concept class C. Let's have profile class 1 / 3, \u03b2, \u03b2, < 1."}, {"heading": "4.3 Lower Bound for Agnostic Learning", "text": "The result is that any fingerprinting code, especially one with optimal length, requires agnostic learning. Let's apply an agnostic concept that requires a (n, k) -trivial terminology for all non-trivial terms. Let's be a (n, k) -precise term class with at least two different concepts. Let's apply the (n, b) -trivial terminology for all non-trivial terms."}, {"heading": "5 Examples where Direct Sum is Optimal", "text": "In this section, we show several examples of cases where the direct sum is (roughly) optimal. As we have seen in section 4, with (, \u03b4) differential privacy, any non-trivial agnostic k-learner requires sample complexity. We can demonstrate a similar result for -private learners, which applies even to non-agnostic learners: Theorem 5.1. Let's have all non-trivial term classes across a domain X (i.e., C | 2). Any proper or inappropriate (\u03b1 = 12,) -private PAC learners for C require sample complexity (k /). In [5, 6] Beimel et al. we presented an agnostic regular learner for POINTX with sample complexity."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "We investigate the direct-sum problem in the context of differentially private PAC learning: What<lb>is the sample complexity of solving k learning tasks simultaneously under differential privacy, and how<lb>does this cost compare to that of solving k learning tasks without privacy? In our setting, an individual<lb>example consists of a domain element x labeled by k unknown concepts (c1, . . . ,<lb>ck). The goal of a<lb>multi-learner is to output k hypotheses (h1, . . . , hk) that generalize the input examples.<lb>Without concern for privacy, the sample complexity needed to simultaneously learn k concepts is<lb>essentially the same as needed for learning a single concept. Under differential privacy, the basic strategy<lb>of learning each hypothesis independently yields sample complexity that grows polynomially with k.<lb>For some concept classes, we give multi-learners that require fewer samples than the basic strategy.<lb>Unfortunately, however, we also give lower bounds showing that even for very simple concept classes,<lb>the sample cost of private multi-learning must grow polynomially in k.", "creator": "LaTeX with hyperref package"}}}