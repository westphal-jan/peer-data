{"id": "1511.08589", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "Shaping Proto-Value Functions via Rewards", "abstract": "In this paper, we combine task-dependent reward shaping and task-independent proto-value functions to obtain reward dependent proto-value functions (RPVFs). In constructing the RPVFs we are making use of the immediate rewards which are available during the sampling phase but are not used in the PVF construction. We show via experiments that learning with an RPVF based representation is better than learning with just reward shaping or PVFs. In particular, when the state space is symmetrical and the rewards are asymmetrical, the RPVF capture the asymmetry better than the PVFs.", "histories": [["v1", "Fri, 27 Nov 2015 09:13:04 GMT  (1911kb,D)", "http://arxiv.org/abs/1511.08589v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["chandrashekar lakshmi narayanan", "raj kumar maity", "shalabh bhatnagar"], "accepted": false, "id": "1511.08589"}, "pdf": {"name": "1511.08589.pdf", "metadata": {"source": "CRF", "title": "Shaping Proto-Value Functions via Rewards", "authors": ["Chandrashekar Lakshmi Narayanan", "Raj Kumar Maity"], "emails": [], "sections": [{"heading": null, "text": "In contrast to supervised learning, the agent performing an RL task has to learn from the rewards. However, in goal-based RL tasks, the rewards are delayed, i.e. the agents receive feedback only after reaching the target state, and such a delay can lead to poor learning rates. Reward education is the mechanism to provide additional rewards for correct behavior in non-target states and thus support the learning process.In this paper, we combine task-dependent reward design and task-independent protovalue functionality to obtain reward-dependent protovalue functionality (RPVFs).In constructing the RPVFs, we use the direct rewards available during the sampling phase, but not used in the PVF construction. We demonstrate from experiments that learning with an RPVF-based representation is better than learning with mere reward design or PVFs. Specifically, if the state space is Vetric and the VFS is better."}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Reinforcement Learning Paradigm", "text": "The dynamics of the underlying environment can be grasped in the context of the Markov decision-making process (MDP). An MDP is a 4-step < S, A, P, R >, where S is the state space, A is the action space, P is the probability transition core and R is the reward function. The probability transition core P defines the probability (s, s) of the transition from state s \u00b2 s to state s \u00b2 s is defined in the context of action a. The reward function R is a map R indicating the reward for performing an action. A in the state environment S and is denoted by ra (s, s). Agent Behavior The behavior of the agent is recorded by the way the actions do it in each state. In the MDP language, this action selection mechanism is referred to as the policy. Formally, by a policy, we mean a sequence \u00b5 =."}, {"heading": "3 Proto-Value Functions", "text": "It is generally a good idea to select basic functions by using domain knowledge. If domain knowledge does not exist, representations based on known primitive functions such as radial, polynomial, or Fourier are used. However, such representations are based on primitive functions that may not yield good results. Therefore, it is desirable to be able to select the basic functions in a task-independent manner. Proto-value functions are task-independent basic functions and are based on the topology of the random operator, the proto-value functions capture connectivity / neighborhood information. We observe that such neighborhood information is also influenced by the reward structure. Let G = (E, V) denote a graph with edge set E and the vertex set V. Let A = (Aij, j = 1) the adjacency matrix associated with Aij and j."}, {"heading": "4 Representational Policy Iteration (RPI)", "text": "The RPI algorithm [10] will be the template algorithm we will use for our experiments. As the model information is not available, the LSTD algorithm learns it from the sample histories. We will now introduce the LSPI algorithm (see algorithm 1) that uses LSTDQ (see algorithm 2), a variant of the LSTD algorithm. Algorithm 1 Representational Policy Iteration (D, \u03b1, \u0445, k, \u03c00) 1: Select the attribute matrix to become the uppermost k-own vectors. 2: for i = 0, 1, 2,."}, {"heading": "5 Reward Shaping", "text": "The most fundamental difference between reinforcement learning (RL) and supervised learning is that in RL, the agent must learn to use the feedback he receives in the form of the rewards he receives for his actions. Such feedback learning makes RL tasks more difficult than supervised learning problems, where the agent is provided with the right / right actions at the training stage of the problem. This difficulty in learning from feedback is particularly pronounced in the case of goal-based tasks, where the agent must reach the target state from all parts of the state, but the agent receives no reward at all in states other than the target state. Therefore, the behavior in states other than the target state is not clear, leading to a slower convergence of RL algorithms. In such a scenario, rewarding the correct behavior of the agent in the interstate can be helpful. This mechanism of providing external rewards for the right behavior in addition to the rewards obtained from the environment, which is called the design of rewards."}, {"heading": "6 Proto Value function shaping using rewards", "text": "The PVFs as well as the reward design, although conceptually different, ultimately help to efficiently learn value functions (PVFs). We capture the underlying neighborhood information by using the connectivity in the graphics associated with the MDP. However, in reality, we are not interested in the proximity associated with the topological neighborhood in the state space, but in the proximity of the value functions. This proximity is also influenced by the underlying reward structure. In addition, PVFs are constructed by scanning the state space, a phase in which we can also observe the immediate rewards. While in the case of goal-oriented tasks the immediate rewards are 0, this may not apply to MDPs with a general reward structure. Immediate rewards are indicators of the preferences and actions of the agent on the ground. For example, consider a goal-based MDP, but with negative rewards for certain states. Given that the fact that the agent phase in each state must move closer to the goal and the negative actions are the ones that affect the states directly."}, {"heading": "7 Experiments", "text": "We demonstrate the following results about the experiments in this section. 1) Similarity matrices other than the diffusion matrix can be used to generate characteristics: To achieve this, we show that the proto-value functions of the three-space problem (10) can be recoverd even if the walls are missing, even if the corresponding negative rewards are assigned to these cells that correspond to the \"wall\" states. In short, we show that the use of W or WR (with negative rewards) is equivalent in this case. 2) Reward design does not work with all characteristics: We show that regardless of whether the additional reward form is used or not, the profile of the learned value function is limited to the choice of the base. In particular, if the state space is symmetrical and the rewards are asymmetrical, the RPVF captures the asymmetry better than the PVFs."}, {"heading": "8 Conclusion", "text": "We combined the task-independent Proto-Value Function (PVF) and task-specific reward design to obtain Proto-Value Functions (RPVFs). RPVFs used the immediate rewards available during the sampling phase but were not used in the PVF design, and we also observed that the RPVFs performed better than the PVFs in target-based RL tasks. A prominent feature of the RPVFs was that they better understood the asymmetry of the value function induced by the reward structure than the PVFs. We can consider expanding the RPVF into continuous areas as an interesting future direction."}], "references": [{"title": "Dynamic Programming and Optimal Control, volume II", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific, Belmont,MA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Least-squares temporal difference learning", "author": ["Justin A Boyan"], "venue": "In ICML, pages 49\u201356. Citeseer,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Policy transfer using reward shaping", "author": ["Tim Brys", "Anna Harutyunyan", "Matthew E Taylor", "Ann Now\u00e9"], "venue": "In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pages 181\u2013188. International Foundation for Autonomous Agents and Multiagent Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Multi-objectivization of reinforcement learning problems by reward shaping", "author": ["Tim Brys", "Anna Harutyunyan", "Peter Vrancx", "Matthew E Taylor", "Daniel Kudenko", "Ann Now\u00e9"], "venue": "In Neural Networks (IJCNN),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Plan-based reward shaping for reinforcement learning", "author": ["Marek Grzes", "Daniel Kudenko"], "venue": "In Intelligent Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Learning shaping rewards in model-based reinforcement learning", "author": ["Marek Grzes", "Daniel Kudenko"], "venue": "In Proc. AAMAS 2009 Workshop on Adaptive Learning Agents,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Autonomous shaping: Knowledge transfer in reinforcement learning", "author": ["George Konidaris", "Andrew Barto"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Proto-value functions: A Laplacian framework for learning representation and control in Markov decision Processes", "author": ["S.S. Mahadevan", "M. Maggioni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Proto-value functions: Developmental reinforcement learning", "author": ["Sridhar Mahadevan"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y Ng", "Daishi Harada", "Stuart Russell"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Markov Decision Processes: Discrete Stochastic Programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}], "referenceMentions": [{"referenceID": 8, "context": "The proto-value functions (PVFs) [9] are bases that can be constructed in task-independent manner, and have been applied to a wide variety of domains.", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "[10] presented the representational policy iteration (RPI) algorithm by combining the PVF basis construction with least squares policy iteration (LSPI).", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Reward shaping [4, 5, 6, 7, 11, 3] is the process of providing additional rewards to the learning agent to guide its learning process.", "startOffset": 15, "endOffset": 34}, {"referenceID": 4, "context": "Reward shaping [4, 5, 6, 7, 11, 3] is the process of providing additional rewards to the learning agent to guide its learning process.", "startOffset": 15, "endOffset": 34}, {"referenceID": 5, "context": "Reward shaping [4, 5, 6, 7, 11, 3] is the process of providing additional rewards to the learning agent to guide its learning process.", "startOffset": 15, "endOffset": 34}, {"referenceID": 6, "context": "Reward shaping [4, 5, 6, 7, 11, 3] is the process of providing additional rewards to the learning agent to guide its learning process.", "startOffset": 15, "endOffset": 34}, {"referenceID": 10, "context": "Reward shaping [4, 5, 6, 7, 11, 3] is the process of providing additional rewards to the learning agent to guide its learning process.", "startOffset": 15, "endOffset": 34}, {"referenceID": 2, "context": "Reward shaping [4, 5, 6, 7, 11, 3] is the process of providing additional rewards to the learning agent to guide its learning process.", "startOffset": 15, "endOffset": 34}, {"referenceID": 0, "context": "Even in the case when agent wants to improve a given policy \u03c0, it has to evaluate Q and then substituting Q in (??) will lead to an improved policy [1].", "startOffset": 148, "endOffset": 151}, {"referenceID": 0, "context": "1In the infinite horizon discounted reward setting that we consider, one can find an SDP that is optimal [1, 12]", "startOffset": 105, "endOffset": 112}, {"referenceID": 11, "context": "1In the infinite horizon discounted reward setting that we consider, one can find an SDP that is optimal [1, 12]", "startOffset": 105, "endOffset": 112}, {"referenceID": 7, "context": "LSPI [8] makes use of least squares temporal difference learning (LSTD) [2] which computes Q\u0302 = k \u2211", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "LSPI [8] makes use of least squares temporal difference learning (LSTD) [2] which computes Q\u0302 = k \u2211", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "Consider the three-room problem [10], wherein, the agent has to move from the starting position S in the top-left side of the first room to the goal state G in the bottom-right of the third room.", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "In [10], the authors demonstrated power of the proto-value functions in approximating such a complicated value function.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "The RPI algorithm [10] will be the template algorithm that we will be using for our experiments.", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "Reward-shaping was first introduced in [11], wherein, the authors furnished the conditions under which reward shaping preserves the optimal policies.", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "Theorem 1 (Theorem 1 of [11]).", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "Further, we show that the proto-value functions of the three-room problem [10] can be recoverd even when the walls are absent if one assigns appropriate negative rewards for those cells corresponding to the \u2018wall\u2019 states.", "startOffset": 74, "endOffset": 78}], "year": 2015, "abstractText": "Learning value function is an important sub-problem in solving a given reinforcement learning task. The choice of representation for the value function directly affects learning. The most widely used representation for the value function is the linear architecture, wherein, the value function is written as a linear combination of a \u2018pre-selected\u2019 set of basis functions. In such a scenario, choosing the right basis function is crucial in achieving success. Often, the basis functions are either selected in an ad-hoc manner or their choice is based on the domain knowledge that is specific to the given RL task. However, it is desirable to be able to choose the basis functions in a task-independent manner. The proto-value functions (PVFs) are taskindependent basis functions and are based on the topology of the state space. Being eigen functions of the random walk operator, the proto-value functions capture the connectivity and neighborhood information. In contrast to supervised learning, agent performing an RL task needs to learn from the rewards. However, in goal-based RL tasks, the rewards are delayed, i.e., the agents receive feedback only after reaching the goal state and such delay can cause poor learning rates. Reward shaping is the mechanism of providing additional rewards for correct behavior in non-goal states, thereby aiding the learning process. In this paper, we combine task-dependent reward shaping and task-independent protovalue functions to obtain reward dependent proto-value functions (RPVFs). In constructing the RPVFs we are making use of the immediate rewards which are avaialble during the sampling phase but are not used in the PVF construction. We show via experiments that learning with an RPVF based representation is better than learning with just reward shaping or PVFs. In particular, when the state space is symmetrical and the rewards are asymmetrical, the RPVF capture the asymmetry better than the PVFs.", "creator": "LaTeX with hyperref package"}}}