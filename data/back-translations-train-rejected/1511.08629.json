{"id": "1511.08629", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "Category Enhanced Word Embedding", "abstract": "Distributed word representations have been demonstrated to be effective in capturing se- mantic and syntactic regularities. Unsuper- vised representation learning from large un- labeled corpora can learn similar representa- tions for those words that present similar co- occurrence statistics. Besides local occurrence statistics, global topical information is also important knowledge that may help discrimi- nate a word from another. In this paper, we in- corporate category information of documents in the learning of word representations and to learn the proposed models in a document- wise manner. Our models outperform several state-of-the-art models in word analogy and word similarity tasks. Moreover, we evaluate the learned word vectors on sentiment analysis and text classification tasks, which shows the superiority of our learned word vectors. We also learn high-quality category embeddings that reflect topical meanings.", "histories": [["v1", "Fri, 27 Nov 2015 11:38:57 GMT  (122kb,D)", "https://arxiv.org/abs/1511.08629v1", null], ["v2", "Mon, 30 Nov 2015 07:33:09 GMT  (122kb,D)", "http://arxiv.org/abs/1511.08629v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chunting zhou", "chonglin sun", "zhiyuan liu", "francis c m lau"], "accepted": false, "id": "1511.08629"}, "pdf": {"name": "1511.08629.pdf", "metadata": {"source": "CRF", "title": "Category Enhanced Word Embedding", "authors": ["Chunting Zhou", "Chonglin Sun", "Zhiyuan Liu", "Francis C.M. Lau"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to recognize themselves and understand what they are doing when they are able to survive on their own."}, {"heading": "2 Related Work", "text": "Word representation is a key component of many NLP and IR related tasks. The conventional representation of words known as \"bag-of-words\" (BOW) ignores the word order and suffers from high dimensionality, reflecting little relation and distance between words. Continuous word embedding was first proposed in (Rumelhart et al., 1988) and has become a successful representation method in many NLP applications, including machine translation (Zou et al., 2013), parsing (Socher et al., 2011), named entity recognition (Passos et al., 2014), sentiment analysis (Glorot et al., 2011), part-speech conference et al., 2011) and text classification (Le and Mikolov, 2014).Many previous work has examined how effective word embedding, embedding and embedding can capture the words."}, {"heading": "3 Methods", "text": "In this section we will show two methods for integrating knowledge of document categories into learning word embedding. First, we will introduce the CeWE model, where the context vector for predicting the middle word is enriched with document categories. Next, we will introduce the GCeWE model, based on CeWE, where word embedding and category embedding are trained together under a document-based global supervision of words within a document."}, {"heading": "3.1 Category Enhanced Word Embedding", "text": "We extend the CBOW (Mikolov et al., 2013a) architecture by including category information from each document to learn more comprehensive and extended word representations. The architecture of the CBOW model is shown in Figure 1, and its goal is to maximize the probability of the current word t, since its context window s: J (n) = V (t) log (t) log (1), where V is the size of the word vocabulary and context (t), is the set of observed context windows for the word t: CBOW essentially defines the probability p (t) with the softmax function: p (t) = exp (t)."}, {"heading": "3.2 Globally Supervised CeWE", "text": "In the model above, we only integrate category information into local windows, force inferred words to capture current information, and use word vectors under the same topic. However, an easily discernible basic assumption is that the distribution of document representation should coincide with the distribution of categories. Therefore, based on CeWE, we use document representation to predict the corresponding categories as a global overview of words, leading to our GCeWE model."}, {"heading": "3.2.1 Model Description", "text": "The objective of GCeWE consists of two parts: the first part is the same as that of the CeWE model, and the second part is to maximise the log probability of adherence to document category i in the face of document m, as follows: J (\u03b8) = V \u2211 t = 1 \u2211 s context (t) log (p (t | s, u) + M \u2211 m = 1 \u2211 i category (m) log (p (i | m))) (5) Similarly, p (i | m) is defined as: p (i | m) = exp (c T i dm) \u2211 j-C, j = i exp (c T i dm) \u2211 j-C, c T j-dm (6) where C is the size of all categories, dm is the document representation of the mth document and i-category (m).Another problem to be solved is how to effectively represent a document in order to discriminate document representation."}, {"heading": "3.2.2 Optimization with Adaptive Negative Sampler", "text": "To accelerate convergence, we use the adaptive and context-dependent negative sample suggested in (Rendle and Freudenthaler, 2014) for pairwise learning. Note that Steffen and Freudenthaler's sampling method aims to select the most informative negative samples for a particular user, and it works well in learning recommendation systems where the goal is to recommend the most relevant items for a user. It is analogous to selecting the most informative negative categories for a document. Note that the popularity of the category has a limited distribution: only a small subset of categories has a high frequency, while the majority of categories do not occur very often. SGD algorithms with samples that exhibit limited distribution may suffer from non-informative categories."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "WikiData is a document-oriented database suitable for our training methodology. We extract document contents and categories from a 2014 Wikipedia dump. Each document is associated with several categories. As both the number of documents and the number of categories are very large, we reserve only documents with category labels that correspond to the 105 most common categories. We note that there are many redundant meaningless category entries, such as \"1880 births,\" \"1789 deaths,\" etc., which normally consist of thousands of documents from different areas under a category. Although we cannot exclude all noisy categories, we eliminate a fraction of these categories by some rules, resulting in 86,664 categories and 2,271,411 documents. These categories occur an average of 152 times throughout the data set. In addition, we remove all stopwords from the corpus in a predefined sentence. In our experiment, we also remove all words that occur less than 20 times. Our final training dataset has a token of 3,775 and a token of 112."}, {"heading": "4.2 Experiment Settings and Training Details", "text": "We use stochastic gradient deviation (SGD) for optimization using four threads on a 3.6 GHz Intel i7-4790 machine. We randomly select 100,000 documents as reserved data to adjust hyperparameters and use all documents for training. The dimension of the word vectors is selected to 300 for all models of the experiment, and thus the dimension of the category vectors is also 300. 20 negative words are scanned in the negative sampling of CeWE and 20 negative categories are scanned in the adaptive negative sampling of GCeWE. Different learning rates are used when the category acts as an additional input and the monitored target, respectively as \u03b1 and \u03b2. We set \u03b1 to 0.02 and \u03b2 0.015, respectively. We also use subsampling more frequently, as suggested in (Mikolov et al., 2013b) (Mikolov et al.) with the parameter 1e4."}, {"heading": "4.3 Evaluation Methods", "text": "We evaluate the CeWE model on five sets of data, including WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), RW (Luong et al., 2013), and SCWS (Huang et al., 2012), which contain 353, 30, 65, 2003, 1762 word pairs. We use SCWS to evaluate our word vectors without context information. In these records, each word is labeled pairis correlation score according to the similarity and relativization of the word pair. We compile the word correlation between the similarity values and the synchronized words based on word embeddings and human terms."}, {"heading": "4.4 Results and Analysis", "text": "For word similarity and word analogy reasoning tasks, we compare our models with CWE, SkipGram and the state-of-the-art GloVe model. GloVe uses the global coexistence statistics with a weighted smallest square. All models presented are trained using our data set. For GloVe, we set the hyperparameters of the model as shown in the original paper, which achieved the best performance. CBOW and Skip-Gram are trained using the word2vec tool 2. We first present our results on word similarity tasks in Table 1, in which the CeWE model consistently performs best on all five data sets. This indicates that additional category information helps to capture high-quality word embeddings, which more accurately capture the semantic meanings. We also find that as the window size increases, the CeWE model is better than BOVK tasks. The reason for this is likely to be that the window size becomes larger as information is added from the context."}, {"heading": "4.5 Qualitative Evaluation of Category", "text": "To show that our learned category embedding captures the current information, we randomly select 5 categories: supercomputers, IOS games, political terminology, animal anatomy, astronomy in the UK, and calculate the 10 closest words for each of them. For a specific category, we select words by comparing the cosinal distance between category embedding and all other words in the vocabulary. Table 1 in the supplementary material lists words that have a distance to the category embedded within the top 10 maximum distances. Considering the category \"Animal Anatomy,\" for example, the anatomical terms that are highly related to animal anatomy are reproduced. In addition, we project the embedding of the categories and words described above onto the 2-dimensional space using the t-SNE algorithm (Van der Maaten and Hinton, 2008), which is displayed in Figure 1 in the supplement material, we project the embedding of the above categories and related categories in five."}, {"heading": "5 Conclusion and Future Work", "text": "We have presented two models that integrate knowledge of document categories into learning word embeddings and demonstrate the ability to generalize learned word embeddings into multiple NLP tasks. In our future research, we plan to integrate refined category knowledge and eliminate redundant categories that may hinder learning word embeddings. We will also consider how to effectively use learned category embeddings in other NLP-related tasks such as multi-label text classification."}], "references": [{"title": "Knowledge-powered deep learning for word embedding", "author": ["Bian et al.2014] Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Bian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bian et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan et al.2008] Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31st International", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Topical word embeddings", "author": ["Liu et al.2015] Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas et al.2011] Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "Workshop at International Conference on Learning Representation", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["Charles."], "venue": "Language and cognitive processes, 6(1):1\u201328.", "citeRegEx": "Charles.,? 1991", "shortCiteRegEx": "Charles.", "year": 1991}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Hinton2007] Andriy Mnih", "Geoffrey Hinton"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In Proceedings of the international workshop on artificial intelligence and statistics,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": "arXiv preprint arXiv:1404.5367", "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Co-learning of word representations and morpheme representations. COLING", "author": ["Qiu et al.2014] Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": null, "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Improving pairwise learning for item recommendation from implicit feedback", "author": ["Rendle", "Christoph Freudenthaler"], "venue": "In Proceedings of the 7th ACM international conference on Web search and data mining,", "citeRegEx": "Rendle et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rendle et al\\.", "year": 2014}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "John B Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "Learning representations by back-propagating errors", "author": ["Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "A neural probabilistic language model", "author": ["Yoshua et al.2003] Bengio Yoshua", "Ducharme R\u00e9jean", "Vincent Pascal", "Jauvin Christian"], "venue": "Journal of Machine Learning Research(JMLR),", "citeRegEx": "Yoshua et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yoshua et al\\.", "year": 2003}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y Zou", "Richard Socher", "Daniel M Cer", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 25, "context": "Representing each word as a dense real-valued vector, also known as word embedding, has been exploited extensively in NLP communities recently (Yoshua et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Socher et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 143, "endOffset": 284}, {"referenceID": 23, "context": "Representing each word as a dense real-valued vector, also known as word embedding, has been exploited extensively in NLP communities recently (Yoshua et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Socher et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 143, "endOffset": 284}, {"referenceID": 18, "context": "Representing each word as a dense real-valued vector, also known as word embedding, has been exploited extensively in NLP communities recently (Yoshua et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Socher et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 143, "endOffset": 284}, {"referenceID": 0, "context": "Such additional knowledge could define a new basis for word representation, enrich input information, and serve as complementary supervision when training the neural network (Bian et al., 2014).", "startOffset": 174, "endOffset": 193}, {"referenceID": 0, "context": "Such additional knowledge could define a new basis for word representation, enrich input information, and serve as complementary supervision when training the neural network (Bian et al., 2014). For example, Yu and Dredze (2014) incorporate relational knowledge in their neural network model to improve lexical semantic embeddings.", "startOffset": 175, "endOffset": 229}, {"referenceID": 0, "context": "Such additional knowledge could define a new basis for word representation, enrich input information, and serve as complementary supervision when training the neural network (Bian et al., 2014). For example, Yu and Dredze (2014) incorporate relational knowledge in their neural network model to improve lexical semantic embeddings. Topical information is another kind of knowledge that appears to be also attractive for training more effective word embeddings. Liu et al (2015) leverage implicit topics generated by LDA to train topical word embeddings for multi-prototype vectors of each word.", "startOffset": 175, "endOffset": 478}, {"referenceID": 18, "context": "In the CeWE model, we find that with local additional category knowledge, word embeddings outperform CBOW and GloVe (Pennington et al., 2014) significantly in word similarity tasks.", "startOffset": 116, "endOffset": 141}, {"referenceID": 22, "context": "bedding was first proposed in (Rumelhart et al., 1988) and has become a successful representation method in many NLP applications including machine translation (Zou et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 27, "context": ", 1988) and has become a successful representation method in many NLP applications including machine translation (Zou et al., 2013), parsing (Socher et al.", "startOffset": 113, "endOffset": 131}, {"referenceID": 23, "context": ", 2013), parsing (Socher et al., 2011), named entity recognition (Passos et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 17, "context": ", 2011), named entity recognition (Passos et al., 2014), sentiment analysis (Glorot et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 5, "context": ", 2014), sentiment analysis (Glorot et al., 2011), partof-speech tagging (Collobert et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 2, "context": ", 2011), partof-speech tagging (Collobert et al., 2011) and text classification (Le and Mikolov, 2014).", "startOffset": 31, "endOffset": 55}, {"referenceID": 18, "context": "Another example that explored the cooccurrence statistics between words is GloVe (Pennington et al., 2014), which combines global matrix", "startOffset": 81, "endOffset": 106}, {"referenceID": 1, "context": ", 2011), partof-speech tagging (Collobert et al., 2011) and text classification (Le and Mikolov, 2014). Many prior works have explored how to learn effective word embeddings that can capture the words\u2019 intrinsic similarities and discriminations. Bengio et al. (2003) proposed to train an n-gram model using a neural network architecture with one hidden layer, and obtained good generalization.", "startOffset": 32, "endOffset": 267}, {"referenceID": 19, "context": "For example, Qiu et al. (2014) incorporated morphological knowledge to help learn embeddings for rare and unknown words.", "startOffset": 13, "endOffset": 31}, {"referenceID": 4, "context": "We evaluate the CeWE model on five datasets including WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), RW (Luong et al.", "startOffset": 66, "endOffset": 92}, {"referenceID": 9, "context": ", 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), RW (Luong et al., 2013), and SCWS (Huang et al.", "startOffset": 81, "endOffset": 101}, {"referenceID": 6, "context": ", 2013), and SCWS (Huang et al., 2012), which contain 353, 30, 65, 2003, 1762 word pairs respectively.", "startOffset": 18, "endOffset": 38}, {"referenceID": 4, "context": "We evaluate the CeWE model on five datasets including WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), RW (Luong et al., 2013), and SCWS (Huang et al., 2012), which contain 353, 30, 65, 2003, 1762 word pairs respectively. We use SCWS to evaluate our word vectors without context information. In these datasets, each word pair is given a human labeled correlation score according to the similarity and relatedness of the word pair. We compute the spearman rank correlation between the similarity scores calculated based on word embeddings and human labeled scores. Word Analogy Task. The word analogy task was first introduced by Mikolov (2013a). It consists of analogical questions in the form of \u201ca is to b as b is to ?\u201d.", "startOffset": 67, "endOffset": 705}, {"referenceID": 10, "context": "Sentiment Classification and Text Classification We evaluate the learned embeddings on two dataset: the IMDB (Maas et al., 2011) and 20NewsGroup 1.", "startOffset": 109, "endOffset": 128}, {"referenceID": 8, "context": "choose LDA, TWE-1 (Liu et al., 2015), Skip-Gram, CBOW, and GloVe as baseline models.", "startOffset": 18, "endOffset": 36}, {"referenceID": 3, "context": "For Skip-Gram, CBOW, GloVe and our models, we simply represent each document by aggregating embeddings of words that have a TF-IDF value larger the AVGT and use them as document features to train a linear classifier with Liblinear (Fan et al., 2008).", "startOffset": 231, "endOffset": 249}, {"referenceID": 8, "context": "For TWE-1, the document embedding is represented by aggregating all topical word embeddings as described in (Liu et al., 2015), and the length of topical word embedding is double that of word embedding or topic embedding.", "startOffset": 108, "endOffset": 126}, {"referenceID": 10, "context": "(Maas et al., 2011) and (Liu et al.", "startOffset": 0, "endOffset": 19}], "year": 2015, "abstractText": "Distributed word representations have been demonstrated to be effective in capturing semantic and syntactic regularities. Unsupervised representation learning from large unlabeled corpora can learn similar representations for those words that present similar cooccurrence statistics. Besides local occurrence statistics, global topical information is also important knowledge that may help discriminate a word from another. In this paper, we incorporate category information of documents in the learning of word representations and to learn the proposed models in a documentwise manner. Our models outperform several state-of-the-art models in word analogy and word similarity tasks. Moreover, we evaluate the learned word vectors on sentiment analysis and text classification tasks, which shows the superiority of our learned word vectors. We also learn high-quality category embeddings that reflect topical meanings.", "creator": "LaTeX with hyperref package"}}}