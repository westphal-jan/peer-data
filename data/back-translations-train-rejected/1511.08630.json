{"id": "1511.08630", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "A C-LSTM Neural Network for Text Classification", "abstract": "Neural network models have been demon- strated to be capable of achieving remarkable performance in sentence and document mod- eling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to ex- tract a sequence of higher-level phrase repre- sentations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence se- mantics. We evaluate the proposed archi- tecture on sentiment classification and ques- tion classification tasks. The experimental re- sults show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.", "histories": [["v1", "Fri, 27 Nov 2015 11:44:17 GMT  (35kb)", "http://arxiv.org/abs/1511.08630v1", null], ["v2", "Mon, 30 Nov 2015 07:20:46 GMT  (35kb)", "http://arxiv.org/abs/1511.08630v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chunting zhou", "chonglin sun", "zhiyuan liu", "francis c m lau"], "accepted": false, "id": "1511.08630"}, "pdf": {"name": "1511.08630.pdf", "metadata": {"source": "CRF", "title": "A C-LSTM Neural Network for Text Classification", "authors": ["Chunting Zhou", "Chonglin Sun", "Zhiyuan Liu", "Francis C.M. Lau"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 151 1,08 630v 1 [cs.C L] 27 N"}, {"heading": "1 Introduction", "text": "Since one of the core steps in NLP, sentence modeling, aims to present sentences as meaningful features for tasks such as sentiment classification, traditional sentence modeling uses the \"bag-ofwords\" model, which often suffers from the curse of dimensionality; others instead use compositional methods to generate semantic word vectors that produce the semantic sentence vector. However, such methods cannot be meaningful due to the loss of word orders. Recent models for distributed sentence representations fall into two categories: sequence-based models and structural models. Sequence-based models construct sentence representations of word sequences by taking into account the relationship between consecutive words (Johnson and Zhang, 2015). Tree-structured models treat each word as nodes in a syntactic parsector tree and learn sentence representations from sheets in a recursive manner (Socher et al, 2013b)."}, {"heading": "2 Related Work", "text": "In many NLP tasks, including learning distributed word, sentence and document representation (Mikolov et al., 2013b; Le and Mikolov, 2014), we have achieved great success. (Socher et al., 2013a), statistical machine translation (Devlin et al., 2014), sensation classification (Kim, 2014), etc. Learning distributed sentence representation through neural network models requires little external domain knowledge and can achieve satisfactory results in related tasks such as sentiment classification, text categorization. In many recent sentence representation learning, neural network models are built on either the input of word sequences or the transformed syntactical parse structure. Among them, convolutionary neural networks (CNN) and recurrent neural networks (RNN) are two popular ones.The ability to capture local correlations, along with extracting higher-level correlations through extraction."}, {"heading": "3 C-LSTM Model", "text": "The architecture of the C-LSTM model is illustrated in Figure 1, which consists of two main components: a Convolutionary Neural Network (CNN) and a Long-Term Short-Term Memory Network (LSTM). The following two subsections describe how we use CNN to extract sequences of word characteristics at a higher level, and LSTM to detect long-term dependencies to window characteristic sequences."}, {"heading": "3.1 N-gram Feature Extraction through Convolution", "text": "The one-dimensional folding contains a filter vector that glides over a sequence and recognizes features at different positions. Let xi-Rd be the d-dimensional word vectors for the i-th word in a sentence. Let x-RL-d denote the input sentence where L is the length of the sentence. Let k be the length of the filter, and the vector m-Rk-d is a filter for the folding process. For each position j in the sentence we have a window vector wj with k consecutive word vectors, which are denoted as: wj = [xj + 1, \u00b7 olence], \u00b7 \u00b7 olence is a filter for the order j in the sentence. A filter m folds with the window vectors (k-grams) at each position in a valid manner to generate a feature card c-RL-k + 1."}, {"heading": "3.2 Long Short-Term Memory Networks", "text": "Recursive neural networks (RNNs) are able to disseminate historical information via a chain-like neural network architecture q = 1. While processing sequential data, it considers the current input texture as well as the previous output of the hidden state ht \u2212 1 at each time step. However, standard RNs will not be able to learn long-term dependencies as the gap between two time steps becomes large. To address this problem, LSTM was first introduced in (Hochreiter and Schmidhuber, 1997) and has emerged as a successful architecture since Ilya et al. (2014) as a remarkable achievement in statistical machine translation. Although many variants of LSTM have been suggested, we adopt the standard architecture (Hochreiter and Schmidhuber, 1997) in this work. The LSTM architecture has a series of repetitive modules for each time step as in a standard RNN. At each time step, the output of the module is controlled by a series of gates in Rd."}, {"heading": "4 Learning C-LSTM for Text Classification", "text": "In the text classification, we consider the output of the hidden state in the last time step of the LSTM as a document representation and add a Softmax layer. We train the whole model by minimizing the cross-entropy error. In the face of a training sample x (i) and its true labeling y (i) {1, 2, \u00b7 \u00b7, k}, where k is the number of possible labels and the estimated probabilities y (i) j [0, 1] for each labeling j [1, 2, \u00b7, k}, the error is defined as follows: L (x (i), y (i) = k \u2211 j = 11 {y (i) = j} log (i) j), (5), where 1 {condition} is an indicator, so that 1 {condition true} = 1 otherwise 1 {condition false} = 0. We apply stochastic gradient derivation (SGD) to learn the model parameters, and the optimizer tone (2012)."}, {"heading": "4.1 Padding and Word Vector Initialization", "text": "First, we use maxlen to specify the maximum length of the sentence in the training set. As the folding layer in our model requires a fixed length input, we fill each sentence that is less than maxlen long with special symbols at the end that indicate the unknown words. For a sentence in the test set, we fill sentences that are shorter than maxlen in the same way, but for sentences that are longer than maxlen, we simply crop out additional words at the end of these sentences to reach the maximum. We initialize word vectors with the publicly available word2vec vectors 1, which are pre-trained with approximately 100B words from the Google News dataset. The dimension of the word vectors is 300. We also initialize the word vector for the unknown words from the uniform distribution [-0.25, 0.25]. We then coordinate the word vectors with other model parameters during the training."}, {"heading": "4.2 Regularization", "text": "For regulation, we use two commonly used techniques: dropout (Hinton et al., 2012) and L2 weight regulation. We use dropout to prevent co-adaptation. In our model, we apply dropout either to word vectors before feeding the word sequence into the wave layer, or to the output of LSTM before the Softmax layer. L2 regulation is applied to the weight of the Softmax layer."}, {"heading": "5 Experiments", "text": "We evaluate the C-LSTM model based on two tasks: (1) Mood classification and (2) Question-type classification. In this section we present the data sets and experimental settings."}, {"heading": "5.1 Datasets", "text": "We use the benchmark Stanford Sentiment Treebank (SST) (Socher et al., 2013b). This dataset consists of 11855 film reviews and is divided into Train (8544), Dev (1101) and Test (2210). Sentences in this corpus are analyzed and all phrases along with the sentences are fully labeled with 1http: / / code.google.com / p / word2vec / 5 labels: very positive, positive, neural, negative, very negative. We consider two classification tasks on this dataset: fine-grained classification with 5 labels and binary classification by removing neural labels. For binary classification, the REvity dataset represents a division into train (6920) / dev (1821)."}, {"heading": "5.2 Experimental Settings", "text": "We implement our model based on Theano (Bastien et al., 2012) - a Python library that supports efficient symbolic differentiation and transparent use of a GPU. To benefit from the efficiency of parallel calculation of tensors, we train the model on a GPU. For word preprocessing, we only convert all the characters in the dataset to lower traps. For SST, we perform hyperparameters (number of filters, filter length in CNN; memory dimension in LSTM; failure rate and the layer to be applied, etc.) tuning to the validation data in the standard split. For TREC, we keep 1000 samples from the training dataset and train the model using the remaining data. In our final settings, we use only one volume layer and one LSTM layer for both tasks."}, {"heading": "6 Results and Model Analysis", "text": "In this section we show our evaluation results on mood classification and question-type classification tasks. In addition, we give some model analyses on filter size configuration."}, {"heading": "6.1 Sentiment Classification", "text": "The results are presented in Table 1. We compare our model with a large number of well-executed models based on the Stanford Sentiment Treebank. Generally, the basic models consist of recursive models, Convolutionary Neural Network Models, LSTM-related models, and others. Recursive models use a syntactic parsect tree as a sentence structure, and sentence representation is recursively computed in a bottom-up manner along the parsect tree. In this category, we choose recursive autoencoders (RAE), matrix vectors (MV-RNN), tensor-based composition (RNTN), and multi-layer stacked (DRNN) recursive neural networks as baselines. We compare with Kim's (2014) CNN model, which we do not use finely tuned word vectors (CNN-non-static) and multi-channel (CNN-multichannel)."}, {"heading": "6.2 Question Type Classification", "text": "We compare our model with a variety of models. The SVM classifier uses unigrams, bigrams, wh-word, header word, POS tags, parsers, hypernyms, WordNet synsets as technical features and 60 hand-coded rules. Ada-CNN is a self-adaptable hierarchical typesetting model with gating networks. In the last task, other baseline models were introduced. From Table 2, we have the following observations: (1) Our result consistently exceeds all published neural baseline models, which means that C-LSTM captures the intentions of TREC questions well. (2) Our result corresponds to the most advanced SVM, which depends on sophisticated features. Such constructed features not only require human work, but also lead to error propagation in the existing NLP tools, so could not be well generalized in other datasets, and with the better ability to learn a set automatically, no scalability is required."}, {"heading": "6.3 Model Analysis", "text": "Here, we examine the impact of different filter configurations in the Convolutionary Layer on model performance. In the Convolutionary Layer of our model, filters are used to capture local n-gram characteristics. Intuitively, several Convolutionary Layers in parallel with different filter sizes should perform better than single Convolutionary Layers with the same length filters in different filter sizes. However, in our experiments, we found that a single Convolutionary Layer with filter length 3 always outperforms other cases. In Figure 2, we show the predictive accuracy of the 6-way question classification task using different filter configurations. Note that we also observe the similar phenomenon in the sensitivity classification task. For each filter configuration, we report the best result from comprehensive grid search for hyperparameters in Figure 2. It has been shown that a single Convolutionary Layer with filter length 3 works best among all filter configurations."}, {"heading": "7 Conclusion and Future Work", "text": "We have described a novel, unified model called CLSTM, which combines revolutionary neural networks with long-term short-term memory networks (LSTM). CLSTM is able to learn features at the phrase level through a revolutionary level; sequences of such high-level representations are then fed into the LSTM to learn long-term dependencies. We have evaluated the learned semantic sentence representations based on mood classifications and question-type classification tasks with very satisfactory results. We could explore ways to replace standard convolution with sorted operations or tree-structured convolutions in the future. We believe that LSTM will benefit from more structured superordinate representations."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Modelling, visualising and summarising documents with a single convolutional neural network. arXiv preprint arXiv:1406.3830", "author": ["Denil et al.2014] Misha Denil", "Alban Demiraj", "Nal Kalchbrenner", "Phil Blunsom", "Nando de Freitas"], "venue": null, "citeRegEx": "Denil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. The Computing Research Repository (CoRR)", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "Claire Cardie"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "Effective use of word order for text categorization with convolutional neural networks. Human Language Technologies: The 2015", "author": ["Johnson", "Zhang2015] Rie Johnson", "Tong Zhang"], "venue": "Annual Conference of the North American Chapter of the ACL,", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences. Association for Computational Linguistics (ACL)", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of Empirical Methods on Natural Language Processing", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Molding cnns for text: non-linear, non-consecutive convolutions", "author": ["Lei et al.2015] Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of Empirical Methods on Natural Language Processing", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Learning question classifiers", "author": ["Li", "Roth2002] Xin Li", "Dan Roth"], "venue": "In Proceedings of the 19th international conference on Computational linguisticsVolume", "citeRegEx": "Li et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Li et al\\.", "year": 2002}, {"title": "When are tree structures necessary for deep learning of representations", "author": ["Li et al.2015] Jiwei Li", "Dan Jurafsky", "Eudard Hovy"], "venue": "In Proceedings of Empirical Methods on Natural Language Processing", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Mou et al.2015] Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": "Unpublished manuscript: http://arxiv. org/abs/1504. 01106v5. Version,", "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["Oriol Vinyals", "Andrew Senior", "Hasim Sak"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "Sainath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2015}, {"title": "From symbolic to sub-symbolic information in question classification", "author": ["Silva et al.2011] Joao Silva", "Lu\u0131\u0301sa Coheur", "Ana Cristina Mendes", "Andreas Wichert"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Silva et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of Empirical Methods on Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of the ACL conference. Citeseer", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. Association for Computational Linguistics (ACL)", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Tang et al.2015] Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In Proceedings of Empirical Methods on Natural Language Processing", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Lecture 6.5 - rmsprop, coursera: Neural networks for machine learning", "author": ["Tieleman", "Hinton2012] T. Tieleman", "G Hinton"], "venue": null, "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "In Proceedings of 2015th International Conference on Ma-", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Self-adaptive hierarchical sentence", "author": ["Zhao et al.2015] Han Zhao", "Zhengdong Lu", "Pascal Poupart"], "venue": null, "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 24, "context": "Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models (Tai et al., 2015; Lei et al., 2015; Tang et al., 2015; Kim, 2014; Kalchbrenner et al., 2014; Mou et al., 2015).", "startOffset": 189, "endOffset": 300}, {"referenceID": 12, "context": "Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models (Tai et al., 2015; Lei et al., 2015; Tang et al., 2015; Kim, 2014; Kalchbrenner et al., 2014; Mou et al., 2015).", "startOffset": 189, "endOffset": 300}, {"referenceID": 25, "context": "Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models (Tai et al., 2015; Lei et al., 2015; Tang et al., 2015; Kim, 2014; Kalchbrenner et al., 2014; Mou et al., 2015).", "startOffset": 189, "endOffset": 300}, {"referenceID": 10, "context": "Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models (Tai et al., 2015; Lei et al., 2015; Tang et al., 2015; Kim, 2014; Kalchbrenner et al., 2014; Mou et al., 2015).", "startOffset": 189, "endOffset": 300}, {"referenceID": 9, "context": "Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models (Tai et al., 2015; Lei et al., 2015; Tang et al., 2015; Kim, 2014; Kalchbrenner et al., 2014; Mou et al., 2015).", "startOffset": 189, "endOffset": 300}, {"referenceID": 16, "context": "Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models (Tai et al., 2015; Lei et al., 2015; Tang et al., 2015; Kim, 2014; Kalchbrenner et al., 2014; Mou et al., 2015).", "startOffset": 189, "endOffset": 300}, {"referenceID": 3, "context": "CNNs have been successfully combined with both sequence-based model (Denil et al., 2014; Kalchbrenner et al., 2014) and tree-structured", "startOffset": 68, "endOffset": 115}, {"referenceID": 9, "context": "CNNs have been successfully combined with both sequence-based model (Denil et al., 2014; Kalchbrenner et al., 2014) and tree-structured", "startOffset": 68, "endOffset": 115}, {"referenceID": 16, "context": "model (Mou et al., 2015) in sentence modeling.", "startOffset": 6, "endOffset": 24}, {"referenceID": 1, "context": "problem of gradient exploding or vanishing in the standard RNN, Long Short-term Memory RNN (LSTM) (Hochreiter and Schmidhuber, 1997) and other variants (Cho et al., 2014) were designed for better remembering and memory accesses.", "startOffset": 152, "endOffset": 170}, {"referenceID": 25, "context": "Along with the sequence-based (Tang et al., 2015) or the tree-structured (Tai et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 24, "context": ", 2015) or the tree-structured (Tai et al., 2015) models, RNNs have achieved remarkable results in sentence or document modeling.", "startOffset": 31, "endOffset": 49}, {"referenceID": 18, "context": "(Sainath et al., 2015) have obtained respectable improvements in WER by learning a deep LSTM from multi-scale inputs.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": ", 2013a), statistical machine translation (Devlin et al., 2014), sentiment classification (Kim, 2014), etc.", "startOffset": 42, "endOffset": 63}, {"referenceID": 10, "context": ", 2014), sentiment classification (Kim, 2014), etc.", "startOffset": 34, "endOffset": 45}, {"referenceID": 2, "context": "In (Collobert et al., 2011), Collobert et al.", "startOffset": 3, "endOffset": 27}, {"referenceID": 10, "context": "As a slight variant, Kim et al. (2014)", "startOffset": 21, "endOffset": 39}, {"referenceID": 12, "context": "In a more recent work (Lei et al., 2015), Tao et al.", "startOffset": 22, "endOffset": 40}, {"referenceID": 9, "context": "To capture word relations of varying sizes, Kalchbrenner et al. (2014) proposed a dynamic k-max pooling mechanism.", "startOffset": 44, "endOffset": 71}, {"referenceID": 1, "context": "Various variants of RNN have been proposed to better store and access memories (Hochreiter and Schmidhuber, 1997; Cho et al., 2014).", "startOffset": 79, "endOffset": 131}, {"referenceID": 27, "context": "The combination of CNN and LSTM can be seen in some computer vision tasks like image caption (Xu et al., 2015) and speech recognition (Sainath et al.", "startOffset": 93, "endOffset": 110}, {"referenceID": 18, "context": ", 2015) and speech recognition (Sainath et al., 2015).", "startOffset": 31, "endOffset": 53}, {"referenceID": 14, "context": "In (Li et al., 2015), the authors suggest that sequence-based models are sufficient to capture the compositional semantics for many NLP tasks, thus in this work the CNN is directly built upon word sequences other than the syntactic parse tree.", "startOffset": 3, "endOffset": 20}, {"referenceID": 13, "context": "Mou et al. (2015) also explores convolutional models on tree-structured sentences.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "Various variants of RNN have been proposed to better store and access memories (Hochreiter and Schmidhuber, 1997; Cho et al., 2014). With the ability of explicitly modeling time-series data, RNNs are being increasingly applied to sentence modeling. For example, Tai et al. (2015) adjusted the standard LSTM to tree-structured topologies and obtained superior results over a sequential LSTM on related tasks.", "startOffset": 114, "endOffset": 280}, {"referenceID": 5, "context": "For regularization, we employ two commonly used techniques: dropout (Hinton et al., 2012) and L2 weight regularization.", "startOffset": 68, "endOffset": 89}, {"referenceID": 20, "context": "We use the Stanford Sentiment Treebank (SST) benchmark (Socher et al., 2013b). This dataset consists of 11855 movie reviews and are split into train (8544), dev (1101), and test (2210).", "startOffset": 56, "endOffset": 168}, {"referenceID": 20, "context": "We use the Stanford Sentiment Treebank (SST) benchmark (Socher et al., 2013b). This dataset consists of 11855 movie reviews and are split into train (8544), dev (1101), and test (2210). Sentences in this corpus are parsed and all phrases along with the sentences are fully annotated with", "startOffset": 56, "endOffset": 185}, {"referenceID": 9, "context": "Since the data is provided in the format of sub-sentences, we train the model on both phrases and sentences but only test on the sentences as in several previous works (Socher et al., 2013b; Kalchbrenner et al., 2014).", "startOffset": 168, "endOffset": 217}, {"referenceID": 0, "context": "We implement our model based on Theano (Bastien et al., 2012) \u2013 a python library, which supports efficient symbolic differentiation and transparent use of a GPU.", "startOffset": 39, "endOffset": 61}, {"referenceID": 9, "context": "5 (Kalchbrenner et al., 2014) Paragraph Vector 48.", "startOffset": 2, "endOffset": 29}, {"referenceID": 20, "context": "9 (Socher et al., 2012) RNTN 45.", "startOffset": 2, "endOffset": 23}, {"referenceID": 10, "context": "2 (Kim, 2014) CNN-multichannel 47.", "startOffset": 2, "endOffset": 13}, {"referenceID": 10, "context": "1 (Kim, 2014) DCNN 48.", "startOffset": 2, "endOffset": 13}, {"referenceID": 9, "context": "8 (Kalchbrenner et al., 2014) Molding-CNN 51.", "startOffset": 2, "endOffset": 29}, {"referenceID": 12, "context": "6 (Lei et al., 2015)", "startOffset": 2, "endOffset": 20}, {"referenceID": 24, "context": "7 (Tai et al., 2015) Constituency Tree-LSTM 51.", "startOffset": 2, "endOffset": 20}, {"referenceID": 24, "context": "0 (Tai et al., 2015) LSTM 46.", "startOffset": 2, "endOffset": 20}, {"referenceID": 10, "context": "Among CNNs, we compare with Kim\u2019s (2014) CNN model with fine-tuned word vectors (CNN-non-static) and multi-channels (CNNmultichannel), DCNN with dynamic k-max pool-", "startOffset": 28, "endOffset": 41}, {"referenceID": 10, "context": "6 Kim (2014)", "startOffset": 2, "endOffset": 13}, {"referenceID": 10, "context": "2 Kim (2014)", "startOffset": 2, "endOffset": 13}, {"referenceID": 9, "context": "0 Kalchbrenner et al. (2014)", "startOffset": 2, "endOffset": 29}, {"referenceID": 24, "context": "Since we could not tune the result of Bi-LSTM to be as good as what has been reported in (Tai et al., 2015) even if following their untied weight configuration, we report our own results.", "startOffset": 89, "endOffset": 107}], "year": 2015, "abstractText": "Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.", "creator": "LaTeX with hyperref package"}}}