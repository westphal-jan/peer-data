{"id": "1511.08724", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "On the convergence of cycle detection for navigational reinforcement learning", "abstract": "Reinforcement learning is a formal framework for modeling agents that learn to solve tasks. For example, one important task for animals is to navigate in an environment to find food or to return to their nest. In such tasks, the agent has to learn a path through the environment from start states to goal states, by visiting a sequence of intermediate states. The agent receives reward on a goal state. Concretely, we need to learn a policy that maps each encountered state to an immediate action that leads to a next state, eventually leading to a goal state. We say that a learning process has converged if eventually the policy will no longer change, i.e., the policy stabilizes. The intuition of paths and navigation policies can be applied generally. Indeed, in this article, we study navigation tasks formalized as a graph structure that, for each application of an action to a state, describes the possible successor states that could result from that application. In contrast to standard reinforcement learning, we essentially simplify numeric reward signals to boolean flags on the transitions in the graph. The resulting framework enables a clear theoretical study of how properties of the graph structure can cause convergence of the learning process. In particular, we formally study a learning process that detects revisits to states in the graph, i.e., we detect cycles, and the process keeps adjusting the policy until no more cycles are made. So, eventually, the agent goes straight to reward from each start state. We identify reducibility of the task graph as a sufficient condition for this learning process to converge. We also syntactically characterize the form of the final policy, which can be used to detect convergence in a simulation.", "histories": [["v1", "Fri, 27 Nov 2015 16:16:55 GMT  (266kb,D)", "http://arxiv.org/abs/1511.08724v1", null], ["v2", "Tue, 5 Jan 2016 14:08:35 GMT  (267kb,D)", "http://arxiv.org/abs/1511.08724v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["tom j ameloot", "jan van den bussche"], "accepted": false, "id": "1511.08724"}, "pdf": {"name": "1511.08724.pdf", "metadata": {"source": "CRF", "title": "Convergence in Navigational Reinforcement Learning", "authors": ["Tom J. Ameloot", "Jan Van den Bussche"], "emails": [], "sections": [{"heading": null, "text": ""}, {"heading": "1 Introduction 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Related Work 5", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Navigational Reinforcement Learning 7", "text": "3.1 Tasks and reducibility........................................................................................................................................................................................."}, {"heading": "4 Results 12", "text": "4.1 Sufficient Conditions for Convergence................... 13 4.2 Determination of Definitive Policies..................................... 18 4.3 Conditions for Convergence Necessary........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Convergence...................................................................................................................................................................................................................."}, {"heading": "5 Examples 27", "text": "5.1 Grid Navigation Tasks.........................................................................."}, {"heading": "6 Conclusion and Further Work 33", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "In fact, it is as if the organism could learn what the consequences of the reward entail (Sutton and Barto, 1998). For example, in a food search, the reward would be used in terms of finding food or returning home. If the organism encounters a state in which it directly encounters this state, the organism could learn what the consequences of the reward entail. Specifically, the organism learns a so-called policy that leads a mapping of states encountered to immediate actions. If the organism encounters a state, politics proposes an immediate action for that state, for which the politics in turn should take action, and thus politics should again propose an action, and thus the organism should reward the organism in order to reward: whenever an action is applied to a state, there is the possibility of observing a reward."}, {"heading": "2 Related Work", "text": "Fre \u0301 maux et al. (2013) and Potjans et al. (2011) elaborate the actor-critic framework of enhanced learning in the context of neurons. In particular, Fre \u0301 maux et al. (2013) study is both physical and more abstract state spaces. As an example of a physical state space, they consider a navigation task in which a simulated mouse must swim to a hidden platform where it can rest, where rest means reward; each state contains only the x and more abstract state coordinates. As an example of an abstract state space, they consider an acrobatic task in which reward is given when the tip of a double pendulum reaches a certain height; this space is abstract because each state contains two angles and two angular velocities, i.e., there are four dimensions. Conceptually, it does not matter how many dimensions a state space has, because essentially the agent is always trying to repeat the graphics of the theme structure of the sutan.sutan.as is repeated by Sutto (1998)."}, {"heading": "3 Navigational Reinforcement Learning", "text": "We formalize tasks and the notion of reducibility in Section 3.1. Next, in Section 3.2, we use operational semantics to formalize the interaction between a task and our learning algorithm, which recognizes cycles. In Section 3.3, we define convergence as the ultimate stability of policy. Finally, in Section 3.4, we impose certain fairness constraints on operational semantics."}, {"heading": "3.1 Tasks and Reducibility", "text": "Tasks To formalize tasks, we use non-deterministic transition systems in which some transitions are designated as immediate rewarding, in which reward is only an on-off flag. Formally, a task is a five-tupT = (Q, Q0, A, rewards, \u03b4) in which Q, Q0, and A are non-empty finite sentences; Q0 Q; rewards are a nonempty subset of Q \u00b7 A; and, \u03b4 is a function that places everyone (q, a) Q \u00d7 A on a nonempty subset of Q. The elements of Q, Q0, and A are respectively called states, start states, and actions. The fixed rewards tell us which pairs of states and actions give immediate reward."}, {"heading": "Li = Li\u22121 \u222a {q \u2208 Q | \u2203a \u2208 A with \u03b4(q, a) \u2286 Li\u22121}.", "text": "We refer to L1, L2, etc. as the (reducibility) layers. We define reduction (T, V) = reducibility (T, I, N0 Li). Note that reduction (T, V) Q. Since Q is finite, there is an index n for which Ln = Ln + 1, i.e. Ln is a fixed point. If you leave V \u00b2 Q, you say that V \u00b2 is reducible to V if V \u00b2 reduces (T, V). Intuitively, any state in V \u00b2 can choose an action to get closer to V. We also say that a single state q \u00b2 Q is reducible to V if q \u00b2 reduces task (T, V). Now we say that task T is reducible (for reward) if the state Q is reducible to targets (T). We use the abbreviation reduce (T) = reducibility (T) = reducibility (V) = 3), if objectives = reducibility (sense of solvability) of T."}, {"heading": "3.2 Navigational Learning", "text": "The inspiration for the learning algorithm comes from biologically plausible learning algorithms based on the global dopamine signal in the brain (Potjans et al., 2011; Fre \u0301 maux et al., 2013; Schultz et al., 2015). When an organism achieves an unexpected physical reward, dopamine is released to amplify synaptic connections between recently activated neurons. In the absence of strong synaptic connections between input neurons and output neurons, random search behavior could be exhibited. In the context of this article and similar theoretical models in the field of amplification learning (Sutton and Barto, 1998), the input neurons are abstractly represented by task states and output neurons. If a state is revisited in the same study, this is a form of disappointment because the agent hopes to have already achieved reward."}, {"heading": "3.3 Convergence", "text": "Consider a task T = (Q, Q0, A, rewards, \u03b4). Let R be a run on T. Definition 3.5. We say that a state q-Q will (eventually) become stable in R if there are only an endless number of non-terminal occurrences of branch configurations containing q.4. Intuitively, the ultimate stability of q means that after a while there is no risk that q will be paired with new actions, so that q will definitely remain associated with the same action.4 Note that states that appear only a finite number of times in R will always be stable under this definition.We say that the run R converges when (i) all studies (with reward) are terminated, and (ii) ultimately all states stay connected with the same action."}, {"heading": "3.4 Fairness", "text": "There are two selection points in each transition of operational semantics: \u2022 if the source configuration of the transition is branched, i.e., the current state is visited again, then we choose a new random action for the current state; and \u2022 if we apply an action a to a state q, we can generally choose between several possible successor states in \u03b4 (q, a). Fairness assumptions are required to give the algorithm sufficient opportunities to detect problems and to test better strategies (Francez, 1986). Intuitively, the choice should be independent of what politics and working memory say about states other than the current state. This intuition is related to the Markov assumption or the independence of path adoption (Sutton and Barto, 1998). Below, we formalize this intuition as a fairness idea for the operational semantics of section 3.2.We say that a run R is fair if for each configuration is infinite."}, {"heading": "4 Results", "text": "The simple learning algorithm formalized in Section 3.2 continuously marks the states encountered as visited. At the end of the studies, i.e. after obtaining the reward, each state is marked as unvisited again. If the algorithm encounters a state q that is already visited within the same experiment, the algorithm suggests generating a new action for q. Intuitively, if the same state q is encountered in the same experiment, the actor could run around in cycles and any new action should be attempted to get q out of the cycle. It is important to avoid cycles if we want to achieve an ultimate cap on the duration of an experiment, i.e. an upper limit on the time it takes to achieve a reward from a given starting state. Repeated testing of a new action for revisited states could ultimately lead to the reward and thus end the process. In this learning process, the non-determinism of the task can be both helpful and hindering the transition taking place under the present state."}, {"heading": "4.1 Sufficient Condition for Convergence", "text": "We can imagine that states near the target states, i.e. near the immediate reward, tend to agree more quickly on an action that leads to a reward. Consequently, states farther away from the immediate reward can be reduced to states near the target states, and this growth process spreads throughout the state space. This intuition is confirmed by the following convergence: Theorem 4.1. Reduceable tasks can be learned under fairness. Let T = (Q, Q0, A, E) be a reducible task. Let R be a fair task on T. Let R be a fair task on T. We show the convergence of R. In Part 1 of the evidence, we show that all studies terminate in R (with reward)."}, {"heading": "4.2 Detecting the Final Policy", "text": "For simulations, it is useful to recognize when convergence has occurred in a run and the policy will not change any more. We refer to the last formed policy of a run as the final policy. In this subsection, we define syntactically the final policy. Generally, the review of the syntactic property of the final policy requires access to the entire set of task states. Outside the simulations, the syntactic properties of the so-called forward and backward directive directives will give a clarifying insight into the shape of the final policy. In this subsection, we do not require that tasks be reduced. First, we present the two key parts of the syntactic characterization, namely the so-called forward and backward directive policies of the states, which are induced by a policy. As we will see below, the syntactic property should be included in the forward policy. Let T = (Q, Q0, A, backward) be a task that is learnable."}, {"heading": "4.3 Necessary Conditions for Convergence", "text": "In Section 4.1, we have seen that reducibility is a sufficient attribute for tasks that can be learned under Fairness (Theorem 4.1). In this section, we also show necessary attributes for tasks that cannot be learned under Fairness. This provides a first step in characterizing the tasks that can be learned under Fairness. We say that there is a (simple) path from a state q to a state q, because it allows us to better understand the tasks that cannot be learned under Fairness. Let T = (Q, Q0, A, Rewards) be a task. We say that there is a (simple) path from a state q to a state q, \"if there is a sequence of actions a1."}, {"heading": "Li = Li\u22121 \u222a {q \u2208 Q | \u2203a \u2208 A with \u03b4(q, a) \u2286 Li\u22121}.", "text": "We also remember the definition backwards (\u03c0) = i N0 Bi from section 4.2. We show by induction to i = 1, 2,.. that Bi Li. 17Note that there are no contradictory assignments of actions to states because, by definition of the path, each state Q1 = 1,.., n + 1) occurs only once on the path. 18In this notation we interpret {q1,., q0} as the result. \u2022 Base case: i = 1. By definition, B1 = soil (\u03c0) = {q \u00b2 goals (T) = stable tasks (q,.). In this notation we can set stable tasks (T.). Hence, B1 goals (T) = L1. Inductive step. Let us leave i 2, and let us assume that Bi \u2212 1 Li \u2212 1. Let us leave q Bi\\ Bi \u2212 1, which by definition of Bi \u2212 1 implies."}, {"heading": "5 Examples", "text": "Theorem 4.1 tells us that all reducible tasks can be learned under fairness, and Theorem 4.5 allows us to see when the final policy was formed. To illustrate these theorems, we now look at two examples of tasks that are reducible, in Section 5.1 and Section 5.2 respectively. Our goal is not to demonstrate the practical efficiency of the learning algorithm, but rather to illustrate the theoretical findings. As the examples under consideration are reducible, they can be learned under fairness through Theorem 4.1. Next, we can use Theorem 4.5 to measure experimentally how long it takes for the learning process to converge. In Section 6, we will identify aspects in which the learning algorithm could be improved in order to become more practical."}, {"heading": "5.1 Grid Navigation Tasks", "text": "In fact, it is as if most people are able to recognize themselves and understand what they are doing. (...) It is not as if they are able to identify themselves. (...) It is not as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves."}, {"heading": "5.2 Chain Tasks", "text": "The general form of a chain task is shown in Figure 5,5. The parameter n tells us how long the chain is; the states of the chain are, in order, 1,., n and a final state n + 1. To get a reward from the initial state, in the worst case we should perform all actions a1,.. one, in the order that is terminated by an arbitrary action. For each i,.,. n, the action ai should be applied to the state i. But there is a forward nondeterminism that could send us to an arbitrary state later in the chain, closer to the state n + 1. Also, there are backward deterministic transitions that bring us back to the initial state whenever we apply the wrong measures to a state."}, {"heading": "6 Conclusion and Further Work", "text": "We have explored the fascinating idea of enhanced learning in a non-numerical framework, where the focus is on the interaction between the graph structure of the task and a learning algorithm. We have explored the graph properties of reducibility, which implies the existence of a policy that makes steady progress towards reward despite non-determinism in the task. Interestingly, reducibility, combined with a natural assumption of fairness, should allow our simple learning method to expand our simple learning method in simulations. We are now discussing some avenues for further work. Characterizing tasks We have seen a sufficient characteristic (Theorem 4.1) and necessary characteristics (Proposition 4.6) for tasks that should be learnable. The gap between sufficient and necessary characteristics seems to be strongly related to the stable of a stable task."}, {"heading": "A Examples", "text": "This section provides additional details for the sample tasks discussed in Section 5.A.1. Since for our formalization of grid tasks Table A.1 indicates the offsets for each action, we calculate the quantile using the statistical package R. For the sake of completeness, let us remember the definition of quantiles that we used in our analysis; this definition is called Definition 1 by Hyndman and Fan (1996). Let L be a non-empty list of numbers that may contain duplicates. Let us specify the length of L. For an index i,."}], "references": [{"title": "Principles of Model Checking (Representation", "author": ["C. Baier", "J. Katoen"], "venue": null, "citeRegEx": "Baier and Katoen,? \\Q2008\\E", "shortCiteRegEx": "Baier and Katoen", "year": 2008}, {"title": "TD(\u03bb) converges with probability 1", "author": ["P. Dayan", "T. Sejnowski"], "venue": null, "citeRegEx": "Dayan and Sejnowski,? \\Q1994\\E", "shortCiteRegEx": "Dayan and Sejnowski", "year": 1994}, {"title": "Reinforcement learning using a continuous time actor-critic framework with spiking neurons", "author": ["N. Fr\u00e9maux", "H. Sprekeler", "W. Gerstner"], "venue": "PLoS Computational Biology, 9(4):e1003024.", "citeRegEx": "Fr\u00e9maux et al\\.,? 2013", "shortCiteRegEx": "Fr\u00e9maux et al\\.", "year": 2013}, {"title": "Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition", "author": ["W. Gerstner", "W. Kistler", "R. Naud", "L. Paninski"], "venue": "Cambridge University Press.", "citeRegEx": "Gerstner et al\\.,? 2014", "shortCiteRegEx": "Gerstner et al\\.", "year": 2014}, {"title": "Spatial cognition in bats and rats: from sensory acquisition to multiscale maps and navigation", "author": ["M. Geva-Sagiv", "L. Las", "Y. Yovel", "N. Ulanovsky"], "venue": "Nature Reviews Neuroscience, 16(4):94\u2013108.", "citeRegEx": "Geva.Sagiv et al\\.,? 2015", "shortCiteRegEx": "Geva.Sagiv et al\\.", "year": 2015}, {"title": "Sample quantiles in statistical packages", "author": ["R. Hyndman", "Y. Fan"], "venue": "The American Statistician, 50(4):361\u2013365.", "citeRegEx": "Hyndman and Fan,? 1996", "shortCiteRegEx": "Hyndman and Fan", "year": 1996}, {"title": "On the convergence of stochastic iterative dynamic programming algorithms", "author": ["T. Jaakkola", "M. Jordan", "S. Singh"], "venue": "Neural Computation, 6(6).", "citeRegEx": "Jaakkola et al\\.,? 1994", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1994}, {"title": "An imperfect dopaminergic error signal can drive temporal-difference learning", "author": ["W. Potjans", "M. Diesmann", "A. Morrison"], "venue": "PLoS Computational Biology, 7(5):e1001133.", "citeRegEx": "Potjans et al\\.,? 2011", "shortCiteRegEx": "Potjans et al\\.", "year": 2011}, {"title": "Updating dopamine reward signals", "author": ["W. Schultz"], "venue": "Current Opinion in Neurobiology, 23(2):229 \u2013 238.", "citeRegEx": "Schultz,? 2013", "shortCiteRegEx": "Schultz", "year": 2013}, {"title": "Neuronal reward and decision signals: From theories to data", "author": ["W. Schultz"], "venue": "Physiological Reviews, 95(3):853\u2013951.", "citeRegEx": "Schultz,? 2015", "shortCiteRegEx": "Schultz", "year": 2015}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R. Sutton"], "venue": "Machine Learning, 3(1):9\u201344.", "citeRegEx": "Sutton,? 1988", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Reinforcement Learning, An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "The MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Probabilistic Robotics", "author": ["S. Thrun", "W. Burgard", "D. Fox"], "venue": "The MIT Press.", "citeRegEx": "Thrun et al\\.,? 2005", "shortCiteRegEx": "Thrun et al\\.", "year": 2005}, {"title": "Asynchronous stochastic approximation and Q-learning", "author": ["J. Tsitsiklis"], "venue": "Machine Learning, 16(3):185\u2013202.", "citeRegEx": "Tsitsiklis,? 1994", "shortCiteRegEx": "Tsitsiklis", "year": 1994}, {"title": "Q-learning", "author": ["C. Watkins", "P. Dayan"], "venue": "Machine Learning, 8(3\u20134):279\u2013 292.", "citeRegEx": "Watkins and Dayan,? 1992", "shortCiteRegEx": "Watkins and Dayan", "year": 1992}], "referenceMentions": [{"referenceID": 4, "context": "Reinforcement Learning Biological organisms can learn to perform tasks in environments, such as navigating from their nest to foraging areas and back again (Geva-Sagiv et al., 2015).", "startOffset": 156, "endOffset": 181}, {"referenceID": 11, "context": "In the field of reinforcement learning, task performance is measured as the amount of reward received (Sutton and Barto, 1998).", "startOffset": 102, "endOffset": 126}, {"referenceID": 2, "context": "One could assume that connections are initially random between these types of neuron, meaning that initially there is no clear preference of actions for states (Fr\u00e9maux et al., 2013).", "startOffset": 160, "endOffset": 182}, {"referenceID": 11, "context": "1The expectation of reward could for example be represented by a third group of neurons, as in the actor-critic framework (Sutton and Barto, 1998; Potjans et al., 2011; Fr\u00e9maux et al., 2013).", "startOffset": 122, "endOffset": 190}, {"referenceID": 7, "context": "1The expectation of reward could for example be represented by a third group of neurons, as in the actor-critic framework (Sutton and Barto, 1998; Potjans et al., 2011; Fr\u00e9maux et al., 2013).", "startOffset": 122, "endOffset": 190}, {"referenceID": 2, "context": "1The expectation of reward could for example be represented by a third group of neurons, as in the actor-critic framework (Sutton and Barto, 1998; Potjans et al., 2011; Fr\u00e9maux et al., 2013).", "startOffset": 122, "endOffset": 190}, {"referenceID": 11, "context": "In standard reinforcement learning (Sutton and Barto, 1998), reward is represented by a numeric value on each transition of the task, where larger positive numbers represent higher rewards.", "startOffset": 35, "endOffset": 59}, {"referenceID": 2, "context": "Such state spaces could be part of a physical world, representing a map of physical places, or they can be more abstract with more than three dimensions (Fr\u00e9maux et al., 2013).", "startOffset": 153, "endOffset": 175}, {"referenceID": 0, "context": "Such a framework brings reinforcement learning theory closer to many areas of computer science that have a more discrete mathematics approach, notably the area of model checking (Baier and Katoen, 2008).", "startOffset": 178, "endOffset": 202}, {"referenceID": 12, "context": "in real-world applications (Thrun et al., 2005).", "startOffset": 27, "endOffset": 47}, {"referenceID": 10, "context": "Many previous works in numerical reinforcement learning try to find optimal policies, and their related optimal value functions (Sutton, 1988; Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Jaakkola et al., 1994; Tsitsiklis, 1994); an optimal policy gives the highest reward in the long run.", "startOffset": 128, "endOffset": 248}, {"referenceID": 14, "context": "Many previous works in numerical reinforcement learning try to find optimal policies, and their related optimal value functions (Sutton, 1988; Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Jaakkola et al., 1994; Tsitsiklis, 1994); an optimal policy gives the highest reward in the long run.", "startOffset": 128, "endOffset": 248}, {"referenceID": 1, "context": "Many previous works in numerical reinforcement learning try to find optimal policies, and their related optimal value functions (Sutton, 1988; Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Jaakkola et al., 1994; Tsitsiklis, 1994); an optimal policy gives the highest reward in the long run.", "startOffset": 128, "endOffset": 248}, {"referenceID": 6, "context": "Many previous works in numerical reinforcement learning try to find optimal policies, and their related optimal value functions (Sutton, 1988; Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Jaakkola et al., 1994; Tsitsiklis, 1994); an optimal policy gives the highest reward in the long run.", "startOffset": 128, "endOffset": 248}, {"referenceID": 13, "context": "Many previous works in numerical reinforcement learning try to find optimal policies, and their related optimal value functions (Sutton, 1988; Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Jaakkola et al., 1994; Tsitsiklis, 1994); an optimal policy gives the highest reward in the long run.", "startOffset": 128, "endOffset": 248}, {"referenceID": 10, "context": "The numerical reinforcement learning technique of temporal difference (TD) learning (Sutton, 1988; Sutton and Barto, 1998) has been associated to the way biological organisms learn.", "startOffset": 84, "endOffset": 122}, {"referenceID": 11, "context": "The numerical reinforcement learning technique of temporal difference (TD) learning (Sutton, 1988; Sutton and Barto, 1998) has been associated to the way biological organisms learn.", "startOffset": 84, "endOffset": 122}, {"referenceID": 9, "context": "In particular, the global dopamine signal in the brain might encode the predication error that is key to temporal difference learning (Schultz, 2015).", "startOffset": 134, "endOffset": 149}, {"referenceID": 7, "context": "However, TD-learning is not perfectly matched to the dopamine signal because the negative TD-prediction error can only be represented by dopamine to a limited extent (Potjans et al., 2011; Schultz, 2013).", "startOffset": 166, "endOffset": 203}, {"referenceID": 8, "context": "However, TD-learning is not perfectly matched to the dopamine signal because the negative TD-prediction error can only be represented by dopamine to a limited extent (Potjans et al., 2011; Schultz, 2013).", "startOffset": 166, "endOffset": 203}, {"referenceID": 11, "context": "Perhaps in contrast to the notion of optimality often used in the previous work on numerical reinforcement learning (Sutton and Barto, 1998), where the aim is to find a policy that gives the highest expected reward, we are only studying a very local form of optimality, where convergence is formalized as the eventual avoidance of cycles in the paths through the state space.", "startOffset": 116, "endOffset": 140}, {"referenceID": 2, "context": "We believe that this viewpoint on convergence aligns well with animal and human learning, where an organism learns just some path to reward, heavily dependent on the experience (Fr\u00e9maux et al., 2013).", "startOffset": 177, "endOffset": 199}, {"referenceID": 8, "context": "In particular, disappointment could be related to the expectation of reward, and can be represented by the suppression of dopamine (Schultz, 2013).", "startOffset": 131, "endOffset": 146}, {"referenceID": 14, "context": "1, showing that convergence always occurs on reducible tasks (with our definition of convergence), is related in intent to previous results showing that certain numerical learning algorithms converge with probability 1 (Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Tsitsiklis, 1994).", "startOffset": 219, "endOffset": 302}, {"referenceID": 1, "context": "1, showing that convergence always occurs on reducible tasks (with our definition of convergence), is related in intent to previous results showing that certain numerical learning algorithms converge with probability 1 (Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Tsitsiklis, 1994).", "startOffset": 219, "endOffset": 302}, {"referenceID": 13, "context": "1, showing that convergence always occurs on reducible tasks (with our definition of convergence), is related in intent to previous results showing that certain numerical learning algorithms converge with probability 1 (Watkins and Dayan, 1992; Dayan, 1992; Dayan and Sejnowski, 1994; Tsitsiklis, 1994).", "startOffset": 219, "endOffset": 302}, {"referenceID": 10, "context": "Also, the notion of reducibility discussed in this article is related to the principles of dynamic programming explained by Sutton and Barto (1998). Indeed, in reducibility, we defer the responsibility of obtaining reward from a given state to one of the successor states under a chosen action.", "startOffset": 124, "endOffset": 148}, {"referenceID": 11, "context": "Our formalization of tasks keeps only the graph structure of models previously studied in reinforcement learning; essentially, compared to finite Markov decision processes (Sutton and Barto, 1998), we omit transition probabilities and we simplify the numerical reward signals to boolean flags.", "startOffset": 172, "endOffset": 196}, {"referenceID": 11, "context": "In this article we also do not consider the topic of generalization (Sutton and Barto, 1998), where an agent only sees concepts that are abstractions of states, and the agent learns to assign actions to concepts instead of directly to states; one concept could represent many states.", "startOffset": 68, "endOffset": 92}, {"referenceID": 7, "context": "for the learning algorithm comes from biologically plausible learning algorithms based on the global dopamine signal in brains (Potjans et al., 2011; Fr\u00e9maux et al., 2013; Schultz, 2013, 2015).", "startOffset": 127, "endOffset": 192}, {"referenceID": 2, "context": "for the learning algorithm comes from biologically plausible learning algorithms based on the global dopamine signal in brains (Potjans et al., 2011; Fr\u00e9maux et al., 2013; Schultz, 2013, 2015).", "startOffset": 127, "endOffset": 192}, {"referenceID": 11, "context": "In the framework of this article, and similar to other theoretical models in standard reinforcement learning (Sutton and Barto, 1998), the input neurons are abstractly represented by task states and output neurons are abstractly represented by actions.", "startOffset": 109, "endOffset": 133}, {"referenceID": 11, "context": "In numeric reinforcement learning, there is a value function that usually converges together with the policy (Sutton and Barto, 1998).", "startOffset": 109, "endOffset": 133}, {"referenceID": 11, "context": "The framework studied in this article may be called episodic (Sutton and Barto, 1998) because the agent should continuously navigate from start states to goal states; and, after obtaining the immediate reward at a goal state, the agent\u2019s location is again reset to a start state.", "startOffset": 61, "endOffset": 85}, {"referenceID": 11, "context": "This intuition is related to the Markov assumption, or independence of path assumption (Sutton and Barto, 1998).", "startOffset": 87, "endOffset": 111}, {"referenceID": 2, "context": "The experience dependence was experimentally observed by Fr\u00e9maux et al. (2013).", "startOffset": 57, "endOffset": 79}, {"referenceID": 11, "context": "Our first example is a navigation task in a grid world (Sutton and Barto, 1998; Potjans et al., 2011).", "startOffset": 55, "endOffset": 101}, {"referenceID": 7, "context": "Our first example is a navigation task in a grid world (Sutton and Barto, 1998; Potjans et al., 2011).", "startOffset": 55, "endOffset": 101}, {"referenceID": 7, "context": "A similar experimental result is also reported by Potjans et al. (2011), in the context of neurons learning a grid navigation task.", "startOffset": 50, "endOffset": 72}, {"referenceID": 11, "context": "Fading Eligibility Traces In some models of biologically plausible learning, the activation between any pair of connected neurons is represented by an eligibility trace (Sutton and Barto, 1998; Fr\u00e9maux et al., 2013; Gerstner et al., 2014).", "startOffset": 169, "endOffset": 238}, {"referenceID": 2, "context": "Fading Eligibility Traces In some models of biologically plausible learning, the activation between any pair of connected neurons is represented by an eligibility trace (Sutton and Barto, 1998; Fr\u00e9maux et al., 2013; Gerstner et al., 2014).", "startOffset": 169, "endOffset": 238}, {"referenceID": 3, "context": "Fading Eligibility Traces In some models of biologically plausible learning, the activation between any pair of connected neurons is represented by an eligibility trace (Sutton and Barto, 1998; Fr\u00e9maux et al., 2013; Gerstner et al., 2014).", "startOffset": 169, "endOffset": 238}, {"referenceID": 12, "context": "In real-world applications, such as robot navigation (Thrun et al., 2005), the agent can only work with limited sensory information available in each time step.", "startOffset": 53, "endOffset": 73}, {"referenceID": 11, "context": "Building concepts is related to the problem of generalization (Sutton and Barto, 1998) because in real-world tasks there might be too many states to store in the policy.", "startOffset": 62, "endOffset": 86}, {"referenceID": 2, "context": "This issue was also raised as an item for future work by Fr\u00e9maux et al. (2013), who initially also have considered a framework in which complete information is available to the learning agent.", "startOffset": 57, "endOffset": 79}], "year": 2017, "abstractText": "Reinforcement learning is a formal framework for modeling agents that learn to solve tasks. For example, one important task for animals is to navigate in an environment to find food or to return to their nest. In such tasks, the agent has to learn a path through the environment from start states to goal states, by visiting a sequence of intermediate states. The agent receives reward on a goal state. Concretely, we need to learn a policy that maps each encountered state to an immediate action that leads to a next state, eventually leading to a goal state. We say that a learning process has converged if eventually the policy will no longer change, i.e., the policy stabilizes. The intuition of paths and navigation policies can be applied generally. Indeed, in this article, we study navigation tasks formalized as a graph structure that, for each application of an action to a state, describes the possible successor states that could result from that application. In contrast to standard reinforcement learning, we essentially simplify numeric reward signals to boolean flags on the transitions in the graph. The resulting framework enables a clear theoretical study of how properties of the graph structure can cause convergence of the learning process. In particular, we formally study a learning process that detects revisits to states in the graph, i.e., we detect cycles, and the process keeps adjusting the policy until no more cycles are made. So, eventually, the agent goes straight to reward from each start state. We identify reducibility of the task graph as a sufficient condition for this learning process to converge. We also syntactically characterize the form of the final policy, which can be used to detect convergence in a simulation.", "creator": "LaTeX with hyperref package"}}}