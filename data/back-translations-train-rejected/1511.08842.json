{"id": "1511.08842", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "Efficient Sum of Outer Products Dictionary Learning (SOUP-DIL) - The $\\ell_0$ Method", "abstract": "The sparsity of natural signals and images in a transform domain or dictionary has been extensively exploited in several applications such as compression, denoising and inverse problems. More recently, data-driven adaptation of synthesis dictionaries has shown promise in many applications compared to fixed or analytical dictionary models. However, dictionary learning problems are typically non-convex and NP-hard, and the usual alternating minimization approaches for these problems are often computationally expensive, with the computations dominated by the NP-hard synthesis sparse coding step. In this work, we investigate an efficient method for $\\ell_{0}$ \"norm\"-based dictionary learning by first approximating the training data set with a sum of sparse rank-one matrices and then using a block coordinate descent approach to estimate the unknowns. The proposed block coordinate descent algorithm involves efficient closed-form solutions. In particular, the sparse coding step involves a simple form of thresholding. We provide a convergence analysis for the proposed block coordinate descent approach. Our numerical experiments show the promising performance and significant speed-ups provided by our method over the classical K-SVD scheme in sparse signal representation and image denoising.", "histories": [["v1", "Fri, 27 Nov 2015 22:32:43 GMT  (1903kb)", "http://arxiv.org/abs/1511.08842v1", "A short version of this work [arXiv:1511.06333] has also been submitted to ICLR 2016"], ["v2", "Fri, 21 Apr 2017 02:31:13 GMT  (1904kb)", "http://arxiv.org/abs/1511.08842v2", "This work is cited by the IEEE Transactions on Computational Imaging PaperarXiv:1511.06333(DOI: 10.1109/TCI.2017.2697206)"]], "COMMENTS": "A short version of this work [arXiv:1511.06333] has also been submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["saiprasad ravishankar", "raj rao nadakuditi", "jeffrey a fessler"], "accepted": false, "id": "1511.08842"}, "pdf": {"name": "1511.08842.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["fessler)@umich.edu."], "sections": [{"heading": null, "text": "ar Xiv: 151 1.08 842v 1 [cs.L G] 27 Nov 201 5Index Terms - Sparse representations, Dictionary learning, Image denoising, Fast algorithms, Machine learning, Convergence analysis, Block coordinate descent, Nonconvex optimizationI. INTRODUCTIONThe rarity of natural signals and images in a transformation area or dictionary has been widely used in several applications such as compression [1], denoising, compressed sensing and other inverse problems. Well-known models for the rarity are the synthesis, analysis [2] and transformational [3], [4] (or generalized analysis) models. More recently, the data-driven adaptation of sparse signal models has favored many applications [4] - [12] compared to fixed or analytical models. In this work we focus on the data-driven adaptation of the synthesis model and present a simple topic and highly efficient analysis just before dealing with this algorithm."}, {"heading": "A. Synthesis Model and Dictionary Learning", "text": "The question about the way in which she deals with the question about the way in which she answers the question about the way in which she answers the question about the way in which she answers the question about the way: \"The question about the way in which she answers the question about the way in which she answers the question about the way in which she asks the question about the way in which she answers the question about the way in which she answers the question about the way in which she answers the question about the way in which she answers the question about the way in which she answers the question about the way in which she asks the question about the way in which she asks the question about the way in which way she answers the question about the way in which way she answers the way in which way she answers the way in which she asks the question about the way in which way in which she asks the question about the way in which way in which she answers the way in which way in which she answers the way in which way in which she answers the question about the way in which way in which way she answers the question about the way in which way in which way she answers the way in which way in which she answers the way in which way in which way in which she answers the question about the way in which way in which way in which way in which she answers the way in which way in which she answers the way in which way in which she answers the question about the way in which way in which she answers the way in which she asks the way in which she asks the way in which she answers the way in which she asks the question about the way in which way in which she asks the way in which she asks the way in which she asks the way in which she answers the way in which she asks the way in which she asks the way in which she answers the question about the way in which she asks the way in which she asks the way in which she answers the way in which she asks the way in which she answers the way in which she asks the question about the way in which she asks the question about the way in which way in which she asks the way in which she answers the question about the way in which she asks the way in which she asks the way in which she asks the way in which she asks the way in which she asks the way in which she asks the way in which she answers the question about the way in which she and the way in which way in which she and the way in which she"}, {"heading": "B. Contributions", "text": "This work focuses on synthesis dictionary learning and examines a dictionary learning problem with a differential penalty of 0 instead of constraints. Our approach first models the training data as an approximate sum of sparse rank-one matrices (or outer products), then we use a simple and exact approach of derivation of block coordinates to estimate the factors of the different rank-one matrices. Specifically, the sparse encryption step in the algorithm uses a simple form of threshold, and the step of updating the dictionary includes a matrix-vector product that is efficiently calculated. We offer a convergence analysis of the proposed method of block coordinate derivation. Our numerical experiments show the promising performance and significant acceleration that our method offers over the classic K-SVD dictionaries, and the dictionary-vector product incorporates a frugal signaling and image comparison scheme that is much better than our SVD algorithm, but more detailed than the SVD."}, {"heading": "C. Organization", "text": "The rest of this paper is structured as follows: Section II discusses problem formulation for dictionary learning as well as some possible alternatives. Section III presents our dictionary learning algorithm and discusses its computational properties. Section IV presents a convergence analysis for the proposed algorithm. Section V illustrates the empirical convergence behavior of our method and demonstrates its usefulness for sparse signal representation and image resolution. Section VI presents conclusions and suggestions for future work."}, {"heading": "II. PROBLEM FORMULATIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. \u21130 Penalized Formulation", "text": "We consider a sparseness-punished variant of the problem (P0) [52]. In particular, by replacing the sparseness constraints in (P0) with a sparseness constraint (P0), we arrive at the following formulation: min D, C, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0."}, {"heading": "B. Alternative Formulations", "text": "Several variants of the SOUP learning problem (P1) could be constructed, with interesting effects. For example, the \"standard\" for sparseness could be replaced by \"standard 1.\" [60] Another interesting alternative to (P1) is the enforcement of orthogonal constraints in Dictionary D. In this case, the dictionary is divided into blocks (instead of individual atoms), each of which consists of orthogonal (square) blocks. For p = 2, the constraints take the form dT2j \u2212 1d2j = 0, 1 \u2264 j J / 2. In the extreme (more restrictive) case of p = n, the dictionary would consist of several (square) orthonormal blocks. For tensor data, problem (P1) can be modified by forcing the dictionary atoms in the form of a Kronecker product. The algorithm proposed in Section III can be easily expanded to examine this problem in detail (P1)."}, {"heading": "III. ALGORITHM AND PROPERTIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Algorithm", "text": "We propose a block degradation method to estimate the unknown variables in problem (P1) and problem (P1). (1) We refer to this step as the sparse encoding step in our method. (1) Once it is updated, we solve (P1) in relation to all other variables. (2) This step is referred to as the sparse encoding step or simply as the sparse encoding step in our method. (2) The algorithms update the factors of the different rankings. (2) Next, we describe the sparse encoding and the sparse encoding and the sparse encoding in the construction stage. (1) Sparse Coding Step: Minimizing (P1) in relation to cj leads to the following non-convex problem, where Ej \u2212 K 6 = j dkc T k is a fixed matrix based on most of the origin values of all other atoms."}, {"heading": "B. Computational Cost", "text": "For each of these procedures, the costs of data processing and preparation are calculated. (...) For each of these procedures, the costs of data processing are calculated. (...) For each of these procedures, the costs of data processing are calculated. (...) For each of these procedures, a large matrix is required. (...) This matrix can be updated sequentially and efficiently for each of these procedures by adding and subtracting the corresponding economical ranks. (...) However, this alternative procedure requires the storage of Etj. (...) This is often a large matrix for large N and n. (...) Instead, the procedure we use helps with memory processing. (...) We are now discussing the costs of each economical coding and atom update in the Fig."}, {"heading": "IV. CONVERGENCE ANALYSIS", "text": "In this section, we present a convergence analysis for the proposed SOUP-DIL algorithm for problem (P1). Problem (P1) is highly non-convex, as it is a non-convex function involving the products of several unknown vectors. However, the proposed algorithm is an exact block coordinate descendant method for problem (P1). Due to the high degree of non-convexity, the standard results on the convergence of block coordinate descendant methods (e.g. [63]) are not applicable here. Recent work [64] on the convergence of block coordinate descendant schemes uses assumptions (such as multi-convexity) that do not apply in our environment. Here, we discuss the convergence of our algorithm to the critical points of the problem."}, {"heading": "A. Definitions and Notations", "text": "The norm and internal product notation in definition 2 also correspond to the Euclidean settings of K-SVD [62] scaled similarly to O (Nn3).Definition 1: For a function g: Penalties g: Penalties g: Penalties g: Penalties g: Penalties g: Penalties g: Penalties f: (-), their domain is defined as Domg = (x) Calculation costs of efficiently implementing K-SVD [62]. The function g is correct if Domg does not empty.Definition 2: Leave g: Penalties g: Penalties g: Penalties g: Penalties g: Penalties g: Penalties f: (-)."}, {"heading": "B. Main Results", "text": "Assume that the initial iteration sequence in the SOUP-DIL is such that it limits the (P1) -D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-"}, {"heading": "V. NUMERICAL EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Framework", "text": "In this section, we present numerical results that illustrate the practical convergence behavior of the proposed algorithm, as well as its usefulness in displaying sparse signals and denociation of images. To demonstrate the convergence behavior and quality of sparse signal representations, we look at data generated with vectorized 2D patches of natural images. We also present results for patch-based image denoization by problem (P1), our dictatorial learning implementation was encoded in Matlab version R2015a, and we compare the performance of dictionaries learned with our method with those using the classic K-SVD4 method [5], [28] Matlab for SVD-learning SVD-2015D, and the denoization codes we use."}, {"heading": "B. Convergence Experiment", "text": "To investigate the convergence behavior of the proposed SOUPDIL algorithm, we extracted 3 x 104 8 x 8 fields from randomly selected locations in the images Barbara, Boat, 4The K-SVD method is a very popular dictionary learning scheme that has been applied to many image processing applications, including denoising [5], [6] and MR image reconstruction [9]. Mairal et al. [33] proposed a nonlocal method of image denoization that also utilizes learned dictionaries and achieves a denoization performance comparable to the well-known BM3D method. Similar extensions of our proposed method of denoization and other applications will be used for future work.5This version [69] was observed to typically provide a similar quality of results in our experiments as [68].7 Barbara Boat CoupleHill LenaFig algorithms. 2. The 512 x 512 standard images used in our experiments."}, {"heading": "C. Sparse Representation of Data", "text": "The second experiment with the same data as in Section V-B and learned values of size 64 x 256 for different types of parameters in (P1) (i.e. the quality of learned data was measured using NSRE measurements.) We compare the values we achieved through our algorithms with the values we achieved through the K-SVD measurements. (The K-SVD method was performed for several options of economy (number of non-zeros) of CT values. (P1) The P1 values were chosen to achieve similar average column levels. (%) 60 60 60 4,74,955,2x 10 8Iteration NumberO bjec tive Fun ctio n3.153,253,35S Parity (%)"}, {"heading": "VI. CONCLUSIONS", "text": "The proposed formulation learns the left and right singular vectors of these rank-one matrices using a zero penalty to enforce scarcity. We chose a method of derivation of block coordinates to efficiently update the unknown sparse codes and dictionary atoms. Specifically, the sparse encryption step in our algorithm is cheaply executed through a truncated hard threshold, and the dictionary update includes a single sparse vector atom that is efficiently computed. A convergence analysis for the proposed block-coordinate derivation algorithm has been presented for a highly non-convex problem. The proposed approach showed comparable or superior performance and significant acceleration compared to the classical K-SVD method [5], [28] in the sparse representation of signals and image compression. The usefulness of our method in other applications such as the expression of the compression of 8 or other areas of interest [8]."}, {"heading": "APPENDIX A PROOF OF THEOREM 1", "text": "Firstly, we are discussing the convergence of the objective order."}, {"heading": "APPENDIX B PROOF OF THEOREM 2", "text": "First, if we look at a subsequence {cqt, dqt} of the iteration sequence that approaches the accumulation point (c) (c). Therefore, due to the optimality of dqt in the dictionary atom update step (11) of the Qtth iteration, we have the following inequality for all d (cqt) T-points (2). (25) If we take the boundary t-points in (25) and use (24) T-points, this results in the following result that applies to every feasible d-point: Qt-points (cqt) T-points (2) T-points (2). (25) If we take the boundary t-points in (25) and use (24), this results in the following result that applies to every feasible d: Qt-points Y-points. (c) T-points (qt)."}, {"heading": "APPENDIX C PROOF OF THEOREM 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Critical Point Property", "text": "In this proof, for simplicity, we denote objective f (21) in the jth sparse stage of iterate t (Fig 1), we are objective f (1), we are objective f (1), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (f), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c), we are objective f (c)."}, {"heading": "B. Convergence of the Difference between Successive Iterates", "text": "We will show that each convergent sub-sequence of this sequence converges to zero, implying that zero is the only accumulation point, i.e. that {at} converges to 0. Similarly, a convergent sub-sequence {Cqnt \u2212 1, Dqt \u2212 1, Cqt, Dqt} (whose elements are formed by pairing consecutive elements of the iteration sequence) must have a convergent sub-sequence {(Cqnt \u2212 1, Dqt \u2212 1, Cqt, Dqt)} (whose elements are formed by pairing successive elements of the iteration sequence). Based on the results in appendix {Cqnt \u2212 1, Dqnt \u2212 1, Dqnt, Dqnt), we must have a converged sub-sequence."}], "references": [{"title": "An overview of JPEG-2000", "author": ["M.W. Marcellin", "M.J. Gormish", "A. Bilgin", "M.P. Boliek"], "venue": "Proc. Data Compression Conf., 2000, pp. 523\u2013541.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Analysis versus synthesis in signal priors", "author": ["M. Elad", "P. Milanfar", "R. Rubinstein"], "venue": "Inverse Problems, vol. 23, no. 3, pp. 947\u2013968, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Hadamard transform image coding", "author": ["W.K. Pratt", "J. Kane", "H.C. Andrews"], "venue": "Proc. IEEE, vol. 57, no. 1, pp. 58\u201368, 1969.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1969}, {"title": "Learning sparsifying transforms", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Trans. Signal Process., vol. 61, no. 5, pp. 1072\u20131086, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Trans. Image Process., vol. 15, no. 12, pp. 3736\u20133745, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Sparse representation for color image restoration", "author": ["J. Mairal", "M. Elad", "G. Sapiro"], "venue": "IEEE Trans. on Image Processing, vol. 17, no. 1, pp. 53\u201369, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Image sequence denoising via sparse and redundant representations", "author": ["M. Protter", "M. Elad"], "venue": "IEEE Trans. on Image Processing, vol. 18, no. 1, pp. 27\u201336, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "J. Mach. Learn. Res., vol. 11, pp. 19\u2013 60, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "MR image reconstruction from highly undersampled k-space data by dictionary learning", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Trans. Med. Imag., vol. 30, no. 5, pp. 1028\u20131041, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Analysis K-SVD: A dictionarylearning algorithm for the analysis sparse model", "author": ["R. Rubinstein", "T. Peleg", "M. Elad"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 3, pp. 661\u2013677, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning doubly sparse transforms for images", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Trans. Image Process., vol. 22, no. 12, pp. 4598\u20134612, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Structured overcomplete sparsifying transform learning with convergence guarantees and applications", "author": ["B. Wen", "S. Ravishankar", "Y. Bresler"], "venue": "International Journal of Computer Vision, vol. 114, no. 2-3, pp. 137\u2013 167, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "From sparse solutions of systems of equations to sparse modeling of signals and images", "author": ["A.M. Bruckstein", "D.L. Donoho", "M. Elad"], "venue": "SIAM Review, vol. 51, no. 1, pp. 34\u201381, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Subspace clustering", "author": ["R. Vidal"], "venue": "IEEE Signal Processing Magazine, vol. 28, no. 2, pp. 52\u201368, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparsity in unions of subspaces for classification and clustering of high-dimensional data", "author": ["E. Elhamifar", "R. Vidal"], "venue": "49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), 2011, pp. 1085\u20131089.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM J. Comput., vol. 24, no. 2, pp. 227\u2013234, Apr. 1995.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Adaptive greedy approximations", "author": ["G. Davis", "S. Mallat", "M. Avellaneda"], "venue": "Journal of Constructive Approximation, vol. 13, no. 1, pp. 57\u201398, 1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Orthogonal matching pursuit : recursive function approximation with applications to wavelet decomposition", "author": ["Y. Pati", "R. Rezaiifar", "P. Krishnaprasad"], "venue": "Asilomar Conf. on Signals, Systems and Comput., 1993, pp. 40\u201344 vol.1.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["S.G. Mallat", "Z. Zhang"], "venue": "IEEE Transactions on Signal Processing, vol. 41, no. 12, pp. 3397\u20133415, 1993.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1993}, {"title": "Neuromagnetic source imaging with FOCUSS: A recursive weighted minimum norm algorithm", "author": ["I.F. Gorodnitsky", "J. George", "B.D. Rao"], "venue": "Electrocephalography and Clinical Neurophysiology, vol. 95, pp. 231\u2013251, 1995.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "A new algorithm for computing sparse solutions to linear inverse problems", "author": ["G. Harikumar", "Y. Bresler"], "venue": "ICASSP, may 1996, pp. 1331\u2013 1334.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Atomic decomposition by basis pursuit", "author": ["S.S. Chen", "D.L. Donoho", "M.A. Saunders"], "venue": "SIAM J. Sci. Comput., vol. 20, no. 1, pp. 33\u201361, 1998.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics, vol. 32, pp. 407\u2013499, 2004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Cosamp: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J. Tropp"], "venue": "Applied and Computational Harmonic Analysis, vol. 26, no. 3, pp. 301 \u2013 321, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Subspace pursuit for compressive sensing signal reconstruction", "author": ["W. Dai", "O. Milenkovic"], "venue": "IEEE Trans. Information Theory, vol. 55, no. 5, pp. 2230\u20132249, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Nature, vol. 381, no. 6583, pp. 607\u2013609, 1996.  12", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1996}, {"title": "Method of optimal directions for frame design", "author": ["K. Engan", "S. Aase", "J. Hakon-Husoy"], "venue": "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, 1999, pp. 2443\u20132446.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Transactions on signal processing, vol. 54, no. 11, pp. 4311\u20134322, 2006.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Dictionary learning for sparse approximations with the majorization method", "author": ["M. Yaghoobi", "T. Blumensath", "M. Davies"], "venue": "IEEE Transaction on Signal Processing, vol. 57, no. 6, pp. 2178\u20132191, 2009.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Compression of facial images using the k-svd algorithm", "author": ["O. Bryt", "M. Elad"], "venue": "Journal of Visual Communication and Image Representation, vol. 19, no. 4, pp. 270\u2013282, 2008.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse and redundant modeling of image content using an image-signature-dictionary", "author": ["M. Aharon", "M. Elad"], "venue": "SIAM Journal on Imaging Sciences, vol. 1, no. 3, pp. 228\u2013247, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning multiscale sparse representations for image and video restoration", "author": ["J. Mairal", "G. Sapiro", "M. Elad"], "venue": "SIAM Multiscale Modeling and Simulation, vol. 7, no. 1, pp. 214\u2013241, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Non-local sparse models for image restoration", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "IEEE International Conference on Computer Vision, Sept 2009, pp. 2272\u20132279.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Alternatively constrained dictionary learning for image superresolution", "author": ["X. Lu", "Y. Yuan", "P. Yan"], "venue": "IEEE Transactions on Cybernetics, vol. 44, no. 3, pp. 366\u2013377, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Classification and clustering via dictionary learning with structured incoherence and shared features", "author": ["I. Ramirez", "P. Sprechmann", "G. Sapiro"], "venue": "Proc. IEEE International Conference on Computer Vision and Pattern Recognition (CVPR) 2010, 2010, pp. 3501\u20133508.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "A dictionary learning approach for classification: Separating the particularity and the commonality", "author": ["S. Kong", "D. Wang"], "venue": "Proceedings of the 12th European Conference on Computer Vision, 2012, pp. 186\u2013199.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Label consistent k-svd: Learning a discriminative dictionary for recognition", "author": ["Z. Jiang", "Z. Lin", "L.S. Davis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 11, pp. 2651\u20132664, 2013.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse representations for limited data tomography", "author": ["H.Y. Liao", "G. Sapiro"], "venue": "Proc. IEEE International Symposium on Biomedical Imaging (ISBI), 2008, pp. 1375\u20131378.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiscale dictionary learning for MRI", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "Proc. ISMRM, 2011, p. 2830.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Undersampled dynamic magnetic resonance imaging using patch-based spatiotemporal dictionaries", "author": ["Y. Wang", "Y. Zhou", "L. Ying"], "venue": "2013 IEEE 10th International Symposium on Biomedical Imaging (ISBI), April 2013, pp. 294\u2013297.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian nonparametric dictionary learning for compressed sensing mri", "author": ["Y. Huang", "J. Paisley", "Q. Lin", "X. Ding", "X. Fu", "X.P. Zhang"], "venue": "IEEE Trans. Image Process., vol. 23, no. 12, pp. 5007\u20135019, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Dictionary identification\u2013sparse matrixfactorization via l1 -minimization", "author": ["R. Gribonval", "K. Schnass"], "venue": "IEEE Trans. Inform. Theory, vol. 56, no. 7, pp. 3523\u20133539, 2010.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning incoherent dictionaries for sparse approximation using iterative projections and rotations", "author": ["D. Barchiesi", "M.D. Plumbley"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 8, pp. 2055\u20132065, 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Double sparsity: Learning sparse dictionaries for sparse signal approximation", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 3, pp. 1553\u20131564, 2010.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Recursive least squares dictionary learning algorithm", "author": ["K. Skretting", "K. Engan"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 4, pp. 2121\u20132130, 2010.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-scale dictionary learning using wavelets", "author": ["B. Ophir", "M. Lustig", "M. Elad"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 5, no. 5, pp. 1014\u20131024, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Dictionary training for sparse representation as generalization of k-means clustering", "author": ["S.K. Sahoo", "A. Makur"], "venue": "IEEE Signal Processing Letters, vol. 20, no. 6, pp. 587\u2013590, June 2013.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving dictionary learning: Multiple dictionary updates and coefficient reuse", "author": ["L.N. Smith", "M. Elad"], "venue": "IEEE Signal Processing Letters, vol. 20, no. 1, pp. 79\u201382, Jan 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Dictionary learning for sparse representation: A novel approach", "author": ["M. Sadeghi", "M. Babaie-Zadeh", "C. Jutten"], "venue": "IEEE Signal Processing Letters, vol. 20, no. 12, pp. 1195\u20131198, Dec 2013.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "A sequential dictionary learning algorithm with enforced sparsity", "author": ["A.-K. Seghouane", "M. Hanif"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 3876\u2013 3880.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Correlation based online dictionary learning algorithm", "author": ["Y. Naderahmadian", "S. Beheshti", "M. Tinati"], "venue": "IEEE Transactions on Signal Processing, 2015.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2015}, {"title": "L0 norm based dictionary learning by proximal methods with global convergence", "author": ["C. Bao", "H. Ji", "Y. Quan", "Z. Shen"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 3858\u2013 3865.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}, {"title": "Direct optimization of the dictionary learning problem", "author": ["A. Rakotomamonjy"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 22, pp. 5495\u20135506, 2013.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "Separable dictionary learning", "author": ["S. Hawe", "M. Seibert", "M. Kleinsteuber"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013, pp. 438\u2013445.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}, {"title": "Exact recovery of sparselyused dictionaries", "author": ["D.A. Spielman", "H. Wang", "J. Wright"], "venue": "Proceedings of the 25th Annual Conference on Learning Theory, 2012, pp. 37.1\u201337.18.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning sparsely used overcomplete dictionaries via alternating minimization", "author": ["A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli", "R. Tandon"], "venue": "2013, preprint: http://arxiv.org/abs/1310.7991.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "New algorithms for learning incoherent and overcomplete dictionaries", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "2013, preprint: http://arxiv.org/pdf/1308.6273v5.pdf.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2013}, {"title": "A fast patch-dictionary method for whole-image recovery", "author": ["Y. Xu", "W. Yin"], "venue": "2013, UCLA CAM report 13-38. [Online]. Available: ftp://ftp.math.ucla.edu/pub/camreport/cam13-38.pdf", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning sparsely used overcomplete dictionaries", "author": ["A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli", "R. Tandon"], "venue": "Journal of Machine Learning Research, vol. 35, pp. 1\u201315, 2014.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning overcomplete dictionaries based on atom-by-atom updating", "author": ["M. Sadeghi", "M. Babaie-Zadeh", "C. Jutten"], "venue": "IEEE Transactions on Signal Processing, vol. 62, no. 4, pp. 883\u2013891, 2014.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "Dictionary learning for sparse coding: Algorithms and analysis", "author": ["C. Bao", "H. Ji", "Y. Quan", "Z. Shen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient implementation of the k-svd algorithm using batch orthogonal matching pursuit", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "http://www.cs.technion.ac.il/\u223cronrubin/Publications/KSVD-OMP-v2.pdf, 2008, technion - Computer Science Department - Technical Report.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2008}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "J. Optim. Theory Appl., vol. 109, no. 3, pp. 475\u2013494, 2001.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2001}, {"title": "A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion", "author": ["Y. Xu", "W. Yin"], "venue": "SIAM Journal on Imaging Sciences, vol. 6, no. 3, pp. 1758\u20131789, 2013.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2013}, {"title": "Variational Analysis and Generalized Differentiation", "author": ["B.S. Mordukhovich"], "venue": "Vol. I: Basic theory. Springer-Verlag,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2006}, {"title": "Image denoising by sparse 3D transform-domain collaborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Trans. on Image Processing, vol. 16, no. 8, pp. 2080\u20132095, 2007.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2007}, {"title": "Michael Elad personal page", "author": ["M. Elad"], "venue": "http://www.cs.technion.ac.il/\u223celad/Various/KSVD Matlab ToolBox.zip, 2009, [Online; accessed Nov. 2015].", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient blind compressed sensing using sparsifying transforms with convergence guarantees and application to magnetic resonance imaging", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "SIAM Journal on Imaging Sciences, vol. 8, no. 4, pp. 2519\u20132557, 2015.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2015}, {"title": "Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the kurdyka-\u0142ojasiewicz inequality", "author": ["H. Attouch", "J. Bolte", "P. Redont", "A. Soubeyran"], "venue": "Math. Oper. Res., vol. 35, no. 2, pp. 438\u2013457, May 2010.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION The sparsity of natural signals and images in a transform domain or dictionary has been extensively exploited in several applications such as compression [1], denoising, compressed sensing and other inverse problems.", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": "Well-known models for sparsity include the synthesis, analysis [2], and the transform [3], [4] (or generalized analysis) models.", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "Well-known models for sparsity include the synthesis, analysis [2], and the transform [3], [4] (or generalized analysis) models.", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "Well-known models for sparsity include the synthesis, analysis [2], and the transform [3], [4] (or generalized analysis) models.", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "More recently, the data-driven adaptation of sparse signal models has benefited many applications [4]\u2013[12] compared to fixed or analytical models.", "startOffset": 98, "endOffset": 101}, {"referenceID": 11, "context": "More recently, the data-driven adaptation of sparse signal models has benefited many applications [4]\u2013[12] compared to fixed or analytical models.", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": ", y = Dx+e with x \u2208 R sparse, and e is assumed to be a small modeling error or approximation error in the signal domain [13].", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": "Since different candidate signals may be approximately spanned by different subsets of columns in the dictionary D, the synthesis model is also known as a union of subspaces model [14], [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "Since different candidate signals may be approximately spanned by different subsets of columns in the dictionary D, the synthesis model is also known as a union of subspaces model [14], [15].", "startOffset": 186, "endOffset": 190}, {"referenceID": 15, "context": "The synthesis sparse coding problem is NP-hard (Non-deterministic Polynomial-time hard) [16], [17].", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "The synthesis sparse coding problem is NP-hard (Non-deterministic Polynomial-time hard) [16], [17].", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "Numerous algorithms [18]\u2013[25] including greedy and relaxation algorithms have been proposed for this problem.", "startOffset": 20, "endOffset": 24}, {"referenceID": 24, "context": "Numerous algorithms [18]\u2013[25] including greedy and relaxation algorithms have been proposed for this problem.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "More recently, the data-driven adaptation of synthesis dictionaries, called dictionary learning, has been investigated in several works [8], [26]\u2013[29].", "startOffset": 136, "endOffset": 139}, {"referenceID": 25, "context": "More recently, the data-driven adaptation of synthesis dictionaries, called dictionary learning, has been investigated in several works [8], [26]\u2013[29].", "startOffset": 141, "endOffset": 145}, {"referenceID": 28, "context": "More recently, the data-driven adaptation of synthesis dictionaries, called dictionary learning, has been investigated in several works [8], [26]\u2013[29].", "startOffset": 146, "endOffset": 150}, {"referenceID": 4, "context": "Dictionary learning has shown promise in several applications including compression, denoising, inpainting, deblurring, demosaicing, super-resolution, and classification [5], [6], [30]\u2013[37].", "startOffset": 170, "endOffset": 173}, {"referenceID": 5, "context": "Dictionary learning has shown promise in several applications including compression, denoising, inpainting, deblurring, demosaicing, super-resolution, and classification [5], [6], [30]\u2013[37].", "startOffset": 175, "endOffset": 178}, {"referenceID": 29, "context": "Dictionary learning has shown promise in several applications including compression, denoising, inpainting, deblurring, demosaicing, super-resolution, and classification [5], [6], [30]\u2013[37].", "startOffset": 180, "endOffset": 184}, {"referenceID": 36, "context": "Dictionary learning has shown promise in several applications including compression, denoising, inpainting, deblurring, demosaicing, super-resolution, and classification [5], [6], [30]\u2013[37].", "startOffset": 185, "endOffset": 189}, {"referenceID": 37, "context": "It has also been demonstrated to be useful in inverse problems such as those in tomography [38], and magnetic resonance imaging (MRI) [9], [39]\u2013[41].", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "It has also been demonstrated to be useful in inverse problems such as those in tomography [38], and magnetic resonance imaging (MRI) [9], [39]\u2013[41].", "startOffset": 134, "endOffset": 137}, {"referenceID": 38, "context": "It has also been demonstrated to be useful in inverse problems such as those in tomography [38], and magnetic resonance imaging (MRI) [9], [39]\u2013[41].", "startOffset": 139, "endOffset": 143}, {"referenceID": 40, "context": "It has also been demonstrated to be useful in inverse problems such as those in tomography [38], and magnetic resonance imaging (MRI) [9], [39]\u2013[41].", "startOffset": 144, "endOffset": 148}, {"referenceID": 27, "context": "Given a collection of training signals {yi} N i=1 that are represented as columns of the matrix Y \u2208 R , the dictionary learning problem is often formulated as follows [28]", "startOffset": 167, "endOffset": 171}, {"referenceID": 41, "context": "The columns of the dictionary are constrained to have unit norm to avoid the scaling ambiguity [42].", "startOffset": 95, "endOffset": 99}, {"referenceID": 34, "context": ", incoherence [35], [43]) for the dictionary D, or solving an online version (where the dictionary is updated sequentially as new training signals arrive) of the problem [8].", "startOffset": 14, "endOffset": 18}, {"referenceID": 42, "context": ", incoherence [35], [43]) for the dictionary D, or solving an online version (where the dictionary is updated sequentially as new training signals arrive) of the problem [8].", "startOffset": 20, "endOffset": 24}, {"referenceID": 7, "context": ", incoherence [35], [43]) for the dictionary D, or solving an online version (where the dictionary is updated sequentially as new training signals arrive) of the problem [8].", "startOffset": 170, "endOffset": 173}, {"referenceID": 7, "context": "Algorithms for Problem (P0) or its variants [8], [27]\u2013 [29], [44]\u2013[52] typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 44, "endOffset": 47}, {"referenceID": 26, "context": "Algorithms for Problem (P0) or its variants [8], [27]\u2013 [29], [44]\u2013[52] typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 49, "endOffset": 53}, {"referenceID": 28, "context": "Algorithms for Problem (P0) or its variants [8], [27]\u2013 [29], [44]\u2013[52] typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 55, "endOffset": 59}, {"referenceID": 43, "context": "Algorithms for Problem (P0) or its variants [8], [27]\u2013 [29], [44]\u2013[52] typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 61, "endOffset": 65}, {"referenceID": 51, "context": "Algorithms for Problem (P0) or its variants [8], [27]\u2013 [29], [44]\u2013[52] typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 66, "endOffset": 70}, {"referenceID": 27, "context": ", [28], [48], [50]) also partially update X in the dictionary update step.", "startOffset": 2, "endOffset": 6}, {"referenceID": 47, "context": ", [28], [48], [50]) also partially update X in the dictionary update step.", "startOffset": 8, "endOffset": 12}, {"referenceID": 49, "context": ", [28], [48], [50]) also partially update X in the dictionary update step.", "startOffset": 14, "endOffset": 18}, {"referenceID": 52, "context": "A few recent methods attempt to solve for D and X jointly in an iterative fashion [53], [54].", "startOffset": 82, "endOffset": 86}, {"referenceID": 53, "context": "A few recent methods attempt to solve for D and X jointly in an iterative fashion [53], [54].", "startOffset": 88, "endOffset": 92}, {"referenceID": 27, "context": "The K-SVD method [28] has been particularly popular in numerous applications [5]\u2013[7], [9], [30], [39].", "startOffset": 17, "endOffset": 21}, {"referenceID": 4, "context": "The K-SVD method [28] has been particularly popular in numerous applications [5]\u2013[7], [9], [30], [39].", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "The K-SVD method [28] has been particularly popular in numerous applications [5]\u2013[7], [9], [30], [39].", "startOffset": 81, "endOffset": 84}, {"referenceID": 8, "context": "The K-SVD method [28] has been particularly popular in numerous applications [5]\u2013[7], [9], [30], [39].", "startOffset": 86, "endOffset": 89}, {"referenceID": 29, "context": "The K-SVD method [28] has been particularly popular in numerous applications [5]\u2013[7], [9], [30], [39].", "startOffset": 91, "endOffset": 95}, {"referenceID": 38, "context": "The K-SVD method [28] has been particularly popular in numerous applications [5]\u2013[7], [9], [30], [39].", "startOffset": 97, "endOffset": 101}, {"referenceID": 51, "context": "Some recent works [52], [55]\u2013[59] have studied the convergence of (specific) synthesis dictionary learning algorithms.", "startOffset": 18, "endOffset": 22}, {"referenceID": 54, "context": "Some recent works [52], [55]\u2013[59] have studied the convergence of (specific) synthesis dictionary learning algorithms.", "startOffset": 24, "endOffset": 28}, {"referenceID": 58, "context": "Some recent works [52], [55]\u2013[59] have studied the convergence of (specific) synthesis dictionary learning algorithms.", "startOffset": 29, "endOffset": 33}, {"referenceID": 51, "context": "[52] find that their method, although a fast proximal scheme, denoises less effectively than the KSVD method [5].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[52] find that their method, although a fast proximal scheme, denoises less effectively than the KSVD method [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 59, "context": "Our work shares similarities with a recent dictionary learning approach [60] that exploits a sum of outer products model for the training data.", "startOffset": 72, "endOffset": 76}, {"referenceID": 59, "context": "However, the specific problem formulation and algorithm studied in this work differ from the prior work [60].", "startOffset": 104, "endOffset": 108}, {"referenceID": 59, "context": "Importantly, unlike the previous approach [60], we provide a detailed convergence analysis of the proposed algorithm and also demonstrate its usefulness (both in terms of speed and quality of results) in sparse representation of natural signals and in image denoising.", "startOffset": 42, "endOffset": 46}, {"referenceID": 51, "context": "l0 Penalized Formulation We consider a sparsity penalized variant of Problem (P0) [52].", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "Such a Sum of OUter Products (SOUP) representation has been exploited in previous dictionary learning algorithms [28], [48].", "startOffset": 113, "endOffset": 117}, {"referenceID": 47, "context": "Such a Sum of OUter Products (SOUP) representation has been exploited in previous dictionary learning algorithms [28], [48].", "startOffset": 119, "endOffset": 123}, {"referenceID": 51, "context": "We also enforce the constraint \u2016cj\u2016\u221e \u2264 L, with L > 0, in (P1) [52] (e.", "startOffset": 62, "endOffset": 66}, {"referenceID": 59, "context": "For example, the l0 \u201cnorm\u201d for sparsity could be replaced by the l1 norm [60].", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": "While Propositions 1 and 2 provide the minimizers of (2) and (11) for the case of real-valued matrices/vectors in the problems, these solutions are trivially extended to the complex-valued case (that may be useful in applications such as magnetic resonance imaging [9]) by using a Hermitian transpose.", "startOffset": 265, "endOffset": 268}, {"referenceID": 4, "context": "For example, the initial sparse coefficients could be set to zero, and the initial dictionary could be a known analytical dictionary such as the overcomplete DCT [5].", "startOffset": 162, "endOffset": 165}, {"referenceID": 51, "context": "[52], [61] has a per-iteration computational cost of at least 2NJn + 6\u03b1NJn + 4\u03b1Nn.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "[52], [61] has a per-iteration computational cost of at least 2NJn + 6\u03b1NJn + 4\u03b1Nn.", "startOffset": 6, "endOffset": 10}, {"referenceID": 27, "context": "This is lower than the periteration cost of learning an n \u00d7 J synthesis dictionary D using K-SVD [28], which scales3 (assuming that the synthesis sparsity level s \u221d n and J \u221d n in K-SVD) as O(Nn).", "startOffset": 97, "endOffset": 101}, {"referenceID": 62, "context": ", [63]) do not apply here.", "startOffset": 2, "endOffset": 6}, {"referenceID": 63, "context": "More recent works [64] on the convergence of block coordinate descent schemes use assumptions (such as multi-convexity) that do not hold in our setting.", "startOffset": 18, "endOffset": 22}, {"referenceID": 64, "context": "Definitions and Notations First, we review the Fr\u00e9chet sub-differential of a function [65], [66].", "startOffset": 92, "endOffset": 96}, {"referenceID": 61, "context": "3When s \u221d n and J \u221d n, the per-iteration computational cost of the efficient implementation of K-SVD [62] also scales similarly as O(Nn).", "startOffset": 101, "endOffset": 105}, {"referenceID": 4, "context": "We compare the performance of dictionaries learned using our method to those learned using the classical K-SVD4 method [5], [28].", "startOffset": 119, "endOffset": 122}, {"referenceID": 27, "context": "We compare the performance of dictionaries learned using our method to those learned using the classical K-SVD4 method [5], [28].", "startOffset": 124, "endOffset": 128}, {"referenceID": 4, "context": "For K-SVD learning and denoising [5], [28], we consider the original Matlab implementation of the methods available from Michael Elad\u2019s website [68].", "startOffset": 33, "endOffset": 36}, {"referenceID": 27, "context": "For K-SVD learning and denoising [5], [28], we consider the original Matlab implementation of the methods available from Michael Elad\u2019s website [68].", "startOffset": 38, "endOffset": 42}, {"referenceID": 66, "context": "For K-SVD learning and denoising [5], [28], we consider the original Matlab implementation of the methods available from Michael Elad\u2019s website [68].", "startOffset": 144, "endOffset": 148}, {"referenceID": 61, "context": "A fast version5 of K-SVD [62] that also uses MEX/C implementations of sparse coding and some sub-steps of dictionary update, is publicly available [69].", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "Convergence Experiment To study the convergence behavior of the proposed SOUPDIL Algorithm, we extracted 3 \u00d7 10 patches of size 8 \u00d7 8 from randomly chosen locations in the images Barbara, Boat, 4The K-SVD method is a highly popular dictionary learning scheme that has been applied to many image processing applications including denoising [5], [6] and MR image reconstruction [9].", "startOffset": 339, "endOffset": 342}, {"referenceID": 5, "context": "Convergence Experiment To study the convergence behavior of the proposed SOUPDIL Algorithm, we extracted 3 \u00d7 10 patches of size 8 \u00d7 8 from randomly chosen locations in the images Barbara, Boat, 4The K-SVD method is a highly popular dictionary learning scheme that has been applied to many image processing applications including denoising [5], [6] and MR image reconstruction [9].", "startOffset": 344, "endOffset": 347}, {"referenceID": 8, "context": "Convergence Experiment To study the convergence behavior of the proposed SOUPDIL Algorithm, we extracted 3 \u00d7 10 patches of size 8 \u00d7 8 from randomly chosen locations in the images Barbara, Boat, 4The K-SVD method is a highly popular dictionary learning scheme that has been applied to many image processing applications including denoising [5], [6] and MR image reconstruction [9].", "startOffset": 376, "endOffset": 379}, {"referenceID": 32, "context": "[33] proposed a nonlocal method for image denoising that also exploits learned dictionaries and achieves denoising performance comparable to the well-known BM3D [67] denoising method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 65, "context": "[33] proposed a nonlocal method for image denoising that also exploits learned dictionaries and achieves denoising performance comparable to the well-known BM3D [67] denoising method.", "startOffset": 161, "endOffset": 165}, {"referenceID": 66, "context": "5This version [69] was observed to typically provide similar quality of results in our experiments as [68].", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": "Specifically, the initial estimate for C is an allzero matrix, and the initial estimate for D is the overcomplete DCT [5], [68].", "startOffset": 118, "endOffset": 121}, {"referenceID": 66, "context": "Specifically, the initial estimate for C is an allzero matrix, and the initial estimate for D is the overcomplete DCT [5], [68].", "startOffset": 123, "endOffset": 127}, {"referenceID": 51, "context": "[52] showed that the distance between successive iterates may not converge to zero for popular algorithms such as K-SVD.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "We compare the NSRE values achieved by our algorithm to those achieved by the K-SVD dictionary learning scheme [28] for the same data.", "startOffset": 111, "endOffset": 115}, {"referenceID": 66, "context": "Sparsity Percentage R un tim es ( se co nd s) K\u2212SVD [68] SOUP\u2212DIL K\u2212SVD [69]", "startOffset": 52, "endOffset": 56}, {"referenceID": 27, "context": "Behavior of the SOUP-DIL Algorithm and K-SVD [28] at various sparsities (or sparsity factors): (a) NSRE (percentage) for SOUP-DIL; (b) Improvements in NSRE (in decibels) provided by SOUP-DIL over K-SVD; and (c) runtimes for SOUP-DIL, the matlab implementation of K-SVD [28], [68], and the efficient (partial) MEX/C implementation of K-SVD [62], [69].", "startOffset": 45, "endOffset": 49}, {"referenceID": 27, "context": "Behavior of the SOUP-DIL Algorithm and K-SVD [28] at various sparsities (or sparsity factors): (a) NSRE (percentage) for SOUP-DIL; (b) Improvements in NSRE (in decibels) provided by SOUP-DIL over K-SVD; and (c) runtimes for SOUP-DIL, the matlab implementation of K-SVD [28], [68], and the efficient (partial) MEX/C implementation of K-SVD [62], [69].", "startOffset": 269, "endOffset": 273}, {"referenceID": 66, "context": "Behavior of the SOUP-DIL Algorithm and K-SVD [28] at various sparsities (or sparsity factors): (a) NSRE (percentage) for SOUP-DIL; (b) Improvements in NSRE (in decibels) provided by SOUP-DIL over K-SVD; and (c) runtimes for SOUP-DIL, the matlab implementation of K-SVD [28], [68], and the efficient (partial) MEX/C implementation of K-SVD [62], [69].", "startOffset": 275, "endOffset": 279}, {"referenceID": 61, "context": "Behavior of the SOUP-DIL Algorithm and K-SVD [28] at various sparsities (or sparsity factors): (a) NSRE (percentage) for SOUP-DIL; (b) Improvements in NSRE (in decibels) provided by SOUP-DIL over K-SVD; and (c) runtimes for SOUP-DIL, the matlab implementation of K-SVD [28], [68], and the efficient (partial) MEX/C implementation of K-SVD [62], [69].", "startOffset": 339, "endOffset": 343}, {"referenceID": 66, "context": "4(c) compares the runtimes of our method to those of the unoptimized Matlab implementation of K-SVD [68] as well as the efficient (partial) MEX/C implementation [69] of K-SVD demonstrating large speedups of 40-50 times for SOUP-DIL over the first K-SVD implementation (at most sparsities), while the runtimes for the SOUP-DIL method are about the same as those of the second K-SVD implementation.", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "The optimal x\u0302 is obtained [5] by summing together the denoised patch estimates D\u0302\u03b1\u0302j at their respective 2D locations, and computing a weighted average between this result and the noisy image.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "K-SVD based denoising [5] involves a similar methodology as described above for (P1), but differs in the dictionary learning procedure, where the l0 \u201cnorms\u201d of the sparse codes are minimized so that a fitting constraint or error constraint of", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "Such a constraint serves as a strong prior [5], [6], and is a key reason for the denoising capability of K-SVD.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Such a constraint serves as a strong prior [5], [6], and is a key reason for the denoising capability of K-SVD.", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "26 TABLE I PSNR VALUES IN DECIBELS FOR DENOISING WITH OVERCOMPLETE DCT (O-DCT), K-SVD [5], AND SOUP-DIL.", "startOffset": 86, "endOffset": 89}, {"referenceID": 4, "context": "Note that for both our method and for K-SVD [5], [68], the means of the patches are removed prior to learning and added back afterwards to obtain the denoised estimates.", "startOffset": 44, "endOffset": 47}, {"referenceID": 66, "context": "Note that for both our method and for K-SVD [5], [68], the means of the patches are removed prior to learning and added back afterwards to obtain the denoised estimates.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "We then denoise these images using both the SOUP-DIL denoising method outlined above and K-SVD [5], [68].", "startOffset": 95, "endOffset": 98}, {"referenceID": 66, "context": "We then denoise these images using both the SOUP-DIL denoising method outlined above and K-SVD [5], [68].", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "6The built-in parameter settings in the K-SVD denoising implementation [5], [68] are very similar except that K-SVD uses a subset of the patches for training.", "startOffset": 71, "endOffset": 74}, {"referenceID": 66, "context": "6The built-in parameter settings in the K-SVD denoising implementation [5], [68] are very similar except that K-SVD uses a subset of the patches for training.", "startOffset": 76, "endOffset": 80}, {"referenceID": 66, "context": "In the latter case, the same strategy (and parameter settings [68]) as used by K-SVD based denoising is adopted but while skipping the learning process.", "startOffset": 62, "endOffset": 66}, {"referenceID": 66, "context": "We have observed similar effects as in Section V-C for the runtimes of SOUPDIL denoising vis-a-vis the unoptimized Matlab [68] or the efficient (partial MEX/C) [69] implementations of the K-SVD approach.", "startOffset": 122, "endOffset": 126}, {"referenceID": 4, "context": "The proposed approach had comparable or superior performance and significant speed-ups over the classical K-SVD method [5], [28] in sparse signal representation and image denoising.", "startOffset": 119, "endOffset": 122}, {"referenceID": 27, "context": "The proposed approach had comparable or superior performance and significant speed-ups over the classical K-SVD method [5], [28] in sparse signal representation and image denoising.", "startOffset": 124, "endOffset": 128}, {"referenceID": 67, "context": "The usefulness of our method in other applications such as blind compressed sensing [70] or other inverse problems in imaging merits further study.", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "Extensions of the proposed method for online learning [8] are also of potential interest.", "startOffset": 54, "endOffset": 57}, {"referenceID": 68, "context": "Finally, (see Proposition 3 in [71]) the subdifferential \u2202f at (c, d) satisfies \u2202f (c, d) = \u2202fc (c, d) \u00d7 \u2202fd (c, d).", "startOffset": 31, "endOffset": 35}], "year": 2017, "abstractText": "The sparsity of natural signals and images in a transform domain or dictionary has been extensively exploited in several applications such as compression, denoising and inverse problems. More recently, data-driven adaptation of synthesis dictionaries has shown promise in many applications compared to fixed or analytical dictionary models. However, dictionary learning problems are typically non-convex and NP-hard, and the usual alternating minimization approaches for these problems are often computationally expensive, with the computations dominated by the NP-hard synthesis sparse coding step. In this work, we investigate an efficient method for l0 \u201cnorm\u201d-based dictionary learning by first approximating the training data set with a sum of sparse rank-one matrices and then using a block coordinate descent approach to estimate the unknowns. The proposed block coordinate descent algorithm involves efficient closed-form solutions. In particular, the sparse coding step involves a simple form of thresholding. We provide a convergence analysis for the proposed block coordinate descent approach. Our numerical experiments show the promising performance and significant speed-ups provided by our method over the classical K-SVD scheme in sparse signal representation and image denoising.", "creator": "LaTeX with hyperref package"}}}