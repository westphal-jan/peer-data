{"id": "1511.08967", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2015", "title": "Robotic Search & Rescue via Online Multi-task Reinforcement Learning", "abstract": "Reinforcement learning (RL) is a general and well-known method that a robot can use to learn an optimal control policy to solve a particular task. We would like to build a versatile robot that can learn multiple tasks, but using RL for each of them would be prohibitively expensive in terms of both time and wear-and-tear on the robot. To remedy this problem, we use the Policy Gradient Efficient Lifelong Learning Algorithm (PG-ELLA), an online multi-task RL algorithm that enables the robot to efficiently learn multiple consecutive tasks by sharing knowledge between these tasks to accelerate learning and improve performance. We implemented and evaluated three RL methods--Q-learning, policy gradient RL, and PG-ELLA--on a ground robot whose task is to find a target object in an environment under different surface conditions. In this paper, we discuss our implementations as well as present an empirical analysis of their learning performance.", "histories": [["v1", "Sun, 29 Nov 2015 04:33:51 GMT  (1976kb,D)", "http://arxiv.org/abs/1511.08967v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.RO", "authors": ["lisa lee"], "accepted": false, "id": "1511.08967"}, "pdf": {"name": "1511.08967.pdf", "metadata": {"source": "META", "title": "Robotic Search & Rescue via Online Multi-task Reinforcement Learning", "authors": ["Lisa Lee", "Eric Eaton", "Haitham Bou Ammar", "Christopher Clingerman"], "emails": ["LILEE@PRINCETON.EDU", "EEATON@SEAS.UPENN.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, it is as if it is a reactionary project that is able to retaliate."}, {"heading": "2. Single-Task Reinforcement Learning", "text": "The amplification-learning problem can be formulated as a Markov decision-making process (MDP) in which the problem model < S, A, P, R, \u03b3 > consists of a state area S, a series of measures A, a state transition probability function P: S \u00b7 A \u00b7 S 7 \u2192 [0, 1], which describes the dynamics of the environment, a reward function r: S \u00b7 A \u00b7 S 7 \u2192 R, and a discount factor \u03b3 [0, 1], which reverses the meaning of earlier rewards for later rewards. We have an agent and an environment that interacts continuously, where the actor selects measures and the environment that respond to these actions and presents new situations to the actor. At any time, the actor t-Z + receives the current state of the environment, and on this basis he selects an action at the A (st), where A (st) is the action and the environment the reaction to these actions. A (as a consequence of the action of the actor and the present state) is expected at the place where A (the environment is expected) the reward (the action is expected), the action is expected at the place (the A) where A (the environment is expected) the reward (the)."}, {"heading": "2.1. Q-learning", "text": "In Q-Learning, we have a finite discrete state space S and a finite set of discrete actions A. The algorithm stores a function Q: S \u00b7 A 7 \u2192 R that estimates the quality of a state pair. Before learning begins, Q is initialized to return any fixed value. In each step t-Z +, the algorithm then selects an action at = argmaxaQt (st, a) with the highest Q value, then updates its estimate of Q via a simple value iteration update, Qt + 1 (st, at) = (1 \u2212 \u03b1) Qt (st, at) Age Value + \u03b1 rt + 1 Reward + \u03b3 Factor max a Q (st + 1, a) Estimation of the optimal future value, where \u03b1 (st, at) Qt (st, at) (st, at) (1) is the learning rate and vice versa [0, 1] is the contraction factor."}, {"heading": "2.2. Policy Gradient Reinforcement Learning", "text": "In the policy gradient RL (PG), we have a (possibly infinite) set of states S'Rd and a set of actions A'Rm. The general goal is to optimize the expected return of policy \u03c0 with the parameters \u043a Rd, defined by J (\u03b8) = [s1, s2,.., sH + 1] and actions a1: H = [a1, a2,.., aH]. The probability of such a development is indicated by the states s1: H + 1 = [s1, s2,.., sH + 1] and actions a1: H = [a1, a2,.., aH]. The probability of such a development is determined by the states s1: 1, and their average return per time step is R (instead of) isR (instead of) isR (instead of) = 1 r (st, at, st + 1).Based on the Markov assumption, we can calculate the probability of such a development."}, {"heading": "3. Multi-Task Reinforcement Learning", "text": "In the following sections, we introduce the Policy Gradient Efficient Lifelong Learning Algorithm (PG-ELLA), an online MTL algorithm that extends PG to enable computationally efficient knowledge exchange between successive tasks (Bou Ammar et al., 2014). PG-ELLA is based on the Efficient Lifelong Learning Algorithm (ELLA), an online MTL algorithm that has proven effective in the monitored MTL scenario (Ruvolo & Eaton, 2013)."}, {"heading": "3.1. The Online MTL Problem", "text": "In the online MTL setting, the agent is presented with a series of tasks Z (1),.., Z (Tmax), with each task t being an MDP S (t), A (t), P (t), R (t), \u03b3 (t) with initial state distribution P (t) 0. The agent learns the tasks one after the other by training on several paths for each task before moving on to the next. We assume that the agent does not know the sequence of tasks, their distribution or the total number of tasks. The agent's goal is to find a set of optimal strategies that are suitable for all tasks."}, {"heading": "3.2. PG-ELLA", "text": "As in section 2.2, we omit d-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T"}, {"heading": "4. Methods", "text": "We implemented Q-Learning, PG and PG-ELLA each as a separate ROS package and trained the robot in simulation using Gazebo.1. For PG and PG-ELLA we also used the Policy Gradient Toolbox, 2 a MATLAB toolbox for Policy Gradient Methods and the MATLAB ROS I / O Package, 3 which allows MATLAB to exchange messages with other nodes in the ROS network. For each of the three RL packages we created four nodes - Agent, Environment, World Control and Teleop - which communicate with each other via ROS messages. For PG and PG-ELLA we also have a fifth grade - MATLAB - which receives the trajectories from Agent and calculates the robot policy according to the episodic REINFORCE Policy Gradient algorithm (Williams, 1992)."}, {"heading": "4.1. Representation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1. STATE SPACE", "text": "Let (xA, yA) and (xG, yG) be the positions of the agent or target object on a 2D map from the1gazebosim.org 2ias.tu-darmstadt.de / Research / PolicyGradientToolbox 3mathworks.com / rosenvironment. Let v-R2 be the vector whose starting and ending points are (xA, yA) and (xG, yG) respectively. Let u-R2 be a vector whose starting point is (xA, yA) and is parallel to the orientation of the agent. Then, for PG and PG-ELLA, we have used a state space specified by {(d, \u03c9), where d = v \u00b2 is the distance (in meters) from the agent to the target object, and \u03c9 is the counter-clock angle (in degrees) from u to v. (see figure)."}, {"heading": "4.1.2. ACTION SPACE", "text": "For PG and PG-ELLA, the action space is {(v ', v\u03c9): v', v\u03c9-R}, where v 'is the linear speed of the robot (in meters / second) and v\u03c9 is the angular speed of the robot (in radians / seconds). In order to prevent abnormal motor behavior, in cases where the robot learns a bizarre policy, we limit the range of v' and v\u03c9 to [\u2212 1.5, 1.5]. In other words, if the robot policy returns a value for v '(or v\u03c9) that exceeds or falls below the specified range, the robot simply uses the maximum (1.5) or minimum (-1.5) value, or the minimum value. For Q-Learning, which requires a discrete action space, we simply chose the actions as {Forward, Left, Right}."}, {"heading": "4.2. Reward function", "text": "We have defined the reward function r: S \u00b7 A \u00b7 S 7 \u2192 R in such a way that the agent is punished for taking too many steps to reach the target and is rewarded for finding the target object. Specifically, the reward function we have chosen for Q-Learning is r (st, at, st + 1) = {100 if [d] < 1 and [\u03c9] / 12 < 2 \u2212 1 otherwise, where ([d], [\u03c9] / 12) = st + 1. Similarly, the reward function we have chosen for PG and PG-ELLA is isr (st, at, st + 1) = {100 if d < 0,55 and \u03c9 < 0,2 \u2212 0,5 | v '| otherwise, where (d, \u03c9) = st + 1 and v' is the linear speed of at."}, {"heading": "4.3. Tasks", "text": "To allow for discrepancies in the tasks, we changed the soil friction coefficient \u00b5t for each task. Ideally, we would like to derive the friction coefficients for each task from a normal distribution, but due to time constraints, we instead used fixed friction coefficients for five different tasks (see Table 1)."}, {"heading": "4.4. Parameter optimization", "text": "For all three RL implementations, we used a discount factor of \u03b3 = 0.9."}, {"heading": "4.4.1. LEARNING RATES", "text": "Determining optimal learning rates for these RL techniques is very important, as they can either affect or interrupt the performance of the algorithm. After testing Q-Learning with different learning rates \u03b1 and measuring their learning performance, we found that learning rates between 0.01 and 0.2 work well for our purposes.In our analysis of Q-Learning performance in Section 5.1, we use a learning rate \u03b1 = 0.1. Since the action space {(v ', v\u03c9) for PG is two-dimensional, the episodic REINFORCE algorithm gives two learning rates \u03b1 \"and \u03b1, one each for v\" and v\u03c9. Using the letter combination U = {10 \u2212 j: j = 3, 4,..., 8} we selected learning rates for each task using the following method: We tested each pair (\u03b1', \u03b1) and selected the pair that yielded the policy with the highest average cumulative reward per episode. Table 1 lists the learning rates for each task chosen."}, {"heading": "5. Testing and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Q-learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1. USER POLICY", "text": "One disadvantage of the Q-learning algorithm is that the cost of learning becomes prohibitive the larger the state space q becomes. To try to accelerate learning, we implemented a user policy \u03c0u: S \u00b7 A 7 \u2192 [0, 1], which allows the robot to learn through demonstration. The idea is to use the user's trajectories to boot the Q-table for learning. The user's trajectories are suboptimal, so RL will induce them to create \u03c0u. We first drive the robot around manually using keyboard teleop and generate an arbitrary number of user trajectories Tu episodes, which eventually lead the robot to the target object. Then, these user trajectories are stored and used to calculate the user policy \u03c0u, where for each state action pair (s, a), \u03c0u (s, a) is the probability that the user chooses the flight sequence Tu, the action in a state."}, {"heading": "5.1.2. SIZE OF THE STATE SPACE", "text": "To illustrate this effect, we also experimented with Q-Learning with a larger three-dimensional state representation {(d), [\u03b2] / 12, [\u0443] / 12), where \u03b2 (or \u0430) is the angle in degrees from the x-axis of the 2D map of the environment to v (or u). (See fig. 3 for illustration.) We note that \u03c9 = \u03b2 \u2212. In fig. 5, we compare the performance of Q-Learning with the two-dimensional state representation {(d), [\u03c9] / 12)} and with the three-dimensional state representation {(\u03b2] / 12, [\u0432] / 12). In both diagrams, the performance of the learner improves faster and achieves significantly better results with the smaller state space. This illustrates the problem with Q-Learning in a realistic environment and shows where ML could be particularly advantageous."}, {"heading": "5.2. Policy Gradient Reinforcement Learning", "text": "To analyze PG's performance, we experimentally initialized the parameters of the robot policy \u03c0 to zero and then trained the robot for 400 episodes using \u03c0u to select its actions, then selected \u03c0 for a further 400 episodes to select the robot's actions, recording the cumulative reward and number of steps to achieve the goal for each episode. Based on these criteria, we found that \u03c0 performs at least as well, if not better, than user policy \u03c0u (see Table 2). We found that the robot learned a policy in a shorter period of time almost as well as Qlearning. PG has several advantages over Q-Learning for this task, as the functional alignment allows PG to handle continuous actions and states, and even imperfect state information."}, {"heading": "5.3. PG-ELLA", "text": "Due to lack of time and resources, we have not carried out a complete empirical analysis of our PG-ELLA implementation. Thus, for example, the ROS-I / O package could not be installed on the workstations of GRASP Lab because the MATLAB software installed on these computers was outdated and therefore all experiments for both PG and PG-ELLA had to be run on a personal laptop. So far, the limited experiments with PG-ELLA have not been as successful, mainly because many of PG-ELLA's assumptions have been violated. For example, we did not have enough tasks or enough trajectories. We will leave a more comprehensive empirical analysis of PG-ELLA to future work."}, {"heading": "6. Conclusion & Future Works", "text": "Since reinforcement learning can easily become prohibitively expensive, we worked with a simplified area in the limited time we had for this project, but now the next step is to take PG-ELLA into a more complex area and perform empirical analysis of its performance. We can also add less informative features to the state vector, such as the position of the robot (xA, yA) on the map or the robot's perception-based data (e.g. is the target visible?). We can also add obstacles to the environment, essentially turning the map into a labyrinth. In addition, we can have variable environmental dynamics: instead of a uniform coefficient of friction on the ground, we can put patches of raw carpet or sand on the floor. Finally, we can place PGELLA on a robot in the real world where imperfect and loud sensor data are customary.Of course, learning quickly becomes more expensive, as we hope with the larger government space for Q1.2, we can overcome the multiple learning tasks we have shown in Section 5.1.2."}, {"heading": "7. Acknowledgements", "text": "This work was supported by the National Science Foundation REU Site at the GRASP Laboratory at the University of Pennsylvania. I would like to pay tribute to Professor Eric Eaton, my research consultant, for his patient guidance and invaluable input during the planning and development of this research work. I would also like to express my sincere gratitude to my mentors Christopher Clingerman and Dr. Haitham Bou Ammar for their invaluable help and support throughout this research project."}], "references": [{"title": "Online multi-task learning for policy gradient methods", "author": ["H. Bou Ammar", "E. Eaton", "P. Ruvolo", "M.E. Taylor"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Multitask Learning", "author": ["R. Caruana"], "venue": "PhD thesis,", "citeRegEx": "Caruana,? \\Q1997\\E", "shortCiteRegEx": "Caruana", "year": 1997}, {"title": "Policy search for motor primitives in robotics", "author": ["J. Kober", "J. Peters"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kober and Peters,? \\Q2008\\E", "shortCiteRegEx": "Kober and Peters", "year": 2008}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Explanation-Based Neural Network Learning: A Lifelong Learning Approach", "author": ["S. Thrun"], "venue": null, "citeRegEx": "Thrun,? \\Q1996\\E", "shortCiteRegEx": "Thrun", "year": 1996}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams,? \\Q1992\\E", "shortCiteRegEx": "Williams", "year": 1992}], "referenceMentions": [{"referenceID": 1, "context": "Multi-task learning (MTL) helps to ameliorate this problem by enabling the agent to use its experience from previously learned tasks to improve learning performance in a related but different task (Caruana, 1997).", "startOffset": 197, "endOffset": 212}, {"referenceID": 4, "context": "The end result is an intelligent, flexible machine which can quickly adapt to new scenarios using learned knowledge, and which can learn new tasks more efficiently (Thrun, 1996).", "startOffset": 164, "endOffset": 177}, {"referenceID": 0, "context": "For the exact PG-ELLA algorithm and other details, please see Bou Ammar et al. (2014).", "startOffset": 66, "endOffset": 86}, {"referenceID": 5, "context": "For PG and PG-ELLA, we also have a fifth note\u2014MATLAB\u2014that receives trajectories from Agent and compute the robot\u2019s policy using the episodic REINFORCE policy gradient algorithm (Williams, 1992).", "startOffset": 177, "endOffset": 193}], "year": 2015, "abstractText": "Reinforcement learning (RL) is a general and well-known method that a robot can use to learn an optimal control policy to solve a particular task. We would like to build a versatile robot that can learn multiple tasks, but using RL for each of them would be prohibitively expensive in terms of both time and wear-and-tear on the robot. To remedy this problem, we use the Policy Gradient Efficient Lifelong Learning Algorithm (PG-ELLA), an online multi-task RL algorithm that enables the robot to efficiently learn multiple consecutive tasks by sharing knowledge between these tasks to accelerate learning and improve performance. We implemented and evaluated three RL methods\u2014Q-learning, policy gradient RL, and PG-ELLA\u2014on a ground robot whose task is to find a target object in an environment under different surface conditions. In this paper, we discuss our implementations as well as present an empirical analysis of their learning performance.", "creator": "LaTeX with hyperref package"}}}