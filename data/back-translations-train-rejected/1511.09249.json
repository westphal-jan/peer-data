{"id": "1511.09249", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2015", "title": "On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models", "abstract": "This paper addresses the general problem of reinforcement learning (RL) in partially observable environments. In 2013, our large RL recurrent neural networks (RNNs) learned from scratch to drive simulated cars from high-dimensional video input. However, real brains are more powerful in many ways. In particular, they learn a predictive model of their initially unknown environment, and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic information theory, we describe RNN-based AIs (RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending sequences of tasks, some of them provided by the user, others invented by the RNNAI itself in a curious, playful fashion, to improve its RNN-based world model. Unlike our previous model-building RNN-based RL machines dating back to 1990, the RNNAI learns to actively query its model for abstract reasoning and planning and decision making, essentially \"learning to think.\" The basic ideas of this report can be applied to many other cases where one RNN-like system exploits the algorithmic information content of another. They are taken from a grant proposal submitted in Fall 2014, and also explain concepts such as \"mirror neurons.\" Experimental results will be described in separate papers.", "histories": [["v1", "Mon, 30 Nov 2015 11:35:26 GMT  (54kb,D)", "http://arxiv.org/abs/1511.09249v1", "36 pages, 1 figure. arXiv admin note: substantial text overlap witharXiv:1404.7828"]], "COMMENTS": "36 pages, 1 figure. arXiv admin note: substantial text overlap witharXiv:1404.7828", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["juergen schmidhuber"], "accepted": false, "id": "1511.09249"}, "pdf": {"name": "1511.09249.pdf", "metadata": {"source": "CRF", "title": "On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models", "authors": ["J\u00fcrgen Schmidhuber"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 151 1.09 249v 1 [csTable of Contents]"}, {"heading": "1 Introduction to Reinforcement Learning (RL) with Recurrent Neural Networks (RNNs) in Partially Observable Environments 2", "text": "1.1 RL by direct and indirect search in the RNN Program Space......... 3 1.2 Deep Learning in NNs: Supervised & Unsupervised Learning (SL & UL)...... 3 1.3 Gradient Descent-Based NNNs for RL........................... 41.3.1 Early RNN Controllers with Predictive RNN World Models......... 5 1.3.2 Early Predictive RNN World Models Combined with Traditional RL..... 51.4 Hierarchical & Multitask RL and Algorithmic Transfer Learning......... 6"}, {"heading": "2 Algorithmic Information Theory (AIT) for RNN-based AIs 6", "text": "2.1 Basic AIT argument....................................... 7 2.2 An RNN-like system actively learns to exploit algorithmic information from another 7 2.3 Consequences of the AIT argument for model-maker controllers........ 8"}, {"heading": "3 The RNNAI and its Holy Data 8", "text": "3.1 Expansion of standard activation in typical RNNs................ 9 3.2 Alternately training phases for Controller C and World Model M............ 9"}, {"heading": "4 The Gradient-Based World Model M 10", "text": "4.1 M's Compression Performance on the History so far......................................................................... 11 4.3 M's Training.................................."}, {"heading": "5 The Controller C Learning to Exploit RNN World Model M 11", "text": "5.1 C as a standard RL machine whose states are M's activations........ 12 5.2 C as an evolutionary RL (R) NN whose inputs are M's activations........ 12 5.3 C learns to think with M: High-level plans and abstractions........... 12 5.4 Incremental / hierarchical / multitask learning of C with M............ 14"}, {"heading": "6 Exploration: Rewarding C for Experiments that Improve M 14", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Conclusion 15", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction to Reinforcement Learning (RL) with Recurrent", "text": "Neural networks (RNs) in partially observable environments 1General Reinforcement Learning (RL) agents must discover, without the help of a teacher, how to interact with a dynamic, initially unknown, partially observable environment in order to maximize their expected cumulative reward signals, e.g., [123, 272, 310]. There may be arbitrary, a priori unknown delays between actions and perceptible consequences. RL problem is as severe as any problem in computer science, since any task can be formulated with a computerized description within the RL, for example, [109] to become a general problem solver capable of executing arbitrary problem-solving programs, controlling a robot or an artificial agent computer must be a general purpose."}, {"heading": "1.1 RL through Direct and Indirect Search in RNN Program Space", "text": "It is possible to train small RNNs with a few 100 or 1000 weights using the evolutionary algorithms [200, 255, 56, 68] to search the space of NN weights [165, 307, 44, 321, 259, 164, 173, 69, 71, 181, 313, 270, 305], or by political gradients (PGs) [314, 315, 274, 18, 63, 313, 210, 191, 256, 85, 190, 82, 93]. For example, our evolutionary algorithms become traditional dynamic programming [20] -based RL methods (272, Sec."}, {"heading": "1.2 Deep Learning in NNs: Supervised & Unsupervised Learning (SL & UL)", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "1.3 Gradient Descent-Based NNs for RL", "text": "Perhaps the most well-known RL application is Tesauro's 1994 backgammon player [280], who learned to reach the level of human world champions by playing against himself. A reactive (memory-free) policy is followed, based on the simplistic assumption of Markov decision-making processes: the current input of the RL agent conveys all the information necessary to calculate an optimal next event or optimal decision. The policy is implemented as a gradient-based RN trained by the method of time differences [245, Sec. 6.2]. During the game, the FNN learns to make predictions of expected cumulative rewards and selects actions that lead to states with maximum predicted rewards. A very similar approach (even on methods over 20 years old) employed a CNN (see Sec. 1.2) to play several Atari video games directly from 84 x 84 pixels of video games."}, {"heading": "1.3.1 Early RNN Controllers with Predictive RNN World Models", "text": "An important application of gradient-based UL is to obtain a predictable world model, M, which a controller, C, can use to achieve its goals more efficiently, e.g. through cheap, \"mental\" M-based studies, as opposed to expensive studies in the real world [301, 273]. The first combination of an RL RNN C and a UL RNN M was ours and dates back to 1990 [223, 222, 226, 227], with previous similar controller / model systems (CM systems) generalized on the basis of FNs [298, 179]; compare related works [177, 119, 301, 300, 209, 120, 178, 302, 73, 45, 144, 166, 153, 196, 60] [245, Sec. 6.1]. M tries to learn to predict C inputs (including reward signals) from previous inputs and actions."}, {"heading": "1.3.2 Early Predictive RNN World Models Combined with Traditional RL", "text": "In the early 1990s, an RNNM, as in paragraph 1.3.1, was also combined with traditional methods of time difference [227, 150] [122, 272] [245, paragraph 6.2], based on the Markov assumption (paragraph 1.3). While M processes the history of actions and observations to predict future inputs and rewards, the internal states of M are used as inputs for a time differential predictor of cumulative predicted reward, to be maximized by appropriate action sequences. One of our systems [227] described in 1991 actually collapsed the cumulative reward predictor into the predictive world model M."}, {"heading": "1.4 Hierarchical & Multitask RL and Algorithmic Transfer Learning", "text": "Work on NN-based hierarchical RL (HRL) without predictable world models has been published since the early 1990s. Specifically, gradient-based RNNs break down RL tasks into subtasks for submodules [225]. Numerous alternative HRL techniques have been proposed [204, 206, 117, 279, 295, 171, 195, 50, 162, 51, 15, 11, 260]. While HRL frameworks such as Feudal RL [48] and Options [275, 16, 261] do not directly address the problem of automatic subtarget finding, HQ-Learning automatically breaks down problems in partially observable environments into sequences of simpler subtasks that can be solved through memoryless strategies that can be learned through reactive substances."}, {"heading": "2 Algorithmic Information Theory (AIT) for RNN-based AIs", "text": "Our early RNN-based CM systems (1990), mentioned in Sec. 1.3.1, learn a predictable model of their initially unknown environment. Real brains seem to do so, but in many ways they are still superior to present-day artificial systems. They seem to use the model more intelligently, e.g. to plan action sequences in a hierarchical manner, or by other kinds of abstract reasoning, continually building on earlier acquired skills and increasingly becoming general problem solvers capable of handling a large number of different and complex tasks. Here, we describe RNN-based artificial intelligence (RNNAIs), which is designed to do the same by learning to think. 2While RNNs have traditionally been linked to concepts of statistical mechanics and information theory [24, 257, 136] the programs of general computers are like RNNNNs for the framework of algorithmic information theory (AIT)."}, {"heading": "2.1 Basic AIT Argument", "text": "According to AIT, in the face of a universal computer U, whose programs are encoded as bit strings, the mutual information between two programs p and q is expressed as K (q | p), the length of the shortest program w \u0442, which calculates q, given p, ignoring an additive constant of O (1) in relation to U (in practical applications, the calculation will be time-limited [147]). That is, if p is a solution to problem P and q is a quick (say linear time) solution to problem Q, and if K (q | p) is small and w \u0442 is both fast and much shorter than q, then the asymptotically optimal universal search [146, 238] for a solution to Q, given p, will generally find w first (to calculate q and solve Q) and thus solve Q much faster than the search for q from scratch [238]."}, {"heading": "2.2 One RNN-Like System Actively Learns to Exploit Algorithmic Information of Another", "text": "The AIT argument 2.1 above has broad applicability. Let's try both RNNs or similar general parallel sequential computer experiments (229, 47, 175, 232, 231, 103, 80, 303), even if the robot can learn another well-defined task (SL or UL or RL algorithm to perform a certain well-defined task in a particular environment).The goal now is to explore the C parameters wC through some learning algorithms to perform another well-defined task, the solution of which can share mutual algorithmic information with the solution of M's task. To facilitate this, let's just allow C to actively inspect and reuse (essentially arbitrary manner) the algorithmic information transmitted by M and wM. Let's consider a study in which C is trying to solve its given task within a series of discrete time steps."}, {"heading": "2.3 Consequences of the AIT Argument for Model-Building Controllers", "text": "The simple above finding of the AIT suggests that in many partially observable environments it should be possible to considerably accelerate the program search of an RL RNN, C by letting it learn to access, query and exploit the program of a typically much larger, gradient-based UL RNN, M, in any predictable manner, which is used to model and compress the entire growing interaction history of all failed and successful attempts of the RL agent. Note that the w-value of Sec. 2.1 can implement all kinds of well-known, predictable types of reasoning, e.g. by hierarchical reuse of subroutines of p [238], by analogy, etc. That is, we may even expect C to learn to exploit M for human-like abstract thoughts. Such novel CM systems will be a central theme of Sec. 5. Sec. 6 will also discuss the improvement of M by exploring efficiently."}, {"heading": "3 The RNNAI and its Holy Data", "text": "The i-th component of each really evaluated vector, v, is denoted by vi. Let the lifespan of the RNNAI be a discrete sequence of time steps, t = 1, 2,.., tdeath.At the beginning of a given time step, there is a \"normal\" sensory input vector, in (t).Rm, and a reward input vector, r (t).Rn. For example, parts of in (t) can represent the pixel intensities of an incoming video image, while components of r (t) reflect all the positive rewards or negative values produced by pain sensors, not even the (t) excessive temperature or pressure. Let Sinn (t).Rm + n concatenate the vectors in (t) and r (t) all the time, or negative values produced by pain sensors to measure the excessive temperature or pressure. Let Sinn (+) the vectors in (t) and r)."}, {"heading": "3.1 Standard Activation Spreading in Typical RNNs", "text": "In fact, it is in such a way that it is a way in which people are able to live, to live, to live, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to live, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to live, to live, to live, to live, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work,"}, {"heading": "3.2 Alternating Training Phases for Controller C and World Model M", "text": "Several novel implementations of C are described in paragraph 5. All of them use a variable size RNN called World Model M, which learns to encode the growing history compactly, for example by predictive encoding, which attempts to predict each input component (the expected value), given the history of actions and observations. M's goal is to discover algorithmic regularities in the data by learning a program that compresses the data better in a lossless manner. 3. Perform a new study by generating a finite action sequence that extends the history of actions and observations. 1. Initialize C and M and their weights. 2. Freeze M's weights so that they cannot change while C. 3. Perform a new study by generating a finite action sequence that extends the history of actions and observations."}, {"heading": "4 The Gradient-Based World Model M", "text": "A central goal of unsupervised learning is the compression of observed data [14, 228]. M's goal is to compress the entire growing interaction history of the RL agent of all failed and successful attempts [239, 241], e.g. by predictive coding [228, 248]. M has m + n + o input units to obtain all (t) t < tdeath in due course, and m + n output units to generate a prediction pred (t + 1) and Rm + n of sense (t + 1) [223, 226, 222, 227]."}, {"heading": "4.1 M \u2019s Compression Performance on the History so far", "text": "Let's examine the details of trainingM in a \"sleep phase\" of step 4 in algorithm 1 (1). (The training of C is in Sec. 5.) Let's consider some M with given (typically suboptimal) weights and a standard initialization of all unit activations. An example of how M compresses the story (but not the only one) is the following: Given H (t) we can M by repeating [149] H (t) in semi-offline training, feeding sequentially all (1), all (2),. all (t) in M's input units in standard RNN mode (Sec. 1.2, 3.1). Given H (pts) < t), M pred (\u03c4 + 1) calculates a prediction of sense (\u03c4 + 1). A standard error function to be minimized by gradients descending into M's. (Sec. 1.2) would be (E)."}, {"heading": "4.2 M \u2019s Training", "text": "To reduce BitsM + BitsH, we add a regulatory term to E to punish excessive complexity [4, 5, 91, 294, 155, 135, 97, 170, 169, 104, 290, 7, 290, 87, 286, 319, 100, 102]. Step 1 of Algorithm 1 begins with a small M. As the story grows to find an M with small BitsM + BitsH, Step 4 uses sequential networking: it regularly changes M's size by adding or cutting units and connections [111, 112, 8, 168, 59, 107, 304, 176, 141, 92, 143, 204, 53, 296, 106, 31, 57, 185, 283]. Whenever this helps (after additional training with BPTT from M - see Sec. 1,2) to improve the BitsM + BitsH in history, the changes are retained if not kept (that animals are not once trained in the brain and not growing phase)."}, {"heading": "4.3 M may have a Built-In FNN Preprocessor", "text": "To facilitate M's task in certain environments, each image of the sensory input stream (video, etc.) can first be compressed separately by autoencoders [211] or autoencoder hierarchies [13, 21] based on CNNs or other FNNs (see paragraph 1.2) [42], which are used as sensory preprocessors to generate less redundant sensory codes [118, 138, 142, 46]. The compressed codes are then fed into an RNN trained to predict not the raw inputs but their compressed codes. These predictions must be decompressed again by the FNN to evaluate the overall compression performance, bitsM + bitsH, the combination FNN-RNN that M represents."}, {"heading": "5 The Controller C Learning to Exploit RNN World Model M", "text": "Here we describe possibilities of using the world model M of paragraph 4 to facilitate the task of the Directive Controller C. In particular, the systems of paragraph 5.3 overcome disadvantages of the early CM systems mentioned in paragraph 1.3.1, 1.3.2. Some of the arrangements of the current paragraph 5 can be regarded as special cases of the general system of paragraph 2.2."}, {"heading": "5.1 C as a Standard RL Machine whose States are M \u2019s Activations", "text": "In fact, we use the history of actions and observations to predict future inputs as inputs for a predictor of cumulative future rewards. More specifically, in step 3 of Algorithm 1, we consider a study that ranges from time to death. M is used as a predictor of future rewards. M is used as a predictor of future rewards. M is used as input for future rewards. M is used as input for future rewards. M is used as input for future rewards."}, {"heading": "5.2 C as an Evolutionary RL (R)NN whose Inputs are M \u2019s Activations", "text": "This approach is essentially the same as that of paragraph 5.1, except that C is now an FNN or RNN trained by evolutionary algorithms [200, 255, 105, 56, 68] on NNs [165, 321, 180, 259, 72, 90, 89, 110, 94] or by political gradient methods [314, 315, 316, 274, 18, 1, 63, 128, 313, 210, 192, 191, 256, 85, 312, 190, 82, 93] [245, paragraph 6.6] or by compressed NN search [245, paragraph 6.6]; see paragraph 1. C has input units and output units of 2 + 2n + h. At a certain time, the state (t) is fed into C, which (t) is calculated; then M (t + 1) and Pred (t + 1) are executed to get a sense (t + 1)."}, {"heading": "5.3 C Learns to Think with M : High-Level Plans and Abstractions", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "5.4 Incremental / Hierarchical / Multitask Learning of C with M", "text": "A variant of the approach in Section 5.3 builds C gradually on a never-ending series of tasks and continuously builds on solutions to previous problems rather than learning each new problem from scratch, principally through incremental NN evolution [70], hierarchical NN evolution [306, 285], hierarchical policy gradient algorithms [63], or asymptotically optimal ways of algorithmic transfer learning [238]. Faced with a new task and an aC trained on several previous tasks, such hierarchical / incremental methods can freeze the current weights of C and then increase C by adding new units and compounds trained on the new task. This process reduces the size of the search space for the new task, giving the new weights the ability to use the frozen parts of C as subprograms. Incremental variants of the compressed RNN search [132] (Section 1) do not directly result in C's already large weight space, but in small weight x, resulting from a matter directly in C's potentially large weight space."}, {"heading": "6 Exploration: Rewarding C for Experiments that Improve M", "text": "This year, it is only a matter of time before a solution can be found in which an agreement can be reached."}, {"heading": "7 Conclusion", "text": "The most general CM systems implement principles of algorithmics [263, 130, 147] as opposed to traditional [24, 257] information theory. Both M and C are RNNs or RNN-like systems. M is actively used in any predictable manner by C, whose program search space is typically much smaller and who can learn to selectively examine and reuse M's internal programs in order to plan and ration. The basic principles are not limited to RL, but apply to all types of active algorithmic transfer learning from one RNN to another. By combining gradient-based RNNNs and RL-RNNNs, we create a qualitatively new type of self-improving, universally valid control architectures. This RNNAI can continuously build on previously acquired problem solving techniques, some of which can be self-invented in a way similar to an unknown scientist, but quickly looking for solutions."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>This paper addresses the general problem of reinforcement learning (RL) in partially observable<lb>environments. In 2013, our large RL recurrent neural networks (RNNs) learned from scratch to<lb>drive simulated cars from high-dimensional video input. However, real brains are more powerful<lb>in many ways. In particular, they learn a predictive model of their initially unknown environment,<lb>and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic<lb>information theory, we describe RNN-based AIs (RNNAIs) designed to do the same. Such an<lb>RNNAI can be trained on never-ending sequences of tasks, some of them provided by the user,<lb>others invented by the RNNAI itself in a curious, playful fashion, to improve its RNN-based world<lb>model. Unlike our previous model-building RNN-based RL machines dating back to 1990, the<lb>RNNAI learns to actively query its model for abstract reasoning and planning and decision making,<lb>essentially \u201clearning to think.\u201d The basic ideas of this report can be applied to many other cases<lb>where one RNN-like system exploits the algorithmic information content of another. They are<lb>taken from a grant proposal submitted in Fall 2014, and also explain concepts such as \u201cmirror<lb>neurons.\u201d Experimental results will be described in separate papers. 1<lb>ar<lb>X<lb>iv<lb>:1<lb>51<lb>1.<lb>09<lb>24<lb>9v<lb>1<lb>[<lb>cs<lb>.A<lb>I]<lb>3<lb>0<lb>N<lb>ov<lb>2<lb>01<lb>5", "creator": "LaTeX with hyperref package"}}}