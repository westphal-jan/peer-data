{"id": "1511.09263", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2015", "title": "Scalable and Accurate Online Feature Selection for Big Data", "abstract": "Feature selection is important in many big data applications. There are at least two critical challenges. Firstly, in many applications, the dimensionality is extremely high, in millions, and keeps growing. Secondly, feature selection has to be highly scalable, preferably in an online manner such that each feature can be processed in a sequential scan. In this paper, we develop SAOLA, a Scalable and Accurate OnLine Approach for feature selection. With a theoretical analysis on bounds of the pairwise correlations between features, SAOLA employs novel online pairwise comparison techniques to address the two challenges and maintain a parsimonious model over time in an online manner. Furthermore, to tackle the dimensionality that arrives by groups, we extend our SAOLA algorithm, and then propose a novel group-SAOLA algorithm for online group feature selection. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse at the level of both groups and individual features simultaneously. An empirical study using a series of benchmark real data sets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on data sets of extremely high dimensionality, and have superior performance over the state-of-the-art feature selection methods.", "histories": [["v1", "Mon, 30 Nov 2015 12:11:43 GMT  (401kb)", "https://arxiv.org/abs/1511.09263v1", null], ["v2", "Thu, 7 Jan 2016 01:04:16 GMT  (451kb)", "http://arxiv.org/abs/1511.09263v2", null], ["v3", "Mon, 25 Jul 2016 03:11:09 GMT  (451kb)", "http://arxiv.org/abs/1511.09263v3", null], ["v4", "Thu, 28 Jul 2016 01:49:01 GMT  (451kb)", "http://arxiv.org/abs/1511.09263v4", "This paper has been accepted by the journal of ACM Transactions on Knowledge Discovery from Data (TKDD) and will be available soon"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kui yu", "xindong wu", "wei ding", "jian pei"], "accepted": false, "id": "1511.09263"}, "pdf": {"name": "1511.09263.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["KUI YU", "XINDONG WU", "WEI DING", "JIAN PEI"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 151 1.09 263v 4 [cs.L G] 28 Jul 2 016AScalable and Accurate Online Feature Selection for Big DataKUI YU, Simon Fraser University XINDONG WU, Hefei University of Technology and University of Vermont WEI DING, University of Massachusetts Boston JIAN PEI, Simon Fraser UniversityFeature selection is important in many big data applications. Two critical challenges are closely related to big data. First, the dimensionality in many big data applications is extremely high, in millions, and continues to grow. Second, big data applications require highly scalable feature selection algorithms in an online way that any feature can be processed in a sequential scan. We present SAOLA, a Scalable and Accurate OnLine Approach for feature selection in this paper."}, {"heading": "1. INTRODUCTION", "text": "In data mining and machine learning, the task of feature mining is to select a subset of relevant features and remove critical features from high-dimensional data to obtain a frugal model [Guyon and Elisseeff 2003; Liu and Yu 2005; Wang et al. 2015; Zhang et al. 2015]. In the age of big data, many emerging applications, such as social media services, high-resolution images, genomic data analysis and document analysis, consumption data are of extremely high dimensionality, in the order of millions or more [Wu et al. 2014; Zhai et al. 2014; Chen et al. 2014; Yu et al. 2015a; Yu et al.]. For example, Web Spam Corpus 2011 [Wang et al. 2012] collects approximately 16 million features (attributes) for website recognition and KDD CUP 2010 data sets on the use of educational data to accurately predict student performance."}, {"heading": "2. RELATED WORK", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves by going in search of their own identity. In fact, it is so that they are able to survive themselves, and that they are able to survive themselves. In fact, it is so that they are able to survive themselves, that they are able to survive themselves. In fact, it is so that they are able to survive themselves, that they are able to survive themselves. In fact, it is so that they are able to survive themselves, that they are able to survive themselves."}, {"heading": "3. THE SAOLA ALGORITHM FOR ONLINE FEATURE SELECTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Problem Definition", "text": "The question whether the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features of the features."}, {"heading": "3.2. Using Mutual Information to Solve Eq.(1)", "text": "To solve Eq. (1), we will apply mutual information to calculate correlations between features. (2) The entropy of feature Y is defined as sequence (Y) = sequence (Y) = sequence (Y). (5) The entropy of feature Y is defined as sequence (Y) = sequence (Y) = sequence (Y). (2) The entropy of feature Y is defined as sequence (Y) = sequence (Y). (2) The entropy of feature Y is defined as sequence (Y). (2) The entropy of feature Y is defined as sequence (Y). (2) The entropy of feature Y is defined as sequence (Y). (2) The entropy of feature Y is defined as sequence (Y)."}, {"heading": "3.3. The Solutions to Equations (2) to (4)", "text": "However, it is mathematically expensive to use definitions 3.2 and 3.3 when the number of features within S-Fi \u2212 1 is large. In assessing whether Fi is redundant in relation to S-Fi, it is necessary to determine all subsets of S-Fi \u2212 1 (the total number of subsets is in relation to S-Fi \u2212 1 |), which subset comprises the forward-looking information that Fi has about C, i.e., that Markov blanket of Fi has. If such a subset is found, Fi becomes redundant and is removed. If we handle a larger number of features, it is computationally forbidden to check all subsets of S-Fi \u2212 1."}, {"heading": "3.4. The SAOLA Algorithm and An Analysis", "text": "Using Eq. (23) and Eq. (25), we will propose the SAOLA algorithms in detail (as shown in Algorithm 1).The SAOLA algorithms will be implemented as follows: In due course, the SAOLA function will be presented as follows: in due course, the SAOLA function will be discarded in step 5, then the SALA function will be discarded as an irrelevant feature, and the SAOLA function will wait for a next feature to come; in due course, SAOLA will be judged whether Fi should be retained in light of the current features."}, {"heading": "4. A GROUP-SAOLA ALGORITHM FOR ONLINE GROUP FEATURE SELECTION", "text": "The SAOLA algorithm selects features only at the individual attribute level. If the data has a particular group structure, the SAOLA algorithm cannot deal directly with features with group structures. In this section, we expand our SAOLA algorithm and propose a novel group SAOLA algorithm to select feature groups that are sparse online at both the attribute and group level."}, {"heading": "4.1. Problem Definition", "text": "Let's say, G = G1, Gi, G2, Gi, Gi, Gi, GnumG, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi, Gi"}, {"heading": "4.2. The Group-SAOLA Algorithm", "text": "With the above analysis, we propose the Group-SAOLA algorithm in algorithm 2: > FI group 2, from step 5 to 8, if Gi is an irrelevant group, it is discarded. If not, then in step 11 and step 16 the redundant characteristics are removed from Gi. Our Group-SAOLA algorithm canALGORITHM 2: The Group-SAOLA algorithm (Group-SAOLA algorithm) (Group-SALA algorithm) (Group-SALA algorithm) (Group-SAOLA algorithm.1: Input: Gi: Characteristic group; C: the class attribute; C: a relevance threshold (0 \u2264 < 1); FI: the Group-SAOLA group (Group-SAOLA group): Gi / I: the Group-SAOLA (Group-SAOLA); Group-SALA: (Group OLA); Group OLA: (Group OLA); Group (Group OLA); SI: Group (Group OLA);"}, {"heading": "5. EXPERIMENT RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Experiment Setup", "text": "The first ten high-dimensional datasets include two biomedical datasets (hiva and breast cancer), two massive high-dimensional text categorization datasets (ohsumed and apcj-etiology), and the thrombin datasets selected from the KDD Cup 2001. The last four extremely high-dimensional datasets are available at the Libsvm dataset website2. In the first ten high-dimensional datasets, we use the original training and validation datasets provided for the three NIPS 2003 challenge datasets and the hiva datasets we select for the remaining six positions."}, {"heading": "5.2. Comparison of SAOLA with Three Online Algorithms", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "5.3. Comparison with the Three Batch Methods", "text": "In fact, the fact is that most of us are able to go in search of a solution that they have got a grip on. (...) Most of us have gone in search of a solution. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions. (...) The others in search of solutions."}, {"heading": "5.4. Comparison with Markov Blanket Discovery Algorithms", "text": "In this section, we compare SAOLA and FCBF (discovery of approximateMarkov blankets) with two state-of-the-art exact Markov design algorithms, IAMB (Incremental Association Markov Blanket) and MMMB (Max-Min Markov Blanket). The IAMB algorithm finds Markov blankets that target the selected features, while the MMMB algorithm Markov blankets that applies to all possible feature subsets of the selected feature sets. Under certain assumptions (sufficient number of data instances and reliable statistical tests), IAMB and MMMB Decks designs of Markov designs apply to all possible feature subsets of the selected feature sets."}, {"heading": "5.5. Analysis of the Effect of Parameters on SAOLA", "text": "In this section, we examine SAOLA-max with the opposite threshold, i.e., max (I (Fi; C), I (Y; C)), which we call the SAOLA-max algorithm. In the experiments, we use the same parameters as SAOLA. Table XVIII shows the accuracy of the prediction of SAOLA and SAOLA-max, which we can see with the summary of w / t / l values in Table XVIII, we can see that SAOLA-max is very competitive in predictive accuracy. With the Friedman test at 95% significance, as for SVM, SAOLA-max. For NN, the zero prediction of LA-max is very accurate in predictive precision."}, {"heading": "5.6. Comparison of Group-SAOLA with OGFS and Sparse Group Lasso", "text": "This year, the time has come for only one person to be able to decide whether he will be able to assert himself or whether he will be able to assert himself."}, {"heading": "6. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we introduced the SAOLA algorithm, a scalable and precise online approach to address the selection of extremely high-dimensionality traits. We performed a theoretical analysis and derived a lower correlation between traits for pair comparisons, and then proposed a series of online pair comparisons to maintain a frugal model over time. To treat group structure information in traits, we expanded the SAOLA algorithm and then proposed a novel group SAOLA algorithm to deal with traits arriving through groups. The group SAOLA algorithm can maintain online a set of traits that are sparse between groups and within each group at the same time. Using a set of benchmark data sets, we compared the SAOLA and group SAOLA algorithms with state-of-the-art online selection methods and established batch algorithms."}, {"heading": "Acknowledgments", "text": "The preliminary version of this manuscript, entitled \"Towards Scalable and Accurate Online Feature Selection for Big Data,\" was published in the Proceedings of the 14th IEEE International Conference on Data Mining (ICDM2014), 660-669, supported in part by a PIMS Post-Doctoral Fellowship Award from the Pacific Institute for the Mathematical Sciences, Canada."}], "references": [{"title": "Local causal and markov blanket induction for causal discovery and feature selection for classification part I: Algorithms and empirical evaluation", "author": ["Constantin F Aliferis", "Alexander Statnikov", "Ioannis Tsamardinos", "Subramani Mani", "Xenofon D Koutsoukos."], "venue": "Journal of Machine Learning Research 11 (2010), 171\u2013234.", "citeRegEx": "Aliferis et al\\.,? 2010", "shortCiteRegEx": "Aliferis et al\\.", "year": 2010}, {"title": "Conditional likelihood maximisation: A unifying framework for information theoretic feature selection", "author": ["Gavin Brown", "Adam Pocock", "Ming-Jie Zhao", "Mikel Luj\u00e1n."], "venue": "Journal of Machine Learning Research 13 (2012), 27\u201366.", "citeRegEx": "Brown et al\\.,? 2012", "shortCiteRegEx": "Brown et al\\.", "year": 2012}, {"title": "A hybrid memory built by SSD and DRAM to support in-memory Big Data analytics", "author": ["Zhiguang Chen", "Yutong Lu", "Nong Xiao", "Fang Liu."], "venue": "Knowledge and Information Systems 41, 2 (2014), 335\u2013354.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Consistency-based search in feature selection", "author": ["Manoranjan Dash", "Huan Liu."], "venue": "Artificial intelligence 151, 1 (2003), 155\u2013176.", "citeRegEx": "Dash and Liu.,? 2003", "shortCiteRegEx": "Dash and Liu.", "year": 2003}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["Janez Dem\u0161ar."], "venue": "The Journal of Machine Learning Research 7 (2006), 1\u201330.", "citeRegEx": "Dem\u0161ar.,? 2006", "shortCiteRegEx": "Dem\u0161ar.", "year": 2006}, {"title": "An extensive empirical study of feature selection metrics for text classification", "author": ["George Forman."], "venue": "The Journal of machine learning research 3 (2003), 1289\u20131305.", "citeRegEx": "Forman.,? 2003", "shortCiteRegEx": "Forman.", "year": 2003}, {"title": "A note on the group lasso and a sparse group lasso", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani."], "venue": "arXiv preprint arXiv:1001.0736 (2010).", "citeRegEx": "Friedman et al\\.,? 2010", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff."], "venue": "Journal of Machine Learning Research 3 (2003), 1157\u20131182.", "citeRegEx": "Guyon and Elisseeff.,? 2003", "shortCiteRegEx": "Guyon and Elisseeff.", "year": 2003}, {"title": "Online feature selection for mining big data", "author": ["Steven CHHoi", "Jialei Wang", "Peilin Zhao", "Rong Jin."], "venue": "Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications. ACM, 93\u2013100.", "citeRegEx": "CHHoi et al\\.,? 2012", "shortCiteRegEx": "CHHoi et al\\.", "year": 2012}, {"title": "Analyzing attribute dependencies", "author": ["Aleks Jakulin", "Ivan Bratko."], "venue": "PKDD 2003. Springer-Verlag, 229\u2013240.", "citeRegEx": "Jakulin and Bratko.,? 2003", "shortCiteRegEx": "Jakulin and Bratko.", "year": 2003}, {"title": "The correctness problem: evaluating the ordering of binary features in rankings", "author": ["Kashif Javed", "Mehreen Saeed", "Haroon A Babri."], "venue": "Knowledge and information systems 39, 3 (2014), 543\u2013563.", "citeRegEx": "Javed et al\\.,? 2014", "shortCiteRegEx": "Javed et al\\.", "year": 2014}, {"title": "Stability of feature selection algorithms: a study on high-dimensional spaces", "author": ["Alexandros Kalousis", "Julien Prados", "Melanie Hilario."], "venue": "Knowledge and information systems 12, 1 (2007), 95\u2013116.", "citeRegEx": "Kalousis et al\\.,? 2007", "shortCiteRegEx": "Kalousis et al\\.", "year": 2007}, {"title": "Wrappers for feature subset selection", "author": ["Ron Kohavi", "George H John."], "venue": "Artificial intelligence 97, 1 (1997), 273\u2013324.", "citeRegEx": "Kohavi and John.,? 1997", "shortCiteRegEx": "Kohavi and John.", "year": 1997}, {"title": "Toward optimal feature selection", "author": ["Daphne Koller", "Mehran Sahami."], "venue": "ICML-1995. 284\u2013292.", "citeRegEx": "Koller and Sahami.,? 1995", "shortCiteRegEx": "Koller and Sahami.", "year": 1995}, {"title": "On information and sufficiency", "author": ["Solomon Kullback", "Richard A Leibler."], "venue": "The Annals of Mathematical Statistics (1951), 79\u201386.", "citeRegEx": "Kullback and Leibler.,? 1951", "shortCiteRegEx": "Kullback and Leibler.", "year": 1951}, {"title": "An efficient orientation distance\u2013 based discriminative feature extraction method for multi-classification.Knowledge and information systems", "author": ["Bo Liu", "Yanshan Xiao", "S Yu Philip", "Zhifeng Hao", "Longbing Cao"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Toward integrating feature selection algorithms for classification and clustering", "author": ["Huan Liu", "Lei Yu."], "venue": "IEEE Transactions on Knowledge and Data Engineering 17, 4 (2005), 491\u2013502.", "citeRegEx": "Liu and Yu.,? 2005", "shortCiteRegEx": "Liu and Yu.", "year": 2005}, {"title": "Learning gaussian graphical models of gene networks with false discovery rate control", "author": ["Jose M Pe\u00f1a."], "venue": "EvoBIO-2008. 165\u2013176.", "citeRegEx": "Pe\u00f1a.,? 2008", "shortCiteRegEx": "Pe\u00f1a.", "year": 2008}, {"title": "Towards scalable and data efficient learning of Markov boundaries", "author": ["Jose M Pe\u00f1a", "Roland Nilsson", "Johan Bj\u00f6rkegren", "Jesper Tegn\u00e9r."], "venue": "International Journal of Approximate Reasoning 45, 2 (2007), 211\u2013232.", "citeRegEx": "Pe\u00f1a et al\\.,? 2007", "shortCiteRegEx": "Pe\u00f1a et al\\.", "year": 2007}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 27, 8 (2005), 1226\u20131238.", "citeRegEx": "Peng et al\\.,? 2005", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Online feature selection using grafting", "author": ["Simon Perkins", "James Theiler."], "venue": "ICML. 592\u2013599.", "citeRegEx": "Perkins and Theiler.,? 2003", "shortCiteRegEx": "Perkins and Theiler.", "year": 2003}, {"title": "Numerical recipes in C", "author": ["William H Press", "Saul A Teukolsky", "William T Vetterling", "Brian P Flannery."], "venue": "Vol. 2. Citeseer.", "citeRegEx": "Press et al\\.,? 1996", "shortCiteRegEx": "Press et al\\.", "year": 1996}, {"title": "Notes on Kullback-Leibler Divergence and Likelihood", "author": ["Jonathon Shlens."], "venue": "arXiv preprint arXiv:1404.2000 (2014).", "citeRegEx": "Shlens.,? 2014", "shortCiteRegEx": "Shlens.", "year": 2014}, {"title": "Feature selection via dependence maximization", "author": ["Le Song", "Alex Smola", "Arthur Gretton", "Justin Bedo", "Karsten Borgwardt."], "venue": "Journal of Machine Learning Research 13 (2012), 1393\u20131434.", "citeRegEx": "Song et al\\.,? 2012", "shortCiteRegEx": "Song et al\\.", "year": 2012}, {"title": "Towards Ultrahigh Dimensional Feature Selection for Big Data", "author": ["Mingkui Tan", "Ivor W Tsang", "Li Wang."], "venue": "Journal of Machine Learning Research 15 (2014), 1371\u20131429.", "citeRegEx": "Tan et al\\.,? 2014", "shortCiteRegEx": "Tan et al\\.", "year": 2014}, {"title": "Learning sparse svm for feature selection on very high dimensional datasets", "author": ["Mingkui Tan", "Li Wang", "Ivor W Tsang."], "venue": "ICML-2010. 1047\u20131054.", "citeRegEx": "Tan et al\\.,? 2010", "shortCiteRegEx": "Tan et al\\.", "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani."], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) (1996), 267\u2013288.", "citeRegEx": "Tibshirani.,? 1996", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Towards principled feature selection: Relevancy, filters and wrappers", "author": ["Ioannis Tsamardinos", "Constantin F Aliferis."], "venue": "Proceedings of the ninth international workshop on Artificial Intelligence and Statistics. Morgan Kaufmann Publishers: Key West, FL, USA.", "citeRegEx": "Tsamardinos and Aliferis.,? 2003", "shortCiteRegEx": "Tsamardinos and Aliferis.", "year": 2003}, {"title": "The max-min hill-climbing Bayesian network structure learning algorithm", "author": ["Ioannis Tsamardinos", "Laura E Brown", "Constantin F Aliferis."], "venue": "Machine learning 65, 1 (2006), 31\u201378.", "citeRegEx": "Tsamardinos et al\\.,? 2006", "shortCiteRegEx": "Tsamardinos et al\\.", "year": 2006}, {"title": "Evolutionary Study of Web Spam: Webb Spam Corpus 2011 versus Webb Spam Corpus 2006", "author": ["De Wang", "Danesh Irani", "Calton Pu."], "venue": "CollaborateCom-2012. 40\u201349.", "citeRegEx": "Wang et al\\.,? 2012", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Online Feature Selection and Its Applications", "author": ["Jialei Wang", "Peilin Zhao", "Steven CH Hoi", "Rong Jin."], "venue": "IEEE Transactions on Knowledge and Data Engineering (2013), 1\u201314.", "citeRegEx": "Wang et al\\.,? 2013", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Feature selection for SVMs", "author": ["Jason Weston", "Sayan Mukherjee", "Olivier Chapelle", "Massimiliano Pontil", "Tomaso Poggio", "Vladimir Vapnik."], "venue": "NIPS, Vol. 12. 668\u2013674.", "citeRegEx": "Weston et al\\.,? 2000", "shortCiteRegEx": "Weston et al\\.", "year": 2000}, {"title": "Model mining for robust feature selection", "author": ["Adam Woznica", "Phong Nguyen", "Alexandros Kalousis."], "venue": "ACM SIGKDD-2012. ACM, 913\u2013921.", "citeRegEx": "Woznica et al\\.,? 2012", "shortCiteRegEx": "Woznica et al\\.", "year": 2012}, {"title": "Online feature selection with streaming features", "author": ["XindongWu", "Kui Yu", "Wei Ding", "HaoWang", "Xingquan Zhu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "XindongWu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "XindongWu et al\\.", "year": 2013}, {"title": "Online streaming feature selection", "author": ["Xindong Wu", "Kui Yu", "Hao Wang", "Wei Ding."], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10). 1159\u20131166.", "citeRegEx": "Wu et al\\.,? 2010", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Data mining with big data", "author": ["Xindong Wu", "Xingquan Zhu", "Gong-Qing Wu", "Wei Ding."], "venue": "Knowledge and Data Engineering, IEEE Transactions on 26, 1 (2014), 97\u2013107.", "citeRegEx": "Wu et al\\.,? 2014", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "Feature-selection-based Dynamic Transfer Ensemble Model for Customer Churn Prediction", "author": ["Jin Xiao", "Yi Xiao", "Annqiang Huang", "Dunhu Liu", "Shouyang Wang."], "venue": "Knowledge and Information Systems 43, 1 (2015), 29\u201351.", "citeRegEx": "Xiao et al\\.,? 2015", "shortCiteRegEx": "Xiao et al\\.", "year": 2015}, {"title": "Classification with Streaming Features: An Emerging-Pattern Mining Approach", "author": ["Kui Yu", "Wei Ding", "Dan A Simovici", "Hao Wang", "Jian Pei", "Xindong Wu."], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD) 9, 4 (2015), 30.", "citeRegEx": "Yu et al\\.,? 2015a", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Tornado Forecasting with Multiple Markov Boundaries", "author": ["Kui Yu", "Dawei Wang", "Wei Ding", "Jian Pei", "David L Small", "Shafiqul Islam", "Xindong Wu."], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2237\u20132246.", "citeRegEx": "Yu et al\\.,? 2015b", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Stable feature selection via dense feature groups", "author": ["Lei Yu", "Chris Ding", "Steven Loscalzo."], "venue": "ACM SIGKDD-2008. ACM, 803\u2013811.", "citeRegEx": "Yu et al\\.,? 2008", "shortCiteRegEx": "Yu et al\\.", "year": 2008}, {"title": "Efficient feature selection via analysis of relevance and redundancy", "author": ["Lei Yu", "Huan Liu."], "venue": "Journal of Machine Learning Research 5 (2004), 1205\u20131224.", "citeRegEx": "Yu and Liu.,? 2004", "shortCiteRegEx": "Yu and Liu.", "year": 2004}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["Ming Yuan", "Yi Lin."], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68, 1 (2006), 49\u201367.", "citeRegEx": "Yuan and Lin.,? 2006", "shortCiteRegEx": "Yuan and Lin.", "year": 2006}, {"title": "The Emerging \u201cBig Dimensionality", "author": ["Yiteng Zhai", "Y Ong", "I Tsang."], "venue": "Computational Intelligence Magazine, IEEE 9, 3 (2014), 14\u201326.", "citeRegEx": "Zhai et al\\.,? 2014", "shortCiteRegEx": "Zhai et al\\.", "year": 2014}, {"title": "Discovering Support and Affiliated Features from Very High Dimensions", "author": ["Yiteng Zhai", "Mingkui Tan", "Ivor Tsang", "Yew Soon Ong."], "venue": "ICML-2012. 1455\u20131462.", "citeRegEx": "Zhai et al\\.,? 2012", "shortCiteRegEx": "Zhai et al\\.", "year": 2012}, {"title": "Scaling Cut Criterionbased Discriminant Analysis for Supervised Dimension Reduction", "author": ["Xiangrong Zhang", "Yudi He", "Licheng Jiao", "Ruochen Liu", "Ji Feng", "Sisi Zhou."], "venue": "Knowledge and information systems 43, 3 (2015), 633\u2013655.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Searching for Interacting Features", "author": ["Zheng Zhao", "Huan Liu"], "venue": "In IJCAI,", "citeRegEx": "Zhao and Liu.,? \\Q2007\\E", "shortCiteRegEx": "Zhao and Liu.", "year": 2007}, {"title": "On similarity preserving feature selection", "author": ["Zheng Zhao", "Lei Wang", "Huan Liu", "Jieping Ye."], "venue": "IEEE Transactions on Knowledge and Data Engineering 25 (2013), 619\u2013632.", "citeRegEx": "Zhao et al\\.,? 2013", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}, {"title": "Streamwise feature selection", "author": ["Jing Zhou", "Dean P Foster", "Robert A Stine", "Lyle H Ungar."], "venue": "Journal of Machine Learning Research 7 (2006), 1861\u20131885.", "citeRegEx": "Zhou et al\\.,? 2006", "shortCiteRegEx": "Zhou et al\\.", "year": 2006}, {"title": "Manifold elastic net: a unified framework for sparse dimension reduction", "author": ["Tianyi Zhou", "Dacheng Tao", "Xindong Wu."], "venue": "Data Mining and Knowledge Discovery 22, 3 (2011), 340\u2013371.", "citeRegEx": "Zhou et al\\.,? 2011", "shortCiteRegEx": "Zhou et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 36, "context": "In data mining and machine learning, the task of feature selection is to choose a subset of relevant features and remove irrelevant and redundant features from high-dimensional data towards maintaining a parsimonious model [Guyon and Elisseeff 2003; Liu and Yu 2005; Xiao et al. 2015; Zhang et al. 2015].", "startOffset": 223, "endOffset": 303}, {"referenceID": 44, "context": "In data mining and machine learning, the task of feature selection is to choose a subset of relevant features and remove irrelevant and redundant features from high-dimensional data towards maintaining a parsimonious model [Guyon and Elisseeff 2003; Liu and Yu 2005; Xiao et al. 2015; Zhang et al. 2015].", "startOffset": 223, "endOffset": 303}, {"referenceID": 35, "context": "In the era of big data today, many emerging applications, such as social media services, high resolution images, genomic data analysis, and document data analysis, consume data of extremely high dimensionality, in the order of millions or more [Wu et al. 2014; Zhai et al. 2014; Chen et al. 2014; Yu et al. 2015a; Yu et al. 2015b].", "startOffset": 244, "endOffset": 330}, {"referenceID": 42, "context": "In the era of big data today, many emerging applications, such as social media services, high resolution images, genomic data analysis, and document data analysis, consume data of extremely high dimensionality, in the order of millions or more [Wu et al. 2014; Zhai et al. 2014; Chen et al. 2014; Yu et al. 2015a; Yu et al. 2015b].", "startOffset": 244, "endOffset": 330}, {"referenceID": 2, "context": "In the era of big data today, many emerging applications, such as social media services, high resolution images, genomic data analysis, and document data analysis, consume data of extremely high dimensionality, in the order of millions or more [Wu et al. 2014; Zhai et al. 2014; Chen et al. 2014; Yu et al. 2015a; Yu et al. 2015b].", "startOffset": 244, "endOffset": 330}, {"referenceID": 37, "context": "In the era of big data today, many emerging applications, such as social media services, high resolution images, genomic data analysis, and document data analysis, consume data of extremely high dimensionality, in the order of millions or more [Wu et al. 2014; Zhai et al. 2014; Chen et al. 2014; Yu et al. 2015a; Yu et al. 2015b].", "startOffset": 244, "endOffset": 330}, {"referenceID": 38, "context": "In the era of big data today, many emerging applications, such as social media services, high resolution images, genomic data analysis, and document data analysis, consume data of extremely high dimensionality, in the order of millions or more [Wu et al. 2014; Zhai et al. 2014; Chen et al. 2014; Yu et al. 2015a; Yu et al. 2015b].", "startOffset": 244, "endOffset": 330}, {"referenceID": 29, "context": "For example, the Web Spam Corpus 2011 [Wang et al. 2012] collected approximately 16 million features (attributes) for web spam page detection, and the data set from KDD CUP 2010 about using educational data mining to accurately predict student performance includes more than 29 million features.", "startOffset": 38, "endOffset": 56}, {"referenceID": 42, "context": "The scalability of feature selection methods becomes critical to tackle millions of features [Zhai et al. 2014].", "startOffset": 93, "endOffset": 111}, {"referenceID": 30, "context": "Another example is feature selection in bioinformatics, where acquiring the full set of features for every training instance is expensive because of the high cost in conducting wet lab experiments [Wang et al. 2013].", "startOffset": 197, "endOffset": 215}, {"referenceID": 1, "context": "To search for a minimal subset of features that leads to the most accurate prediction model, two types of feature selection approaches were proposed in the literature, namely, batch methods [Brown et al. 2012; Woznica et al. 2012; Javed et al. 2014] and online methods [Wu et al.", "startOffset": 190, "endOffset": 249}, {"referenceID": 32, "context": "To search for a minimal subset of features that leads to the most accurate prediction model, two types of feature selection approaches were proposed in the literature, namely, batch methods [Brown et al. 2012; Woznica et al. 2012; Javed et al. 2014] and online methods [Wu et al.", "startOffset": 190, "endOffset": 249}, {"referenceID": 10, "context": "To search for a minimal subset of features that leads to the most accurate prediction model, two types of feature selection approaches were proposed in the literature, namely, batch methods [Brown et al. 2012; Woznica et al. 2012; Javed et al. 2014] and online methods [Wu et al.", "startOffset": 190, "endOffset": 249}, {"referenceID": 30, "context": "2014] and online methods [Wu et al. 2013; Wang et al. 2013].", "startOffset": 25, "endOffset": 59}, {"referenceID": 30, "context": "Moreover, a batch method has to access the full feature set prior to the learning task [Wu et al. 2013; Wang et al. 2013].", "startOffset": 87, "endOffset": 121}, {"referenceID": 30, "context": "One assumes that the number of features on training data is fixed while the number of data points changes over time, such as the OFS algorithm [Hoi et al. 2012; Wang et al. 2013] that performs feature selection upon each data instance.", "startOffset": 143, "endOffset": 178}, {"referenceID": 47, "context": "2013] and alphainvesting algorithms [Zhou et al. 2006].", "startOffset": 36, "endOffset": 54}, {"referenceID": 30, "context": "[Wang et al. 2013] further proposed the OGFS (Online Group Feature Selection) algorithm by assuming that feature groups are processed in a sequential scan.", "startOffset": 0, "endOffset": 18}, {"referenceID": 42, "context": "It is still an open research problem to efficiently reduce computational cost when the dimensionality is in the scale of millions or more [Wu et al. 2013; Zhai et al. 2014].", "startOffset": 138, "endOffset": 172}, {"referenceID": 31, "context": "The embedded methods attempt to simultaneously maximize classification performance and minimize the number of features used based on a classification or regression model with specific penalties on coefficients of features [Tibshirani 1996; Weston et al. 2000; Zhou et al. 2011].", "startOffset": 222, "endOffset": 277}, {"referenceID": 48, "context": "The embedded methods attempt to simultaneously maximize classification performance and minimize the number of features used based on a classification or regression model with specific penalties on coefficients of features [Tibshirani 1996; Weston et al. 2000; Zhou et al. 2011].", "startOffset": 222, "endOffset": 277}, {"referenceID": 19, "context": "A filter method is independent of any classifiers, and applies evaluation measures such as distance, information, dependency, or consistency to select features [Dash and Liu 2003; Forman 2003; Peng et al. 2005; Song et al. 2012; Liu et al. 2014].", "startOffset": 160, "endOffset": 245}, {"referenceID": 23, "context": "A filter method is independent of any classifiers, and applies evaluation measures such as distance, information, dependency, or consistency to select features [Dash and Liu 2003; Forman 2003; Peng et al. 2005; Song et al. 2012; Liu et al. 2014].", "startOffset": 160, "endOffset": 245}, {"referenceID": 15, "context": "A filter method is independent of any classifiers, and applies evaluation measures such as distance, information, dependency, or consistency to select features [Dash and Liu 2003; Forman 2003; Peng et al. 2005; Song et al. 2012; Liu et al. 2014].", "startOffset": 160, "endOffset": 245}, {"referenceID": 19, "context": "Due to their simplicity and low computational cost, many filter methods have been proposed to solve the feature selection problem, such as the well-established mRMR (minimal-Redundancy-Maximal-Relevance) algorithm [Peng et al. 2005] and the FCBF (Fast Correlation-Based Filter) algorithm [Yu and Liu 2004].", "startOffset": 214, "endOffset": 232}, {"referenceID": 46, "context": "[Zhao et al. 2013] proposed a novel framework to consolidate different criteria to handle feature redundancies.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "[Brown et al. 2012] presented a unifying framework for information theoretic feature selection using an optimized loss function of the conditional likelihood of the training labels.", "startOffset": 0, "endOffset": 19}, {"referenceID": 25, "context": "[Tan et al. 2010; Tan et al. 2014] proposed the efficient embedded algorithm, the FGM (Feature Generating Machine) algorithm, and Zhai et al.", "startOffset": 0, "endOffset": 34}, {"referenceID": 24, "context": "[Tan et al. 2010; Tan et al. 2014] proposed the efficient embedded algorithm, the FGM (Feature Generating Machine) algorithm, and Zhai et al.", "startOffset": 0, "endOffset": 34}, {"referenceID": 43, "context": "[Zhai et al. 2012] further presented the GDM (Group Discovery Machine) algorithm that outperforms the FGM algorithm.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "[Friedman et al. 2010], which enables to encourage sparsity at the levels of both features and groups simultaneously.", "startOffset": 0, "endOffset": 22}, {"referenceID": 30, "context": "[Wang et al. 2013] proposed an online feature selection method, OFS, which assumes data instances are sequentially added.", "startOffset": 0, "endOffset": 18}, {"referenceID": 47, "context": "[Zhou et al. 2006] presented Alpha-investing which sequentially considers new features as additions to a predictive model by modeling the candidate feature set as a dynamically generated stream.", "startOffset": 0, "endOffset": 18}, {"referenceID": 34, "context": "[Wu et al. 2010; Wu et al. 2013] presented the OSFS (Online Streaming Feature Selection) algorithm and its faster version, the FastOSFS algorithm.", "startOffset": 0, "endOffset": 32}, {"referenceID": 30, "context": "[Wang et al. 2013] proposed the OGFS (Online Group Feature Selection) algorithm.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "(1)? Based on the work of [Brown et al. 2012], Eq.", "startOffset": 26, "endOffset": 45}, {"referenceID": 1, "context": "By the work of [Brown et al. 2012], Eq.", "startOffset": 15, "endOffset": 34}, {"referenceID": 1, "context": "The second term equals to I(C;S\u03b8\u0304|S\u03b8), that is, the conditional mutual information between C and S\u03b8\u0304, given S\u03b8 [Brown et al. 2012].", "startOffset": 111, "endOffset": 130}, {"referenceID": 1, "context": "1 of [Brown et al. 2012] for the details on how to get Eq.", "startOffset": 5, "endOffset": 24}, {"referenceID": 19, "context": "This is essentially the idea behind the well-established batch feature selection algorithms, such as mRMR [Peng et al. 2005] and FCBF [Yu and Liu 2004].", "startOffset": 106, "endOffset": 124}, {"referenceID": 0, "context": "Due to pairwise comparisons, our algorithm focuses on finding an approximate Markov blanket (the parents and children of the class attribute in a Bayesian network [Aliferis et al. 2010]) and does not attempt to discover positive interactions between features (there exists a positive interaction between Fi and Fj with respect to C even though Fi is completely useless by itself with respect to C, but Fi can provide significantly discriminative power jointly with Fj [Jakulin and Bratko 2003; Zhao and Liu 2007]).", "startOffset": 163, "endOffset": 185}, {"referenceID": 21, "context": "We calculate symmetrical uncertainty [Press et al. 1996] instead of I(X,Y ), which is defined by", "startOffset": 37, "endOffset": 56}, {"referenceID": 0, "context": "We use fourteen benchmark data sets as our test beds, including ten high-dimensional data sets [Aliferis et al. 2010; Yu et al. 2008] and four extremely high-dimensional data sets, as shown in Table II.", "startOffset": 95, "endOffset": 133}, {"referenceID": 39, "context": "We use fourteen benchmark data sets as our test beds, including ten high-dimensional data sets [Aliferis et al. 2010; Yu et al. 2008] and four extremely high-dimensional data sets, as shown in Table II.", "startOffset": 95, "endOffset": 133}, {"referenceID": 47, "context": "2013], Alpha-investing [Zhou et al. 2006], and OFS [Wang et al.", "startOffset": 23, "endOffset": 41}, {"referenceID": 30, "context": "2006], and OFS [Wang et al. 2013].", "startOffset": 15, "endOffset": 33}, {"referenceID": 46, "context": "Fast-OSFS and Alpha-investing assume features on training data arrive one by one at a time while OFS assumes data examples come one by one; \u2014Three batch methods: one well-established algorithm of FCBF [Yu and Liu 2004], and two state-of-the-art algorithms, SPSF-LAR [Zhao et al. 2013] and GDM [Zhai et al.", "startOffset": 266, "endOffset": 284}, {"referenceID": 43, "context": "2013] and GDM [Zhai et al. 2012].", "startOffset": 14, "endOffset": 32}, {"referenceID": 47, "context": "01 for Fast-OSFS, and for Alpha-investing, the parameters are set to the values used in [Zhou et al. 2006].", "startOffset": 88, "endOffset": 106}, {"referenceID": 11, "context": "[Kalousis et al. 2007].", "startOffset": 0, "endOffset": 22}, {"referenceID": 39, "context": "[Yu et al. 2008] to evaluate the stabilities of SAOLA, Fast-OSFS, Alpha-investing, and OFS.", "startOffset": 0, "endOffset": 16}, {"referenceID": 43, "context": "We also select the GDM algorithm [Zhai et al. 2012] which is one of the most recent batch feature selection methods in dealing with very large dimensionality.", "startOffset": 33, "endOffset": 51}, {"referenceID": 28, "context": "In this section, we compare SAOLA and FCBF (discovery of approximateMarkov blankets) with two state-of-the-art exact Markov blanket discovery algorithms, IAMB (Incremental Association Markov Blanket) [Tsamardinos and Aliferis 2003] and MMMB (Max-Min Markov Blanket) [Tsamardinos et al. 2006].", "startOffset": 266, "endOffset": 291}, {"referenceID": 18, "context": "Under certain assumptions (sufficient number of data instances and reliably statistical tests), IAMB and MMMB can return the Markov blanket of a given target feature [Pe\u00f1a et al. 2007; Aliferis et al. 2010].", "startOffset": 166, "endOffset": 206}, {"referenceID": 0, "context": "Under certain assumptions (sufficient number of data instances and reliably statistical tests), IAMB and MMMB can return the Markov blanket of a given target feature [Pe\u00f1a et al. 2007; Aliferis et al. 2010].", "startOffset": 166, "endOffset": 206}, {"referenceID": 39, "context": "[Yu et al. 2008].", "startOffset": 0, "endOffset": 16}, {"referenceID": 30, "context": "In this section, we compare our group-SAOLA algorithm with the state-of-the-art online group feature selection methods, OGFS [Wang et al. 2013] and a well-established batch group feature selection method, Sparse Group Lasso [Friedman et al.", "startOffset": 125, "endOffset": 143}, {"referenceID": 6, "context": "2013] and a well-established batch group feature selection method, Sparse Group Lasso [Friedman et al. 2010].", "startOffset": 86, "endOffset": 108}], "year": 2016, "abstractText": "Feature selection is important in many big data applications. Two critical challenges closely associate with big data. Firstly, in many big data applications, the dimensionality is extremely high, in millions, and keeps growing. Secondly, big data applications call for highly scalable feature selection algorithms in an online manner such that each feature can be processed in a sequential scan. We present SAOLA, a Scalable and Accurate OnLine Approach for feature selection in this paper. With a theoretical analysis on bounds of the pairwise correlations between features, SAOLA employs novel pairwise comparison techniques and maintain a parsimonious model over time in an onlinemanner. Furthermore, to deal with upcoming features that arrive by groups, we extend the SAOLA algorithm, and then propose a new group-SAOLA algorithm for online group feature selection. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse at the levels of both groups and individual features simultaneously. An empirical study using a series of benchmark real data sets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on data sets of extremely high dimensionality, and have superior performance over the state-of-the-art feature selection methods.", "creator": "LaTeX with hyperref package"}}}