{"id": "1512.00242", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2015", "title": "Towards Dropout Training for Convolutional Neural Networks", "abstract": "Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture. Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve state-of-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation. Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.", "histories": [["v1", "Tue, 1 Dec 2015 12:46:11 GMT  (802kb)", "http://arxiv.org/abs/1512.00242v1", "This paper has been published in Neural Networks,this http URL"]], "COMMENTS": "This paper has been published in Neural Networks,this http URL", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["haibing wu", "xiaodong gu"], "accepted": false, "id": "1512.00242"}, "pdf": {"name": "1512.00242.pdf", "metadata": {"source": "CRF", "title": "Towards Dropout Training for Convolutional Neural Networks", "authors": ["Haibing Wu", "Xiaodong Gu"], "emails": ["haibingwu13@fudan.edu.cn", "xdgu@fudan.edu.cn"], "sections": [{"heading": null, "text": "For deep Convolutionary Neural Networks, dropout is known to work well in fully networked layers, but its effect in both Convolutionary and Pooling layers is not yet clear. This paper shows that Max Pooling Dropout is synonymous with randomly selected activation based on a multinomial distribution during training. In light of this insight, we support the use of our proposed Probabilistic Weighted Pooling instead of the commonly used Max Pooling to act as a model averaging during the test period. Empirical evidence confirms the superiority of Probabilistic Weighted Pooling. We also demonstrate empirically that the effect of Constitutional Dropout is not trivial, despite the dramatically reduced possibility of overfitting due to the Convolutionary Architecture. Sophisticated Dropout Training at the same time in Max Pooling and Fully Networked Layers, we achieve state-of-the-art pooling based on MNISCT, IAR-10."}, {"heading": "1 Introduction", "text": "In fact, we will be able to put ourselves at the top of society, and we will be able to put ourselves at the top of society, \"he said in an interview."}, {"heading": "2 Review of Dropout Training for Convolutional Neural Networks", "text": "In fact, it is so that most people are able to take themselves into duty, to take themselves into duty. (...) In fact, it is so that most people are able to take themselves into duty. (...) It is so that they are taken into duty. (...) It is so that they are taken into duty. (...) It is so that they are taken into duty. (...) It is so that they are taken into duty. (...) It is so that they are taken into duty. (...) It is as if they are taken into duty. (...) It is as if they are taken into duty. (...) It is as if they are taken into duty. (...). (...) It is so. (...). (...) It is so. (...). (...). (It is. (...). (It is.) It is. (...). (It is. (...). (It is. (...). (It is. () It is. (...). (It is. (). (It is. (...). (It is. (). (It is. (It is.). (It is. (...). (It is. (It is.). (It is. (it.). (It is. (it.). (It is. (it.). (it.). (it is. (it. (it.). (it is. (it.). (it. (it is.). (it. (it.). (it. (it. (it.). (it.). (it is. (it.). (it is. (it. (it.). (it.). (it is. (it.). (it. (it. (it.). (it is.). (it is. (it is. (it.). (it is. (it is.). (it is. (it. (it.). (it is.). (it is.). (it is. (it is. (it.). (it is.). (it is.). (it is. (it is. (it is.). (it is. (it is. (it"}, {"heading": "3 Max-Pooling Dropout and Convolutional Dropout", "text": "We now show that max pooling dropout is equivalent to sample activation according to a multinomial distribution during training. Based on this interpretation, we propose to use probabilistically weighted pooling at test time. We also describe convolutionary dropout."}, {"heading": "3.1 Max-Pooling Dropout", "text": "Consider a standard CNN, which is composed of alternating revolutionary and pooling layers, with) (liafully-connected layers on top. In each presentation of a training example, forward propagation without dropout can be described as) () () (1) (), () 1 (),..., (lj l l i ll j Riaaaapoola. (1) (l jR is the pooling function) (l jR pooling region j at layer l and is the activity of each neuron within it. | |) (l jRn is the number of units in) (l jR. Pool () denotes the pooling function. Pooling operation provides a form of spatial transformation invariance and reduces the computational complexity for the upper layers. An ideal pooling method is to obtain task-related information while discarding relevant image details."}, {"heading": "3.1.1 Max-Pooling Dropout at Training Time", "text": "(1) (1) (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2),"}, {"heading": "3.1.2 Probabilistic Weighted Pooling at Test Time", "text": "When using dropout in fully interconnected layers during training, the entire network containing all hidden units should be used at test time, halving the outgoing weights (Hinton et al. 2012) or halving their activations. If using max-pooling dropout during training, one could intuitively select the strongest activation, which is reduced by the probability of maintaining:),...,..., max () (1) 1 (l n l l i l i l i l i l j aaapa. (7) We call this pooling scheme scaled max-pooling.Instead, we suggest using probabilistic pooling to efficiently obtain a more accurate approximation of the averaging of all possibly formed dropout networks. In this pooling scheme, the pooled activity is a linear weighted summation of activations in each region: 1) () (niliinilij apa (8) Here the probability is exactly iqn."}, {"heading": "3.2 Convolutional Dropout", "text": "In this case, it is a matter of a pure experimental arrangement, in which it concerns an experimental arrangement, in which the experimental arrangements of the experimental arrangements of the experimental arrangements of the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the experimental arrangements, the arrangements, the arrangements, the experimental arrangements, the arrangements, the experimental arrangements, the arrangements, the arrangements, the arrangements, the experimental arrangements, the experimental arrangements, the arrangements, the arrangements, the arrangements, the arrangements, the arrangements, the arrangements, the, the arrangements, the arrangements, the, the, the arrangements, the, the, the, the, the, the, the, the, the, the, the arrangements, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the"}, {"heading": "4 Empirical Evaluations", "text": "The purpose of our experiments is threefold: (1) to provide empirical evidence that probable weighted pooling is a more accurate approximation to averaging all possible max pooling dropout models than max pooling and scaled max pooling, (2) to explore dropout training in different layers, and (3) to compare maximized pooling dropout against stochastic pooling. We use rectified linear functions (Krizhevsky, Sutskever, & Hinton, 2010) for conventional and fully connected layers, and Softmax activation functions for the base layer. Commonly used sigmoidal and tanh nonlinearities are not accepted due to the progression problem with them. Vanizing gradient effects cause slow optimization convergence, but the formation of a deep model is already very expensive to compute."}, {"heading": "4.1 MNIST", "text": "We first conduct experiments with MNIST, a widely used benchmark dataset in computer visualization. It consists of 28x28 pixels of grayscale images, each with a digit from 0 to 9. There are 60,000 training examples and 10,000 test examples. We do no pre-processing except to scale the pixel values to [0, 1]."}, {"heading": "4.1.1 Probabilistic Weighted Pooling vs. Scaled Max-Pooling", "text": "In order to confirm the superiority of probabilistic pooling over max pooling and scaled max pooling, we first train different CNN models using two different architectures, 1x28x28-6C52P2-12C5-2P2-10N and 1x28x28-12C5-2P2-1000N. The maximum failure rate is applied during the training, the maximum failure rate, the maximum pooling and the probable weighted pooling are used to act as averages each. Figure 3 illustrates the training and test error rates of both CNN architectures over 300 training pools. The use of probabilistic weighted pooling in the test time is always lower than the training errors generated by the scaled max pooling."}, {"heading": "4.1.2 Dropout Training in Different Layers", "text": "As such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such, as such,"}, {"heading": "4.2 CIFAR-10", "text": "The CIFAR-10 dataset (Krizhevsky, 2009) consists of ten classes of nature images with 50,000 examples for training and 10,000 for exam. Each example is a 32 x 32 cm RGB image obtained from the tiny images collected from the web. We also scale to [0, 1] for this dataset and subtract the mean value of each channel calculated for each image. Compared to MNIST, the CIFAR-10 images are very different within each class, so we build deeper and wider networks to model the complex non-linear relationships. The architecture is 3x32 x 32 x 96x 5 x 128 x 128 x 128 x 12x 12x 12x 12x 12x 12x 12x 12x 12x."}, {"heading": "4.3 CIFAR-100", "text": "CIFAR-100 (Krizhevsky, 2009) is exactly like CIFAR-10, but with 100 categories. We also scale to [0, 1] for CIFAR-100 and subtract the mean value of each R / G / B channel. The architecture is the same as for CIFAR-10, except that the number of output units is 100. Models are trained for 1000 epochs. Fig.7 compares different pooling methods at test point for max-pooling dropout trained models to CIFAR-100. The retention probability is 0.3, 0.5 and 0.7, respectively. Here again, probabilistically weighted pooling shows clear superiority over max-pooling and scaled max-pooling.We then train different CNN models separately and simultaneously with dropout models to CIFAR-100."}, {"heading": "4.4 Max-Pooling Dropout and Stochastic Pooling", "text": "It is therefore a question of whether and in what form and in what form people in the individual countries abide by the rules. (...) It is a question of whether and in what form they abide by the rules. (...) It is a question of whether and in what form they abide by the rules. (...) It is a question of whether and in what form they abide by the rules. (...) It is a question of whether and in what form they abide by the rules. (...) It is a question of whether and in what form and in what form they abide by the rules. (...) It is a question of how and in what form and in what form they abide by the rules. (...) It is a question of and in what form and in which form they are applied. (...) It is a question of how and in which form they are applied."}, {"heading": "4.5 A Brief Summary of Experimental Results", "text": "The summary of the experimental results is threefold in response to the purpose of our experiments. (1) For the maximum pooling probability trained dropout CNN models, the probabilistic weighted pooling method not only fits better to the training data, but generalizes better to the test data than the maximum pooling method and the scaled max pooling method. For the low embedding probability, the maximum pooling method and the scaled max pooling method lead to very poor results, while the probabilistic weighted pooling method works very well. As the embedding probability increases, the performance gap narrows. (2) The separate application of the Constitutional Maximum Pooling method and the fully networked dropout method improves performance, but the Convolutionary study dropout seems to be far less beneficial."}, {"heading": "5 Conclusions", "text": "This paper focuses on the problem of understanding and using input dropouts to maximize the pool of layers of Convolutionary Neural Networks. At the time of training, the dropout in max pooling is equivalent to a random selection of activation according to a multinomial distribution, and the number of potentially formed networks is exponential in the number of input units to the pooling layers. At test date, a new pooling method is proposed, probabilistic weighted pooling, to act as a model mean. Experimental evidence confirms the benefits of using dropouts in max pooling and confirms the superiority of probable weighted pooling over max pooling and scaled max pooling. Simultaneous use of dropouts in different layers could further improve performance, but should be careful."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the National Natural Science Foundation of China with grant 61371148."}], "references": [{"title": "The dropout learning algorithm", "author": ["P. Baldi", "P. Sadowski"], "venue": "Artificial Intelligence,", "citeRegEx": "Baldi and Sadowski,? \\Q2014\\E", "shortCiteRegEx": "Baldi and Sadowski", "year": 2014}, {"title": "Representation learning: a review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Y.L. Boureau", "Ponce J", "Y. LeCun"], "venue": "In Proceedings 27th of International Conference on Machine Learning (ICML", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning, 24, 123-140.", "citeRegEx": "Breiman,? 1996", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Multi-column deep neural networks for image classification", "author": ["Ciresan. D", "U. Meier", "J. Schmidhuber"], "venue": "In Proceedings of 2014 IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "D. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "D. et al\\.", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "Improving neural networks by preventing co-adaption of feature detectors", "author": ["G.E. Hinton", "N. Srivastave", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Improving deep neural networks with probabilistic maxout units", "author": ["T. Springenberg J", "M. Riedmiller"], "venue": "In Proceedings of 3rd International Conference on Learning Representations (ICLR", "citeRegEx": "J. and Riedmiller,? \\Q2014\\E", "shortCiteRegEx": "J. and Riedmiller", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "M.S. diss., Department of Computer Science, University of Toronto.", "citeRegEx": "Krizhevsky,? 2009", "shortCiteRegEx": "Krizhevsky", "year": 2009}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Probability in banach spaces", "author": ["M. Ledoux", "M. Talagrand"], "venue": null, "citeRegEx": "Ledoux and Talagrand,? \\Q1991\\E", "shortCiteRegEx": "Ledoux and Talagrand", "year": 1991}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "Yan S"], "venue": "In Proceedings of 3rd International Conference on Learning Representations (ICLR", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Probable networks and plausible predictions: A review of practical Bayesian methods for supervised neural networks", "author": ["D.C. Mackay"], "venue": "Bayesian Methods for Backpropagation Networks.", "citeRegEx": "Mackay,? 1995", "shortCiteRegEx": "Mackay", "year": 1995}, {"title": "Evaluation of pooling operations in convolutional architectures for object recognition", "author": ["D. Scherer", "A. Muller", "S. Behnke"], "venue": "In Proceedings of 20th International Conference on Artificial Neural Networks (ICANN", "citeRegEx": "Scherer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Scherer et al\\.", "year": 2010}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "Hinton. G. E", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["N. Vinod", "G.E. Hinton"], "venue": "In Proceedings 27th of International Conference on Machine Learning (ICML", "citeRegEx": "Vinod and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Vinod and Hinton", "year": 2010}, {"title": "Regularization of neural networks using DropConnect", "author": ["L. Wan", "M.D. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "In Proceedings of 30th International Conference on Machine Learning (ICML", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "An empirical analysis of dropout in piecewise linear networks", "author": ["F.D. Warde", "I.J. Goodfellow", "A. Courville", "Y. Bengio"], "venue": "In Proceedings of 3rd International Conference on Learning Representations (ICLR", "citeRegEx": "Warde et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Warde et al\\.", "year": 2014}, {"title": "Dropout training as adaptive regularization", "author": ["S. Wager", "S. Wang", "P. Liang"], "venue": "In Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "Fergus R"], "venue": "In Proceedings of 2nd International Conference on Learning Representations (ICLR", "citeRegEx": "Zeiler and R.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler and R.", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "Dropout (Hinton et al., 2012) is a recently proposed regularizer to fight against over-fitting.", "startOffset": 8, "endOffset": 29}, {"referenceID": 17, "context": "Dropout has also inspired other stochastic model averaging methods such as stochastic pooling (Zeiler & Fergus, 2013) and DropConnect (Wan et al., 2013).", "startOffset": 134, "endOffset": 152}, {"referenceID": 6, "context": "Although dropout is known to work well in fully-connected layers of convolutional neural nets (Hinton et al., 2012; Wan et al., 2013; Krizhevsky, Sutskever, & Hinton, 2012), its effect in convolutional and pooling layers is, however, not well studied.", "startOffset": 94, "endOffset": 172}, {"referenceID": 17, "context": "Although dropout is known to work well in fully-connected layers of convolutional neural nets (Hinton et al., 2012; Wan et al., 2013; Krizhevsky, Sutskever, & Hinton, 2012), its effect in convolutional and pooling layers is, however, not well studied.", "startOffset": 94, "endOffset": 172}, {"referenceID": 10, "context": "CNNs have far been known to produce remarkable performance on MNIST (LeCun et al., 1998), but they, together with other neural network models, fell out of favor in practical machine learning as simpler models such as SVMs became the popular choices in the 1990s and 2000s.", "startOffset": 68, "endOffset": 88}, {"referenceID": 13, "context": "Other common forms of regularization include early stopping, Bayesian fitting (Mackay, 1995), weight elimination (Ledoux & Talagrand, 1991) and data augmentation.", "startOffset": 78, "endOffset": 92}, {"referenceID": 3, "context": "It is similar to bagging (Breiman, 1996), in which a set of models are trained on different subsets of the same training data.", "startOffset": 25, "endOffset": 40}, {"referenceID": 3, "context": "It is similar to bagging (Breiman, 1996), in which a set of models are trained on different subsets of the same training data. At test time, different models\u2019 predictions are averaged together. In traditional bagging, each model has independent parameters, and all members would be trained explicitly. In the case of dropout training, there are exponentially many possibly trained models, and these models share the same parameters, but not all of them are explicitly trained. Actually, the number of explicitly trained models is not larger than m\uf0b4e, where m is the number of training example, and e is the training epochs. This is much smaller than the number of possibly trained models, n 2 ( n is number of hidden units in a feed-forward neural networks). Therefore, a vast majority of models are not explicitly trained at training time. At test time, bagging makes a prediction by averaging together all the sub-models\u2019 predictions with the arithmetic mean, but it is not obvious how to do so with the exponentially many models trained by dropout. Fortunately, the average prediction of exponentially many sub-models can be approximately computed simply by running the whole network with the weights scaled by retaining probability. The approximation has been mathematically characterized for linear and sigmoidal networks (Baldi & Sadowski, 2014; Wager el al., 2013); for piecewise linear networks such as rectified linear networks, Warde et al. (2014) empirically showed that weight-scaling approximation is a remarkable and accurate surrogate for the true geometric mean, by comparing against the true average in small enough networks", "startOffset": 26, "endOffset": 1458}, {"referenceID": 15, "context": "Compared to original work on dropout, (Srivastava et al., 2014) provided more exhaustive experimental results.", "startOffset": 38, "endOffset": 63}, {"referenceID": 17, "context": "DropConnect (Wan et al., 2013) is a natural generalization of dropout for regularizing large feed-forward nets.", "startOffset": 12, "endOffset": 30}, {"referenceID": 6, "context": "Since dropout was thought to be far less advantageous in convolutional layers, pioneering work by Hinton et al. (2012) only applied it to fully-connected layers.", "startOffset": 98, "endOffset": 119}, {"referenceID": 6, "context": "Since dropout was thought to be far less advantageous in convolutional layers, pioneering work by Hinton et al. (2012) only applied it to fully-connected layers. It was the reason they provided that the convolutional shared-filter architecture was a drastic reduction in the number of parameters and thus reduced its possibility to overfit in convolutional layers. Wonderful work by Krizhevsky et al. (2012) trained a very big convolutional neural net, which had 60 million parameters, to classify 1.", "startOffset": 98, "endOffset": 408}, {"referenceID": 6, "context": "Using dropout in fully-connected layers during training, the whole network containing all the hidden units should be used at test time, but with their outgoing weights halved (Hinton et al. 2012), or with their activations halved.", "startOffset": 175, "endOffset": 195}, {"referenceID": 6, "context": "However, it is far less advantageous, since the shared-filter and local-connectivity architecture in convolutional layers is a drastic reduction in the number of parameters and this already reduces the possibility to overfit (Hinton et al., 2012).", "startOffset": 225, "endOffset": 246}, {"referenceID": 8, "context": "The CIFAR-10 dataset (Krizhevsky, 2009) consists of ten classes of natural images with 50,000 examples for training and 10,000 for testing.", "startOffset": 21, "endOffset": 39}, {"referenceID": 6, "context": "60% (Hinton et al., 2012).", "startOffset": 4, "endOffset": 25}, {"referenceID": 8, "context": "CIFAR-100 (Krizhevsky, 2009) is just like CIFAR-10, but with 100 categories.", "startOffset": 10, "endOffset": 28}, {"referenceID": 6, "context": "14 Dropout the last hidden layer (Hinton et al., 2012) 15.", "startOffset": 33, "endOffset": 54}], "year": 2015, "abstractText": "Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture. Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve state-of-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation. Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.", "creator": "Microsoft\u00ae Word 2013"}}}