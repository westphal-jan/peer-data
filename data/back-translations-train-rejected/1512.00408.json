{"id": "1512.00408", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2015", "title": "Reinforcement Learning Applied to an Electric Water Heater: From Theory to Practice", "abstract": "Electric water heaters have the ability to store energy in their water buffer without impacting the comfort of the end user. This feature makes them a prime candidate for residential demand response. However, the stochastic and nonlinear dynamics of electric water heaters, makes it challenging to harness their flexibility. Driven by this challenge, this paper formulates the underlying sequential decision-making problem as a Markov decision process and uses techniques from reinforcement learning. Specifically, we apply an auto-encoder network to find a compact feature representation of the sensor measurements, which helps to mitigate the curse of dimensionality. A wellknown batch reinforcement learning technique, fitted Q-iteration, is used to find a control policy, given this feature representation. In a simulation-based experiment using an electric water heater with 50 temperature sensors, the proposed method was able to achieve good policies much faster than when using the full state information. In a lab experiment, we apply fitted Q-iteration to an electric water heater with eight temperature sensors. Further reducing the state vector did not improve the results of fitted Q-iteration. The results of the lab experiment, spanning 40 days, indicate that compared to a thermostat controller, the presented approach was able to reduce the total cost of energy consumption of the electric water heater by 15%.", "histories": [["v1", "Sun, 29 Nov 2015 18:03:13 GMT  (1007kb,D)", "http://arxiv.org/abs/1512.00408v1", "Submitted to IEEE transaction on smart grid"]], "COMMENTS": "Submitted to IEEE transaction on smart grid", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["frederik ruelens", "bert claessens", "salman quaiyum", "bart de schutter", "robert babuska", "ronnie belmans"], "accepted": false, "id": "1512.00408"}, "pdf": {"name": "1512.00408.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning Applied to an Electric Water Heater: From Theory to Practice", "authors": ["F. Ruelens", "B. J. Claessens", "S. Quaiyum", "B. De Schutter", "R. Belmans"], "emails": ["(frederik.ruelens@esat.kuleuven.be).", "(bert.claessens@vito.be)."], "sections": [{"heading": null, "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "II. REINFORCEMENT LEARNING", "text": "This section provides a non-exhaustive overview of recent developments related to Reinforcement Learning (RL) and demand response. Perhaps the most common model-free RL technique applied to a demand response is standard Q-Learning [17], [18], [20]. After each interaction with the environment, Q-Learning uses time difference learning [14] to update its state response function or Qfunction. A major drawback of Q-Learning is that the given observation is discarded after each update. As a result, more interactions are needed to disseminate already known information across the state. This inefficient use of information limits the application of Q-Learning to real-world applications. Unlike Q-Learning, Batch RL techniques [22], [23] more data is efficient as they store and reuse previous interactions."}, {"heading": "III. PROBLEM FORMULATION", "text": "The goal of this paper is to develop a controller or agent that minimizes the cost of energy consumption of an electric water heater based on an external price profile, which is provided to the agent at the beginning of each day. An agent can measure the temperature of the water buffer through a series of temperature sensors connected along the shell of the buffer tank. In accordance with the approach outlined in [28], the electric water heater is equipped with a backup controller that monitors the agent's control actions if the safety or comfort limitations of the end user are violated. A challenge in developing such an agent is that the dynamics of the electric water heater, future hot water demand and settings of the backup controller are unknown to the agent. To overcome this challenge, this paper draws on the previous work of [16], [21], [28] and applies RL techniques."}, {"heading": "A. Markov decision process framework", "text": "To apply RL, this paper formulates the underlying sequential decision problem of the learner: Q = Q = Q = Q = Q = Q = A. The formulation of the Markov decision process is defined by its d-dimensional state space X-Rd, its action space U-R, its stochastic discrete time transition function f and its cost function \u03c1. The optimization horizon is considered finite, with each discrete time step k the state develops as follows: xk + 1 = f (xk, uk, wk) k = f (k, k) k = f (1,..., T \u2212 1}, (1) where a random error wk is drawn from a conditional distribution pW (xk), with the state being the control action and xk the state X. Linked to each state transition is an optimal cost termination function, which is determined by: ck = x x, uk, wk) k = kk."}, {"heading": "B. Observable state vector", "text": "The observable state vector of an electric water heater contains a time-related component and a controllable component.The time-related component xt describes the time-related part of the state that is relevant for the dynamics of the system.Specifically, the tap water demand of the end user is regarded as a day and week pattern. As such, the time-related component contains the day of the week and the quarter of the day.The controllable component xph represents physical state information that is measured locally and influenced by the control.The controllable component contains the temperature measurements of the ns sensors connected along the hull of the storage.The observable state vector is given by: xk = (d, t xtk, T 1k,..., T i-k,.,., T-n-k), (8), where d-t is the current day of the week, T-t value or T value in the time of the day."}, {"heading": "C. Control action", "text": "The learning expert can control the heating element of the electric water heater with a binary control action uk (0, 1), with 0 on Off and 1 on. However, the safety mechanism that implements the comfort and safety limitations of the end user may override this control action of the learning expert. Function B: X \u00b7 U \u2192 Uph orders the control action uk (U) to a physical action uphk (UP) according to: uphk = B (xk, UK, \u03b8), (9) where the vector defines the safety and user-defined comfort settings of the backup controller. In order to have a generic approach, we assume that the logic of the backup controller is unknown to the learner. However, the learner can measure the physical action uphk enforced by the backup controller (see Fig. 2), which is required to calculate the cost. The logic of the backup controller is enforced by the backup physician (see Backup Controller)."}, {"heading": "D. Cost function", "text": "At the beginning of each optimization phase T \u2206 t, the learner receives a price vector \u03bb = {\u03bbk} Tk = 1 for the next T-time steps. For each time step, the learner receives a cost factor according to: ck = u ph k \u03bbk \u0445 t, (11) where \u03bbk is the price of electricity during the time step k, and the length of a control period."}, {"heading": "IV. BATCH OF FOUR-TUPLES", "text": "In general, the Q function is evaluated on the basis of a series of four tuples (Q = 11, x \"l, cl), but this paper takes into account the following series of four tuples: F = {(xl, x\" l, u \"l), l = 1,.,.,., # F}, (12), where the cost depends on each individual step, how the next state x\" l, and the physical action u ph l was achieved as a result of control measures ul in state xl. Note: F does not include the observed cost cl, since the cost depends on the price vector provided to the learning agent at the beginning of each day., as defined by (8), xl contains all temperature measurements of the sensors connected to the envelope of the water buffer. Learning in a high-dimensional state space requires further observations from the environment to assess the Q function, as more tuples are needed to cover the state of the action space."}, {"heading": "V. FITTED Q-ITERATION", "text": "This section describes the learning algorithm and exploration strategy of the agent based on the series of attribute vectors R introduced in the previous section."}, {"heading": "A. Fitted Q-iteration", "text": "The adapted Q iteration forms an iterative training set Treg with all state-effect pairs (z, u) in R as input. The target values consist of the corresponding Q values, based on the approximation of the Q function of the previous iteration. In the first iteration (N = 1), the Q values approach the expected costs (line 5 in algorithm 1). In the subsequent iterations, the Q values are updated using the Q function of the previous iteration. Consequently, the algorithm needs 1 T iterations until the Q function contains all the information about the future costs. Note that the costs corresponding to each tupel are recalculated using the price vector \u03bb provided at the beginning of the day (line 4 in algorithm 1). As a result, the algorithm can reuse past experiences to find a control guideline for the next day. Following [21], an algorithm uses an extremely randomized tree regression game to estimate the properties of the Q1."}, {"heading": "B. Boltzmann exploration", "text": "During the day, the learning agent uses a Boltzmann exploration strategy [36] and selects an action with the following probability: P (u | z) = e Q (z, u) / \u03c4d\u0442\u0435\u0441\u0442\u0438\u0441\u0442\u0438\u0441\u0442\u0438\u0441\u0442\u0438\u0441\u0442\u0438\u0441\u0442\u0438\u0441\u0442\u043e (z, u \") / \u0441\u0442\u043e\u0441\u0441\u0442\u0438\u0441\u0442\u0438\u0435 (z, u\") / \u0442d (15), where \u0441d is the Boltzmann temperature on day d, Q \u0441\u0442\u0438\u043c is the Qfunction of algorithm 1 and z is the current feature vector measured by the learning agent. If \u0441d \u2192 0, the exploration decreases and the policy becomes greasy. Thus, the exploration begins with a high \u0441\u0442d completely random, but as soon as \u0441d the policy is reduced to the most interesting state action pairs. In the evaluation experiments, Q \u0432 in (15) is linearly scaled between 100 probability, [which is updated to a probability of 1 at the beginning of the experiment]."}, {"heading": "VI. SIMULATION-BASED RESULTS", "text": "This section describes the results of the simulation-based experiments using a non-linear stratified tank model with 50 temperature layers. A detailed description of the stratified tank model can be found in [3]. The specifications of the electric water heater are selected in accordance with the electric water heater used during the laboratory experiment (see Section VII). The simulated electric water heater has an output of 2.36 kW and a water buffer of 200 litres. The experiments use realistic hot water profiles with an average daily consumption of 120 litres [37] and use price information from the Belgian supply market [38] and balancing market [39]. The learner can measure the temperature of the 50 temperature layers obtained with the simulation model. The aim of the first simulation-based experiment is to find a compact condition representation using an auto encoder network and to evaluate the effects of the condition representation on the built-in IQ performance."}, {"heading": "A. Step 1: feature selection", "text": "This experiment compares the performance of the built-in Qiteration combined with different characteristic representations for different fixed batch sizes. Simulations are repeated for 100 simulation days. The average energy costs of these 100 simulations are shown in Fig. 3. As shown in Fig. 3, the performance of the built-in Qiteration combined with a specific state representation will depend on the number of tuples in the batch. For example, AE 3 leads to lower costs than AE 15 for a batch of 10 days, while AE 15 leads to lower costs than AE 3 after 75 days. Furthermore, as shown in Fig. 3, AE 1 will lead to a relatively poor policy, regardless of the batch size. Generally, it can be concluded that for a batch of limited size, set-up Q iteration is a better observation result with lower room temperature."}, {"heading": "B. Step 2: evaluation", "text": "Fig. 4 compares the total cost of energy consumption using a built-in Q iteration combined with AE 5 with the standard thermostat controller for two relevant price profiles, i.e. dayahead prices (top chart) and imbalance prices (bottom chart).The standard thermostat controller activates the heating element when the charge level drops below its minimum threshold and remains activated until the charge level reaches 100%. Note that the standard controller, unlike the learning agent, is agnostic about the price project.The experiment begins with an empty batch and the tuples of the current day are added to the given batch at the end of each day.At the beginning of each day, the auto encoder is trained to find a batch of compact feature vectors, which are then used by a built-in Q iteration to estimate the Q function for the next day.Online, the learning agent Boltzmann uses a prediction based on a prediction 10 leading to an exploration strategy."}, {"heading": "VII. LAB RESULTS", "text": "The aim of our laboratory experiment is to show that a built-in Q-iteration can be successfully applied to minimize the energy costs of a real electric water heater."}, {"heading": "A. Lab setup", "text": "The setup used in the laboratory experiment was part of a pilot project to respond to housing demand in Belgium [40], in which a cluster of 10 electric water heaters was used for direct load control. Fig. 1 shows the electric water heater used in the laboratory experiment. The electric water heater is a standard unit equipped with eight temperature sensors and a controllable power relay. A controllable valve connected to the buffer storage socket is used to simulate the hot water demand of a household with an average daily flow volume of 120 liters [37]. An Arduino prototyping platform with a JSON / RPC 2.0 interface is used to communicate with a computer in the laboratory that operates the learning agents using the appropriate Qiteration. The corresponding Q iteration is implemented in Python and Scikit-learn [41] to estimate the Q function using an extremely randomized ensemble of trees."}, {"heading": "B. Evaluation", "text": "However, the best performance was achieved by integrating the eight temperature measurements into the observable state vector. Using identical price profiles and tap water requirements, Fig. 7 and Fig. 8 show the temperature measurements and performance profiles of the mature learning resource using imbalance prices and daily prices. As can be seen, the learning resource successfully minimized the cost of energy consumption by consuming during low price moments. 1Intel Core i5 - 4GB MemoryFig. 9 shows the experimental results, which extended over 40 days, the built-in Q iteration and the standard thermostat controller. The upper figure of this figure shows the cumulative cost of energy consumption and the lower figure shows the daily cost of energy consumption. After 40 days, the built-in Q iteration was able to reduce the cost of energy consumption by 15% compared to the standard thermostat controller."}, {"heading": "VIII. CONCLUSIONS AND FUTURE WORK", "text": "This work has shown how an auto-encoder network in combination with a well-established batch amplification learning algorithm, called Q-iteration, can be used to reduce the cost of energy consumption of an electric water heater. The auto-encoder network was used to find a compact representation of the state vector. In a series of simulation-based experiments with an electric water heater with 50 temperature sensors, the proposed method was able to converge to good strategies much faster than using full state information. Compared to a standard thermostat controller, the presented approach has reduced the cost of energy consumption by 24% by reducing day-to-day predictive prices and by 34% by using imbalance prices. In a laboratory experiment, Q-iteration was successfully applied to an electric water heater with eight temperature sensors. A decrease in the state vector did not improve the performance of the built-in Iteration."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank Davy Geysen, Geert Jacobs, Koen Vanthournout and Jef Verbeeck from Vito for providing the laboratory setup. This work was supported by a Ph.D. fellowship from the Institute for the Promotion of Innovation through Science and Technology in Flanders (IWT-Vlaanderen) and by the Stable MultI agent LEarnIng for neTworks (SMILE-IT)."}], "references": [{"title": "World Energy Outlook 2013: Renewable Energy Outlook, An annual report released by the International Energy Agency", "author": ["F. Birol"], "venue": "http://www.worldenergyoutlook.org/media/weowebsite/ 2013, Paris, France, [Online: accessed July 21, 2015].", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Ten years of operating experience with a remote controlled water heater load management system at detroit edison", "author": ["B. Hastings"], "venue": "IEEE Trans. on Power Apparatus and Syst., no. 4, pp. 1437\u20131441, 1980.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1980}, {"title": "A smart domestic hot water buffer", "author": ["K. Vanthournout", "R. D\u2019hulst", "D. Geysen", "G. Jacobs"], "venue": "IEEE Trans. on Smart Grid, vol. 3, no. 4, pp. 2121\u20132127, Dec. 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Energy cost calculator for electric and gas water heaters", "author": ["U.S. Department of Energy"], "venue": "http://energy.gov/eere/femp/ energy-cost-calculator-electric-and-gas-water-heaters-0{#}output, [Online: accessed November 10, 2015].", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Electric water heater modeling and control strategies for demand response", "author": ["R. Diao", "S. Lu", "M. Elizondo", "E. Mayhorn", "Y. Zhang", "N. Samaan"], "venue": "Proc. 2012 IEEE Power and Energy Society General Meeting,, pp. 1\u20138.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed voltage control mechanism in low-voltage distribution grid field test", "author": ["S. Iacovella", "K. Lemkens", "F. Geth", "P. Vingerhoets", "G. Deconinck", "R. D\u2019Hulst", "K. Vanthournout"], "venue": "Proc. 4th IEEE PES Innov. Smart Grid Technol. Conf. (ISGT Europe), Oct 2013, pp. 1\u20135.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling and control of aggregated heterogeneous thermostatically controlled loads for ancillary services", "author": ["S. Koch", "J.L. Mathieu", "D.S. Callaway"], "venue": "Proc. 17th IEEE Power Sys. Comput. Conf. (PSCC), Stockholm, Sweden, Aug. 2011, pp. 1\u20137.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "State estimation and control of heterogeneous thermostatically controlled loads for load following", "author": ["J. Mathieu", "D. Callaway"], "venue": "Proc. 45th Hawaii Int. Conf. on System Science (HICSS), Maui, HI, Jan. 2012, pp. 2002\u20132011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Scheduling of domestic water heater power demand for maximizing PV self-consumption using model predictive control", "author": ["F. Sossan", "A.M. Kosek", "S. Martinenas", "M. Marinelli", "H. Bindner"], "venue": "Proc. 4th IEEE PES Innov. Smart Grid Technol. Conf. (ISGT Europe), Oct 2013, pp. 1\u20135.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Model Predictive Control, 2nd ed", "author": ["E.F. Camacho", "C. Bordons"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Beyond theory: the challenge of implementing model predictive control in buildings", "author": ["J. Cigler", "D. Gyalistras", "J. \u0160irok\u1ef3", "V. Tiet", "L. Ferkl"], "venue": "Proc. 11th REHVA World Congress (CLIMA), Czech Republic, Prague, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Model predictive control for energy efficient buildings", "author": ["Y. Ma"], "venue": "Ph.D. dissertation, University of California Berkeley, Mechanical Engineering, Berkeley, CA, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Selecting building predictive control based on model uncertainty", "author": ["M. Maasoumy", "M. Razmara", "M. Shahbakhti", "A. Sangiovanni Vincentelli"], "venue": "Proc. American Control Conference (ACC), Portland, OR, June 2014, pp. 404\u2013411.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Reinforcement learning versus model predictive control: a comparison on a power system problem", "author": ["D. Ernst", "M. Glavic", "F. Capitanescu", "L. Wehenkel"], "venue": "IEEE Trans. Syst., Man, Cybern., Syst., vol. 39, no. 2, pp. 517\u2013529, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep auto-encoder neural networks in reinforcement learning", "author": ["S. Lange", "M. Riedmiller"], "venue": "Proc. IEEE 2010 Int. Joint Conf. on Neural Networks (IJCNN), Barcelona, Spain, July 2010, pp. 1\u20138.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Using smart devices for system-level management and control in the smart grid: A reinforcement learning framework", "author": ["E.C. Kara", "M. Berges", "B. Krogh", "S. Kar"], "venue": "Proc. 3rd IEEE Int. Conf. on Smart Grid Commun. (SmartGridComm), Tainan, Taiwan, Nov. 2012, pp. 85\u201390.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluation of reinforcement learning control for thermal energy storage systems", "author": ["G.P. Henze", "J. Schoenmann"], "venue": "HVAC&R Research, vol. 9, no. 3, pp. 259\u2013275, 2003.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Optimal demand response using device-based reinforcement learning", "author": ["Z. Wen", "D. O\u2019Neill", "H. Maei"], "venue": "IEEE Trans. on Smart Grid, vol. 6, no. 5, pp. 2312\u20132324, Sept 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal bidding of plug-in electric vehicles in a market-based control setup", "author": ["M. Gonz\u00e1lez", "R. Luis Briones", "G. Andersson"], "venue": "Proc. 18th IEEE Power Sys. Comput. Conf. (PSCC), Wroclaw, Poland, 2014, pp. 1\u20137.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Tree-based batch mode reinforcement learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel"], "venue": "Journal of Machine Learning Research, pp. 503\u2013556, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural fitted Q-iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["M. Riedmiller"], "venue": "Proc. 16th European Conference on Machine Learning (ECML), vol. 3720. Porto, Portugal: Springer, Oct. 2005, p. 317.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning for robot soccer", "author": ["M. Riedmiller", "T. Gabel", "R. Hafner", "S. Lange"], "venue": "Autonomous Robots, vol. 27, no. 1, pp. 55\u201373, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Variable selection for dynamic treatment regimes: a reinforcement learning approach", "author": ["R. Fonteneau", "L. Wehenkel", "D. Ernst"], "venue": "Proc. European Workshop on Reinforcement Learning (EWRL), Villeneuve d\u2019Ascq, France, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Experience replay for realtime reinforcement learning control", "author": ["S. Adam", "L. Busoniu", "R. Babu\u0161ka"], "venue": "IEEE Trans. on Syst., Man, and Cybern., Part C: Applications and Reviews, vol. 42, no. 2, pp. 201\u2013212, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Demand response of a heterogeneous cluster of electric water heaters using batch reinforcement learning", "author": ["F. Ruelens", "B. Claessens", "S. Vandael", "S. Iacovella", "P. Vingerhoets", "R. Belmans"], "venue": "Proc. 18th IEEE Power Sys. Comput. Conf. (PSCC), Wroc\u0142aw, Poland, Aug. 2014, pp. 1\u20138.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Residential Demand Response Applications Using Batch Reinforcement Learning", "author": ["F. Ruelens", "B. Claessens", "S. Vandael", "B. De Schutter", "R. Babu\u0161ka", "R. Belmans"], "venue": "Submitted to IEEE Trans. on Smart Grid (http://arxiv.org/pdf/1504.02125.pdf), Apr. 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "PILCO: A model-based and data-efficient approach to policy search", "author": ["M. Deisenroth", "C.E. Rasmussen"], "venue": "Proceedings of the 28th International Conference on machine learning (ICML-11), 2011, pp. 465\u2013472.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximate model-assisted neural fitted q-iteration", "author": ["T. Lampe", "M. Riedmiller"], "venue": "2014 International Joint Conference on Neural Networks (IJCNN), July 2014, pp. 2698\u20132704.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Experimental analysis of data-driven control for a building heating system", "author": ["G.T. Costanzo", "S. Iacovella", "F. Ruelens", "T. Leurs", "B. Claessens"], "venue": "CoRR, vol. abs/1507.03638, 2015. [Online]. Available: http://arxiv.org/abs/1507.03638", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Using PCA to efficiently represent state spaces", "author": ["W. Curran", "T. Brys", "M. Taylor", "W. Smart"], "venue": "The 12th European Workshop on Reinforcement Learning (EWRL 2015), Lille, France, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "Journal of Artificial Intelligence Research, pp. 237\u2013 285, 1996.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1996}, {"title": "Realistic domestic hot-water profiles in different time scales: Report for the international energy agency, solar heating and cooling task (IEA-SHC)", "author": ["U. Jordan", "K. Vajen"], "venue": "Universit\u00e4t Marburg, Marburg, Germany, Tech. Rep., 2001.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2001}, {"title": "LINEAR breakthrough project: Large-scale implementation of smart grid technologies in distribution grids", "author": ["B. Dupont", "P. Vingerhoets", "P. Tant", "K. Vanthournout", "W. Cardinaels", "T. De Rybel", "E. Peeters", "R. Belmans"], "venue": "Proc. 3rd IEEE PES Innov. Smart Grid Technol. Conf. (ISGT Europe), Berlin, Germany, Oct. 2012, pp. 1\u20138.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Batch mode reinforcement learning based on the synthesis of artificial trajectories", "author": ["R. Fonteneau", "S.A. Murphy", "L. Wehenkel", "D. Ernst"], "venue": "Annals of Operations Research, vol. 208, no. 1, pp. 383\u2013416, 2013.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Forecasting electricity consumption by aggregating specialized experts", "author": ["M. Devaine", "P. Gaillard", "Y. Goude", "G. Stoltz"], "venue": "Machine Learning, vol. 90, no. 2, pp. 231\u2013260, 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "THE share of renewable energy sources is expected to reach 25% of the global power generation portfolio by 2020 [1].", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "A prominent example of flexible loads are electric water heaters with a hot water storage tank [2], [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "A prominent example of flexible loads are electric water heaters with a hot water storage tank [2], [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 3, "context": "In addition to having significant flexibility, electric water heaters can consume about 2 MWh per year for a household with a daily hot water demand of 100 liters [4].", "startOffset": 163, "endOffset": 166}, {"referenceID": 4, "context": "Previously, the flexibility offered by electric water heaters has been used for frequency control [5], local voltage control [6], and energy arbitrage [7], [8].", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "Previously, the flexibility offered by electric water heaters has been used for frequency control [5], local voltage control [6], and energy arbitrage [7], [8].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "Previously, the flexibility offered by electric water heaters has been used for frequency control [5], local voltage control [6], and energy arbitrage [7], [8].", "startOffset": 151, "endOffset": 154}, {"referenceID": 7, "context": "Previously, the flexibility offered by electric water heaters has been used for frequency control [5], local voltage control [6], and energy arbitrage [7], [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 6, "context": "Perhaps the most researched control paradigm applied to demand response are model-based approaches, such as Model Predictive Control (MPC) [7], [9], [8].", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "Perhaps the most researched control paradigm applied to demand response are model-based approaches, such as Model Predictive Control (MPC) [7], [9], [8].", "startOffset": 144, "endOffset": 147}, {"referenceID": 7, "context": "Perhaps the most researched control paradigm applied to demand response are model-based approaches, such as Model Predictive Control (MPC) [7], [9], [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "Given this mathematical model, an optimal control action can be found by solving a receding horizon problem [10].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "All these steps make MPC an expensive technique, the cost of which needs to be balanced out by the possible financial gains [11].", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "Moreover, possible model errors resulting from an inaccurate model or forecast, can effect the stability of the MPC controller [12], [13].", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "Moreover, possible model errors resulting from an inaccurate model or forecast, can effect the stability of the MPC controller [12], [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 13, "context": "In contrast to MPC, Reinforcement Learning (RL) techniques [14] do not require expert knowledge and consider their environment as a black-box.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "In [15], Ernst et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Inspired by the work of [16], this paper applies an autoencoder network to reduce the dimensionality of the state ar X iv :1 51 2.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "Perhaps the most widely used model-free RL technique applied to a demand response setting is standard Q-learning [17], [18], [19], [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "Perhaps the most widely used model-free RL technique applied to a demand response setting is standard Q-learning [17], [18], [19], [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 18, "context": "Perhaps the most widely used model-free RL technique applied to a demand response setting is standard Q-learning [17], [18], [19], [20].", "startOffset": 125, "endOffset": 129}, {"referenceID": 19, "context": "Perhaps the most widely used model-free RL technique applied to a demand response setting is standard Q-learning [17], [18], [19], [20].", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "After each interaction with the environment, Q-learning uses temporal difference learning [14] to update its state-action value function or Qfunction.", "startOffset": 90, "endOffset": 94}, {"referenceID": 20, "context": "In contrast to Q-learning, batch RL techniques [21], [22], [23] are more data efficient, since they store and reuse past interactions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "In contrast to Q-learning, batch RL techniques [21], [22], [23] are more data efficient, since they store and reuse past interactions.", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": "In contrast to Q-learning, batch RL techniques [21], [22], [23] are more data efficient, since they store and reuse past interactions.", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "Perhaps the most popular batch RL technique which has been applied to a wide range of applications [16], [24], [25] is fitted Q-iteration developed by Ernst et al.", "startOffset": 99, "endOffset": 103}, {"referenceID": 23, "context": "Perhaps the most popular batch RL technique which has been applied to a wide range of applications [16], [24], [25] is fitted Q-iteration developed by Ernst et al.", "startOffset": 105, "endOffset": 109}, {"referenceID": 24, "context": "Perhaps the most popular batch RL technique which has been applied to a wide range of applications [16], [24], [25] is fitted Q-iteration developed by Ernst et al.", "startOffset": 111, "endOffset": 115}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "in [22].", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "Finally, an interesting alternative is to combine experience replay to an incremental RL technique such as Qlearning or SARSA [26].", "startOffset": 126, "endOffset": 130}, {"referenceID": 26, "context": "In [27], the authors demonstrate how fitted Q-iteration can be used to control a cluster of electric water heaters.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "In addition, [28] shows how fitted Q-iteration can be extended to reduce the cost of energy consumption of a heat-pump thermostat given that a forecast of the outside temperature is provided.", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "For example, the authors of [29] present a model-based Fig.", "startOffset": 28, "endOffset": 32}, {"referenceID": 29, "context": "In addition, inspired by [30], the authors of [31] demonstrate how a model-assisted batch RL technique can be applied to control a building heating system.", "startOffset": 25, "endOffset": 29}, {"referenceID": 30, "context": "In addition, inspired by [30], the authors of [31] demonstrate how a model-assisted batch RL technique can be applied to control a building heating system.", "startOffset": 46, "endOffset": 50}, {"referenceID": 27, "context": "Following the approach presented in [28], the electric water heater is equipped with a backup controller that overrules the control action from the agent when the safety or comfort constraints of the end user are violated.", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "To overcome this challenge, this paper leverages on the previous work of [16], [21], [28] and applies techniques from RL.", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "To overcome this challenge, this paper leverages on the previous work of [16], [21], [28] and applies techniques from RL.", "startOffset": 79, "endOffset": 83}, {"referenceID": 27, "context": "To overcome this challenge, this paper leverages on the previous work of [16], [21], [28] and applies techniques from RL.", "startOffset": 85, "endOffset": 89}, {"referenceID": 27, "context": "Following the notation introduced in [28], the next three paragraphs give a description of the state, the action, and the cost function tailored to an electric water heater.", "startOffset": 37, "endOffset": 41}, {"referenceID": 2, "context": "A detailed description of how the state of charge is calculated can by found in [3].", "startOffset": 80, "endOffset": 83}, {"referenceID": 20, "context": "Algorithm 1 Fitted Q-iteration [21] using feature vectors.", "startOffset": 31, "endOffset": 35}, {"referenceID": 31, "context": "A pre-processing step can be used to find a compact and more efficient representation of the state space and can help to converge to a good policy much faster [33].", "startOffset": 159, "endOffset": 163}, {"referenceID": 15, "context": "Alternative approaches that do not require prior knowledge are unsupervised feature learning algorithms, such as auto-encoders [16] or a principal component analysis [33].", "startOffset": 127, "endOffset": 131}, {"referenceID": 31, "context": "Alternative approaches that do not require prior knowledge are unsupervised feature learning algorithms, such as auto-encoders [16] or a principal component analysis [33].", "startOffset": 166, "endOffset": 170}, {"referenceID": 20, "context": "Following [21], Algorithm 1 applies an ensemble of extremely randomized trees as a regression algorithm to estimate the Qfunction.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "An empirical study of the accuracy and convergence properties of extremely randomized trees can be found in [21].", "startOffset": 108, "endOffset": 112}, {"referenceID": 22, "context": "However, in principle, any regression algorithm, such as neural networks [23], [24], can be used to estimate the Q-function.", "startOffset": 73, "endOffset": 77}, {"referenceID": 23, "context": "However, in principle, any regression algorithm, such as neural networks [23], [24], can be used to estimate the Q-function.", "startOffset": 79, "endOffset": 83}, {"referenceID": 32, "context": "During the day, the learning agent uses a Boltzmann exploration strategy [36] and selects an action with the following probability:", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": "A detailed description of the stratified tank model can be found in [3].", "startOffset": 68, "endOffset": 71}, {"referenceID": 33, "context": "The experiments use realistic hot water profiles with a mean daily consumption of 120 liter [37] and use price information from the Belgian day-ahead [38] and balancing market [39].", "startOffset": 92, "endOffset": 96}, {"referenceID": 34, "context": "The setup used in the lab experiment was part of a pilot project on residential demand response in Belgium [40], where 0 48 96 144 192 240 288 10 20 30 40 50 60 70", "startOffset": 107, "endOffset": 111}, {"referenceID": 33, "context": "A controllable valve connected to the outlet of the buffer tank is used to simulate the hot water demand of a household with a mean daily flow volume of 120 liter [37].", "startOffset": 163, "endOffset": 167}, {"referenceID": 35, "context": "Fitted Q-iteration is implemented in Python and Scikit-learn [41] is used to estimate the Q-function, using an ensemble of extremely randomized trees [21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "Fitted Q-iteration is implemented in Python and Scikit-learn [41] is used to estimate the Q-function, using an ensemble of extremely randomized trees [21].", "startOffset": 150, "endOffset": 154}, {"referenceID": 36, "context": "A metric based on the performance of each expert, as presented in [42], could then be used to select the expert with the highest metric as described in [43].", "startOffset": 66, "endOffset": 70}, {"referenceID": 37, "context": "A metric based on the performance of each expert, as presented in [42], could then be used to select the expert with the highest metric as described in [43].", "startOffset": 152, "endOffset": 156}], "year": 2015, "abstractText": "Electric water heaters have the ability to store energy in their water buffer without impacting the comfort of the end user. This feature makes them a prime candidate for residential demand response. However, the stochastic and nonlinear dynamics of electric water heaters, makes it challenging to harness their flexibility. Driven by this challenge, this paper formulates the underlying sequential decision-making problem as a Markov decision process and uses techniques from reinforcement learning. Specifically, we apply an auto-encoder network to find a compact feature representation of the sensor measurements, which helps to mitigate the curse of dimensionality. A wellknown batch reinforcement learning technique, fitted Q-iteration, is used to find a control policy, given this feature representation. In a simulation-based experiment using an electric water heater with 50 temperature sensors, the proposed method was able to achieve good policies much faster than when using the full state information. In a lab experiment, we apply fitted Q-iteration to an electric water heater with eight temperature sensors. Further reducing the state vector did not improve the results of fitted Q-iteration. The results of the lab experiment, spanning 40 days, indicate that compared to a thermostat controller, the presented approach was able to reduce the total cost of energy consumption of the electric water heater by 15%.", "creator": "LaTeX with hyperref package"}}}