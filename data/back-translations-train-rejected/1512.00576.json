{"id": "1512.00576", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2015", "title": "Probabilistic Latent Semantic Analysis (PLSA) untuk Klasifikasi Dokumen Teks Berbahasa Indonesia", "abstract": "One task that is included in managing documents is how to find substantial information inside. Topic modeling is a technique that has been developed to produce document representation in form of keywords. The keywords will be used in the indexing process and document retrieval as needed by users. In this research, we will discuss specifically about Probabilistic Latent Semantic Analysis (PLSA). It will cover PLSA mechanism which involves Expectation Maximization (EM) as the training algorithm, how to conduct testing, and obtain the accuracy result.", "histories": [["v1", "Wed, 2 Dec 2015 04:41:58 GMT  (356kb)", "http://arxiv.org/abs/1512.00576v1", "17 pages, 6 figures, 3 tables, Technical Report Program Studi Doktor Ilmu Komputer Universitas Indonesia"]], "COMMENTS": "17 pages, 6 figures, 3 tables, Technical Report Program Studi Doktor Ilmu Komputer Universitas Indonesia", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["derwin suhartono"], "accepted": false, "id": "1512.00576"}, "pdf": {"name": "1512.00576.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Latent Semantic Analysis (PLSA) untuk Klasifikasi Dokumen Teks Berbahasa Indonesia", "authors": ["DERWIN SUHARTONO"], "emails": [], "sections": [{"heading": null, "text": "\"It is not as if we were in a position to move in a world in which we find ourselves in a world, in which we find ourselves in a world, in which we find ourselves in a world, in which we live in a world, in which we are in a world, in which we are in a world, in which we are in a world, in which we are in a world, in which we live in a world, in which we live in a world, in which we are in a world, in which we are in a world, in which we are in a world, in which we are in a world, in which we are in a world, in which we are in a world, in which we live in a world, in a world, in which we live in a world, in a world, in a world, in which we live in a world, in a world, in which we live in a world, in which we live in a world, in a world, in which we live in a world, in which we live in a world, in which we live in a world, in which we live in a world, in which we live in a world, in which we live in a world, in which we live in a world, in which we live in a world, in which we live in which we live in a world, in which we live in which we live in a world, in which we live in a world, in which we live in which we live in which we live in a world, in which we live in a world, in which we live in which we live in a world, in which we live in which we live in a world, in which we live in a world, in which we live in a world, in which we live in which we live in which we live in a world, in which we live in a world in which we live in which we live in a world, in which we live in which we live in a world, in which we live in a world in which we live in which we live in a world, in which we live in a world in which we live in a world in which we live in which we live in a world, in which we live in a world, in which we live in a world in which we live in a world in which we live in a world, in which we live in a world in which we live in a world in which we are in a world"}, {"heading": "2.1. Probabilistic Latent Semantic Analysis", "text": "Dsa \"s.S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S\".S \".S"}, {"heading": "2.2. Classifier", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1. Support Vector Machine (SVM)", "text": "Support Vector Machine (SVM) adalah sebuah metode klasifikasi dan regresi yangmengkombinasikan algoritma komputasional dengan hasil teoretikal; kedua karakteristik ini memberikannya reputasi yang bagus dan menaikkan pamornya dalam penggunaannya di berbagai area. (Cortes and Vapnik, 1995).SVM merupakan sebuah teknik yang baru yang cok untuk binary classification task, yang terkait dan memuat elemen-elemen statistik terapan non parameterik, jaringan saraf tiruan, dan machine learning. (Auria and Moro, 2008)."}, {"heading": "2.2.2. Logistic Regression", "text": "Penggunaan logistic regression untuk memprediksi class probalah sebuahpemilihan pemodelan, sama seperti pemilihan pemodelan untuk memprediksi variable kuantitatifTechnical Report Program Studi Doctor Ilmu Computer Faculties Ilmu Computer Universitas Indonesia, Desember 2014dengan menggunakan linear regression. Logistic regression adalah salah satu dari tool yang umyang digunakan pada statistics terapan dan analisis data diskrit (Shalizi, 2012).3 MetodologiPLSA dapat diterapkan dalam berbagai berbagai seperti penilai esai otomatis, peringkas doori otomatis, dan lain sebagainjumi hituzan hitlak kakakakan."}, {"heading": "3.1. Document Preprocessing", "text": "The pre-processing of Merupakan proses pengolahan dokumen ke dalam bentuk yanglebih padat, dimana bentuk tersebut mewakili makna dari dokumen secara utuh. Keseluruhan data yang digunakan akan melewati tahap document preprocessing terlebih dahulu. 1. TokenisasiTokenisasi merupakan proses pemotongan dari bentuk kalimat menjadi kumpulan kata yang disebut sebagai token. Token akan digunakan sebagai representasi data pada tiap baris yang ada di matriks probabilitas.2. Pembuangan stop wordSebelum token yang sudah dihasilkan dari stop surdan tokenisasi digunakan pada matriks probabilitas."}, {"heading": "3.2. Proses Training Data", "text": "In this case, it is not the case that it is an \"imperfect,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"\" incomplete, \"\" \"incomplete,\" \"\" incomplete, \"\" \"incomplete,\" \"\" \"incomplete,\" \"\" \"incomplete,\" \"\" \"incomplete,\" \"\" \"incomplete,\" \"\" incomplete, \"\" etc."}, {"heading": "3.3. Proses Testing Data", "text": "It is not as if the data were data that could not be evaluated. (...) It is not as if it were data. (...) It is not as if it were data. (...) It is not as if it were data. (...) It is not as if it were data. (...) It is not as if it were data. (...) It is not as if it were data. (...) It is as if it were data. (...) It is as if it were data. (...) It is as if it were data. (...). (...) It is as if it were data. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...).). (...). (...).). (...).). (...). (...).). (...).). (...).). (...).). (...).). (...).). (...).). (...).). (...).). (...).). (...).).). (...). (...).).). (...).).). (...). (...).).). (...).). (...). (...).). (...). (...).).).). (...).).). (...). (...).).)... (...). (...). (...).).).). (...).). (...).).). (...).).). (...). (...)..).)..)..............."}, {"heading": "3.4. Pengujian", "text": "\"It's very important that we play by the rules,\" he says. \"We have to play by the rules we have imposed on ourselves.\" \"We have to play by the rules,\" he says. \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" \"We have to play by the rules.\" We have to play by the rules. \"We have to play by the rules.\""}, {"heading": "5.1. Jumlah Topik", "text": "Dari masing-masing jumlah topik pada berbagai konfigurasi yang sudah dibuat maka akan dihitung rata-rata akurasinya. Data hasil perhitungan rata-rata dari masing-masing jumlah topik adalah: - Jumlah topik 3: 47% - Jumlah topik 4: 52% - Jumlah topik 5: 52% Dengan variasi jumlah topik diperoleh bahwa akurasi tertinggi ada pada jumlah topik 4 dan 5.Gambar 4. Perbandingan Akurasi dari Variasi Jumlah Topik44% 45% 46% 47% 49% 50% 51% 52% 53% Perbandingan Akurasi dari Variasi Jumlah TopikTechnical Report Program Studi Doktor Ilmu Komputer Fakultas Ilmu Komputer Universitas Indonesia, December 2014"}, {"heading": "5.2. Jenis Classifier", "text": "The facts: - SVM: 45% - SVM: 45% - SVM: 45% - SVM: 45% - SVM: 45% - SVM: 45% - SVM: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% - SR: 0% 0 -% 0% - SR: 0% 0 -% SR: 0% 0 -% SR: 0% 0 -% SR: 0% 0 -% SR: 0 -% 0 -% SR: 0 -% SR: 0% 0 -% SR: 0 -% SR: 0 -% SR: 0 -% SR: 0 -% SR: 0 -% 0 -% SR: 0 -% SR: 0 -% SR: 0 -% 0 -% SR: 0 -% SR: 0 -% 0 -% SR: 0 -% SR: 0 -% SR: 0 -% 0 -% SR: 0 -% SR: 0 -% SR: 0 -% SR: 0 -% 0 -% SR: 0 -% SR: 0 -% SR: 0 -% 0 -% SR: 0 -% SR: 0 -% SR: 0 -% SR: 0 -% 0 -% SR: 0 -% SR: 0 -% SR: 0 -% SR: 0 -% 0 -% SR: 0 -% SR: 0 -% SR: 0 -% SR: 0 -% SR: 0 -% SR: 0 -% SR: 0 -% SR: 0 -% 0 -% SR: 0 -% SR: 0 -% SR: 0 -% SR: 0 -"}, {"heading": "5.4. Jumlah Iterasi", "text": "The data they have collected over the past few years are not new, but they are not new: \"It's not over yet,\" she says, \"but it's not over yet.\" \"It's not over yet.\" \"It's not over yet.\" \"It's not over yet.\" \"It's not over yet.\" \"It's not over yet.\" \"It's not over yet.\" \"\" It's not over yet. \"\" \"It's not over yet.\" \"\" It's not over yet. \"\" \"It's not over yet.\" \"\" \"It's not over yet.\" \"\" \".\" \"\". \"\" \".\" \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\" \".\" \".\". \"\" \".\" \".\" \"\". \"\" \"\" \".\". \"\" \"\". \".\" \"\". \"\". \"\" \".\" \".\" \"\" \"\" \"\" \".\". \"\" \"\" \".\". \".\" \"\" \".\" \".\" \".\". \"\" \"\" \"\" \"\" \".\". \".\". \"\" \"\" \".\". \"\" \".\" \".\" \"\" \"\" \".\" \"\". \".\" \"\" \"\" \".\" \"\" \"\". \"\". \".\" \"\" \".\" \"\" \"\" \"\". \"\". \"\" \"\" \"\". \"\" \"\" \".\" \"\". \".\" \"\" \"\" \".\" \"\" \"\". \"\". \"\" \".\". \"\" \"\" \"\" \"\". \"\". \"\" \"\". \"\" \"\" \".\" \"\" \".\" \".\" \"\" \"\". \"\" \"\" \".\" \"\". \"\" \"\" \".\" \"\". \"\". \"\" \".\" \".\" \"\" \"\". \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \".\". \"\". \"\" \".\" \".\" \"\". \"\" \"\" \""}], "references": [{"title": "SupportVector Machines (SVM) as a Technique for Solvency", "author": ["L. Referensi 1. Auria", "R.A. Moro"], "venue": "Proceedings of the Fifteenth conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}], "referenceMentions": [], "year": 2015, "abstractText": "Abstrak Salah satu pekerjaan yang ada di dalam mengelola dokumen adalah bagaimana menemukan intisari dari dokumen. Topic modeling merupakan teknik yang dikembangkan untuk menghasilkan representasi dokumen berupa kata-kata kunci dari dokumen. Kata-kata kunci tersebut yang akan digunakan dalam proses pengindeksan serta pencarian dokumen untuk ditemukan kembali sesuai kebutuhan pengguna. Pada penelitian ini, akan dibahas secara spesifik mengenai Probabilistic Latent Semantic Analysis (PLSA). Pembahasan akan meliputi mekanisme bagaimana PLSA yang juga melibatkan algoritma Expectation Maximization (EM) sebagai pelatihan diterapkan pada sekumpulan data korpus, serta bagaimana melakukan uji coba dan memperoleh akurasi hasil penggunaan PLSA. Keyword: topic modelling, Probabilistic Latent Semantic Analysis, Expectation Maximization", "creator": "Microsoft\u00ae Office Word 2007"}}}