{"id": "1512.00659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2015", "title": "Centroid Based Binary Tree Structured SVM for Multi Classification", "abstract": "Support Vector Machines (SVMs) were primarily designed for 2-class classification. But they have been extended for N-class classification also based on the requirement of multiclasses in the practical applications. Although N-class classification using SVM has considerable research attention, getting minimum number of classifiers at the time of training and testing is still a continuing research. We propose a new algorithm CBTS-SVM (Centroid based Binary Tree Structured SVM) which addresses this issue. In this we build a binary tree of SVM models based on the similarity of the class labels by finding their distance from the corresponding centroids at the root level. The experimental results demonstrates the comparable accuracy for CBTS with OVO with reasonable gamma and cost values. On the other hand when CBTS is compared with OVA, it gives the better accuracy with reduced training time and testing time. Furthermore CBTS is also scalable as it is able to handle the large data sets.", "histories": [["v1", "Wed, 2 Dec 2015 11:48:38 GMT  (123kb,D)", "http://arxiv.org/abs/1512.00659v1", "Presented in ICACCI, Kochi, India, 2015"]], "COMMENTS": "Presented in ICACCI, Kochi, India, 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aruna govada", "bhavul gauri", "s k sahay"], "accepted": false, "id": "1512.00659"}, "pdf": {"name": "1512.00659.pdf", "metadata": {"source": "CRF", "title": "Centroid Based Binary Tree Structured SVM for Multi Classification", "authors": ["Aruna Govada"], "emails": ["garuna@goa.bits-pilani.ac.in,", "bhavul93@gmail.com,", "ssahay@goa.bits-pilani.ac.in"], "sections": [{"heading": null, "text": "Keywords-K-Means Clustering / Centroid based clustering; SVM; Multi-Classification; Binary Tree; I. INTRODUCTIONA Support Vector Machine (SVM) is a differentiating classifier that is conventionally defined by a separating hyperplane. In other words, the algorithm produces an optimal hyperplane that classifies the invisible examples [1-2]. SVM is not a limited statistic or machine learning, but can be used in a large number of applications. SVM has proven to be the best classifier in various applications ranging from handwritten digital recognition to text categorization. SVM has no effect on classification due to the curse of dimensionality. It works well with high-dimensional data. Literature survey proves that support of vector machines are the best classifiers for 2-class classification problems."}, {"heading": "II. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Binary SVM", "text": "The learning task in the binary SVM can be presented as the following task: minw = \u0441\u0435\u0441w \u0445 22 is subject to yi (w.xi + b) \u2265 1, i = 1, 2,.... k, where w and b are the parameters of the model for the total number of instances. The Lagrange multiplier method is used to solve the following equation: Lp = \u0441i\u0441w \u0445 22 \u2212 \u2211 i = 1... k \u03bbi (yi (w.xi + b) \u2212 1)) The dual version of the above problem is isLD = \u2211 i = 1... k\u03bbi \u2212 12 \u0445 i, j \u03bbi\u03bbi\u03bbjyiyjxi.xji is subject to the prediction of the class name of a test object z (yi (w.xi + b) \u2212 1) = 0where \u03bbi are known as Lagrange multipliers. By solving this dual problem, SVM will be found. Once the SVM model is built, the class name of a test object can be predicted as the following cz (i.ib = i.i.ib) (ck = i.i.ib)."}, {"heading": "B. Multi-Class SVM", "text": "1) One against all: The simple approach is to divide the problem of classifying N classes into N binary problems, where each problem distinguishes a particular class from the other N-1 classes. [8] In this approach, we need K = N binary classifiers, where the N classifier is trained with positive examples of class N and negative examples of the other N-1 classes. If an unknown example is to be predicted, the classifier that achieves the maximum performance is considered the best choice, and the corresponding class label is assigned to that test object. Although this approach is simple [8], it offers performance comparable to other more complicated approaches if the binary classifier is well matched. 2) One against one: In this approach, each class is compared with each other class [9-10]. A binary classifier is built to distinguish between each pair of classes while discarding the rest of classes."}, {"heading": "III. THE PROPOSED APPROACH", "text": "The N-class problem is broken down into several 2-class problems in a binary tree structure. K-Means clustering is used as a pre-processing step to get a rough estimate of the similarity between the class labels, which allows us to split the class labels into two disjointed groups and form the SVM for the root node. Afterwards, each node is split in the middle to create disjointed groups. The order of class labels is calculated based on the SSE. The lowest SSE will come first on the list and the highest SSE will appear last on the list. In this way (n-1) binary SVMs are built and therefore require only (n-1) / 2 SVM evaluation to classify the unclassified dataset. This is better than the worst case (n1) of OVA and n (n-1) / 2 of OVO. At the same time, our experimental results show that accuracy is comparable to OVO."}, {"heading": "A. The Training model of CBTS SVM", "text": "Enter the training objects. Add all the training objects to the root node. Let the class labels be 1.... N pre-processing step: Divide the training objects, i.e. the root node, into two clusters / nodes (IL) and (IR) using K-middle clusters (centroid-based).1) The objects are adapted to (IL) as a positive class or (IR) as a negative class based on the majority of their class labels from the two clusters.2) Because (IL) and (IR) calculate the SSE of all objects based on the same class labels and sort them in ascending order. The SSE results from SSE = \u2211 i = 1... k (xi, C) 3) For both (IL) and (IR) Repeata) If the number of class labels of the node is two, we construct the binary classification and return it."}, {"heading": "B. The Testing model of CBTS SVM", "text": "1) The test object should be evaluated on the root node of the binary tree of SVMs.2). Repeat: \u2022 If the value is positive transverse to the left node (IL), otherwise to the right node (IR). \u2022 Until we reach the leaf node.3) Classify the test object into the class name of the leaf node. Let us go through an example. If we look at Figure 1, suppose we have 8 classes, we first perform k-mean clustering with k = 2 to divide the data objects according to their distribution. Then, through the cluster distribution and based on the majority, we learn which class labels fall on one side (positive) and which on the other side (negative). In the example shown, the set IL {1,3,7} belongs to the positive class and the rest of the class labels, IR {4,6,8,2,5} belong to the negative class. The order of class labels within the node will be in the ascending order of SSE."}, {"heading": "IV. EXPERIMENTAL ANALYSIS", "text": "We implemented the algorithms on twelve classification datasets and compared the training time, test time and accuracy with OVO and OVA 2. Although the time for collecting data with OVA and OVA is limited, the time for collecting data is too short. Detailed characteristics of the datasets are given in Table I. For letters, shuttle and satimage datasets, a separate training and test file was used, but for the rest of all datasets 2 / 3 part of the data is considered for testing. All datasets were scaled to [0,1] and are randomized (similar class identifiers will not appear together)."}, {"heading": "V. CONCLUSION", "text": "We propose a new algorithm CBTS (Centroid based Binary Tree Structured SVM), a binary tree structure in which the root node contains all the class names and is divided in the first step on the basis of K-means clustering, and then the left and right nodes are cut in half recursively until we have only two class names left. In this N1 SVMs must be constructed, but the required SVMs in OVO, OVA are N (N-1) / 2 or N. The test time for all OVO, OVA, DAG is linear to N, i.e. O (N), but for our algorithm it is O (log N), as it only needs (N1) / 2 classifiers to predict the class name for the test object. Experimental results show that the accuracy of CBTS is comparable to the OVO approach, and it performs with OVA both in accuracy and in training from CTS, with CTS being able to CTS scales at the same time, with the help of CTS being able to work in large sets of data sets."}, {"heading": "ACKNOWLEDGMENT", "text": "We are grateful for the resources provided by the Faculty of Computer Science and Information Systems, BITS, Pilani, K.K. Birla Goa Campus to carry out the experimental analysis."}], "references": [{"title": "Support Vector network. Machine Learning", "author": ["C.Cortes", "V. Vapnik"], "venue": "Volume 20, Issue", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "The Nature of Statistical Learning Theory", "author": ["V. Vapnik"], "venue": "NY: Springer-Verlag", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "A comparison of Methods for Multi Class Support Vector Machines", "author": ["Chih-Wei Hsu", "Chih-jen Lin"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Analysis of Multi Support Vector Machines", "author": ["Abe", "Shigeo"], "venue": "Proc. International Conference on Computational Intelligence for Modelling Control and Automation (CIMCA2003),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Classification by pairwise coupling", "author": ["Trevor Hastie", "Robert Tibshirani"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["Erin Allwein", "Robert Shapire", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Comparison of classifier methods : a case study in handwriting digit recognition", "author": ["L.Bottou", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "L. Jackel", "Y. LeCun", "U. Muller", "E. Sackinger", "P. Simard", "V. Vapnik"], "venue": "In International Conference on Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Single-layer learning revisited: a stepwise procedure for building and training a neural network. Neurocomputing: Algorithms", "author": ["S. Knerr", "L. Personnaz", "G. Dreyfus"], "venue": "Architectures and Applications. Springer-Verlag,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}, {"title": "Pairwise classification and support vector machines", "author": ["U. Krebel"], "venue": "Advances in Kernel Methods Support Vector learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "LIBSVM : a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "API design for machine learning software: experiences from the scikit-learn project", "author": ["Buitinck"], "venue": "Available at http : //scikit \u2212 learn.org/stable/autoexamples/svm/plotrbfparameters", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which classifies the unseen examples[1-2].", "startOffset": 147, "endOffset": 152}, {"referenceID": 1, "context": "In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which classifies the unseen examples[1-2].", "startOffset": 147, "endOffset": 152}, {"referenceID": 2, "context": "There are several algorithms based on the second approach like OVO(one-versus-one),OVA(one-versusall), DAG(Directed Acyclic Graph)[6-7].", "startOffset": 130, "endOffset": 135}, {"referenceID": 3, "context": "There are several algorithms based on the second approach like OVO(one-versus-one),OVA(one-versusall), DAG(Directed Acyclic Graph)[6-7].", "startOffset": 130, "endOffset": 135}, {"referenceID": 4, "context": "2) One versus One: In this approach, each class is compared to every other class [9-10].", "startOffset": 81, "endOffset": 87}, {"referenceID": 2, "context": "Results [6,11]show that this approach is in general better than the one-versus-all approach.", "startOffset": 8, "endOffset": 14}, {"referenceID": 5, "context": "Results [6,11]show that this approach is in general better than the one-versus-all approach.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "All the datasets were scaled to [0,1] and have been randomized (similar class labels data will not appear together).", "startOffset": 32, "endOffset": 37}, {"referenceID": 9, "context": "In all the three approaches OVO, OVA, CBTS the best combination of \u03b3 and C is chosen from the above mentioned range so as to provide the higher accuracy and lower testing and training time [17].", "startOffset": 189, "endOffset": 193}, {"referenceID": 9, "context": "60GHz Intel i5-4200U Dual core processor and RAM of 6GB and using the software LIBSVM [17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "If \u03b3 is too large there is a possibility that model will become over fit the data [18].", "startOffset": 82, "endOffset": 86}], "year": 2015, "abstractText": "Support Vector Machines (SVMs) were primarily designed for 2-class classification. But they have been extended for N-class classification also based on the requirement of multiclasses in the practical applications. Although N-class classification using SVM has considerable research attention, getting minimum number of classifiers at the time of training and testing is still a continuing research. We propose a new algorithm CBTS-SVM (Centroid based Binary Tree Structured SVM) which addresses this issue. In this we build a binary tree of SVM models based on the similarity of the class labels by finding their distance from the corresponding centroids at the root level. The experimental results demonstrates the comparable accuracy for CBTS with OVO with reasonable gamma and cost values. On the other hand when CBTS is compared with OVA, it gives the better accuracy with reduced training time and testing time. Furthermore CBTS is also scalable as it is able to handle the large data sets. Keywords-K-Means Clustering / Centroid based clustering; SVM; Multi-Classification; Binary Tree;", "creator": "LaTeX with hyperref package"}}}