{"id": "1512.00765", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2015", "title": "Learning Semantic Similarity for Very Short Texts", "abstract": "Levering data on social media, such as Twitter and Facebook, requires information retrieval algorithms to become able to relate very short text fragments to each other. Traditional text similarity methods such as tf-idf cosine-similarity, based on word overlap, mostly fail to produce good results in this case, since word overlap is little or non-existent. Recently, distributed word representations, or word embeddings, have been shown to successfully allow words to match on the semantic level. In order to pair short text fragments - as a concatenation of separate words - an adequate distributed sentence representation is needed, in existing literature often obtained by naively combining the individual word representations. We therefore investigated several text representations as a combination of word embeddings in the context of semantic pair matching. This paper investigates the effectiveness of several such naive techniques, as well as traditional tf-idf similarity, for fragments of different lengths. Our main contribution is a first step towards a hybrid method that combines the strength of dense distributed representations - as opposed to sparse term matching - with the strength of tf-idf based methods to automatically reduce the impact of less informative terms. Our new approach outperforms the existing techniques in a toy experimental set-up, leading to the conclusion that the combination of word embeddings and tf-idf information might lead to a better model for semantic content within very short text fragments.", "histories": [["v1", "Wed, 2 Dec 2015 16:31:20 GMT  (1564kb,D)", "http://arxiv.org/abs/1512.00765v1", "6 pages, 5 figures, 3 tables, ReLSD workshop at ICDM 15"]], "COMMENTS": "6 pages, 5 figures, 3 tables, ReLSD workshop at ICDM 15", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["cedric de boom", "steven van canneyt", "steven bohez", "thomas demeester", "bart dhoedt"], "accepted": false, "id": "1512.00765"}, "pdf": {"name": "1512.00765.pdf", "metadata": {"source": "CRF", "title": "Learning Semantic Similarity for Very Short Texts", "authors": ["Cedric De Boom", "Steven Van Canneyt", "Steven Bohez", "Thomas Demeester", "Bart Dhoedt"], "emails": ["bart.dhoedt}@ugent.be"], "sections": [{"heading": null, "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city, in which it is a country, in which it is a country and in which it is a country, in which it is a country and in which it is a country, in which it is a country, in which it is a country, in which it is a city, in which it is a country and in which it is a country."}, {"heading": "II. EXPERIMENTAL SET-UP AND ANALYSIS", "text": "It's not just a matter of time until it will come to an end, but also of time until it will come to an end. It's a matter of time until it will come to an end. It's a matter of time until it will come to an end. It's a matter of time until it will come to an end. It's a matter of time until it will come to an end. It's a matter of time until it will come to an end. It's a matter of time until it will come to an end. It's a matter of time until it will come to an end. \""}, {"heading": "III. LEARNING SEMANTIC SIMILARITY", "text": "As was clear from the data analysis, combining knowledge from both tf-idf and word embedding into 1.5 million pairs is beneficial. If we only use the part with the highest idf component of all words, this significantly reduces the splintering error and improves the divergence of JS. Finally, low idf words do not have a clear semantic meaning, and since these words are present in many sentences, there is more random overlap between unrelated sentences. Removing these words - or reducing their impact - from a text representation succeeds in distinguishing the average similarity between pairs and between non-pairs. In this section, we will examine how we can learn to optimally weigh words in a short text. In this way, we want to make the top idf words or weighing these words with their idf component better, in order to maximize the average distance between pairs and non-pairs."}, {"heading": "IV. DISCUSSION AND CONCLUSION", "text": "We also learned how to optimally combine knowledge from tf-idf and word embedding techniques to maximize the separation between pairs and non-pairs. The best functioning traditional technique is a concatenation of maximum and minimum vectors, and for a large number of words tf-idf leads to comparable results. As this is not yet complete, however, some remarks need to be made. First, our experimental setup is close to a toy setting. Wikipedia is a completely different textual medium than a social platform like Twitter, where the language used is full of slang, hashtags and spelling errors."}, {"heading": "V. ACKNOWLEDGMENTS", "text": "Cedric De Boom is funded by a Ph.D. scholarship from the University of Ghent, the Collaborative Research Fund (BOF) and the Flanders Research Foundation (FWO). Steven Van Canneyt and Steven Bohez are supported by a Ph.D. scholarship from the Agency for Innovation by Science and Technology in Flanders (IWT)."}], "references": [{"title": "The Evaluation of Sentence Similarity Measures", "author": ["P. Achananuparp", "X. Hu", "X. Shen"], "venue": "DaWaK 2008: International Conference on Data Warehousing and Knowledge Discovery, Jul. 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "Proceedings of NAACL HLT, Apr. 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "NIPS 2013: Advances in neural information processing systems, Oct. 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Proceedings of Workshop at ICLR, Jan. 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Alleviating Manual Feature Engineering for Part-of-Speech Tagging of Twitter Microposts using Distributed Word Representations", "author": ["F. Godin", "B. Vandersmissen", "A. Jalalvand", "W. De Neve", "R. Van de Walle"], "venue": "Workshop on Modern Machine Learning and Natural Language Processing, NIPS 2014, Oct. 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional Neural Network Architectures for Matching Natural Language Sentences", "author": ["B. Hu", "Z. Lu", "H. Li", "Q. Chen"], "venue": "NIPS 2014: Advances in Neural Information Processing Systems, 2014, pp. 2042\u20132050.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "TagSpace: Semantic embeddings from hashtags", "author": ["J. Weston", "S. Chopra", "K. Adams"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts", "author": ["C.N. dos Santos", "M. Gatti"], "venue": "COLING 2014, the 25th International Conference on Computational Linguistics, Dublin, Jul. 2014, pp. 69\u201378.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, vol. 12, Feb. 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "A Short Texts Matching Method using Shallow Features and Deep Features", "author": ["L. Kang", "B. Hu", "X. Wu", "Q. Chen", "Y. He"], "venue": "Third CCF Conference, NLPCC 2014, Nov. 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Text Understanding from Scratch", "author": ["X. Zhang", "Y. Lecun"], "venue": "arXiv.org, Feb. 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "arXiv.org, May 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to Reweight Terms with Distributed Representations", "author": ["G. Zheng", "J. Callan"], "venue": "the 38th International ACM SIGIR Conference. New York, New York, USA: ACM Press, 2015, pp. 575\u2013584.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "an example of a traditional and very popular representation to compare texts, such as news articles, with each other [1], [2].", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "published three papers on the topic of distributed word embeddings to catch semantic similarities between words [3], [4], [5], which resulted in Google\u2019s word2vec software.", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "published three papers on the topic of distributed word embeddings to catch semantic similarities between words [3], [4], [5], which resulted in Google\u2019s word2vec software.", "startOffset": 117, "endOffset": 120}, {"referenceID": 3, "context": "published three papers on the topic of distributed word embeddings to catch semantic similarities between words [3], [4], [5], which resulted in Google\u2019s word2vec software.", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "extensively used such embeddings to improve state-of-theart algorithms in natural language processing, such as partof-speech tagging [6], sentence completion [7], hashtag prediction [8], etc.", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "extensively used such embeddings to improve state-of-theart algorithms in natural language processing, such as partof-speech tagging [6], sentence completion [7], hashtag prediction [8], etc.", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "extensively used such embeddings to improve state-of-theart algorithms in natural language processing, such as partof-speech tagging [6], sentence completion [7], hashtag prediction [8], etc.", "startOffset": 182, "endOffset": 185}, {"referenceID": 6, "context": "Many authors choose to average or maximize across the embeddings in a text [8], [9], [10] or combine them through a multi-layer perceptron [6], [11], by clustering [12], or by trimming the text to a fixed length [11].", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "Many authors choose to average or maximize across the embeddings in a text [8], [9], [10] or combine them through a multi-layer perceptron [6], [11], by clustering [12], or by trimming the text to a fixed length [11].", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "Many authors choose to average or maximize across the embeddings in a text [8], [9], [10] or combine them through a multi-layer perceptron [6], [11], by clustering [12], or by trimming the text to a fixed length [11].", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "Many authors choose to average or maximize across the embeddings in a text [8], [9], [10] or combine them through a multi-layer perceptron [6], [11], by clustering [12], or by trimming the text to a fixed length [11].", "startOffset": 139, "endOffset": 142}, {"referenceID": 9, "context": "Many authors choose to average or maximize across the embeddings in a text [8], [9], [10] or combine them through a multi-layer perceptron [6], [11], by clustering [12], or by trimming the text to a fixed length [11].", "startOffset": 144, "endOffset": 148}, {"referenceID": 10, "context": "Many authors choose to average or maximize across the embeddings in a text [8], [9], [10] or combine them through a multi-layer perceptron [6], [11], by clustering [12], or by trimming the text to a fixed length [11].", "startOffset": 164, "endOffset": 168}, {"referenceID": 9, "context": "Many authors choose to average or maximize across the embeddings in a text [8], [9], [10] or combine them through a multi-layer perceptron [6], [11], by clustering [12], or by trimming the text to a fixed length [11].", "startOffset": 212, "endOffset": 216}, {"referenceID": 11, "context": "The Paragraph Vector algorithm by Le and Mikolov\u2014 also termed paragraph2vec\u2014is a powerful method to find suitable vector representations for sentences, paragraphs and documents of variable length [13].", "startOffset": 196, "endOffset": 200}, {"referenceID": 12, "context": "based on linear regression to find relevant terms in a query [15].", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "to extract pairs and non-pairs from the Reuters corpus [7].", "startOffset": 55, "endOffset": 58}], "year": 2015, "abstractText": "Levering data on social media, such as Twitter and Facebook, requires information retrieval algorithms to become able to relate very short text fragments to each other. Traditional text similarity methods such as tf-idf cosinesimilarity, based on word overlap, mostly fail to produce good results in this case, since word overlap is little or nonexistent. Recently, distributed word representations, or word embeddings, have been shown to successfully allow words to match on the semantic level. In order to pair short text fragments\u2014as a concatenation of separate words\u2014an adequate distributed sentence representation is needed, in existing literature often obtained by naively combining the individual word representations. We therefore investigated several text representations as a combination of word embeddings in the context of semantic pair matching. This paper investigates the effectiveness of several such naive techniques, as well as traditional tf-idf similarity, for fragments of different lengths. Our main contribution is a first step towards a hybrid method that combines the strength of dense distributed representations\u2014 as opposed to sparse term matching\u2014with the strength of tf-idf based methods to automatically reduce the impact of less informative terms. Our new approach outperforms the existing techniques in a toy experimental set-up, leading to the conclusion that the combination of word embeddings and tf-idf information might lead to a better model for semantic content within very short text fragments.", "creator": "LaTeX with hyperref package"}}}