{"id": "1512.00965", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2015", "title": "Neural Enquirer: Learning to Query Tables with Natural Language", "abstract": "We proposed Neural Enquirer as a neural network architecture to execute a SQL-like query on a knowledge-base (KB) for answers. Basically, Neural Enquirer finds the distributed representation of a query and then executes it on knowledge-base tables to obtain the answer as one of the values in the tables. Unlike similar efforts in end-to-end training of semantic parser, Neural Enquirer is fully neuralized: it not only gives distributional representation of the query and the knowledge-base, but also realizes the execution of compositional queries as a series of differentiable operations, with intermediate results (consisting of annotations of the tables at different levels) saved on multiple layers of memory. Neural Enquirer can be trained with gradient descent, with which not only the parameters of the controlling components and semantic parsing component, but also the embeddings of the tables and query words can be learned from scratch. The training can be done in an end-to-end fashion, but it can take stronger guidance, e.g., the step-by-step supervision for complicated queries, and benefit from it. Neural Enquirer is one step towards building neural network systems which seek to understand language by executing it on real-world. Our experiments show that Neural Enquirer can learn to execute fairly complicated queries on tables with rich structures.", "histories": [["v1", "Thu, 3 Dec 2015 06:46:27 GMT  (1326kb,D)", "http://arxiv.org/abs/1512.00965v1", null], ["v2", "Thu, 21 Jan 2016 02:46:25 GMT  (1720kb,D)", "http://arxiv.org/abs/1512.00965v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG cs.NE", "authors": ["pengcheng yin", "zhengdong lu", "hang li", "ben kao"], "accepted": false, "id": "1512.00965"}, "pdf": {"name": "1512.00965.pdf", "metadata": {"source": "CRF", "title": "Neural Enquirer: Learning to Query Tables", "authors": ["Pengcheng Yin", "Zhengdong Lu", "Hang Li", "Ben Kao"], "emails": ["kao}@cs.hku.hk", "HangLi.HL}@huawei.com"], "sections": [{"heading": "1 Introduction", "text": "The traditional pipeline is to put the query through a semantic parser to get some \"executable\" representations, typically logical forms, and then apply this representation to a knowledge base for the answer. However, in order to partially overcome this difficulty, there have been efforts [11] to make the result of the query execution quite chaotic, in order to revise the semantic representation of the query that actually falls into the working process with handmade features or rules. In part, to overcome this difficulty, there have been efforts to revise the result of the query execution."}, {"heading": "2 Overview of Neural Enquirer", "text": "Faced with a Q query and a T KB table, Neural Enquirer executes the query against the table and issues a ranking of query responses, first using encoders to encode the query and table into distributed representations, which are then sent to a cascaded pipeline of executors to derive the response. Figure 1 provides a graphic example (with five executors) of different types of components involved: Query Encoder (Section 3.1), which encodes the query into a distributed representation that contains the semantic information of the original query. Encrypted embedding of the query is sent to different executors to calculate its execution result. Table Encoder (Section 3.2) encodes entries in the table into distributed vectors. Table Encoder prints an embedding vector for each table entry that maintains the two-dimensional structure of the execution."}, {"heading": "3 Model", "text": "In this section we give a more detailed description of the different types of components in the Neural Enquirer model."}, {"heading": "3.1 Query Encoder", "text": "In this case, it is a kind of emancipatory, emancipatory, emancipatory, emancipatory, emancipatory, emancipatory, emancipatory, emancipatory, emancipatory, emancipatory and emancipatory emancipation."}, {"heading": "3.3 Executor", "text": "A query is executed step by step by a sequence of stacked executors, such a cascaded architecture allows neurons to answer complex compositional questions. As shown in Figure 2, an executor member in Figure 2 has an execution memory in sequence of execution operations. (Editor's note: These are two important neural network components: reader and annotator.) The executor processes a table in sequence of rows in which N (field, value) are composed, composed of embeddingsRm = {em1, emN}, emN}, reader and annotator."}, {"heading": "3.3.1 Reader", "text": "As shown in Figure 3, an executor reads in Layer- 'a vector r'm for each line m, which is defined as the weighted sum of compound embeddings for entries in this line: r' m = f'r (Rm, FT, q, M '\u2212 1) = N \u2211 n = 1 \u03c9 (fn, q, g' \u2212 1) emnwhere \u03c9 (\u00b7) are the normalized attention weights given by: \u03c9 (fn, q, g '\u2212 1) = exp (\u03c9 (fn, q, g' \u2212 1)) \u2211 Nn \u2032 = 1 exp (\u03c9 (fn \u2032, q, g '\u2212 1) (3) and \u03c9 (\u00b7) is modeled as DNN (') 1."}, {"heading": "3.3.2 Annotator", "text": "In Executor- \"the annotator computes annotations on rows and tables based on the reader's read vector r'm, which is then stored in the\" -th memory layer M, \"which is accessible to executor (\" + 1).This process repeats in intermediate layers until the executor in the last layer finally generates the report.Line annotations A line annotation encodes the local calculation result in a particular line. As shown in Figure 4, a line annotation for line m is stored in Executor-, \"given by a'm = f'a (r'm, q, M '\u2212 1) = DNN ('r'm; q; a' \u2212 1 m; g '\u2212 1]). (4) saves the corresponding reader vector r'm, the results stored in the previous memory layer (row and table annotations)."}, {"heading": "3.3.3 Last Layer of Executor", "text": "Instead of calculating annotations based on read vectors, the last executor in Neural Enquirer directly prints the probability that the value of each entry in T is the answer: p (wmn | Q, T) = exp (f'Ans (emn, q, a '\u2212 1 m, g' \u2212 1)) \u2211 M m \u2032 = 1 \u2211 N \u2032 = 1 exp (f'Ans (em'n \u2032, q, a '\u2212 1 m \u2032, g' \u2212 1)) (6), where f'ans (\u00b7) is modeled as DNN. Note that the last executor dedicated to returning answers performs a certain type of execution with f'Ans (\u00b7) based on the input value, query, and annotation from the previous level."}, {"heading": "3.4 Handling Multiple Tables", "text": "In fact, it is a matter of a way in which it is a matter of answering questions that relate to answering questions. (...) It is about answering questions that relate to answering questions. (...) It is about answering questions that relate to answering questions. (...) It is about answering questions that relate to answering questions that relate to answering questions. (...) It is about answering questions that relate to answering questions that relate to answering questions. (...) It is about answering questions that relate to answering questions that are asked to answering questions. (...) It is about answering questions that relate to answering questions that relate to answering questions. (...) It is about answering questions that relate to answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering questions from answering from answering questions from answering questions from answering questions from answering questions from answering from answering from answering from answering from answering questions from answering from answering from answering questions from answering from answering from answering from answering from answering questions from answering from answering from answering from answering from answering questions from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from questions from answering from answering from answering from answering from answering from questions from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from answering from ans"}, {"heading": "4 Learning", "text": "During the training, both the representations of the queries and table inputs as well as the execution logic captured by the weights of the performers are learned. Specifically, we optimize the model parameters by maximizing the log probability of gold standard answers: LN2N (D) = ND \u2211 i = 1 log p (y (i) = wmn | Q (i), T (i)))))) (7) In end-to-end training, each performer discovers his or her operational logic from training data in a purely data-driven manner that could be difficult for complicated queries requiring four or five sequential operations. < This can be mitigated by gently guiding the learning process by controlling the weights w (\u00b7) in a purely data-driven manner required to perform operations."}, {"heading": "5 Experiments", "text": "In this section, we evaluate Neural Enquirer about synthetic QA tasks with queries of varying compositional depth. We will first briefly describe our synthetic QA task for benchmark and experimental setup, and then discuss the results under different settings."}, {"heading": "5.1 Synthetic QA Task", "text": "We present a synthetic QA task to evaluate the performance of Neural Enquirer, where a large number of QA examples are generated at different levels of complexity to evaluate the individual tables and multiple tables cases of the model. Our data have the same complexity as a real world, but we manipulate them (e.g., redeployment of input values) to generate enoughtraining instances. Our synthetic data set consists of query table response triples {(Q (i), T (i), y (i)}. To generate such a triple example of research on modeling symbolic calculations using DNNs [6, 15] we first randomly stamp a table T (i) of size 10 x 10 from a synthetic scheme of the Olympic Games, the 10 fields city and 60 values for QS (i), 6 values for continents we generate a triple series, which we combine with a series of Qs (Qi)."}, {"heading": "5.2 Setup", "text": "We use the same configuration for all test cases. All DNNs in the Neural Enquirer are instantiated with a hidden level, with the exception of the last executive, which has two hidden levels. We set the dimensionality of word / entity embedding and line / table annotations to 20, hidden levels to 50, and the hidden states of the GRU in the query encoder to 100. \u03b1 in Equation (8) is set to 0.5. We set the beginning of all input queries to a fixed size. Neural Enquirer is trained using standard back propagation. Objective functions are optimized using SGD in a mini-batch of size 100 with adaptive learning rate (AdaDelta [16]). The model converges quickly within 200 epochs. Unless otherwise noted, the results reported in Section 5.3 are obtained using an end-to-end training setting (N2N)."}, {"heading": "5.3 Main Results", "text": "To simulate the read-world scenario, in which queries of different types are output to the model, we construct a mixed dataset with 110K examples, consisting of 60K simple queries (Select Where, Superlative and Where Superlative), with 20K for each type, and 50K complex Nest queries. We trained a Neural Enquirer model with five performers who use the power for each type of execution (N2N), with the results summarized in the first two rows of Table 24. We also break down the overall accuracy (85.2%) on this dataset to study the performance for each type of execution. Neural Enquirer is very effective in answering simple queries such as Select Where, Superlative and Where Superlative, whose accuracy exceeds 96%. Nest queries, which are much more complicated, also register a reasonable performance of 68%. These results indicate that our proposed model is very effective at responding to complex queries."}, {"heading": "5.4 With Additional Step-by-Step Supervision", "text": "In order to alleviate the problem of the disappearing gradient in training on complex questions as described in Section 5.3, in our next experiments we have trained our Neural Enquirer model using step-by-step training (SbS) (Eq. 8), in which we encourage each performer to turn to a specific field known a priori to be relevant to its execution logic. Results are shown in the last two lines of Table 2. With a stronger monitoring signal, the model significantly outperforms the results in end-to-end setting and achieves nearly 100% accuracy in all types of queries, showing that our proposed Neural Enquirer is able to use the additional monitoring signal given in the SbS training setting intermediate layers, and to answer complex and compositional queries with perfect accuracy. Let us perform the query Q2 in the SbS setting with the weights visualization in Figure 12. (In contrast to the two execution layers in each case, the result NN or the result of the two)."}, {"heading": "5.5 Dealing with Out-Of-Vocabulary Words", "text": "One of the biggest challenges in applying neural network models to NLP applications is dealing with words from the vocabulary (OOV) that are particularly serious for QA. It is difficult to cover existing entities while new entities appear simultaneously in custom queries and in back-end KB everyday life. Interestingly, we note that a simple variation of Neural Enquirer is able to handle invisible entities with almost no loss of accuracy. Basically, we divide the words in the vocabulary into operating words and entity words. Operating words contain all numbers (e.g. 90) and operator names (e.g. Select), the embedding of which have semantic significance for execution and should be optimized during training; while embedding entity words (e.g. Beijing, China) works in a way that facilitates comparison between entities in queries and tables."}, {"heading": "5.6 Multiple Tables Queries", "text": "In our final set of experiments, we present preliminary results for Neural EnquirerM, which we evaluated on Select Where Queries. We evaluated a data set of 100K Select Where Queries on two tables, about half of which must be merged (referred to as \"join\") in order to derive answers from the two tables. We tested on a model with three executors. Table 4 lists the results. The accuracy of Join queries is lower than that of non-join queries caused by additional interaction between the two tables involved in answering Join Queries. We find that Neural Enquirer-M is able to determine that the country field is the foreign key that connects the two tables. Figure 13 illustrates the attention weights for a correctly answered Join Query Q4. Although the query contains no clues for the foreign key (country field), the foreign key is the one that connects the two country keys in the M field."}, {"heading": "6 Related Work", "text": "In this context, it is worth mentioning that this project is a project, which is a project, which is primarily a project, which is about putting people's needs at the centre."}, {"heading": "7 Conclusion and Future Work", "text": "In this paper, we propose Neural Enquirer, a fully neural, consistently differentiated network that learns to execute queries on tables. We present results on a series of synthetic QA tasks to demonstrate Neural Enquirer's ability to answer relatively complicated compositional queries across multiple tables. In the future, we plan to push this work in the following directions: First, we apply Neural Enquirer to natural language questions and natural language answers, where both the input query and output monitoring are noisier and less informative; second, we scale the QA task to the real world as in [11], for which we need to deal with a large vocabulary and novel predicates; third, we will work on the question of computing efficiency in executing queries by heavily borrowing the symbolic operation."}, {"heading": "A Computation of GRU", "text": "Given a sequence of word embedding in Q: {x1, x2,.., xT}, the GRU calculates the hidden state ht as follows: ht = ztht \u2212 1 + (1 \u2212 zt) h-t = tanh (Wxt + U (rt \u0441ht \u2212 1) zt = \u03c3 (Wzxt + Uzht \u2212 1) rt = \u03c3 (Wrxt + Urht \u2212 1), where W, Wz, Wr, U, Uz, Ur are parametric matrices, 1 the column vector of all ones and \u0435 the elemental multiplication. We use the last hidden state, hT as vector representation of the query, i.e. q = hT."}, {"heading": "B Performance on Separate Datasets", "text": "We also examined the separate performance of Neural Enquirer in answering different types of queries on different dataset sizes. Table 5 lists the results. This time, for each type of query, we used a model with an optimized number of performers as indicated in the table. Neural Enquirer achieves 100% accuracy for simple queries when the size of training data increases to 50K. It is also worth noting that for Select Where and Superlative queries, which require only minimal execution steps, their accuracy is already very close to 100% when using only 5K training examples. Also, when comparing the results obtained here with their mixed-setting counterparts (second row in Table 2), we can find an interesting fact that the numbers reported here are actually lower, although the model is matched with an optimal number of performers for a particular type of query."}], "references": [{"title": "Broad-coverage ccg semantic parsing with amr", "author": ["Y. Artzi", "K. Lee", "L. Zettlemoyer"], "venue": "EMNLP, pages 1699\u20131710,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": "EMNLP, pages 1533\u20131544,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garca-Durn", "J. Weston", "O. Yakhnenko"], "venue": "NIPS, pages 2787\u20132795,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to sportscast: a test of grounded language acquisition", "author": ["D.L. Chen", "R.J. Mooney"], "venue": "ICML, pages 128\u2013135,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "CoRR, abs/1410.5401,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["T. Kwiatkowski", "E. Choi", "Y. Artzi", "L.S. Zettlemoyer"], "venue": "EMNLP, pages 1545\u20131556,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Environment-driven lexicon induction for high-level instructions", "author": ["D.K. Misra", "K. Tao", "P. Liang", "A. Saxena"], "venue": "ACL (1), pages 992\u20131002,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Programmer: Inducing Latent Programs with Gradient Descent", "author": ["A. Neelakantan", "Q.V. Le", "I. Sutskever"], "venue": "ArXiv e-prints, Nov.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Zero-shot entity extraction from web pages", "author": ["P. Pasupat", "P. Liang"], "venue": "ACL (1), pages 391\u2013401,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["P. Pasupat", "P. Liang"], "venue": "ACL (1), pages 1470\u20131480,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["T.-H. Wen", "M. Gasic", "N. Mrksic", "P. hao Su", "D. Vandyke", "S.J. Young"], "venue": "In EMNLP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "CoRR, abs/1502.05698,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "CoRR, abs/1410.4615,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "CoRR, abs/1212.5701,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "UAI, pages 658\u2013666,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Online learning of relaxed ccg grammars for parsing to logical form", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "EMNLP-CoNLL, pages 678\u2013687,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 10, "context": "Unlike similar efforts in endto-end training of semantic parser [11, 9], Neural Enquirer is fully \u201cneuralized\u201d: it not only gives distributional representation of the query and the knowledge-base, but also realizes the execution of compositional queries as a series of differentiable operations, with intermediate results (consisting of annotations of the tables at different levels) saved on multiple layers of memory.", "startOffset": 64, "endOffset": 71}, {"referenceID": 8, "context": "Unlike similar efforts in endto-end training of semantic parser [11, 9], Neural Enquirer is fully \u201cneuralized\u201d: it not only gives distributional representation of the query and the knowledge-base, but also realizes the execution of compositional queries as a series of differentiable operations, with intermediate results (consisting of annotations of the tables at different levels) saved on multiple layers of memory.", "startOffset": 64, "endOffset": 71}, {"referenceID": 11, "context": "1 Introduction In models for natural language dialogue and question answering, there is ubiquitous need for querying a knowledge-base [13, 11].", "startOffset": 134, "endOffset": 142}, {"referenceID": 10, "context": "1 Introduction In models for natural language dialogue and question answering, there is ubiquitous need for querying a knowledge-base [13, 11].", "startOffset": 134, "endOffset": 142}, {"referenceID": 10, "context": "Partially to overcome this difficulty, there has been effort [11] to \u201cbackpropagate\u201d the result of query execution to revise the semantic representation of the query, which actually falls into the thread of work on learning from grounding [5].", "startOffset": 61, "endOffset": 65}, {"referenceID": 4, "context": "Partially to overcome this difficulty, there has been effort [11] to \u201cbackpropagate\u201d the result of query execution to revise the semantic representation of the query, which actually falls into the thread of work on learning from grounding [5].", "startOffset": 239, "endOffset": 242}, {"referenceID": 13, "context": "The recent work on learning to execute simple Python code with LSTM [15] pioneers in the direction on learning to parse structured objects through executing it in a purely neural way, while the later work on Nerual Turing Machine (NTM) [6] introduces more modeling flexibility by equipping the LSTM with external memory and various means of interacting with it.", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": "The recent work on learning to execute simple Python code with LSTM [15] pioneers in the direction on learning to parse structured objects through executing it in a purely neural way, while the later work on Nerual Turing Machine (NTM) [6] introduces more modeling flexibility by equipping the LSTM with external memory and various means of interacting with it.", "startOffset": 236, "endOffset": 239}, {"referenceID": 3, "context": ", TransE [4]), where embeddings of entities (entry values) and relations (field names) are learned in a unsupervised fashion via minimizing certain reconstruction errors.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "Our design of executor is inspired by Neural Turing Machines [6], where data is fetched from an external memory using a read head, and subsequently processed by a controller, whose outputs are flushed back in to memories.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "This is related to the content-based addressing of Neural Turing Machines [6] and the attention mechanism in neural machine translation models [2].", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "This is related to the content-based addressing of Neural Turing Machines [6] and the attention mechanism in neural machine translation models [2].", "startOffset": 143, "endOffset": 146}, {"referenceID": 12, "context": "Starting with \u201cartificial\u201d tasks eases the process of developing novel deep models [14], and has gained increasing popularity in recent advances of the research on modeling symbolic computation using DNNs [6, 15].", "startOffset": 83, "endOffset": 87}, {"referenceID": 5, "context": "Starting with \u201cartificial\u201d tasks eases the process of developing novel deep models [14], and has gained increasing popularity in recent advances of the research on modeling symbolic computation using DNNs [6, 15].", "startOffset": 205, "endOffset": 212}, {"referenceID": 13, "context": "Starting with \u201cartificial\u201d tasks eases the process of developing novel deep models [14], and has gained increasing popularity in recent advances of the research on modeling symbolic computation using DNNs [6, 15].", "startOffset": 205, "endOffset": 212}, {"referenceID": 14, "context": "Objective functions are optimized using SGD in a mini-batch of size 100 with adaptive learning rates (AdaDelta [16]).", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": ", Combinatory Categorial Grammar) to parse NL queries and generate corresponding logical forms, which requires curated/learned lexicons defining the correspondence between NL phrases and symbolic constituents [17, 7, 1, 18].", "startOffset": 209, "endOffset": 223}, {"referenceID": 6, "context": ", Combinatory Categorial Grammar) to parse NL queries and generate corresponding logical forms, which requires curated/learned lexicons defining the correspondence between NL phrases and symbolic constituents [17, 7, 1, 18].", "startOffset": 209, "endOffset": 223}, {"referenceID": 0, "context": ", Combinatory Categorial Grammar) to parse NL queries and generate corresponding logical forms, which requires curated/learned lexicons defining the correspondence between NL phrases and symbolic constituents [17, 7, 1, 18].", "startOffset": 209, "endOffset": 223}, {"referenceID": 16, "context": ", Combinatory Categorial Grammar) to parse NL queries and generate corresponding logical forms, which requires curated/learned lexicons defining the correspondence between NL phrases and symbolic constituents [17, 7, 1, 18].", "startOffset": 209, "endOffset": 223}, {"referenceID": 4, "context": ", answers) as supervision signal [5, 3, 10, 11, 12].", "startOffset": 33, "endOffset": 51}, {"referenceID": 2, "context": ", answers) as supervision signal [5, 3, 10, 11, 12].", "startOffset": 33, "endOffset": 51}, {"referenceID": 9, "context": ", answers) as supervision signal [5, 3, 10, 11, 12].", "startOffset": 33, "endOffset": 51}, {"referenceID": 10, "context": ", answers) as supervision signal [5, 3, 10, 11, 12].", "startOffset": 33, "endOffset": 51}, {"referenceID": 2, "context": "The parsers, designed towards this new learning paradigm, take different types of forms, ranging from generic chart parsers [3, 11] to more specifically engineered, task-oriented ones [12, 8].", "startOffset": 124, "endOffset": 131}, {"referenceID": 10, "context": "The parsers, designed towards this new learning paradigm, take different types of forms, ranging from generic chart parsers [3, 11] to more specifically engineered, task-oriented ones [12, 8].", "startOffset": 124, "endOffset": 131}, {"referenceID": 7, "context": "The parsers, designed towards this new learning paradigm, take different types of forms, ranging from generic chart parsers [3, 11] to more specifically engineered, task-oriented ones [12, 8].", "startOffset": 184, "endOffset": 191}, {"referenceID": 5, "context": "Pioneered by the development of Neural Turing Machines (NTMs) [6], this line of research studies the problem of using differentiable neural networks to perform \u201chard\u201d symbolic execution.", "startOffset": 62, "endOffset": 65}, {"referenceID": 13, "context": "[15] designed a LSTM-RNN to execute simple Python programs, where the parameters are learned by comparing the neural network output and the correct answer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": ", the attention-based reading in the operations in Reader) and like [15], Neural Enquirer learns to execute a sequence with complicated structure, and the model is tuned from the executing them.", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "[9], which studies the same task of executing queries on tables with Deep Neural Networks.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "We proposed Neural Enquirer as a neural network architecture to execute a SQLlike query on a knowledge-base (KB) for answers. Basically, Neural Enquirer finds the distributed representation of a query and then executes it on knowledge-base tables to obtain the answer as one of the values in the tables. Unlike similar efforts in endto-end training of semantic parser [11, 9], Neural Enquirer is fully \u201cneuralized\u201d: it not only gives distributional representation of the query and the knowledge-base, but also realizes the execution of compositional queries as a series of differentiable operations, with intermediate results (consisting of annotations of the tables at different levels) saved on multiple layers of memory. Neural Enquirer can be trained with gradient descent, with which not only the parameters of the controlling components and semantic parsing component, but also the embeddings of the tables and query words can be learned from scratch. The training can be done in an end-to-end fashion, but it can take stronger guidance, e.g., the step-by-step supervision for complicated queries, and benefit from it. Neural Enquirer is one step towards building neural network systems which seek to understand language by executing it on real-world. Our experiments show that Neural Enquirer can learn to execute fairly complicated queries on tables with rich structures.", "creator": "LaTeX with hyperref package"}}}