{"id": "1512.01100", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2015", "title": "Effective LSTMs for Target-Dependent Sentiment Classification", "abstract": "Target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence representation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.", "histories": [["v1", "Thu, 3 Dec 2015 14:54:39 GMT  (763kb,D)", "http://arxiv.org/abs/1512.01100v1", "7 pages, 3 figures"], ["v2", "Thu, 29 Sep 2016 09:40:39 GMT  (762kb,D)", "http://arxiv.org/abs/1512.01100v2", "7 pages, 3 figures published in COLING 2016"]], "COMMENTS": "7 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["duyu tang", "bing qin", "xiaocheng feng", "ting liu"], "accepted": false, "id": "1512.01100"}, "pdf": {"name": "1512.01100.pdf", "metadata": {"source": "CRF", "title": "Target-Dependent Sentiment Classification with Long Short Term Memory", "authors": ["Duyu Tang", "Bing Qin", "Xiaocheng Feng", "Ting Liu"], "emails": ["dytang@ir.hit.edu.cn", "qinb@ir.hit.edu.cn", "xcfeng@ir.hit.edu.cn", "tliu@ir.hit.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Related Work", "text": "In this section we review short existing studies on target-dependent mood classification and neural network approaches to mood classification."}, {"heading": "2.1 Target-Dependent Sentiment Classification", "text": "Target-dependent sentiment classification is typically considered in the literature as a kind of text classification problem. Therefore, standard text classification approaches such as the function-based Supported Vector Machine [Pang et al., 2002; Jiang et al., 2011] can be used naturally to create a sentiment classifier. For example, Jiang et al. [2011] design target-independent features and target-dependent features manually using expert knowledge, syntactical parser, and external resources. Despite the effectiveness of feature engineering, it is labor-intensive and unable to detect the discriminating or explanatory factors of data. To address this problem, some recent studies [Dong et al., 2014; Vo and Zhang, 2015] use neural network methods and encode each sentence in continuous and low-dimensional vector space without feature engineering. Dong et al. [2014] transfer a dependency tree of a sentence to a recurrent-specific representational structure to use it in a higher recurrent cursive structure."}, {"heading": "2.2 Neural Network for Sentiment Classification", "text": "Neural network approaches have shown promising results in many tasks of sentiment analysis, such as sentence / documentation level sentiment classification [Socher et al., 2013b; Tang et al., 2015], opinion extraction [Irsoy et al., 2014b], sentiment lexicon construction [Tang et al., 2014a], open domain sentiment analysis [Zhang et al., 2015], fine-grained opinion formation [Liu et al., 2015], etc. The power of the neural model lies in its ability to learn continuous text representation from data without feature engineering. Previous studies have mostly consisted of two steps for sentence / document level sentiment classification. First, they learn continuous word vector embedding from data [Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tansang et al., 2014; and then to constitute a set of documents [Bengio et al., 2003]."}, {"heading": "3 The Approach", "text": "In this section, we describe the proposed approach to target-based sentiment classification. First, we present a basic long-term short-term memory (LSTM) approach that models the semantic representation of a sentence without taking into account the target word to be evaluated. Then, we add the target word to LSTM and obtain the TargetDependent Long-Term Memory (TD-LSTM) model. Finally, we add the target link to TD-LSTM, which takes into account the semantic relationship of the target with its context words."}, {"heading": "3.1 Long Short-Term Memory (LSTM)", "text": "In this part, we describe a long, short-term memory (LSTM) in which we aim at the goal-dependent sentiment classification. It is a basic version of our approach. In this context, the goal to be evaluated is ignored, so that the task is considered a goal. It is able to calculate the representation of a longer expression (e.g. a sentence) from the representation of its children with multiple levels of abstraction. Sentence representation can, of course, be considered a feature to determine the sensibility of the sence.Specifically, each word is presented as a low-dimensional, continuous, and real-rated vector, also known as word embedding. [Bengio et al al, 2003; Mikolov et al al al al., 2013; Pennington et al al., 2014; Tang et al al.] The word is presented as a low-dimensional, real-rated vector, also known as word embedding al al. [Bengio et al al al, al al; 2013 al; v al al, al al; al al al; al al al; al al; al al al."}, {"heading": "3.2 Target-Dependent LSTM (TD-LSTM)", "text": "The above-mentioned LSTM model solves the goal-dependent sentiment classification in a goal-independent way. That is, the feature representation used for sentiment classification remains the same, regardless of the target words. Let's say again, \"I bought a new camera. The image quality is amazing, but the battery life is too short\" as an example. The representations of this sentence in terms of image quality and battery life are identical. This is obviously problematic because the sentiment polarity labels for these two goals are different. To take the target information into account, we make a slight modification to the above-mentioned LSTM model and introduce a goal-dependent LSTM (TD-LSTM) in this subsection. The basic idea is to model the preceding and following context around the target string, so that contexts in both directions can be used as feature representation for the sentiment classification."}, {"heading": "3.3 Target-Connection LSTM (TC-LSTM)", "text": "Compared to the LSTM model, the target-based LSTM (TDLSTM) could make better use of the target information. However, we think that TD-LSTM is still not good enough because it does not capture the interactions between the target word and its contexts. Furthermore, a person who is asked to perform a target-based sentiment classification will select the relevant context words that are helpful in determining the sensitivity of a sentence to the target. Based on the above consideration, we will go a step further and develop a target-based short-term memory (TC-LSTM). This model expands TD-LSTM by including a target-link component that explicitly uses the connections between the target word and each context word when composing the representation of a sentence. An overview of TC-LSTM is illustrated in Figure 2. Entering TC-LSTM is a sentence consisting of 2 words..."}, {"heading": "3.4 Model Training", "text": "We train LSTM, TD-LSTM and TC-LSTM in an end-to-end manner in a supervised learning framework. The loss function is the cross-entropy error of the sentiment classification. Loss = \u2212 Looking S C \u2211 c = 1 P gc (s) \u00b7 log (Pc (s)) (9), where S is the training data, C is the number of sentiment categories, s is a set, Pc (s) is the probability of predicting s as class c given by the Softmax layer, P gc (s) indicates whether class c is the correct sentiment category whose value is 1 or 0. We take the derivation of the loss function by backpropagation with respect to all parameters and update parameters with stochastic gradient descent."}, {"heading": "4 Experiment", "text": "We apply the proposed method to target-based mood classification to evaluate its effectiveness. In this section, we describe experimental frameworks and empirical results."}, {"heading": "4.1 Experimental Settings", "text": "We conduct experiments in a monitored environment using a benchmark data set [Dong et al., 2014]. Each instance in the training / test set has a manually labeled mood polarity. Training set contains 6,248 sets and test set has 692 sets. The percentage of positive, negative and neutral in training and test sets is both 25%, 25%, 50%. We train the model on the training set and evaluate performance on the test set. Rating parameters are accuracy and macro F1 score over positive, negative and neutral categories [Manning and students, 1999; Jurafsky and Martin, 2000]."}, {"heading": "4.2 Comparison to Other Methods", "text": "In fact, it is that we are able to hide, and that we are able to hide, \"he said."}, {"heading": "4.3 Effects of Word Embeddings", "text": "We are therefore investigating the effects of different word embeddings on LSTM, TD-LSTM and TC-LSTM in this part. Since the benchmark dataset from [Dong et al., 2014] comes from Twitter, we are comparing sentimental-specific word embeddings (SSWE) 2 [Tang et al., 2014b] and glove vectors3 [Pennington et al., 2014]. All of these word embeddings are 50-dimensional and learn from Twitter. SSWEh, SSWEr and SSWEu perform different embeddings algorithms for learning algorithms introduced in [Tang et al., 2014b]. SSWEh and SSWEr learn word embeddings only by using sentence sentiment. SSWEu takes into account the sensitivity of sentences and contexts of words Er simultaneously. SSWEh and SSWEr learn word embeddings only by using sentiment of sentences / SER information that is publicly available."}, {"heading": "4.4 Case Study", "text": "In this section, we examine the extent to which target-dependent LSTM models, including TD-LSTM and TC-LSTM, improve the performance of a basic LSTM model. In Table 3, we list a few examples whose polarity markers are derived incorrectly from LSTM, but are correctly predicted by both TD-LSTM and TC-LSTM. We observe that the LSTM model prefers to assign the polarity of the entire sentence while ignoring the target to be evaluated. TD-LSTM and TC-LSTM may take target information into account to some extent. In the second example, for example, the opinion leader expresses a negative opinion about his work, but maintains a neutral attitude toward the target \"lindsay rewarding TC.\" In the last example, the entire sentence expresses a neutral sentiment, while containing a positive opinion toward \"google.\" We analyze the error cases that both TD-LSTM and TCLSTSTM cannot handle well, noting that 85.4% of the examples evaluated are incorrect in relation to the STM."}, {"heading": "4.5 Discussion", "text": "To capture the semantic relationship between target and context words, we are adding a target connection component to TD-LSTM. Other extensions could also be tried to capture the connection between target and context words. For example, we have also tried an attention-based LSTM model inspired by the recent success of the attention-based neural network in machine translation [Bahdanau et al., 2015] and document encoding [Li et al., 2015b]. We are implementing the soft attention mechanism [Bahdanau et al., 2015] to improve TD-LSTM. We include two attention layers for previous LSTM and subsequent LSTM. The output vector for each attention layer is the weighted average among the hidden vectors of LSTM, where the weight of each hidden vector is pre-calculated with a neural network."}, {"heading": "5 Conclusion", "text": "We develop target-specific short-term memory models for target-based sentiment classification. The approach captures the connection between the target word and its contexts in generating the representation of a sentence. We train the model consistently using a benchmark data set and show that the inclusion of target information could increase the performance of a long-term short-term memory model. The target-based LSTM model achieves state-of-the-art classification accuracy."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "ICLR,", "citeRegEx": "Bahdanau et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. Neural Networks"], "venue": "5(2):157\u2013166,", "citeRegEx": "Bengio et al.. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Journal of Machine Learning Research", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin. A neural probabilistic language model"], "venue": "3:1137\u20131155,", "citeRegEx": "Bengio et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "In EMNLP", "author": ["Parminder Bhatia", "Yangfeng Ji", "Jacob Eisenstein. Better document-level sentiment analysis from rst discourse parsing"], "venue": "pages 2212\u20132218,", "citeRegEx": "Bhatia et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In ACL", "author": ["Li Dong", "Furu Wei", "Chuanqi Tan", "Duyu Tang", "Ming Zhou", "Ke Xu. Adaptive recursive neural network for target-dependent twitter sentiment classification"], "venue": "pages 49\u201354,", "citeRegEx": "Dong et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Ludlow (1997)", "author": ["Gottlob Frege. On sense", "reference"], "venue": "pages 563\u2013584,", "citeRegEx": "Frege. 1892", "shortCiteRegEx": null, "year": 1892}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "In NIPS", "author": ["Ozan Irsoy", "Claire Cardie. Deep recursive neural networks for compositionality in language"], "venue": "pages 2096\u20132104,", "citeRegEx": "Irsoy and Cardie. 2014a", "shortCiteRegEx": null, "year": 2014}, {"title": "In EMNLP", "author": ["Ozan Irsoy", "Claire Cardie. Opinion mining with deep recurrent neural networks"], "venue": "pages 720\u2013728,", "citeRegEx": "Irsoy and Cardie. 2014b", "shortCiteRegEx": null, "year": 2014}, {"title": "ACL", "author": ["Long Jiang", "Mo Yu", "Ming Zhou", "Xiaohua Liu", "Tiejun Zhao. Target-dependent twitter sentiment classification"], "venue": "1:151\u2013160,", "citeRegEx": "Jiang et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech & language processing", "author": ["Dan Jurafsky", "James H Martin"], "venue": "Pearson Education India,", "citeRegEx": "Jurafsky and Martin. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "In ACL", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom. A convolutional neural network for modelling sentences"], "venue": "pages 655\u2013665,", "citeRegEx": "Kalchbrenner et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In EMNLP", "author": ["Yoon Kim. Convolutional neural networks for sentence classification"], "venue": "pages 1746\u20131751,", "citeRegEx": "Kim. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In ICML", "author": ["Quoc V. Le", "Tomas Mikolov. Distributed representations of sentences", "documents"], "venue": "pages 1188\u20131196,", "citeRegEx": "Le and Mikolov. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Eudard Hovy", "author": ["Jiwei Li", "Dan Jurafsky"], "venue": "When are tree structures necessary for deep learning of representations? EMNLP,", "citeRegEx": "Li et al.. 2015a", "shortCiteRegEx": null, "year": 2015}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky"], "venue": "ACL,", "citeRegEx": "Li et al.. 2015b", "shortCiteRegEx": null, "year": 2015}, {"title": "In EMNLP", "author": ["Pengfei Liu", "Shafiq Joty", "Helen Meng. Fine-grained opinion mining with recurrent neural networks", "word embeddings"], "venue": "pages 1433\u2013 1443,", "citeRegEx": "Liu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Synthesis Lectures on Human Language Technologies", "author": ["Bing Liu. Sentiment analysis", "opinion mining"], "venue": "5(1):1\u2013167,", "citeRegEx": "Liu. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Foundations of statistical natural language processing", "author": ["Christopher D Manning", "Hinrich Sch\u00fctze"], "venue": "MIT press,", "citeRegEx": "Manning and Sch\u00fctze. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "In NIPS", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean. Distributed representations of words", "phrases", "their compositionality"], "venue": "pages 3111\u20133119,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Foundations and trends in information retrieval", "author": ["Bo Pang", "Lillian Lee. Opinion mining", "sentiment analysis"], "venue": "2(1-2):1\u2013135,", "citeRegEx": "Pang and Lee. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "EMNLP, pages 79\u2013 86,", "citeRegEx": "Pang et al.. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "EMNLP, pages 1532\u20131543,", "citeRegEx": "Pennington et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Y Ng"], "venue": "NIPS,", "citeRegEx": "Socher et al.. 2013a", "shortCiteRegEx": null, "year": 2013}, {"title": "In EMNLP", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank"], "venue": "pages 1631\u20131642,", "citeRegEx": "Socher et al.. 2013b", "shortCiteRegEx": null, "year": 2013}, {"title": "context and entity with neural networks for entity disambiguation", "author": ["Yaming Sun", "Lei Lin", "Duyu Tang", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang. Modeling mention"], "venue": "IJCAI,", "citeRegEx": "Sun et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": "ACL,", "citeRegEx": "Tai et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Building large-scale twitter-specific sentiment lexicon: A representation learning approach", "author": ["Duyu Tang", "Furu Wei", "Bing Qin", "Ming Zhou", "Ting Liu"], "venue": "COLING, pages 172\u2013182,", "citeRegEx": "Tang et al.. 2014a", "shortCiteRegEx": null, "year": 2014}, {"title": "In ACL", "author": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin. Learning sentimentspecific word embedding for twitter sentiment classification"], "venue": "pages 1555\u20131565,", "citeRegEx": "Tang et al.. 2014b", "shortCiteRegEx": null, "year": 2014}, {"title": "EMNLP", "author": ["Duyu Tang", "Bing Qin", "Ting Liu. Document modeling with gated recurrent neural network for sentiment classification"], "venue": "pages 1422\u20131432,", "citeRegEx": "Tang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Targetdependent twitter sentiment classification with rich automatic features", "author": ["Duy-Tin Vo", "Yue Zhang"], "venue": "IJCAI,", "citeRegEx": "Vo and Zhang. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In EMNLP", "author": ["Meishan Zhang", "Yue Zhang", "Duy Tin Vo. Neural networks for open domain targeted sentiment"], "venue": "pages 612\u2013621,", "citeRegEx": "Zhang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory over tree structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "ICML,", "citeRegEx": "Zhu et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Sentiment analysis, also known as opinion mining [Pang and Lee, 2008; Liu, 2012], is a fundamental task in natural language processing and computational linguistics [Manning and Sch\u00fctze, 1999; Jurafsky and Martin, 2000].", "startOffset": 49, "endOffset": 80}, {"referenceID": 17, "context": "Sentiment analysis, also known as opinion mining [Pang and Lee, 2008; Liu, 2012], is a fundamental task in natural language processing and computational linguistics [Manning and Sch\u00fctze, 1999; Jurafsky and Martin, 2000].", "startOffset": 49, "endOffset": 80}, {"referenceID": 18, "context": "Sentiment analysis, also known as opinion mining [Pang and Lee, 2008; Liu, 2012], is a fundamental task in natural language processing and computational linguistics [Manning and Sch\u00fctze, 1999; Jurafsky and Martin, 2000].", "startOffset": 165, "endOffset": 219}, {"referenceID": 10, "context": "Sentiment analysis, also known as opinion mining [Pang and Lee, 2008; Liu, 2012], is a fundamental task in natural language processing and computational linguistics [Manning and Sch\u00fctze, 1999; Jurafsky and Martin, 2000].", "startOffset": 165, "endOffset": 219}, {"referenceID": 9, "context": "In this paper, we focus on target-dependent sentiment classification [Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015], which is a fundamental and extensively studied task in the field of sentiment analysis.", "startOffset": 69, "endOffset": 128}, {"referenceID": 4, "context": "In this paper, we focus on target-dependent sentiment classification [Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015], which is a fundamental and extensively studied task in the field of sentiment analysis.", "startOffset": 69, "endOffset": 128}, {"referenceID": 30, "context": "In this paper, we focus on target-dependent sentiment classification [Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015], which is a fundamental and extensively studied task in the field of sentiment analysis.", "startOffset": 69, "endOffset": 128}, {"referenceID": 9, "context": "Majority of existing studies build sentiment classifiers with supervised machine learning approach, such as feature based Supported Vector Machine [Jiang et al., 2011] or neural network approaches [Dong et al.", "startOffset": 147, "endOffset": 167}, {"referenceID": 4, "context": ", 2011] or neural network approaches [Dong et al., 2014; Vo and Zhang, 2015].", "startOffset": 37, "endOffset": 76}, {"referenceID": 30, "context": ", 2011] or neural network approaches [Dong et al., 2014; Vo and Zhang, 2015].", "startOffset": 37, "endOffset": 76}, {"referenceID": 6, "context": "The approach is an extension on long short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997] by incorporating target information.", "startOffset": 62, "endOffset": 96}, {"referenceID": 4, "context": "We apply the neural model to target-dependent sentiment classification on a benchmark dataset [Dong et al., 2014].", "startOffset": 94, "endOffset": 113}, {"referenceID": 9, "context": "We compare with feature-based SVM [Jiang et al., 2011], adaptive recursive neural network [Dong et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 4, "context": ", 2011], adaptive recursive neural network [Dong et al., 2014] and lexiconenhanced neural network [Vo and Zhang, 2015].", "startOffset": 43, "endOffset": 62}, {"referenceID": 30, "context": ", 2014] and lexiconenhanced neural network [Vo and Zhang, 2015].", "startOffset": 43, "endOffset": 63}, {"referenceID": 21, "context": "Therefore, standard text classification approach such as feature-based Supported Vector Machine [Pang et al., 2002; Jiang et al., 2011] can be naturally employed to build a sentiment classifier.", "startOffset": 96, "endOffset": 135}, {"referenceID": 9, "context": "Therefore, standard text classification approach such as feature-based Supported Vector Machine [Pang et al., 2002; Jiang et al., 2011] can be naturally employed to build a sentiment classifier.", "startOffset": 96, "endOffset": 135}, {"referenceID": 4, "context": "To handle this problem, some recent studies [Dong et al., 2014; Vo and Zhang, 2015] use neural network methods and encode each sentence in continuous and low-dimensional vector space without feature engineering.", "startOffset": 44, "endOffset": 83}, {"referenceID": 30, "context": "To handle this problem, some recent studies [Dong et al., 2014; Vo and Zhang, 2015] use neural network methods and encode each sentence in continuous and low-dimensional vector space without feature engineering.", "startOffset": 44, "endOffset": 83}, {"referenceID": 24, "context": "Neural network approaches have shown promising results on many sentiment analysis tasks like sentence/documentlevel sentiment classification [Socher et al., 2013b; Tang et al., 2015], opinion expression extraction [Irsoy and Cardie, 2014b], building sentiment lexicon [Tang et al.", "startOffset": 141, "endOffset": 182}, {"referenceID": 29, "context": "Neural network approaches have shown promising results on many sentiment analysis tasks like sentence/documentlevel sentiment classification [Socher et al., 2013b; Tang et al., 2015], opinion expression extraction [Irsoy and Cardie, 2014b], building sentiment lexicon [Tang et al.", "startOffset": 141, "endOffset": 182}, {"referenceID": 8, "context": ", 2015], opinion expression extraction [Irsoy and Cardie, 2014b], building sentiment lexicon [Tang et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 27, "context": ", 2015], opinion expression extraction [Irsoy and Cardie, 2014b], building sentiment lexicon [Tang et al., 2014a], open domain sentiment analysis [Zhang et al.", "startOffset": 93, "endOffset": 113}, {"referenceID": 31, "context": ", 2014a], open domain sentiment analysis [Zhang et al., 2015], fine-grained opinion mining [Liu et al.", "startOffset": 41, "endOffset": 61}, {"referenceID": 16, "context": ", 2015], fine-grained opinion mining [Liu et al., 2015], etc.", "startOffset": 37, "endOffset": 55}, {"referenceID": 2, "context": "They first learn continuous word vector embeddings from data [Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b].", "startOffset": 61, "endOffset": 149}, {"referenceID": 19, "context": "They first learn continuous word vector embeddings from data [Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b].", "startOffset": 61, "endOffset": 149}, {"referenceID": 22, "context": "They first learn continuous word vector embeddings from data [Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b].", "startOffset": 61, "endOffset": 149}, {"referenceID": 28, "context": "They first learn continuous word vector embeddings from data [Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b].", "startOffset": 61, "endOffset": 149}, {"referenceID": 5, "context": "Afterwards, semantic compositional approaches are used to compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality [Frege, 1892].", "startOffset": 180, "endOffset": 193}, {"referenceID": 24, "context": "Representative compositional approaches to learn sentence representation include recursive neural networks [Socher et al., 2013b; Irsoy and Cardie, 2014a], convolutional neural network [Kalchbrenner et al.", "startOffset": 107, "endOffset": 154}, {"referenceID": 7, "context": "Representative compositional approaches to learn sentence representation include recursive neural networks [Socher et al., 2013b; Irsoy and Cardie, 2014a], convolutional neural network [Kalchbrenner et al.", "startOffset": 107, "endOffset": 154}, {"referenceID": 11, "context": ", 2013b; Irsoy and Cardie, 2014a], convolutional neural network [Kalchbrenner et al., 2014; Kim, 2014], long short-term memory [Li et al.", "startOffset": 64, "endOffset": 102}, {"referenceID": 12, "context": ", 2013b; Irsoy and Cardie, 2014a], convolutional neural network [Kalchbrenner et al., 2014; Kim, 2014], long short-term memory [Li et al.", "startOffset": 64, "endOffset": 102}, {"referenceID": 14, "context": ", 2014; Kim, 2014], long short-term memory [Li et al., 2015a] and tree-structured LSTM [Tai et al.", "startOffset": 43, "endOffset": 61}, {"referenceID": 26, "context": ", 2015a] and tree-structured LSTM [Tai et al., 2015; Zhu et al., 2015].", "startOffset": 34, "endOffset": 70}, {"referenceID": 32, "context": ", 2015a] and tree-structured LSTM [Tai et al., 2015; Zhu et al., 2015].", "startOffset": 34, "endOffset": 70}, {"referenceID": 13, "context": "There also exists some studies focusing on learning continuous representation of documents [Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015].", "startOffset": 91, "endOffset": 153}, {"referenceID": 29, "context": "There also exists some studies focusing on learning continuous representation of documents [Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015].", "startOffset": 91, "endOffset": 153}, {"referenceID": 3, "context": "There also exists some studies focusing on learning continuous representation of documents [Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015].", "startOffset": 91, "endOffset": 153}, {"referenceID": 14, "context": "We use LSTM as it is a state-of-the-art performer for semantic composition in the area of sentiment analysis [Li et al., 2015a; Tang et al., 2015].", "startOffset": 109, "endOffset": 146}, {"referenceID": 29, "context": "We use LSTM as it is a state-of-the-art performer for semantic composition in the area of sentiment analysis [Li et al., 2015a; Tang et al., 2015].", "startOffset": 109, "endOffset": 146}, {"referenceID": 2, "context": "Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding [Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b].", "startOffset": 125, "endOffset": 213}, {"referenceID": 19, "context": "Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding [Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b].", "startOffset": 125, "endOffset": 213}, {"referenceID": 22, "context": "Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding [Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b].", "startOffset": 125, "endOffset": 213}, {"referenceID": 28, "context": "Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding [Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b].", "startOffset": 125, "endOffset": 213}, {"referenceID": 22, "context": "In this work, we pre-train the values of word vectors from text corpus with embedding learning algorithms [Pennington et al., 2014; Tang et al., 2014b] to make better use of semantic and grammatical associations of words.", "startOffset": 106, "endOffset": 151}, {"referenceID": 28, "context": "In this work, we pre-train the values of word vectors from text corpus with embedding learning algorithms [Pennington et al., 2014; Tang et al., 2014b] to make better use of semantic and grammatical associations of words.", "startOffset": 106, "endOffset": 151}, {"referenceID": 1, "context": "However, standard RNN suffers the problem of gradient vanishing or exploding [Bengio et al., 1994; Hochreiter and Schmidhuber, 1997], where gradients may grow or decay exponentially over long sequences.", "startOffset": 77, "endOffset": 132}, {"referenceID": 6, "context": "However, standard RNN suffers the problem of gradient vanishing or exploding [Bengio et al., 1994; Hochreiter and Schmidhuber, 1997], where gradients may grow or decay exponentially over long sequences.", "startOffset": 77, "endOffset": 132}, {"referenceID": 6, "context": "These gates adaptively remember input vector, forget previous history and generate output vector [Hochreiter and Schmidhuber, 1997].", "startOffset": 97, "endOffset": 131}, {"referenceID": 14, "context": "After calculating the hidden vector of each position, we regard the last hidden vector as the sentence representation [Li et al., 2015a; Tang et al., 2015].", "startOffset": 118, "endOffset": 155}, {"referenceID": 29, "context": "After calculating the hidden vector of each position, we regard the last hidden vector as the sentence representation [Li et al., 2015a; Tang et al., 2015].", "startOffset": 118, "endOffset": 155}, {"referenceID": 23, "context": "We obtain target vector vtarget by averaging the vectors of words it contains, which has been proven to be simple and effective in representing named entities [Socher et al., 2013a; Sun et al., 2015].", "startOffset": 159, "endOffset": 199}, {"referenceID": 25, "context": "We obtain target vector vtarget by averaging the vectors of words it contains, which has been proven to be simple and effective in representing named entities [Socher et al., 2013a; Sun et al., 2015].", "startOffset": 159, "endOffset": 199}, {"referenceID": 4, "context": "We conduct experiment in a supervised setting on a benchmark dataset [Dong et al., 2014].", "startOffset": 69, "endOffset": 88}, {"referenceID": 18, "context": "over positive, negative and neutral categories [Manning and Sch\u00fctze, 1999; Jurafsky and Martin, 2000].", "startOffset": 47, "endOffset": 101}, {"referenceID": 10, "context": "over positive, negative and neutral categories [Manning and Sch\u00fctze, 1999; Jurafsky and Martin, 2000].", "startOffset": 47, "endOffset": 101}, {"referenceID": 9, "context": "In SVM-dep, target-dependent features [Jiang et al., 2011] are also concatenated as the feature representation.", "startOffset": 38, "endOffset": 58}, {"referenceID": 4, "context": "In Recursive NN, standard Recursive neural network is used for feature learning over a transfered target-dependent dependency tree [Dong et al., 2014].", "startOffset": 131, "endOffset": 150}, {"referenceID": 4, "context": "AdaRNN-w/oE, AdaRNN-w/E and AdaRNN-comb are different variations of adaptive recursive neural network [Dong et al., 2014], whose composition functions are adaptively selected according to the inputs.", "startOffset": 102, "endOffset": 121}, {"referenceID": 30, "context": "In Target-dep, SVM classifier is built based on rich targetindependent and target-dependent features [Vo and Zhang, 2015].", "startOffset": 101, "endOffset": 121}, {"referenceID": 4, "context": "Since the benchmark dataset from [Dong et al., 2014] comes from Twitter, we compare between sentiment-specific word embedding (SSWE)2 [Tang et al.", "startOffset": 33, "endOffset": 52}, {"referenceID": 28, "context": ", 2014] comes from Twitter, we compare between sentiment-specific word embedding (SSWE)2 [Tang et al., 2014b] and Glove vectors3 [Pennington et al.", "startOffset": 89, "endOffset": 109}, {"referenceID": 22, "context": ", 2014b] and Glove vectors3 [Pennington et al., 2014].", "startOffset": 28, "endOffset": 53}, {"referenceID": 28, "context": "SSWEh, SSWEr and SSWEu are different embedding learning algorithms introduced in [Tang et al., 2014b].", "startOffset": 81, "endOffset": 101}, {"referenceID": 28, "context": "From Figure 3, we can find that SSWEh and SSWEr perform worse than SSWEu, which is consistent with the results reported on target-independent sentiment classification of tweets [Tang et al., 2014b].", "startOffset": 177, "endOffset": 197}, {"referenceID": 0, "context": "For example, we also tried an attention-based LSTM model, which is inspired by the recent success of attention-based neural network in machine translation [Bahdanau et al., 2015] and document encoding [Li et al.", "startOffset": 155, "endOffset": 178}, {"referenceID": 15, "context": ", 2015] and document encoding [Li et al., 2015b].", "startOffset": 30, "endOffset": 48}, {"referenceID": 0, "context": "We implement the soft-attention mechanism [Bahdanau et al., 2015] to enhance TD-LSTM.", "startOffset": 42, "endOffset": 65}], "year": 2015, "abstractText": "Target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence representation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.1", "creator": "LaTeX with hyperref package"}}}