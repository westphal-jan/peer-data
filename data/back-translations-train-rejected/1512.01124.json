{"id": "1512.01124", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2015", "title": "Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions", "abstract": "Many real-world problems come with action spaces represented as feature vectors. Although high-dimensional control is a largely unsolved problem, there has recently been progress for modest dimensionalities. Here we report on a successful attempt at addressing problems of dimensionality as high as $2000$, of a particular form. Motivated by important applications such as recommendation systems that do not fit the standard reinforcement learning frameworks, we introduce Slate Markov Decision Processes (slate-MDPs). A Slate-MDP is an MDP with a combinatorial action space consisting of slates (tuples) of primitive actions of which one is executed in an underlying MDP. The agent does not control the choice of this executed action and the action might not even be from the slate, e.g., for recommendation systems for which all recommendations can be ignored. We use deep Q-learning based on feature representations of both the state and action to learn the value of whole slates. Unlike existing methods, we optimize for both the combinatorial and sequential aspects of our tasks. The new agent's superiority over agents that either ignore the combinatorial or sequential long-term value aspect is demonstrated on a range of environments with dynamics from a real-world recommendation system. Further, we use deep deterministic policy gradients to learn a policy that for each position of the slate, guides attention towards the part of the action space in which the value is the highest and we only evaluate actions in this area. The attention is used within a sequentially greedy procedure leveraging submodularity. Finally, we show how introducing risk-seeking can dramatically imporve the agents performance and ability to discover more far reaching strategies.", "histories": [["v1", "Thu, 3 Dec 2015 15:51:30 GMT  (470kb,D)", "http://arxiv.org/abs/1512.01124v1", null], ["v2", "Wed, 16 Dec 2015 17:34:55 GMT  (470kb,D)", "http://arxiv.org/abs/1512.01124v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.HC cs.LG", "authors": ["peter sunehag", "richard evans", "gabriel dulac-arnold", "yori zwols", "daniel visentin", "ben coppin"], "accepted": false, "id": "1512.01124"}, "pdf": {"name": "1512.01124.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions", "authors": ["Peter Sunehag", "Richard Evans", "Gabriel Dulac-Arnold", "Yori Zwols", "Daniel Visentin", "Ben Coppin"], "emails": ["sunehag@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.01 124v 1 [cs"}, {"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Reinforcement Learning with Slate Actions", "text": "/ / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / [[[[[[[[[[[[[[[[[[[[[]] [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[["}, {"heading": "3 Experimental comparison", "text": "We conduct an experimental comparison of a series of agents described in the previous section via a test environment of the Q-Q system. Q-Q Weight patterns Q-Q Weight patterns Q-Q Weight patterns Q-Q Weight patterns Q-Q Weight patterns Q-Q Weight patterns Q-Q Weight patterns Q-Q-Q Weight patterns Q-Q-Q-Q Weight patterns Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q"}, {"heading": "4 Conclusions", "text": "We introduced agents that successfully address sequential decision problems with high-dimensional combinatorial slate action spaces found in important applications, including recommendation systems. We are focusing on the Slate-Markov decision processes introduced here that provide a formal framework for such applications, demonstrating the superiority of the new agents over relevant baselines through a series of environments derived from real data in a live recommendation system."}], "references": [{"title": "The use of MMR, diversity-based reranking for reordering documents and producing summaries", "author": ["Jaime Carbonell", "Jade Goldstein"], "venue": "In In SIGIR,", "citeRegEx": "Carbonell and Goldstein.,? \\Q1998\\E", "shortCiteRegEx": "Carbonell and Goldstein.", "year": 1998}, {"title": "Search engines: information retrieval in practice", "author": ["W. Bruce Croft", "Donald Metzler", "Trevor Strohman"], "venue": null, "citeRegEx": "Croft et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Croft et al\\.", "year": 2010}, {"title": "Fast reinforcement learning in large discrete action spaces", "author": ["Gabriel Dulac-Arnold", "Richard Evans", "Peter Sunehag"], "venue": "In preparation,", "citeRegEx": "Dulac.Arnold et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dulac.Arnold et al\\.", "year": 2016}, {"title": "Non-deterministic policies in markovian decision processes", "author": ["M.M. Fard", "J. Pineau"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Fard and Pineau.,? \\Q2011\\E", "shortCiteRegEx": "Fard and Pineau.", "year": 2011}, {"title": "Submodular Functions and Optimization: Second Edition", "author": ["S. Fujishige"], "venue": "Annals of Discrete Mathematics. Elsevier Science,", "citeRegEx": "Fujishige.,? \\Q2005\\E", "shortCiteRegEx": "Fujishige.", "year": 2005}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen.,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen.", "year": 2002}, {"title": "Non-Stochastic Bandit Slate Problems", "author": ["S. Kale", "L. Reyzin", "R. Schapire"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Kale et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kale et al\\.", "year": 2010}, {"title": "Prospect theory: An analysis of decisions under risk", "author": ["Daniel Kahneman", "Amos Tversky"], "venue": null, "citeRegEx": "Kahneman and Tversky.,? \\Q1979\\E", "shortCiteRegEx": "Kahneman and Tversky.", "year": 1979}, {"title": "Matroid bandits: Fast combinatorial optimization with learning", "author": ["B. Kveton", "Z. Wen", "A. Ashkan", "H. Eydgahi", "B. Eriksson"], "venue": "CoRR, abs/1403.5045,", "citeRegEx": "Kveton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2014}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "CoRR, abs/1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Rusu", "J. Veness", "M. Bellemare", "A. Graves", "M. Riedmiller", "A. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Autonomous inverted helicopter flight via reinforcement learning", "author": ["A. Ng", "A. Coates", "M. Diel", "V. Ganapathi", "J. Schulte", "B. Tse", "E. Berger", "E. Liang"], "venue": "In Experimental Robotics IX,", "citeRegEx": "Ng et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2004}, {"title": "A literature review and classification of recommender systems research", "author": ["Deuk Hee Park", "Hyea Kyeong Kim", "Il Young Choi", "Jae Kyeong Kim"], "venue": "Expert Systems with Applications,", "citeRegEx": "Park et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Park et al\\.", "year": 2012}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M. Puterman"], "venue": null, "citeRegEx": "Puterman.,? \\Q1994\\E", "shortCiteRegEx": "Puterman.", "year": 1994}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S.J. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig.,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig.", "year": 2010}, {"title": "Reinforcement Learning", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Deterministic policy gradient algorithms", "author": ["David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin A. Riedmiller"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Silver et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Using continuous action spaces to solve discrete problems", "author": ["Hado Van Hasselt", "Marco Wiering"], "venue": "In Neural Networks,", "citeRegEx": "Hasselt and Wiering,? \\Q2009\\E", "shortCiteRegEx": "Hasselt and Wiering", "year": 2009}, {"title": "Linear submodular bandits and their application to diversified retrieval", "author": ["Y. Yue", "C. Guestrin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Yue and Guestrin.,? \\Q2011\\E", "shortCiteRegEx": "Yue and Guestrin.", "year": 2011}], "referenceMentions": [], "year": 2016, "abstractText": "Many real-world problems come with action spaces represented as feature vectors. Although high-dimensional control is a largely unsolved problem, there has recently been progress for modest dimensionalities. Here we report on a successful attempt at addressing problems of dimensionality as high as 2000, of a particular form. Motivated by important applications such as recommendation systems that do not fit the standard reinforcement learning frameworks, we introduce Slate Markov Decision Processes (slate-MDPs). A Slate-MDP is an MDP with a combinatorial action space consisting of slates (tuples) of primitive actions of which one is executed in an underlying MDP. The agent does not control the choice of this executed action and the action might not even be from the slate, e.g., for recommendation systems for which all recommendations can be ignored. We use deep Q-learning based on feature representations of both the state and action to learn the value of whole slates. Unlike existing methods, we optimize for both the combinatorial and sequential aspects of our tasks. The new agent\u2019s superiority over agents that either ignore the combinatorial or sequential long-term value aspect is demonstrated on a range of environments with dynamics from a real-world recommendation system. Further, we use deep deterministic policy gradients to learn a policy that for each position of the slate, guides attention towards the part of the action space in which the value is the highest and we only evaluate actions in this area. The attention is used within a sequentially greedy procedure leveraging submodularity. Finally, we show how introducing risk-seeking can dramatically imporve the agents performance and ability to discover more far reaching strategies. 1 ar X iv :1 51 2. 01 12 4v 1 [ cs .A I] 3 D ec 2 01 5", "creator": "LaTeX with hyperref package"}}}