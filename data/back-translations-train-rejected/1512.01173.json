{"id": "1512.01173", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2015", "title": "Building Memory with Concept Learning Capabilities from Large-scale Knowledge Base", "abstract": "We present a new perspective on neural knowledge base (KB) embeddings, from which we build a framework that can model symbolic knowledge in the KB together with its learning process. We show that this framework well regularizes previous neural KB embedding model for superior performance in reasoning tasks, while having the capabilities of dealing with unseen entities, that is, to learn their embeddings from natural language descriptions, which is very like human's behavior of learning semantic concepts.", "histories": [["v1", "Thu, 3 Dec 2015 17:52:50 GMT  (15kb)", "http://arxiv.org/abs/1512.01173v1", "Accepted to NIPS 2015 Cognitive Computation workshop (CoCo@NIPS 2015)"]], "COMMENTS": "Accepted to NIPS 2015 Cognitive Computation workshop (CoCo@NIPS 2015)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["jiaxin shi", "jun zhu"], "accepted": false, "id": "1512.01173"}, "pdf": {"name": "1512.01173.pdf", "metadata": {"source": "CRF", "title": "Building Memory with Concept Learning Capabilities from Large-scale Knowledge Base", "authors": ["Jiaxin Shi", "Jun Zhu"], "emails": ["ishijiaxin@126.com", "dcszj@mail.tsinghua.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.01 173v 1 [cs.C L] 3"}, {"heading": "1 Introduction", "text": "In fact, most people are able to feel as if they are able to move around."}, {"heading": "2 Related work", "text": "Traditional methods such as Markov logic networks [8] often suffer from scaling problems due to intractable conclusions. Following the success of low-level models [9] in collaborative filtering, tensor factorization [10, 11] has been proposed as a more general way to deal with multilateral learning (i.e., there are several types of relationships between two entities); another perspective is to consider elements in factored tensors as probable latent properties of entities, leading to methods that apply non-parametric Bayesian conclusions to learn latent properties [12, 13, 14] for left prediction; and attempts have also been made to address the interpretative capacity of latent feature-based models in Bayesian clustering."}, {"heading": "3 The framework", "text": "Our framework consists of two parts: the first part is a storage of storage media for embedding representations. We use it to model the large-scale symbolic knowledge in the KB that can be thought of as a memory of concepts; the second part is a concept learning module that accepts descriptions of concepts in natural language as input and then converts them into entities embedded in the same memory area of the memory. In this paper, we use the embedding model from [5] as a storage medium and use neural networks for the concept learning module."}, {"heading": "3.1 Translating embedding model as memory storage", "text": "We first describe the embedding (TransE) of the model [5], which we use as memory storage of concepts < < < < < relationships are presented as translations in the embedding room. < Suppose we have a set of N true facts D = {(e1, r, e2) N as a training set. If a fact (e1, r, e2) is true, then TransE e1 + r, \"close to e2,\" we formally define the set of entity vectors as E, the set of relational vectors as R, where R, E, e1, e2, e2, e2, e3, e4, e4, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5, e5,"}, {"heading": "3.2 Concept learning module", "text": "As already mentioned, the concept learning module accepts natural language descriptions of concepts such as the ending of program items, and the results of corresponding embedding. As this requires a natural understanding of language with knowledge of the world, good candidates may emerge for this task. We are researching two types of neural network architectures for the concept learning module, including multi-layered perceptions (MLP) and Convolutionary Neural Networks (CNN). Since MLP is fully connected, we cannot afford the computer-related costs if the input length is too long. For large amounts of data, the vocabulary size is often as large as millions, meaning that bag-of-words features cannot be used. Here, we use bag-of-n-gramms features as inputs (there are 263 = 17576 types of pure English text)."}, {"heading": "3.3 Training", "text": "Together we train our embedding model and our concept learning module by stochastic gradient descent with mini-batch and Nesterov pulse [20], using the loss defined by Equation 1, where the entity embedding is given by outputs of the concept learning module. If we perform SGD with mini-batch, we propagate the error gradients back into the neural network and for CNN finally into word vectors. The relation embedding is also updated with SGD and normalised again in each iteration so that its L2 standards remain 1."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "Since no public datasets meet our needs, we have built two new datasets to test our method and make them public for research purposes, the first being based on FB15k of [5]. We dump the descriptions of all units in FB15k from Freebase [21], which are stored under Relation / Common / Topic / Description. We refer to this dataset as FB15k-desc2. The other dataset also comes from Freebase, while we make it much larger. In fact, we include all units that have descriptions in Freebase and remove triplets with relationships in a filter set. Most relationships in the filter set are schema relationships such as / type / object / key. This dataset has more than 4M units for which we call it FB4M-desc3."}, {"heading": "4.2 Link prediction", "text": "First of all, we describe the task of link prediction. In view of a relationship and an entity on the one hand, the task is to predict the entity on the other. It is a natural thought process that occurs in our minds all the time. [5] After previous work, we use the following evaluation protocol for this task. For each test triplet (e1, r, e2), e1 is removed and replaced by all other entities in the training set. The neural embedding model should provide values for these corrupt triplets. The rank of the correct entity is stored. We then specify the mean of the predicted ranking on the test set as the left mean. This procedure is repeated by correcting e2 and then we get the correct mean. The percentage of correct entities ranked in the top 10 is another index that we call hit @ 10."}, {"heading": "4.3 Concept learning capabilities", "text": "In Section 4.2, it was shown that our framework regulates the neural embedding model for storage media well. Next, we use FB4M-desc to evaluate the ability of our framework to learn new concepts and execute arguments based on learned embeddings. We report on the link prognosis on FB4M-desc in Table 4. Note that the test set contains millions of triples, which is very time consuming in rank-based evaluation. Therefore, we randomly sample triples from 1k, 10k and 80k from the test set to report the evaluation statistics. We see that CNN MLP contains both the mean and hits @ 10. All triples in the FB4M-desc test set contain a unit that is not visible in the training set on one side, forcing the model to understand natural language descriptions and to work on the basis of Bur's reasoning with this set, which we do not know to compete with the traditional knowledge base for this set."}, {"heading": "5 Conclusions and future work", "text": "We present a novel perspective on embedding knowledge that enables us to build a framework with concept learning capabilities from large-scale KB models based on earlier neural embedding models. We evaluate our framework based on two newly constructed data sets from Freebase, and the results show that our framework regulates the neural embedding model well to provide superior performance, while it is able to learn new concepts and use the newly learned embedding to perform semantic tasks (e.g. reasoning). Future work could include consistently improving the performance of learned concept embedding on large data sets such as FB4M-Desc. For applications, we consider this framework to be very promising in solving problems of unknown entities in KB-driven dialog systems."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Conference on Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1923}, {"title": "How to grow a mind: Statistics, structure, and abstraction", "author": ["Joshua B Tenenbaum", "Charles Kemp", "Thomas L Griffiths", "Noah D Goodman"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Factorizing yago: scalable machine learning for linked data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 21st international conference on World Wide Web,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["Charles Kemp", "Joshua B Tenenbaum", "Thomas L Griffiths", "Takeshi Yamada", "Naonori Ueda"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Nonparametric latent feature models for link prediction", "author": ["Kurt Miller", "Michael I Jordan", "Thomas L Griffiths"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Max-margin nonparametric latent feature models for link prediction", "author": ["Jun Zhu"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Modelling relational data using bayesian clustered tensor factorization", "author": ["Ilya Sutskever", "Joshua B Tenenbaum", "Ruslan R Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "A semantic matching energy function for learning with multi-relational data", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Knowledge graph and text jointly embedding", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "The parallel distributed processing approach to semantic cognition", "author": ["James L McClelland", "Timothy T Rogers"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "With deep neural networks, people are able to achieve superior performance in various machine learning tasks [1, 2, 3, 4].", "startOffset": 109, "endOffset": 121}, {"referenceID": 1, "context": "With deep neural networks, people are able to achieve superior performance in various machine learning tasks [1, 2, 3, 4].", "startOffset": 109, "endOffset": 121}, {"referenceID": 2, "context": "With deep neural networks, people are able to achieve superior performance in various machine learning tasks [1, 2, 3, 4].", "startOffset": 109, "endOffset": 121}, {"referenceID": 3, "context": "With deep neural networks, people are able to achieve superior performance in various machine learning tasks [1, 2, 3, 4].", "startOffset": 109, "endOffset": 121}, {"referenceID": 4, "context": ", vectors [5] or matrices [6], and learn them from the KB.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": ", vectors [5] or matrices [6], and learn them from the KB.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "1 We use a neural embedding model [5] to model the memory of concepts.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Concept learning in cognitive science usually refers to the cognitive process where people grow abstract generalizations from several example objects [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "Following the success of low rank models [9] in collaborative filtering, tensor factorization [10, 11] was proposed as a more general form to deal with multi-relational learning (i.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "Following the success of low rank models [9] in collaborative filtering, tensor factorization [10, 11] was proposed as a more general form to deal with multi-relational learning (i.", "startOffset": 94, "endOffset": 102}, {"referenceID": 9, "context": "Following the success of low rank models [9] in collaborative filtering, tensor factorization [10, 11] was proposed as a more general form to deal with multi-relational learning (i.", "startOffset": 94, "endOffset": 102}, {"referenceID": 10, "context": "This leads to methods that apply nonparametric Bayesian inference to learn latent features [12, 13, 14] for link prediction.", "startOffset": 91, "endOffset": 103}, {"referenceID": 11, "context": "This leads to methods that apply nonparametric Bayesian inference to learn latent features [12, 13, 14] for link prediction.", "startOffset": 91, "endOffset": 103}, {"referenceID": 12, "context": "This leads to methods that apply nonparametric Bayesian inference to learn latent features [12, 13, 14] for link prediction.", "startOffset": 91, "endOffset": 103}, {"referenceID": 13, "context": "Also, attempts have been made to address the interpretability of latent feature based models under the framework of Bayesian clustering [15].", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "More recently, with the noticeable achievements of neural embedding models like word vectors [16] in natural language processing area, various neural embedding models [6, 17, 5, 4, 18] for relational data have been proposed as strong competitors in both scalability and predictability for reasoning tasks.", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "More recently, with the noticeable achievements of neural embedding models like word vectors [16] in natural language processing area, various neural embedding models [6, 17, 5, 4, 18] for relational data have been proposed as strong competitors in both scalability and predictability for reasoning tasks.", "startOffset": 167, "endOffset": 184}, {"referenceID": 15, "context": "More recently, with the noticeable achievements of neural embedding models like word vectors [16] in natural language processing area, various neural embedding models [6, 17, 5, 4, 18] for relational data have been proposed as strong competitors in both scalability and predictability for reasoning tasks.", "startOffset": 167, "endOffset": 184}, {"referenceID": 4, "context": "More recently, with the noticeable achievements of neural embedding models like word vectors [16] in natural language processing area, various neural embedding models [6, 17, 5, 4, 18] for relational data have been proposed as strong competitors in both scalability and predictability for reasoning tasks.", "startOffset": 167, "endOffset": 184}, {"referenceID": 3, "context": "More recently, with the noticeable achievements of neural embedding models like word vectors [16] in natural language processing area, various neural embedding models [6, 17, 5, 4, 18] for relational data have been proposed as strong competitors in both scalability and predictability for reasoning tasks.", "startOffset": 167, "endOffset": 184}, {"referenceID": 16, "context": "More recently, with the noticeable achievements of neural embedding models like word vectors [16] in natural language processing area, various neural embedding models [6, 17, 5, 4, 18] for relational data have been proposed as strong competitors in both scalability and predictability for reasoning tasks.", "startOffset": 167, "endOffset": 184}, {"referenceID": 8, "context": "For example, [10] can be seen as having a feature vector for each entity in factorized tensors, while [6] also represents entities in separate vectors, or embeddings, thus the number of parameters scales linearly with the number of entities.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "For example, [10] can be seen as having a feature vector for each entity in factorized tensors, while [6] also represents entities in separate vectors, or embeddings, thus the number of parameters scales linearly with the number of entities.", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "In this paper we use translating embedding model from [5] as our memory storage and use neural networks for the concept learning module.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "We first describe translating embedding (TransE) model [5], which we use as the memory storage of concepts.", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": "Besides, TransE forces the L2 norms of entity embeddings to be 1, which is essential for SGD to perform well according to [5], because it prevents the training process from trivially minimizing loss by increasing entity embedding norms.", "startOffset": 122, "endOffset": 125}, {"referenceID": 17, "context": ", <A, has father, B>) [19].", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "During experiments in this paper, we set d = 50 and initialize v(wi) with wi\u2019s word vector pretrained from large scale corpus, using methods in [16].", "startOffset": 144, "endOffset": 148}, {"referenceID": 18, "context": "We jointly train our embedding model and concept learning module together by stochastic gradient descent with mini-batch and Nesterov momentum [20], using the loss defined by equation 1, where the entity embeddings are given by outputs of the concept learning module.", "startOffset": 143, "endOffset": 147}, {"referenceID": 4, "context": "The first dataset is based on FB15k released by [5].", "startOffset": 48, "endOffset": 51}, {"referenceID": 19, "context": "We dump natural language descriptions of all entities in FB15k from Freebase [21], which are stored under relation /common/topic/description.", "startOffset": 77, "endOffset": 81}, {"referenceID": 4, "context": "Model Mean rank Hits@10 (%) Left Right Avg Left Right Avg TransE[5] - - 243 - - 34.", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "Following previous work [5], we use below evaluation protocol for this task.", "startOffset": 24, "endOffset": 27}], "year": 2015, "abstractText": "We present a new perspective on neural knowledge base (KB) embeddings, from which we build a framework that can model symbolic knowledge in the KB together with its learning process. We show that this framework well regularizes previous neural KB embedding model for superior performance in reasoning tasks, while having the capabilities of dealing with unseen entities, that is, to learn their embeddings from natural language descriptions, which is very like human\u2019s behavior of learning semantic concepts.", "creator": "LaTeX with hyperref package"}}}