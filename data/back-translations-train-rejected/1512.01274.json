{"id": "1512.01274", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2015", "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems", "abstract": "MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters.", "histories": [["v1", "Thu, 3 Dec 2015 22:49:21 GMT  (100kb,D)", "http://arxiv.org/abs/1512.01274v1", "In Neural Information Processing Systems, Workshop on Machine Learning Systems, 2016"]], "COMMENTS": "In Neural Information Processing Systems, Workshop on Machine Learning Systems, 2016", "reviews": [], "SUBJECTS": "cs.DC cs.LG cs.MS cs.NE", "authors": ["tianqi chen", "mu li", "yutian li", "min lin", "naiyan wang", "minjie wang", "tianjun xiao", "bing xu", "chiyuan zhang", "zheng zhang"], "accepted": false, "id": "1512.01274"}, "pdf": {"name": "1512.01274.pdf", "metadata": {"source": "CRF", "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems", "authors": ["Tianqi Chen", "Mu Li", "Yutian Li", "Min Lin", "Naiyan Wang", "Minjie Wang", "Tianjun Xiao", "Bing Xu", "Chiyuan Zhang", "Zheng Zhang"], "emails": ["(muli@cs.cmu.edu)"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to move to another world, in which they can move to another world."}, {"heading": "2 Programming Interface", "text": "), or a complex neural network layer (e.g. conversion layer).An operator can take several input variables that produce more than one output variable and have internalstate variables. A variable can be either free, which we can associate with value later, or an output of another symbol. Figure 2 shows the construction of a multi-layer perception symbol by concatenating a variable that presents the input data, and several layers of operators.using MXNet mlp = @ mx.chain mx.Variable (data)."}, {"heading": "2.4 Other Modules", "text": "MXNet comes with tools that can pack arbitrary-sized examples into a single compact file to facilitate both sequential and random searches; data iterators are also provided; data pre-extraction and pre-processing is done with multiple threads, reducing the cost of possible read operations and / or decoding and transformation of images remotely; and the training module implements commonly used optimization algorithms, such as stochastic gradient reduction; it trains a model on a given symbolic module and data iterators, optionally distributed when an additional KVStore is provided."}, {"heading": "3 Implementation", "text": "3.1 Computation Graph fullcrelu \u2202 W WX \u2202 Xb \u2202 fullc \u2202 relu \u2202 bFigure 4: Computation graph for forward and backward.A binded symbolic expression is presented as a computation graph for evaluation. Figure 4 shows a portion of the graph both forward and backward of the MLP symbol in Figure 2. Prior to the evaluation, MXNet transforms the graph to optimize efficiency and allocate memory to internal variables. Graph Optimization. We examine the following simple optimizations. First, we note that only the subgraph is needed to obtain the results given during the binding. For example, the prediction requires only the predictive graph, while the last layers can be skipped for the extraction of features from internal layers. Second, operators can be grouped into a single subgraph needed to obtain the results during the binding."}, {"heading": "3.2 Dependency Engine", "text": "In MXNet, each source unit, including NDArray, random number generator, and time space, is registered to the motor with a unique tag. All operations, such as matrix operation or data communication, are then pushed into the motor with the required resource tags. The motor continuously schedules the deferred operations to execute when dependencies are resolved. Since there are usually multiple computing resources such as CPUs, GPUs, and the memory / PCIe buses, the motor uses multiple threads to schedule the operations for better resource utilization and parallelization. Unlike most dataflow motors [14], our motor tracks mutation operations as an existing resource unit. That is, our system supports specifying the tags that an operation will write in addition to reading. This allows planning array mutations as in numbers and other tensors or libraries. It also allows easier reuse of parameters by presenting parameter fields as mutation updates."}, {"heading": "4 Evaluation", "text": "Raw Performance We compare MXNet with Torch7, Caffe and TensorFlow on the popular Convnet benchmarks [2], all of which are compiled with CUDA 7.5 and CUDNN 3, with the exception of TensorFlow, which only supports CUDA 7.0 and CUDNN 2. We use batch size 32 for all networks and run the experiments on a single Nvidia GTX 980 card. Results are shown in Figure 6, which may be due to the use of a lower version of CUDNN. Memory utilization Figure 7 shows the memory usage of internal variables, as most calculations are made for the CUDA / CUDNN cores. TensorFlow is always twice slower, which is due to the use of a lower version of CUDNN. Memory utilization Figure 7 shows the memory usage of internal variables, except for output volumes."}, {"heading": "5 Conclusion", "text": "MXNet is a machine learning library that combines symbolic expression with tensor computation to maximize efficiency and flexibility. It is lightweight and can be run in multiple host languages. Experimental results are encouraging. As we continue to work on new design possibilities, we believe it can already benefit the relevant research community. Codes are available at http: / / dmlc.io.Recognize. We sincerely thank Dave Andersen, Carlos Guestrin, Tong He, Chuntao Hong, Qiang Kou, Hu Shiwen, Alex Smola, Junyuan Xie, Dale Schuurmans and all other contributors."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5590,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Easy benchmarking of all public open-source implementations of convnets", "author": ["Soumith Chintala"], "venue": "https://github.com/soumith/convnet-benchmarks", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "J. Park", "A.J. Smola", "A. Amhed", "V. Josifovski", "J. Long", "E. Shekita", "B.Y. Su"], "venue": "In OSDI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Communication efficient distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "A.J. Smola", "K. Yu"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Purine: A bi-graph based deep learning framework", "author": ["Min Lin", "Shuo Li", "Xuan Luo", "Shuicheng Yan"], "venue": "arXiv preprint arXiv:1412.6249,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous systems", "author": ["Abadi Martn", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal Jozefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Mane", "Rajat Monga", "Sherry Moore", "Derek Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Viegas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei- Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Minerva: A scalable and highly efficient training platform", "author": ["Minjie Wang", "Tianjun Xiao", "Jianpeng Li", "Jiaxing Zhang", "Chuntao Hong", "Zheng Zhang"], "venue": "for deep learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "Almost all recent ImageNet challenge [12] winners employ neural networks with very deep layers, requiring billions of floating-point operations to process one single sample.", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "For example, Minerva [14] combines imperative programming with asynchronize execution.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "System Core Binding Devices Distri- Imperative Declarative Lang Langs (beyond CPU) buted Program Program Caffe [7] C++ Python/Matlab GPU \u00d7 \u00d7 \u221a", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "Torch7 [3] Lua GPU/FPGA \u00d7 \u221a \u00d7 Theano [1] Python GPU \u00d7 \u00d7 \u221a", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "Torch7 [3] Lua GPU/FPGA \u00d7 \u221a \u00d7 Theano [1] Python GPU \u00d7 \u00d7 \u221a", "startOffset": 37, "endOffset": 40}, {"referenceID": 9, "context": "TensorFlow [11] C++ Python GPU/Mobile \u221a \u00d7 \u221a", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "Similar discipline was adopted in Purine2 [10].", "startOffset": 42, "endOffset": 46}, {"referenceID": 5, "context": "Instead, CXXNet adopts declarative programming (over tensor abstraction) and concrete execution, similar to Caffe [7].", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "Comparing to other open-source ML systems, MXNet provides a superset programming interface to Torch7 [3], Theano [1], Chainer [5] and Caffe [7], and supports more systems such as GPU clusters.", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "Comparing to other open-source ML systems, MXNet provides a superset programming interface to Torch7 [3], Theano [1], Chainer [5] and Caffe [7], and supports more systems such as GPU clusters.", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "Comparing to other open-source ML systems, MXNet provides a superset programming interface to Torch7 [3], Theano [1], Chainer [5] and Caffe [7], and supports more systems such as GPU clusters.", "startOffset": 140, "endOffset": 143}, {"referenceID": 9, "context": "Besides supporting the optimization for declarative programs as TensorFlow [11] do, MXNet additionally embed imperative tensor operations to provide more flexibility.", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "Finally, model divergence is controlled via consistency model [8].", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": "Different to most dataflow engines [14], our engine tracks mutation operations as an existing resource unit.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "We implemented KVStore based on the parameter server [8, 9, 4](Figure 5).", "startOffset": 53, "endOffset": 62}, {"referenceID": 7, "context": "We implemented KVStore based on the parameter server [8, 9, 4](Figure 5).", "startOffset": 53, "endOffset": 62}, {"referenceID": 3, "context": "We implemented KVStore based on the parameter server [8, 9, 4](Figure 5).", "startOffset": 53, "endOffset": 62}, {"referenceID": 1, "context": "Raw performance We fist compare MXNet with Torch7, Caffe, and TensorFlow on the popular \u201cconvnet-benchmarks\u201d [2].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "We train googlenet with batch normalization [6] on the ILSVRC12 dataset [13] which consists of 1.", "startOffset": 44, "endOffset": 47}, {"referenceID": 11, "context": "We train googlenet with batch normalization [6] on the ILSVRC12 dataset [13] which consists of 1.", "startOffset": 72, "endOffset": 76}], "year": 2015, "abstractText": "MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.", "creator": "LaTeX with hyperref package"}}}