{"id": "1512.01283", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2015", "title": "Predicting the top and bottom ranks of billboard songs using Machine Learning", "abstract": "The music industry is a $130 billion industry. Predicting whether a song catches the pulse of the audience impacts the industry. In this paper we analyze language inside the lyrics of the songs using several computational linguistic algorithms and predict whether a song would make to the top or bottom of the billboard rankings based on the language features. We trained and tested an SVM classifier with a radial kernel function on the linguistic features. Results indicate that we can classify whether a song belongs to top and bottom of the billboard charts with a precision of 0.76.", "histories": [["v1", "Thu, 3 Dec 2015 23:42:10 GMT  (123kb,D)", "http://arxiv.org/abs/1512.01283v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["vivek datla", "abhinav vishnu"], "accepted": false, "id": "1512.01283"}, "pdf": {"name": "1512.01283.pdf", "metadata": {"source": "CRF", "title": "Predicting the top and bottom ranks of billboard songs using Machine Learning", "authors": ["Vivek Datla", "Abhinav Vishnu"], "emails": ["vivek.datla@pnnl.gov", "abhinav.vishnu@pnnl.gov"], "sections": [{"heading": "Introduction", "text": "The German philosopher Friedrich Nietzsche said, \"Without music, life would be a mistake.\" In this digital age, we have access to a large collection of music composed at an astonishing price. The iTunes music store alone offers 37 million songs and has sold more than 25 billion songs worldwide. Each society has its own version of the music and popularity of the songs, and sometimes they transcend societies as well as continents. Nineties pop and rock music were dominated by artists such as Micheal Jackson, Sting, U2 and many others. The entire generation of the Nineties can immediately identify with \"Beat it.\""}, {"heading": "Related Work", "text": "Language is a strong indicator of a person's stress and mood. Identifying these characteristics has helped computational linguists and computer scientists relate the language characteristics to several complex problems in the tutor systems (Rus et al., 2013; Graesser et al., 2005), recognition (DMello et al., 2008), sensation (Hu and Liu, 2004), opinion-forming, and many others. Su, Fung, and Auguin (2013) have a Multimodal Music-Emotion Classification (MEC) for classifying 14 types of emotions from the music and lyrics of Western music. Their dataset consisted of 3 x 500 songs with emotions / mood such as sad, groovy, lonely, sexy, romantic, angry, nostalgic, jazzy, jazzy, and calm."}, {"heading": "Data", "text": "Billboard magazine (Billboard, 2015) has been a leading music publication worldwide since 1984. Billboard's music charts have become the primary source of information about trends and innovations in the music industry, with more than 10 million users making their ranking the standard in the music industry. Billboard publishes the weekly ranking of the 100 best songs in various categories such as rock, pop, hip-hop, etc. For this study, we used the top 100 hits of each week from 2001 to 2010. We collected the song lyrics from www.lyrics.com. Since the song ratings are given every week, there are many reruns of the same song that are available in several weeks. To simplify the problem, we chose the top rank of the song over the years as the rank of the song. After cleaning the lyrics from hypertext annotations and punctuation, we had a total of 2683 songs from 808 artists."}, {"heading": "Features", "text": "There are few analyses that perform an entire battery of linguistic algorithms that describe syntax, semantics, emotions, and the contribution of words contained in texts. Generally, these algorithms can be incorporated into general structural (e.g. word counting), syntactic (e.g. connecting), and semantic (e.g. word choice) dimensions of language, some of which use a dead end of the word, while others use a probability approach (MR), while others rely on the calculation of various factors (e.g. word typing)."}, {"heading": "Classification", "text": "After linguistic analysis, we approached the problem as a classification problem. As discussed earlier, we extracted the language characteristics from the texts using the linguistic algorithms shown in Figure 2. We extracted 261 characteristics from each of the 2616 songs. The goal is to build a classifier that predicts the top and bottom songs on the billboard. As there are many characteristics and very few songs, we removed the noise generated by the characteristics using Principal Component Analysis (PCA). Characteristics that explain the 0.6 variance were selected, and this reduced the characteristics to 39 out of 261. It is important to note that the main advantage of performing a PCA is noise reduction, and also identifying the best characteristics that capture the variance in the data. The disadvantage is that the variables lose their semantic meaning compared to the raw characteristics we have."}, {"heading": "Discussion", "text": "There are several studies (Mihalcea and Strapparava, 2012; Su, Fung and Auguin, 2013; Laurier, Grivolla and Herrera, 2008; Kim et al., 2010) that examined emotions in music using language, as well as few audio features. All studies explicitly showed that language characteristics were more useful than surface features in music to identify emotions in songs. Songs contain both music and text. In this work, we used only the text as our data. Songs \"lyrics are publicly available when compared with music. As previous studies have shown the importance of language in music for the identification of emotions, we have expanded the study to identify the language characteristics that help distinguish the top-rated songs on the billboard. To the best of our knowledge, this is a first study that uses computer-aided linguistic algorithms and machine learning models to predict whether a song belongs to the top or bottom of the leaderboard."}, {"heading": "Conclusion and Future Work", "text": "The music industry is a dynamic business world where many artists release their work in the form of albums, individual songs and performances. There is a huge financial incentive for companies to identify the songs that are most likely to be a hit. Machine learning allows them to train different language traits to predict whether a song is in the top 30 or bottom 30 of the Billboard ratings. In the future, we would like to expand our research to predict whether or not the song will reach the top 100 of the Billboard list."}], "references": [{"title": "The CELEX lexical database", "author": ["H.R. Baayen", "R. Piepenbrock", "L. Gulikers"], "venue": "release 2 (CD-ROM). Philadelphia, Pennsylvania: Linguistic Data Consortium, University of Pennsylvania.", "citeRegEx": "Baayen et al\\.,? 1995", "shortCiteRegEx": "Baayen et al\\.", "year": 1995}, {"title": "Variation across speech and writing", "author": ["D. Biber"], "venue": "Cambridge University Press.", "citeRegEx": "Biber,? 1991", "shortCiteRegEx": "Biber", "year": 1991}, {"title": "Web 1T 5-gram Version 1", "author": ["T. Brants", "A. Franz"], "venue": "Philadelphia: Linguistic Data Consortium.", "citeRegEx": "Brants and Franz,? 2006", "shortCiteRegEx": "Brants and Franz", "year": 2006}, {"title": "Smote: synthetic minority over-sampling technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "Journal of artificial intelligence research 321\u2013 357.", "citeRegEx": "Chawla et al\\.,? 2002", "shortCiteRegEx": "Chawla et al\\.", "year": 2002}, {"title": "The mrc psycholinguistic database", "author": ["M. Coltheart"], "venue": "The Quarterly Journal of Experimental Psychology 33(4):497\u2013505.", "citeRegEx": "Coltheart,? 1981", "shortCiteRegEx": "Coltheart", "year": 1981}, {"title": "Automatic detection of learner?s affect from conversational cues", "author": ["S. DMello", "S. Craig", "A. Witherspoon", "B. McDaniel", "A. Graesser"], "venue": "User Modeling and UserAdapted Interaction 18(1-2):45\u201380.", "citeRegEx": "DMello et al\\.,? 2008", "shortCiteRegEx": "DMello et al\\.", "year": 2008}, {"title": "Autotutor: An intelligent tutoring system with mixed-initiative dialogue", "author": ["A.C. Graesser", "P. Chipman", "B.C. Haynes", "A. Olney"], "venue": "Education, IEEE Transactions on 48(4):612\u2013618.", "citeRegEx": "Graesser et al\\.,? 2005", "shortCiteRegEx": "Graesser et al\\.", "year": 2005}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201904, 168\u2013177. New York, NY, USA: ACM.", "citeRegEx": "Hu and Liu,? 2004", "shortCiteRegEx": "Hu and Liu", "year": 2004}, {"title": "The language of emotions: An analysis of a semantic field", "author": ["P.N. Johnson-laird", "K. Oatley"], "venue": "Cognition and Emotion 3(2):81\u2013123.", "citeRegEx": "Johnson.laird and Oatley,? 1989", "shortCiteRegEx": "Johnson.laird and Oatley", "year": 1989}, {"title": "Music emotion recognition: A state of the art review", "author": ["Y.E. Kim", "E.M. Schmidt", "R. Migneco", "B.G. Morton", "P. Richardson", "J. Scott", "J.A. Speck", "D. Turnbull"], "venue": "Proc. ISMIR, 255\u2013266. Citeseer.", "citeRegEx": "Kim et al\\.,? 2010", "shortCiteRegEx": "Kim et al\\.", "year": 2010}, {"title": "Multimodal music mood classification using audio and lyrics", "author": ["C. Laurier", "J. Grivolla", "P. Herrera"], "venue": "Machine Learning and Applications, 2008. ICMLA\u201908. Seventh International Conference on, 688\u2013693. IEEE.", "citeRegEx": "Laurier et al\\.,? 2008", "shortCiteRegEx": "Laurier et al\\.", "year": 2008}, {"title": "An analytic and cognitive parametrization of coherence relations", "author": ["M. Louwerse"], "venue": "Cognitive Linguistics 12(3):291\u2013316.", "citeRegEx": "Louwerse,? 2001", "shortCiteRegEx": "Louwerse", "year": 2001}, {"title": "Tell us your story: Investigating the linguistic features of trauma narrative", "author": ["J.A. Luno", "J.G. Beck", "M. Louwerse"], "venue": "The Cognitive Science Society.", "citeRegEx": "Luno et al\\.,? 2013", "shortCiteRegEx": "Luno et al\\.", "year": 2013}, {"title": "Emotion and music: Inherent responses and the importance of empirical cross-cultural research", "author": ["C. McKay"], "venue": "Course Paper, McGill University, Canada.", "citeRegEx": "McKay,? 2002", "shortCiteRegEx": "McKay", "year": 2002}, {"title": "Lyrics, music, and emotions", "author": ["R. Mihalcea", "C. Strapparava"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 590\u2013599. Association for Computational Linguistics.", "citeRegEx": "Mihalcea and Strapparava,? 2012", "shortCiteRegEx": "Mihalcea and Strapparava", "year": 2012}, {"title": "Five Papers on WordNet", "author": ["G.A. Miller", "R. Beckwith", "C. Fellbaum", "D. Gross", "K. Miller"], "venue": "Fellbaum, C., ed., WordNet: An Electronic Lexical Database. MIT Press.", "citeRegEx": "Miller et al\\.,? 1998", "shortCiteRegEx": "Miller et al\\.", "year": 1998}, {"title": "Recommendations for the generalized intelligent framework for tutoring based on the development of the deeptutor tutoring service", "author": ["V. Rus", "N. Niraula", "M. Lintean", "R. Banjade", "D. Stefanescu", "W. Baggett"], "venue": "AIED 2013 Workshops Proceedings Volume 7, 116.", "citeRegEx": "Rus et al\\.,? 2013", "shortCiteRegEx": "Rus et al\\.", "year": 2013}, {"title": "A circumplex model of affect", "author": ["J.A. Russell"], "venue": "Journal of personality and social psychology 39(6):1161.", "citeRegEx": "Russell,? 1980", "shortCiteRegEx": "Russell", "year": 1980}, {"title": "The cognitive functions of linguistic categories in describing persons: Social cognition and language", "author": ["G.R. Semin", "K. Fiedler"], "venue": "Journal of Personality and Social Psychology 54(4):558\u2013568.", "citeRegEx": "Semin and Fiedler,? 1988", "shortCiteRegEx": "Semin and Fiedler", "year": 1988}, {"title": "The linguistic category model, its bases, applications and range", "author": ["G.R. Semin", "K. Fiedler"], "venue": "European Review of Social Psychology 2(1):1\u201330.", "citeRegEx": "Semin and Fiedler,? 1991", "shortCiteRegEx": "Semin and Fiedler", "year": 1991}, {"title": "Multimodal music emotion classification using adaboost with decision stumps", "author": ["D. Su", "P. Fung", "N. Auguin"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, 3447\u20133451.", "citeRegEx": "Su et al\\.,? 2013", "shortCiteRegEx": "Su et al\\.", "year": 2013}, {"title": "The psychological meaning of words: Liwc and computerized text analysis methods", "author": ["Y.R. Tausczik", "J.W. Pennebaker"], "venue": "Journal of Language and Social Psychology 29(1):24\u201354.", "citeRegEx": "Tausczik and Pennebaker,? 2010", "shortCiteRegEx": "Tausczik and Pennebaker", "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "Identifying these features has helped computational linguists as well as computer scientists to correlate the language features with several complex problems arising in tutoring systems (Rus et al., 2013; Graesser et al., 2005), affect recognition(DMello et al.", "startOffset": 186, "endOffset": 227}, {"referenceID": 6, "context": "Identifying these features has helped computational linguists as well as computer scientists to correlate the language features with several complex problems arising in tutoring systems (Rus et al., 2013; Graesser et al., 2005), affect recognition(DMello et al.", "startOffset": 186, "endOffset": 227}, {"referenceID": 5, "context": ", 2005), affect recognition(DMello et al., 2008), sentiment mining (Hu and Liu, 2004), opinion mining, and many others.", "startOffset": 27, "endOffset": 48}, {"referenceID": 7, "context": ", 2008), sentiment mining (Hu and Liu, 2004), opinion mining, and many others.", "startOffset": 26, "endOffset": 44}, {"referenceID": 17, "context": "They have shown that language features extracted from the songs fit well with Russel\u2019s valence(negative-positive) and arousal(inactive-active) model (Russell, 1980).", "startOffset": 149, "endOffset": 164}, {"referenceID": 13, "context": "Several cross-cultural studies show evidence for universal emotional cues in music and language across different cultures and traditions (McKay, 2002).", "startOffset": 137, "endOffset": 150}, {"referenceID": 9, "context": "While significant advances have been made in the area of emotion detection and mood classification based on music and lyrics analysis, through large-scale machine learning operating on vast feature sets, sometimes spanning multiple domains, applied to relatively short musical selections (Kim et al., 2010).", "startOffset": 288, "endOffset": 306}, {"referenceID": 1, "context": "For general linguistic features, we used the frequency of 67 linguistic features described by (Biber, 1991).", "startOffset": 94, "endOffset": 107}, {"referenceID": 15, "context": "For semantic categories of the words, we used Wordnet (Miller et al., 1998).", "startOffset": 54, "endOffset": 75}, {"referenceID": 8, "context": "Louwerse (2001), Biber (1991), Semin and Fiedler (1988, 1991), Johnson-laird and Oatley (1989), Miller et al.", "startOffset": 0, "endOffset": 16}, {"referenceID": 1, "context": "Louwerse (2001), Biber (1991), Semin and Fiedler (1988, 1991), Johnson-laird and Oatley (1989), Miller et al.", "startOffset": 17, "endOffset": 30}, {"referenceID": 1, "context": "Louwerse (2001), Biber (1991), Semin and Fiedler (1988, 1991), Johnson-laird and Oatley (1989), Miller et al.", "startOffset": 17, "endOffset": 95}, {"referenceID": 1, "context": "Louwerse (2001), Biber (1991), Semin and Fiedler (1988, 1991), Johnson-laird and Oatley (1989), Miller et al. (1998), Coltheart (1981), Baayen, Piepenbrock, and Gulikers (1995), Tausczik and Pennebaker (2010)", "startOffset": 17, "endOffset": 117}, {"referenceID": 1, "context": "Louwerse (2001), Biber (1991), Semin and Fiedler (1988, 1991), Johnson-laird and Oatley (1989), Miller et al. (1998), Coltheart (1981), Baayen, Piepenbrock, and Gulikers (1995), Tausczik and Pennebaker (2010)", "startOffset": 17, "endOffset": 135}, {"referenceID": 1, "context": "Louwerse (2001), Biber (1991), Semin and Fiedler (1988, 1991), Johnson-laird and Oatley (1989), Miller et al. (1998), Coltheart (1981), Baayen, Piepenbrock, and Gulikers (1995), Tausczik and Pennebaker (2010)", "startOffset": 17, "endOffset": 177}, {"referenceID": 1, "context": "Louwerse (2001), Biber (1991), Semin and Fiedler (1988, 1991), Johnson-laird and Oatley (1989), Miller et al. (1998), Coltheart (1981), Baayen, Piepenbrock, and Gulikers (1995), Tausczik and Pennebaker (2010)", "startOffset": 17, "endOffset": 209}, {"referenceID": 2, "context": "We also collected all the English words from Google unigrams (Brants and Franz, 2006) and binned them into one of the 44 categories if one of their synonyms belonged to those categories.", "startOffset": 61, "endOffset": 85}, {"referenceID": 21, "context": "To capture the various emotions expressed by the statement, we have used the emotion words given by (Tausczik and Pennebaker, 2010), classified into two classes broadly basic emotions (anger, fear, disgust, happiness, etc.", "startOffset": 100, "endOffset": 131}, {"referenceID": 8, "context": "Also, we used the MRC Psycholinguistic Database (Johnson-laird and Oatley, 1989), to get linguistic measures such as familiarity, concreteness, and meaningfulness.", "startOffset": 48, "endOffset": 80}, {"referenceID": 3, "context": "5 to 1, and to balance the classes we performed synthetic minority oversampling (SMOTE) (Chawla et al., 2002).", "startOffset": 88, "endOffset": 109}, {"referenceID": 14, "context": "There are several studies (Mihalcea and Strapparava, 2012; Su, Fung, and Auguin, 2013; Laurier, Grivolla, and Herrera, 2008; Kim et al., 2010) that have looked into emotions in music based on language as well as few audio features.", "startOffset": 26, "endOffset": 142}, {"referenceID": 9, "context": "There are several studies (Mihalcea and Strapparava, 2012; Su, Fung, and Auguin, 2013; Laurier, Grivolla, and Herrera, 2008; Kim et al., 2010) that have looked into emotions in music based on language as well as few audio features.", "startOffset": 26, "endOffset": 142}], "year": 2015, "abstractText": "The music industry is a $130 billion industry. Predicting whether a song catches the pulse of the audience impacts the industry. In this paper we analyze language inside the lyrics of the songs using several computational linguistic algorithms and predict whether a song would make to the top or bottom of the billboard rankings based on the language features. We trained and tested an SVM classifier with a radial kernel function on the linguistic features. Results indicate that we can classify whether a song belongs to top and bottom of the billboard charts with a precision of 0.76.", "creator": "LaTeX with hyperref package"}}}