{"id": "1512.01322", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2015", "title": "Fixed-Point Performance Analysis of Recurrent Neural Networks", "abstract": "Recurrent neural networks have shown excellent performance in many applications, however they require increased complexity in hardware or software based implementations. The hardware complexity can be much lowered by minimizing the word-length of weights and signals. This work analyzes the fixed-point performance of recurrent neural networks using a retrain based quantization method. The quantization sensitivity of each layer in RNNs is studied, and the overall fixed-point optimization results minimizing the capacity of weights while not sacrificing the performance are presented. A language model and a phoneme recognition examples are used.", "histories": [["v1", "Fri, 4 Dec 2015 06:07:28 GMT  (610kb,D)", "http://arxiv.org/abs/1512.01322v1", null], ["v2", "Sat, 23 Jan 2016 11:53:34 GMT  (611kb,D)", "http://arxiv.org/abs/1512.01322v2", null], ["v3", "Tue, 27 Sep 2016 11:42:34 GMT  (611kb,D)", "http://arxiv.org/abs/1512.01322v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["sungho shin", "kyuyeon hwang", "wonyong sung"], "accepted": false, "id": "1512.01322"}, "pdf": {"name": "1512.01322.pdf", "metadata": {"source": "CRF", "title": "FIXED-POINT PERFORMANCE ANALYSIS OF RECURRENT NEURAL NETWORKS", "authors": ["Sungho Shin", "Kyuyeon Hwang", "Wonyong Sung"], "emails": ["shshin@dsp.snu.ac.kr,", "khwang@dsp.snu.ac.kr,", "wysung@snu.ac.kr"], "sections": [{"heading": null, "text": "Index terms - recurring neural network, quantization, word length optimization, fixed point optimization, deep neural network"}, {"heading": "1. INTRODUCTION", "text": "Recursive neural networks (RNNs) deal with feedback path within, and they are suitable for processing input data whose dimension is not fixed. Speech models for automatic speech recognition systems, human action recognition, and text generation are important applications with RNNNs [1, 2, 3]. Among many types of RNNs [4, 5, 6], the most powerful is short-term memory (LSTM) RNN [5]. A standard LSTM RNN is represented in Figure 1. A layer of LSTM RNN contains an input gate, an output gate, and a forward gate. An LSTM layer of N units requires a total of about 4N2 + 4NM + 5N weights in which M is the previous layer size. Considering a language level language model that employs three 1024 large LSTM layers, the trained network layers require the weights, about 22.3 megahertz."}, {"heading": "2. RETRAIN-BASED WEIGHT QUANTIZATION", "text": "The retraining method is based on the number of quantization steps. In this method, the floating point weights for RNNs are prepared in a similar way as the adaptation of a forward-facing neuronal network with reverse propagation. For direct quantization, a uniform quantization function is applied and the function Q (\u00b7) is defined as follows: Q (w) = sgn (w) = \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p = \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p = \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p p p p p p = p \u00b7 p \u00b7 p \u00b7 p \u00b7 p p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 \u00b7 p \u00b7 p \u00b7 \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p"}, {"heading": "3. QUANTIZATION SENSITIVITY ANALYSIS", "text": "LSTM RNNs usually contain millions of weights and thousands of signals. Therefore, it is necessary to group them according to their range and quantization sensitivity [15]. Fortunately, a neural network can easily be grouped into 7 groups, which are in-L1, L1, L1-L2, L2, L2-L3, L3 and L3-Out groups, in which In-L1 connects the input and the first LSTM layer and L1 is the recursive path in the first level LSTM. Figure1. b) illustrates the grouping of these units. In sensitivity analysis, we only quantify the weights of the selected group \u2212 \u2212 \u2212 tantantic \u2212 in the layer and L1 is the recursive path in the first level LSTM. Figure1. b) illustrates the grouping of these units (STx)."}, {"heading": "4. EXPERIMENTAL RESULTS", "text": "In this section we will first show the result of the layer-by-layer sensitivity analysis for signals and weights and then the fully quantified network performance. The proposed quantization strategy will be evaluated using two RNNs, one for phoneme recognition and the other for character-level language modelling. Advanced training techniques such as early pausing, adaptive learning rate and Nesterow dynamics will be used."}, {"heading": "4.1. Phoneme Recognition", "text": "The detailed experimental conditions are identical to [17], except for the figure of output classes for scoring; the input layer consists of 123 linear units to accept real inputs; three hidden LSTM layers are of the same size of 512; the output layer consists of 61 Softmax units corresponding to 61 target phoneme labels; the network is trained by fractal with training parameters comprising 32 forward steps and 64 backward steps with 64 streams [18]; the initial learning rate was 10 \u2212 5 and is reduced to 10 \u2212 7 with correct timing; the momentum was 0.9 [19] and adadelta was applied for the weight actualization [20]; the network requires approximately 5.5 megaweights; as a result, it requires about 22 MB with a 32-bit gliding commercial format; Figure 2 shows the result of the layered sensation analysis for the weights; the original SToneme fraction group consists of only 5 megaweights; the total SToneme performance group consists of ST27.5% of the STM, with only three M."}, {"heading": "2-2-3-4-4-4-6 (10.52%) 6-6-7 1.848 1.573", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3-3-4-5-5-5-7 (13.64%) 6-6-7 1.770 1.537", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2-2-3-4-4-4-6 (10.52%) 7-7-8 5.113 1.573", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3-3-4-5-5-5-7 (13.64%) 7-7-8 1.970 1.523", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4-4-5-6-6-6-8 (16.75%) 8-8-9 1.560 1.500", "text": "for the floating-point version."}, {"heading": "4.2. Language Model", "text": "A character-level language model was trained using the English Wikipedia dataset used in [21]. Each character was entered into the neural network with its own ASCII code values, requiring 256 units. The input layer contains 256 linear units to accommodate real value inputs. Three hidden LSTM layers are the same size as 1024. The output layer consists of 256 Softmax units corresponding to 256 characters at the character level One-Hot-Coding. The network is trained using a fractal with training parameters showing 128 forward steps and 128 backward steps with 64 streams [18]. The learning rate was reduced from 10 \u2212 6 to 10 \u2212 8 during training. Momentum was 0.9 and adadelta was used for weight analysis. This network requires approximately 22.3 megaweights. Figure 3 shows the results of the proven sensitivity analysis for weights."}, {"heading": "5. CONCLUDING REMARKS", "text": "This paper examines the fixed-point characteristics of RNs for phoneme recognition and speech modeling. Retrain-based fixed-point optimization significantly reduces the word length of weights and signals. In the phoneme recognition example, most weights can be represented in 3 bits, while voice modeling requires 5 or 6 bits to achieve approximately floating-point results. By optimizing this, the required weighting capacity can be reduced to only 10% or 17% of what is required for floating-point implementations. Reduced weights and signals can lead to efficient hardware implementations or a higher hit rate of cache memory in software-based systems."}, {"heading": "6. REFERENCES", "text": "[1]..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}], "references": [{"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan Honza \u010cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5528\u20135531.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequential deep learning for human action recognition", "author": ["Moez Baccouche", "Franck Mamalet", "Christian Wolf", "Christophe Garcia", "Atilla Baskurt"], "venue": "Human Behavior Understanding, pp. 29\u201339. Springer, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "arXiv preprint arXiv:1411.4555, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "The echo state approach to analysing and training recurrent neural networks-with an erratum note", "author": ["Herbert Jaeger"], "venue": "Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, vol. 148, pp. 34, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Neural network adaptations to hardware implementations", "author": ["Perry Moerland", "Emile Fiesler"], "venue": "Tech. Rep., IDIAP, 1997.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Finite precision error analysis of neural network hardware implementations", "author": ["Jordan L Holi", "Jenq-Neng Hwang"], "venue": "Computers, IEEE Transactions on, vol. 42, no. 3, pp. 281\u2013290, 1993.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "The effects of quantization on multilayer neural networks", "author": ["Gunhan Dundar", "Kenneth Rose"], "venue": "IEEE transactions on neural networks/a publication of the IEEE Neural Networks Council, vol. 6, no. 6, pp. 1446\u2013 1451, 1994.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "Hardware accelerated convolutional neural networks for synthetic vision systems", "author": ["Cl\u00e9ment Farabet", "Berin Martini", "Polina Akselrod", "Sel\u00e7uk Talay", "Yann LeCun", "Eugenio Culurciello"], "venue": "Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. IEEE, 2010, pp. 257\u2013260.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Multilayer feedforward neural networks with single powers-of-two weights", "author": ["Chuan Zhang Tang", "Hon Keung Kwan"], "venue": "Signal Processing, IEEE Transactions on, vol. 41, no. 8, pp. 2724\u20132727, 1993.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1993}, {"title": "Fixed-point feedforward deep neural network design using weights+ 1, 0, and- 1", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "Signal Processing Systems (SiPS), 2014 IEEE Workshop on. IEEE, 2014, pp. 1\u20136.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 1131\u20131135.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}, {"title": "Simulation-based word-length optimization method for fixed-point digital signal processing systems", "author": ["Wonyong Sung", "Ki-II Kum"], "venue": "Signal Processing, IEEE Transactions on, vol. 43, no. 12, pp. 3087\u20133090, 1995.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1", "author": ["John S Garofolo", "Lori F Lamel", "William M Fisher", "Jonathon G Fiscus", "David S Pallett"], "venue": "NASA STI/Recon Technical Report N, vol. 93, pp. 27403, 1993.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Single stream parallelization of generalized lstm-like rnns on a gpu", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "arXiv preprint arXiv:1503.02852, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Increased rates of convergence through learning rate adaptation", "author": ["Robert A Jacobs"], "venue": "Neural networks, vol. 1, no. 4, pp. 295\u2013307, 1988.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1988}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1017\u20131024.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Language models for automatic speech recognition systems, human action recognition and text generation are important applications using RNNs [1, 2, 3].", "startOffset": 141, "endOffset": 150}, {"referenceID": 1, "context": "Language models for automatic speech recognition systems, human action recognition and text generation are important applications using RNNs [1, 2, 3].", "startOffset": 141, "endOffset": 150}, {"referenceID": 2, "context": "Language models for automatic speech recognition systems, human action recognition and text generation are important applications using RNNs [1, 2, 3].", "startOffset": 141, "endOffset": 150}, {"referenceID": 3, "context": "Among many types of RNNs [4, 5, 6], the most powerful one is the long short term memory (LSTM) RNN [5].", "startOffset": 25, "endOffset": 34}, {"referenceID": 4, "context": "Among many types of RNNs [4, 5, 6], the most powerful one is the long short term memory (LSTM) RNN [5].", "startOffset": 25, "endOffset": 34}, {"referenceID": 5, "context": "Among many types of RNNs [4, 5, 6], the most powerful one is the long short term memory (LSTM) RNN [5].", "startOffset": 25, "endOffset": 34}, {"referenceID": 4, "context": "Among many types of RNNs [4, 5, 6], the most powerful one is the long short term memory (LSTM) RNN [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "quantization [7, 8, 9, 10].", "startOffset": 13, "endOffset": 26}, {"referenceID": 7, "context": "quantization [7, 8, 9, 10].", "startOffset": 13, "endOffset": 26}, {"referenceID": 8, "context": "quantization [7, 8, 9, 10].", "startOffset": 13, "endOffset": 26}, {"referenceID": 9, "context": "quantization [7, 8, 9, 10].", "startOffset": 13, "endOffset": 26}, {"referenceID": 10, "context": "Retraining based quantization that readjust the fixed-point weights by training after direct quantization of floating-point parameters was adopted in [11, 12, 13].", "startOffset": 150, "endOffset": 162}, {"referenceID": 11, "context": "Retraining based quantization that readjust the fixed-point weights by training after direct quantization of floating-point parameters was adopted in [11, 12, 13].", "startOffset": 150, "endOffset": 162}, {"referenceID": 12, "context": "Retraining based quantization that readjust the fixed-point weights by training after direct quantization of floating-point parameters was adopted in [11, 12, 13].", "startOffset": 150, "endOffset": 162}, {"referenceID": 11, "context": "The retrain based quantization method includes the fixedpoint conversion process inside of the training procedure so that the network learns the quantization effects [12].", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "In this method, the floating-point weights for RNNs are prepared with the backpropagation through time (BPTT) algorithm [14].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "For selecting a proper step size, L2 error minimization criteria is applied as adopted in [12].", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "Therefore, it is necessary to group them according to their range and the quantization sensitivity [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 15, "context": "Phoneme recognition experiments were performed on the TIMIT corpus [16].", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "The detailed experimental conditions are the same with [17] except the mapping of output classes for scoring.", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "The network is trained using Fractal with training parameters that are 32 forward steps and 64 backward steps with 64 streams [18].", "startOffset": 126, "endOffset": 130}, {"referenceID": 18, "context": "9 [19] and adadelta was adopted for weights update [20].", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "9 [19] and adadelta was adopted for weights update [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "A character level language model was trained using English Wikipedia dataset which was used in [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 17, "context": "The network is trained using Fractal with training parameters that are 128 forward steps and 128 backward steps with 64 streams [18].", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "Recurrent neural networks have shown excellent performance in many applications, however they require increased complexity in hardware or software based implementations. The hardware complexity can be much lowered by minimizing the word-length of weights and signals. This work analyzes the fixed-point performance of recurrent neural networks using a retrain based quantization method. The quantization sensitivity of each layer in RNNs is studied, and the overall fixedpoint optimization results minimizing the capacity of weights while not sacrificing the performance are presented. A language model and a phoneme recognition examples are used.", "creator": "LaTeX with hyperref package"}}}