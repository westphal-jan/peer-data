{"id": "1512.01332", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2015", "title": "Q-Networks for Binary Vector Actions", "abstract": "In this paper reinforcement learning with binary vector actions was investigated. We suggest an effective architecture of the neural networks for approximating an action-value function with binary vector actions. The proposed architecture approximates the action-value function by a linear function with respect to the action vector, but is still non-linear with respect to the state input. We show that this approximation method enables the efficient calculation of greedy action selection and softmax action selection. Using this architecture, we suggest an online algorithm based on Q-learning. The empirical results in the grid world and the blocker task suggest that our approximation architecture would be effective for the RL problems with large discrete action sets.", "histories": [["v1", "Fri, 4 Dec 2015 07:51:48 GMT  (832kb)", "http://arxiv.org/abs/1512.01332v1", "9 pages, 5 figures, accepted for Deep Reinforcement Learning Workshop, NIPS 2015"]], "COMMENTS": "9 pages, 5 figures, accepted for Deep Reinforcement Learning Workshop, NIPS 2015", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["naoto yoshida"], "accepted": false, "id": "1512.01332"}, "pdf": {"name": "1512.01332.pdf", "metadata": {"source": "CRF", "title": "Q-Networks for Binary Vector Actions\u2217", "authors": ["Naoto Yoshida"], "emails": ["naotoyoshida@pfsl.mech.tohoku.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.01 332v 1 [cs.N E] 4D ecIn this paper, learning with binary vector actions has been studied. We propose an effective architecture of neural networks to approximate an action-value function with binary vector actions. The proposed architecture approaches the action-value function by a linear function in relation to the action vector, but is still not linear in relation to the state input. We show that this approach method enables the efficient calculation of greedy action selection and Softmax action selection. With this architecture, we propose an online algorithm based on Q-Learning. Empirical results in the grid world and the blocker task suggest that our approach architecture would be effective for RL problems with large discrete action sets."}, {"heading": "1 Introduction", "text": "One of the great challenges in reinforcement learning (RL) is learning in high-dimensional state action spaces (e.g. ATARI game tasks) [1]. Although several approaches to RL with continuous actions are proposed [2], RL with a large scope for action is still problematic, especially when we treat binary vectors as representations of actions. The difficulty is that the number of actions grows exponentially as the binary vector grows. Recursively, several approaches have been used to address this problem. Sallans & Hinton proposed an energy-based approach in which limited Boltzmann machines [4] were incorporated into the algorithm and their free energy was used exponentially as a functional approximator."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Markov Decision Process and Reinforcement Learning", "text": "The value-oriented amplification learning algorithms use the Markov Decision Assumption (MDP), which is defined by a tuple < S, A, P, R >. S is the specified state, A is the specified probability of action, P is the transition probability P (s \u00b2 | s, a), where s \u00b2 S is the next state to be given a state action pair (s, a). Finally, R is the average reward function R (s, a) = E [r | s, a] and r is the reward sample. In the value-oriented RL, the action-value function Q\u03c0 (s, a) is defined by Q\u03c0 (s (s, a) = E\u03c0."}, {"heading": "2.2 Q-learning with Function Approximation", "text": "In the Q-Learning algorithm with function approximations, we approach the optimal value function by the function Q\u03b8 (s, a), where \u03b8 is the parameter of the function. Gradient-based updating of the function Q\u03b8 (s, a) calculates the gradient of the error functionL = 12 (T \u2212 Q\u03b8 (s, a) 2, (2), where T is the target signal. Then, the gradient of the error is determined by a transition sample (s, a, s). Then, the direction of the parameter updating is determined at its own discretion. (3) The target signal in Q-Learning is T = r + g max."}, {"heading": "3 Proposed Method", "text": "In this study, we assume that the function approximation is performed by the multi-layer perceptrons (MLPs) shown in Figure 1. In order to efficiently calculate the maximum operations in Q-Learning with a large, discrete action space, we propose the network architecture of the MLPs. In this architecture, the outputs of the network are composed of a continuous scalar variable and continuous vector variables. In this study, we approach the action value function by the linear function in relation to the action vector: Q\u03b8 (s, a) = experience vector and ai is the i-th component of the action.The gradient of the function QTB (s, a) = experience lead (s) + an experience lead (s) (7) Here, a is the action represented by the binary vector, and ai is the i-th component of the action.The gradient of the function Qs, a) is given by the effect of the Qs (TB), a (TB) and TB (TB)."}, {"heading": "3.1 Sampling of the Actions", "text": "The proposed approximation architecture provides an efficient calculation of the greedy action. For actions with the most uniform representation, greedy politics is obvious. This is the second hottest (s) = second hottest (s). For K-bits binary vector actions, the sampling of greedy actions relative to function 7 is still tradable. The i-th element of the greedy action vector is the second hottest (s) action. (s) < 0 1 otherwise. (11) Since we can still make the greedy actions related to function 7 tradable, the i-th element of the greedy action vector is the second hottest action."}, {"heading": "4 Experiment", "text": "In the experiment, we tested our proposed architecture in several areas. In all experiments, we used the three-layer MLPs described in Figure 1. We also adjusted the activation function of the hidden units based on the linear rectifier units (ReLU). All weights associated with the output units are scanned from the uniform distribution via [\u2212 0.01, 0.01], and all weights between input units and hidden units are scanned from the uniform distribution via [\u2212 \u221a 6 / \u221a Nhidden + Ninput, \u221a 6 / \u221a Nhidden + Ninput], where Nhidden and Ninput are the number of units in the layers. The parameter was updated by stochastic gradient descent with a constant step size \u03b1 = 0.01. The discount rate of the objective function in RL is also the same in all experiments, so we used \u03b3 = 0.95."}, {"heading": "4.1 Grid World with One-hot Representation", "text": "First, we tested our algorithm in the conventional grid world with a unified representation, which is the problem with the shortest path in the grid world, as Sutton & Barto suggests. [12] The state4space consists of 47 individual states and they are given by the unified representation; the agent has 4 different actions corresponding to the four directional movements (North, South, East, West); the action in this experiment is represented by a unified representation (for example, the \"North\" action corresponds to the vector (1, 0, 0)); the agent receives a zero reward when the agent reaches the target, but otherwise receives a \u2212 1 reward; the agent was trained in episodic fashion; a single episode was terminated when the agent reached the target or exceeded 800 time steps in the episode; the agents were implemented by MLPs with 50 hidden units."}, {"heading": "4.2 Grid World with 4-bit Binary Vector Actions", "text": "Action Binary VectorNorth 1,1,0,0South 0,0,1,1East 1,0,1,0West 0,1,0,1Stay Elsewhere Right panel of Figure 3 The horizontal axis represents the number of episodes, the vertical axis the step size in the episode. The black line is the mean power of 10 runs and the bars are standard deviations. The broken line is the optimal step size. Here, too, the agent successfully achieved the optimal policy through the experiment, even in the area of binary vector action. This result shows that the proposed method has successfully improved the behavior of the agent without Monte Calro-based samples of the actions, even if the representation of the actions is not the most uniform representation."}, {"heading": "4.3 Grid World with Population Coding", "text": "In this experiment, the action is represented by a 40-bit binary vector, and the movements of the agent5 are driven by the type of population coding. Specifically, when the environment receives a 40-bit vector, one of the four-direction steps (1: north; 2: south; 4: west) or the residence behavior (5: stay) becomes according to the probability Pj = 5 k = 1 ekj = 1, 3, 5, 5 (20), where Ej is given by the action."}, {"heading": "4.4 Blocker", "text": "The blocker is the multi-agent task proposed by Sallans and Hinton [5] [11]. This environment consists of a 4-7 grid, three agents and two pre-programmed blockers. Agents and blockers never overlap in the grid. To get a positive reward, agents must cooperate in this environment. The \"team\" of agents receives a + 1 reward if one of the three agents enters the end zone, otherwise the team receives a \u2212 1 reward. The state vector is given as a 141 binary vector consisting of the positions (grid cells) of all agents (28-bit-3-agents). The easternmost position of each blocker (28-bit-2-blocker) and a bias bit that is always one (1-bit). Each agent can move in any of the four directions. Therefore, the size of the action space is 43-bit-3-bit-agent task next."}, {"heading": "5 Discussion", "text": "In the environment with uniform representation behavior, the linear functional approximation of the action value corresponds to the bilinear function with respect to the action vector and the state vector Q\u03b8 (s, a) = a \u03b8s. (23) 7In this case, the parameter \u03b8 is given as a matrix. If the state is given by a uniform representation, this approximation is identical to the tabular representation. As suggested in our method, the linear architecture allows an efficient scanning of the greedy action with respect to the action. More recently, Mnih et al. have proposed a DQN architecture [10]. In this case, we evaluate the action values corresponding to all discrete actions by a single forward propagation. And then, the training of the approximator is carried out only on the basis of the output corresponding to the selected action. This architecture can be performed as a linear functional approximation with respect to the action QTB (s, a) as an approximate approach (s, b)."}, {"heading": "6 Conclusion", "text": "In this paper, we propose a novel architecture of multi-layer perceptrons for RL with a large discrete action set. In our architecture, the action-value function is approximated by a linear function in relation to the vector actions. This approximation method allows us to efficiently try out the greedy policies and softmax policies. Thus, the Q-learning-based off-policy algorithm is comprehensible in our architecture without any Monte Carlo approximations. We have empirically tested our method in several discrete action areas and the results supported its effectiveness. Based on these promising results, we expect to extend our approach in future work to include deep architectures."}], "references": [{"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Reinforcement learning of motor skills in high dimensions: A path integral approach", "author": ["Evangelos Theodorou", "Jonas Buchli", "Stefan Schaal"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Information processing in dynamical systems: foundations of harmony theory", "author": ["P Smolensky"], "venue": "In Parallel distributed processing: explorations in the microstructure of cognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1986}, {"title": "Using free energies to represent q-values in a multiagent reinforcement learning task", "author": ["Brian Sallans", "Geoffrey E Hinton"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Actor-critic reinforcement learning with energy-based policies", "author": ["Nicolas Heess", "David Silver", "Yee Whye Teh"], "venue": "In EWRL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Learning from delayed rewards", "author": [], "venue": "PhD thesis, University of Cambridge,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["Long-Ji Lin"], "venue": "Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Reinforcement learning with factored states and actions", "author": ["Brian Sallans", "Geoffrey E Hinton"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "ATARI game plays) [1].", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "Even though several approaches are suggested for RL with continuous actions [2][3], RL with a large action space is still problematic, especially when we treat binary vectors as representations of the actions.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "Even though several approaches are suggested for RL with continuous actions [2][3], RL with a large action space is still problematic, especially when we treat binary vectors as representations of the actions.", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "Sallans & Hinton suggested an energy-based approach in which restricted Boltzmann machines [4] were adopted in the algorithm and their free energy was used as the function approximator [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "Sallans & Hinton suggested an energy-based approach in which restricted Boltzmann machines [4] were adopted in the algorithm and their free energy was used as the function approximator [5].", "startOffset": 185, "endOffset": 188}, {"referenceID": 5, "context": "followed their energy-based approach and investigated natural actor-critic algorithms with energybased policies by RBMs [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "Q-learning is an algorithm for finding the optimal policy in MDP [7], and the advantage of Q-learning is its off-policy property: the agent can directly approximate the action-value of an optimal policy \u03c0 while following the other policy \u03c0.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "Using this gradient, the stochastic gradient descent or more sophisticated gradient-based algorithms are used for approximating the optimal action-value function [9][10].", "startOffset": 162, "endOffset": 165}, {"referenceID": 8, "context": "Using this gradient, the stochastic gradient descent or more sophisticated gradient-based algorithms are used for approximating the optimal action-value function [9][10].", "startOffset": 165, "endOffset": 169}, {"referenceID": 4, "context": "However, if the actions are composed of binary vectors or factored representation [5][11], the number of total actions exponentially grows and quickly become intractable.", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "However, if the actions are composed of binary vectors or factored representation [5][11], the number of total actions exponentially grows and quickly become intractable.", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "When the environment is represented by the factored MDP [5][11], the action may be represented by the binary vector, which is composed of a concatenation of one-hot representation vectors (for example, the agent may have to decide one of 2 options and one of 3 options simultaneously.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "When the environment is represented by the factored MDP [5][11], the action may be represented by the binary vector, which is composed of a concatenation of one-hot representation vectors (for example, the agent may have to decide one of 2 options and one of 3 options simultaneously.", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "This task is the shortest-path problem in the grid world, as suggested by Sutton & Barto [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "4 Blocker The blocker is the multi-agent task suggested by Sallans and Hinton [5][11].", "startOffset": 78, "endOffset": 81}, {"referenceID": 9, "context": "4 Blocker The blocker is the multi-agent task suggested by Sallans and Hinton [5][11].", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "proposed a DQN architecture [10].", "startOffset": 28, "endOffset": 32}], "year": 2015, "abstractText": "In this paper reinforcement learning with binary vector actions was investigated. We suggest an effective architecture of the neural networks for approximating an action-value function with binary vector actions. The proposed architecture approximates the action-value function by a linear function with respect to the action vector, but is still non-linear with respect to the state input. We show that this approximation method enables the efficient calculation of greedy action selection and softmax action selection. Using this architecture, we suggest an online algorithm based on Q-learning. The empirical results in the grid world and the blocker task suggest that our approximation architecture would be effective for the RL problems with large discrete action sets.", "creator": "LaTeX with hyperref package"}}}