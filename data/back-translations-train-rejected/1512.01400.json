{"id": "1512.01400", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2015", "title": "Max-Pooling Dropout for Regularization of Convolutional Neural Networks", "abstract": "Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.", "histories": [["v1", "Fri, 4 Dec 2015 13:18:37 GMT  (212kb)", "http://arxiv.org/abs/1512.01400v1", "The journal version of this paper [arXiv:1512.00242] has been published in Neural Networks,this http URL"]], "COMMENTS": "The journal version of this paper [arXiv:1512.00242] has been published in Neural Networks,this http URL", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["haibing wu", "xiaodong gu"], "accepted": false, "id": "1512.01400"}, "pdf": {"name": "1512.01400.pdf", "metadata": {"source": "CRF", "title": "Max-Pooling Dropout for Regularization of Convolutional Neural Networks", "authors": ["Haibing Wu"], "emails": ["haibingwu13@fudan.edu.cn,", "xdgu@fudan.edu.cn"], "sections": [{"heading": null, "text": "This paper shows that dropouts from the Max Pooling System are randomly selected activations based on a multinomial distribution during training. In light of this finding, we support the use of our proposed probabilistically weighted pooling instead of the commonly used Max Pooling to act as a model mean during the test. Empirical evidence confirms the superiority of probabilistically weighted pooling. We also compare Max Pooling dropouts and stochastic pooling, both of which introduce stochasticity based on multinomial distributions during pooling.Keywords: Deep learning, Convolutionary neural Network, Max Pooling dropouts"}, {"heading": "1 Introduction", "text": "In this context, it should be noted that this is not a mere formality, but a mere formality."}, {"heading": "2 Related Work", "text": "Pioneering work by Hinton et al. [1], dropouts applied only to fully connected layers. Therefore, they formed a very large convolutionary neural network to classify 1.2 million ImageNet images. Two primary methods were used to reduce overadjustment in their experiments.The first was data augmentation, a simplest and most commonly used approach to reduce overadjustment of image data. Dropouts was precisely the second method. Moreover, it was applied only in fully connected layers. Stochastic pooling [4] is a method of data overadjustment, a simplest and most commonly used method to reduce overadjustment of image data. Instead of always capturing the strongest activity within each pooling region, it was applied only in fully connected layers. Stochastic pooling [4] is a method that dropouts-inspired regulation methods of networks [do not directly maximize other]."}, {"heading": "3 Max-Pooling Dropout", "text": "We now show that max pooling dropout is synonymous with sampling activation."}, {"heading": "3.1 Max-Pooling Dropout at Training Time", "text": "It is about the question of whether and how people will be able to help themselves. (...) It is about the question of whether they are ready to help themselves. (...) It is about the question of whether they are ready to help themselves. (...) It is about the question of whether they are ready to help themselves. (...) It is about the question of whether they are ready to help themselves. (...) It is about the question of whether they are ready to support themselves. (...) \"(...) It is about the question of whether they are ready to help themselves. (...)\" (...). \"(...) It is about the question of whether they are ready.\" (...) \"(...) It is about the question of whether they are ready.\" (...). \"(...) It is about the question of whether they are ready.\" (...). \"(...) It is about.\" (...). \"It is about.\" (...). \"It is about.\" (...). \"It is about.\" (...). \"It is about.\" (...). \"It is about.\" (... \"It is about.\" It is about. \"(...).\" It is about. \"(...\" It is about. \"It is about.\" (...). \"It is about.\" (...). \"It is about.\" (... \"It is about.\" (...). \"It is about.\" (... \"It is about.\" It is about. \"(...\" it is about. \"(...).\" It is about. \"It is about.\" (... \"it is about.\" (...). \"it is about.\" (... \"it is about.\" it is about. \"(...\" it is about. \"It is about.\"). \"(... It is about.\" (... \"it is about.\" it is about. \"(... It is about.\" It is about. \"It is about.\"). \"(... it is about.\" (... it is about. \"It is about.\" It is about. (... it is about. \"it is about.\"). \"it is about.\" It is about. \"It is about.\" It is about. \"(... it is about."}, {"heading": "3.2 Probabilistic Weighted Pooling at Test Time", "text": "When using dropouts in fully interconnected layers during training, the entire network containing all hidden units should be used at the test date, halving the outgoing weights to compensate for the fact that twice as many of them are active [1], or halving their activations. When using max pooling dropouts during training, one could intuitively select the strongest activation multiplied by the maintenance probability:),...,..., max () (1) 1 (l i l i l i l i l i l i l j aaapa. (7) Since the strongest activation in a pooling region is scaled down by the maintenance probability, we call this a scaled max pooling.Accurate estimate of the averaging of all dropout networks that may be formed. In this pooling scheme, the pooled activity is linearly weighted and weighted by the activations in each region: 1) (0) (Napility) (8) (Here the probability is weighted by a linear."}, {"heading": "4 Empirical Evaluations", "text": "The experiments are carried out on three sets of data: MNIST, CIFAR-10 and CIFAR-100. MNIST consists of 28x28 pixel grayscale images, each with a digit from 0 to 9. There are 60,000 training examples and 10,000 test examples. We do not perform any pre-processing except scaling the pixel values to [0, 1]. The CIFAR-10 dataset [2] consists of ten classes of natural images with 50,000 examples for training and 10,000 for testing. Each example is a 32x32 RGB image taken from the tiny images collected from the web. CIFAR-100 is just like CIFAR-10, but with 100 categories. We also scale to [0, 1] for CIFAR-10 and CIFAR-100 and subtract the mean value of each channel unit calculated from the dataset."}, {"heading": "4.1 Probabilistic Weighted Pooling vs. (Scaled) Max-Pooling", "text": "First, we validate the superiority of the probabilistically weighted pool over the max pooling and the scaled max pooling using MNIST. The CNNs are trained for 1000 epochs. CNN models with different retention probabilities are trained for the max pooling dropout. Figure 2 compares the test performance achieved by different pooling methods at the test date. In general, the probabilistically weighted pooling performs significantly better than the max pooling and the scaled max pooling with different retention probabilities. For the small p (retention probability), the max pooling and the scaled max pooling perform very poorly; the probabilistically weighted pooling is much better. As the p increases, the performance gap narrows. This is not surprising, since the pooled outputs for different pooling methods are close together and the scaling probabilities for extremes of pooling are calculated with a maximum probability of 0.5, which is exactly 0.0 for each maximum pooling."}, {"heading": "4.2 Max-Pooling Dropout vs. Stochastic Pooling", "text": "Similar to Max Pooling Dropouts, in stochastic pooling [4] activation is randomly selected on the basis of a multinomial distribution during the training, and a probabilistic weighting is also carried out during the test period. Specifically, in training, the probability pi for each unit within the pooling region j is first calculated at level l by normalizing the activations.:),..., 2,1 (, 1) (niaa pnklklii (9) An index i in the pooling region is then selected on the basis of a multinomial distribution pi. Pooled activation is simple) (l ia:)., (~ where, 21) () 1 (n l j ppplMultinomiaiaa (10) During the test period, the probabilistic weighting is assumed to be the model mean, i.e. the activations in each pooling region are weighted and summarized by the probability stooling."}, {"heading": "5 Conclusions", "text": "This paper focuses on the problem of understanding and using dropouts on the input layers of Convolutionary Neural Networks. At the time of training, the dropout in max pooling corresponds to the random selection of activation according to a multinomial distribution, and the number of possibly trained networks is exponential in the number of input units to the pool layers. At test date, a new pooling method is proposed, probabilistic weighted pooling, to act as a model mean. Experimental evidence confirms the advantages of using dropouts in max pooling and confirms the superiority of probabilistic weighted pooling over max pooling and scaled max pooling. Considering that stochastic pooling is similar to the dropout in max pooling, we compare it empirically and show that the performance of stochastic pooling lies between those caused by dropouts with different probabilities of dropouts."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the National Natural Science Foundation of China under grant 61371148 and the Shanghai National Natural Science Foundation under grant 12ZR1402500."}], "references": [{"title": "Improving neural networks by preventing co-adaption of feature detectors", "author": ["G.E. Hinton", "N. Srivastave", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "M.S. diss., University of Toronto", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "Fergus R."], "venue": "ICLR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Regularization of neural networks using DropConnect", "author": ["L. Wan", "M.D. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "ICML", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["N. Vinod", "G.E. Hinton"], "venue": "ICML", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive dropout for training deep neural networks", "author": ["J.L. Ba", "B. Frey"], "venue": "NIPS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Dropout [1] is a recently proposed regularizer to fight against over-fitting.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "Dropout has also inspired other stochastic model averaging methods such as stochastic pooling [4], drop-connect [5] and maxout networks [3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "Dropout has also inspired other stochastic model averaging methods such as stochastic pooling [4], drop-connect [5] and maxout networks [3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "Dropout has also inspired other stochastic model averaging methods such as stochastic pooling [4], drop-connect [5] and maxout networks [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 0, "context": "neural nets [1, 5, 6], its effect in pooling layers is, however, not well studied.", "startOffset": 12, "endOffset": 21}, {"referenceID": 4, "context": "neural nets [1, 5, 6], its effect in pooling layers is, however, not well studied.", "startOffset": 12, "endOffset": 21}, {"referenceID": 5, "context": "neural nets [1, 5, 6], its effect in pooling layers is, however, not well studied.", "startOffset": 12, "endOffset": 21}, {"referenceID": 3, "context": "As both stochastic pooling [4] and max-pooling dropout randomly sample activation based on multinomial distributions at pooling stage, it becomes interesting to compare their performance.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "[1] only applied dropout to fully connected layers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] trained a very big convolutional neural net to classify 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Stochastic pooling [4] is a dropout-inspired regularization method.", "startOffset": 19, "endOffset": 22}, {"referenceID": 2, "context": "Maxout network [3] is another model inspired by dropout.", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "Dropout has also motivated other stochastic model averaging methods, such as drop-connect [5] and adaptive dropout [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "Dropout has also motivated other stochastic model averaging methods, such as drop-connect [5] and adaptive dropout [8].", "startOffset": 115, "endOffset": 118}, {"referenceID": 0, "context": "Using dropout in fully-connected layers during training, the whole network containing all the hidden units should be used at test time, but with their outgoing weights halved to compensate for the fact that twice as many of them are active [1], or with their activations halved.", "startOffset": 240, "endOffset": 243}, {"referenceID": 0, "context": "We do not perform any preprocessing except scaling the pixel values to [0, 1].", "startOffset": 71, "endOffset": 77}, {"referenceID": 1, "context": "The CIFAR-10 dataset [2] consists of ten classes of natural images with 50,000 examples for training and 10,000 for testing.", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": "We also scale to [0, 1] for CIFAR-10 and CIFAR-100 and subtract the mean value of each channel computed over the dataset for each image.", "startOffset": 17, "endOffset": 23}, {"referenceID": 6, "context": "We use rectified linear function [7] for convolutional and fully-connected layers, and softmax activation function for the output layer.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "Similar to max-pooling dropout, stochastic pooling [4] also randomly picks activation according to a multinomial distribution at training time, and also involves probabilistic weighting at test time.", "startOffset": 51, "endOffset": 54}], "year": 2015, "abstractText": "Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.", "creator": "Microsoft\u00ae Word 2013"}}}