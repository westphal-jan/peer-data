{"id": "1512.01563", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2015", "title": "State of the Art Control of Atari Games Using Shallow Reinforcement Learning", "abstract": "The recently introduced Deep Q-Networks (DQN) algorithm has gained attention as one of the first successful combinations of deep neural networks and reinforcement learning. Its promise was demonstrated in the Arcade Learning Environment (ALE), a challenging framework composed of dozens of Atari 2600 games used to evaluate general competency in AI. It achieved dramatically better results than earlier approaches, showing that its ability to learn good representations is quite robust and general. This paper attempts to understand the principles that underly DQN's impressive performance and to better contextualize its success. We systematically evaluate the importance of key representational biases encoded by DQN's network by proposing simple linear representations that make use of these concepts. Incorporating these characteristics, we obtain a computationally practical feature set that achieves competitive performance to DQN in the ALE. Besides offering insight into the strengths and weaknesses of DQN, we provide a generic representation for the ALE, significantly reducing the burden of learning a representation for each game. Moreover, we also provide a simple, reproducible benchmark for the sake of comparison to future work in the ALE.", "histories": [["v1", "Fri, 4 Dec 2015 21:06:04 GMT  (632kb,D)", "http://arxiv.org/abs/1512.01563v1", null], ["v2", "Thu, 21 Apr 2016 21:55:54 GMT  (384kb,D)", "http://arxiv.org/abs/1512.01563v2", "A shorter version of this paper appears in the Proceedings of the 15th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2016)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yitao liang", "marlos c machado", "erik talvitie", "michael bowling"], "accepted": false, "id": "1512.01563"}, "pdf": {"name": "1512.01563.pdf", "metadata": {"source": "CRF", "title": "State of the Art Control of Atari Games Using Shallow Reinforcement Learning", "authors": ["Yitao Liang", "Marlos C. Machado", "Erik Talvitie", "Michael Bowling"], "emails": ["erik.talvitie}@fandm.edu", "mbowling}@ualberta.ca"], "sections": [{"heading": null, "text": "Categories and Subject Descriptions I.2.6 [Learning]: Miscellaneous General Terms Algorithms, Performance Keywords Reinforcement Learning, Functional Approach, DQN, Representation Learning, Arcade Learning Environment"}, {"heading": "1. INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2. BACKGROUND", "text": "In this section we present the learning problems of reinforcement and describe the existing results in ALE."}, {"heading": "2.1 Reinforcement Learning", "text": "In the problem of amplification learning (RL) [24, 25] an actor interacts with an unknown environment and tries to maximize a \"reward\" signal. However, the environment is generally defined as a Markov decision-making process (MDP) M as a 5x M = < S, A, R, P, \u03b3 >. However, the actor is currently in the state st + S in which he takes a measure leading to the next state st + 1% S according to the transition probability core P encoding Pr (st + 1 | st, at). The actor also observes a reward rt + 1% R (st, at, st + 1). The goal of the actor is to learn the optimal policy st + 1% S according to the transition probability (a | s) which maximizes the state value function (s)."}, {"heading": "2.2 Arcade Learning Environment", "text": "In recent years, the number of people living in the United States has multiplied in the United States, both in the United States and in Europe. (...) In the United States, the number of people living in the United States has doubled in the last ten years. (...) In the United States, the number of people living in the United States has doubled in the last ten years. (...) In the last ten years, the number of people living in the United States has doubled. (...) In the last ten years, the number of people living in the United States has doubled. (...) In the United States, the number of people living in the United States has doubled. (...) In the last ten years, the number of people living in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in Europe, in the United States, in Europe, in Europe, in Europe, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in Europe, in the United States, in the United States, in the United States, in Europe, in the United States, in the United States, in Europe, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in Europe, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in Europe, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in the United States, in"}, {"heading": "3. SPATIAL INVARIANCE", "text": "Remember that basic features detect the presence of each color in different regions of the screen. As discussed above, this can be seen as an analogy to a single folding with a raw filter. BASS feature set, which was one of the most powerful representations before the introduction of DQN, encodes pairs of relationships between basic features that are still anchored at certain positions, so it is in some ways analogous to a second Convolutionary Layer. But in many games, the absolute positions of objects are not as important as their relative positions to each other. To investigate the effect of ignoring absolute positions, we impose a nonlinearity over BASS (analogous to the fully connected layers of DQN \u2212 kr), especially the maximum positions of BASS features over absolute positions.We refer to the resulting feature set as pairs of relative offsets in space (B-PROS), which are likely (objects-PROS) because it captures only one object between three relative distances."}, {"heading": "3.1 Empirical Evaluation", "text": "Our first series of experiments compares B-PROS with the previous state of the art in linear representations for the Atari 2600 [2, 3] to evaluate the effects of nonlinearity applied to BASS. For the purposes of comparison, we follow Bellemare et al. \"s methodology [2]. In particular, in 24 independent studies, the agent was trained for 5000 episodes. After the learning phase, we frozen the weights and evaluated the policy learned by recording their average performance over 499 episodes. We report the average score across the 24 studies. Following Bellemare et al., we defined a maximum length for each episode: 18,000 frames, i.e. five minutes of real-time play. In addition, we used a frame-skipping technique in which the agent selects actions and updates its value function for each x-frame and repeats the selected action for the next x-1 frames. This allows agent to play approximately x faster than the one."}, {"heading": "4. NON-MARKOVIAN FEATURES", "text": "B-PROS is able to encode relative distances between objects, but it fails to encode movements, which can be a page-long reference. Full results with 53 games are available in Table 3 attached to the important aspect of Atari games. For example, the agent needs to know whether the ball is moving towards or away from the paddle in Pong. Previous linear representations have similarly relied on the most recent screen. In contrast, Mnih et al. use the four most recent screens as input, allowing DQN to represent short-term Markov features of the game screens. In this section, we present an extension to B-PROS, which takes a similar approach and extracts information from the two most recent screens. Basic pairwise Relative Offsets in Time (B-PROT) features represent pairs of relative offsets between the basic features that have been obtained from the screen five frames in the past and basic features in PROS-i.e."}, {"heading": "4.1 Empirical Evaluation", "text": "In these experiments, we have introduced an evaluation protocol similar to that of Mnih et al.'s to compare the results with those of DQN (see Section 6). Each agent was trained for 200,000,000 frames (equivalent to 40,000,000 decisions) through 24 independent studies. The strategies learned in each study were evaluated by recording his average performance in 499 episodes without learning success. We report on the average evaluation in the 24 studies. In an effort to make our results comparable to those of DQN, we started each episode with a random number of \"no-op\" actions and limited the agent to the minimum number of actions that have a unique effect in each game. The first two columns of Table 2 present results based on B-PROS and B-PROST in the training games. B-PROST outperforms B-PROS in all but one of the training games, with a particularly dramatic improvement in PST taking place in each game; while the score of 20-PROB is significantly higher in this game with an average score of 49-PROB."}, {"heading": "5. OBJECT DETECTION", "text": "In fact, it is in such a way that most people are in a position to embark on the search for another way, which leads them into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they get themselves caught, in which they put themselves into, in which they become entangled, in which they are entangled in themselves, in which they become entangled, in which they become entangled in themselves, in which they become entangled in themselves, in which they become themselves, in which they become entangled they become, in which they become themselves, in which they become entangled they become, in which they become themselves, in which they become entangled they become, in which they become themselves, in which they become, in which they become entangled they become, in which they become themselves."}, {"heading": "5.1 Empirical Evaluation", "text": "The last column of Table 2 shows the results obtained with BlobPROST in training games. Blob-PROST outperformed B-PROST statistically in all but one of the training games (using Welch's t-test with p < 0.05). Asterix is a game in which Blob-PROST fails significantly; perhaps the blob detection confuses different objects in a harmful way. Q * Bert is a remarkable success; the values of B-PROS and B-PROST indicate that they rarely complete the first level, while Blob-PROST consistently holes the first level. Of the 49 games rated by Mnih et al., the average performance of Blob-PROST was higher than that of B-PROS and B-PROST in 59% (29 / 49)."}, {"heading": "6. COMPARISON WITH DQN", "text": "The next experiments compare Blob-PROST with current state-of-the-art DQN, and show that the three improvements examined above largely explain the performance gap between DQN and previous LFA approaches. Inevitably, when performing this comparison, we have to address some problematic aspects of the original DQN evaluation (some of which we adopt for comparison's sake), and in Section 7, we present a more methodologically sound ALE benchmark for comparison with future work."}, {"heading": "6.1 DQN Evaluation Methodology", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "6.2 Comparing Blob-PROST and DQN", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "7. ALE BENCHMARK", "text": "As discussed above, in addition to a better understanding of the core issues underlying DQN's success in ALE, it is important to present a reproducible benchmark without the problematic errors discussed in Section 6 (i.e. one that reports on more than one study, evaluates the final set of weights, and refrains from game-specific prior knowledge). Furthermore, establishing this benchmark is important for the continued use of ALE as an evaluation platform. Principled evaluation of complicated approaches depends on the existence of a solid basis for what can be achieved with simple approaches. In order to provide a benchmark free of game-specific information, we also present the performance of Blob-PROST using the full action set, which Mnih et al. did not do. The average performance of our agents after 24 attempts using the full action set is reported in the last column of Table 4 in the Appendix. We recommend that future comparisons with BlobPROST use these results as a minimum of surprise, if we do not fully implement these actions in this block set."}, {"heading": "8. CONCLUSIONS AND FUTURE WORK", "text": "In this context, it should be noted that this is a very complex and lengthy matter."}, {"heading": "Acknowledgements", "text": "This research was supported by Alberta Innovates Technology Futures and the Alberta Innovates Centre for Machine Learning. Computing resources were provided by Computing Canada through CalculQue \u0301 bec.APPENDIX."}, {"heading": "A. FULL RESULTS", "text": "Table 4 presents the results of the three features presented in this paper (B-PROS, BPROST, and Blob-PROST), as well as the results of BlobPROST with the complete set of measures that we recommend as a benchmark for the future (see sections 4, 5, and 7). Finally, Table 5 presents the data used to compare BlobPROST with the reported results of the DQN (see section 6)."}], "references": [{"title": "Residual Algorithms: Reinforcement Learning with Function Approximation", "author": ["III L.C. Baird"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), pages 30\u201337", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Investigating Contingency Awareness using Atari 2600 Games", "author": ["M.G. Bellemare", "J. Veness", "M. Bowling"], "venue": "Proceedings of the Twenty-Sixth Conference on Artificial Intelligence (AAAI), pages 864\u2013871", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Sketch-Based Linear Value Function Approximation", "author": ["M.G. Bellemare", "J. Veness", "M. Bowling"], "venue": "Proceedings of the Advances in Neural Information Processing Systems (NIPS), pages 2222\u20132230", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Heads-up Limit Hold\u2019em Poker is Solved", "author": ["M. Bowling", "N. Burch", "M. Johanson", "O. Tammelin"], "venue": "Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "A", "author": ["M. Campbell"], "venue": "J. H. Jr., and F.-H. Hsu. Deep Blue. Artificial Intelligence, 134(1\u00e2\u0102\u015e2):57 \u2013 83", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "A Comparison of Learning Algorithms on the Arcade Learning Environment", "author": ["A. Defazio", "T. Graepel"], "venue": "CoRR, abs/1410.8620", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Building Watson: An Overview of the DeepQA Project", "author": ["D.A. Ferrucci", "E.W. Brown", "J. Chu-Carroll", "J. Fan", "D. Gondek", "A. Kalyanpur", "A. Lally", "J.W. Murdock", "E. Nyberg", "J.M. Prager", "N. Schlaefer", "C.A. Welty"], "venue": "AI Magazine, 31(3):59\u201379", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Similarity Search in High Dimensions via Hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "Proceedings of International Conference on Very Large Data Bases (VLDB), pages 518\u2013529", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "Reinforcement Learning with Function Approximation Converges to a Region", "author": ["G.J. Gordon"], "venue": "Proceedings of the Advances in Neural Information Processing Systems (NIPS), pages 1040\u20131046", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning", "author": ["X. Guo", "S.P. Singh", "H. Lee", "R.L. Lewis", "X. Wang"], "venue": "Proceedings of the Advances in Neural Information Processing Systems (NIPS), pages 3338\u20133346", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Recurrent Q-Learning for Partially Observable MDPs", "author": ["M.J. Hausknecht", "P. Stone"], "venue": "CoRR, abs/1507.06527", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proceedings of the Advances in Neural Information Processing Systems (NIPS), pages 1106\u20131114", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-Based Learning Applied to Document Recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Classical Planning with Simulators: Results on the Atari Video Games", "author": ["N. Lipovetzky", "M. Ramirez", "H. Geffner"], "venue": "Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Domain-Independent Optimistic Initialization for  Reinforcement Learning", "author": ["M.C. Machado", "S. Srinivasan", "M. Bowling"], "venue": "AAAI Workshop on Learning for General Competency in Video Games", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "An Analysis of Reinforcement Learning with Function Approximation", "author": ["F.S. Melo", "S.P. Meyn", "M.I. Ribeiro"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), pages 664\u2013671", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Playing Atari With Deep Reinforcement Learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "NIPS Deep Learning Workshop", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Human-level Control through Deep Reinforcement Learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "On-line Q-Learning using Connectionist Systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": "CUED/F-INFENG/TR 166,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Checkers is Solved", "author": ["J. Schaeffer", "N. Burch", "Y. Bj\u00f6rnsson", "A. Kishimoto", "M. M\u00fcller", "R. Lake", "P. Lu", "S. Sutphen"], "venue": "Science, 317(5844):1518\u20131522", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Trust Region Policy Optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), pages 1889\u20131897", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Parameter Selection for the Deep Q-learning Algorithm ((Extended Abstract))", "author": ["N. Sprague"], "venue": "Proceedings of the Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["C. Szepesv\u00e1ri"], "venue": "Synthesis lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Temporal Difference Learning and TD-Gammon", "author": ["G. Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1995}, {"title": "Deep Reinforcement Learning with Double Q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "CoRR, abs/1509.06461,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Compress and Control", "author": ["J. Veness", "M.G. Bellemare", "M. Hutter", "A. Chua", "G. Desjardins"], "venue": "Proceedings of the Conference on Artificial Intelligence (AAAI)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Technical Note: Q-Learning", "author": ["C.J.C.H. Watkins", "P. Dayan"], "venue": "Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1992}], "referenceMentions": [{"referenceID": 18, "context": "The recent Deep Q-Network (DQN) algorithm [19] aims to tackle this problem, presenting one of the first successful combinations of", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "RL and deep convolutional neural-networks (CNN) [14, 13], which are proving to be a powerful approach to representation learning in many areas.", "startOffset": 48, "endOffset": 56}, {"referenceID": 12, "context": "RL and deep convolutional neural-networks (CNN) [14, 13], which are proving to be a powerful approach to representation learning in many areas.", "startOffset": 48, "endOffset": 56}, {"referenceID": 28, "context": "DQN is based upon the well-known Q-learning algorithm [29] and uses a CNN to simultaneously learn a problem-specific representation and estimate a value function.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "Games have always been an important testbed for AI, frequently being used to demonstrate major contributions to the field [5, 6, 8, 21, 26].", "startOffset": 122, "endOffset": 139}, {"referenceID": 5, "context": "Games have always been an important testbed for AI, frequently being used to demonstrate major contributions to the field [5, 6, 8, 21, 26].", "startOffset": 122, "endOffset": 139}, {"referenceID": 7, "context": "Games have always been an important testbed for AI, frequently being used to demonstrate major contributions to the field [5, 6, 8, 21, 26].", "startOffset": 122, "endOffset": 139}, {"referenceID": 20, "context": "Games have always been an important testbed for AI, frequently being used to demonstrate major contributions to the field [5, 6, 8, 21, 26].", "startOffset": 122, "endOffset": 139}, {"referenceID": 25, "context": "Games have always been an important testbed for AI, frequently being used to demonstrate major contributions to the field [5, 6, 8, 21, 26].", "startOffset": 122, "endOffset": 139}, {"referenceID": 1, "context": "DQN follows this tradition, demonstrating its success by achieving human-level performance in the majority of games within the Arcade Learning Environment (ALE) [2].", "startOffset": 161, "endOffset": 164}, {"referenceID": 11, "context": "In fact, early follow-up work already indicates that researchers may be unable to robustly match the single trial performance reported in the DQN paper [12].", "startOffset": 152, "endOffset": 156}, {"referenceID": 23, "context": "In the reinforcement learning (RL) problem [24, 25] an agent interacts with an unknown environment and attempts to maximize a \u201creward\u201d signal.", "startOffset": 43, "endOffset": 51}, {"referenceID": 24, "context": "In the reinforcement learning (RL) problem [24, 25] an agent interacts with an unknown environment and attempts to maximize a \u201creward\u201d signal.", "startOffset": 43, "endOffset": 51}, {"referenceID": 19, "context": "One of the most popular reinforcement learning algorithms is Sarsa(\u03bb) [20].", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "Theoretical results suggest that, as an on-policy method, Sarsa(\u03bb) may be more stable in the linear function approximation case than off-policy methods such as Q-learning, which are known to risk divergence [1, 10, 17].", "startOffset": 207, "endOffset": 218}, {"referenceID": 9, "context": "Theoretical results suggest that, as an on-policy method, Sarsa(\u03bb) may be more stable in the linear function approximation case than off-policy methods such as Q-learning, which are known to risk divergence [1, 10, 17].", "startOffset": 207, "endOffset": 218}, {"referenceID": 16, "context": "Theoretical results suggest that, as an on-policy method, Sarsa(\u03bb) may be more stable in the linear function approximation case than off-policy methods such as Q-learning, which are known to risk divergence [1, 10, 17].", "startOffset": 207, "endOffset": 218}, {"referenceID": 6, "context": "These theoretical insights are confirmed in practice; Sarsa(\u03bb) seems to be far less likely to diverge in the ALE than Q-learning and other off-policy methods [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "[2] presented an RL benchmark using four different feature sets obtained from the game screen.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "LSH simply applies Locally Sensitive Hashing [9] to raw Atari 2600 screens.", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": "This additional information is known as contingency awareness [3].", "startOffset": 62, "endOffset": 65}, {"referenceID": 6, "context": "These feature sets were, for some time, the standard representations for Atari 2600 games, being directly used in other work [7, 15, 16] or serving as the base for more elaborate feature sets [4].", "startOffset": 125, "endOffset": 136}, {"referenceID": 14, "context": "These feature sets were, for some time, the standard representations for Atari 2600 games, being directly used in other work [7, 15, 16] or serving as the base for more elaborate feature sets [4].", "startOffset": 125, "endOffset": 136}, {"referenceID": 15, "context": "These feature sets were, for some time, the standard representations for Atari 2600 games, being directly used in other work [7, 15, 16] or serving as the base for more elaborate feature sets [4].", "startOffset": 125, "endOffset": 136}, {"referenceID": 3, "context": "These feature sets were, for some time, the standard representations for Atari 2600 games, being directly used in other work [7, 15, 16] or serving as the base for more elaborate feature sets [4].", "startOffset": 192, "endOffset": 195}, {"referenceID": 1, "context": "[2, 3]) we subtract the background from the screen at each frame before processing it.", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[2, 3]) we subtract the background from the screen at each frame before processing it.", "startOffset": 0, "endOffset": 6}, {"referenceID": 27, "context": "[28], for example, propose the use of compression techniques as a policy evaluation approach in RL, evaluating their method in the ALE.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Soon after, the research landscape changed due to the success of using deep learning techniques to approximate the actionvalue functions, introduced by the DQN algorithm [18, 19].", "startOffset": 170, "endOffset": 178}, {"referenceID": 18, "context": "Soon after, the research landscape changed due to the success of using deep learning techniques to approximate the actionvalue functions, introduced by the DQN algorithm [18, 19].", "startOffset": 170, "endOffset": 178}, {"referenceID": 10, "context": "[11, 22, 23]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 21, "context": "[11, 22, 23]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 22, "context": "[11, 22, 23]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 1, "context": "Note that there have been other attempts to represent pairwise spatial relationships between objects, for instance DISCO [2] and contingency awareness features [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "Note that there have been other attempts to represent pairwise spatial relationships between objects, for instance DISCO [2] and contingency awareness features [3].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "Our first set of experiments compares B-PROS to the previous state of the art in linear representations for the Atari 2600 [2, 3] in order to evaluate the impact of the nonlinearity applied to BASS.", "startOffset": 123, "endOffset": 129}, {"referenceID": 2, "context": "Our first set of experiments compares B-PROS to the previous state of the art in linear representations for the Atari 2600 [2, 3] in order to evaluate the impact of the nonlinearity applied to BASS.", "startOffset": 123, "endOffset": 129}, {"referenceID": 1, "context": "\u2019s methodology [2].", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "Bold denotes the largest value between B-PROS and Best Linear [2].", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": "\u201cBest Linear\u201d denotes the best performance obtained among four different feature sets: Basic, BASS, DISCO and LSH [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "For reference, in the \u201cCAF\u201d column we also include results obtained by contingency awareness features [3], as reported by Mnih et al.", "startOffset": 102, "endOffset": 105}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2], i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "do not report DQN\u2019s runtime, accelerated or otherwise, but in recent follow-up work the GPU accelerated speed has been reported to be approximately 330 frames per second [27], still slower than the Blob-PROST agent in most games.", "startOffset": 170, "endOffset": 174}, {"referenceID": 1, "context": "Bold indicates the best mean score in the first five columns, which were obtained using all the 18 actions available, after learning for 5,000 episodes and evaluating for 500 episodes [2].", "startOffset": 184, "endOffset": 187}, {"referenceID": 2, "context": "CAF denotes contingency awareness features [3].", "startOffset": 43, "endOffset": 46}], "year": 2017, "abstractText": "The recently introduced Deep Q-Networks (DQN) algorithm has gained attention as one of the first successful combinations of deep neural networks and reinforcement learning. Its promise was demonstrated in the Arcade Learning Environment (ALE), a challenging framework composed of dozens of Atari 2600 games used to evaluate general competency in AI. It achieved dramatically better results than earlier approaches, showing that its ability to learn good representations is quite robust and general. This paper attempts to understand the principles that underly DQN\u2019s impressive performance and to better contextualize its success. We systematically evaluate the importance of key representational biases encoded by DQN\u2019s network by proposing simple linear representations that make use of these concepts. Incorporating these characteristics, we obtain a computationally practical feature set that achieves competitive performance to DQN in the ALE. Besides offering insight into the strengths and weaknesses of DQN, we provide a generic representation for the ALE, significantly reducing the burden of learning a representation for each game. Moreover, we also provide a simple, reproducible benchmark for the sake of comparison to future work in the ALE.", "creator": "LaTeX with hyperref package"}}}