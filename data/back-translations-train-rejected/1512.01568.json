{"id": "1512.01568", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2015", "title": "Hybrid Approach for Inductive Semi Supervised Learning using Label Propagation and Support Vector Machine", "abstract": "Semi supervised learning methods have gained importance in today's world because of large expenses and time involved in labeling the unlabeled data by human experts. The proposed hybrid approach uses SVM and Label Propagation to label the unlabeled data. In the process, at each step SVM is trained to minimize the error and thus improve the prediction quality. Experiments are conducted by using SVM and logistic regression(Logreg). Results prove that SVM performs tremendously better than Logreg. The approach is tested using 12 datasets of different sizes ranging from the order of 1000s to the order of 10000s. Results show that the proposed approach outperforms Label Propagation by a large margin with F-measure of almost twice on average. The parallel version of the proposed approach is also designed and implemented, the analysis shows that the training time decreases significantly when parallel version is used.", "histories": [["v1", "Wed, 2 Dec 2015 12:04:30 GMT  (404kb,D)", "http://arxiv.org/abs/1512.01568v1", "Presented in the 11th International Conference, MLDM, Germany, July 20 - 21, 2015. Springer, Machine Learning and Data Mining in Pattern Recognition, LNAI Vol. 9166, p. 199-213, 2015"]], "COMMENTS": "Presented in the 11th International Conference, MLDM, Germany, July 20 - 21, 2015. Springer, Machine Learning and Data Mining in Pattern Recognition, LNAI Vol. 9166, p. 199-213, 2015", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["aruna govada", "pravin joshi", "sahil mittal", "sanjay k sahay"], "accepted": false, "id": "1512.01568"}, "pdf": {"name": "1512.01568.pdf", "metadata": {"source": "CRF", "title": "Hybrid Approach for Inductive Semi Supervised Learning using Label Propagation and Support Vector Machine", "authors": ["Aruna Govada", "Pravin Joshi", "Sahil Mittal", "Sanjay K Sahay"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Semi-Supervised Learning, Data Mining, Support Vector Machine, Label Propagation"}, {"heading": "1 Introduction", "text": "Semi-supervised learning methods are mainly divided into two broad classes: inductive and transductive. Both inductive and transductive learning methods have a labeled learning dataset (xi, yi) i = 1... l \u00b2 p (x, y) and an unlabeled learning dataset (xi) i = l + 1... l + u p (x), in which the labeled learner learns a labeled learning dataset. The labeled learning dataset learns a predictor f: X \u2192 Y, f \u00b2 F, in which F is the hybrid space, x \u00b2 X is an input instance, y \u00b2 Y is its class name. The predictor learns in such a way that it predicts future test data better than the predictor, which learns solely from the labeled data. In transductive learning, the unlabeled data is expected to be predicted (xi) i = l + 1... l + u without expectations of a generalization of the test data to come."}, {"heading": "2 Related Work", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "3 Label Propagation", "text": "Let (x1, y1)...... (xl, yl) label the data, where (x1... xl) are the instances, Y = (y1... yl)... (1.... C) are the corresponding class names. Let (xl + 1, yl + 1)..... (xl + u, yl + u) be the blank data where YU = (yl + 1..... yu) are not observed. They must be estimated with X and YL, where X = (x1...... xl + u) F: L-U \u2212 \u2192 R wij is a similarity between i and j, where i, j and X F should minimize the energy function (f) = 12 \u0445i, j wij (f (i) \u2212 f (j) = fT 4 and fi, Fj should be similar for a high spread of labels. Labels assume that the number of class names is known and all classes are present in the labeled data."}, {"heading": "3.1 Support Vector Machine", "text": "The learning task in binary SVM can be presented as follows: minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = minw = mini = minw = minw = minw = minw = minw = minw = minw = minw = minw = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = mini = min"}, {"heading": "4 Proposed Approach", "text": "The proposed algorithm is an inductive semi-supervised learning, an iterative approach in which the following steps are performed with each iteration. In the first step, the label propagation is executed on the training data and the probability matrix for the unlabeled data is calculated. In the second step, the SVM is trained on the basis of the available label data. In the third step, the classes for unlabeled data are predicted on the basis of SVM and the class probabilities calculated in step 1 are compared with the threshold. In step 4, all data sets for which both label propagation and SVM on the class label match are labeled with the corresponding class. This continues until all data sets are labeled or no new data sets are labeled in one iteration. As the data consists of several classes, 1-1 multi-class SVM is used."}, {"heading": "4.1 Serial Version", "text": "Input: Classifier, Threshold Output: F-measure1. (labeled records, unlabeled records) = select next train folds () # Each folding of data is divided into labeled and unlabeled records with 20: 80 ratio # unlabeled records. # unlabeled records have their class field set to -1 2. test records = select next test fold () # Concatenate labeled and unlabeled records to get train records 3. train records = labeled records + unlabeled records 4. new labeled = 0 5. while len (labeled records) < len (train records): 5.1 lp probability matrix = run lp (labeled records + unlabeled records) 5.2 model = fit classifier (classifier, labeled records) 5.3 labeled atleast one = False5.4 for record in unlabeled records: i. classifier out = model.feature vordels out = model.feature (record.vector) # test ecord.class ier."}, {"heading": "4.2 Parallel Version", "text": "Input: Classifier, Threshold, No of tasks # Number of parallel processes # Number of parallel processes Output: F-measure1. new labeled = 0 2. while len (labeled records) < len (train records): 2.1 lp train records = labeled records + unlabeled records 2.2 lp probability matrix = []; classifier out = [] 2.3 lp process = new process (target = run lp, args = (lp train records, lp probability matrix)) 2.4 lp process.start () 2.5 classifier process = new process (target = fit classifier, args = (classifier, labeled records, unlabeled all out) 2.6 classifier process.start () 2.7 lp process.join () 2.8 classifier process.join ().join () 2.9 atleast one labeled = False2.10 chunk size = len."}, {"heading": "5 Experimental Section", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "6 Conclusion and Future work", "text": "The proposed approach uses SVM together with the Label Propagation Algorithm to achieve a very high overall predictive quality. It can use a small amount of labeled data along with a large amount of unlabeled data to achieve a high F level of test data. It has a very low margin of error as it labels the unlabeled data with the consent of both - SVM and Label Propagation. If we test both algorithms on 12 different sets of data, we can conclude that the proposed approach performs much better than the label propagation [16] alone. It provides F measurements that are almost twice as high as the label propagation. In addition, we have designed the parallel version of the approach and have been able to significantly shorten the training time. In the future, the parallel algorithm can be further improved to enable linear or superlinear scaling. Further research on the role of monitored algorithms in the area of semi-supervised learning could be beneficial to computers, thanks to the Science Department's support and we are thankful for that."}], "references": [{"title": "The exponential value of labeled samples", "author": ["V. Castelli", "T. Cover"], "venue": "Pattern Recognition Letters,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter", "author": ["V. Castelli", "T. Cover"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Learning from a mixture of labeled and unlabeled examples with parametric side information", "author": ["J. Ratsaby", "S. Venkatesh"], "venue": "Proceedings of the Eighth Annual Conference on Computational Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Semi-supervised learning of mixture models.", "author": ["Cozman", "Fabio Gagliardi", "Ira Cohen", "Marcelo Cesar Cirelo"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Stable mixing of complete and incomplete information (Technical Report AIM-2001-030)", "author": ["A. Corduneanu", "T. Jaakkola"], "venue": "MIT AI Memo", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Statistical machine translation with word- and sentence-aligned parallel corpora", "author": ["C. Callison-Burch", "D. Talbot", "M. Osborne"], "venue": "Proceedings of the ACL", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Text classification from labeled and unlabeled documents using EM", "author": ["K. Nigam", "A.K. McCallum", "S. Thrun", "T. Mitchell"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1977}, {"title": "Semi-supervised support vector machines.Advances in Neural Information", "author": ["K. Bennett", "A. Demiriz"], "venue": "Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Clsutering unlabeled data with SOMs improves classification of labeled real-world data", "author": ["R. Dara", "S. Kremer", "D. Stacey"], "venue": "Proceedings of the World Congress on Computational Intelligence (WCCI)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["D. Yarowsky"], "venue": "Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (pp. 189196)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Learning subjective nouns using extraction pattern bootstrapping", "author": ["E. Riloff", "J. Wiebe", "T. Wilson"], "venue": "Proceedings of the Seventh Conference on Natural Language Learning", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Semi-supervised self training of object detection models", "author": ["C. Rosenberg", "M. Hebert", "H. Schneiderman"], "venue": "Seventh IEEE Workshop on Applications of Computer Vision", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Combining labeled and unlabeled data with cotraining", "author": ["A. Blum", "T. Mitchell"], "venue": "COLT: Proceedings of the Workshop on Computational Learning Theory", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "An augmented pac model for semi-supervised learning", "author": ["Balcan", "M.-F", "A. Blum"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["Xiaojin Zhu", "Zoubin Ghahramani"], "venue": "Technical Report CMU-CALD-02-107,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Label Propagation Through Linear Neighborhoods 2008 http://machinelearning.wustl.edu/mlpapers/paper files/icml2006 WangZ06.pdf", "author": ["Fei Wang", "Changshui Zhang"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Semi-supervised learning with graphs. Diss.Carnegie Mellon University, Language Technologies Institute", "author": ["Xiaojin Zhu", "John Lafferty", "Ronald Rosenfeld"], "venue": "School of Computer Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Support vector machines,", "author": ["M.A. Hearst", "S.T. Dumais", "E. Osman", "J. Platt", "B. Scholkopf"], "venue": "Intelligent Systems and their Applications,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "[1],[2] and Ratsaby et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1],[2] and Ratsaby et al.", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "[3] showed that unlabeled data can predict better if the model assumption is correct.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] provide theoretical analysis of deterioration in performance with an increase in unlabeled data and argue that bias is adversely affected in such situations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "CallisonBurch et al [6] used the down-weighing scheme to estimate word alignment for machine translation.", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "[7] apply the Expectation Maximization[8] algorithm on mixture of multinomial for the task of text classification and showed that the resulting classifiers predict better than classifier trained only on labeled data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7] apply the Expectation Maximization[8] algorithm on mixture of multinomial for the task of text classification and showed that the resulting classifiers predict better than classifier trained only on labeled data.", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "[9] and Dara et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] used this cluster and label approach successfully to increase prediction performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] using self-training.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "al[12].", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": "[13] in detection of object systems from images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Co-training [14] assumes that (i) features can be split into two sets; (ii) Each sub-feature set is sufficient to train a good classifier; (iii) the two sets are conditionally independent given the class.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "[15] show that co-training can be quite effective and that in the extreme case only one labeled point is needed to learn the classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] which propagated labels from labeled data points to unlabeled ones.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] which can propagate the labels from the labeled points to the whole data set using these linear neighborhoods with sufficient smoothness.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Graph based method is proposed in [18] wherein vertices represent the labeled and unlabeled records and edge weights denote similarity between them.", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "al[16].", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "The performance of the proposed approach is compared with the label propagation algorithm[16] for all the datasets and shown in Fig 5.", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "The comparison of the proposed approach with the label propagation[16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 15, "context": "On testing both the algorithms on 12 different datasets we can conclude that the proposed approach performs much better than label propagation[16] alone.", "startOffset": 142, "endOffset": 146}], "year": 2015, "abstractText": "Semi supervised learning methods have gained importance in today\u2019s world because of large expenses and time involved in labeling the unlabeled data by human experts. The proposed hybrid approach uses SVM and Label Propagation to label the unlabeled data. In the process, at each step SVM is trained to minimize the error and thus improve the prediction quality. Experiments are conducted by using SVM and logistic regression(Logreg). Results prove that SVM performs tremendously better than Logreg. The approach is tested using 12 datasets of different sizes ranging from the order of 1000s to the order of 10000s. Results show that the proposed approach outperforms Label Propagation by a large margin with F-measure of almost twice on average. The parallel version of the proposed approach is also designed and implemented, the analysis shows that the training time decreases significantly when parallel version is used.", "creator": "LaTeX with hyperref package"}}}