{"id": "1512.01596", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2015", "title": "Creation of a Deep Convolutional Auto-Encoder in Caffe", "abstract": "The development of a deep (stacked) convolutional auto-encoder in the Caffe deep learning framework is presented in this paper. We describe simple principles which we used to create this model in Caffe. The proposed model of convolutional auto-encoder does not have pooling/unpooling layers yet. The results of our experimental research show comparable accuracy of dimensionality reduction in comparison with a classic auto-encoder on the example of MNIST dataset.", "histories": [["v1", "Fri, 4 Dec 2015 23:58:47 GMT  (1184kb)", "http://arxiv.org/abs/1512.01596v1", "8 pages, 7 figures, 5 tables, 28 references in the list"], ["v2", "Thu, 21 Apr 2016 01:51:14 GMT  (1242kb)", "http://arxiv.org/abs/1512.01596v2", "9 pages, 7 figures, 5 tables, 34 references in the list; Added references, corrected Table 3, changed several paragraphs in the text"], ["v3", "Fri, 22 Apr 2016 03:20:41 GMT  (1258kb)", "http://arxiv.org/abs/1512.01596v3", "9 pages, 7 figures, 5 tables, 34 references in the list; Added references, corrected Table 3, changed several paragraphs in the text"]], "COMMENTS": "8 pages, 7 figures, 5 tables, 28 references in the list", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["volodymyr turchenko", "artur luczak"], "accepted": false, "id": "1512.01596"}, "pdf": {"name": "1512.01596.pdf", "metadata": {"source": "CRF", "title": "Creation of a Deep Convolutional Auto-Encoder in Caffe", "authors": ["Volodymyr Turchenko", "Artur Luczak"], "emails": ["luczak}@uleth.ca"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "4. Conclusions", "text": "The development of a deep (stacked) revolutionary auto encoder within the framework of Caffe deep learning and its experimental evaluation are presented in this paper. Contrary to the classic stacked auto encoder proposed by Hinton et al. [2], revolutionary auto encoders allow the use of the desirable properties of Convolutionary Neural Networks for image processing tasks while operating within an unsupervised learning paradigm. The results of our experimental research show comparable accuracy in a dimensionality reduction task compared to the classic auto encoder using the example of MNIST data.In creating this revolutionary auto encoder, we have used well-known principles mentioned in Section 2 above, which are used by many machine learning researchers every day. Nevertheless, we believe that our approach and the research results presented in this paper will contribute to the fact that the deep encoder developed in general architecture will be the most efficient in the future application of the complex work environment."}, {"heading": "Acknowledgements", "text": "We thank the Caffe developers for creating such a powerful framework for deep machine learning research. We thank Hyeonwoo Noh and Dr. David Silver for discussing some of the results presented in this paper and Dr. Eric Chalmers for editorial help. We thank them for their financial support through the NSERC CREATE GDP program and the Department of Neuroscience at the University of Lethbridge, Canada."}], "references": [{"title": "Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction", "author": ["J. Masci", "U. Meier", "D. Ciresan", "J. Schmidhuber"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Reducing the Dimensionality of Data with", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Neural Networks, Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Gradientbased Learning Applied to Document Recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition", "author": ["M. Ranzato", "F.J. Huang", "Y.-L. Boureau", "Y. LeCun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Extracting and Composing Robust Features with Denoising  Autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "Proceedings of the 25th International Conference on Machine Learning (ICML", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Dimensionality Reduction by Learning an Invariant Mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Learning Deconvolution Network for Semantic Segmentation, arXiv:1505.04366v1", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Visualizing and Understanding Convolutional Networks, Lecture", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Notes in Computer Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Accelerating t-SNE using TreeBased Algorithms", "author": ["L.J.P. van der Maaten"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Adaptive Deconvolutional Networks for Mid and High Level Feature Learning, 2011", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "IEEE International Conference on Computer Vision (ICCV\u201911),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "It is trained in unsupervised fashion allowing it to extract generally useful features from unlabeled data, to detect and remove input redundancies and to present essential aspects of analyzing data in robust and discriminative representations [1].", "startOffset": 244, "endOffset": 247}, {"referenceID": 1, "context": "Compared to the architecture of a classic stacked auto-encoder [2], CAE may be better suited to image processing tasks because it fully utilizes the properties of convolutional neural networks, which have been proven to provide better results on noisy, shifted and corrupted image data [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "Compared to the architecture of a classic stacked auto-encoder [2], CAE may be better suited to image processing tasks because it fully utilizes the properties of convolutional neural networks, which have been proven to provide better results on noisy, shifted and corrupted image data [3].", "startOffset": 286, "endOffset": 289}, {"referenceID": 0, "context": "Theoretical issues of CAE developments are well described in many research papers [1, 4-6].", "startOffset": 82, "endOffset": 90}, {"referenceID": 3, "context": "Theoretical issues of CAE developments are well described in many research papers [1, 4-6].", "startOffset": 82, "endOffset": 90}, {"referenceID": 4, "context": "Theoretical issues of CAE developments are well described in many research papers [1, 4-6].", "startOffset": 82, "endOffset": 90}, {"referenceID": 1, "context": "The first is a classic stacked auto-encoder, proposed by Hinton et al [2], and the second is a Siamese network, proposed by LeCun et al [22].", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "The first is a classic stacked auto-encoder, proposed by Hinton et al [2], and the second is a Siamese network, proposed by LeCun et al [22].", "startOffset": 136, "endOffset": 140}, {"referenceID": 2, "context": "The Siamese network consists of two LeNet [3] architectures coupled in a Siamese way and ended by a contrastive loss function.", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "Following the success of the LeNet [3] architecture and the paper [1] which showed experimentally that convolutional+pooling layers provide a better representation of convolutional filters and, furthermore, classification results, we started to construct CAE by the scheme conv1-pool1-conv2-pool2 in the encoder part and deconv2-unpool2-deconv1-unpool1 in the decoder part.", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "Following the success of the LeNet [3] architecture and the paper [1] which showed experimentally that convolutional+pooling layers provide a better representation of convolutional filters and, furthermore, classification results, we started to construct CAE by the scheme conv1-pool1-conv2-pool2 in the encoder part and deconv2-unpool2-deconv1-unpool1 in the decoder part.", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "We also have been inspired by the work of Hyeonwoo Noh et al [23] and we have used their nonofficial Caffe distribution [24], where they have implemented an unpooling layer, which is still absent in the official Caffe distribution.", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "Masci et al [1] showed that convolutional architectures without max-pooling layers give worse results, but architectures without pooling layers are definitely working architectures, and it is a good point to start first with some simpler working architecture and then to increase the complexity of the model.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "After we have eliminated pooling-unpooling layers and added a non-linear activation function, <Sigmoid> in our case, after each convolutional and deconvolution layer [4], we have noticed, that the developed model is very similar to the classic auto-encoder model [2].", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "After we have eliminated pooling-unpooling layers and added a non-linear activation function, <Sigmoid> in our case, after each convolutional and deconvolution layer [4], we have noticed, that the developed model is very similar to the classic auto-encoder model [2].", "startOffset": 263, "endOffset": 266}, {"referenceID": 7, "context": "Visualization of the values (along with its numerical representation) of trainable filters, feature maps and hidden units from layer to layer allows better understanding of how data are converted/processed from layer to layer [25]; 4.", "startOffset": 226, "endOffset": 230}, {"referenceID": 3, "context": "The main purpose of the activation function after each convolutional/deconvolution layer is non-linear data processing [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "presented the best architecture (3 row in Table 1) in his paper [2], but, as mentioned above, we wanted to research the generalization properties of smaller and bigger architectures in order to use this experience to create our CAE model.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "According to Hinton et al [2] these architectures are called 2-, 10and 30-dimensional auto-encoders.", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "We have used t-SNE technique [26] to visualize 10and 30- dimensional data, produced by auto-encoder.", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "Since the deconvolution operation has the same nature as convolution [27], we have used the same approach to calculate the number of trainable parameters both in the encoder and decoder parts.", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "Similarly we have used t-SNE technique [26] to visualize 10- and 30- dimensional data, produced by the 10- and 30dimensional CAEs.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "In contrast to the classic stacked auto-encoder proposed by Hinton et al [2], convolutional autoencoders allow using the desirable properties of convolutional neural networks for image data processing tasks while working within an unsupervised learning paradigm.", "startOffset": 73, "endOffset": 76}], "year": 2015, "abstractText": "The development of a deep (stacked) convolutional auto-encoder in the Caffe deep learning framework is presented in this paper. We describe simple principles which we used to create this model in Caffe. The proposed model of convolutional autoencoder does not have pooling/unpooling layers yet. The results of our experimental research show comparable accuracy of dimensionality reduction in comparison with a classic auto-encoder on the example of MNIST dataset.", "creator": "Acrobat PDFMaker 11 \u0434\u043b\u044f Word"}}}