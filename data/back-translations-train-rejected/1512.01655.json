{"id": "1512.01655", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2015", "title": "Approximated and User Steerable tSNE for Progressive Visual Analytics", "abstract": "Progressive Visual Analytics aims at improving the interactivity in existing analytics techniques by means of visualization as well as interaction with intermediate results. One key method for data analysis is dimensionality reduction, for example, to produce 2D embeddings that can be visualized and analyzed efficiently. t-Distributed Stochastic Neighbor Embedding (tSNE) is a well-suited technique for the visualization of several high-dimensional data. tSNE can create meaningful intermediate results but suffers from a slow initialization that constrains its application in Progressive Visual Analytics. We introduce a controllable tSNE approximation (A-tSNE), which trades off speed and accuracy, to enable interactive data exploration. We offer real-time visualization techniques, including a density-based solution and a Magic Lens to inspect the degree of approximation. With this feedback, the user can decide on local refinements and steer the approximation level during the analysis. We demonstrate our technique with several datasets, in a real-world research scenario and for the real-time analysis of high-dimensional streams to illustrate its effectiveness for interactive data analysis.", "histories": [["v1", "Sat, 5 Dec 2015 12:05:52 GMT  (3418kb)", "https://arxiv.org/abs/1512.01655v1", null], ["v2", "Tue, 8 Dec 2015 14:56:25 GMT  (4842kb)", "http://arxiv.org/abs/1512.01655v2", null], ["v3", "Thu, 16 Jun 2016 09:36:40 GMT  (4967kb)", "http://arxiv.org/abs/1512.01655v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["nicola pezzotti", "boudewijn p f lelieveldt", "laurens van der maaten", "thomas h\\\"ollt", "elmar eisemann", "anna vilanova"], "accepted": false, "id": "1512.01655"}, "pdf": {"name": "1512.01655.pdf", "metadata": {"source": "CRF", "title": "Approximated and User Steerable tSNE for Progressive Visual Analytics", "authors": ["Nicola Pezzotti", "Boudewijn P.F. Lelieveldt", "Laurens van der Maaten", "Thomas H\u00f6llt", "Elmar Eisemann", "Anna Vilanova"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 151 2.01 655v 3 [cs.C V] 16 Jun 2016 IEEE TRANSACTIONS ON VISUALISATION AND COMPUTER GRAPHICS, VOL. -, NO. -, MONTH - 1Index Terms - High Dimensional Data, Dimensionality Reduction, Progressive Visual Analytics, Approximate Computation"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they"}, {"heading": "2 RELATED WORK", "text": "In fact, most of them are able to survive themselves by seeking their own identity. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) (...) () () () () () () (...) () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () ()) () () ()) () () ()) () () () ()) () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() (() (() () () () (() () ((() () (() () (((() () (() (() (() (() (() (() ((() (() (() (((()) (((()) ((((())) ((((())) (((((((((())))) ((((((((((((())))))) (((((("}, {"heading": "3 TSNE", "text": "In this section, we provide a brief introduction to tSNE [5], which is necessary to explain our contribution power = 1. tSNE interprets the total distances between the data points in high-dimensional space as symmetric probability of distribution P. Similarly, a distributional equality Q is calculated, which describes the similarity in low-dimensional space. The goal is to achieve a representation, called embedding, in low-dimensional space, in which Q faithfully represents P. This is achieved by optimizing the positions in low-dimensional space to minimize the cost function C given by the Kullback conductor (KL)."}, {"heading": "3.1 Barnes-Hut-SNE", "text": "In the original tSNE, the force is calculated using a bruteforce approach, resulting in a computational and memory complexity of O (N2). Barnes-Hut-SNE (BH-SNE) [4], [6] is an evolution of the tSNE algorithm, which introduces two different approaches to reduce the computational complexity to O (N log (N) and the memory complexity to O (N). The first approach is based on the observation that the probability pij is infinitely low if xi and xj are dissimilar. Therefore, the similarities of a data point xi can be calculated taking into account only those points that belong to the group of closest neighbours Ni. The cardinality of Ni can be set to K = 3\u00b5, with \u00b5 describing the used perplexity and the V2xxi value rounding to the next lower integer."}, {"heading": "4 A-TSNE FOR PROGRESSIVE VISUAL ANALYTICS", "text": "In this work we will have to deal with the question to what extent we will be able to get the situation under control, \"he told the Deutsche Presse-Agentur.\" I think we will be able to get the situation under control, \"he told the Deutsche Presse-Agentur.\" I don't think we will be able to get the situation under control, \"he told the Deutsche Presse-Agentur.\" I don't think we will be able to get the situation under control, \"he told the Deutsche Presse-Agentur.\" I don't think we will be able to get the situation under control. \""}, {"heading": "4.1 A-tSNE", "text": "A-tSNE improves the BH-SNE algorithm by using fast and approximate KNN calculations to form the approximate high-dimensional joint probability distribution PA, rather than the exact distribution P. The cost function C (PA, QA) is therefore minimized to obtain the approximate embedding described by QA. The similarity between points can be calculated by using the set of approximate neighbors NAi, rather than the exact neighborhood Ni (see Eq. 9). We define the precision of the algorithm as such, which describes the average percentage of points in the approximate neighborhood NAi belonging to the exact neighborhood Ni belonging to the exact neighborhood Ni: \u03c1 = N-N-k = Nk structures (10), where the precision of the neighborhood indicates the cardinality of Nk is specified indirectly by the user, as explained in Sec.3.1, as three times the value of the perplexity."}, {"heading": "4.2 Approximated KNN", "text": "We achieve different levels of precision using different parameterizations of an approximate KNN algorithm called Forest of Randomized Kd-Trees. In this section, we describe this technique and how its parameters affect the precision \u03c1.When the dimensionality of the data is high, there are no exact KNN algorithms that work better than linear search [37]. Therefore, the development of approximate KNN algorithms is required to deal with high-dimensional spaces. An examination of existing algorithms, including an extensive set of experiments, can be found in the work of Muja et al. [38].In this work, we use a spatial division technique called Forest of Randomized KD-Trees [39] to calculate the approximate neighborhoods. This technique has proven to be fast and effective in querying high-dimensional spaces."}, {"heading": "4.3 Steerability", "text": "A-tSNE is computationally controllable [22] in the sense that the user can define the level of approach to specific, interesting areas. In this section, we present the changes we have made to the BH-SNE algorithm to allow the refinement of the approach. The refinement we propose occurs by calculating the exact neighborhood for a point at a time. This process results in a mixture of exact and approximate neighborhoods. For each updated neighborhood, a Gaussian distribution Pi is calculated and the sparse common probability distribution PA must be updated accordingly. However, this update is not easy. Firstly, it requires the symmetrization of PA into equivalent. 2 the combination of Gaussian distributions that are forced by different data points and, secondly, the sparse nature of the distribution PA makes quick updates.We solve these problems by observing that a direct calculation of PA can be avoided and the distribution of the two distributions can be enforced directly using the Gausses in two components."}, {"heading": "4.4 Performance and Accuracy Benchmarking", "text": "In this section we present a detailed performance analysis of A-tSNE compared to the BH-SNE data we have used to use several standard benchmark datasets. (All performance measurements were made using a DELL precision T3600 workstation with an 8-core Intel Xeon E5 1650 CPU @ 3.2GHz, 32GB RAM and an NVIDIA GTX 680. We use the MNIST dataset [35] (60k data-points with 784 dimensions), the NORB dataset [40], without applying a preliminary dimensionality reduction using a Principal Component Analysis. We use the MNIST dataset [35] (60k data-points with 784 dimensions), the NORB dataset [40]. The CIFAR-10 dataset-10 dataset [41] dataset (50k with 1024 dimensions) and the TIMIT dataset."}, {"heading": "5 INTERACTIVE ANALYSIS SYSTEM", "text": "With the help of A-tSNE, the user can start analyzing the data without waiting for the exact calculation of the similarities in the high-dimensional space. However, the embedding is created on the basis of approximate information. We present various strategies that the user can apply to refine the high-dimensional similarities, which leads to the generation of different and more precise embedding. During the refinement, the user must be aware of the approximation of the level in the embedding. Therefore, we present a visualization that supports the inspection of the approach level and at the same time is able to deal with the increased size of the embedding that A-tSNE is able to generate interactively. We also use the maneuverability of A-tSNE to allow direct manipulation of the high-dimensional data and its representation, for example by adding and removing data points or the high-dimensional representation of the data in the technical frameworks. Finally, we coordinate these in a technical framework."}, {"heading": "5.1 User Driven Refinement", "text": "The refinement method used to calculate an A-tSNE embedding works on a point basis, see paragraph 4.3. We propose three different strategies for selecting the points to be refined; the basic strategy is to refine the neighborhoods of all points in X in a random order. However, when computing resources are scarce, it makes sense to control the refinement process in order to increase precision in areas of embedding that the analyst finds interesting, e.g. based on initial visual clusters appearing in the embedding."}, {"heading": "5.1.1 User Selection", "text": "This strategy is less effective if only a few points are selected for refinement, since the forces exerted on its neighbors are still approximate, which can lead to an incorrect description of the high-dimensional data."}, {"heading": "5.1.2 Breadth-First Search", "text": "If only a few points are selected for refinement, we extend the process to their neighborhoods. Therefore, a wide-first visit [43] of the graph generated by the KNN relationships can be used for refinement. If a priority list [43] is used to track the points that need to be refined, different strategies can be implemented. For example, the priority in the queue can be given by the distances in high-dimensional space or it can be determined by a domain-specific criterion."}, {"heading": "5.1.3 Density-Based Refinement", "text": "If the user is more interested in gaining a global overview of the potential embedding, a density-based refinement strategy can be used instead of local refinement based on the observation that points in the less dense areas of high-dimensional space are responsible for creating the global relationship in a tSNE embedding [5]. Data points are refined in a sequence determined by the density in high-dimensional space. An indication of this density is the variance \u03c3i of the Gaussian distribution as explained in paragraph 3. This strategy can work within a custom selection or across the entire dataset."}, {"heading": "5.2 Visualization and Interaction", "text": "Visualizing the tSNE embedding is not instructive if it is not combined with the ability to inspect the high-dimensional data. Especially in the context of explorative data analysis, where a classification as shown in Fig. 2 is not available, such a solution is very helpful. In our system, the user can explore the embedding using density-based visualization. To visualize the high-dimensional data in a coordinated multiview frame, embedding choices are used. To display the approach level in the embedding, we use two specially designed visualizations."}, {"heading": "5.2.1 Density-Based Visualization", "text": "The visualization of the embedding using simple dots is influenced by visual clutter when the number of points is actively selected. (Density-based [44] visualizations are often used to show a tSNE embedding [4], [7], [12] because of their ability to visualize properties on different scales. We use a real-time kernel density estimate (KDE) [31] to create an interactive density-based visualization of the embedding. We use color changes to visualize choices, for example, to highlight data points analyzed in other views of the coordinated multiple view frame. KDE is used by assigning a value for each pixel p using the kernel density estimator f (p, h) as follows: f (p, h) = 1NN dataset i = 1G (| p \u2212 yi, h)."}, {"heading": "5.2.2 Visualization of the Approximation", "text": "During refinement, the precision of the high-dimensional similarities is gradually refined until they are accurate. To give the user the opportunity to check the level of approximation, we expand our density-based visualization to show the desired precision. Note that \u03c1i is different for each data point and the changes during the grafting process, as in paragraph 4.3For each pixel point, we assign a value given by the function a (p, h), which represents the approximate value representing the bandwidth h: a (p, h) = 1f (p, h) 1 \u2211 Ni = 1 \u03c1iN \u2211 i = 1\u03c1iG (| p \u2212 yi |, h), which represents the approximate values of the bandwidth h: a (p, h)."}, {"heading": "5.3 Data Manipulation", "text": "In paragraph 4.3, we show that we are able to update high-dimensional similarities between data points during gradient minimization. In this section, we take advantage of this possibility by introducing various operations that allow the analyst to manipulate the original data points in their high-dimensional attribute space. Embedding does not need to be recalculated, but evolves dynamically as the data changes. At the heart of an interactive exploration of data is the ability to add or remove data on demand, use different representations of the same data set, or adapt to changes in the data."}, {"heading": "5.3.1 Inserting Points", "text": "For a point xa that we want to add to the embedding, its neighborhood Na must be calculated. An approximate algorithm can be used to calculate the neighborhood, and a refinement can be planned. To complete the embedding, we need to check whether xa belongs to the KNN of each point in X. We define dMaxi as the maximum distance between a point xi and the points in its neighborhood Ni. Updating the neighborhood is written as follows:. When dMaxi is cracked, a priority queue is used to efficiently update dMaxi after inserting xa into a given neighborhood Ni. It is important to note that inserting xa into Ni does not decrease the estimated precision."}, {"heading": "5.3.2 Deleting Points", "text": "Removing a point xr-X is done by removing xr from the KNN of each point xi-X. This operation has a computational complexity of O (N). By removing xr from a neighborhood Ni, we reduce the number of xi neighbors to K \u2212 1 and a new neighbor must be found to maintain the accuracy level. However, the new point in the neighborhood is the most dissimilar to the points in Ni, so its attraction is rather low, and we suggest ignoring the contribution of the missing point, thus increasing the estimated precision \u03c1i by 1 / K. As for inserting a new data point, the forest of random KD trees in O (log (N)) is updated."}, {"heading": "5.3.3 Dimensionality Modification", "text": "We edit changes in the description of a single high-dimensional data point, e.g. when the data vary in time, by a combination of distance and addition operations. If the user wishes to completely change the high-dimensional representation of the data, e.g. by adding or removing dimensions, a new approximate probability distribution PA is calculated and the embedding can develop accordingly."}, {"heading": "5.4 Visual Analysis Tool", "text": "We implemented A-tSNE as a module in an integrated, interactive, multi-view system for analyzing high-dimensional data. Fig. 7 shows a screenshot of the system and its different views. Fig. 7. Screenshot of our integrated system with several linked views for interaction. The system includes an embedded viewer (a), a data viewer (b) and a refinement viewer (c). The interface is divided into two main areas. At the top, three different views are used to show the intermediate embedding (7a), data manipulation (f) and refinements (g) are at the bottom of the interface. The interface is divided into two main areas."}, {"heading": "6 IMPLEMENTATION", "text": "We implemented the system with a combination of C + + and Qt as well as OpenGL with custom shaders in GLSL to visualize the embedding. Where possible, we used the parallelization of our approach. Approximate neighborhoods are calculated using the FLANN library [37], which implements KNN algorithms. Condensation-based visualization is calculated on the GPU using OpenGL and GLSL shaders. A precalculated floating point texture is generated using a Gaussian kernel. A geometry shader is used to create a quad for each point that is colored using the precalculated texture, the KDE is obtained by drawing a frame buffer object using additive mixture [31]."}, {"heading": "7 CASE STUDY I: EXPLORATORY ANALYSIS OF GENE EXPRESSION IN THE MOUSE BRAIN", "text": "In this section, we show the advantages of using AtSNE in our visual analysis tool for the visual analysis of high-dimensional data. Therefore, we present a use case, based on the work of Mahfouz et al. [11], which uses tSNE to explore genetic expression at the corresponding spatial position [46]. The dataset is compiled from 61164 voxels obtained by slicing the mouse brain in 68 sub-regions. Each voxel is a 4345-dimensional vector containing genetic expression at the corresponding spatial position. tSNE is calculated using voxels as datapoints and expression of genes as high-dimensional space. Note that no spatial information is used to build the high-dimensional space. Mahfouz et al. Discuss the hypothesis that genetic information can be used to distinguish anatomical structures in the brain."}, {"heading": "8 CASE STUDY II: REAL-TIME MONITORING OF HIGH-DIMENSIONAL STREAMS", "text": "It is the time in which people in the USA and in other countries set out in search of new ways to develop. (...) It is the time in which people in the USA, in Europe and in the whole world are in search of new ways and ways. (...) It is the time in which the world has changed. (...) It is the time in which the world has changed. (...) It is the time in which the world has changed. \"(...) It is the time in which we have caught ourselves in, in which we live, in which we live, in which we live, in which we live. (...) It is the time in which we live. (...) It is the time in which we live. (...) It is the time in which we live. (...) It is the time in which we live. (...) It is the time in which we live. (...) It is the time in which we live. (...) It is the time in which we live. (...) It is the time in which we live. (...) It is the time in which we live. (...) It is the time in which we live. (...). (... It is the time in which we live. (...) We live."}, {"heading": "9 CONCLUSIONS", "text": "Motivated by the need for interactivity in Visual Analytics, we developed Approximated-tSNE (A-tSNE). A-tSNE enables the fast generation of Approximated tSNE Embeddings. We used a fast approximated KNN algorithm to calculate the high-dimensional similarities. Our algorithm is designed to be used within the context of Progressive Visual Analytics, giving the user a quick preview of the data. The insights gained with the approximated embeddings can be validated by removing the approximation in interesting areas with different strategies. The user is aware of the level of approximation in the embeddings by using two specifically designed visualizations."}], "references": [{"title": "Parallel coordinates", "author": ["A. Inselberg", "B. Dimsdale"], "venue": "Human- Machine Interactive Systems. Springer, 1991, pp. 199\u2013233.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1991}, {"title": "Printer graphics for clustering.", "author": ["J.A. Hartigan"], "venue": "Journal of Statistical Computing and Simulation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1975}, {"title": "Dimensionality reduction: A comparative review", "author": ["L. van der Maaten", "E.O. Postma", "H.J. van den Herik"], "venue": "pp. 66\u201371, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Accelerating t-sne using tree-based algorithms", "author": ["L. van der Maaten"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 3221\u2013 3245, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. 2579-2605, p. 85, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Barnes-Hut-SNE", "author": ["L. van der Maaten"], "venue": "International Conference on Learning Representations, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "viSNE enables visualization of high dimensional single-cell data and reveals phenotypic heterogeneity of leukemia", "author": ["E.-a. D. Amir", "K.L. Davis", "M.D. Tadmor", "E.F. Simonds", "J.H. Levine", "S.C. Bendall", "D.K. Shenfeld", "S. Krishnaswamy", "G.P. Nolan", "D. Pe\u2019er"], "venue": "Nature biotechnology, vol. 31, no. 6, pp. 545\u2013552, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Deorphanizing the human transmembrane genome: A landscape of uncharacterized membrane proteins", "author": ["J.J. Babcock", "M. Li"], "venue": "Acta Pharmacologica Sinica, vol. 35, no. 1, pp. 11\u201323, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "High-dimensional analysis of the murine myeloid cell system", "author": ["B. Becher", "A. Schlitzer", "J. Chen", "F. Mair", "H.R. Sumatoh", "K.W.W. Teng", "D. Low", "C. Ruedl", "P. Riccardi-Castagnoli", "M. Poidinger"], "venue": "Nature immunology, vol. 15, no. 12, pp. 1181\u20131189, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Alignmentfree visualization of metagenomic data by nonlinear dimension reduction", "author": ["C.C. Laczny", "N. Pinel", "N. Vlassis", "P. Wilmes"], "venue": "Scientific reports, vol. 4, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing the spatial gene expression organization in the brain through nonlinear similarity embeddings", "author": ["A. Mahfouz", "M. van de Giessen", "L. van der Maaten", "S. Huisman", "M. Reinders", "M.J. Hawrylycz", "B.P. Lelieveldt"], "venue": "Methods, vol. 73, pp. 79 \u2013 89, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic classification of cellular expression by nonlinear stochastic embedding (ACCENSE)", "author": ["K. Shekhar", "P. Brodin", "M.M. Davis", "A.K. Chakraborty"], "venue": "Proceedings of the National Academy of Sciences, vol. 111, no. 1, pp. 202\u2013207, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Progressive visual analytics: User-driven visual exploration of in-progress analytics", "author": ["C. Stolper", "A. Perer", "D. Gotz"], "venue": "Visualization and Computer Graphics, IEEE Transactions on, vol. 20, no. 12, pp. 1653\u20131662, Dec 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Opening the black box: Strategies for increased user involvement in existing algorithm implementations", "author": ["T. M\u00fchlbacher", "H. Piringer", "S. Gratzl", "M. Sedlmair", "M. Streit"], "venue": "Visualization and Computer Graphics, IEEE Transactions on, vol. 20, no. 12, pp. 1643\u20131652, Dec 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "PIVE: A per-iteration visualization environment for supporting real-time interactions with computational methods", "author": ["J. Choo", "H. Kim", "C. Lee", "H. Park"], "venue": "Visual Analytics Science and Technology (VAST), 2014 IEEE Symposium on, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "DeViSE: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2121\u2013 2129.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014, pp. 580\u2013587.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, Feb. 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Scalable optimization of neighbor embedding for visualization", "author": ["Z. Yang", "J. Peltonen", "S. Kaski"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), 2013, pp. 127\u2013135.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Visual analytics infrastructures: From data management to exploration", "author": ["J.-D. Fekete"], "venue": "Computer, vol. 46, no. 7, pp. 22\u201329, July 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Trust me, i\u2019m partially right: Incremental visualization lets analysts explore large datasets faster", "author": ["D. Fisher", "I. Popov", "S. Drucker", "m. Schraefel"], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ser. CHI \u201912, 2012, pp. 1673\u20131682.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "A survey of computational steering environments", "author": ["J.D. Mulder", "J.J. van Wijk", "R. van Liere"], "venue": "Future generation computer systems, vol. 15, no. 1, pp. 119\u2013129, 1999.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "An algorithm for finding best matches in logarithmic expected time", "author": ["J.H. Friedman", "J.L. Bentley", "R.A. Finkel"], "venue": "ACM Transactions on Mathematical Software (TOMS), vol. 3, no. 3, pp. 209\u2013 226, 1977.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1977}, {"title": "Data structures and algorithms for nearest neighbor search in general metric spaces", "author": ["P.N. Yianilos"], "venue": "Proceedings of the fourth annual ACM-SIAM Symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 1993, pp. 311\u2013321.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1993}, {"title": "Steerable, progressive multidimensional scaling", "author": ["M. Williams", "T. Munzner"], "venue": "Information Visualization, 2004. INFOVIS 2004. IEEE Symposium on. IEEE, 2004, pp. 57\u201364.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "A fast approximation to multidimensional scaling", "author": ["T. Yang", "J. Liu", "L. McMillan", "W. Wang"], "venue": "Proceedings of the ECCV Workshop on Computation Intensive Methods for Computer Vision (CIMCV), 2006, pp. 354\u2013359.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Local affine multidimensional projection", "author": ["P. Joia", "F. Paulovich", "D. Coimbra", "J. Cuminato", "L. Nonato"], "venue": "Visualization and Computer Graphics, IEEE Transactions on, vol. 17, no. 12, pp. 2563\u20132571, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Two-phase mapping for projecting massive data sets", "author": ["F.V. Paulovich", "C.T. Silva", "L.G. Nonato"], "venue": "Visualization and Computer Graphics, IEEE Transactions on, vol. 16, no. 6, pp. 1281\u20131290, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Least square projection: A fast high-precision multidimensional projection technique and its application to document mapping", "author": ["F.V. Paulovich", "L.G. Nonato", "R. Minghim", "H. Levkowitz"], "venue": "IEEE Transactions on Visualization and Computer Graphics, vol. 14, no. 3, pp. 564\u2013575, 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Dimensionality reduction for documents with nearest neighbor queries", "author": ["S. Ingram", "T. Munzner"], "venue": "Neurocomputing, vol. 150, pp. 557\u2013569, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Interactive visualization of streaming data with kernel density estimation", "author": ["O. Lampe", "H. Hauser"], "venue": "Pacific Visualization Symposium (PacificVis), 2011 IEEE, 2011, pp. 171\u2013178.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "iVisClassifier: An interactive visual analytics system for classification based on supervised dimension reduction.", "author": ["J. Choo", "H. Lee", "J. Kihm", "H. Park"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Gravitational N-Body Simulations", "author": ["S.J. Aarseth"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "A hierarchical O(N log N) force-calculation algorithm", "author": ["J. Barnes", "P. Hut"], "venue": "Nature, vol. 324, no. 4, pp. 446\u2013449, 1986.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1986}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1999}, {"title": "Graph drawing by force-directed placement", "author": ["T.M. Fruchterman", "E.M. Reingold"], "venue": "Software: Practice and experience, vol. 21, no. 11, pp. 1129\u20131164, 1991.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1991}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D.G. Lowe"], "venue": "International Conference on Computer Vision Theory and Application VISSAPP\u201909), 2009, pp. 331\u2013340.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Scalable nearest neighbor algorithms for high dimensional data", "author": ["M. Muja", "D. Lowe"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 36, no. 11, pp. 2227\u20132240, Nov 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimised kd-trees for fast image descriptor matching", "author": ["C. Silpa-Anan", "R. Hartley"], "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, 2008, pp. 1\u20138.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F.J. Huang", "L. Bottou"], "venue": "Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, vol. 2. IEEE, 2004, pp. II\u201397.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Computer Science Department, University of Toronto, Tech. Rep, vol. 1, no. 4, p. 7, 2009.  IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. -, NO. -, MONTH -  14", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Large margin gaussian mixture modeling for phonetic classification and recognition", "author": ["F. Sha", "L.K. Saul"], "venue": "Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on, vol. 1. IEEE, 2006, pp. I\u2013I.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Density estimation for statistics and data analysis", "author": ["B.W. Silverman"], "venue": "CRC press,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1986}, {"title": "A survey on interactive lenses in visualization", "author": ["C. Tominski", "S. Gladisch", "U. Kister", "R. Dachselt", "H. Schumann"], "venue": "EuroVis State-ofthe-Art Reports, pp. 43\u201362, 2014.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Genome-wide atlas of gene expression in the adult mouse brain", "author": ["E.S. Lein", "M.J. Hawrylycz", "N. Ao", "M. Ayres", "A. Bensinger", "A. Bernard", "A.F. Boe", "M.S. Boguski", "K.S. Brockway", "E.J. Byrnes"], "venue": "Nature, vol. 445, no. 7124, pp. 168\u2013176, 2007.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2007}, {"title": "Creating and benchmarking a new dataset for physical activity monitoring", "author": ["A. Reiss", "D. Stricker"], "venue": "Proceedings of the 5th International Conference on PErvasive Technologies Related to Assistive Environments. ACM, 2012, p. 40.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Direct visualizations such as parallel coordinates [1] or scatterplot matrices [2] work well for a few dimensions, but do not scale to hundreds or thousands of dimensions.", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "Direct visualizations such as parallel coordinates [1] or scatterplot matrices [2] work well for a few dimensions, but do not scale to hundreds or thousands of dimensions.", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "viable techniques [3].", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].", "startOffset": 47, "endOffset": 50}, {"referenceID": 6, "context": "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].", "startOffset": 237, "endOffset": 240}, {"referenceID": 7, "context": "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].", "startOffset": 242, "endOffset": 245}, {"referenceID": 8, "context": "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].", "startOffset": 247, "endOffset": 250}, {"referenceID": 9, "context": "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].", "startOffset": 252, "endOffset": 256}, {"referenceID": 10, "context": "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].", "startOffset": 258, "endOffset": 262}, {"referenceID": 11, "context": "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].", "startOffset": 264, "endOffset": 268}, {"referenceID": 12, "context": "[13], as well as M\u00fchlbacher et", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] introduced Progressive Visual Analytics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] and can therefore be used directly for a per-iteration visualization, as well as interaction with the intermediate results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Even with a per-iteration visualization of the intermediate results [13], [14], [15] the initialization time will force the user to wait minutes, or even hours, before the first intermediate result can be generated on a state-of-theart desktop computer.", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "Even with a per-iteration visualization of the intermediate results [13], [14], [15] the initialization time will force the user to wait minutes, or even hours, before the first intermediate result can be generated on a state-of-theart desktop computer.", "startOffset": 74, "endOffset": 78}, {"referenceID": 14, "context": "Even with a per-iteration visualization of the intermediate results [13], [14], [15] the initialization time will force the user to wait minutes, or even hours, before the first intermediate result can be generated on a state-of-theart desktop computer.", "startOffset": 80, "endOffset": 84}, {"referenceID": 4, "context": "The tSNE [5] algorithm builds the foundation of this work.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].", "startOffset": 171, "endOffset": 174}, {"referenceID": 7, "context": "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].", "startOffset": 176, "endOffset": 179}, {"referenceID": 8, "context": "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].", "startOffset": 181, "endOffset": 184}, {"referenceID": 9, "context": "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].", "startOffset": 186, "endOffset": 190}, {"referenceID": 10, "context": "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].", "startOffset": 192, "endOffset": 196}, {"referenceID": 11, "context": "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].", "startOffset": 198, "endOffset": 202}, {"referenceID": 15, "context": "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].", "startOffset": 204, "endOffset": 208}, {"referenceID": 16, "context": "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].", "startOffset": 210, "endOffset": 214}, {"referenceID": 17, "context": "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].", "startOffset": 216, "endOffset": 220}, {"referenceID": 3, "context": "An evolution of the algorithm, called Barnes-Hut-SNE (BH-SNE) [4], [6], reduces the computational complexity to O(N log(N)) and the memory complexity to O(N).", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "An evolution of the algorithm, called Barnes-Hut-SNE (BH-SNE) [4], [6], reduces the computational complexity to O(N log(N)) and the memory complexity to O(N).", "startOffset": 67, "endOffset": 70}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "New analytical tools and algorithms, which are able to trade accuracy for speed and offer the possibility to interactively refine results [20], [21], are needed to deal with the scalability issues of existing", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": "New analytical tools and algorithms, which are able to trade accuracy for speed and offer the possibility to interactively refine results [20], [21], are needed to deal with the scalability issues of existing", "startOffset": 144, "endOffset": 148}, {"referenceID": 13, "context": "[14] defined different strategies to increase the user involvement in existing algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] defined the term Progressive Visual Analytics that describes techniques that allow the analyst to directly interact with the analytics process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "approximation of tSNE\u2019s initialization stage, followed by a user steerable [22] refinement of the level of approximation.", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "Under these conditions, a traditional algorithm and data structure, such as a KD-Tree [23], will not perform well.", "startOffset": 86, "endOffset": 90}, {"referenceID": 5, "context": "In the BH-SNE [6] algorithm, a Vantage-Point Tree [24] is used for the KNN search, but it is slow to query.", "startOffset": 14, "endOffset": 17}, {"referenceID": 23, "context": "In the BH-SNE [6] algorithm, a Vantage-Point Tree [24] is used for the KNN search, but it is slow to query.", "startOffset": 50, "endOffset": 54}, {"referenceID": 24, "context": "For example MDSteer [25] works on a subset of the data and allows the user to control the insertion of points by selecting areas in the reduced space.", "startOffset": 20, "endOffset": 24}, {"referenceID": 25, "context": "[26] present a dimensionality-reduction technique using a dissimilarity matrix as input.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] and Paulovich et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28]", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29], propose the use of a non-linear dimensionality-reduction algorithm on a small number of automatically-selected control points.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Ingram and Munzner\u2019s Q-SNE [30] is based on a similar idea, also using Approximated KNN queries for the computation of the high-dimensional similarities.", "startOffset": 27, "endOffset": 31}, {"referenceID": 3, "context": "Density-based visualization of the tSNE embedding has been used in several works [4], [7], [12], however, they employ slow to compute offline techniques.", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "Density-based visualization of the tSNE embedding has been used in several works [4], [7], [12], however, they employ slow to compute offline techniques.", "startOffset": 86, "endOffset": 89}, {"referenceID": 11, "context": "Density-based visualization of the tSNE embedding has been used in several works [4], [7], [12], however, they employ slow to compute offline techniques.", "startOffset": 91, "endOffset": 95}, {"referenceID": 30, "context": "In our work, we integrate real time Kernel Density Estimation (KDE) as described by Lampe and Hauser [31].", "startOffset": 101, "endOffset": 105}, {"referenceID": 31, "context": "The iVisClassifier system [32] is an example of such a solution.", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "In this section, we provide a short introduction to tSNE [5], which is necessary to explain our contribution.", "startOffset": 57, "endOffset": 60}, {"referenceID": 32, "context": "The gradient descent can be seen as a N-body simulation [33], where each data-point exerts an attractive and a repulsive force on all the other points (F attr i and F rep i ).", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "Barnes-Hut-SNE (BH-SNE) [4], [6] is an evolution of the tSNE algorithm that introduces two different approximations to reduce the computational complexity to O(N log(N)) and the memory complexity to O(N).", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "Barnes-Hut-SNE (BH-SNE) [4], [6] is an evolution of the tSNE algorithm that introduces two different approximations to reduce the computational complexity to O(N log(N)) and the memory complexity to O(N).", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "Without compromising the quality of the embedding [4], [6], we can adopt a sparse approximation of the high-dimensional similarities.", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "Without compromising the quality of the embedding [4], [6], we can adopt a sparse approximation of the high-dimensional similarities.", "startOffset": 55, "endOffset": 58}, {"referenceID": 23, "context": "The computation of the K-Nearest Neighbors is performed using a Vantage-Point Tree (VP-Tree) [24].", "startOffset": 93, "endOffset": 97}, {"referenceID": 33, "context": "As described above tSNE can be seen as an N-body simulation and thus the BarnesHut algorithm [34] can be used to reduce the computational complexity to O(N log(N)).", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "For further details, we refer to van der Maaten [4], [6].", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "For further details, we refer to van der Maaten [4], [6].", "startOffset": 53, "endOffset": 56}, {"referenceID": 21, "context": "The level of approximation can be refined by the analyst in interesting regions of the embedding, making A-tSNE a computational steerable algorithm [22].", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "timization process can be interpreted by the analyst while they change over time, as shown in previous work [14], [15].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "timization process can be interpreted by the analyst while they change over time, as shown in previous work [14], [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "the initialization of the technique, that cannot be implemented in an iterative way, creating a speed bump [13] in", "startOffset": 107, "endOffset": 111}, {"referenceID": 34, "context": "In the following section, we describe the A-tSNE algorithm in detail using the MNIST [35] dataset for illustration.", "startOffset": 85, "endOffset": 89}, {"referenceID": 35, "context": "To better understand the effect of the approximated queries, it is useful to interpret the BH-SNE algorithm as a force-directed layout algorithm [36], which acts on an", "startOffset": 145, "endOffset": 149}, {"referenceID": 36, "context": "search [37].", "startOffset": 7, "endOffset": 11}, {"referenceID": 37, "context": "[38].", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "In this work, we use a space partitioning technique called Forest of Randomized KD-Trees [39] to compute the approximated neighborhoods.", "startOffset": 89, "endOffset": 93}, {"referenceID": 36, "context": "to be fast and effective in querying of high-dimensional spaces [37].", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "A KD-Tree [23] is a binary tree used to partition a kdimensional space.", "startOffset": 10, "endOffset": 14}, {"referenceID": 36, "context": "[37] and expose only the single precision parameter \u03c1 to the user.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "A-tSNE is computationally steerable [22], in the sense that the user can define the level of approximation to specific, interesting areas.", "startOffset": 36, "endOffset": 40}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "We apply the same preprocessing steps as presented by van der Maaten [4], without applying a preliminary dimensionality-reduction by means of a Principal Component Analysis.", "startOffset": 69, "endOffset": 72}, {"referenceID": 34, "context": "We use the MNIST dataset [35] (60k data-points with 784 dimensions), the NORB dataset [40] (24300 data-points with 9216 dimensions), the CIFAR-10 dataset [41] (50k points with 1024 dimensions) and the", "startOffset": 25, "endOffset": 29}, {"referenceID": 39, "context": "We use the MNIST dataset [35] (60k data-points with 784 dimensions), the NORB dataset [40] (24300 data-points with 9216 dimensions), the CIFAR-10 dataset [41] (50k points with 1024 dimensions) and the", "startOffset": 86, "endOffset": 90}, {"referenceID": 40, "context": "We use the MNIST dataset [35] (60k data-points with 784 dimensions), the NORB dataset [40] (24300 data-points with 9216 dimensions), the CIFAR-10 dataset [41] (50k points with 1024 dimensions) and the", "startOffset": 154, "endOffset": 158}, {"referenceID": 41, "context": "TIMIT dataset [42], consisting of 1M data-points with 39 dimensions.", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "Throughout the experiments we used a parameter setup similar to the one used to benchmark the BHSNE [4], [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 5, "context": "Throughout the experiments we used a parameter setup similar to the one used to benchmark the BHSNE [4], [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 36, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Computation time of the high-dimensional similarities in the MNIST dataset by using BH-SNE [6] and by using A-tSNE with different parameters.", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "areas of the high-dimensional space are responsible for the creation of the global relationship in a tSNE embedding [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 42, "context": "Density-based [44] visualizations are commonly used to show a tSNE embedding [4], [7], [9], [12] because of their ability to visualize features at different scales.", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "Density-based [44] visualizations are commonly used to show a tSNE embedding [4], [7], [9], [12] because of their ability to visualize features at different scales.", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "Density-based [44] visualizations are commonly used to show a tSNE embedding [4], [7], [9], [12] because of their ability to visualize features at different scales.", "startOffset": 82, "endOffset": 85}, {"referenceID": 8, "context": "Density-based [44] visualizations are commonly used to show a tSNE embedding [4], [7], [9], [12] because of their ability to visualize features at different scales.", "startOffset": 87, "endOffset": 90}, {"referenceID": 11, "context": "Density-based [44] visualizations are commonly used to show a tSNE embedding [4], [7], [9], [12] because of their ability to visualize features at different scales.", "startOffset": 92, "endOffset": 96}, {"referenceID": 30, "context": "We apply a real-time kernel density estimation (KDE) [31] for the creation of an interactive density-based visualization of the embedding.", "startOffset": 53, "endOffset": 57}, {"referenceID": 43, "context": "First, we introduce a Magic Lens [45] that shows the approximation with a minimal conceal of the data.", "startOffset": 33, "endOffset": 37}, {"referenceID": 36, "context": "The approximated neighborhoods are computed using the FLANN library [37], which implements KNN algorithms.", "startOffset": 68, "endOffset": 72}, {"referenceID": 30, "context": "A geometry shader is used to generate a quad for each point that is colored using the precomputed texture, the KDE is obtained by drawing into a Frame Buffer Object using an additive blending [31].", "startOffset": 192, "endOffset": 196}, {"referenceID": 10, "context": "[11], who use tSNE to explore the Allen Mouse Brain dataset [46].", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[11], who use tSNE to explore the Allen Mouse Brain dataset [46].", "startOffset": 60, "endOffset": 64}, {"referenceID": 45, "context": "As proof of concept, we selected a dataset for physical activity monitoring [47] that comprises readings of three Inertial Measurement Units (IMU) and a heart rate monitor applied to 9 different subjects.", "startOffset": 76, "endOffset": 80}], "year": 2016, "abstractText": "Progressive Visual Analytics aims at improving the interactivity in existing analytics techniques by means of visualization as well as interaction with intermediate results. One key method for data analysis is dimensionality reduction, for example, to produce 2D embeddings that can be visualized and analyzed efficiently. t-Distributed Stochastic Neighbor Embedding (tSNE) is a well-suited technique for the visualization of several high-dimensional data. tSNE can create meaningful intermediate results but suffers from a slow initialization that constrains its application in Progressive Visual Analytics. We introduce a controllable tSNE approximation (A-tSNE), which trades off speed and accuracy, to enable interactive data exploration. We offer real-time visualization techniques, including a density-based solution and a Magic Lens to inspect the degree of approximation. With this feedback, the user can decide on local refinements and steer the approximation level during the analysis. We demonstrate our technique with several datasets, in a real-world research scenario and for the real-time analysis of high-dimensional streams to illustrate its effectiveness for interactive data analysis.", "creator": null}}}