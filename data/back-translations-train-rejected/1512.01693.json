{"id": "1512.01693", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2015", "title": "Deep Attention Recurrent Q-Network", "abstract": "A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels. Its creators at the Google DeepMind's team called the approach: Deep Q-Network (DQN). We present an extension of DQN by \"soft\" and \"hard\" attention mechanisms. Tests of the proposed Deep Attention Recurrent Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of performance superior to that of DQN. Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions.", "histories": [["v1", "Sat, 5 Dec 2015 18:35:40 GMT  (126kb,D)", "http://arxiv.org/abs/1512.01693v1", "7 pages, 5 figures, Deep Reinforcement Learning Workshop, NIPS 2015"]], "COMMENTS": "7 pages, 5 figures, Deep Reinforcement Learning Workshop, NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ivan sorokin", "alexey seleznev", "mikhail pavlov", "aleksandr fedorov", "anastasiia ignateva"], "accepted": false, "id": "1512.01693"}, "pdf": {"name": "1512.01693.pdf", "metadata": {"source": "CRF", "title": "Deep Attention Recurrent Q-Network", "authors": ["Ivan Sorokin", "Alexey Seleznev", "Mikhail Pavlov", "Aleksandr Fedorov", "Anastasiia Ignateva"], "emails": ["5visionteam@gmail.com"], "sections": [{"heading": "1 Introduction and Related Work", "text": "In fact, most of them will be able to play by the rules they have established in the past, and they will be able to play by the rules they have established in the past."}, {"heading": "2 Deep Attention Recurrent Q-Network", "text": "The DARQN architecture is shown schematically in Figure 1 and consists of three types of networks: convolutional (CNN), attention, and recurrent. At each point in time, CNN receives a representation of the current state of play st in the form of a visual frame, on the basis of which it produces a series of D-characteristic cards, each with a dimension of m \u00b7 m. The attention network transforms these cards into a series of vectors vt = {v1t,..., vLt}, vit-RD, L = m \u00b2 m, and outputs their linear combination zt-RD, which is called context vector. The recursive network, in our case LSTM, takes as input the context vector, along with the previous hidden state ht-1 and the memory state ct \u2212 1, and produces a hidden state that is used by (i) a linear layer to evaluate the Q value of each action."}, {"heading": "2.1 Soft attention", "text": "The \"soft\" attention mechanism assumes that the context vector zt can be represented as a weighted sum of all vectors vit, each of which corresponds to the properties CNN has extracted in different image regions. Therefore, weights in this sum are chosen in relation to the vectors evaluated by the attention network g. The g network contains two fully connected layers followed by a Softmax activation. Its output can be written as follows: g (vit, ht \u2212 1) = exp (Linear (Tanh (v i t) + Wht \u2212 1))) / Z, (1) where Z is a normalizing constant, W is a weighting matrix, Linear (x) = Ax \u2212 b is an affine transformation with some weighting matrix A and bias b. Once we have defined the importance of each location vector vit, we can calculate the context vit vit: vit = L = i (vt \u2212 1)."}, {"heading": "2.2 Hard attention", "text": "The \"hard\" attention mechanism requires the sample of only one attention location of L, which is available at each time step and whose weights are the political parameters. To train a network of stochastic units, the statistical gradient-following algorithm REINFORCE [11] can be used. In the literature [8, 6] there are several successful examples of integrating this algorithm with deep learning. Unlike models proposed in these papers and trained by maximizing probability, the proposed algorithm is trained by minimizing a sequence of loss functions (3). Therefore, its training process is different. Suppose st (and therefore) the attention has been sampled from the distribution influenced by attention politics (es | vt, ht \u2212 1), a categorical distribution with parameters given by a soft layer of attention."}, {"heading": "3 Experiments", "text": "The proposed algorithm was tested on several popular games of the Atari 2600: Breakout, Seaquest, Space Invaders, Tutankham and Gopher. The results obtained were compared with the corresponding results of (i) DQN proposed by Mnih et al. [1] and implemented in Torch, (ii) DRQN proposed by Hausknecht and Stone [2] and implemented in Caffe. Our realization of DARQN is based on the source code [1] and is available online at 1.1https: / / github.com / 5vision / DARQN."}, {"heading": "3.1 Network Architecture", "text": "The revolutionary network architecture in DARQN is similar to that used in [1], with two distinctive features: the input is made by an 84 x 84 x 1 tensor, and the output of the last (third) layer contains 256 feature cards 7 x 7. The attention network takes 49 vectors as input, each vector has a dimension of 256. The number of hidden units in the attention network is set to 256. The LSTM network also includes 256 units, which is consistent with the number of outputs from the attention network.It is interesting to compare the DARQN capacity with the capacities of DQN and DRQN. Depending on the game type, they may vary slightly. In Seaquest, a game with 18 possible actions has both DQN and DRQN (with a roll-out step) 1, 693, 362 adjustable parameters, while the proposed hard and soft DARQN models have only 845, 8428 and 171 parameters."}, {"heading": "3.2 Hyper-parameters", "text": "In all experiments, the discount factor was set to \u03b3 = 0.99, the learning rate \u03b1 starts at 0.01 and decays linearly to 0.0025 over 1M steps for the soft attention model and from 0.0001 to 0.0025 for the hard attention model. The number of steps between target network updates was 10,000. Training took place in 5 million steps. The agent was evaluated after 50,000 steps based on the average reward per episode achieved by executing a greedy policy of = 0.05 for 25,000 steps. Experience memory size was 500,000 tuples. Memory was scanned to update the network every 4 steps with size 32 minibatches. The model was trained over time based on backpropogation. For each new minibatch, the initial hidden and memory states of the LSTM were set to zero. To update the weights, the RMSProp algorithm was used with a dynamics of 95."}, {"heading": "3.3 Results", "text": "The most important results of the model comparison to the five Atari games are presented in Table 1. It can be seen that not all games achieve the results that are superior to the corresponding results of DQN and DRQN. In order to give an insight into the advantages and disadvantages of the proposed models, the training process in the two games in which DARQN achieves the best and worst results is shown in Figure 2.On Seaquest. Both DARQN models have a high level of performance. However, the hard attention-based agent seems to be worse in terms of the soft one. In particular, he is unable to learn that the submarine has to recur regularly in order to survive. This problem can be attributed to one of the shortcomings of the approach applied in the training process of the hard attention mechanism, namely its propensity to adapt to a local optimum."}, {"heading": "4 Conclusion and Future Work", "text": "To test this model, a series of experiments were conducted on five Atari 2600 games, and the results obtained allow us to conclude that, despite less optimized parameters, our model outperforms the results of the original DQN model, showing greater generalization capability. In addition, our attention-based algorithm provides some insight into the logic of agent behavior by displaying the regions of the game screen on which the agent focuses when making decisions. Attention mechanisms can be considered an additional filter gate in the LSTM, which processes structured visual data generated by CNN for the entire image. Therefore, a promising direction for future research would be the application of multi-scale [14] or fleeting [8] visual attention mechanisms on DQN. The simple political gradient-based algorithm introduced to train the hard attention of the DARQN model has shown that future attention would be applied in a relatively low and chaotic way (13)."}, {"heading": "Acknowledgments", "text": "We would like to thank Deep Knowledge Venture for their financial support, and in developing the ideas presented here, we received helpful input from the organizers of the DeepHack.Game 2015 hackathon, in particular Sergey Plis (Datalytic Solutions), and Greg Scantlen, CEO of CreativeC.com, who let us work on his private GPU cloud."}], "references": [{"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["Matthew J. Hausknecht", "Peter Stone"], "venue": "CoRR, abs/1507.06527,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen"], "venue": "arXiv preprint arXiv:1507.04296,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Hado van Hasselt", "Arthur Guez", "David Silver"], "venue": "arXiv preprint arXiv:1509.06461,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "koray kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Misha Denil", "Loris Bazzani", "Hugo Larochelle", "Nando de Freitas"], "venue": "Neural computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S. Sutton", "David Mcallester", "Satinder Singh", "Yishay Mansour"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Learning wake-sleep recurrent attention models", "author": ["Jimmy Ba", "Roger Grosse", "Ruslan Salakhutdinov", "Brendan Frey"], "venue": "arXiv preprint arXiv:1509.06812,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["John Schulman", "Nicolas Heess", "Theophane Weber", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1506.05254,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The recent success of Deep Q-Learning (DQL) in mastering human-level control policies on a variety of different Atari 2600 games [1] inspires artificial intellegence researchers to seek possible improvements to Google DeepMind\u2019s algorithm in order to further enhance its learning abilities [2, 3, 4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "The recent success of Deep Q-Learning (DQL) in mastering human-level control policies on a variety of different Atari 2600 games [1] inspires artificial intellegence researchers to seek possible improvements to Google DeepMind\u2019s algorithm in order to further enhance its learning abilities [2, 3, 4].", "startOffset": 290, "endOffset": 299}, {"referenceID": 2, "context": "The recent success of Deep Q-Learning (DQL) in mastering human-level control policies on a variety of different Atari 2600 games [1] inspires artificial intellegence researchers to seek possible improvements to Google DeepMind\u2019s algorithm in order to further enhance its learning abilities [2, 3, 4].", "startOffset": 290, "endOffset": 299}, {"referenceID": 3, "context": "The recent success of Deep Q-Learning (DQL) in mastering human-level control policies on a variety of different Atari 2600 games [1] inspires artificial intellegence researchers to seek possible improvements to Google DeepMind\u2019s algorithm in order to further enhance its learning abilities [2, 3, 4].", "startOffset": 290, "endOffset": 299}, {"referenceID": 4, "context": "The goal of this concise paper is to present the authors\u2019 approach to addressing this challenge by providing DQN, a deep neural network used in DQL as an analogue of a classic actionutility function, with such tools of modern machine learning as Long Short-Term Memory (LSTM) [5] and visual attention mechanisms [6, 7, 8].", "startOffset": 276, "endOffset": 279}, {"referenceID": 5, "context": "The goal of this concise paper is to present the authors\u2019 approach to addressing this challenge by providing DQN, a deep neural network used in DQL as an analogue of a classic actionutility function, with such tools of modern machine learning as Long Short-Term Memory (LSTM) [5] and visual attention mechanisms [6, 7, 8].", "startOffset": 312, "endOffset": 321}, {"referenceID": 6, "context": "The goal of this concise paper is to present the authors\u2019 approach to addressing this challenge by providing DQN, a deep neural network used in DQL as an analogue of a classic actionutility function, with such tools of modern machine learning as Long Short-Term Memory (LSTM) [5] and visual attention mechanisms [6, 7, 8].", "startOffset": 312, "endOffset": 321}, {"referenceID": 7, "context": "The goal of this concise paper is to present the authors\u2019 approach to addressing this challenge by providing DQN, a deep neural network used in DQL as an analogue of a classic actionutility function, with such tools of modern machine learning as Long Short-Term Memory (LSTM) [5] and visual attention mechanisms [6, 7, 8].", "startOffset": 312, "endOffset": 321}, {"referenceID": 1, "context": "In particular, Hausknecht and Stone [2] pointed out that in practice, DQN decides on the next optimal action based on the visual information corresponding to the last four game states encountered by the agent.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "[1] was observed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "According to [1], it takes 12-14 days on a GPU to train the network.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "[3] proposed a new massively parallel version of the algorithm geared to address this problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Recent achievements of visual attention models in caption generation [6], object tracking [9, 7], and machine translation [10] have induced the authors of this paper to conduct a series of experiments so as to assess possible benefits from incorporating attention mechanisms into the structure of the \u2217The authors are members of the 5vision team from the hackathon DeepHack.", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "Recent achievements of visual attention models in caption generation [6], object tracking [9, 7], and machine translation [10] have induced the authors of this paper to conduct a series of experiments so as to assess possible benefits from incorporating attention mechanisms into the structure of the \u2217The authors are members of the 5vision team from the hackathon DeepHack.", "startOffset": 90, "endOffset": 96}, {"referenceID": 6, "context": "Recent achievements of visual attention models in caption generation [6], object tracking [9, 7], and machine translation [10] have induced the authors of this paper to conduct a series of experiments so as to assess possible benefits from incorporating attention mechanisms into the structure of the \u2217The authors are members of the 5vision team from the hackathon DeepHack.", "startOffset": 90, "endOffset": 96}, {"referenceID": 9, "context": "Recent achievements of visual attention models in caption generation [6], object tracking [9, 7], and machine translation [10] have induced the authors of this paper to conduct a series of experiments so as to assess possible benefits from incorporating attention mechanisms into the structure of the \u2217The authors are members of the 5vision team from the hackathon DeepHack.", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "The whole DARQN model is trained by minimizing a sequence of loss functions: Jt(\u03b8t) = Est,at\u223c\u03c1(\u00b7),rt [(Est+1\u223cE [Yt | st, at]\u2212Q(st, at; \u03b8t)) ], (3) where Yt = rt + \u03b3maxat+1 Q(st+1, at+1; \u03b8t\u22121) is an approximate target value, rt is an immediate reward after taking action at in state st, \u03b3 \u2208 [0, 1] is a discount factor, E is an environment distribution, \u03c1(st, at) is a behaviour distribution selected as -greedy strategy, \u03b8t is a vector of all DARQN weights, including those belonging to the attention network.", "startOffset": 290, "endOffset": 296}, {"referenceID": 0, "context": "[1], namely target network and experience replay.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "In order to train a network with stochastic units, the statistical gradient-following algorithm REINFORCE [11] may be used.", "startOffset": 106, "endOffset": 110}, {"referenceID": 7, "context": "In literature [8, 6], there are several successful examples of integrating this algorithm with Deep Learning.", "startOffset": 14, "endOffset": 20}, {"referenceID": 5, "context": "In literature [8, 6], there are several successful examples of integrating this algorithm with Deep Learning.", "startOffset": 14, "endOffset": 20}, {"referenceID": 11, "context": "Then, in the policy gradient approach [12], updates of the policy parameters may be written as: \u2206\u03b8 t \u221d \u2207\u03b8g t log \u03c0g(it | vt, ht\u22121)Rt, (5) where Rt is a future discounted return after the agent selects the attention location it.", "startOffset": 38, "endOffset": 42}, {"referenceID": 12, "context": "The final update rule for the attention network\u2019s parameters has the following form: \u03b8 t+1 = \u03b8 g t + \u03b1\u2207\u03b8g t log \u03c0g(it | vt, ht\u22121)(Gt \u2212 Yt) (6) where the expression Gt \u2212 Yt can be interpreted in terms of advantage function estimation [13].", "startOffset": 233, "endOffset": 237}, {"referenceID": 6, "context": "Training (6) can also be described [7] as adjusting the parameters \u03b8 t of the attention network so that the log-probability of attention location it that has led to a higher expected future reward is increased, while that of locations having produced a lower reward is decreased.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "In order to reduce a high variance of the stochastic gradient, a practical trick proposed in [6] is utilized.", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "[1] and implemented in Torch, (ii) DRQN suggested by Hausknecht and Stone [2] and implemented in Caffe.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1] and implemented in Torch, (ii) DRQN suggested by Hausknecht and Stone [2] and implemented in Caffe.", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "Our realization of DARQN is based on the source code [1] and is available online 1.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "The convolutional network architecture in DARQN is similar to that used in [1], except for two peculiarities: its input is a 84 \u00d7 84 \u00d7 1 tensor, and the output of its last (third) layer contains 256 feature maps 7\u00d7 7.", "startOffset": 75, "endOffset": 78}, {"referenceID": 13, "context": "Therefore, one promising direction of future research would be to apply multi-scale [14] or glimpse [8] visual attention mechanisms to DQN.", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "Therefore, one promising direction of future research would be to apply multi-scale [14] or glimpse [8] visual attention mechanisms to DQN.", "startOffset": 100, "endOffset": 103}, {"referenceID": 12, "context": "That is why another auspicious direction of future research would be (i) to test different techniques for reducing stochastic gradient variability [13], (ii) to apply different approaches to training stochastic attention networks [14, 15].", "startOffset": 147, "endOffset": 151}, {"referenceID": 13, "context": "That is why another auspicious direction of future research would be (i) to test different techniques for reducing stochastic gradient variability [13], (ii) to apply different approaches to training stochastic attention networks [14, 15].", "startOffset": 230, "endOffset": 238}, {"referenceID": 14, "context": "That is why another auspicious direction of future research would be (i) to test different techniques for reducing stochastic gradient variability [13], (ii) to apply different approaches to training stochastic attention networks [14, 15].", "startOffset": 230, "endOffset": 238}], "year": 2015, "abstractText": "A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels. Its creators at the Google DeepMind\u2019s team called the approach: Deep Q-Network (DQN). We present an extension of DQN by \u201csoft\u201d and \u201chard\u201d attention mechanisms. Tests of the proposed Deep Attention Recurrent Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of performance superior to that of DQN. Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions.", "creator": "LaTeX with hyperref package"}}}