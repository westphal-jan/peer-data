{"id": "1512.01712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2015", "title": "Generating News Headlines with Recurrent Neural Networks", "abstract": "We describe an application of an encoder-decoder recurrent neural network with LSTM units and attention to generating headlines from the text of news articles. We find that the model is quite effective at concisely paraphrasing news articles. Furthermore, we study how the neural network decides which input words to pay attention to, and specifically we identify the function of the different neurons in a simplified attention mechanism. Interestingly, our simplified attention mechanism performs better that the more complex attention mechanism on a held out set of articles.", "histories": [["v1", "Sat, 5 Dec 2015 23:41:22 GMT  (215kb,D)", "http://arxiv.org/abs/1512.01712v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["konstantin lopyrev"], "accepted": false, "id": "1512.01712"}, "pdf": {"name": "1512.01712.pdf", "metadata": {"source": "CRF", "title": "Generating News Headlines with Recurrent Neural Networks", "authors": ["Konstantin Lopyrev"], "emails": ["klopyrev@stanford.edu"], "sections": [{"heading": "1 Background", "text": "Recurrent neural networks have recently proven to be very effective for many transmission tasks - that is, they transform text from one form to another. Examples of such applications include machine translation [1,2] and speech recognition [3]. These models are trained on large amounts of input and expected output sequences and are then able to generate output sequences that have never been presented to the model before during the training. Recurrent neural networks have also recently been applied to read comprehension [4], where models are trained to retrieve facts or statements from input texts. Our work is closely related to [5], which also uses a neural network to generate news headlines using the same data set as this work. The main difference to this work is that they do not use recursive neural networks for coding, but a simpler attention-based model."}, {"heading": "2 Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Overview", "text": "The architecture consists of two parts - an encoder and a decoder - both of which by themselves form a recurrent neural network."}, {"heading": "2.2 Attention", "text": "Attention is a mechanism that helps the network better remember certain aspects of input, including names and numbers. The attention mechanism is used in the output of each word in the decoder. For each output word, the attention mechanism calculates a weight over each of the input words, which determines how much attention should be paid to that input word. Weights add up to 1, and are used to calculate a weighted average of the last hidden layers that are generated after processing each of the input words. This weighted average, relative to the context, is then entered into the Softmax layer along with the last hidden layer from the current step of decryption. We experiment with two different attention mechanisms, which we call complex attention, is the same as the dot mechanism in [2]. This mechanism is shown in Figure 2, which attention weight for the input word at the position is calculated, calculated, the last layer is calculated."}, {"heading": "3 Dataset", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Overview", "text": "The model is trained using the Stanford Linguistics English Gigaword Dataset. This dataset consists of perennial news articles from 6 major news agencies, including the New York Times and Associated Press. Each news article has a clearly defined headline and text, with the text divided into paragraphs. After pre-processing described below, the training data consists of 5.5M news articles with 236M words."}, {"heading": "3.2 Preprocessing", "text": "Articles that have no headline or text, or whose headline or text length exceeds 25 or 50 characters, respectively, are filtered out for computational efficiency.All rare words are replaced by the < unk > symbol, preserving only the 40,000 most common words.The data is split into a training set and a holdout set. The holdout set consists of articles from the last month of the data, with the penultimate month not included in either the training or holdout sets. This split helps ensure that neither the training nor holdout sets create nearly duplicate articles. Finally, the training data is randomly shuffled."}, {"heading": "3.3 Dataset Issues", "text": "There are a number of problems with the dataset used. There are many training examples where the headline actually does not summarize the text particularly well or does not summarize it at all, including many articles that are misformatted, with the actual headline in the text section and the headline containing words such as \"(For use by customers of the New York Times News service).\" There are many articles where the headline has an encoded form, such as \"biz-cover-1stld-writethru-nyt\" or \"bc-iraq-post 1stld-subpickup4thgraf.\" There has been no filtering of such articles. An ideal model should be able to solve such problems automatically, and attempts have been made, for example, to randomly insert generated words during training, as described in the Model section."}, {"heading": "4 Evaluation", "text": "The performance of the model was measured in two different ways: First, we looked at training and holdout losses. Second, we used the BLEU [12] evaluation metric over the next defined holdout set. For efficiency reasons, the holdout metrics were calculated using only 384 examples. The BLEU evaluation metric looks at which fraction of n-grams of different length of expected headlines is actually output by the model. It also takes into account the number of words generated compared to the number of words used in the expected headlines. Both are calculated using all 384 heldout examples rather than each individual example. For the exact definition see [12]."}, {"heading": "5 Analysis", "text": "Each model takes 4.5 days to train on a GTX 980 Ti GPU. Figures 4 and 5 show the ratings metrics depending on the training epoch. Note that in our lineup, the training loss is generally higher than the holdout loss, as we do not feed into generated words 10% of the time when calculating the holdout loss. The model is very effective at predicting headlines from the same newspapers on which it was trained. Table 1 lists 7 randomly selected examples from the examples given. Generally, the model seems to capture the core of the text and manages to paraphrase the text, sometimes with completely new words. However, it makes mistakes, for example in sentences 2, 4 and 7. The model performs much more mixed when used to generate headlines for news articles from sources that differ from the training. Table 2 shows generated headlines for articles from several major news sites. The Journal model performs quite well compared with the Post's article on Wall Street and Huffington's article comparisons very poorly, however."}, {"heading": "5.1 Understanding information stored in last layer of the neural network", "text": "We note that there are several ways to understand the function of the attention mechanism. Let's consider the formula for calculating the input to the softmax function: oyt \u2032 = Wcocyt \u2032 + Whohyt \u2032 + bowhere cyt \u2032 is the context that is calculated for the current step of decoding, hyt \u2032 is the last hidden layer from the current step of decoding, and Wco, Who and bo are model parameters. First, let's note that if we look at the word with the highest Wohyt \u2032 + bowe, we can get an idea of what exactly the hidden layer from the current step of decoding contributes to the final generated output. Similarly, if we can do the words with the highest Wcocyt \u2032 + bowe values for the attention context, the context is only a weighted sum above the hidden layer of the decoder, which we first consider to be related to it, and each of these are good words."}, {"heading": "5.2 Understanding how the attention weight vector is computed", "text": "We had an initial hypothesis of how the network calculates the attention weight vector. We assumed that the network roughly remembered which word should be generated next, and that the dot product hTxthyt would calculate the similarity between the remembered word and the actual word at the location. In fact, if the network remembered that the text was in the same space as the units of the last hidden layer of the encoder, the attention mechanism would allow the exact number to be obtained. An implication of this hypothesis is that the units of the last hidden layer of the decoder are able to form the units of the last hidden layer of the encoder."}, {"heading": "5.3 Errors", "text": "An error of the neural network mechanism is its tendency to fill in details when details are missing. For example, after simplifying the text given in Table 1, \"72 people died when a truck crashed into a ravine on Friday.\" The model predicts \"72 deaths in a truck accident in Russia.\" The model takes into account the fact that the accident occurred in Russia. These errors most often happen when the number of decryption bars is low, as the model no longer takes into account the decryption where the sentence ends prematurely before the invented details are issued. Occasionally, the network also issues a headline that has nothing to do with the input text (e.g. \"urgent,\" \"\" bc times \"or even\" individual purchases can be made by calling 212 - 556 - 4204 or - 1927 bars. \"This problem is caused by the fact that such headers occur frequently in the large data input commands, if we increase the probability that the number of bars used is very high."}, {"heading": "6 Future Work", "text": "We demonstrate above that the relapsing neural network learns to model complex linguistic phenomena because large amounts of training data are available. An interesting direction to follow is the use of a dataset like Gigaword to prepare a relapsing neural network, which is then finely tuned to solve a task like part-of-speech tagging on a much smaller dataset. Another way to improve the model is to use a bidirectional RNN. We suspect that the attention mechanism would work better with a bidirectional RNN as more information would be available to model some of the phenomena outlined in Table 3. In our current model, the network needs to make a decision on what values to use to assign the neurons to calculate the attention weight for the current input word, since only the current and previous words are available and none of the following words to facilitate the decision about the network following."}, {"heading": "7 Conclusion", "text": "We have trained a recurrent neural network of encoders and decoders with LSTM units and attention to generate news headlines using the text of news articles from the Gigaword dataset. Using only the first 50 words of a news article, the model generates a compact summary of those 50 words, and most of the time the summary is valid and grammatically correct. The model does not work quite as well with generic text and shows that many articles in Gigaword follow a particular form. We are studying two different versions of the attention mechanism with the aim of understanding how the model decides which words of the input text to pay attention to when generating each output word. We are introducing a simplified attention mechanism that uses a small set of neurons to calculate attention weights. This simplified mechanism facilitates the study of the function of the network. We find that the network learns to recognize linguistic phenomena, such as phenomena, objects, and entries, so that we recognize them."}, {"heading": "Acknowledgements", "text": "We would like to thank Thang Luong for his helpful suggestions and for passing on his machine translation code. We would like to thank Samy Bengio and Andrej Karpathy for suggesting some interesting design alternatives. Finally, we would like to thank Chris Re. \"Part of the code written in his laboratory as part of his research was used to complete this work."}], "references": [{"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "CoRR, abs/1508.04025,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "venue": "CoRR, abs/1303.5778,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "CoRR, abs/1506.03340,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "CoRR, abs/1509.00685,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Deep learning. Book in preparation for", "author": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": "CoRR, abs/1506.03099,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals"], "venue": "CoRR, abs/1508.01211,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "CoRR, abs/1409.2329,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Fei-Fei Li"], "venue": "CoRR, abs/1412.2306,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Examples of such applications include machine translation [1,2] and speech recognition [3].", "startOffset": 58, "endOffset": 63}, {"referenceID": 1, "context": "Examples of such applications include machine translation [1,2] and speech recognition [3].", "startOffset": 58, "endOffset": 63}, {"referenceID": 2, "context": "Examples of such applications include machine translation [1,2] and speech recognition [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "Recurrent neural networks have also been applied recently to reading comprehension [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "Our work is closely related to [5] who also use a neural network to generate news headlines using the same dataset as this work.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "We use the encoder-decoder architecture described in [1] and [2], and shown in figure 1.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "We use the encoder-decoder architecture described in [1] and [2], and shown in figure 1.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "Note that during training of the model it is necessary to use what is called \u201cteacher forcing\u201d [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "To overcome this disconnect, during training we randomly feed in a generated word, instead of the expected word, as suggested in [7].", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "Specifically, we do this 10% of the time, as also done in [8].", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "We use 4 hidden layers of LSTM units, specifically the variant described in [9].", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "We attempted using dropout as is also described in [9].", "startOffset": 51, "endOffset": 54}, {"referenceID": 9, "context": "We initialize the biases for each word in the softmax layer to the log-probability of its occurence in the training data, as suggested in [10].", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "01 along with the RMSProp [11] adaptive gradient method.", "startOffset": 26, "endOffset": 30}, {"referenceID": 1, "context": "The first attention mechanism, which we refer to as complex attention, is the same as the dot mechanism in [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 11, "context": "Second, we used the BLEU [12] evaluation metric over the holdout set, defined next.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "For the exact definition see [12].", "startOffset": 29, "endOffset": 33}, {"referenceID": 6, "context": "One solution worthy of investigation is to use the scheduled sampling mechanism described in [7].", "startOffset": 93, "endOffset": 96}], "year": 2015, "abstractText": "We describe an application of an encoder-decoder recurrent neural network with LSTM units and attention to generating headlines from the text of news articles. We find that the model is quite effective at concisely paraphrasing news articles. Furthermore, we study how the neural network decides which input words to pay attention to, and specifically we identify the function of the different neurons in a simplified attention mechanism. Interestingly, our simplified attention mechanism performs better that the more complex attention mechanism on a held out set of articles.", "creator": "LaTeX with hyperref package"}}}