{"id": "1512.01715", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2015", "title": "A Restricted Visual Turing Test for Deep Scene and Event Understanding", "abstract": "This paper presents a restricted visual Turing test (VTT) for story-line based deep understanding in long-term and multi-camera captured videos. Given a set of videos of a scene (such as a multi-room office, a garden, and a parking lot.) and a sequence of story-line based queries, the task is to provide answers either simply in binary form \"true/false\" (to a polar query) or in an accurate natural language description (to a non-polar query). Queries, polar or non-polar, consist of view-based queries which can be answered from a particular camera view and scene-centered queries which involves joint inference across different cameras. The story lines are collected to cover spatial, temporal and causal understanding of input videos. The data and queries distinguish our VTT from recently proposed visual question answering in images and video captioning. A vision system is proposed to perform joint video and query parsing which integrates different vision modules, a knowledge base and a query engine. The system provides unified interfaces for different modules so that individual modules can be reconfigured to test a new method. We provide a benchmark dataset and a toolkit for ontology guided story-line query generation which consists of about 93.5 hours videos captured in four different locations and 3,426 queries split into 127 story lines. We also provide a baseline implementation and result analyses.", "histories": [["v1", "Sun, 6 Dec 2015 00:40:02 GMT  (1012kb,D)", "https://arxiv.org/abs/1512.01715v1", null], ["v2", "Wed, 16 Dec 2015 19:19:25 GMT  (1851kb,D)", "http://arxiv.org/abs/1512.01715v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["hang qi", "tianfu wu", "mun-wai lee", "song-chun zhu"], "accepted": false, "id": "1512.01715"}, "pdf": {"name": "1512.01715.pdf", "metadata": {"source": "CRF", "title": "A Restricted Visual Turing Test for Deep Scene and Event Understanding", "authors": ["Hang Qi", "Tianfu Wu", "Mun-Wai Lee", "Song-Chun Zhu"], "emails": ["hangqi@cs.ucla.edu,", "szchu}@stat.ucla.edu", "mlee@i-a-i.com"], "sections": [{"heading": "1. Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1. Motivation and Objective", "text": "In fact, most of them will be able to move into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live."}, {"heading": "1.2. Overview", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2. Related Work and Our Contributions", "text": "The integration of computer vision and natural language processing, as well as other modal skills, have been a hot topic in the recent development of deeper image and scene understanding. Visual Turing Test. Inspired by the generic Turing test principle in AI [36], a visual Turing test [24] has been proposed for the detection of objects in images, which organizes queries into lines of action within which complexity is gradually increased - similar to conversations between human beings. In a similar situation, Malinowski and Fritz have proposed a multiword method to address actual queries of scene images."}, {"heading": "3. Dataset", "text": "In this section, we present the video data set we collected for the VTT. In our data set, we organize data from several independent scenes. Each scene consists of video footage from eight to twelve cameras with overlapping fields of view over the same period. Meanwhile, we have a total of 14 collections recorded in four different locations: two interiors (an office and an auditorium) and two outdoor areas (a parking lot and a garden). Table 1 gives a summary of the data collections. Our data set reflects the real video surveillance data and presents unique challenges to modern computer vision algorithms: multiple entities. In our data set, activities in the scene could include multiple interacting entities."}, {"heading": "4. Queries", "text": "A query is a first-order logical set (with modifications) composed of variables, predicates (as shown in Figure 3), logical operators (as shown in Figure 1), arithmetic operators, and quantifiers. The answer to a query is either true or false, whether the fact indicated by the sentence includes the data and state of the system. Formal speech representation eliminates the need for natural speech processing and allows us to limit computer vision problems to a limited number of predictors. We evaluate computer vision systems by asking a sequence of questions organized into several lines of action. Each line of action examines a natural event over a period of time in a similar manner to conversations between humans. At the beginning of a line of action, important objects of interest are defined first. The visual system under evaluation is intended to indicate whether it recognizes these objects."}, {"heading": "5. System", "text": "We developed and implemented a computer vision system to perform the test as shown in Figure 2. It consists of three main parts: an offline parsing pipeline that splits visual perception into several subtasks; a knowledge base in which the parsing results (including entities, properties and relationships between them) are stored; and a query engine that answers queries by scanning the knowledge base. It also has a flexible architecture and a visualization toolkit."}, {"heading": "5.1. Offline parsing pipeline", "text": "Offline parsing pipeline processes the multiview videos. Each view is initially processed through a single view parsing pipeline, in which video sequences from multiple cameras are handled independently of each other. Then, the multiview fusion is compared with tracks from multiple views, the results from single view analyses are aligned, and scene-based results are generated for answering questions. To take advantage of the achievements in different sub-areas of computer vision, we organize a pipeline of modules, each of which focuses on a specific set of predicates by generating appropriate labels for the input data. Each module gains access to the original video sequence and products from previous modules in the pipeline. The modules implemented are described as follows: Most components are derived from the most advanced methods at the time we developed the system in the last year.Scene parsing generates a homography matrix for each sensor (generated by calibration) and for each camera."}, {"heading": "5.2. Knowledge base and query answering", "text": "The identified objects, actions, attribute names are all modeled as nodes, the connections between them are modeled as edges. In our implementation, the analysis results are stored in the diagrams of the Resource Description Framework (RDF) [38], which can be queried by a standard query language SPARQL [39]. Since the questions are formal language, our query engine first analyzes the query and transforms the query into a sequence of SPARQL statements. Apache Jena [27] is used to execute these statements and return answers from the knowledge base. Figure 8 shows the architecture of the query technique. In practice, it is impossible to calculate all possible predictions in advance and store each individual knowledge segment in the knowledge base. For example, the pre-calculation of all \"clear-line-of-sight (x)\" relationships ensures. In practice, it is impossible to detect all possible predictions in this system to actually predict."}, {"heading": "5.3. Design Decisions", "text": "The system was designed with two objectives in mind: first, to integrate existing tasks into computer vision; second, to make the architecture flexible enough to replace a module later with alternatives to pursuing incremental improvements. To this end, we defined a set of APIs for each vision task and connected all modules to each other via Remote Procedure Calls (RPC). This allows the system to focus only on the logical connection between modules and provides the implementation flexibility for individual components. In practice, we deploy all modules on different dedicated machines. Among the RPC interfaces, computing-intensive algorithms typically use GPU and MPI internally to seek faster calculations and data parallelism. This design allows us to use this system as an experimental platform by switching between alternative models and implementations to investigate their impact and investigate contributions to respond to queries. To make the system user-friendly, we also have developed a quick visualization board with visualization and 7."}, {"heading": "6. Evaluation", "text": "In fact, most of them will be able to play by the rules that they have established in recent years, and they will be able to play by the rules that they have set themselves."}, {"heading": "7. Discussion and Conclusion", "text": "This work presented a limited visual Turing Test (VTT) for a deeper understanding of scenes and events in long-term and multi-camera videos. Our VTT emphasizes a common spatial, temporal and causal understanding through the use of scene-centric representation and storyline-based queries. The dataset and queries differentiate the proposed VTT from the recently proposed Visual Response (VQA). In addition, we presented a prototype of an integrated vision system that is present in our VTT. In our ongoing work, we generate more storyline-based queries and set up a website to conduct a VTT competition. In the proposed competition, we will release the entire system as a playground. Our system architecture allows the user to replace one or more modules with their own methods and then walk through the VTT to see the improvements. One of our next steps is the creation of a publicly accessible \"Vision Module Market\" where researchers can evaluate different components of the TT from their own perspective."}], "references": [{"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Vizwiz: nearly real-time answers to visual questions", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White", "T. Yeh"], "venue": "User Interface Software and Technology, pages 333\u2013342,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Hierarchical semantic indexing for large scale image retrieval", "author": ["J. Deng", "A.C. Berg", "F. Li"], "venue": "CVPR, pages 785\u2013792,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["M. Everingham", "S.A. Eslami", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision, 111(1):98\u2013136,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "S.M.M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D.A. Forsyth"], "venue": "ECCV, pages 15\u201329,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Object detection with discriminatively trained partbased models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "TPAMI, 32(9):1627\u20131645, Sept.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences, 112(12):3618\u20133623,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "ICCV,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "The pyramid match kernel: Efficient learning with sets of features", "author": ["K. Grauman", "T. Darrell"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ICCV,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Analyzing 3d objects in cluttered images", "author": ["M. Hejrati", "D. Ramanan"], "venue": "NIPS, pages 602\u2013610,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning 3d object templates by quantizing geometry and appearance spaces", "author": ["W. Hu", "S. Zhu"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Undoing the damage of dataset bias", "author": ["A. Khosla", "T. Zhou", "T. Malisiewicz", "A.A. Efros", "A. Torralba"], "venue": "ECCV, pages 158\u2013171,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1106\u20131114,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR, pages 1601\u20131608,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "MOTChallenge 2015: Towards a benchmark for multitarget tracking", "author": ["L. Leal-Taix\u00e9", "A. Milan", "I. Reid", "S. Roth", "K. Schindler"], "venue": "arXiv:1504.01942 [cs], Apr.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B.E. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W.E. Hubbard", "L.D. Jackel"], "venue": "Neural Computation, 1(4):541\u2013551,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Microsoft coco: Com- 10  mon objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "Computer Vision\u2013ECCV 2014, pages 740\u2013755. Springer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Single-view 3d scene parsing by attributed grammar", "author": ["X. Liu", "Y. Zhao", "S.-C. Zhu"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 684\u2013 691. IEEE,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS, pages 1682\u20131690,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards a visual turing challenge", "author": ["M. Malinowski", "M. Fritz"], "venue": "CoRR, abs/1410.8027,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Jena: A semantic web toolkit", "author": ["B. McBride"], "venue": "IEEE Internet computing, 6(6):55\u201359,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Attributed grammars for joint estimation of human attributes, part and pose", "author": ["S. Park", "S.-C. Zhu"], "venue": "Proc. of International Conference on Computer vision (ICCV),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Globallyoptimal greedy algorithms for tracking a variable number of objects", "author": ["H. Pirsiavash", "D. Ramanan", "C.C. Fowlkes"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1201\u20131208. IEEE,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "arXiv preprint arXiv:1505.02074,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "NIPS,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "ICCV, pages 433\u2013440,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV), pages 1\u201342, April", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Discriminatively trained and-or tree models for object detection", "author": ["X. Song", "T. Wu", "Y. Jia", "S.-C. Zhu"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 3278\u20133285. IEEE,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminatively trained and-or tree models for object detection", "author": ["X. Song", "T.-F. Wu", "Y. Jia", "S.-C. Zhu"], "venue": "CVPR,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Computing machinery and intelligence", "author": ["A.M. Turing"], "venue": "Mind, 59(236):433\u2013460,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1950}, {"title": "Action recognition by dense trajectories", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C.-L. Liu"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 3169\u20133176. IEEE,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning and-or models to represent context and occlusion for car detection and viewpoint estimation", "author": ["T. Wu", "B. Li", "S.-C. Zhu"], "venue": "IEEE Trans on Pattern Analysis and Machine Intelligence,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint action recognition and pose estimation from video", "author": ["B. Xiaohan Nie", "C. Xiong", "S.-C. Zhu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1293\u20131301,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Animated pose templates for modeling and detecting human actions", "author": ["B.Z. Yao", "B.X. Nie", "Z. Liu", "S.-C. Zhu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 36(3):436\u2013452,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "A reconfigurable tangram model for scene representation and categorization", "author": ["J. Zhu", "T. Wu", "S.-C. Zhu", "X. Yang", "W. Zhang"], "venue": "TIP, 2015 (Accepted)", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "A stochastic grammar of images", "author": ["S.-C. Zhu", "D. Mumford"], "venue": "Found. Trends. Comput. Graph. Vis., 2(4):259\u2013362, Jan.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 6, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 141, "endOffset": 156}, {"referenceID": 10, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 141, "endOffset": 156}, {"referenceID": 18, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 141, "endOffset": 156}, {"referenceID": 40, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 141, "endOffset": 156}, {"referenceID": 7, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 178, "endOffset": 197}, {"referenceID": 34, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 178, "endOffset": 197}, {"referenceID": 41, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 178, "endOffset": 197}, {"referenceID": 9, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 178, "endOffset": 197}, {"referenceID": 30, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 178, "endOffset": 197}, {"referenceID": 4, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 245, "endOffset": 248}, {"referenceID": 32, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 269, "endOffset": 273}, {"referenceID": 20, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 312, "endOffset": 324}, {"referenceID": 16, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 312, "endOffset": 324}, {"referenceID": 11, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 312, "endOffset": 324}, {"referenceID": 3, "context": "Motivation and Objective During the past decades, we have seen tremendous progress in individual vision modules such as image classification [7, 11, 19, 44] and object detection [8, 35, 45, 10, 31], especially after competitions like PASCAL VOC [5] and ImageNet ILSVRC [33] and the convolutional neural networks [21, 17, 12] trained on the ImageNet dataset [4]", "startOffset": 357, "endOffset": 360}, {"referenceID": 35, "context": "For example, a chatterbot named Eugene Goostman1 was reported as the first computer program which has passed the famed Turing test [36] in an event organized at the University of Reading.", "startOffset": 131, "endOffset": 135}, {"referenceID": 8, "context": "The success of text-based QA and the recent achievements of individual vision modules have inspired visual Turing tests (VTT) [9, 25] where image-based questions (so-called visual question answering, VQA) or story-line queries are used to test a computer vision system.", "startOffset": 126, "endOffset": 133}, {"referenceID": 24, "context": "The success of text-based QA and the recent achievements of individual vision modules have inspired visual Turing tests (VTT) [9, 25] where image-based questions (so-called visual question answering, VQA) or story-line queries are used to test a computer vision system.", "startOffset": 126, "endOffset": 133}, {"referenceID": 1, "context": "Most existing work on VTT focus on images and emphasize free-form and open-ended Q/A\u2019s [2, 1].", "startOffset": 87, "endOffset": 93}, {"referenceID": 0, "context": "Most existing work on VTT focus on images and emphasize free-form and open-ended Q/A\u2019s [2, 1].", "startOffset": 87, "endOffset": 93}, {"referenceID": 0, "context": "In VQA [1], the input is an image and a \u201cbag-of-questions\u201d (e.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "This is challenging even for simple tasks like image labeling as investigated in the ImageNet dataset [4] and the LabelMe dataset [16].", "startOffset": 102, "endOffset": 105}, {"referenceID": 15, "context": "This is challenging even for simple tasks like image labeling as investigated in the ImageNet dataset [4] and the LabelMe dataset [16].", "startOffset": 130, "endOffset": 134}, {"referenceID": 8, "context": "\u2019s Turing test framework [9], we design a easyto-use toolkit by which several people with certain expertise can create a large number of story lines covering different interesting and important spatial, temporal and, causal aspects in videos with the quality of queries and answers controlled.", "startOffset": 25, "endOffset": 28}, {"referenceID": 20, "context": "Almost all the recent methods proposed for image captioning and VQA are based on the combination of convolutional neural network [21, 17] and recurrent neural network like long shortterm memory [14].", "startOffset": 129, "endOffset": 137}, {"referenceID": 16, "context": "Almost all the recent methods proposed for image captioning and VQA are based on the combination of convolutional neural network [21, 17] and recurrent neural network like long shortterm memory [14].", "startOffset": 129, "endOffset": 137}, {"referenceID": 13, "context": "Almost all the recent methods proposed for image captioning and VQA are based on the combination of convolutional neural network [21, 17] and recurrent neural network like long shortterm memory [14].", "startOffset": 194, "endOffset": 198}, {"referenceID": 8, "context": "Based on the ontology, we build a toolkit for story-line query generation following the statistical principles stated in [9].", "startOffset": 121, "endOffset": 124}, {"referenceID": 35, "context": "Inspired by the generic Turing test principle in AI [36], Geman et al.", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "proposed a visual Turing test [9] for object detection tasks in images which organizes queries into story lines, within which queries are connected and the complexities are increased gradually \u2013 similar to conversations between human beings.", "startOffset": 30, "endOffset": 33}, {"referenceID": 23, "context": "In a similar spirit, Malinowski and Fritz [24, 25] proposed a multi-word method to address factual queries of scene images.", "startOffset": 42, "endOffset": 50}, {"referenceID": 24, "context": "In a similar spirit, Malinowski and Fritz [24, 25] proposed a multi-word method to address factual queries of scene images.", "startOffset": 42, "endOffset": 50}, {"referenceID": 8, "context": "In the dataset and evaluation framework proposed in this paper, we adopt similar evaluation structure to [9], but focus on a more complex scenario which features videos and overlapping cameras to facilitate a broader scope of vision tasks.", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "To go beyond labels and bounding boxes, image tagging [3], image captioning [6, 18, 26], and video captioning [32] have been proposed recently.", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "To go beyond labels and bounding boxes, image tagging [3], image captioning [6, 18, 26], and video captioning [32] have been proposed recently.", "startOffset": 76, "endOffset": 87}, {"referenceID": 17, "context": "To go beyond labels and bounding boxes, image tagging [3], image captioning [6, 18, 26], and video captioning [32] have been proposed recently.", "startOffset": 76, "endOffset": 87}, {"referenceID": 25, "context": "To go beyond labels and bounding boxes, image tagging [3], image captioning [6, 18, 26], and video captioning [32] have been proposed recently.", "startOffset": 76, "endOffset": 87}, {"referenceID": 31, "context": "To go beyond labels and bounding boxes, image tagging [3], image captioning [6, 18, 26], and video captioning [32] have been proposed recently.", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "Microsoft COCO [22] provides descriptions or captions for images.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "IQA [30] converts image descriptions into Q/A pairs.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "VQA [1] evaluates in a free-formed and open-ended questions about images, where the question-answer pairs are given by human annotators.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "For Detection: AP is calculated as in PASCAL VOC 2012 [5] based on results by Faster-RCNN [31].", "startOffset": 54, "endOffset": 57}, {"referenceID": 30, "context": "For Detection: AP is calculated as in PASCAL VOC 2012 [5] based on results by Faster-RCNN [31].", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "For Tracking: MOTA and MOTP are calculated as in Multiple Object Tracking Benchmark [20] based on results by [29].", "startOffset": 84, "endOffset": 88}, {"referenceID": 28, "context": "For Tracking: MOTA and MOTP are calculated as in Multiple Object Tracking Benchmark [20] based on results by [29].", "startOffset": 109, "endOffset": 113}, {"referenceID": 30, "context": "To demonstrate the difficulties of our dataset, we conduct a set of experiments on a typical subset of data using the state-of-the-art object detection models [31] and multiple-object tracking methods [29].", "startOffset": 159, "endOffset": 163}, {"referenceID": 28, "context": "To demonstrate the difficulties of our dataset, we conduct a set of experiments on a typical subset of data using the state-of-the-art object detection models [31] and multiple-object tracking methods [29].", "startOffset": 201, "endOffset": 205}, {"referenceID": 8, "context": "in [9].", "startOffset": 3, "endOffset": 6}, {"referenceID": 22, "context": "The implementation is derived from [23].", "startOffset": 35, "endOffset": 39}, {"referenceID": 33, "context": "Object detection [34, 31] processes the video frames and generates bounding boxes for major objects of interest.", "startOffset": 17, "endOffset": 25}, {"referenceID": 30, "context": "Object detection [34, 31] processes the video frames and generates bounding boxes for major objects of interest.", "startOffset": 17, "endOffset": 25}, {"referenceID": 28, "context": "Multiple object tracking [29] generates tracks for all detected objects.", "startOffset": 25, "endOffset": 29}, {"referenceID": 27, "context": "Human attributes [28] classifies appearance attributes of detected human including gender, color of clothes, type of clothes, and accessories (e.", "startOffset": 17, "endOffset": 21}, {"referenceID": 38, "context": "The implementation is derived form [42, 43, 40].", "startOffset": 35, "endOffset": 47}, {"referenceID": 39, "context": "The implementation is derived form [42, 43, 40].", "startOffset": 35, "endOffset": 47}, {"referenceID": 36, "context": "The implementation is derived form [42, 43, 40].", "startOffset": 35, "endOffset": 47}, {"referenceID": 37, "context": "Vehicle parsing [41, 15, 13] produces bounding boxes and fluent labels for specific parts of detected cars (e.", "startOffset": 16, "endOffset": 28}, {"referenceID": 14, "context": "Vehicle parsing [41, 15, 13] produces bounding boxes and fluent labels for specific parts of detected cars (e.", "startOffset": 16, "endOffset": 28}, {"referenceID": 12, "context": "Vehicle parsing [41, 15, 13] produces bounding boxes and fluent labels for specific parts of detected cars (e.", "startOffset": 16, "endOffset": 28}, {"referenceID": 26, "context": "Apache Jena [27] is used to execute these statements and to return answers derived from the knowledge base.", "startOffset": 12, "endOffset": 16}], "year": 2015, "abstractText": "This paper presents a restricted visual Turing test (VTT) for story-line based deep understanding in long-term and multi-camera captured videos. Given a set of videos of a scene (such as a multi-room office, a garden, and a parking lot.) and a sequence of story-line based queries, the task is to provide answers either simply in binary form \u201ctrue/false\u201d (to a polar query) or in an accurate natural language description (to a non-polar query). Queries, polar or nonpolar, consist of view-based queries which can be answered from a particular camera view and scene-centered queries which involves joint inference across different cameras. The story lines are collected to cover spatial, temporal and causal understanding of input videos. The data and queries distinguish our VTT from recently proposed visual question answering in images and video captioning. A vision system is proposed to perform joint video and query parsing which integrates different vision modules, a knowledge base and a query engine. The system provides unified interfaces for different modules so that individual modules can be reconfigured to test a new method. We provide a benchmark dataset and a toolkit for ontology guided story-line query generation which consists of about 93.5 hours videos captured in four different locations and 3,426 queries split into 127 story lines . We also provide a baseline implementation and result analyses.", "creator": "LaTeX with hyperref package"}}}