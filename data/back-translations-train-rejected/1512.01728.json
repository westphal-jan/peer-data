{"id": "1512.01728", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2015", "title": "Similarity Learning via Adaptive Regression and Its Application to Image Retrieval", "abstract": "We study the problem of similarity learning and its application to image retrieval with large-scale data. The similarity between pairs of images can be measured by the distances between their high dimensional representations, and the problem of learning the appropriate similarity is often addressed by distance metric learning. However, distance metric learning requires the learned metric to be a PSD matrix, which is computational expensive and not necessary for retrieval ranking problem. On the other hand, the bilinear model is shown to be more flexible for large-scale image retrieval task, hence, we adopt it to learn a matrix for estimating pairwise similarities under the regression framework. By adaptively updating the target matrix in regression, we can mimic the hinge loss, which is more appropriate for similarity learning problem. Although the regression problem can have the closed-form solution, the computational cost can be very expensive. The computational challenges come from two aspects: the number of images can be very large and image features have high dimensionality. We address the first challenge by compressing the data by a randomized algorithm with the theoretical guarantee. For the high dimensional issue, we address it by taking low rank assumption and applying alternating method to obtain the partial matrix, which has a global optimal solution. Empirical studies on real world image datasets (i.e., Caltech and ImageNet) demonstrate the effectiveness and efficiency of the proposed method.", "histories": [["v1", "Sun, 6 Dec 2015 02:56:32 GMT  (157kb)", "http://arxiv.org/abs/1512.01728v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qi qian", "inci m baytas", "rong jin", "anil jain", "shenghuo zhu"], "accepted": false, "id": "1512.01728"}, "pdf": {"name": "1512.01728.pdf", "metadata": {"source": "CRF", "title": "Similarity Learning via Adaptive Regression and Its Application to Image Retrieval", "authors": ["Qi Qian", "Inci M. Baytas", "Rong Jin", "Shenghuo Zhu"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 151 2.01 728v 1 [cs.L G] 6D ec2 01"}, {"heading": "Introduction", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is not a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country,"}, {"heading": "Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Distance Metric Learning", "text": "Many methods have been developed to estimate the pairwise similarity, some of which can be categorized as remote metric learning, where distance metrics are learned to increase the pairwise distance with examples from different classes and reduce the pairwise distance with examples from the same class. Representative methods include the methods of Xing (Xing et al. 2002), POLA (ShalevShwartz, Singer and Ng 2004), ITML (Davis et al. 2007), LMNN (Weinberger and Saul 2009) and FRML (Lim and Lanckriet 2014). ITML learns a metric according to picturesque constraints where the distance between pairs from the same class should be smaller than a predefined threshold and the distance between pairs from different classes should be greater than a second threshold. LMNN is developed with triplet constraints and a metric is learned to ensure that pairs from the same class are separated from the pairs from a large class with a time-wide range."}, {"heading": "Similarity Learning with Bilinear Model", "text": "In addition to these DML methods, the bilinear model was developed to learn a similarity without the PSD constraint. Unlike the similarity defined by the removal of two examples, the bilinear model calculates the inner product of two examples by the learned similarity function. OASIS (Chechik et al. 2010) has successfully applied it to the ranking problem, which requires that the more relevant examples show greater similarity and a loss of hinges for a safety margin is assumed. The corresponding optimization problem is solved by an online learning method for large-scale image repetition tasks. Compared to DML, the learned matrix is not symmetrical, which offers more flexibility in real applications. Although OASIS could process millions of images, the compression costs for each iteration are still O (d2). Furthermore, online learning cannot list all three-dimensional constraints that due to its enormous number (i.e. O (n3) can lead to an optimal solution to this problem."}, {"heading": "Our Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Adaptive Regression for Similarity Learning", "text": "In view of a series of training images X-Rd \u00b7 n, similarity problems with bilinear model are defined as follows: \"Learning with bilinear model\" = \"Learning a good matrix M\" for estimating pairwise similarity asSimM (xi, xj) = x-i. \"With the target matrix Y-Rn \u00b7 n, M can be learned by solving a matrix regression problem in M-Rd \u00b7 d-D-X-MX \u2212 Y-2F (1), where Y is learned through the labeling information and definition asYi, j = {1: label (xi) = labeling (xj) 0: otherwise (2) This formulation is popular for similarity learning and is often used in various tasks (Yi et al. 2012; Feng, Jin, and Jain 2013). It has a closed solution for the matrix M: M label (X) = X (\u2020 X), where X-\u00b2 is the labeling."}, {"heading": "Large-scale Challenge", "text": "To solve the problem, we must use the pseudo-reversal of X (1), the O (min {n2d, d2n}) costs, (2), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2, (2), (2), (2, (2), (2), (2), (2, (2), (2), (2), (2, (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2, (2), (2), (2), (2, (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2, (2), (2, (2), (2), (), (2, (2), (2), (), (2), (2), (2), (2, (2), (2), (2), (2), (, (2), (, (2), (2), (2), (2), (2), (2), (, (2), (2), (2), ("}, {"heading": "High Dimensional Challenge", "text": "To avoid overadjustment and alleviation of the memory challenge, the learned matrix is forced to be of low rank and the corresponding problem ismin M-Rd-d, rank (M) = r-X-Y-2F, where r is the rank of the learned matrix and r-d. Although this problem has the closed form solution (Yu and Schuurmans 2011), the solution is standard only for Frobenius and the computing costs expensive (i.e., O (dn2 + n3)). Inspired by the idea of alternative optimization (Jain, Netrapalli and Sanghavi 2012; Netrapalli, Jain and Sanghavi 2013), we can use M-LR, where L, R-R-Rd-X-R and YR-R is the same solution."}, {"heading": "Theoretical Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Theorem 1", "text": "The key of our evidence stems from the following Korollary.Corollary 3 (Zhang et al. 2013) Let S-Rr \u00b7 m be a standard Gaussian random matrix. Then, for all 0 < \u03b5 \u2264 1 / 2, with a probability of 1 \u2212 3, we have a similar idea of proof as in (Drineas, Mahoney, and Muthukrishnan 2006). Before the proof is provided, we first give some useful lemmas.Lemma 2. Given a ranking r matrix A = UAV Rn \u00b7 d and a standardized Gaussian random matrix S Rn \u00b7 m m, which is by a probability of 1 \u2212 2, we have a probability of 1 \u2212 2 that we have."}, {"heading": "Proof of Lemma 1", "text": "Proof. We define the random variable Qi asQi = nqiq i \u2212 Iwo Qi is randomly selected from {e1, \u00b7 \u00b7 \u00b7, en} with the probability 1 / n. It is obvious that Qi adjoins itself, E [Qi] = 0 and \u03b4max \u2264 n \u2212 1. In addition, we have E [Q2i] = n 2E [qiq i qiq i] \u2212 2nE [qiq i] + I = (n \u2212 1) I Therefore, we have E [Q 2 i] \u0445 2 = m (n \u2212 1). According to Bernstein's inequality (law 2011), we have Pr {\u03b4max (m \u00b2 i = 1Qi) \u2264 n exp (\u2212 \u03b52 / 2m (n \u2212 1) + (n \u2212 1) \u03b5 / 3) With a similar procedure, we can combine them with a probability \u00b7 \u00b2 (n \u00b2) and a universal provision (n \u00b2)."}, {"heading": "Experiments", "text": "Four state-of-the-art similarity learning algorithms are included in the comparison to verify the effectiveness of the proposed method. \u2022 LMNN (Weinberger and Saul 2009): DML methods with tripling constraints; \u2022 FRML (Lim and Lanckriet 2014): Efficient DML methods for ranking with mini-batch strategy; \u2022 OASIS (Chechik et al. 2010): Bilinear model for similar learning; \u2022 SLR: similarity learning with adaptive regression and al-ternating solver. In addition, Euclid is the baseline with euclidean removal directly. We use the implementations provided by the authors with the recommended parameter settings. We set the number of constraints for FRML and OASIS as N = 106 to fully explore the information contained in the data."}, {"heading": "Caltech101", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "Conclusions", "text": "In order to further compress the data, the randomized variant of the proposed method is being developed. Unlike many previous methods that used stochastic algorithms to increase efficiency, our method allows us to traverse all image pairs without loss of information. In the future, we plan to apply our randomized algorithm to the data set with the huge number of images that cannot be processed by a single machine. We will also explore our method in other applications (e.g. high-dimensional document data)."}], "references": [{"title": "Similarity learning for provably accurate sparse linear classification", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "ICML.", "citeRegEx": "Bellet et al\\.,? 2012", "shortCiteRegEx": "Bellet et al\\.", "year": 2012}, {"title": "Large scale online learning of image similarity through ranking", "author": ["G. Chechik", "V. Sharma", "U. Shalit", "S. Bengio"], "venue": "JMLR 11:1109\u20131135.", "citeRegEx": "Chechik et al\\.,? 2010", "shortCiteRegEx": "Chechik et al\\.", "year": 2010}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "ICML, 209\u2013 216.", "citeRegEx": "Davis et al\\.,? 2007", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Sampling algorithms for l2 regression and applications", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"], "venue": "SODA, 1127\u20131136.", "citeRegEx": "Drineas et al\\.,? 2006", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Large-scale image annotation by efficient and robust kernel metric learning", "author": ["Z. Feng", "R. Jin", "A.K. Jain"], "venue": "ICCV, 1609\u20131616.", "citeRegEx": "Feng et al\\.,? 2013", "shortCiteRegEx": "Feng et al\\.", "year": 2013}, {"title": "Guaranteed classification via regularized similarity learning", "author": ["Z. Guo", "Y. Ying"], "venue": "Neural Computation 26(3):497\u2013522.", "citeRegEx": "Guo and Ying,? 2014", "shortCiteRegEx": "Guo and Ying", "year": 2014}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["P. Jain", "P. Netrapalli", "S. Sanghavi"], "venue": "CoRR abs/1212.0467.", "citeRegEx": "Jain et al\\.,? 2012", "shortCiteRegEx": "Jain et al\\.", "year": 2012}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["F. Li", "R. Fergus", "P. Perona"], "venue": "Computer Vision and Image Understanding 106(1):59\u201370.", "citeRegEx": "Li et al\\.,? 2007", "shortCiteRegEx": "Li et al\\.", "year": 2007}, {"title": "Efficient learning of mahalanobis metrics for ranking", "author": ["D. Lim", "G.R.G. Lanckriet"], "venue": "ICML, 1980\u20131988.", "citeRegEx": "Lim and Lanckriet,? 2014", "shortCiteRegEx": "Lim and Lanckriet", "year": 2014}, {"title": "On the generalized distance in statistics", "author": ["P.C. Mahalanobis"], "venue": "Proceedings of the National Institute of Sciences (Calcutta) 2:49\u201355.", "citeRegEx": "Mahalanobis,? 1936", "shortCiteRegEx": "Mahalanobis", "year": 1936}, {"title": "Phase retrieval using alternating minimization", "author": ["P. Netrapalli", "P. Jain", "S. Sanghavi"], "venue": "NIPS, 2796\u20132804.", "citeRegEx": "Netrapalli et al\\.,? 2013", "shortCiteRegEx": "Netrapalli et al\\.", "year": 2013}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": "SIAM Review 52(3):471\u2013501.", "citeRegEx": "Recht et al\\.,? 2010", "shortCiteRegEx": "Recht et al\\.", "year": 2010}, {"title": "A simpler approach to matrix completion", "author": ["B. Recht"], "venue": "JMLR 12:3413\u20133430.", "citeRegEx": "Recht,? 2011", "shortCiteRegEx": "Recht", "year": 2011}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng"], "venue": "ICML.", "citeRegEx": "Shalev.Shwartz et al\\.,? 2004", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2004}, {"title": "Features of similarity", "author": ["A. Tversky"], "venue": "Psychological Review 84(4):327\u2013352.", "citeRegEx": "Tversky,? 1977", "shortCiteRegEx": "Tversky", "year": 1977}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "JMLR 10:207\u2013244.", "citeRegEx": "Weinberger and Saul,? 2009", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S.J. Russell"], "venue": "NIPS, 505\u2013512.", "citeRegEx": "Xing et al\\.,? 2002", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T.S. Huang"], "venue": "2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, 1794\u20131801.", "citeRegEx": "Yang et al\\.,? 2009", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Semi-crowdsourced clustering: Generalizing crowd labeling by robust distance metric learning", "author": ["J. Yi", "R. Jin", "A.K. Jain", "S. Jain", "T. Yang"], "venue": "NIPS, 1781\u20131789.", "citeRegEx": "Yi et al\\.,? 2012", "shortCiteRegEx": "Yi et al\\.", "year": 2012}, {"title": "Rank/norm regularization with closed-form solutions: Application to subspace clustering", "author": ["Y. Yu", "D. Schuurmans"], "venue": "UAI 2011, Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, Barcelona, Spain, July 14-17, 2011, 778\u2013785.", "citeRegEx": "Yu and Schuurmans,? 2011", "shortCiteRegEx": "Yu and Schuurmans", "year": 2011}, {"title": "Recovering the optimal solution by dual random projection", "author": ["L. Zhang", "M. Mahdavi", "R. Jin", "T. Yang", "S. Zhu"], "venue": "COLT, 135\u2013157.", "citeRegEx": "Zhang et al\\.,? 2013", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": ", classification (Guo and Ying 2014; Bellet, Habrard, and Sebban 2012), clustering (Yi et al.", "startOffset": 17, "endOffset": 70}, {"referenceID": 19, "context": ", classification (Guo and Ying 2014; Bellet, Habrard, and Sebban 2012), clustering (Yi et al. 2012; Xing et al. 2002), ranking (Chechik et al.", "startOffset": 83, "endOffset": 117}, {"referenceID": 17, "context": ", classification (Guo and Ying 2014; Bellet, Habrard, and Sebban 2012), clustering (Yi et al. 2012; Xing et al. 2002), ranking (Chechik et al.", "startOffset": 83, "endOffset": 117}, {"referenceID": 1, "context": "2002), ranking (Chechik et al. 2010; Lim and Lanckriet 2014), etc.", "startOffset": 15, "endOffset": 60}, {"referenceID": 8, "context": "2002), ranking (Chechik et al. 2010; Lim and Lanckriet 2014), etc.", "startOffset": 15, "endOffset": 60}, {"referenceID": 9, "context": "Given the learned metric M , most of the DML methods compute the similarity in the form of Mahalanobis distance (Mahalanobis 1936): disM (xi,xj) =", "startOffset": 112, "endOffset": 130}, {"referenceID": 15, "context": "Additionally, the task of image retrieval is known to be non-symmetric according to human judgement (Tversky 1977).", "startOffset": 100, "endOffset": 114}, {"referenceID": 5, "context": "In this scenario, the similarity can be measured by the similarity function in bilinear form (Guo and Ying 2014; Chechik et al. 2010): SimM (xi,xj) = x \u22a4 i Mxj , which computes the inner product between two examples.", "startOffset": 93, "endOffset": 133}, {"referenceID": 1, "context": "In this scenario, the similarity can be measured by the similarity function in bilinear form (Guo and Ying 2014; Chechik et al. 2010): SimM (xi,xj) = x \u22a4 i Mxj , which computes the inner product between two examples.", "startOffset": 93, "endOffset": 133}, {"referenceID": 19, "context": "In this paper, we attempt to learn a good similarity function by solving a matrix regression problem (Yi et al. 2012).", "startOffset": 101, "endOffset": 117}, {"referenceID": 16, "context": "\u2022 To alleviate the high dimensional challenge, we take the low rank assumption (Weinberger and Saul 2009) that the rank of the optimal M is significantly smaller than the dimensionality of data (i.", "startOffset": 79, "endOffset": 105}, {"referenceID": 17, "context": "The representative methods include Xing\u2019s method (Xing et al. 2002), POLA (ShalevShwartz, Singer, and Ng 2004), ITML (Davis et al.", "startOffset": 49, "endOffset": 67}, {"referenceID": 2, "context": "2002), POLA (ShalevShwartz, Singer, and Ng 2004), ITML (Davis et al. 2007), LMNN (Weinberger and Saul 2009), and FRML (Lim and Lanckriet 2014).", "startOffset": 55, "endOffset": 74}, {"referenceID": 16, "context": "2007), LMNN (Weinberger and Saul 2009), and FRML (Lim and Lanckriet 2014).", "startOffset": 12, "endOffset": 38}, {"referenceID": 8, "context": "2007), LMNN (Weinberger and Saul 2009), and FRML (Lim and Lanckriet 2014).", "startOffset": 49, "endOffset": 73}, {"referenceID": 1, "context": "OASIS (Chechik et al. 2010) successfully applied it to the ranking problem, which requires that the more relevant examples have larger similarity and a hinge loss is adopted for a safety margin.", "startOffset": 6, "endOffset": 27}, {"referenceID": 19, "context": "This formulation is popular for similarity learning and widely used in different tasks (Yi et al. 2012; Feng, Jin, and Jain 2013).", "startOffset": 87, "endOffset": 129}, {"referenceID": 20, "context": "Although this problem has the closed-form solution (Yu and Schuurmans 2011), the solution is only available for Frobenius norm and the computational cost is expensive (i.", "startOffset": 51, "endOffset": 75}, {"referenceID": 21, "context": "(Zhang et al. 2013) Let S \u2208 R be a standard Gaussian random matrix.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "According to Bernstein\u2019s inequality (Recht 2011), we have", "startOffset": 36, "endOffset": 48}, {"referenceID": 16, "context": "\u2022 LMNN (Weinberger and Saul 2009): DML methods with triplet constraints; \u2022 FRML (Lim and Lanckriet 2014): Efficient DML methods for ranking with mini-batch strategy; \u2022 OASIS (Chechik et al.", "startOffset": 7, "endOffset": 33}, {"referenceID": 8, "context": "\u2022 LMNN (Weinberger and Saul 2009): DML methods with triplet constraints; \u2022 FRML (Lim and Lanckriet 2014): Efficient DML methods for ranking with mini-batch strategy; \u2022 OASIS (Chechik et al.", "startOffset": 80, "endOffset": 104}, {"referenceID": 1, "context": "\u2022 LMNN (Weinberger and Saul 2009): DML methods with triplet constraints; \u2022 FRML (Lim and Lanckriet 2014): Efficient DML methods for ranking with mini-batch strategy; \u2022 OASIS (Chechik et al. 2010): Bilinear model for similarity learning; \u2022 SLR: Similarity learning with adaptive regression and alternating solver.", "startOffset": 174, "endOffset": 195}, {"referenceID": 1, "context": "We evaluate the learned metric on a ranking task, which is the same as in (Chechik et al. 2010).", "startOffset": 74, "endOffset": 95}, {"referenceID": 18, "context": "We extract LLC features (Yang et al. 2009) as representations for each image and reduce the dimension to 1, 000 by PCA to make the comparison with other methods.", "startOffset": 24, "endOffset": 42}, {"referenceID": 13, "context": "ImageNet50 consists of 50 randomly selected categories from the ImageNet dataset (Russakovsky et al. 2014).", "startOffset": 81, "endOffset": 106}], "year": 2015, "abstractText": "We study the problem of similarity learning and its application to image retrieval with large-scale data. The similarity between pairs of images can be measured by the distances between their high dimensional representations, and the problem of learning the appropriate similarity is often addressed by distance metric learning. However, distance metric learning requires the learned metric to be a PSD matrix, which is computational expensive and not necessary for retrieval ranking problem. On the other hand, the bilinear model is shown to be more flexible for large-scale image retrieval task, hence, we adopt it to learn a matrix for estimating pairwise similarities under the regression framework. By adaptively updating the target matrix in regression, we can mimic the hinge loss, which is more appropriate for similarity learning problem. Although the regression problem can have the closed-form solution, the computational cost can be very expensive. The computational challenges come from two aspects: the number of images can be very large and image features have high dimensionality. We address the first challenge by compressing the data by a randomized algorithm with the theoretical guarantee. For the high dimensional issue, we address it by taking low rank assumption and applying alternating method to obtain the partial matrix, which has a global optimal solution. Empirical studies on real world image datasets (i.e., Caltech and ImageNet) demonstrate the effectiveness and efficiency of the proposed method.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}