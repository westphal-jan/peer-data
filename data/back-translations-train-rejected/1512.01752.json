{"id": "1512.01752", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2015", "title": "Large Scale Distributed Semi-Supervised Learning Using Streaming Approximation", "abstract": "Traditional graph-based semi-supervised learning (SSL) approaches, even though widely applied, are not suited for massive data and large label scenarios since they scale linearly with the number of edges $|E|$ and distinct labels $m$. To deal with the large label size problem, recent works propose sketch-based methods to approximate the distribution on labels per node thereby achieving a space reduction from $O(m)$ to $O(\\log m)$, under certain conditions. In this paper, we present a novel streaming graph-based SSL approximation that captures the sparsity of the label distribution and ensures the algorithm propagates labels accurately, and further reduces the space complexity per node to $O(1)$. We also provide a distributed version of the algorithm that scales well to large data sizes. Experiments on real-world datasets demonstrate that the new method achieves better performance than existing state-of-the-art algorithms with significant reduction in memory footprint. We also study different graph construction mechanisms for natural language applications and propose a robust graph augmentation strategy trained using state-of-the-art unsupervised deep learning architectures that yields further significant quality gains.", "histories": [["v1", "Sun, 6 Dec 2015 06:58:57 GMT  (464kb,D)", "https://arxiv.org/abs/1512.01752v1", "10 pages"], ["v2", "Mon, 16 May 2016 19:40:37 GMT  (461kb,D)", "http://arxiv.org/abs/1512.01752v2", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["sujith ravi", "qiming diao"], "accepted": false, "id": "1512.01752"}, "pdf": {"name": "1512.01752.pdf", "metadata": {"source": "CRF", "title": "Large Scale Distributed Semi-Supervised Learning Using Streaming Approximation", "authors": ["Sujith Ravi", "Qiming Diao"], "emails": ["sravi@google.com", "qiming.ustc@gmail.com"], "sections": [{"heading": null, "text": "Conventional graph-based semi-supervised learning approaches (SSL) are not suitable for massive amounts of data and large label scenarios because they are linearly scaled with the number of edges | E | and different labels m. To address the problem of large label size, current work proposes outline-based methods to approximate label distribution per node, thereby reducing space requirements from O (m) to O (logm) under certain conditions. In this paper, we present a novel streaming graph-based SSL approach that effectively captures the low distribution of labels and further reduces space complexity per node to O (1). In addition, we offer a distributed version of the algorithm that scales well to large data sizes. Experiments with real-world datasets show that the new method performs better than existing state-of-the-art algorithms with significant memory print reduction."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "2 Graph-based Semi-Supervised Learning", "text": "Preliminary: The goal is a soft mapping of labels to each node in a diagram G = (V, E, W), where V is the set of nodes, E is the set of edges, and W is the edge weight matrix.2 Each edge (v, u) / E is assigned a weight wvu = 0. Among the | V | = n number of nodes are | Vl | = nl labeled by them, while | Vu | = nu are unlabeled. We use the diagonal matrix S to capture the seeds, where svv = 1 is when the node is v seed. L stands for the output label set, the size of which can be large in the real world. Y is a n-m matrix that records the distribution of training labels for the seeds in which Yvl = 0 is for v-Vu, and Y is an n-m label distribution matrix based on our general algorithm, which is a semi-algorithm for our method."}, {"heading": "2.1 Graph SSL Optimization", "text": "This year, it has reached the point where it will be able to retaliate and retaliate."}, {"heading": "3 Streaming Algorithm for Scaling To Large Label Spaces", "text": "The authors also point to other related papers that attempt to induce thrift by means of regulation techniques [32, 18] but for an entirely different purpose [11]. In contrast, our work tackles exactly the same problem as [28] the graph-based SSL settings for large labels."}, {"heading": "4 Graph Construction", "text": "The most important ingredient for graph-based SSL approaches is the input graph itself. We show that the choice of graph construction mechanism has an important influence on the quality of SSL output. Depending on the edge connection information and the choice of text representation, there are several ways to create an input graph for SSL - (a) generic graphs that represent observed neighborhood or linking information that link vertices (e.g. connections in a social network), (b) graphs constructed from sparse feature representations for each node (e.g. a two-part freebase graph that links entity nodes to cell value nodes, captures the properties of the entity occurring in a scheme or table), (c) graphs constructed from dense feature representations for each node, i.e. dense feature per node we use to define neighborhood (the next section will discuss the graph itself), since (d) we use the different graph options for each node."}, {"heading": "4.1 Graph Augmentation with Dense Semantic Representations", "text": "In fact, most people are able to move to another world, in which they are able to integrate, and in which they are able to change the world."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experiment Setup", "text": "We will use two real data sets (publicly available from Freebase) for evaluation in this section. (Data Name Nodes Edges Labels Avg, Deg.Freebase Entity 301, 638 1, 155, 001 192 3.83 Freebase Relation 9, 367, 013 16, 817, 110 7, 664 1.80Freebase Entity (referred to as FB-E) is the exact same dataset and setup used in previous work. This dataset consists of cell value nodes and property nodes that are entities and table properties in Freebase. An edge indicates that an entity appears in a table cell, and the second dataset is Freebase Relation (referred to as FB-R). This dataset includes Entity1 Entity1 Entity1 Entity1 Entiyp Entiyp Entity Entity Entity Entity Entity Entity Entity and Table Properties in Freebase that consist of more than 7000 relationships."}, {"heading": "5.2 Graph SSL Results", "text": "First, we compare graph-based SSL methods quantitatively with respect to MRR and Precision @ K without taking into account spatial and temporal complexity. Table 1 shows the results with 5 seeds / labels and 10 seeds / labels on the freebase entity records. We have several findings from the results: (1) Both EXPANDER-based methods outperform MAD with respect to MRR and Precison @ K. (2) Our algorithm on the extended graph using semantic embedding (last line) results in significant increases over the original graph, suggesting that the densification of the graph with additional information is a useful technique to improve SSL in such scenarios."}, {"heading": "5.3 Streaming Sparsity versus Sketch", "text": "In fact, most people are able to survive themselves, and they are able to survive themselves, \"he said.\" But it's not as if they are able to survive themselves. \"He added,\" It's not as if they are able to change the world. \"He added,\" It's not as if they are able to change the world, as if they are able to change the world, to change it, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change and to change. \""}, {"heading": "5.4 Graph SSL with Large Data, Label Sizes", "text": "This year, the number of job-related redundancies has fallen by 20 per cent compared to the previous year, while the number of job-related redundancies has increased by 20 per cent."}, {"heading": "6 Conclusion", "text": "Existing graph-based SSL algorithms typically require O (m) disk space per node and do not scale to scenarios with large label sizes m and massive diagrams. We propose a novel streaming algorithm that effectively and accurately captures the small distribution of labels; the algorithm operates efficiently in streaming mode, reducing space complexity per node to O (1), while delivering high performance; we also extend the method with a distributed algorithm that elegantly scales to large data and label sizes (for example, billions of nodes / edges and millions of labels); and we show that graph augmentation with uncontrolled learning techniques can be a robust strategy for achieving performance gains in SSL problems with natural language."}, {"heading": "Acknowledgements", "text": "We thank Partha Talukdar for useful hints about the MAD code and Kevin Murphy for accessing the freebase relation dataset."}], "references": [{"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["R. Agrawal", "A. Gupta", "Y. Prabhu", "M. Varma"], "venue": "Proceedings of the International World Wide Web Conference,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Video suggestion and discovery for Youtube: Taking random walks through the view graph", "author": ["S. Baluja", "R. Seth", "D. Sivakumar", "Y. Jing", "J. Yagnik", "S. Kumar", "D. Ravichandran", "M. Aly"], "venue": "Proceedings of the 17th International Conference on World Wide Web, WWW \u201908, pages 895\u2013904,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "On manifold regularization", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Proceeding of the Conference on Artificial Intelligence and Statistics (AIS- TATS),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Label propagation and quadratic criterion", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "O. Chapelle, B. Sch\u00f6lkopf, and A. Zien, editors, Semi-Supervised Learning, pages 193\u2013216. MIT Press,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Semi-supervised learning for natural language processing", "author": ["J. Blitzer", "X.J. Zhu"], "venue": "ACL-HLT Tutorial, June", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Toward an architecture for never-ending language learning", "author": ["A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E.R.H. Jr.", "T.M. Mitchell"], "venue": "In AAAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien", "editors"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M. Charikar"], "venue": "Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 380\u2013388,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "An improved data stream summary: The count-min sketch and its applications", "author": ["G. Cormode", "S. Muthukrishnan"], "venue": "Journal of Algorithms, 55(1):58\u2013 75,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Graph-based lexicon expansion with sparsity-inducing penalties", "author": ["D. Das", "N.A. Smith"], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 677\u2013687. Association for Computational Linguistics,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q.V. Le", "M.Z. Mao", "M. Ranzato", "A.W. Senior", "P.A. Tucker", "K. Yang", "A.Y. Ng"], "venue": "Proceedings of NIPS, pages 1232\u20131240,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR09,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Streaming pointwise mutual information", "author": ["B.V. Durme", "A. Lall"], "venue": "Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1892\u20131900.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Streaming for large scale NLP: Language modeling", "author": ["A. Goyal", "III H. Daum\u00e9", "S. Venkatasubramanian"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "Proceedings of the Sixteenth International Conference on Machine Learning, pages 200\u2013209,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "Transductive learning via spectral graph partitioning", "author": ["T. Joachims"], "venue": "Proceedings of ICML, pages 290\u2013297,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Sparsity and persistence: mixed norms provide simple signal models with dependent coefficients", "author": ["M. Kowalski", "B. Torr\u00e9sani"], "venue": "Signal, Image and Video Processing, 3(3):251\u2013264, Sept.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Class label enhancement via related instances", "author": ["Z. Kozareva", "K. Voevodski", "S.-H. Teng"], "venue": "Proceedings of EMNLP, pages 118\u2013128,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Pregel: a system for large-scale graph processing", "author": ["G. Malewicz", "M.H. Austern", "A.J. Bik", "J.C. Dehnert", "I. Horn", "N. Leiser", "G. Czajkowski"], "venue": "Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, pages 135\u2013146,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximate frequency counts over data streams", "author": ["G.S. Manku", "R. Motwani"], "venue": "Proceedings of the 28th International Conference on Very Large Data Bases, VLDB \u201902, pages 346\u2013357,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Proceedings of Workshop at ICLR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Proceedings of NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Exponential reservoir sampling for streaming language models", "author": ["M. Osborne", "A. Lall", "B.V. Durme"], "venue": "Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics, ACL \u20192014, pages 687\u2013692,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning with labeled and unlabeled data", "author": ["M. Seeger"], "venue": "Technical report,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Entropic graph regularization in non-parametric semi-supervised classification", "author": ["A. Subramanya", "J.A. Bilmes"], "venue": "Proceedings of NIPS, pages 1803\u20131811,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient graph-based semi-supervised learning of structured tagging models", "author": ["A. Subramanya", "S. Petrov", "F. Pereira"], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201910, pages 167\u2013 176,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Scaling graph-based semi supervised learning to large number of labels using count-min sketch", "author": ["P. Talukdar", "W. Cohen"], "venue": "Proceedings of AISTATS, pages 940\u2013947,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "New regularized algorithms for transductive learning", "author": ["P.P. Talukdar", "K. Crammer"], "venue": "Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, ECML PKDD \u201909, pages 442\u2013457,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Experiments in graph-based semi-supervised learning methods for class-instance acquisition", "author": ["P.P. Talukdar", "F. Pereira"], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, pages 1473\u20131481,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Weakly-supervised acquisition of labeled class instances using graph random walks", "author": ["P.P. Talukdar", "J. Reisinger", "M. Pasca", "D. Ravichandran", "R. Bhagat", "F. Pereira"], "venue": "EMNLP, pages 582\u2013590,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society (Series B), 58:267\u2013288,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "On information regularization", "author": ["A.C. Tommi", "T. Jaakkola"], "venue": "Proceedings of the 19th UAI,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Balanced label propagation for partitioning massive graphs", "author": ["J. Ugander", "L. Backstrom"], "venue": "Proceedings of the Sixth ACM International Conference on Web Search and Data Mining, pages 507\u2013516,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient online locality sensitive hashing via reservoir counting", "author": ["B. Van Durme", "A. Lall"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT \u201911, pages 18\u201323,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Label propagation from imagenet to 3d point clouds", "author": ["Y. Wang", "R. Ji", "S.-F. Chang"], "venue": "Proceedings of CVPR, pages 3135\u20133142. IEEE,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Technical Report 1530, Computer Sciences, University of Wisconsin-Madison,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Semisupervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "Proceedings of ICML, pages 912\u2013919,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 23, "context": "There are several surveys that cover various SSL methods in the literature [25, 37, 8, 6].", "startOffset": 75, "endOffset": 89}, {"referenceID": 35, "context": "There are several surveys that cover various SSL methods in the literature [25, 37, 8, 6].", "startOffset": 75, "endOffset": 89}, {"referenceID": 6, "context": "There are several surveys that cover various SSL methods in the literature [25, 37, 8, 6].", "startOffset": 75, "endOffset": 89}, {"referenceID": 4, "context": "There are several surveys that cover various SSL methods in the literature [25, 37, 8, 6].", "startOffset": 75, "endOffset": 89}, {"referenceID": 14, "context": "The majority of SSL algorithms are computationally expensive; for example, transductive SVM [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 36, "context": "Graph-based SSL algorithms [38, 17, 33, 4, 26, 30] are a subclass of SSL techniques that have received a lot of attention recently, as they scale much better to large problems and data sizes.", "startOffset": 27, "endOffset": 50}, {"referenceID": 15, "context": "Graph-based SSL algorithms [38, 17, 33, 4, 26, 30] are a subclass of SSL techniques that have received a lot of attention recently, as they scale much better to large problems and data sizes.", "startOffset": 27, "endOffset": 50}, {"referenceID": 31, "context": "Graph-based SSL algorithms [38, 17, 33, 4, 26, 30] are a subclass of SSL techniques that have received a lot of attention recently, as they scale much better to large problems and data sizes.", "startOffset": 27, "endOffset": 50}, {"referenceID": 2, "context": "Graph-based SSL algorithms [38, 17, 33, 4, 26, 30] are a subclass of SSL techniques that have received a lot of attention recently, as they scale much better to large problems and data sizes.", "startOffset": 27, "endOffset": 50}, {"referenceID": 24, "context": "Graph-based SSL algorithms [38, 17, 33, 4, 26, 30] are a subclass of SSL techniques that have received a lot of attention recently, as they scale much better to large problems and data sizes.", "startOffset": 27, "endOffset": 50}, {"referenceID": 28, "context": "Graph-based SSL algorithms [38, 17, 33, 4, 26, 30] are a subclass of SSL techniques that have received a lot of attention recently, as they scale much better to large problems and data sizes.", "startOffset": 27, "endOffset": 50}, {"referenceID": 36, "context": "Graph-based methods based on label propagation [38, 29] work by using class label information associated with each labeled \u201cseed\u201d node, and propagating these labels over the graph in a principled, iterative manner.", "startOffset": 47, "endOffset": 55}, {"referenceID": 27, "context": "Graph-based methods based on label propagation [38, 29] work by using class label information associated with each labeled \u201cseed\u201d node, and propagating these labels over the graph in a principled, iterative manner.", "startOffset": 47, "endOffset": 55}, {"referenceID": 34, "context": "Successful applications include a wide range of tasks in computer vision [36], information retrieval (IR) and social networks [34] and natural language processing (NLP); for example, class instance acquisition and relation prediction, to name a few [30, 27, 19].", "startOffset": 73, "endOffset": 77}, {"referenceID": 32, "context": "Successful applications include a wide range of tasks in computer vision [36], information retrieval (IR) and social networks [34] and natural language processing (NLP); for example, class instance acquisition and relation prediction, to name a few [30, 27, 19].", "startOffset": 126, "endOffset": 130}, {"referenceID": 28, "context": "Successful applications include a wide range of tasks in computer vision [36], information retrieval (IR) and social networks [34] and natural language processing (NLP); for example, class instance acquisition and relation prediction, to name a few [30, 27, 19].", "startOffset": 249, "endOffset": 261}, {"referenceID": 25, "context": "Successful applications include a wide range of tasks in computer vision [36], information retrieval (IR) and social networks [34] and natural language processing (NLP); for example, class instance acquisition and relation prediction, to name a few [30, 27, 19].", "startOffset": 249, "endOffset": 261}, {"referenceID": 17, "context": "Successful applications include a wide range of tasks in computer vision [36], information retrieval (IR) and social networks [34] and natural language processing (NLP); for example, class instance acquisition and relation prediction, to name a few [30, 27, 19].", "startOffset": 249, "endOffset": 261}, {"referenceID": 5, "context": "traction from the Web or social media; scenarios involving complex overlapping classes [7]; or fine-grained classification at large scale for natural language and computer vision applications [28, 13].", "startOffset": 87, "endOffset": 90}, {"referenceID": 26, "context": "traction from the Web or social media; scenarios involving complex overlapping classes [7]; or fine-grained classification at large scale for natural language and computer vision applications [28, 13].", "startOffset": 192, "endOffset": 200}, {"referenceID": 11, "context": "traction from the Web or social media; scenarios involving complex overlapping classes [7]; or fine-grained classification at large scale for natural language and computer vision applications [28, 13].", "startOffset": 192, "endOffset": 200}, {"referenceID": 26, "context": "Talukdar and Cohen [28] recently proposed a method that seeks to overcome the label scale problem by using a Count-Min Sketch [10] to approximate labels and their scores for each node.", "startOffset": 19, "endOffset": 23}, {"referenceID": 8, "context": "Talukdar and Cohen [28] recently proposed a method that seeks to overcome the label scale problem by using a Count-Min Sketch [10] to approximate labels and their scores for each node.", "startOffset": 126, "endOffset": 130}, {"referenceID": 26, "context": "While the sketching technique from [28] approximates the label space succinctly, it does not utilize the sparsity (a naturally occurring phenomenon in real data) to full benefit during learning.", "startOffset": 35, "endOffset": 39}, {"referenceID": 3, "context": "The optimization criterion is inspired from [5] and similar to some existing approaches such as Adsorption [3] and MAD [29] but uses a slightly different objective function, notably the matrices have different constructions.", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "The optimization criterion is inspired from [5] and similar to some existing approaches such as Adsorption [3] and MAD [29] but uses a slightly different objective function, notably the matrices have different constructions.", "startOffset": 107, "endOffset": 110}, {"referenceID": 27, "context": "The optimization criterion is inspired from [5] and similar to some existing approaches such as Adsorption [3] and MAD [29] but uses a slightly different objective function, notably the matrices have different constructions.", "startOffset": 119, "endOffset": 123}, {"referenceID": 3, "context": "More details for deriving the update equation can be found in [5].", "startOffset": 62, "endOffset": 65}, {"referenceID": 18, "context": "We turn to Pregel [20] and its open source version Giraph [2] as the underlying framework for our distributed algorithm.", "startOffset": 18, "endOffset": 22}, {"referenceID": 29, "context": "Previously, some works have explored using MapReduce framework to scale to large graphs [31].", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "MapReduce, however, is essentially functional, so expressing a graph algorithm as a chained MapReduce requires passing the entire state of the graph from one stage to the next\u2014in general requiring much more communication and associated serialization overhead which results in significant network cost (refer [20] Algorithm 1 DIST-EXPANDER Algorithm 1: Input: A graph G = (V,E,W ), where V = Vl \u222a Vu Vl = seed/labeled nodes, Vu = unlabeled nodes 2: Output: A label distribution \u0176v = \u0176v1\u0176v2.", "startOffset": 308, "endOffset": 312}, {"referenceID": 18, "context": "Furthermore, we use a version of Pregel that allows spilling to disk instead of storing the entire computation state in RAM unlike [20].", "startOffset": 131, "endOffset": 135}, {"referenceID": 26, "context": "Talukdar and Cohen [28] proposed to deal with the issue of large label spaces by employing a Count-Min Sketch approximation to store the label distribution of each node.", "startOffset": 19, "endOffset": 23}, {"referenceID": 30, "context": "The authors also mention other related works that attempt to induce sparsity using regularization techniques [32, 18] but for a very different purpose [11].", "startOffset": 109, "endOffset": 117}, {"referenceID": 16, "context": "The authors also mention other related works that attempt to induce sparsity using regularization techniques [32, 18] but for a very different purpose [11].", "startOffset": 109, "endOffset": 117}, {"referenceID": 9, "context": "The authors also mention other related works that attempt to induce sparsity using regularization techniques [32, 18] but for a very different purpose [11].", "startOffset": 151, "endOffset": 155}, {"referenceID": 26, "context": "In contrast, our work tackles the exact same problem as [28] to scale graph-based SSL for large label settings.", "startOffset": 56, "endOffset": 60}, {"referenceID": 28, "context": "based SSL baselines [30, 1] that use heuristics to discard poorly scored labels and retain only top ranking labels per node out of a large label set.", "startOffset": 20, "endOffset": 27}, {"referenceID": 0, "context": "based SSL baselines [30, 1] that use heuristics to discard poorly scored labels and retain only top ranking labels per node out of a large label set.", "startOffset": 20, "endOffset": 27}, {"referenceID": 19, "context": "Preliminary: Manku and Motwani [21] presented an algorithm for computing frequency counts exceeding a user-specified threshold over data streams, and others have applied this algorithm to handle large amounts of data in NLP problems [15, 14, 35, 24].", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "Preliminary: Manku and Motwani [21] presented an algorithm for computing frequency counts exceeding a user-specified threshold over data streams, and others have applied this algorithm to handle large amounts of data in NLP problems [15, 14, 35, 24].", "startOffset": 233, "endOffset": 249}, {"referenceID": 12, "context": "Preliminary: Manku and Motwani [21] presented an algorithm for computing frequency counts exceeding a user-specified threshold over data streams, and others have applied this algorithm to handle large amounts of data in NLP problems [15, 14, 35, 24].", "startOffset": 233, "endOffset": 249}, {"referenceID": 33, "context": "Preliminary: Manku and Motwani [21] presented an algorithm for computing frequency counts exceeding a user-specified threshold over data streams, and others have applied this algorithm to handle large amounts of data in NLP problems [15, 14, 35, 24].", "startOffset": 233, "endOffset": 249}, {"referenceID": 22, "context": "Preliminary: Manku and Motwani [21] presented an algorithm for computing frequency counts exceeding a user-specified threshold over data streams, and others have applied this algorithm to handle large amounts of data in NLP problems [15, 14, 35, 24].", "startOffset": 233, "endOffset": 249}, {"referenceID": 19, "context": "Moreover, each epoch t, which is neighbor ut in our task, is weighted by the edge weight wvut unlike previous settings [21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "The proof for the first part of the statement can be derived following a similar analysis as [21] using label weights instead of frequency.", "startOffset": 93, "endOffset": 97}, {"referenceID": 17, "context": "Recently, some researchers have explored strategies to enhance the input graphs [19] using external sources such as the Web or a knowledge base.", "startOffset": 80, "endOffset": 84}, {"referenceID": 20, "context": "[22, 23] to compute continuous vector representations of words (or phrases) from very large datasets.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[22, 23] to compute continuous vector representations of words (or phrases) from very large datasets.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "We use the continuous skip-gram model [22] combined with a hierarchical softmax layer in which each word in a sentence is used as an input to a log-linear classifier which tries to maximize classification of another word within the same sentence using the current word.", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "More details about the deep learning architecture and training procedure can be found in [22].", "startOffset": 89, "endOffset": 93}, {"referenceID": 10, "context": "Moreover, these models can be efficiently parallelized and scale to huge datasets using a distributed training framework [12].", "startOffset": 121, "endOffset": 125}, {"referenceID": 7, "context": "To address this challenge, we resort to locality sensitive hashing (LSH) [9], a random projection method used to efficiently approximate nearest neighbor lookups when data size and dimensionality is large.", "startOffset": 73, "endOffset": 76}, {"referenceID": 26, "context": "Freebase-Entity (referred as FB-E) is the exact same dataset and setup used in previous works [28, 30].", "startOffset": 94, "endOffset": 102}, {"referenceID": 28, "context": "Freebase-Entity (referred as FB-E) is the exact same dataset and setup used in previous works [28, 30].", "startOffset": 94, "endOffset": 102}, {"referenceID": 27, "context": "For baseline comparison, we consider two state-of-art existing works MAD [29] and MAD-SKETCH [28].", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "For baseline comparison, we consider two state-of-art existing works MAD [29] and MAD-SKETCH [28].", "startOffset": 93, "endOffset": 97}, {"referenceID": 28, "context": "Talukdar and Pereira [30] show that MAD outperforms traditional graph-based SSL algorithms.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "28) for MAD on the Freebase-Entity dataset (10 seeds/label) as reported by [28].", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "We tried multiple settings for MAD and MAD-SKETCH algorithms and replicated the best reported performance metrics from [28] using these values, so the baseline results are comparable to their system.", "startOffset": 119, "endOffset": 123}, {"referenceID": 26, "context": "For experiments, we use the same procedure as reported in literature [28], running each algorithm for 10 iterations per round (verified to be sufficient for convergence on these datasets) and then taking the average performance over 3 rounds.", "startOffset": 69, "endOffset": 73}, {"referenceID": 26, "context": "Following previous work [28], we use the identity of each node as a label.", "startOffset": 24, "endOffset": 28}], "year": 2016, "abstractText": "Traditional graph-based semi-supervised learning (SSL) approaches are not suited for massive data and large label scenarios since they scale linearly with the number of edges |E| and distinct labels m. To deal with the large label size problem, recent works propose sketch-based methods to approximate the label distribution per node thereby achieving a space reduction from O(m) to O(logm), under certain conditions. In this paper, we present a novel streaming graphbased SSL approximation that effectively captures the sparsity of the label distribution and further reduces the space complexity per node to O(1). We also provide a distributed version of the algorithm that scales well to large data sizes. Experiments on real-world datasets demonstrate that the new method achieves better performance than existing state-of-the-art algorithms with significant reduction in memory footprint. Finally, we propose a robust graph augmentation strategy using unsupervised deep learning architectures that yields further significant quality gains for SSL in natural language applications.", "creator": "LaTeX with hyperref package"}}}