{"id": "1512.01872", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "Driverseat: Crowdstrapping Learning Tasks for Autonomous Driving", "abstract": "While emerging deep-learning systems have outclassed knowledge-based approaches in many tasks, their application to detection tasks for autonomous technologies remains an open field for scientific exploration. Broadly, there are two major developmental bottlenecks: the unavailability of comprehensively labeled datasets and of expressive evaluation strategies. Approaches for labeling datasets have relied on intensive hand-engineering, and strategies for evaluating learning systems have been unable to identify failure-case scenarios. Human intelligence offers an untapped approach for breaking through these bottlenecks. This paper introduces Driverseat, a technology for embedding crowds around learning systems for autonomous driving. Driverseat utilizes crowd contributions for (a) collecting complex 3D labels and (b) tagging diverse scenarios for ready evaluation of learning systems. We demonstrate how Driverseat can crowdstrap a convolutional neural network on the lane-detection task. More generally, crowdstrapping introduces a valuable paradigm for any technology that can benefit from leveraging the powerful combination of human and computer intelligence.", "histories": [["v1", "Mon, 7 Dec 2015 01:34:23 GMT  (2364kb,D)", "http://arxiv.org/abs/1512.01872v1", null]], "reviews": [], "SUBJECTS": "cs.HC cs.AI cs.RO", "authors": ["pranav rajpurkar", "toki migimatsu", "jeff kiske", "royce cheng-yue", "sameep tandon", "tao wang", "rew ng"], "accepted": false, "id": "1512.01872"}, "pdf": {"name": "1512.01872.pdf", "metadata": {"source": "META", "title": "Driverseat: Crowdstrapping Learning Tasks for Autonomous Driving", "authors": ["Pranav Rajpurkar", "Toki Migimatsu", "Jeff Kiske", "Royce Cheng-Yue", "Sameep Tandon", "Tao Wang"], "emails": ["PRANAVSR@CS.STANFORD.EDU", "TAKATOKI@CS.STANFORD.EDU", "JKISKE@STANFORD.EDU", "RCHENGYUE@GMAIL.COM", "SAMEEP@STANFORD.EDU", "TWANGCAT@STANFORD.EDU", "ANG@CS.STANFORD.EDU"], "sections": [{"heading": null, "text": "Lectures at the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W & CP Volume 37. Copyright 2015 by author (s). Figure 1. By embedding the mass in learning systems for both education and evaluation, we can leverage the powerful combination of human and computer intelligence."}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. Driverseat: Engineering and Design", "text": "Driver's Seat is a web system that uses human intelligence to (a) collect complex labels and (b) identify scenario-specific weaknesses in learning systems. Modules to achieve these goals are called RoadEdit and TagEval and TagEval, respectively. Driver's significant technical contribution is the introduction of a 3D web interface for crowd interaction. Traditional labeling interfaces only use 2D labeling (Russell et al., 2008; Russakovsky et al., 2014). While 2D labeling is well suited for many tasks, complex labeling tasks in autonomous driving require a more complex 3D interaction interface. Some of the main deficiencies of 2D labeling for a task such as lane marking are that (a) depth information of the dots is not easily visible or modifiable (b) points farther from the car are difficult to label accurately, and (if multiple segments have to be duplicated)."}, {"heading": "2.1. RoadEdit Labeling", "text": "In fact, it is the case that it is a way in which people are able to determine for themselves what they want and what they want. In fact, it is the case that they are able to decide whether they want to or not. In fact, it is the case that they do not want people to be able to decide whether they want to or not. In fact, it is the case that they do not want people to be able to decide whether they want to or not."}, {"heading": "2.2. TagEval Evaluation", "text": "If systems are able to detect particularly precarious scenarios for which confidence in detection is low, the driver can be warned preventatively. TagEval is Driverseat's tagging interface for scenarios marking road conditions (splits, merges, bridges), lighting conditions (shadows, sun) and weather conditions (rainy, snowy) on relevant road sections. These tags can then be used to assess the performance of learning systems under these different scenarios. The TagEval interface is simple: workers use a combination of the 3D virtual environment and 2D camera images to mark road section conditions (illustrated in Figure 2).The tagging process involves selecting a start and end frame, as well as selecting a new / existing category associated with the tag. Access to multiple streams of 3D images is useful to mark road section conditions (illustrated in Figure 2)."}, {"heading": "3. Seeding Human Computation With Automation", "text": "After describing how humans can be used to mark and evaluate data for learning systems, we now describe how computer automation can be used to complement the task of human annotation.Track labeling is one of a series of tasks that are complex for automatic labeling and time-consuming human labeling, and we describe how we can advance automatic labeling to create an initial estimate of the truth and facilitate the manual labeling task, so that annotators only need to validate and correct labeling in complex scenarios (D'Orazio et al., 2009).To generate an initial estimate of the truth-telling, we use a two-step technical methodology. First, we extract the boundaries of the left and right tracks to exceed the boundaries of the ego track. Second, we extend the boundaries of the ego track to multiple tracks."}, {"heading": "4. Crowdstrapping A Convolutional Neural Network for Lane Detection", "text": "Specifically, we are trying to integrate people's experiences into crowdstrap learning systems for autonomous driving by using Driverseat to train and evaluate a Convolutionary Neural Network on the Track Detection Task. To collect marked data to train the Neural Network, we collect (a) data on California highways with our sensor-equipped research vehicle, (b) process the data to create 3D maps of the roadways and to create an initial estimate of the actual location of the roadways using automated labeling, and finally (c) we use Driverseat's RoadEdit interface to allow human annotators to correct and label complex roadway topologies. This marked data serves as input into the Neural Network, whose architecture is detailed (Huval et al., 2015). The neural network's task is to correct the pixel (x, y, deep) locations of the roadway boundaries and to label the images we create by projecting the large datasets."}, {"heading": "5. Results", "text": "We evaluate the performance of the Convolutionary Neural Network in the lane detection task in a number of different scenarios marked with Driverseat's TagEval interface, covering (a) left and right turns, (b) road bends, (c) shadows and (d) sidewalk changes (when the nature / color of the road changes).The qualitative evaluation of network performance in these scenarios reveals strengths and difficulties for the network (see Figure 5).On the one hand, (a) off-ramps are challenging for the network, and emerging roadways are often overlooked, (b) shadows and sidewalk changes are also slightly difficult, and the network becomes prone to misinterpretation of reflective patches on the road with roadway markings. On the other hand, the network is (a) robust against highway curves and (b) able to detect roadway boundaries even when shadows occur."}, {"heading": "6. Conclusions", "text": "The work presents Driverseat, a system that embeds crowds around it, or Crowdstraps, learning systems. In the training of learning systems, we show how Driverseat can use crowd posts to comment on labels that are difficult to automate. In the evaluation of learning systems, we show how Driverseat can use human intelligence to pinpoint scenario-specific weaknesses. To further motivate how human computation can be combined with machine computation, we show how automation can be used to provide initial assessments of the basic truth of the human annotation task. We conclude by applying all of these techniques to the concrete task of lane detection in autonomous driving, a prominent feature of the next generation of autonomous vehicles. We evaluate the performance of the network against a variety of scenarios, some of which are handled better than others. As our learning systems become better and more advanced, new challenges and opportunities for applying complex learning systems to increasingly complex problems arise."}, {"heading": "7. Acknowledgements", "text": "We would like to thank Professor Michael Bernstein for the helpful discussions on this work and Mary McDevitt for her support in writing this essay."}], "references": [{"title": "Recent progress in road and lane detection: a survey", "author": ["Bar Hillel", "Aharon", "Lerner", "Ronen", "Levi", "Dan", "Raz", "Guy"], "venue": "Machine Vision and Applications,", "citeRegEx": "Hillel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hillel et al\\.", "year": 2012}, {"title": "GOLD: a parallel real-time stereo vision system for generic obstacle and lane detection", "author": ["M. Bertozzi", "A. Broggi"], "venue": "IEEE Trans. on Image Processing,", "citeRegEx": "Bertozzi and Broggi,? \\Q1998\\E", "shortCiteRegEx": "Bertozzi and Broggi", "year": 1998}, {"title": "A novel lane detection system with efficient ground truth generation", "author": ["A. Borkar", "M. Hayes", "M.T. Smith"], "venue": "Intelligent Transportation Systems, IEEE Transactions on,", "citeRegEx": "Borkar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Borkar et al\\.", "year": 2012}, {"title": "Lane detection with moving vehicles in the traffic scenes", "author": ["Cheng", "Hsu-Yung", "Jeng", "Bor-Shenn", "Tseng", "Pei-Ting", "Fan", "Kuo-Chin"], "venue": "Intelligent Transportation Systems, IEEE Transactions on,", "citeRegEx": "Cheng et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2006}, {"title": "Automatic single-image 3d reconstructions of indoor manhattan world scenes", "author": ["Delage", "Erick", "Lee", "Honglak", "Ng", "Andrew Y"], "venue": "In Robotics Research,", "citeRegEx": "Delage et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Delage et al\\.", "year": 2007}, {"title": "A semi-automatic system for ground truth generation of soccer video sequences", "author": ["T. D\u2019Orazio", "M. Leo", "N. Mosca", "P. Spagnolo", "P.L. Mazzeo"], "venue": "In Advanced Video and Signal Based Surveillance,", "citeRegEx": "D.Orazio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "D.Orazio et al\\.", "year": 2009}, {"title": "Virtual and Augmented Architecture (VAA01): Proceedings of the International Symposium on Virtual and Augmented Architecture (VAA01)", "author": ["B. Fisher", "K. Dawson-Howe", "C. O\u2019Sullivan"], "venue": "Trinity College, Dublin,", "citeRegEx": "Fisher et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fisher et al\\.", "year": 2001}, {"title": "Deep belief net learning in a long-range vision system for autonomous off-road driving", "author": ["R. Hadsell", "A. Erkan", "P. Sermanet", "M. Scoffier", "U. Muller", "LeCun", "Yann"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "Hadsell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hadsell et al\\.", "year": 2008}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun", "Awni Y", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan C", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam", "Ng", "Andrew Y"], "venue": "CoRR, abs/1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "An Empirical Evaluation of Deep Learning on Highway Driving", "author": ["B. Huval", "T. Wang", "S. Tandon", "J. Kiske", "W. Song", "J. Pazhayampallil", "M. Andriluka", "P. Rajpurkar", "T. Migimatsu", "R. Cheng-Yue", "F. Mujica", "A. Coates", "A.Y. Ng"], "venue": null, "citeRegEx": "Huval et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huval et al\\.", "year": 2015}, {"title": "Crowdforge: Crowdsourcing complex work", "author": ["Kittur", "Aniket", "Smus", "Boris", "Khamkar", "Susheel", "Kraut", "Robert E"], "venue": "In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology,", "citeRegEx": "Kittur et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kittur et al\\.", "year": 2011}, {"title": "Towards fully autonomous driving: Systems and algorithms", "author": ["Levinson", "Jesse", "Askeland", "Jake", "Becker", "Jan", "Dolson", "Jennifer", "Held", "David", "Kammel", "Soeren", "Kolter", "J. Zico", "Langer", "Dirk", "Pink", "Oliver", "Pratt", "Vaughan"], "venue": "IEEE Intelligent Vehicles Symposium (IV),", "citeRegEx": "Levinson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levinson et al\\.", "year": 2011}, {"title": "Drivers With Hands Full Get a Backup: The Car", "author": ["Markoff", "John", "Sengupta", "Somini"], "venue": "New York Times,", "citeRegEx": "Markoff et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Markoff et al\\.", "year": 2013}, {"title": "Deep learning: Machine learning via largescale brain", "author": ["Ng", "Andrew"], "venue": null, "citeRegEx": "Ng and Andrew.,? \\Q2014\\E", "shortCiteRegEx": "Ng and Andrew.", "year": 2014}, {"title": "Alvinn: An autonomous land vehicle in a neural network", "author": ["Pomerleau", "Dean A"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Pomerleau and A.,? \\Q1989\\E", "shortCiteRegEx": "Pomerleau and A.", "year": 1989}, {"title": "Human computation: a survey and taxonomy of a growing field", "author": ["Quinn", "Alexander J", "Bederson", "Benjamin B"], "venue": "In Proceedings of the SIGCHI conference on human factors in computing systems,", "citeRegEx": "Quinn et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Quinn et al\\.", "year": 2011}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael S", "Berg", "Alexander C", "Fei-Fei", "Li"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Labelme: A database and webbased tool for image annotation", "author": ["Russell", "Bryan C", "Torralba", "Antonio", "Murphy", "Kevin P", "Freeman", "William T"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Russell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Russell et al\\.", "year": 2008}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean Y", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Labeling images with a computer game", "author": ["Von Ahn", "Luis", "Dabbish", "Laura"], "venue": "In Proceedings of the SIGCHI conference on Human factors in computing systems,", "citeRegEx": "Ahn et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2004}, {"title": "Lane detection and tracking using b-snake", "author": ["Wang", "Yue", "Teoh", "Eam Khwang", "Shen", "Dinggang"], "venue": "Image and Vision computing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Yenikaya, Gkhan, and Dven, Ekrem", "author": ["Yenikaya", "Sibel"], "venue": "Keeping the vehicle on the road. CSUR,", "citeRegEx": "Yenikaya and Sibel,? \\Q2013\\E", "shortCiteRegEx": "Yenikaya and Sibel", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "(Levinson et al., 2011).", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "Deep learning systems represent an alternative approach (Hadsell et al., 2008).", "startOffset": 56, "endOffset": 78}, {"referenceID": 8, "context": ", 2012), speech (Hannun et al., 2014), and language understanding tasks (Socher et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 18, "context": ", 2014), and language understanding tasks (Socher et al., 2013).", "startOffset": 42, "endOffset": 63}, {"referenceID": 2, "context": "On one end of the learning pipeline, approaches to labeling datasets for detection tasks in autonomous driving have focused on hand-engineering (Borkar et al., 2012) or on synthetic generation of labels (Pomerleau, 1989).", "startOffset": 144, "endOffset": 165}, {"referenceID": 3, "context": "Approaches have relied on sophisticated modelling of road and motion (Bertozzi & Broggi, 1998; Cheng et al., 2006; Wang et al., 2004), yet have not been able to accurately model lane topologies in their full complexity.", "startOffset": 69, "endOffset": 133}, {"referenceID": 20, "context": "Approaches have relied on sophisticated modelling of road and motion (Bertozzi & Broggi, 1998; Cheng et al., 2006; Wang et al., 2004), yet have not been able to accurately model lane topologies in their full complexity.", "startOffset": 69, "endOffset": 133}, {"referenceID": 17, "context": "tional labeling interfaces exploit only 2D labeling (Russell et al., 2008; Russakovsky et al., 2014).", "startOffset": 52, "endOffset": 100}, {"referenceID": 16, "context": "tional labeling interfaces exploit only 2D labeling (Russell et al., 2008; Russakovsky et al., 2014).", "startOffset": 52, "endOffset": 100}, {"referenceID": 10, "context": "While some labeling tasks involve little more than drawing a bounding box around an object in an image, other labeling tasks are more complex (Kittur et al., 2011).", "startOffset": 142, "endOffset": 163}, {"referenceID": 4, "context": "While there are techniques for automatically estimating depth maps (Delage et al., 2007), implementation of such techniques requires a significant engineering effort.", "startOffset": 67, "endOffset": 88}, {"referenceID": 5, "context": "We describe how we can bootstrap automatic labeling to generate an initial ground-truth estimate, and ease the manual labeling task so that annotators only have to validate and make corrections to labels in complex scenarios (D\u2019Orazio et al., 2009).", "startOffset": 225, "endOffset": 248}, {"referenceID": 9, "context": "This labeled data serves as input to the neural network, the architecture of which is detailed in (Huval et al., 2015).", "startOffset": 98, "endOffset": 118}], "year": 2015, "abstractText": "While emerging deep-learning systems have outclassed knowledge-based approaches in many tasks, their application to detection tasks for autonomous technologies remains an open field for scientific exploration. Broadly, there are two major developmental bottlenecks: the unavailability of comprehensively labeled datasets and of expressive evaluation strategies. Approaches for labeling datasets have relied on intensive hand-engineering, and strategies for evaluating learning systems have been unable to identify failure-case scenarios. Human intelligence offers an untapped approach for breaking through these bottlenecks. This paper introduces Driverseat, a technology for embedding crowds around learning systems for autonomous driving. Driverseat utilizes crowd contributions for (a) collecting complex 3D labels and (b) tagging diverse scenarios for ready evaluation of learning systems. We demonstrate how Driverseat can crowdstrap a convolutional neural network on the lane-detection task. More generally, crowdstrapping introduces a valuable paradigm for any technology that can benefit from leveraging the powerful combination of human and computer intelligence. Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s). Figure 1. Embedding the crowd around learning systems for both training and evaluation, we can leverage the powerful combination of human and computer intelligence.", "creator": "LaTeX with hyperref package"}}}