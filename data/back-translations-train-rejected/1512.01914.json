{"id": "1512.01914", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "Rademacher Complexity of the Restricted Boltzmann Machine", "abstract": "Boltzmann machine, as a fundamental construction block of deep belief network and deep Boltzmann machines, is widely used in deep learning community and great success has been achieved. However, theoretical understanding of many aspects of it is still far from clear. In this paper, we studied the Rademacher complexity of both the asymptotic restricted Boltzmann machine and the practical implementation with single-step contrastive divergence (CD-1) procedure. Our results disclose the fact that practical implementation training procedure indeed increased the Rademacher complexity of restricted Boltzmann machines. A further research direction might be the investigation of the VC dimension of a compositional function used in the CD-1 procedure.", "histories": [["v1", "Mon, 7 Dec 2015 05:20:30 GMT  (9kb)", "http://arxiv.org/abs/1512.01914v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiao zhang"], "accepted": false, "id": "1512.01914"}, "pdf": {"name": "1512.01914.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["zhang923@purdue.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.01 914v 1 [cs.L G] 7Boltzmann machine as a fundamental building block of a deep faith network and deep Boltzmann machines is widely used in the deep learning community and great successes have been achieved. However, the theoretical understanding of many aspects of this is still far from clear. In this work we investigated the Rademacher complexity of both the asymptotically limited Boltzmann machine and the practical implementation with a one-step contrastive divergence method (CD-1). Our results show that practical implementation training actually increased the Rademacher complexity of restricted Boltzmann machines. Another research direction could be the investigation of the VC dimension of a compositional function used in the CD-1 process."}, {"heading": "1 Introduction", "text": "A restricted Boltzmann machine (RBM) is a generative graphical model that can learn a probability distribution via its input factors. Originally proposed by Smolensky [1986] for modeling cognitive processes, it grew in importance after successful application by Geoffrey Hinton and his collaborators [Hinton and Salakhutdinov, 2006, 2012; Salakhutdinov and Hinton, 2009]. As a building block for a deep faith network (DBN) and deep Boltzmann machines (DBM), RBM is extremely useful for pre-training data by projecting it onto a hidden layer. Furthermore, it is proven that adding another layer on an RBM can increase the variation-related lower limit of data probability [Hinton et al., 2006; Salakhutdinov and Hinton, 2012], which demonstrates the theoretical advantage of building multi-layer RBMs."}, {"heading": "2 Preliminaries", "text": "At the beginning of this section, we introduce the Lipschitz continuity.Definition 1. Given the parameters we know (RM1) and (M2), a differentiable function f (set point) and f (set point) is called the Lipschitz continuity with respect to the lp standard, if there is a constant K \u2265 0, so that: (set point 1, set point 2) and f (set point 2, set point 1 \u2212 set point 2) or equivalent: (set point). Next, we introduce the wheel maker complexity. Definition 2.Definition 2.Definition 2.1 \u2212 f (set point 1, set point 1 \u2212 set point 2 (set point)."}, {"heading": "3 Rademacher Complexity", "text": "In this section we offer an upward binding of the empirical Rademacher complexity for the probability of the restricted Boltzmann machine. Since part 2 of the division function of the restricted Boltzmann machine, equation 7, does not depend on the amount of data, this part has no randomness and the Rademacher complexity thereof is by definition 0. Therefore, we can only concentrate on the Rademacher complexity of part 1 of equation 7. Denote Wj as the j-th column of the matrix W, cj as the j-th element of c, hj as the j-th element of h. By extending part 1 of equation 7, we getpart 1 = ln {\u2212 hexp energy (x, h; lemp.)} (8) = ln {hmexpxxxxTb = hexix class of h."}, {"heading": "4 Rademacher Complexity with CD-1 Approximation", "text": "s \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \"W\".W \".W\".W \".W\".W \"W\".W \".W\".W \".W\" W \".W\".W \"W\".W \".W\".W \"W\".W \".W\" W \".W\" W \".W\".W \".W\".W \"W\".W \"W\".W \".W\" W \"W\".W \".W\".W. W \"W\" W \".W\".W. W \"W\".W \"W\" W \".W\" W \".W\" W \".W. W\" W \".W\" W \"W\".W \".W. W\".W \"W\".W \"W\".W \".W. W\" W \".W\" W \"W\".W \".W\".W \"W\""}, {"heading": "5 Future Direction", "text": "Can we extend these results to multi-layer Boltzmann machines such as Deep Believe Networks (DBN) or Deep Boltzmann Machines (DBM)? Is it possible to obtain the exact expression of the VC dimension of our constructed function T?"}], "references": [{"title": "Learning deep architectures for ai", "author": ["Yoshua Bengio"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "A better way to pretrain deep Boltzmann machines", "author": ["Geoffrey Hinton", "Ruslan Salakhutdinov"], "venue": "Advances in Neural Information,", "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2012\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E. Hinton", "R R Salakhutdinov"], "venue": "Science (New York, N.Y.),", "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Lipschitz parametrization of probabilistic graphical models", "author": ["Jean Honorio"], "venue": "Uncertainty in Artificial Intelligence", "citeRegEx": "Honorio.,? \\Q2012\\E", "shortCiteRegEx": "Honorio.", "year": 2012}, {"title": "Deep Boltzmann Machines", "author": ["Ruslan Salakhutdinov", "Geoffrey Hinton"], "venue": "Artificial Intelligence,", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "An Efficient Learning Procedure for Deep Boltzmann Machines", "author": ["Ruslan Salakhutdinov", "Geoffrey Hinton"], "venue": "Neural Computation,", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2012}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["Paul Smolensky"], "venue": "Parallel Distributed Processing Explorations in the Microstructure of Cognition,", "citeRegEx": "Smolensky.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky.", "year": 1986}], "referenceMentions": [{"referenceID": 5, "context": "Initially proposed by Smolensky [1986] for modeling cognitive process, it grew to prominence after successful application were found by Geoffrey Hinton and his collaborators [Hinton and Salakhutdinov, 2006, 2012; Salakhutdinov and Hinton, 2009].", "startOffset": 174, "endOffset": 244}, {"referenceID": 3, "context": "Also, it is proved that by adding another layer on top of a RBM, the variational lower bound of the data likelihood can be increased [Hinton et al., 2006; Salakhutdinov and Hinton, 2012], which conveys the theoretical advantage of building multilayer RBMs.", "startOffset": 133, "endOffset": 186}, {"referenceID": 6, "context": "Also, it is proved that by adding another layer on top of a RBM, the variational lower bound of the data likelihood can be increased [Hinton et al., 2006; Salakhutdinov and Hinton, 2012], which conveys the theoretical advantage of building multilayer RBMs.", "startOffset": 133, "endOffset": 186}, {"referenceID": 2, "context": "Initially proposed by Smolensky [1986] for modeling cognitive process, it grew to prominence after successful application were found by Geoffrey Hinton and his collaborators [Hinton and Salakhutdinov, 2006, 2012; Salakhutdinov and Hinton, 2009].", "startOffset": 22, "endOffset": 39}, {"referenceID": 4, "context": "Honorio [2012] also proved that discrete factor graphs, including Markov random fields, are Lipschitz continuous, which motivated this work to further investigate the properties of RBM.", "startOffset": 0, "endOffset": 15}, {"referenceID": 0, "context": "But experiments have shown that, even one step (CD-1) can yield a good performance for the model [Bengio, 2009].", "startOffset": 97, "endOffset": 111}], "year": 2015, "abstractText": "Boltzmann machine, as a fundamental construction block of deep belief network and deep Boltzmann machines, is widely used in deep learning community and great success has been achieved. However, theoretical understanding of many aspects of it is still far from clear. In this paper, we studied the Rademacher complexity of both the asymptotic restricted Boltzmann machine and the practical implementation with single-step contrastive divergence (CD-1) procedure. Our results disclose the fact that practical implementation training procedure indeed increased the Rademacher complexity of restricted Boltzmann machines. A further research direction might be the investigation of the VC dimension of a compositional function used in the CD-1 procedure.", "creator": "LaTeX with hyperref package"}}}