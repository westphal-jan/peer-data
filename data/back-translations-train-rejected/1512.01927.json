{"id": "1512.01927", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "Fast Optimization Algorithm on Riemannian Manifolds and Its Application in Low-Rank Representation", "abstract": "The paper addresses the problem of optimizing a class of composite functions on Riemannian manifolds and a new first order optimization algorithm (FOA) with a fast convergence rate is proposed. Through the theoretical analysis for FOA, it has been proved that the algorithm has quadratic convergence. The experiments in the matrix completion task show that FOA has better performance than other first order optimization methods on Riemannian manifolds. A fast subspace pursuit method based on FOA is proposed to solve the low-rank representation model based on augmented Lagrange method on the low rank matrix variety. Experimental results on synthetic and real data sets are presented to demonstrate that both FOA and SP-RPRG(ALM) can achieve superior performance in terms of faster convergence and higher accuracy.", "histories": [["v1", "Mon, 7 Dec 2015 06:44:23 GMT  (614kb,D)", "http://arxiv.org/abs/1512.01927v1", null]], "reviews": [], "SUBJECTS": "cs.NA cs.CV cs.LG", "authors": ["haoran chen", "yanfeng sun", "junbin gao", "yongli hu"], "accepted": false, "id": "1512.01927"}, "pdf": {"name": "1512.01927.pdf", "metadata": {"source": "CRF", "title": "Fast Optimization Algorithm on Riemannian Manifolds and Its Application in Low-Rank Representation", "authors": ["Haoran Chen", "Yanfeng Sun", "Junbin Gao", "Yongli Hu"], "emails": ["chen@emails.bjut.edu.cn,", "yfsun@bjut.edu.cn,", "huyongli@bjut.edu.cn", "jbgao@csu.edu.au"], "sections": [{"heading": null, "text": "Keywords Fast Optimization Algorithm \u00b7 Rieman Multiplicity \u00b7 Proximal Riemannian Gradient \u00b7 Subspace Tracking \u00b7 Low Rank Matrix Diversity \u00b7 Low Rank Display \u00b7 Advanced Lagrange Method \u00b7 Clustering"}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Notations and Preliminaries", "text": "It's a matter of time before it's going to come to a solution, which is in the question of the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which way in which it's going to be about the way in which way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which way in which it's going to be about in which way in which it's going to be about the way in which it's going to be about the way in which way in which way in which way in which it's going to be about the way in which way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which it's going to be about the way in which way in which it's going to be about the way in which way in which it's going to be about it's going to be about it's going to be about the way in which way in which way in which way in which way in which way in which"}, {"heading": "3 The Fast Optimization Algorithm on Riemannian manifold", "text": "We consider the following general model (13) on Riemannian manifold defined in S2.It is usually directly difficult to solve (13).We assume that the problem formula is manifold in [3] min X-M F (X) = f (X) + g (X) + g (13).We make the following assumptions throughout the section: 1. g: Rm \u00b7 n \u2192 R is a continuous convex function that may not be smooth. 2. f: Rm \u00b7 n \u2192 R is a smooth convex function of type C2, there is a finite number L (f), so that max (H). L (f), where there is the largest single value of the function f.3. F (X) fulfills the following inequality, F (X). F (Y). F (X)."}, {"heading": "4 The application of FOA in low rank representation on low rank matrix varieties", "text": "In this section, we try to apply algorithm 2 to low ranks (see section 2.1). (27) The model is defined as follows: min X, E, E, E, S, S, D, E, E, E, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S"}, {"heading": "5 Experimental Results and Analysis", "text": "In this section, we conduct several experiments to evaluate the proposed algorithms. - We focus on the low rate of matrix processing on a synthetic data and cluster system that compares the performance of FOA in converged rate and error rate. - We focus on the low rate (R2014a) and implement on the PC machine a 64-bit operating system with an iterl (R) Core (TM) i7 CPU (3.4GHz with a single thread mode) and 4GB memory.Algorithm 3 based on ALM \u2264 r for model (27) Input: 0, 0 < p < p < p < p < p < p > 1 and 4GB memory.D Data matricesD are positive. 1: Initial X00, E 0 and U 0 are zero matrices, r = l."}, {"heading": "6 Conclusions", "text": "Unlike 3 http: / / archive.org /.most optimization methods on Riemannian manifold, our algorithm uses only information about firstor functions, but has the convergence rate O (k \u2212 2). Experiments on some data sets show that optimization performance in terms of convergence rate and accuracy far exceeds existing methods. In addition, we transform the low representation model into optimization problems based on the extended lagrange approach, then fast sub-space tracking methods based on FOA are applied to solve optimization problems. Extensive experimental results prove the superiority of our proposed ALM with a fast subspace purist approach."}, {"heading": "Appendix A", "text": "The premise that the vector transport Ts is positive means that for all X, Y and all Y (all X, Y and all X, TXM, the equation < TX, TX and Y (all X and all Y) (all X and all Y) (all X and all Y) (all X and all Y) (all X and all Y) (all X and all Y) (all X and all Y) (all Y) (all X and all Y) (all Y) (all X and all Y) (all Y) (all X and all Y) (all Y) (all Y) (all Y) (all Y) (all X and all Y) (all Y) (all Y) (all X and all Y) (all Y) (all Y) (all Y) (all Y) (all Y) (all Y)."}], "references": [{"title": "Trust-region methods on riemannian manifolds", "author": ["P.A. Absil", "C. Baker", "K.A. Gallivan"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Optimization Algorithm on Matrix Manifolds", "author": ["P.A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Kdd cup and workshop", "author": ["J. Bennett", "C. Elkan", "B. Liu", "P. Smyth", "D. Tikk"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "RTRMC: A riemannian trust-region method for low-rank matrix completion", "author": ["N. Boumal", "P.A. Absil"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Minimizing a differentiable function over a differential manifold", "author": ["D. Gabay"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Extrinsic methods for coding and dictionary learning on", "author": ["M. Harandi", "R. Hartley", "C. Shen", "B. Lovell", "C. Sanderson"], "venue": "Grassmann manifolds,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A riemannian symmetric rank-one trust-region method", "author": ["W. Huang", "P.A. Absil", "K.A. Gallivan"], "venue": "Mathematical Programming,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "An accelerated gradient method for trace norm minimization", "author": ["S. Ji", "J. Ye"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Introduction to smooth manifolds, Springer New York", "author": ["J.M. Lee"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Linearized alternating direction method with adaptive penalty for low-rank representation", "author": ["Z. Lin", "R. Liu", "Z. Su"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["G. Liu", "Z. Lin", "S. Yan", "J. Sun", "Y. Yu", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["G. Liu", "Z. Lin", "Y. Yu"], "venue": "Pro- ceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "A riemannian geometry for low-rank matrix com- pletion", "author": ["B. Mishra", "K. Apuroop", "R. Sepulchre"], "venue": "arXiv preprint arXiv:1211.1550", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Low-rank optimization with trace norm penalty", "author": ["B. Mishra", "G. Meyer", "F. Bach", "R. Sepulchre"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Fixed-rank matrix factorizations and riemannian low-rank optimization", "author": ["B. Mishra", "G. Meyer", "S. Bonnabel", "R. Sepulchre"], "venue": "Computational Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A method for solving the convex programming problem with convergence rate O(1/k2)", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1983}, {"title": "Gradient methods for minimizing composite functions", "author": ["Y. Nesterov"], "venue": "Mathematical Pro- gramming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Scaled gradients on grassmann manifolds for matrix completion", "author": ["T. Ngo", "Y. Saad"], "venue": "Ad- vances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": "SIAM Review,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Optimization methods on riemannian manifolds and their application to shape space", "author": ["W. Ring", "B. Wirth"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Convergence results for projected line-search methods on varieties of low-rank matrices via lojasiewicz inequality", "author": ["R. Schneider", "A. Uschmajew"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Geometric optimization methods for adaptive filtering, Division of applied sciences", "author": ["S.T. Smith"], "venue": "Harvard University,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1993}, {"title": "Rate-invariant analysis of trajectories on riemannian manifolds with application in visual speech recognition", "author": ["J. Su", "A. Srivastava", "F. de Souza", "S. Sarkar"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Scalable nuclear-norm minimization by subspace pursuit proximal riemannian gradient", "author": ["M. Tan", "J. Shi", "J. Gao", "A. Hengel", "D. Xu", "S. Xiao"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Riemannian pursuit for big matrix recovery", "author": ["M. Tan", "I. Tsang", "L. Wang", "B. Vandereycken", "S. Pan"], "venue": "Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Subspace clutering for sequential data", "author": ["S. Tierney", "J. Gao", "Y. Guo"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems, Pacific", "author": ["K. Toh", "S. Yun"], "venue": "Journal of Optimization", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Statistical computations on grassmann and stiefel manifolds for image and video-based recognition", "author": ["P. Turaga", "A. Veeraraghavan", "A. Srivastava", "R. Chellappa"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Convex functions and optimization methods on Riemannian manifolds, Springer Science & Business Media", "author": ["C. Udriste"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1994}, {"title": "Vandereycken,B., Line-search methods and rank increase on low-rank matrix varieties", "author": ["A. Uschmajew"], "venue": "Proceedings of the 2014 International Symposium on Nonlinear Theory and its Applications,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Low-rank matrix completion by riemannian optimization", "author": ["B. Vandereycken"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "The Riemannian optimization have been successfully applied in machine learning, computer vision and data mining tasks, including fixed low rank optimization [5, 33], Riemannian dictionary learning [7], and computer vision [?, 30, 34], and tensor clustering [25].", "startOffset": 157, "endOffset": 164}, {"referenceID": 31, "context": "The Riemannian optimization have been successfully applied in machine learning, computer vision and data mining tasks, including fixed low rank optimization [5, 33], Riemannian dictionary learning [7], and computer vision [?, 30, 34], and tensor clustering [25].", "startOffset": 157, "endOffset": 164}, {"referenceID": 6, "context": "The Riemannian optimization have been successfully applied in machine learning, computer vision and data mining tasks, including fixed low rank optimization [5, 33], Riemannian dictionary learning [7], and computer vision [?, 30, 34], and tensor clustering [25].", "startOffset": 197, "endOffset": 200}, {"referenceID": 1, "context": "Methods of solving minimization problems on Riemannian manifolds have been extensively researched [2, 31].", "startOffset": 98, "endOffset": 105}, {"referenceID": 29, "context": "Methods of solving minimization problems on Riemannian manifolds have been extensively researched [2, 31].", "startOffset": 98, "endOffset": 105}, {"referenceID": 5, "context": "As one of fundamental optimization algorithms, the steepest descent method was first proposed for optimization on Riemmanian manifolds in [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "In contrast, the Newton\u2019s method [6] and the BFGS quasi-Newton scheme (BFGS rank2-update) [21] have higher convergence rate, however, in practical applications, using the full second-order Hessian information is computationally prohibitive.", "startOffset": 33, "endOffset": 36}, {"referenceID": 20, "context": "In contrast, the Newton\u2019s method [6] and the BFGS quasi-Newton scheme (BFGS rank2-update) [21] have higher convergence rate, however, in practical applications, using the full second-order Hessian information is computationally prohibitive.", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "[1] proposed the Trust-region method on Riemannian manifolds.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "For example, the Trust-region method has been applied the optimization problem on Grassmann manifold for the matrix completion problem [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 22, "context": "Each iteration of the trust-region method involves solving the Riemannian Newton equation [23], which increases the complexity of the algorithm.", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "[8] generalized symmetric rank-one trust-region method to a vector variable optimization problem on a d-dimensional Riemannian manifold, where an approximated Hessian matrix was generated by using the symmetric rank-one update without solving Riemannian Newton equation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "FletcherReeves conjugate gradient method on Riemannian manifolds [21] is a typical type of methods which need only the first-order function information, and its convergence is superlinear but lower than the desired 2-order speed.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "based on linear search [17].", "startOffset": 23, "endOffset": 27}, {"referenceID": 2, "context": "More strategies have been proposed by considering special structures of objectives, such as composite objective functions [3,18,29].", "startOffset": 122, "endOffset": 131}, {"referenceID": 17, "context": "More strategies have been proposed by considering special structures of objectives, such as composite objective functions [3,18,29].", "startOffset": 122, "endOffset": 131}, {"referenceID": 27, "context": "More strategies have been proposed by considering special structures of objectives, such as composite objective functions [3,18,29].", "startOffset": 122, "endOffset": 131}, {"referenceID": 13, "context": "The low-rank learning has been widely applied in the low-rank matrix recovery (MR)/low-rank matrix completion [14\u201316] and low-rank representation (LRR) for subspace clustering [11\u201313].", "startOffset": 110, "endOffset": 117}, {"referenceID": 14, "context": "The low-rank learning has been widely applied in the low-rank matrix recovery (MR)/low-rank matrix completion [14\u201316] and low-rank representation (LRR) for subspace clustering [11\u201313].", "startOffset": 110, "endOffset": 117}, {"referenceID": 15, "context": "The low-rank learning has been widely applied in the low-rank matrix recovery (MR)/low-rank matrix completion [14\u201316] and low-rank representation (LRR) for subspace clustering [11\u201313].", "startOffset": 110, "endOffset": 117}, {"referenceID": 10, "context": "The low-rank learning has been widely applied in the low-rank matrix recovery (MR)/low-rank matrix completion [14\u201316] and low-rank representation (LRR) for subspace clustering [11\u201313].", "startOffset": 176, "endOffset": 183}, {"referenceID": 11, "context": "The low-rank learning has been widely applied in the low-rank matrix recovery (MR)/low-rank matrix completion [14\u201316] and low-rank representation (LRR) for subspace clustering [11\u201313].", "startOffset": 176, "endOffset": 183}, {"referenceID": 12, "context": "The low-rank learning has been widely applied in the low-rank matrix recovery (MR)/low-rank matrix completion [14\u201316] and low-rank representation (LRR) for subspace clustering [11\u201313].", "startOffset": 176, "endOffset": 183}, {"referenceID": 3, "context": "One of low-rank matrix completion application examples is the Netflix problem [4], in which one would like to recover a low-rank matrix from a sparse sampled matrix entries.", "startOffset": 78, "endOffset": 81}, {"referenceID": 19, "context": "However, the lowest rank optimization problems are NP hard and generally extremely hard to solve (and also hard to approximate [20]).", "startOffset": 127, "endOffset": 131}, {"referenceID": 1, "context": "As the set of rank-r matrix Mr is a smooth Riemannian manifold under an appropriately chosen metric [2, 14, 16, 33], one can convert a rank constrained optimization problem to an unconstrained optimization problem on the low rank matrix manifold.", "startOffset": 100, "endOffset": 115}, {"referenceID": 13, "context": "As the set of rank-r matrix Mr is a smooth Riemannian manifold under an appropriately chosen metric [2, 14, 16, 33], one can convert a rank constrained optimization problem to an unconstrained optimization problem on the low rank matrix manifold.", "startOffset": 100, "endOffset": 115}, {"referenceID": 15, "context": "As the set of rank-r matrix Mr is a smooth Riemannian manifold under an appropriately chosen metric [2, 14, 16, 33], one can convert a rank constrained optimization problem to an unconstrained optimization problem on the low rank matrix manifold.", "startOffset": 100, "endOffset": 115}, {"referenceID": 31, "context": "As the set of rank-r matrix Mr is a smooth Riemannian manifold under an appropriately chosen metric [2, 14, 16, 33], one can convert a rank constrained optimization problem to an unconstrained optimization problem on the low rank matrix manifold.", "startOffset": 100, "endOffset": 115}, {"referenceID": 31, "context": "Vandereycken [33] considered this problem as an optimization problem on fixed-rank matrix Riemannian manifold.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "[14, 16] proposed a Riemannian quotient manifold for low-rank matrix completion.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[14, 16] proposed a Riemannian quotient manifold for low-rank matrix completion.", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[19] addressed this problem based on a scaled metric on the Grassmann manifold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] proposed a pursuit algorithm that alternates between fixed-rank optimization and rank-one updates to search the best rank value.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] proposed a Riemannian pursuit approach which converts low-rank problem into a series of fixed rank problems, and further confirmed that low-rank problem can be considered as optimization whose search space is varieties of low-rank matrices M\u2264r [26], see (7), using the subspace pursuit approach on Riemannian manifold to look for the desired rank value r.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27] proposed a Riemannian pursuit approach which converts low-rank problem into a series of fixed rank problems, and further confirmed that low-rank problem can be considered as optimization whose search space is varieties of low-rank matrices M\u2264r [26], see (7), using the subspace pursuit approach on Riemannian manifold to look for the desired rank value r.", "startOffset": 249, "endOffset": 253}, {"referenceID": 9, "context": "A manifold M of dimension m [10] is a topological space that locally resembles a Euclidean space R in a neighbourhood of each point X \u2208M.", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "A geodesic \u03b3 : [0, 1] \u2192 M is a smooth curve with a vanishing covariant derivative of its tangent vector field, and in particular, the Riemmannian distance between two points Xi,Xj \u2208 M is the shortest smooth path connecting them on the manifold, that is the infimum of the lengths of all paths joining Xi and Xj .", "startOffset": 15, "endOffset": 21}, {"referenceID": 31, "context": "It can be proved that Mr is a Riemannian manifold of dimension m+ n\u2212 r [33].", "startOffset": 71, "endOffset": 75}, {"referenceID": 21, "context": "At a singular point X where rank(X)= s < r, we have to use search directions in the tangent cone (instead of tangent space), The tangent cones of M\u2264r are explicitly known [22], TXM\u2264r = TXMs \u2295 {\u039er\u2212s \u2208 U \u2297 V}, (8) where U = rang(X) and V = rang(X), X = U\u03a3V, rank(X) = s < r.", "startOffset": 171, "endOffset": 175}, {"referenceID": 24, "context": "Moreover, \u039er\u2212s can be efficiently computed with the same complexity as on Mr in [26].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "We consider the following general model (13) on Riemannian manifold which also naturally extends the problem formulation in [3]", "startOffset": 124, "endOffset": 127}, {"referenceID": 24, "context": "Here we adopt the following proximal Riemannian gradient (PRG) method [26] to update X.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "Note the above local model is different from that on vector spaces [18, 29].", "startOffset": 67, "endOffset": 75}, {"referenceID": 27, "context": "Note the above local model is different from that on vector spaces [18, 29].", "startOffset": 67, "endOffset": 75}, {"referenceID": 1, "context": "This conclusion has been shown in [2] when g(X) = 0.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "In order to obtain linear search with quadratic convergence, we propose a fast optimization algorithm (FOA) which extends the acceleration methods in [3,9,29] onto Riemannian manifold.", "startOffset": 150, "endOffset": 158}, {"referenceID": 8, "context": "In order to obtain linear search with quadratic convergence, we propose a fast optimization algorithm (FOA) which extends the acceleration methods in [3,9,29] onto Riemannian manifold.", "startOffset": 150, "endOffset": 158}, {"referenceID": 27, "context": "In order to obtain linear search with quadratic convergence, we propose a fast optimization algorithm (FOA) which extends the acceleration methods in [3,9,29] onto Riemannian manifold.", "startOffset": 150, "endOffset": 158}, {"referenceID": 1, "context": "Lemma 1 (Optimality Condition) A point X\u2217 \u2208 M is a local minimizer of (13) if and only if there exists \u03b7 \u2208 \u2202g(X) such that [2]", "startOffset": 123, "endOffset": 126}, {"referenceID": 10, "context": "In order to solve the problem (23) following [11], we can optimize the variables X and E by alternating direction approach.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "In spite of the non-smooth of setM\u2264r, it has been shown in [22] that the tangent cone of M\u2264r at singular points with rank(X) < r has a rather simple characterization and can be calculated easily (see section2.", "startOffset": 59, "endOffset": 63}, {"referenceID": 30, "context": "Hence the model (29) can be directly optimized on M\u2264r [32] as section 3.", "startOffset": 54, "endOffset": 58}, {"referenceID": 24, "context": "(30) where U+diag(\u03c3)+V T + = RYk(\u2212gradfX(Yk,Ek)/\u03b1) (see [26]), and P\u03b1(Yk) can be efficiently computed in the sense that RYk(\u2212gradfX(Yk,Ek)/\u03b1) can be cheaply computed without expensive SVDs [33].", "startOffset": 56, "endOffset": 60}, {"referenceID": 31, "context": "(30) where U+diag(\u03c3)+V T + = RYk(\u2212gradfX(Yk,Ek)/\u03b1) (see [26]), and P\u03b1(Yk) can be efficiently computed in the sense that RYk(\u2212gradfX(Yk,Ek)/\u03b1) can be cheaply computed without expensive SVDs [33].", "startOffset": 189, "endOffset": 193}, {"referenceID": 24, "context": "where Eki denotes the ith column of Ek, \u2200i [26].", "startOffset": 43, "endOffset": 47}, {"referenceID": 31, "context": "Following [33], we generate ground-truth low-rank matrices A = LR \u2208 Rm\u00d7n of rank r, where L \u2208 Rm\u00d7r,R \u2208 Rr\u00d7n is rank r matrix generated randomly.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "We test FOA on the matrix completion task and compare their optimization performance against qGeomMC [15], LRGeomCG [33], and LRGeomSD (steepest descent method) on fixed-rank Riemannian manifold.", "startOffset": 101, "endOffset": 105}, {"referenceID": 31, "context": "We test FOA on the matrix completion task and compare their optimization performance against qGeomMC [15], LRGeomCG [33], and LRGeomSD (steepest descent method) on fixed-rank Riemannian manifold.", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "The compared algorithm include LRR [12](solving model (27) on euclidean space), SP-RPRG [26](solving model (27) based on low rank matrix varieties) and SP-RPRG(ALM)(solving model (27) based on augmented Lagrange method on low rank matrix vatieties).", "startOffset": 35, "endOffset": 39}, {"referenceID": 24, "context": "The compared algorithm include LRR [12](solving model (27) on euclidean space), SP-RPRG [26](solving model (27) based on low rank matrix varieties) and SP-RPRG(ALM)(solving model (27) based on augmented Lagrange method on low rank matrix vatieties).", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "1 for LRR 2 [12], and \u03bb = 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 24, "context": "01, \u03c1 = 1 for SP-RPRG [26].", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "The video sequences are drawn from two short animations freely available from the Internet Archive , this are same as the data used in [28].", "startOffset": 135, "endOffset": 139}], "year": 2015, "abstractText": "The paper addresses the problem of optimizing a class of composite functions on Riemannian manifolds and a new first order optimization algorithm (FOA) with a fast convergence rate is proposed. Through the theoretical analysis for FOA, it has been proved that the algorithm has quadratic convergence. The experiments in the matrix completion task show that FOA has better performance than other first order optimization methods on Riemannian manifolds. A fast subspace pursuit method based on FOA is proposed to solve the low-rank representation model based on augmented Lagrange method on the low rank matrix variety. Experimental results on synthetic and real data sets are presented to demonstrate that both FOA and SP-RPRG(ALM) can achieve superior performance in terms of faster convergence and higher accuracy.", "creator": "LaTeX with hyperref package"}}}