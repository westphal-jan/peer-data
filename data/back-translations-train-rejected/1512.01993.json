{"id": "1512.01993", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "A Novel Approach to Distributed Multi-Class SVM", "abstract": "With data sizes constantly expanding, and with classical machine learning algorithms that analyze such data requiring larger and larger amounts of computation time and storage space, the need to distribute computation and memory requirements among several computers has become apparent. Although substantial work has been done in developing distributed binary SVM algorithms and multi-class SVM algorithms individually, the field of multi-class distributed SVMs remains largely unexplored. This research proposes a novel algorithm that implements the Support Vector Machine over a multi-class dataset and is efficient in a distributed environment (here, Hadoop). The idea is to divide the dataset into half recursively and thus compute the optimal Support Vector Machine for this half during the training phase, much like a divide and conquer approach. While testing, this structure has been effectively exploited to significantly reduce the prediction time. Our algorithm has shown better computation time during the prediction phase than the traditional sequential SVM methods (One vs. One, One vs. Rest) and out-performs them as the size of the dataset grows. This approach also classifies the data with higher accuracy than the traditional multi-class algorithms.", "histories": [["v1", "Mon, 7 Dec 2015 11:44:35 GMT  (812kb)", "http://arxiv.org/abs/1512.01993v1", "8 Pages"]], "COMMENTS": "8 Pages", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["aruna govada", "shree ranjani", "aditi viswanathan", "s k sahay"], "accepted": false, "id": "1512.01993"}, "pdf": {"name": "1512.01993.pdf", "metadata": {"source": "CRF", "title": "A Novel Approach to Distributed Multi-Class SVM", "authors": ["Aruna Govada", "Shree Ranjani", "Aditi Viswanathan"], "emails": ["garuna@goa.bits-pilani.ac.in"], "sections": [{"heading": "1. Introduction and Related Work", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "2. Proposed Framework", "text": "Our algorithm aims to reduce the total number of SVMs required for the classification of a data point, thus enabling better efficiency during the runtime of the model built from our algorithm. While One vs. One, One vs. Rest and DAGSVM classify using!!!!, n and!!!! SVMs, we use 2 (+ 1) SVMs to classify the data point during runtime. One possible structure that can be represented in Figure II. It is crucial to choose the most suitable combination to obtain the optimal case while testing a new sample data point. To this end, we have divided the training phase into two significant phases, in which the first phase (training) is dedicated to calculating all possible support vectors, and the second phase (cross validation) evaluates them all and returns the best division."}, {"heading": "2.1 Training", "text": "Given that N-classes divide the entire dataset into two halves, each containing [n / 2] classes that use support vectors, the difference between the classes is neglected on the one hand, with no loss of generality assigning one half as a positive class and the other as a negative class. To create support vectors, Atbrox's [12] method for parallel machine learning was used, which gave us the mapper and reduction implementation for binary classification. Atbrox's method implements incremental SVM algorithm for binary classification as described below: The SVM classifier solves the following problem, namely the coefficients of the support vector, which were formulated as Where I - Identity matrix \u00b5 - Parameters > 0 E = [A -e] D - Diagonal matrix with plus or minus ones to classify a test sample with characteristic vector."}, {"heading": "2.2 Cross Validation", "text": "This training phase primarily involves determining the best level from the possible options of the previous stage. A single pass of the second stage looks like this. \u2022 For each of the partitions thus obtained, the accuracy with which each level divides is calculated based on the classification accuracy metric (true positives + true negatives) / total samples. \u2022 Mappers divide the task of creating the confusion matrix (the matrix containing true positives, true negatives, false positives, false negatives). Reducers assimilate the values in the confusion matrix of each node and calculate the classification accuracy metrix. This metrix is used to determine the best division and store the two separate lists of classes for further calculation. \u2022 At the end of this second stage, we get a set of positive and negative classes together with the corresponding accuracy calculation. Both phases are repeated until the number of classes in the positive and one in the negative set becomes effective, which means that all classes have been divided into the effective N."}, {"heading": "2.3 Testing", "text": "The classification of a test pattern begins at the root of the tree. At each node of the binary tree, a decision is made on the assignment of the input pattern to one of the two possible groups, which are reached after the training phase. Each of these groups may contain several classes. This is repeated recursively downwards until the sample reaches a leaf node, which represents the class it has assigned (Figure II). Each test sample is learned by a maximum of two SVMs during the test phase.STEP 1: Training of SVMs with the training classes. - Output - 1 SVM is learned for each of the combinations, which is in (i).E.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I"}, {"heading": "3. Experiments and Results", "text": "For all of these experiments, we used a 3-node cluster to measure the metrics of our approach, and Python's Scikit Learn library for One vs. One and One vs. Rest)."}, {"heading": "3.1 Datasets used", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2 Accuracy (Figure IV.)", "text": "We have measured the accuracy of the algorithm using the following formula on the test samples I. =! \"#!\" # $%! \"# $% & 'Our approach provides better accuracy in all datasets except the SDSS dataset. SDSS is a distorted dataset, so accuracy is not the best performance measurement that can be used during cross-validation. We will need to use metrics other than accuracy (such as precision, retrieval and F1 measurement) to find the best SVM dataset name SDSS [15] Iris [16] Mfeat [17] # Training samples 40000 150 1500 # Testing samples 10000 50 500 # Features 6 3 6 6 (mor) 47 (zer) 64 (kar) # Class 3 4 10Figure III: Datasets to be used 1 4 1,2,3,4,5 1,4 2,3,5 2,5 3 5 Figure II: This structure is created during the training phase."}, {"heading": "3.3 Training Time (Figure V.)", "text": "While the individual machine implementations are more efficient for the smaller data sets, we see in the SDSS data set that our training time is comparable to the individual machine implementations due to the large data size of the SDSS. We can therefore show that the distribution of the calculation becomes more advantageous as the data size increases."}, {"heading": "3.4 Testing Time (Figure VI.)", "text": "We show a significant reduction (53.7%) of the test time for the SVM dataset, a result of the distributed approach that works hand in hand with the decision tree-based algorithm. 4. Conclusion and future working time0 100 200 300 400 600 700 Iris Mfeat- \u2010 mor Mfeat- \u2010 zer Mfeat- \u2010 kar SDSS Figure V: Comparison of training time (seconds) One against one against one against one against one Our approach 0 50 100 150 200 250 Iris Mfeat- mor Mfeat- \u2010 zer Mfeat- \u2010 kar SDSS Figure VI: Comparison of training time (seconds) One against one against one. Our approach 0 20 40 60 80 100 120 Iris Mfeat- \u2010 mor Mfeat- \u2010 mor SDSS Figure IV: Comparison of accuracy (%) One against one Our approach."}], "references": [{"title": "Krebel Pairwise classification and support vector machines", "author": ["H.G. U"], "venue": "In B.Scholkopf,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Support Vector machines for 3-d object recognition IEEE Transactions On Pattern Analysis and Machine", "author": ["M. Pontil", "A.Verri"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Ussikavul Multiclass support vector machines using adaptive directed acyclic graph", "author": ["N.B.Kijsirikul"], "venue": "In Proceedings of International Joint Conference on Neural Networks (IJCNN", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "A fast Parallel Optimization for Training Support Vector Machine.", "author": ["J X Dong", "A Krzyzak", "C Y Suen"], "venue": "Proceedings of 3rd International Conference on Machine Learning and Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "A Parallel Mixture of SVMs for Very Large Scale Problems", "author": ["R. Collobert", "Y. Bengio", "S. Bengio"], "venue": "Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Distributed Parallel Support Vector Machines in Strongly Connected Networks", "author": ["Y Lu", "V Roychowdhury", "L. Vandenberghe"], "venue": "IEEE Transactions on Neural Networks 2008;", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "LIBSVM: a library for support vector machines", "author": ["C C Chang", "C J Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Krebel [1] showed that by this formulation, unclassifiable regions reduce, but still they remain.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "[3] proposed to use rules of a tennis tournament to solve unclassified regions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] proposed the same method and called it Adaptive Directed Acyclic Graph.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "A parallelization scheme was proposed where the kernel matrix is approximated by a block-diagonal approach [7].", "startOffset": 107, "endOffset": 110}, {"referenceID": 4, "context": "[9] proposed a new parallel SVM training and classification algorithm that each subset of a dataset is trained with SVM and then the classifiers are combined into a final single classifier function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[10] proposed a connected network based distributed support vector machine algorithm.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "With data sizes constantly expanding, and with classical machine learning algorithms that analyze such data requiring larger and larger amounts of computation time and storage space, the need to distribute computation and memory requirements among several computers has become apparent. Although substantial work has been done in developing distributed binary SVM algorithms and multi-class SVM algorithms individually, the field of multi-class distributed SVMs remains largely unexplored. This research proposes a novel algorithm that implements the Support Vector Machine over a multi-class dataset and is efficient in a distributed environment (here, Hadoop). The idea is to divide the dataset into half recursively and thus compute the optimal Support Vector Machine for this half during the training phase, much like a divide and conquer approach. While testing, this structure has been effectively exploited to significantly reduce the prediction time. Our algorithm has shown better computation time during the prediction phase than the traditional sequential SVM methods (One vs. One, One vs. Rest) and out-performs them as the size of the dataset grows. This approach also classifies the data with higher accuracy than the traditional multiclass algorithms.", "creator": "Word"}}}