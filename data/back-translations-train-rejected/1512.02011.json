{"id": "1512.02011", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies", "abstract": "Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity. Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN). When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments. We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.", "histories": [["v1", "Mon, 7 Dec 2015 12:25:18 GMT  (954kb,D)", "http://arxiv.org/abs/1512.02011v1", "NIPS 2015 Deep Reinforcement Learning Workshop"], ["v2", "Wed, 20 Jan 2016 10:33:00 GMT  (959kb,D)", "http://arxiv.org/abs/1512.02011v2", "NIPS 2015 Deep Reinforcement Learning Workshop"]], "COMMENTS": "NIPS 2015 Deep Reinforcement Learning Workshop", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["vincent fran\\c{c}ois-lavet", "raphael fonteneau", "damien ernst"], "accepted": false, "id": "1512.02011"}, "pdf": {"name": "1512.02011.pdf", "metadata": {"source": "CRF", "title": "How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies", "authors": ["Vincent Fran\u00e7ois-Lavet", "Raphael Fonteneau"], "emails": ["v.francois@ulg.ac.be", "raphael.fonteneau@ulg.ac.be", "dernst@ulg.ac.be"], "sections": [{"heading": "1 Introduction", "text": "One of the most important challenges in designing learning algorithms is the fact that government space can be very large or continuous, which can lead to the fact that the state does not fully represent (action) value functions (for example, by using reference books). However, one of the main drawbacks of using (deep) neural networks as an approximator is that they become potentially unstable when using a functional approximation mechanism. [4] In this paper we will discuss the role of the discounter in terms of stability."}, {"heading": "2 Problem Formulation", "text": "We consider tasks in which an agent interacts with an environment to maximize the expected future reward Q = Q (s, a) = max \u03c0 E [rt, \u03b3rt + 1, \u03b32rt + 2 +... | st = s, at = a, \u03c0] (1), which is the maximum sum of rewards that r is subtracted in each time step t, attainable by a behavior stochastic time system \u03c0: S \u00b7 A \u2192 [0, 1], in which \u03c0 (s, a) denotes the probability that an action can be chosen in a political network. In this essay, the task is described by a time-invariant stochastic discrete time system, the dynamics of which can be described by the following equation: st + 1 = f (st, wt) (2), where st for all t, st is an element of the state space S, the action at is an element of the action space A, and the random disturbance wt is an element of the disturbance wt is generated W."}, {"heading": "3 Instabilities of the online neural fitted Q-learning", "text": "The Q-Learning rule from Equation 5 can be implemented directly online with the help of a neural network (as is the case for the Deep Q-Network). However, due to the generalization and extrapolation capabilities of neural networks, they can build up unpredictable changes at various points in the state sphere of action. It is known that errors are propagated and that this can even become unstable. Extra care must therefore be taken, as it cannot be guaranteed that the current estimate for the accumulated costs always underestimates the optimal costs, and therefore convergence is not secured [7, 8]. These convergence problems are being tested experimentally and it has been reported that convergence can be slow or even unreliable with this online update rule. We offer an analysis on the complexity of the political class [10] via a parallel with machine learning. High complexity machine learning techniques have the ability to represent their training well, but reduce the risk of overadaptation."}, {"heading": "4 Experiments", "text": "The deep Q-learning algorithm described in [1] serves as a benchmark. All hyperparameters remain the same unless otherwise specified, the most important change being the discount factor, which is increased in each epoch (250 000 steps) with the following formula: \u03b3k + 1 = 1 \u2212 0.98 (1 \u2212 \u03b3k) (6) The strategies learned are then evaluated for 125000 steps with a -greedy policy identical to the original benchmark (test = 0.05) The reported values are the highest average episode value of any simulation in which these evaluation phases were not cut off after 5 minutes. Each game is simulated five times for each configuration with different seeds and the results are reported in Figure 2. It can be observed that by simply using an increasing discount factor, learning is faster in four of the five games tested and similarly for the remaining game. We conclude that by starting with a low discount factor, we achieve less political instability thanks to faster improvements."}, {"heading": "4.1 Convergence of the neural network", "text": "We will now discuss the stability of DQN. We will use the experimental rule from Equation 6 and let \u03b3 either increase or remain constant when it reaches 0.99 (at 20M steps), as illustrated in Figure 3. It is shown that increasing \u03b3 without extra care greatly worsens the value that is reached beyond \u03b3 \u2248 0.99. Looking at the average V-value, one can see that the overestimation is particularly severe, which causes the bad policy."}, {"heading": "4.2 Further improvement with an adaptive learning rate", "text": "Since instabilities are severe only when the discounting factor is high, we are now investigating the possibility of using a more aggressive learning rate in the neural network with a low discounting factor, as potential errors would have less impact at this stage. We will then reduce the learning rate along with the increasing discounting factor in order to have a stable neural Q-learning function at the end. We will start with a learning rate of 0.005, which is twice as high as the one considered in the original benchmark, and use the following simple rule in each epoch: \u03b1k + 1 = 0.98\u03b1kWith \u03b3 following Equation 6 and being kept constant at 0.99, we will be able to further improve the score achieved in all the games tested. Results are shown in Figure 4 and a figure for two different games in Figure 5. It can be noted that the value function V decreases when \u03b3 is fixed and when the learning rate is lowered, which is a sign of a decrease in the value of the overestimation of the function Q-Q."}, {"heading": "4.3 Exploration / Exploitation Dilemma", "text": "Errors in the strategy can also have a positive effect, as it increases exploration [13]. Using a lower discount factor reduces exploration and opens up the risk of falling into a local optimum in repetition of value. This is observed because the agent repeatedly receives a value below the optimum while not being able to discover some parts of the state space. In this case, an algorithm of the type actor-critic that increases the degree of exploration could solve this problem. We believe that an agent who also controls the exploration level adaptively is important in order to further improve the learning algorithms of deep amplification. To illustrate this, a simple rule has been applied in the case of looking for the game, as shown in Figure 6. This rule adjusts the exploration during the training process to the -greedy action selection until the agent was able to get out of the local optimum."}, {"heading": "4.4 Towards an actor-critic algorithm", "text": "Following the ideas discussed above, Figure 7 presents the general update scheme that we propose to further improve the performance of deep amplification learning algorithms."}, {"heading": "5 Conclusion", "text": "This paper introduced an approach to accelerate convergence and improve the quality of the learned Q function in deep enhancement learning algorithms. It works by adjusting the discount factor and learning rate on the way to convergence. We used the deep Q-learning algorithms for Atari 2600 computer games as a benchmark and our approach showed improved performance for the 6 games tested. These results motivate further experiments, in particular it would be interesting to develop an automatic way to adjust the discount factor along with the learning rate and possibly the level of exploration online. It would also be interesting to combine this approach with the recent advances in deep enhancement learning: the massively parallel architecture \"Gorilla\" [14], the double Q-learning algorithm [12], the prioritized experience replay [15], the dueling network architecture [16] or a recurring architecture such as [17]."}], "references": [{"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Introduction to reinforcement learning", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Reinforcement learning and dynamic programming using function approximators, volume 39", "author": ["Lucian Busoniu", "Robert Babuska", "Bart De Schutter", "Damien Ernst"], "venue": "CRC press,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["Leemon Baird"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Cognitive and attentional mechanisms in delay of gratification", "author": ["Walter Mischel", "Ebbe B Ebbesen", "Antonette Raskoff Zeiss"], "venue": "Journal of personality and social psychology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1972}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["John N Tsitsiklis", "Benjamin Van Roy"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Approximate solutions to markov decision processes", "author": ["Geoffrey J Gordon"], "venue": "Robotics Institute,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Neural fitted Q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["Martin Riedmiller"], "venue": "In Machine Learning: ECML", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "The dependence of effective planning horizon on model accuracy", "author": ["Nan Jiang", "Alex Kulesza", "Satinder Singh", "Richard Lewis"], "venue": "In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A sparse sampling algorithm for nearoptimal planning in large markov decision processes", "author": ["Michael Kearns", "Yishay Mansour", "Andrew Y Ng"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["Hado Van Hasselt", "Arthur Guez", "Silver David"], "venue": "arXiv preprint arXiv:1509.06461,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen"], "venue": "arXiv preprint arXiv:1507.04296,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Nando de Freitas", "Marc Lanctot"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Deep recurrent Q-learning for partially observable MDPs", "author": ["Matthew Hausknecht", "Peter Stone"], "venue": "arXiv preprint arXiv:1507.06527,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity such as [1].", "startOffset": 189, "endOffset": 192}, {"referenceID": 1, "context": "Reinforcement learning is a learning paradigm aiming at learning optimal behaviors while interacting within an environment [2].", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": "One of the main challenge met when designing reinforcement learning algorithms is the fact that the state space may be very large or continuous, potentially leading to the fact that state(-action) value functions may not be represented comprehensively (for instance, using lookup tables) [3].", "startOffset": 288, "endOffset": 291}, {"referenceID": 0, "context": "Recently, it has been empirically demonstrated that using deep neural networks as a function approximator may be very powerful in situations with high-dimensional sensory inputs [1].", "startOffset": 178, "endOffset": 181}, {"referenceID": 3, "context": "However, one of the main drawback of using (deep) neural networks as function approximator is that they may potentially become unstable when combined with a Q-Learning-type recursion [4].", "startOffset": 183, "endOffset": 186}, {"referenceID": 4, "context": "One well known experiment in the domain was a series of studies in which a child was offered a choice between one small reward provided immediately or two small rewards if they waited for a short period (\"marshmallow experiment\" [5]).", "startOffset": 229, "endOffset": 232}, {"referenceID": 0, "context": "The remaining of the paper is organized the following: we first recall the main equations used in the deep reinforcement learning problem formulation originally introduced in [1].", "startOffset": 175, "endOffset": 178}, {"referenceID": 0, "context": "To illustrate our approach, we use the benchmark proposed in [1].", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "which is the maximum sum of rewards r discounted by \u03b3 at each time-step t, achievable by a behaviour stochastic policy \u03c0 : S \u00d7 A \u2192 [0, 1] where \u03c0(s, a) denotes the probability that action a may be chosen by policy \u03c0 in state s.", "startOffset": 131, "endOffset": 137}, {"referenceID": 5, "context": "Additional care must therefore be taken since it can not be guaranteed that the current estimation for the accumulated costs always underestimates the optimal cost, and therefore convergence is not assured [7, 8].", "startOffset": 206, "endOffset": 212}, {"referenceID": 6, "context": "Additional care must therefore be taken since it can not be guaranteed that the current estimation for the accumulated costs always underestimates the optimal cost, and therefore convergence is not assured [7, 8].", "startOffset": 206, "endOffset": 212}, {"referenceID": 7, "context": "These convergence problems are verified experimentally and it has been reported that convergence may be slow or even unreliable with this online update rule [9].", "startOffset": 157, "endOffset": 160}, {"referenceID": 8, "context": "We offer an analysis based on the complexity of the policy class [10] via a parallel with machine learning.", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "in the case where a small number of tuples is available) [10].", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "It is also well known that the longer the planning horizon, the greater the computational expense of computing an optimal policy [11].", "startOffset": 129, "endOffset": 133}, {"referenceID": 0, "context": "Some practical ways to prevent instabilities make use of a replay memory, clipping the error term, a separate target Q-network in Equation 5 and a convolutional network architecture [1].", "startOffset": 182, "endOffset": 185}, {"referenceID": 10, "context": "Using the double Q-learning algorithm also help reducing overestimations of Q-value function caused by the generalization exageration of the regression method [12].", "startOffset": 159, "endOffset": 163}, {"referenceID": 0, "context": "The deep Q-learning algorithm described in [1] is used as a benchmark.", "startOffset": 43, "endOffset": 46}, {"referenceID": 11, "context": "Errors in the policy may also have positive impacts since it increases exploration [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "It would also be of interest to combine this approach with the recent advances in deep reinforcement learning : the massively parallel architecture \"Gorilla\" [14],", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "the double Q-learning algorithm [12], the prioritized experience replay [15], the dueling network architecture [16] or a recurrent architecture such as [17].", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "the double Q-learning algorithm [12], the prioritized experience replay [15], the dueling network architecture [16] or a recurrent architecture such as [17].", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "the double Q-learning algorithm [12], the prioritized experience replay [15], the dueling network architecture [16] or a recurrent architecture such as [17].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "the double Q-learning algorithm [12], the prioritized experience replay [15], the dueling network architecture [16] or a recurrent architecture such as [17].", "startOffset": 152, "endOffset": 156}], "year": 2017, "abstractText": "Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity such as [1]. Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN). When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments. We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.", "creator": "LaTeX with hyperref package"}}}