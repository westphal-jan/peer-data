{"id": "1512.02033", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "Risk Minimization in Structured Prediction using Orbit Loss", "abstract": "We introduce a new surrogate loss function called orbit loss in the structured prediction framework, which has good theoretical and practical advantages. While the orbit loss is not convex, it has a simple analytical gradient and a simple perceptron-like learning rule. We analyze the new loss theoretically and state a PAC-Bayesian generalization bound. We also prove that the new loss is consistent in the strong sense; namely, the risk achieved by the set of the trained parameters approaches the infimum risk achievable by any linear decoder over the given features. Methods that are aimed at risk minimization, such as the structured ramp loss, the structured probit loss and the direct loss minimization require at least two inference operations per training iteration. In this sense, the orbit loss is more efficient as it requires only one inference operation per training iteration, while yields similar performance. We conclude the paper with an empirical comparison of the proposed loss function to the structured hinge loss, the structured ramp loss, the structured probit loss and the direct loss minimization method on several benchmark datasets and tasks.", "histories": [["v1", "Mon, 7 Dec 2015 13:30:27 GMT  (23kb)", "http://arxiv.org/abs/1512.02033v1", null], ["v2", "Wed, 9 Dec 2015 09:59:56 GMT  (23kb)", "http://arxiv.org/abs/1512.02033v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["danny karmon", "joseph keshet"], "accepted": false, "id": "1512.02033"}, "pdf": {"name": "1512.02033.pdf", "metadata": {"source": "CRF", "title": "Risk Minimization in Structured Prediction using Orbit Loss", "authors": ["Danny Karmon"], "emails": ["danny.karmon@biu.ac.il", "joseph.keshet@biu.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.02 033v 1 [cs.L G"}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Formal Settings", "text": "We formulate the structured supervised learning problem by defining X as an abstract set of all possible input objects and Y as an abstract set of all possible output targets. We assume that the input objects x-X and the target labels y-Y originate from an unknown common distribution. We define a set of d fixed mappings \u03c6: X-Y-Rd called feature functions from the set of input objects and target labels to a real vector of length. Here, we consider a linear decoder with parameters w-Rd so that the parameters evaluate the feature functions. We designate the value of the label w-Rd by w-Rd (x-y), where the decoder predicts the label w-w with the highest score: y-w (x) = Argmax y-Gate minus Yw-Rd (x, y) (1).Ideally, we would like to find the parameters w that present the risk for invisible data."}, {"heading": "3 Orbit Loss", "text": "The difference between the property functions of the labels y, y \"and\" Y \"(respectively:\" p \"and\" p \") (\" p \"and\" p \") (\" p \"and\" p \") (\" p \"and\" p \") (\" p \"and\" p \") (\" p \"and\" p \") (\" p \"and\" p \") (\" p \"and\" p \") (\" p \"and\" p \") (\" p. \"(\" p \"and\" p \") (\" p \"and\" p. \") (\" p \"and\" p. \"),\" p \"and\" p. \"(\" p \"and\" p \") (\" p \"and\" p \") (\" p \"and\" p. \"p.\" p. \"and\" p. \"p.\" p. \"and\" p. \"p.\" and \"p.\" p. \"(\" p. \"and\" p. \"p.\" and \"p.\" p. \"and\" p. \"p.\" and \"p.\" p. \"(\" and \"p.\" p. \"and\" p. \"and\" p. \"and\" p. \"p.\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" (\"and\" p. \"and\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" p. \"and\" and \"p.\" and \"p.\" and \"p.\" p. \"and\" and \"p.\" and \"p.\" and \"p.\" p. \"and\" p. \"and\" p. \"and\" p. \"and\" p. \"and\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" and \"p.\" p. \"and\" p. \"and\" and \"p.\" and \"p.\" and \"and\" p. \"p.\" and \"and\" p. \"and\" p. \"and\" p. \""}, {"heading": "4 Analysis", "text": "In this section, we analyze the orbital loss. We deduce from this a generalization based on the PAC-Bayesian theory (where we begin by optimizing the orbital loss with the orbital loss and then transforming it into a PAC-Bayesian generalization). Then, we show that the structural \"empirical\" loss \"is defined as:\" probit \"(w, x, y) =\" empirical \"(0, I) [empirical\" empirical \"empirical\" empirical \"loss\" (w, y) = \"empirical\" empirical \"empirical,\" \"empirical\" empirical \"empirical\" empirical, \"\" empirical \"empirical\" empirical \"empirical\"), \"empirical\" empirical \"(\" empirical \"empirical\" empirical \"),\" empirical \"(\" empirical \"empirical\" empirical \"),\" empirical \"(\" empirical \"empirical\" empirical \"empirical\"), \"(\" empirical \"empirical\" empirical \"empirical\")."}, {"heading": "5 Experiments", "text": "We investigated the performance of orbit loss by conducting a series of experiments in different areas and tasks, and compared the results with other approaches aimed at minimizing risk, namely direct loss minimization (McAllester et al., 2010), structured ramp losses (Do et al., 2008), and structured probit losses (Keshet et al., 2011). As a reference, we present results for the structured perceptron, as we wanted to emphasize the empirical differences between the updating rule in (10) and the rule in (11), as well as for structured hinges loss."}, {"heading": "5.1 MNIST", "text": "In our first experiment, we tested the Orbit Update Rule for a multiclass problem. MNIST is a set of handwritten numerical images (10 classes) divided into a training set of 50,000 examples, a test set of 10,000 examples and a validation set of 10,000 examples. We preprocess the data by normalizing the input images and increasing the dimension from the original 784 attributes to 100 with PCA. We used the Orbit Update Rule as in (9). We defined the weight vector w as a concatenation of 10 weight vectors w = (w0, w1,.), each corresponding to one of the 10 digits. The updating rule of the example (xi, yi), yi, yi, yi, 0,. 9} can be simplified on Kesler's design (Crammer and Singer, 2001) as follows."}, {"heading": "5.2 Phoneme alignment", "text": "Our next experiment focused on phoneme alignment, which is used as a tool in the development of speech recognition and text-to-speech systems. This is a structured prediction task - the input x represents a speech utterance and consists of a pair of x = (s, p) of a sequence of acoustic characteristic vectors (signal frequency receiver coefficients), s = (s1,., sT), where st-Rd, 1 \u2264 t \u2264 T; and a sequence of phonemes p = (p1,., pK), where pk-P, 1 \u2264 k-K is a phoneme symbol and P is a finite series of phoneme symbols. The lengths K and T may differ for different inputs, although typically T is significantly greater than K. The goal is to generate an equation between the two sequences in the input."}, {"heading": "5.3 Vowel duration", "text": "When measuring vowel duration, we are provided with a speech signal that contains exactly one vowel preceded and followed by consonants (i.e. CVC).Our goal is to accurately predict the vowel duration. An accurate measurement of vowel duration in a given context is required in many phonological experiments and is currently being performed manually (Heller and Goldrick, 2014).The speech signal is presented as a sequence of acoustic characteristics x = (x1, x2,., xT), with each xi (1 \u2264 i \u2264 T) being a d-dimensional vector representing acoustic parameters such as high and low energy, pitch, correlation coefficient, and so on (we extract d = 22 acoustic characteristics every 5 msec).We designate the domain of feature vectors by X-Rd. The length of the input signal varies from one signal to another, so T is not fixed."}, {"heading": "6 Discussion and Future Work", "text": "We introduced a new surrogate loss function that provides an efficient and effective learning rule. We gave a qualitative theoretical analysis using a PACBayean generalization limit and a consistency theorem. Despite the fact that the consistency feature affects training performance when the number of training examples is large, it has been shown that the proposed loss function works well for several tasks, even if the training set was of small or medium size. In terms of theoretical properties, we think that the theoretical analysis can be improved, and in particular we would like to have a better upper limit of the probital loss in terms of orbit loss, as expressed in Lemma 2, which depends on the minimum distance between the predicted label and its nearest neighbor. In any case, it is clear that when the norm of the weight vector becomes large in relation to the norm of noise, the conclusion with the weight vector and the conclusion with the disturbed weight vector we believe the probability that the loss is expressed in the same part of our research work - both the loss is high."}], "references": [{"title": "Automatic segmentation and labeling of speech based on hidden markov models", "author": ["F. Brugnara", "D. Falavigna", "M. Omologo"], "venue": "Speech Communication,", "citeRegEx": "Brugnara et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brugnara et al\\.", "year": 1993}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Collins,? \\Q2002\\E", "shortCiteRegEx": "Collins", "year": 2002}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Jornal of Machine Learning Research,", "citeRegEx": "Crammer and Singer,? \\Q2001\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2001}, {"title": "Tighter bounds for structured estimation", "author": ["C. Do", "Q. Le", "Teo", "C.-H", "O. Chapelle", "A. Smola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Do et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Do et al\\.", "year": 2008}, {"title": "Grammatical constraints on phonological encoding in speech production", "author": ["J.R. Heller", "M. Goldrick"], "venue": "Psychonomic bulletin & review,", "citeRegEx": "Heller and Goldrick,? \\Q2014\\E", "shortCiteRegEx": "Heller and Goldrick", "year": 2014}, {"title": "Speaker-independent phoneme alignment using transitiondependent states", "author": ["Hosom", "J.-P"], "venue": "Speech Communication,", "citeRegEx": "Hosom and J..P.,? \\Q2009\\E", "shortCiteRegEx": "Hosom and J..P.", "year": 2009}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Joachims", "T. Tsochantaridis", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Joachims et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Joachims et al\\.", "year": 2005}, {"title": "PAC-Bayesian approach for minimization of phoneme error rate", "author": ["J. Keshet", "D. McAllester", "T. Hazan"], "venue": "In International Conference on Acoustics, Speech, and Signal Processing (ICASSP)", "citeRegEx": "Keshet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Keshet et al\\.", "year": 2011}, {"title": "A large margin algorithm for speech and audio segmentation", "author": ["J. Keshet", "S. Shalev-Shwartz", "Y. Singer", "D. Chazan"], "venue": "IEEE Trans. on Audio, Speech and Language Processing,", "citeRegEx": "Keshet et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Keshet et al\\.", "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Proceedings of the Eightneenth International Conference on Machine Learning (ICML),", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Simplified PAC-Bayesian margin bounds", "author": ["D. McAllester"], "venue": "In Proceedings of the Sixteenth Annual Conference on Computational Learning Theory", "citeRegEx": "McAllester,? \\Q2003\\E", "shortCiteRegEx": "McAllester", "year": 2003}, {"title": "Generalization bounds and consistency for structured labeling", "author": ["D. McAllester"], "venue": "Predicting Structured Data,", "citeRegEx": "McAllester,? \\Q2006\\E", "shortCiteRegEx": "McAllester", "year": 2006}, {"title": "Direct loss minimization for structured prediction", "author": ["D. McAllester", "T. Hazan", "J. Keshet"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "McAllester et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McAllester et al\\.", "year": 2010}, {"title": "Some pac-bayesian theorems", "author": ["D.A. McAllester"], "venue": "In Proceedings of the Eleventh Annual Conference on Computational Learning Theory", "citeRegEx": "McAllester,? \\Q1998\\E", "shortCiteRegEx": "McAllester", "year": 1998}, {"title": "Optimizing nondecomposable loss functions in structured prediction", "author": ["M. Ranjbar", "T. Lan", "Y. Wang", "S.N. Robinovitch", "Li", "Z.-N", "G. Mori"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Ranjbar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ranjbar et al\\.", "year": 2013}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Minimum risk annealing for training loglinear models", "author": ["D.A. Smith", "J. Eisner"], "venue": "In Proc. of the COLING/ACL,", "citeRegEx": "Smith and Eisner,? \\Q2006\\E", "shortCiteRegEx": "Smith and Eisner", "year": 2006}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Taskar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2003}, {"title": "Regularized structured perceptron: A case study on chinese word segmentation, pos tagging and parsing. The 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL), page 164", "author": ["K. Zhang", "P. Fujian", "J. Su", "C. Zhou"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "The structured perceptron (Collins, 2002) solves a feasibility problem, which is independent of the cost.", "startOffset": 26, "endOffset": 41}, {"referenceID": 6, "context": "In structural SVM (Joachims et al., 2005) the measure of goodness is a convex upper bound to the cost called structural hinge loss.", "startOffset": 18, "endOffset": 41}, {"referenceID": 17, "context": "While there exists generalization bounds for the structured hinge loss (e.g., McAllester, 2006; Taskar et al., 2003), they all include terms which are not directly related to the cost, such as the Hamming loss, and inherently the structured hinge loss cannot be consistent as it fails to converge to the performance of the optimal linear predictor in the limit of infinite training data (McAllester, 2006).", "startOffset": 71, "endOffset": 116}, {"referenceID": 11, "context": ", 2003), they all include terms which are not directly related to the cost, such as the Hamming loss, and inherently the structured hinge loss cannot be consistent as it fails to converge to the performance of the optimal linear predictor in the limit of infinite training data (McAllester, 2006).", "startOffset": 278, "endOffset": 296}, {"referenceID": 9, "context": "In CRFs the measure of goodness is the log loss function, which is independent of the cost (Lafferty et al., 2001).", "startOffset": 91, "endOffset": 114}, {"referenceID": 1, "context": "The structured perceptron (Collins, 2002) solves a feasibility problem, which is independent of the cost. In structural SVM (Joachims et al., 2005) the measure of goodness is a convex upper bound to the cost called structural hinge loss. It is based on a generalization of the binary SVM hinge loss to the structured case, and there is no guarantee for the risk. While there exists generalization bounds for the structured hinge loss (e.g., McAllester, 2006; Taskar et al., 2003), they all include terms which are not directly related to the cost, such as the Hamming loss, and inherently the structured hinge loss cannot be consistent as it fails to converge to the performance of the optimal linear predictor in the limit of infinite training data (McAllester, 2006). In CRFs the measure of goodness is the log loss function, which is independent of the cost (Lafferty et al., 2001). Smith and Eisner (2006) tried to address this shortcoming of CRFs and proposed to minimize the risk under the Gibbs measure.", "startOffset": 27, "endOffset": 910}, {"referenceID": 1, "context": "The structured perceptron (Collins, 2002) solves a feasibility problem, which is independent of the cost. In structural SVM (Joachims et al., 2005) the measure of goodness is a convex upper bound to the cost called structural hinge loss. It is based on a generalization of the binary SVM hinge loss to the structured case, and there is no guarantee for the risk. While there exists generalization bounds for the structured hinge loss (e.g., McAllester, 2006; Taskar et al., 2003), they all include terms which are not directly related to the cost, such as the Hamming loss, and inherently the structured hinge loss cannot be consistent as it fails to converge to the performance of the optimal linear predictor in the limit of infinite training data (McAllester, 2006). In CRFs the measure of goodness is the log loss function, which is independent of the cost (Lafferty et al., 2001). Smith and Eisner (2006) tried to address this shortcoming of CRFs and proposed to minimize the risk under the Gibbs measure. While it seems that this loss function is consistent, we are not aware of any formal analysis. Recently, several works have focused on directly minimizing the expected cost. In particular, McAllester et al. (2010) presented a theorem stating that a certain perceptron-like learning rule, involving feature vectors derived from costaugmented inference, directly corresponds to the gradient of the risk.", "startOffset": 27, "endOffset": 1225}, {"referenceID": 1, "context": "The structured perceptron (Collins, 2002) solves a feasibility problem, which is independent of the cost. In structural SVM (Joachims et al., 2005) the measure of goodness is a convex upper bound to the cost called structural hinge loss. It is based on a generalization of the binary SVM hinge loss to the structured case, and there is no guarantee for the risk. While there exists generalization bounds for the structured hinge loss (e.g., McAllester, 2006; Taskar et al., 2003), they all include terms which are not directly related to the cost, such as the Hamming loss, and inherently the structured hinge loss cannot be consistent as it fails to converge to the performance of the optimal linear predictor in the limit of infinite training data (McAllester, 2006). In CRFs the measure of goodness is the log loss function, which is independent of the cost (Lafferty et al., 2001). Smith and Eisner (2006) tried to address this shortcoming of CRFs and proposed to minimize the risk under the Gibbs measure. While it seems that this loss function is consistent, we are not aware of any formal analysis. Recently, several works have focused on directly minimizing the expected cost. In particular, McAllester et al. (2010) presented a theorem stating that a certain perceptron-like learning rule, involving feature vectors derived from costaugmented inference, directly corresponds to the gradient of the risk. Direct loss needs two inference operations per training iteration and is extremely sensitive to its hyper-parameter. Do et al. (2008) generalized the notion of the ramp loss from binary classification to structured prediction and proposed a loss function, which is a non-convex bound to the cost, and was found to be a tighter bound than the structured hinge loss function.", "startOffset": 27, "endOffset": 1547}, {"referenceID": 1, "context": "The structured perceptron (Collins, 2002) solves a feasibility problem, which is independent of the cost. In structural SVM (Joachims et al., 2005) the measure of goodness is a convex upper bound to the cost called structural hinge loss. It is based on a generalization of the binary SVM hinge loss to the structured case, and there is no guarantee for the risk. While there exists generalization bounds for the structured hinge loss (e.g., McAllester, 2006; Taskar et al., 2003), they all include terms which are not directly related to the cost, such as the Hamming loss, and inherently the structured hinge loss cannot be consistent as it fails to converge to the performance of the optimal linear predictor in the limit of infinite training data (McAllester, 2006). In CRFs the measure of goodness is the log loss function, which is independent of the cost (Lafferty et al., 2001). Smith and Eisner (2006) tried to address this shortcoming of CRFs and proposed to minimize the risk under the Gibbs measure. While it seems that this loss function is consistent, we are not aware of any formal analysis. Recently, several works have focused on directly minimizing the expected cost. In particular, McAllester et al. (2010) presented a theorem stating that a certain perceptron-like learning rule, involving feature vectors derived from costaugmented inference, directly corresponds to the gradient of the risk. Direct loss needs two inference operations per training iteration and is extremely sensitive to its hyper-parameter. Do et al. (2008) generalized the notion of the ramp loss from binary classification to structured prediction and proposed a loss function, which is a non-convex bound to the cost, and was found to be a tighter bound than the structured hinge loss function. The structured ramp loss also needs two inference operations per training iteration. Keshet et al. (2011) generalized the notion of the binary probit loss to the structured prediction case.", "startOffset": 27, "endOffset": 1893}, {"referenceID": 1, "context": "Under this condition the update rule becomes w \u2190 (1\u2212 \u03b7\u03bb)w + \u03b7 l(y, \u0177) \u03b4\u03c6(y, \u0177w), (10) which generalizes the regularized structured perceptron\u2019s update rule (Collins, 2002; Zhang et al., 2014).", "startOffset": 156, "endOffset": 191}, {"referenceID": 18, "context": "Under this condition the update rule becomes w \u2190 (1\u2212 \u03b7\u03bb)w + \u03b7 l(y, \u0177) \u03b4\u03c6(y, \u0177w), (10) which generalizes the regularized structured perceptron\u2019s update rule (Collins, 2002; Zhang et al., 2014).", "startOffset": 156, "endOffset": 191}, {"referenceID": 6, "context": "Decomposable cost functions are needed in order to solve the cost-augmented inference that is used in the training of structural SVMs (Joachims et al., 2005; Ranjbar et al., 2013), direct loss minimization (McAllester et al.", "startOffset": 134, "endOffset": 179}, {"referenceID": 14, "context": "Decomposable cost functions are needed in order to solve the cost-augmented inference that is used in the training of structural SVMs (Joachims et al., 2005; Ranjbar et al., 2013), direct loss minimization (McAllester et al.", "startOffset": 134, "endOffset": 179}, {"referenceID": 12, "context": ", 2013), direct loss minimization (McAllester et al., 2010), or structured ramp loss (Do et al.", "startOffset": 34, "endOffset": 59}, {"referenceID": 3, "context": ", 2010), or structured ramp loss (Do et al., 2008).", "startOffset": 33, "endOffset": 50}, {"referenceID": 7, "context": "Another property of the orbit loss is its similarity to the structured probit loss (Keshet et al., 2011).", "startOffset": 83, "endOffset": 104}, {"referenceID": 7, "context": "The probit loss was derived from the concept of stochastic decoder in the PAC-Bayesian framework (McAllester, 2003, 1998) and was shown to have both good theoretical properties and practical advantages (Keshet et al., 2011).", "startOffset": 202, "endOffset": 223}, {"referenceID": 7, "context": "(16) The following theorem states a generalization bound for the probit loss function (Keshet et al., 2011).", "startOffset": 86, "endOffset": 107}, {"referenceID": 12, "context": "5 Experiments We evaluated the performance of the orbit loss by executing a number of experiments on several domains and tasks and compared the results with other approaches that are aimed at risk minimization, namely direct loss minimization (McAllester et al., 2010), structured ramp loss (Do et al.", "startOffset": 243, "endOffset": 268}, {"referenceID": 3, "context": ", 2010), structured ramp loss (Do et al., 2008), and structured probit loss (Keshet et al.", "startOffset": 30, "endOffset": 47}, {"referenceID": 7, "context": ", 2008), and structured probit loss (Keshet et al., 2011).", "startOffset": 36, "endOffset": 57}, {"referenceID": 2, "context": ", 9} can be simplified based on Kesler\u2019s construction (Crammer and Singer, 2001) as follows: wi \u2190 (1\u2212 \u03b7\u03bb)wi + \u03b7 e\u2212|wy \u00b7xi\u2212w\u00b7xi|/2 l(\u0177, yi) xi w \u2190 (1\u2212 \u03b7\u03bb)w \u2212 \u03b7 e\u2212|wy \u00b7xi\u2212w\u00b7xi|/2 l(\u0177, yi)xi w \u2190 (1\u2212 \u03b7\u03bb)w for all r 6= yi, \u0177 Note that the exponent values throughout the training were very close to 1 and, practically, the update rule (10) could be used.", "startOffset": 54, "endOffset": 80}, {"referenceID": 2, "context": "01 (Crammer and Singer, 2001).", "startOffset": 3, "endOffset": 29}, {"referenceID": 0, "context": "\u03c4 -alignment accuracy [%] \u03c4 -insensitive t \u2264 10ms t \u2264 20ms t \u2264 30ms t \u2264 40ms loss Brugnara et al. (1993)* 79.", "startOffset": 82, "endOffset": 105}, {"referenceID": 0, "context": "\u03c4 -alignment accuracy [%] \u03c4 -insensitive t \u2264 10ms t \u2264 20ms t \u2264 30ms t \u2264 40ms loss Brugnara et al. (1993)* 79.7 92.1 96.2 98.1 Keshet et al. (2007)* 75.", "startOffset": 82, "endOffset": 147}, {"referenceID": 0, "context": "For this task we used the TIMIT speech corpus for which there are published benchmark results (Brugnara et al., 1993; Hosom, 2009; Keshet et al., 2007).", "startOffset": 94, "endOffset": 151}, {"referenceID": 8, "context": "For this task we used the TIMIT speech corpus for which there are published benchmark results (Brugnara et al., 1993; Hosom, 2009; Keshet et al., 2007).", "startOffset": 94, "endOffset": 151}, {"referenceID": 0, "context": "For this task we used the TIMIT speech corpus for which there are published benchmark results (Brugnara et al., 1993; Hosom, 2009; Keshet et al., 2007). We divided a portion of the TIMIT corpus (excluding the SA1 and SA2 utterances) into three disjoint parts containing 1500, 1796 and 400 utterances, respectively. The first part was used to train a phoneme frame-based classifier, which given the pair of speech frame and a phoneme, returns the level of confidence that the phoneme was uttered in that frame. The output classifier is then used along with other features as a seven dimensional feature map \u03c6(x,y) = \u03c6((s,p),y) as described in Keshet et al. (2007). The seven dimensional weight vector w was trained on the second set of 150 aligned utterances for \u03c4 -insensitive loss", "startOffset": 95, "endOffset": 663}, {"referenceID": 15, "context": "2; the structured perceptron update rule; the structural SVM optimized using stochastic gradient descent with C=5 (Shalev-Shwartz et al., 2011); structured ramp-loss with \u03b7 = 1.", "startOffset": 114, "endOffset": 143}, {"referenceID": 4, "context": "Precise measurement of vowel duration in a given context is needed in many phonological experiments, and currently is done manually (Heller and Goldrick, 2014).", "startOffset": 132, "endOffset": 159}], "year": 2017, "abstractText": "We introduce a new surrogate loss function called orbit loss in the structured prediction framework, which has good theoretical and practical advantages. While the orbit loss is not convex, it has a simple analytical gradient and a simple perceptron-like learning rule. We analyze the new loss theoretically and state a PAC-Bayesian generalization bound. We also prove that the new loss is consistent in the strong sense; namely, the risk achieved by the set of the trained parameters approaches the infimum risk achievable by any linear decoder over the given features. Methods that are aimed at risk minimization, such as the structured ramp loss, the structured probit loss and the direct loss minimization require at least two inference operations per training iteration. In this sense, the orbit loss is more efficient as it requires only one inference operation per training iteration, while yields similar performance. We conclude the paper with an empirical comparison of the proposed loss function to the structured hinge loss, the structured ramp loss, the structured probit loss and the direct loss minimization method on several benchmark datasets and tasks.", "creator": "LaTeX with hyperref package"}}}