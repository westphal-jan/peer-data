{"id": "1512.02167", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "Simple Baseline for Visual Question Answering", "abstract": "We describe a very simple bag-of-words baseline for visual question answering. This baseline concatenates the word features from the question and CNN features from the image to predict the answer. When evaluated on the challenging VQA dataset [2], it shows comparable performance to many recent approaches using recurrent neural networks. To explore the strength and weakness of the trained model, we also provide an interactive web demo and open-source code. .", "histories": [["v1", "Mon, 7 Dec 2015 19:00:54 GMT  (376kb,D)", "http://arxiv.org/abs/1512.02167v1", null], ["v2", "Tue, 15 Dec 2015 05:17:49 GMT  (523kb,D)", "http://arxiv.org/abs/1512.02167v2", "One comparison method's scores are put into the correct column, and a new experiment of generating attention map is added"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["bolei zhou", "yuandong tian", "sainbayar sukhbaatar", "arthur szlam", "rob fergus"], "accepted": false, "id": "1512.02167"}, "pdf": {"name": "1512.02167.pdf", "metadata": {"source": "CRF", "title": "Simple Baseline for Visual Question Answering", "authors": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "2 iBOWIMG for Visual Question Answering", "text": "In most of the newer models proposed, visual quality assurance is simplified into a classification task: the number of different answers in the training set is the number of last classes the models must learn to predict the response class. The general pipeline of these models is that the word feature extracted from the question set is associated with the visual feature extracted from the picture, and then fed into a Softmax level to predict the response class. Normally, the visual feature is taken from the top of the VGG network or GoogLeNet [14], while the word features of the question set are usually the popular LSTM-based features [12, 2]. In our iBOWIMG model, we simply use na\u00efve word stumps as a text feature and use the deep features from GoogLeNet [14] as visual features. Figure 1 shows the framework of the iBOWIMG model, which can be implemented in torches with no more than 10 lines of code."}, {"heading": "3 Experiments", "text": "Here we train and evaluate the iBOWIMG model based on the full release of the COCO VQA dataset [2], the largest VQA dataset to date. In the COCO VQA dataset, for each image of the COCO dataset, there are 3 questions commented on by Amazon Mechanical Turk (AMT) staff. For each question, there are 10 answers that are commented on by another group of AMT staff. To prepare the annotation for the training, we conduct majority votes on the 10 answers that correspond to the truth to get the safest answer for each question. Here, the answer may be in a single word or more words. Then, we have the 3 question answer pairs from each image for the training. There are a total of 248,349 pairs in the 2014 training and 121,512 pairs in the 2014 training, for a total of 123,287 images in the training set. Here, we will split the Train2014 and the standard parameters of the COCO image parameters for the 2014 training parameters to become our COCO data parameters in the training group."}, {"heading": "3.1 Benchmark Performance", "text": "In accordance with the evaluation standard of the VQA dataset, the result of the proposed VQA models should report on the accuracy of the test standard set for a fair comparison. We report on our baseline on the test standard set in Table 1 and the test standard set in Table 2. However, since this VQA dataset is relatively new, the publicly available models evaluated on the dataset are all from non-reviewed arXiv papers. We incorporate the performance of the models available at the time of writing (December 5, 2015) [2, 6, 1, 13, 16, 11]. Note that some models are evaluated either on a test standard or test standard basis (either on an open-ended or multiple-choice basis)."}, {"heading": "3.2 Training Details", "text": "The learning rate for the Word embedding layer should be much higher than the learning rate for the Softmax layer to learn good word embedding. BOW's performance in Table 1 indicates that a good word model is critical to accuracy, as BOW model alone could reach just under 48% without looking at the image contents. Model parameters for customization. Although our model could be considered the simplest starting point to date for visual QA, there are several model parameters for customization: 1) the number of eras to be trained. 2) the learning rate and the weight clip. 3) the threshold for removing rare question and answer classes. We iterate to search for the best value of each model parameter separately on the Val2014 subset B. In our best model, there are 5,746 words in the dictionary of the question set, 5,216 classes of the source code."}, {"heading": "3.3 Understanding the Visual QA model", "text": "From the comparisons above, we can see that our baseline model works as well as the recurring neural network models on the VQA dataset. Furthermore, thanks to the simplicity of the model, we are able to easily interpret its behavior in an attempt to see what it has learned for visual QA. Essentially, the BOWIMG baseline model learns to remember the correlation between the response class and the informative words in the questionnaire along with the visual characteristic. We divide the learned weights from Softmax into two parts, one part for the word characteristic and the other part for the visual characteristic, thusr = Mwxw + Mvxv. (1) Here, the Softmax matrix M breaks down into weights Mw for word characteristic xw and weights Mv for the visual characteristic xv, whereas M = [Mw, Mv] is the response of the pre-normalization response class."}, {"heading": "4 Interactive Visual QA Demo", "text": "Answering questions is essentially an interactive activity, so it would be good to enable the trained models to interact with people in real time. Using the simplicity of the basic model, we have built a web demo where people can type questions about a particular image, and our AI system with iBOWIMG support answers most possible answers. Here, the deeper features of the images are pre-extracted. Figure 3 shows a snapshot of the demo. You could play with the demo to see the strength and weakness of the VQA model."}, {"heading": "5 Concluding Remarks", "text": "For visual answers to questions in the COCO dataset, our implementation of a simple baseline achieves comparable performance with several recently proposed recurrent neural networking approaches. To achieve accurate prediction, the baseline captures the correlation between the informative words in the question and answer, as well as between image content and the answer. How to go beyond that, from memorizing the correlations to actual reasoning and understanding the question and image, is a goal of future research."}], "references": [{"title": "Deep compositional question answering with neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "arXiv preprint arXiv:1511.02799,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "arXiv preprint arXiv:1505.00468,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Abc-cnn: An attention based convolutional neural network for visual question answering", "author": ["K. Chen", "J. Wang", "L.-C. Chen", "H. Gao", "W. Xu", "R. Nevatia"], "venue": "arXiv preprint arXiv:1511.05960,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["J. Devlin", "S. Gupta", "R. Girshick", "M. Mitchell", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1505.04467,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "arXiv preprint arXiv:1505.05612,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Compositional memory for visual question answering", "author": ["A. Jiang", "F. Wang", "F. Porikli", "Y. Li"], "venue": "arXiv preprint arXiv:1511.05676,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. Yuille"], "venue": "arXiv preprint arXiv:1412.6632,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["H. Noh", "P.H. Seo", "B. Han"], "venue": "arXiv preprint arXiv:1511.05756,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "arXiv preprint arXiv:1511.07394,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Ask me anything: Freeform visual question answering based on knowledge from external sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "A. v. d. Hengel", "A. Dick"], "venue": "arXiv preprint arXiv:1511.06973,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["H. Xu", "K. Saenko"], "venue": "arXiv preprint arXiv:1511.05234,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "arXiv preprint arXiv:1511.02274,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "When evaluated on the challenging VQA dataset [2], it shows comparable performance to many recent approaches using recurrent neural networks.", "startOffset": 46, "endOffset": 49}, {"referenceID": 9, "context": ", image captioning [10, 15, 7, 4].", "startOffset": 19, "endOffset": 33}, {"referenceID": 14, "context": ", image captioning [10, 15, 7, 4].", "startOffset": 19, "endOffset": 33}, {"referenceID": 6, "context": ", image captioning [10, 15, 7, 4].", "startOffset": 19, "endOffset": 33}, {"referenceID": 3, "context": ", image captioning [10, 15, 7, 4].", "startOffset": 19, "endOffset": 33}, {"referenceID": 7, "context": "These works have benefited from the rapid development of deep learning for visual recognition (object recognition [8] and scene recognition [19]), and have been made possible by the emergence of large image datasets and text corpus (e.", "startOffset": 114, "endOffset": 117}, {"referenceID": 18, "context": "These works have benefited from the rapid development of deep learning for visual recognition (object recognition [8] and scene recognition [19]), and have been made possible by the emergence of large image datasets and text corpus (e.", "startOffset": 140, "endOffset": 144}, {"referenceID": 8, "context": ", [9]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 11, "context": "Beyond image captioning, a natural next step is visual question answering (QA) [12, 2, 5].", "startOffset": 79, "endOffset": 89}, {"referenceID": 1, "context": "Beyond image captioning, a natural next step is visual question answering (QA) [12, 2, 5].", "startOffset": 79, "endOffset": 89}, {"referenceID": 4, "context": "Beyond image captioning, a natural next step is visual question answering (QA) [12, 2, 5].", "startOffset": 79, "endOffset": 89}, {"referenceID": 12, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 16, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 4, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 17, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 15, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 2, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 10, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 0, "context": "Recently, several papers have appeared on arXiv (after CVPR\u201916 submission deadline) proposing neural network architectures for visual question answering, such as [13, 17, 5, 18, 16, 3, 11, 1].", "startOffset": 162, "endOffset": 191}, {"referenceID": 15, "context": ", LSTM [16, 11, 1]) applied to the question sentence is concatenated with visual features from VGG or other CNNs to feed a classifier to predict the answer.", "startOffset": 7, "endOffset": 18}, {"referenceID": 10, "context": ", LSTM [16, 11, 1]) applied to the question sentence is concatenated with visual features from VGG or other CNNs to feed a classifier to predict the answer.", "startOffset": 7, "endOffset": 18}, {"referenceID": 0, "context": ", LSTM [16, 11, 1]) applied to the question sentence is concatenated with visual features from VGG or other CNNs to feed a classifier to predict the answer.", "startOffset": 7, "endOffset": 18}, {"referenceID": 16, "context": "Other models integrate visual attention mechanisms [17, 13, 3] and visualize how the network learns to attend the local image regions relevant to the content of the question.", "startOffset": 51, "endOffset": 62}, {"referenceID": 12, "context": "Other models integrate visual attention mechanisms [17, 13, 3] and visualize how the network learns to attend the local image regions relevant to the content of the question.", "startOffset": 51, "endOffset": 62}, {"referenceID": 2, "context": "Other models integrate visual attention mechanisms [17, 13, 3] and visualize how the network learns to attend the local image regions relevant to the content of the question.", "startOffset": 51, "endOffset": 62}, {"referenceID": 11, "context": "Interestingly, we notice that in one of the earliest VQA papers [12], the simple baseline Bag-ofwords + image feature (referred to as BOWIMG baseline) outperforms the LSTM-based models on a synthesized visual QA dataset built up on top of the image captions of COCO dataset [9].", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "Interestingly, we notice that in one of the earliest VQA papers [12], the simple baseline Bag-ofwords + image feature (referred to as BOWIMG baseline) outperforms the LSTM-based models on a synthesized visual QA dataset built up on top of the image captions of COCO dataset [9].", "startOffset": 274, "endOffset": 277}, {"referenceID": 1, "context": "For the recent much larger COCO VQA dataset [2], the BOWIMG baseline performs worse than the LSTM-based models [2].", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "For the recent much larger COCO VQA dataset [2], the BOWIMG baseline performs worse than the LSTM-based models [2].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "We call it iBOWIMG to avoid confusion with the implementation in [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 11, "context": "The visual feature is usually taken from the top of the VGG network or GoogLeNet, while the word features of the question sentence are usually the popular LSTM-based features [12, 2].", "startOffset": 175, "endOffset": 182}, {"referenceID": 1, "context": "The visual feature is usually taken from the top of the VGG network or GoogLeNet, while the word features of the question sentence are usually the popular LSTM-based features [12, 2].", "startOffset": 175, "endOffset": 182}, {"referenceID": 13, "context": "In our iBOWIMG model, we simply use naive bag-of-words as the text feature, and use the deep features from GoogLeNet [14] as the visual features.", "startOffset": 117, "endOffset": 121}, {"referenceID": 1, "context": "Here we train and evaluate the iBOWIMG model on the Full release of COCO VQA dataset [2], the largest VQA dataset so far.", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "Open-Ended Overall yes/no number others IMG [2] 28.", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "77 BOW [2] 48.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "14 BOWIMG [2] 52.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "37 LSTMIMG [2] 53.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "42 CompMem [6] 52.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "46 NMN+LSTM [1] 54.", "startOffset": 12, "endOffset": 15}, {"referenceID": 12, "context": "[13] ACK [16] 55.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[13] ACK [16] 55.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "13 DPPnet [11] 57.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "Open-Ended Overall yes/no number others LSTMIMG [2] 54.", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "06 NMN+LSTM [1] 55.", "startOffset": 12, "endOffset": 15}, {"referenceID": 15, "context": "10 ACK [16] 55.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "10 DPPnet [11] 57.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "5, 2015) [2, 6, 1, 13, 16, 11].", "startOffset": 9, "endOffset": 30}, {"referenceID": 5, "context": "5, 2015) [2, 6, 1, 13, 16, 11].", "startOffset": 9, "endOffset": 30}, {"referenceID": 0, "context": "5, 2015) [2, 6, 1, 13, 16, 11].", "startOffset": 9, "endOffset": 30}, {"referenceID": 12, "context": "5, 2015) [2, 6, 1, 13, 16, 11].", "startOffset": 9, "endOffset": 30}, {"referenceID": 15, "context": "5, 2015) [2, 6, 1, 13, 16, 11].", "startOffset": 9, "endOffset": 30}, {"referenceID": 10, "context": "5, 2015) [2, 6, 1, 13, 16, 11].", "startOffset": 9, "endOffset": 30}, {"referenceID": 16, "context": "[17] (arXiv dated at Nov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "9 version of VQA with their own split of training and testing; [18] (arXiv dated at Nov.", "startOffset": 63, "endOffset": 67}, {"referenceID": 2, "context": "7 2015) used their own split of training and testing for the val2014; [3] (arXiv dated at Nov.", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "Except for these IMG, BOW, BOWIMG baselines provided in the [2], all the compared methods use either deep or recursive neural networks.", "startOffset": 60, "endOffset": 63}, {"referenceID": 10, "context": "However, our iBOWIMG baseline performs comparably to these much more complex models, except for DPPnet [11] that is about 1.", "startOffset": 103, "endOffset": 107}], "year": 2017, "abstractText": "We describe a very simple bag-of-words baseline for visual question answering. This baseline concatenates the word features from the question and CNN features from the image to predict the answer. When evaluated on the challenging VQA dataset [2], it shows comparable performance to many recent approaches using recurrent neural networks. To explore the strength and weakness of the trained model, we also provide an interactive web demo1, and open-source code2.", "creator": "LaTeX with hyperref package"}}}