{"id": "1512.02393", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2015", "title": "Online Crowdsourcing", "abstract": "With the success of modern internet based platform, such as Amazon Mechanical Turk, it is now normal to collect a large number of hand labeled samples from non-experts. The Dawid- Skene algorithm, which is based on Expectation- Maximization update, has been widely used for inferring the true labels from noisy crowdsourced labels. However, Dawid-Skene scheme requires all the data to perform each EM iteration, and can be infeasible for streaming data or large scale data. In this paper, we provide an online version of Dawid- Skene algorithm that only requires one data frame for each iteration. Further, we prove that under mild conditions, the online Dawid-Skene scheme with projection converges to a stationary point of the marginal log-likelihood of the observed data. Our experiments demonstrate that the online Dawid- Skene scheme achieves state of the art performance comparing with other methods based on the Dawid- Skene scheme.", "histories": [["v1", "Tue, 8 Dec 2015 10:35:29 GMT  (20kb)", "http://arxiv.org/abs/1512.02393v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changbo zhu", "huan xu", "shuicheng yan"], "accepted": false, "id": "1512.02393"}, "pdf": {"name": "1512.02393.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Changbo Zhu", "Huan Xu", "Shuicheng Yan"], "emails": ["eleyans}@nus.edu.sg"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.02 393v 1 [cs.L G] 8D ec2 01"}, {"heading": "1 Introduction", "text": "In fact, it is such that most people who are able to identify themselves, to identify themselves, to identify themselves, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify, to identify"}, {"heading": "2 Related work", "text": "In the existing literature, depending on different practical situations, there are many methods [Liu et al., 2012; Ghosh et al., 2011; Karger et al., 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved on the basis of the Dawid Skene algorithm [Dawid and Skene, 1979]. In particular, if we have only two classes, Ghosh et al. [2011] can suggest a method that turns Singular Value Decomposition into moderate online content based on crowdsourced user ratings. Based on the assumption that each worker labels all items, Ghosh et al. [2011] show that when the number of observations increases, their algorithm can deduce the quality of contributions with errors converging to zero. Dalvi et al. [2013] we relax the assumption that the graph between worker and item is either random or complete and suggests another SVD-based algorithm."}, {"heading": "3 Problem Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Notation", "text": "In this essay, we use [a] to denote the integer set {1, 2, \u00b7 \u00b7, a}. 3-mode tensors are denoted by calligraphic uppercase letters (e.g. C, S, Rm \u00b7 k \u00b7 k); matrices are denoted by uppercase letters (e.g. Z); vectors are denoted by lowercase letters (e.g. z); scalars are denoted by lowercase letters (e.g. z). In a 3-mode tensor C, Rm \u00b7 k \u00b7 k, we use cilg to denote the (i, l, g) -th input of C. In a matrix Z, we use zi to denote their i-th column, and zij to denote their (i, j) -th input. In a vector z, their i-th element is denoted by zi. Next, sentences are denoted by bold letters (e.g. R, D, O) and a certain projection, e.g. V, a variation, a function, a variation and a variation (E)."}, {"heading": "3.2 Estimate true labels form crowds", "text": "We use yj to denote the true labels of item j [n] and y to denote the true labels for all items. Let's use a scalar zij \u0435R to denote the label that assigns worker i item j. If the assigned label is g [k], we write zij = g. If item j is not labeled by worker i, we write zij = 0. Therefore, we use the vector zj to denote all labels of workers that correspond to the jten item. In addition, we use the matrix Z to denote all labels whose j-th column is zj and (i, j) -th entry zij. Our goal is to estimate the true labels y from workers labels Z."}, {"heading": "4 Dawid-Skene Scheme", "text": "In Dawid-Skene-Schema we can assume that the probability that the worker i a value in the class l = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 ="}, {"heading": "5 Online Dawid-Skene Scheme", "text": "The basic idea of the proposed method is to replace the maximization step with a stochastic approximation step, while the expectation step remains unchanged. Specifically, setsilg (n) = 1nn \u2211 j = 1P (yj = l | C, Z) 1 (zij = g).Then we can rewrite the tensor (i, l, g) -th element of the Dawid-Skene scheme (6) online. Using tensor S and the user confusion tensor C. The E-step of the Dawid-Skene scheme can be used as a calculation of S based on C. Accordingly, the M-step can be considered a calculation of C (i, l, g) -th element. In order to define the online algorithm rigorously, we need to make some assumptions about how the labels are delivered to the online algorithm."}, {"heading": "6 Convergence Analysis", "text": "In this section we show that the online Dawid-Skene scheme with projection approaches a stationary point of the marginal log probability under mild conditions (2)."}, {"heading": "6.1 Preliminary", "text": "In this subsection, we define some functions that capture the key update property of the Dawid-Skene schema and the online Dawid-Skene schema. Then, we analyze the online algorithm by examining these functions. In iteration j, the M step can be used as a calculation of C based on S. For the online Dawid-Skene schema without loss of universality, we select 0 < silg (0) < 1 for all i-m], l-k [k], g-k [k], then it can easily be shown that 0 < silg (i) < 1 for each iteration i. So, for simplicity's sake, we define D = (0, 1) m \u00d7 k-k. Then, given the S-D label, we define the G: D 7 \u2192 D asG (S) = C function, where cilg = seigk = g-g = 1seig.Then, we define the S-D function as a whole, and assume that we have the corresponding S-D (S = n)."}, {"heading": "6.2 Main Results", "text": "The T function is defined in (9), T (S) > < V1 (S) > < V1 (S) > lim S \u2192 V2 (S) > V2 (S) > V2 (S) > V2 (S) > V2 (S) > V2 (S) > V2 (S) > V2 (S) > V2 (S) > V2 (S) > V2 (S) > V2 (S) > V2 (S) > V2 (S) > V2) > V2 (S) > V2 (S) > V2 (S) > V2) > V2 (S) > V2 (S) > V2 (S) > V2 (S) > V2 (S) > V2)."}, {"heading": "7 Experiment", "text": "In this section, we will experimentally examine the performance and efficiency of the proposed online algorithm (7). Experiments are based on three sets of data: one binary task and two multi-class tasks. The binary task is to detect the textual entanglement Snow et al. [2008] (RTE). Multi-class tasks include tagging the images of 4 bread dogs from ImageNet Zhou et al. [2012] (DOG) and assessing the relevance of query URL pairs on a 5-level rating scale Zhou et al. [2012] (WEB). Characteristics of the data sets are summarized in Table (2)."}, {"heading": "7.1 Step Size", "text": "Decreasing slowly with respect to j means that the online algorithm learns slowly and learns from new information at a low speed. Thus, the speed of decreasing the learning rate \u03b7j is a compromise between learning and forgetting. Next, we examine how to choose the learning rate to achieve the most accurate results.In our experiment, two online algorithms corresponding to two step sizes (learning rate) are taken into account that meet condition (8). Specifically, a step size is chosen to decrease linearly in relation to the number of iterations and the corresponding online algorithm is called online1. The other step size is chosen to be slower than linear in relation to the number of iterations. < The next step size is chosen to decrease linearly in relation to the number of iterations. < b) The selection of two variables linearly in relation to b = linear is taken into consideration."}, {"heading": "7.2 Performace Comparison", "text": "Comparison with the Dawid-Skene schema: We compare the convergence rate with respect to the number of epochs between the online Dawid-Skene schema and the original version. Both algorithms assume the same initialization; the results are shown in Figure 2. The convergence rates between the online Dawid-Skene schema and the original schema are comparable: Both methods will converge over several epochs. In terms of performance, the online Dawid-Skene schema works surprisingly well. Both online1 and online2 outperform the Dawid-Skene schema on the RTE and WEB datasets. On the DOG dataset, online2 is slightly worse than Dawid-Skene schemas and online1 achieves the same error rate.Comparison with other methods: We compare online1 and online2 with other methods that are also based on the generative model of Dawid-Skene."}, {"heading": "8 Conclusion", "text": "We proposed an online version of the Dawid-Skene crowdsourcing scheme. Empirically, the proposed method exceeds the original Dawid-Skene scheme and various other methods and achieves state-of-the-art results. From a theoretical point of view, we showed that the online DawidSkene scheme converges to a stationary point of the marginal log probability of the observed data with the presence of two Lyapunov functions."}], "references": [{"title": "Online em algorithm for latent data models", "author": ["Olivier Capp\u00e9", "Eric Moulines"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Capp\u00e9 and Moulines.,? \\Q2008\\E", "shortCiteRegEx": "Capp\u00e9 and Moulines.", "year": 2008}, {"title": "Convergence and robustness of the robbins-monro algorithm truncated at randomly varying bounds", "author": ["Han-Fu Chen", "Lei Guo", "Ai-Jun Gao"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "Chen et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1987}, {"title": "Optimistic knowledge gradient policy for optimal budget allocation in crowdsourcing", "author": ["Xi Chen", "Qihang Lin", "Dengyong Zhou"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Aggregating crowdsourced binary ratings", "author": ["Nilesh Dalvi", "Anirban Dasgupta", "Ravi Kumar", "Vibhor Rastogi"], "venue": "In Proceedings of the 22nd international conference on World Wide Web,", "citeRegEx": "Dalvi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dalvi et al\\.", "year": 2013}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["Alexander Philip Dawid", "Allan M Skene"], "venue": "Applied statistics,", "citeRegEx": "Dawid and Skene.,? \\Q1979\\E", "shortCiteRegEx": "Dawid and Skene.", "year": 1979}, {"title": "General results on the convergence of stochastic algorithms", "author": ["Bernard Delyon"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Delyon.,? \\Q1996\\E", "shortCiteRegEx": "Delyon.", "year": 1996}, {"title": "Stochastic approximation with decreasing gain: Convergence and asymptotic theory", "author": ["Bernard Delyon"], "venue": "Unpublished lecture notes, Universite\u0301 de Rennes,", "citeRegEx": "Delyon.,? \\Q2000\\E", "shortCiteRegEx": "Delyon.", "year": 2000}, {"title": "Who moderates the moderators?: crowdsourcing abuse detection in user-generated content", "author": ["Arpita Ghosh", "Satyen Kale", "Preston McAfee"], "venue": "In Proceedings of the 12th ACM conference on Electronic commerce,", "citeRegEx": "Ghosh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2011}, {"title": "Stability of motion, volume 422", "author": ["Wolfgang Hahn", "Arne P Baartz"], "venue": null, "citeRegEx": "Hahn and Baartz.,? \\Q1967\\E", "shortCiteRegEx": "Hahn and Baartz.", "year": 1967}, {"title": "Efficient crowdsourcing for multi-class labeling", "author": ["David R Karger", "Sewoong Oh", "Devavrat Shah"], "venue": "In ACM SIGMETRICS Performance Evaluation Review,", "citeRegEx": "Karger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2013}, {"title": "Budgetoptimal task allocation for reliable crowdsourcing systems", "author": ["David R Karger", "Sewoong Oh", "Devavrat Shah"], "venue": "Operations Research,", "citeRegEx": "Karger et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2014}, {"title": "Stochastic approximation algorithms and applications", "author": ["Harold J Kushner", "G George Yin"], "venue": null, "citeRegEx": "Kushner and Yin.,? \\Q1997\\E", "shortCiteRegEx": "Kushner and Yin.", "year": 1997}, {"title": "Variational inference for crowdsourcing", "author": ["Qiang Liu", "Jian Peng", "Alex T Ihler"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "The EM algorithm and extensions, volume 382", "author": ["Geoffrey McLachlan", "Thriyambakam Krishnan"], "venue": null, "citeRegEx": "McLachlan and Krishnan.,? \\Q2007\\E", "shortCiteRegEx": "McLachlan and Krishnan.", "year": 2007}, {"title": "Learning from crowds", "author": ["Vikas C Raykar", "Shipeng Yu", "Linda H Zhao", "Gerardo Hermosillo Valadez", "Charles Florin", "Luca Bogoni", "Linda Moy"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Raykar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raykar et al\\.", "year": 2010}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "The annals of mathematical statistics,", "citeRegEx": "Robbins and Monro.,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro.", "year": 1951}, {"title": "On-line em algorithm for the normalized gaussian network", "author": ["Masa-Aki Sato", "Shin Ishii"], "venue": "Neural computation,", "citeRegEx": "Sato and Ishii.,? \\Q2000\\E", "shortCiteRegEx": "Sato and Ishii.", "year": 2000}, {"title": "Convergence of stochastic approximation algorithms under irregular conditions", "author": ["Jian Zhang", "Faming Liang"], "venue": "Statistica Neerlandica,", "citeRegEx": "Zhang and Liang.,? \\Q2008\\E", "shortCiteRegEx": "Zhang and Liang.", "year": 2008}, {"title": "Spectral methods meet em: A provably optimal algorithm for crowdsourcing", "author": ["Yuchen Zhang", "Xi Chen", "Dengyong Zhou", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Learning from the wisdom of crowds by minimax entropy", "author": ["Dengyong Zhou", "Sumit Basu", "Yi Mao", "John C Platt"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}, {"title": "Aggregating ordinal labels from crowds by minimax conditional entropy", "author": ["Dengyong Zhou", "Qiang Liu", "John Platt", "Christopher Meek"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "But, can we do better in crowdsourcing to estimate the true labels from the crowdsourced noisy labels? Observing that different workers may have different talents, Dawid and Skene [1979] develop a maximum likelihood approach based on the idea that each worker has a confusion matrix.", "startOffset": 164, "endOffset": 187}, {"referenceID": 4, "context": "But, can we do better in crowdsourcing to estimate the true labels from the crowdsourced noisy labels? Observing that different workers may have different talents, Dawid and Skene [1979] develop a maximum likelihood approach based on the idea that each worker has a confusion matrix. More precisely, suppose that the items can be divided into k classes, and assume that each worker is associated with a k \u00d7 k confusion matrix, where the (l, c)-th entry represents the probability that a randomly chosen item in class l is labeled as class g by the worker. The true labels and the worker confusion matrices are jointly estimated by maximizing the marginal log-likelihood of the observed labels, where the unobserved true labels are treated as hidden variables. However, the value of the likelihood is extremely difficult to calculate, as the number of terms, whose sum is the likelihood of the observed labels, is exponentially growing with respect to the number of items. Instead, the worker confusion matrices are estimated iteratively by a Expectation-Maximization (EM) procedure McLachlan and Krishnan [2007]. Unfortunately, the memory consumption of Dawid-Skene scheme increases linearly with the size of data, as the intrinsic structure of this methods requires to access all the data in each iteration.", "startOffset": 164, "endOffset": 1112}, {"referenceID": 12, "context": "In the existing literature, depending on different practical situations, there are many methods [Liu et al., 2012; Ghosh et al., 2011; Karger et al., 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979].", "startOffset": 96, "endOffset": 201}, {"referenceID": 7, "context": "In the existing literature, depending on different practical situations, there are many methods [Liu et al., 2012; Ghosh et al., 2011; Karger et al., 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979].", "startOffset": 96, "endOffset": 201}, {"referenceID": 3, "context": "In the existing literature, depending on different practical situations, there are many methods [Liu et al., 2012; Ghosh et al., 2011; Karger et al., 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979].", "startOffset": 96, "endOffset": 201}, {"referenceID": 18, "context": "In the existing literature, depending on different practical situations, there are many methods [Liu et al., 2012; Ghosh et al., 2011; Karger et al., 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979].", "startOffset": 96, "endOffset": 201}, {"referenceID": 4, "context": ", 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979].", "startOffset": 66, "endOffset": 89}, {"referenceID": 0, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997].", "startOffset": 95, "endOffset": 203}, {"referenceID": 16, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997].", "startOffset": 95, "endOffset": 203}, {"referenceID": 6, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997].", "startOffset": 95, "endOffset": 203}, {"referenceID": 17, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997].", "startOffset": 95, "endOffset": 203}, {"referenceID": 11, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997].", "startOffset": 95, "endOffset": 203}, {"referenceID": 0, "context": ", 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979]. In particular, when we only have two classes, Ghosh et al. [2011] propose a method using Singular Value Decomposition to moderate online content given crowdsourced user ratings.", "startOffset": 14, "endOffset": 203}, {"referenceID": 0, "context": ", 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979]. In particular, when we only have two classes, Ghosh et al. [2011] propose a method using Singular Value Decomposition to moderate online content given crowdsourced user ratings. Based on the assumption that each worker labels all items, Ghosh et al. [2011] show that if the number of observations increases, their algorithm can infer the quality of contributions with error that converges to zero.", "startOffset": 14, "endOffset": 394}, {"referenceID": 0, "context": ", 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979]. In particular, when we only have two classes, Ghosh et al. [2011] propose a method using Singular Value Decomposition to moderate online content given crowdsourced user ratings. Based on the assumption that each worker labels all items, Ghosh et al. [2011] show that if the number of observations increases, their algorithm can infer the quality of contributions with error that converges to zero. Dalvi et al. [2013] relax the assumption that the graph between worker and item is either random or complete and propose another SVD-based algorithm which considers arbitrary worker-item graph.", "startOffset": 14, "endOffset": 555}, {"referenceID": 0, "context": ", 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979]. In particular, when we only have two classes, Ghosh et al. [2011] propose a method using Singular Value Decomposition to moderate online content given crowdsourced user ratings. Based on the assumption that each worker labels all items, Ghosh et al. [2011] show that if the number of observations increases, their algorithm can infer the quality of contributions with error that converges to zero. Dalvi et al. [2013] relax the assumption that the graph between worker and item is either random or complete and propose another SVD-based algorithm which considers arbitrary worker-item graph. Also, under the one coin model setting and considering random regular worker-item graph, Karger et al. [2014] uses an iterative approach to infer the true binary labels and later generalize it to multi-class labeling tasks in Karger et al.", "startOffset": 14, "endOffset": 839}, {"referenceID": 0, "context": ", 2013, 2014; Dalvi et al., 2013; Zhang et al., 2014] developed and improved based on the Dawid-Skene algorithm [Dawid and Skene, 1979]. In particular, when we only have two classes, Ghosh et al. [2011] propose a method using Singular Value Decomposition to moderate online content given crowdsourced user ratings. Based on the assumption that each worker labels all items, Ghosh et al. [2011] show that if the number of observations increases, their algorithm can infer the quality of contributions with error that converges to zero. Dalvi et al. [2013] relax the assumption that the graph between worker and item is either random or complete and propose another SVD-based algorithm which considers arbitrary worker-item graph. Also, under the one coin model setting and considering random regular worker-item graph, Karger et al. [2014] uses an iterative approach to infer the true binary labels and later generalize it to multi-class labeling tasks in Karger et al. [2013]. Focusing on learning a classifier, Chen et al.", "startOffset": 14, "endOffset": 976}, {"referenceID": 0, "context": "Focusing on learning a classifier, Chen et al. [2013]; Raykar et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 0, "context": "Focusing on learning a classifier, Chen et al. [2013]; Raykar et al. [2010]; Liu et al.", "startOffset": 35, "endOffset": 76}, {"referenceID": 0, "context": "Focusing on learning a classifier, Chen et al. [2013]; Raykar et al. [2010]; Liu et al. [2012] proposes a Bayesian algorithm by imposing a prior distribution on the experts.", "startOffset": 35, "endOffset": 95}, {"referenceID": 0, "context": "Focusing on learning a classifier, Chen et al. [2013]; Raykar et al. [2010]; Liu et al. [2012] proposes a Bayesian algorithm by imposing a prior distribution on the experts. By assuming that labels are generated by a probability distribution over workers, items, and labels Zhou et al. [2012, 2014] propose an algorithm by minimizing the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. This method can also output item difficulty and worker expertise as by-products. Zhou et al. [2014] observe that it is difficult to distinguish between two adjacent ordinal classes while distinguishing between two far-away classes is much easier and they propose an algorithm based on minimax conditional entropy.", "startOffset": 35, "endOffset": 531}, {"referenceID": 0, "context": "Focusing on learning a classifier, Chen et al. [2013]; Raykar et al. [2010]; Liu et al. [2012] proposes a Bayesian algorithm by imposing a prior distribution on the experts. By assuming that labels are generated by a probability distribution over workers, items, and labels Zhou et al. [2012, 2014] propose an algorithm by minimizing the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. This method can also output item difficulty and worker expertise as by-products. Zhou et al. [2014] observe that it is difficult to distinguish between two adjacent ordinal classes while distinguishing between two far-away classes is much easier and they propose an algorithm based on minimax conditional entropy. Initialization is very important for DawidSkene algorithm, as it aims to maximize a nonconcave function. Addressing this issue, Zhang et al. [2014] propose to initialize Dawid-Skene algorithm using spectral methods and show that the resulting algorithm has a high probability to converge to the global optimum.", "startOffset": 35, "endOffset": 893}, {"referenceID": 0, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997]. In particular, Capp\u00e9 and Moulines [2008] replace the Expectation step by a stochastic approximation step to derive an online EM algorithm for latent data models, while our online algorithms can be seen as replacing the Maximization step by a stochastic approximation step.", "startOffset": 96, "endOffset": 246}, {"referenceID": 0, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997]. In particular, Capp\u00e9 and Moulines [2008] replace the Expectation step by a stochastic approximation step to derive an online EM algorithm for latent data models, while our online algorithms can be seen as replacing the Maximization step by a stochastic approximation step. Further, Capp\u00e9 and Moulines [2008] prove that, under some conditions, the online EM converges to a stationary points of the KL divergence between the marginal distribution of the observation and the model distribution, while we prove that our online Dawid-Skene scheme convergence to a stationary point of the the marginal log-likelihood functions.", "startOffset": 96, "endOffset": 513}, {"referenceID": 0, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997]. In particular, Capp\u00e9 and Moulines [2008] replace the Expectation step by a stochastic approximation step to derive an online EM algorithm for latent data models, while our online algorithms can be seen as replacing the Maximization step by a stochastic approximation step. Further, Capp\u00e9 and Moulines [2008] prove that, under some conditions, the online EM converges to a stationary points of the KL divergence between the marginal distribution of the observation and the model distribution, while we prove that our online Dawid-Skene scheme convergence to a stationary point of the the marginal log-likelihood functions. Notice our result directly connects to the objective function that we want to maximize. Sato and Ishii [2000] propose an online EM algorithm for Normalized Gaussian Network by introducing a discount factor to forget the previous incorrect estimated parameters.", "startOffset": 96, "endOffset": 937}, {"referenceID": 0, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997]. In particular, Capp\u00e9 and Moulines [2008] replace the Expectation step by a stochastic approximation step to derive an online EM algorithm for latent data models, while our online algorithms can be seen as replacing the Maximization step by a stochastic approximation step. Further, Capp\u00e9 and Moulines [2008] prove that, under some conditions, the online EM converges to a stationary points of the KL divergence between the marginal distribution of the observation and the model distribution, while we prove that our online Dawid-Skene scheme convergence to a stationary point of the the marginal log-likelihood functions. Notice our result directly connects to the objective function that we want to maximize. Sato and Ishii [2000] propose an online EM algorithm for Normalized Gaussian Network by introducing a discount factor to forget the previous incorrect estimated parameters. Later, Sato [2000] shows that if the probability distributions for both observed and unobserved data belong to an exponential family, then their algorithm converges to the corresponding stationary point of the marginal log-likelihood function.", "startOffset": 96, "endOffset": 1107}, {"referenceID": 0, "context": "Our work is motivated by stochastic approximation methods and variants of online EM algorithms [Capp\u00e9 and Moulines, 2008; Sato and Ishii, 2000; Delyon, 2000; Zhang and Liang, 2008; Kushner and Yin, 1997]. In particular, Capp\u00e9 and Moulines [2008] replace the Expectation step by a stochastic approximation step to derive an online EM algorithm for latent data models, while our online algorithms can be seen as replacing the Maximization step by a stochastic approximation step. Further, Capp\u00e9 and Moulines [2008] prove that, under some conditions, the online EM converges to a stationary points of the KL divergence between the marginal distribution of the observation and the model distribution, while we prove that our online Dawid-Skene scheme convergence to a stationary point of the the marginal log-likelihood functions. Notice our result directly connects to the objective function that we want to maximize. Sato and Ishii [2000] propose an online EM algorithm for Normalized Gaussian Network by introducing a discount factor to forget the previous incorrect estimated parameters. Later, Sato [2000] shows that if the probability distributions for both observed and unobserved data belong to an exponential family, then their algorithm converges to the corresponding stationary point of the marginal log-likelihood function. But, this is not applicable to our case, as under the crowdsourcing setting, the joint probability distribution does not belong to the exponential family defined in Sato [2000].", "startOffset": 96, "endOffset": 1509}, {"referenceID": 3, "context": "\u2022 The existence of a Lyapunov function is usually theoretically non-restrictive, as is critical to the convergence of the trajectories of the corresponding vector field, see Delyon [2000, 1996]; Hahn and Baartz [1967] for more information.", "startOffset": 174, "endOffset": 218}, {"referenceID": 1, "context": "A more elegant projection procedure is proposed in Chen et al. [1987]; Delyon [2000].", "startOffset": 51, "endOffset": 70}, {"referenceID": 1, "context": "A more elegant projection procedure is proposed in Chen et al. [1987]; Delyon [2000]. Thus, in the following analysis, we focus on the projected version of (7).", "startOffset": 51, "endOffset": 85}, {"referenceID": 1, "context": "A more elegant projection procedure is proposed in Chen et al. [1987]; Delyon [2000]. Thus, in the following analysis, we focus on the projected version of (7). Considering the projection procedure in Delyon [2000], we assume that there is a sequence of compact sets Kt, t = 0, 1, 2, \u00b7 \u00b7 \u00b7 , whose union is D.", "startOffset": 51, "endOffset": 215}, {"referenceID": 13, "context": "2 in McLachlan and Krishnan [2007], we know that DawidSkene Scheme converges to a stationary point of the marginal log-likelihood (2).", "startOffset": 5, "endOffset": 35}, {"referenceID": 12, "context": "(15) The online iterative step (14) have the same form as Robbins-Monro algorithms Robbins and Monro [1951]; Kushner and Yin [1997]; Delyon [2000], which is used to find the zeros of function T.", "startOffset": 83, "endOffset": 108}, {"referenceID": 9, "context": "(15) The online iterative step (14) have the same form as Robbins-Monro algorithms Robbins and Monro [1951]; Kushner and Yin [1997]; Delyon [2000], which is used to find the zeros of function T.", "startOffset": 109, "endOffset": 132}, {"referenceID": 5, "context": "(15) The online iterative step (14) have the same form as Robbins-Monro algorithms Robbins and Monro [1951]; Kushner and Yin [1997]; Delyon [2000], which is used to find the zeros of function T.", "startOffset": 133, "endOffset": 147}, {"referenceID": 5, "context": "Under assumption (2) and (1), by Theorem 12 in Delyon [2000] (or equivalently stochastic approximation theories in Zhang and Liang [2008]; Kushner and Yin [1997]), we have S(j) \u2192 S for some S \u2208 O such that T(S) = 0.", "startOffset": 47, "endOffset": 61}, {"referenceID": 5, "context": "Under assumption (2) and (1), by Theorem 12 in Delyon [2000] (or equivalently stochastic approximation theories in Zhang and Liang [2008]; Kushner and Yin [1997]), we have S(j) \u2192 S for some S \u2208 O such that T(S) = 0.", "startOffset": 47, "endOffset": 138}, {"referenceID": 5, "context": "Under assumption (2) and (1), by Theorem 12 in Delyon [2000] (or equivalently stochastic approximation theories in Zhang and Liang [2008]; Kushner and Yin [1997]), we have S(j) \u2192 S for some S \u2208 O such that T(S) = 0.", "startOffset": 47, "endOffset": 162}, {"referenceID": 0, "context": "\u2022 Alternatively, one can use the results provided in Capp\u00e9 and Moulines [2008] to show that the online Dawid-Skene Scheme converges to a stationary point of the KL divergence between the induced partial distribution P (y|C) and true distribution P\u0302 for y, while our results directly connects to the marginal log-likelihood function that Dawid-Skene Scheme tries to maximize.", "startOffset": 53, "endOffset": 79}, {"referenceID": 19, "context": "Multi-class tasks include labeling the images of 4 breads dogs from ImageNet Zhou et al. [2012] (DOG) and judging the relevance of query-URL pairs with a 5-level rating scale Zhou et al.", "startOffset": 77, "endOffset": 96}, {"referenceID": 19, "context": "Multi-class tasks include labeling the images of 4 breads dogs from ImageNet Zhou et al. [2012] (DOG) and judging the relevance of query-URL pairs with a 5-level rating scale Zhou et al. [2012] (WEB).", "startOffset": 77, "endOffset": 194}, {"referenceID": 16, "context": "Specifically we compare online1, online2 and Dawid-Skene initialized by majority voting, majority voting method, Dawid-Skene scheme initialized by spectral method proposed by Zhang et al. [2014] (referred as Opt-Dawid-Skene), the multi-class labeling algorithm proposed by Karger et al.", "startOffset": 175, "endOffset": 195}, {"referenceID": 9, "context": "[2014] (referred as Opt-Dawid-Skene), the multi-class labeling algorithm proposed by Karger et al. [2013] (referred as KOS).", "startOffset": 85, "endOffset": 106}], "year": 2015, "abstractText": "With the success of modern internet based platform, such as Amazon Mechanical Turk, it is now normal to collect a large number of hand labeled samples from non-experts. The DawidSkene algorithm, which is based on ExpectationMaximization update, has been widely used for inferring the true labels from noisy crowdsourced labels. However, Dawid-Skene scheme requires all the data to perform each EM iteration, and can be infeasible for streaming data or large scale data. In this paper, we provide an online version of DawidSkene algorithm that only requires one data frame for each iteration. Further, we prove that under mild conditions, the online Dawid-Skene scheme with projection converges to a stationary point of the marginal log-likelihood of the observed data. Our experiments demonstrate that the online DawidSkene scheme achieves state of the art performance comparing with other methods based on the DawidSkene scheme.", "creator": "LaTeX with hyperref package"}}}