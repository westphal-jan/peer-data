{"id": "1512.02394", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2015", "title": "Online Gradient Descent in Function Space", "abstract": "In many problems in machine learning and operations research, we need to optimize a function whose input is a random variable or a probability density function, i.e. to solve optimization problems in an infinite dimensional space. On the other hand, online learning has the advantage of dealing with streaming examples, and better model a changing environ- ment. In this paper, we extend the celebrated online gradient descent algorithm to Hilbert spaces (function spaces), and analyze the convergence guarantee of the algorithm. Finally, we demonstrate that our algorithms can be useful in several important problems.", "histories": [["v1", "Tue, 8 Dec 2015 10:38:19 GMT  (13kb)", "http://arxiv.org/abs/1512.02394v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changbo zhu", "huan xu"], "accepted": false, "id": "1512.02394"}, "pdf": {"name": "1512.02394.pdf", "metadata": {"source": "CRF", "title": "Online Gradient Descent in Function Space", "authors": ["Changbo Zhu", "Huan Xu"], "emails": ["mpexuh}@nus.edu.sg"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.02 394v 1 [cs.L G] 8D ec2 01"}, {"heading": "1 Introduction", "text": "Regret is a general attitude used in decision-making and prediction."}, {"heading": "2 Motivating Examples", "text": "In this section, we present some examples of problems that require either a gradient decrease or an online gradient decrease in the functional area."}, {"heading": "2.1 Online Classifier Selection", "text": "Assuming that we can collect data points {(xt, yt)} Tt = 1 from an unknown distribution P to X \u00b7 Y, where X'Rn is the space of data points and Y'R is the space of labels. Then we can learn a classifier f: X \u2192 Y that can best predict the label of a data point by solving the following problem in f'C1TT \u2211 t = 1l (f (xt), yt). (1) where C is a predefined set of classifiers and l is the loss function. If f is linear, then (1) can be reduced to a standard minimization problem (such as linear SVM) in Rn. A more interesting case is considered by authors C as a set of all linear combinations of an endless number of base classifiers, and then the labeling is applied in a suitable function space."}, {"heading": "2.2 Risk Measure Minimization", "text": "Risk measurement is used to quantify and compare uncertain outcomes, which is a central concept in decision theory [1, 20]. In this subsection, a real estimated random variable X shall be a probability space, i.e. that it represents a set of outcomes. We will focus on space L2, which contains all random variables X, so that the random variable X | X | 2 d\u00b5 < \u221e is a Hilbert space with an inner product defined as < X, Y > = \"XY d\u00b5.\" A risk measurement refers to a function L2, which contains all random variables X."}, {"heading": "2.3 Distributionally Robust Stochastic Program", "text": "Robust Optimization (RO) is a framework for decision-making under uncertainty that has attracted fast-growing attention. However, RO deals with decision-making problems where the problem parameter is not specific but is known to be part of a series of uncertainties [4, 9, 24, 5]. Mathematically, robust optimization problem can be formulated as the following x-factor: X-Max probability S f (x), (4) where X-Rm is the workable solution, and S-Rn the unbearable proposition. If the uncertainty is instead probable and is regulated by a probability distribution P, which is itself uncertain and belongs to a series of distributions P, we get the following distributionally robust Stochastic Program (DRSP) aka Distribution Robust Optimization [8, 10, 23], min x-X (max P-Max x value), (X value), (5) where X-Rn and P value consist of a problem of a distribution problem."}, {"heading": "3 Online Functional Gradient Descent", "text": "In this section we present the online functionality gradient and its variants. < p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p."}, {"heading": "3.1 Calculate the Projection", "text": "The following two examples show that it is easy to project a point onto a closed sphere or a hyperplanet. Example 1. If we let B'H be the closed sphere with radius 1, then B'H is the closed sphere with radius 1, then we have x'x 'H PB (x) = 1max x' H, 1 'x. Example 2. If we let u'H be a vector non-zero, let us leave E'R and set C = {x'H | < x, u > H = \u03b7}, then we have x'H PC (x) = x \u2212 n; < x, u > H 2 H and sometimes a function f'L2 (x'm) is required to have non-negative values (e.g. the density functions), and in this case we have the following formula: Example 3. Set C = {p'L2 (m)."}, {"heading": "4 Applications", "text": "In this section, we will discuss some concrete examples to illustrate the application of the developed framework, in particular the calculation of the corresponding derivatives and projections. For each specific application, a suitable Hilbert space will be selected."}, {"heading": "4.1 Online Classifier Selection", "text": "The selection problem of the online classifier is described above in Section 2.1. Let us consider here the problem of minimizing the following regrets: Repentance (T) = (1TT) = 1lt (ft) \u2212 min f (C1TT) = 1lt (f), (10) where {lt} and C by lt (f) = (f (xt) \u2212 yt) 2, C = (m i = 1 C (ft). (RB) where B is the closed sphere of H. (Given that we have G, i = {f \u00b2 H | < f > H = ai}. Each C \u00b2 i corresponds to a linear constraint imposed on f \u00b2."}, {"heading": "4.2 Risk Measure Minimization", "text": "The x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "4.3 Distributionally Robust Stochastic Program", "text": "Here we are looking at a slightly different version of problem (5), i.e., we are focusing on probability density functions instead of probability distributions. In particular, we are looking at space L2 (Rn) = L2 (Rn) = L2 (Rn > L2 =), where either the \u03c3 algebra of Lebesgue is measurable quantities and the Lebesgue measurement quantity is Rn or the Lebesgue measurement quantity is Rn. Then, L2 (Rn) is a Hilbert space with an internal product defined as < f > L2 = fg d\u00b5. Accordingly, the induced standard on L2 (Rn) is a problem \u00b7 L2 and we are using BL2 to denote the closed sphere of L2 (Rn). We are assuming that we are an optimal variable with the probability function p P, then the Distributional Robust Stochastic Program can be written."}, {"heading": "4.3.1 Functional Dual Gradient Descent", "text": "If we set g (x, p) = Ep [h (x, p)], we assume that g in x is convex (which is true if h (\u00b7, p) is convex), P'RBL2 convex and maxx, p'pg (x, p) and L2 \u2264 G. Then we propose the following algorithm to solve problem (12). Algorithm 1: Functional Dual Gradient Descent with Noisy Projection Input: \u03b4, G Initialize p0 \"P\" arbitrarily T, \"Tt = max\" T, \"so that RG\" T + (2) T + (2) T \"\u2264\" P \"\u2212 implicit\") G. \"And we set it = R2\" Plot \"/ G\" for t. \"\u00b7 \u00b7, T\" \u00b7, T \"do Calculate\" pt \"pt\" p, \"so that pt\" P \"and pt \u2212 PP (pt \u2212 1 \u2212 xt)."}, {"heading": "4.3.2 A Concrete Example", "text": "In this subsection we consider the following simple example to illustrate how DRSP can be resolved over the above-outlined framework: min x-B max p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-"}, {"heading": "5 Conclusion", "text": "In this paper, we look at online learning in infinite dimensional space. In particular, we propose an online learning algorithm called Online Functional Gradient Descent, which extends Zinkevich's well-known online gradient descent algorithm [25], and then provide theoretical results for the proposed algorithms. Finally, we demonstrate how to apply our algorithm to practical problems in machine learning and surgical research, including the selection of online classifiers, minimization of risk measures, and distribution-robust optimization."}], "references": [{"title": "Coherent measures of risk", "author": ["Philippe Artzner", "Freddy Delbaen", "Jean-Marc Eber", "David Heath"], "venue": "Mathematical finance,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Convex analysis and monotone operator theory in Hilbert spaces", "author": ["Heinz H Bauschke", "Patrick L Combettes"], "venue": "Springer Science & Business Media,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Robust convex optimization", "author": ["Aharon Ben-Tal", "Arkadi Nemirovski"], "venue": "Mathematics of Operations Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "The price of robustness", "author": ["Dimitris Bertsimas", "Melvyn Sim"], "venue": "Operations research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Probabilistic and randomized methods for design under uncertainty", "author": ["Giuseppe Calafiore", "Fabrizio Dabbene"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Distributionally robust optimization under moment uncertainty with application to data-driven problems", "author": ["Erick Delage", "Yinyu Ye"], "venue": "Operations research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Robust solutions to least-square problems to uncertain data matrices", "author": ["L EI-Ghaoui", "H Lebret"], "venue": "Sima Journal on Matrix Analysis and Applications,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Distributionally robust optimization and its tractable approximations", "author": ["Joel Goh", "Melvyn Sim"], "venue": "Operations research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Optimization in the space of distribution functions and applications in the Bayes analysis", "author": ["Alexandr N Golodnikov", "Pavel S Knopov", "Panos M Pardalos", "Stanislav P Uryasev"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "The convex optimization approach to regret minimization. Optimization for machine learning, pages", "author": ["Elad Hazan"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Adaptive online gradient descent", "author": ["Elad Hazan", "Alexander Rakhlin", "Peter L Bartlett"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "A correspondence between bayesian estimation on stochastic processes and smoothing by splines", "author": ["George S Kimeldorf", "Grace Wahba"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1970}, {"title": "Online learning with kernels", "author": ["Jyrki Kivinen", "Alexander J Smola", "Robert C Williamson"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Convex functional analysis", "author": ["Andrew J Kurdila", "Michael Zabarankin"], "venue": "Springer Science & Business Media,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Boosting algorithms as gradient descent in function space", "author": ["Llew Mason", "Jonathan Baxter", "Peter Bartlett", "Marcus Frean"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Grafting: Fast, incremental feature selection by gradient descent in function space", "author": ["Simon Perkins", "Kevin Lacker", "James Theiler"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Optimization of convex risk functions", "author": ["Andrzej Ruszczynski", "Alexander Shapiro"], "venue": "Mathematics of operations research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Bernhard Scholkopf", "Alexander J. Smola"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "A distributional interpretation of robust optimization", "author": ["Huan Xu", "Constantine Caramanis", "Shie Mannor"], "venue": "Mathematics of Operations Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Robustness and generalization", "author": ["Huan Xu", "Shie Mannor"], "venue": "Machine learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In Proceedings of the Twentieth International Conference on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "A merge of convex optimization and regret minimization leads to the online convex optimization problem [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 9, "context": "Due to its simple setting and generality, online convex optimization has raised many attentions recently [12, 22].", "startOffset": 105, "endOffset": 113}, {"referenceID": 19, "context": "Due to its simple setting and generality, online convex optimization has raised many attentions recently [12, 22].", "startOffset": 105, "endOffset": 113}, {"referenceID": 22, "context": "Zinkevich proposed the following algorithm called online gradient descent [25] for the above problem: play x1 \u2208 K arbitrarily and at round t play xt := PK(xt\u22121 \u2212\u2207f(xt\u22121)), where PK stands for projection into K.", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "By assuming that all ft has second derivatives bounded below by a strictly positive number, Hazan et al [13] gave an algorithm which can achieve O(log(T )) regret bound.", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "Subsequently, Hazan et al [14] provide an algorithm achieving rates inteplay between O( \u221a T ) and O(log(T )) without a priori knowledge of the lower bound on the second derivatives.", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "For example, minimizing a risk measure over a set of random variables (chapter 4 in [6]), solving an optimization problem in the space of distribution functions [11] and learning a classifier over a set of functions [16, 18, 19].", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "For example, minimizing a risk measure over a set of random variables (chapter 4 in [6]), solving an optimization problem in the space of distribution functions [11] and learning a classifier over a set of functions [16, 18, 19].", "startOffset": 161, "endOffset": 165}, {"referenceID": 13, "context": "For example, minimizing a risk measure over a set of random variables (chapter 4 in [6]), solving an optimization problem in the space of distribution functions [11] and learning a classifier over a set of functions [16, 18, 19].", "startOffset": 216, "endOffset": 228}, {"referenceID": 15, "context": "For example, minimizing a risk measure over a set of random variables (chapter 4 in [6]), solving an optimization problem in the space of distribution functions [11] and learning a classifier over a set of functions [16, 18, 19].", "startOffset": 216, "endOffset": 228}, {"referenceID": 16, "context": "For example, minimizing a risk measure over a set of random variables (chapter 4 in [6]), solving an optimization problem in the space of distribution functions [11] and learning a classifier over a set of functions [16, 18, 19].", "startOffset": 216, "endOffset": 228}, {"referenceID": 22, "context": "In this paper, we propose the online functional gradient algorithm which extends online gradient descent algorithm of Zinkevich [25] to a general Hilbert space (function space).", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "A more interesting case is studied in [18], in which the authors considered C as the set of all linear combinations of finitely many base-classifiers and then apply the gradient descent algorithm in a suitable function space to solve the problem.", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "Another example is the nonlinear SVM considered in [21, 7], which is solved by letting C be a certain Reproducing Kernel Hilbert Space (RKHS) and using the representer theorem [15] to convert the problem into an optimization problem in R.", "startOffset": 51, "endOffset": 58}, {"referenceID": 12, "context": "Another example is the nonlinear SVM considered in [21, 7], which is solved by letting C be a certain Reproducing Kernel Hilbert Space (RKHS) and using the representer theorem [15] to convert the problem into an optimization problem in R.", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "We remark that the case when C is an entire Reproducing Kernel Hilbert Space is considered in [16], which is a significantly simpler case than the general setup we considered in this paper, as there is no projection involved.", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "2 Risk Measure Minimization Risk Measure is used to quantify and compare uncertain outcomes, which is a central concept in decision theory [1, 20].", "startOffset": 139, "endOffset": 146}, {"referenceID": 17, "context": "2 Risk Measure Minimization Risk Measure is used to quantify and compare uncertain outcomes, which is a central concept in decision theory [1, 20].", "startOffset": 139, "endOffset": 146}, {"referenceID": 17, "context": "In practice, uncertain outcomes (random variables) often result from decisions (actions) in some uncertain systems [20].", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "RO addresses decision problems in which the problem parameter is not specific but known to belong to an uncertainty set [4, 9, 24, 5].", "startOffset": 120, "endOffset": 133}, {"referenceID": 6, "context": "RO addresses decision problems in which the problem parameter is not specific but known to belong to an uncertainty set [4, 9, 24, 5].", "startOffset": 120, "endOffset": 133}, {"referenceID": 21, "context": "RO addresses decision problems in which the problem parameter is not specific but known to belong to an uncertainty set [4, 9, 24, 5].", "startOffset": 120, "endOffset": 133}, {"referenceID": 3, "context": "RO addresses decision problems in which the problem parameter is not specific but known to belong to an uncertainty set [4, 9, 24, 5].", "startOffset": 120, "endOffset": 133}, {"referenceID": 5, "context": "If the uncertainty instead is probabilistic, and is governed by a probability distribution P , which itself is uncertain and belongs to a set of distributions P , we get the following Distributionally Robust Stochastic Program (DRSP) aka Distributionally Robust Optimization [8, 10, 23], min x\u2208X (", "startOffset": 275, "endOffset": 286}, {"referenceID": 7, "context": "If the uncertainty instead is probabilistic, and is governed by a probability distribution P , which itself is uncertain and belongs to a set of distributions P , we get the following Distributionally Robust Stochastic Program (DRSP) aka Distributionally Robust Optimization [8, 10, 23], min x\u2208X (", "startOffset": 275, "endOffset": 286}, {"referenceID": 20, "context": "If the uncertainty instead is probabilistic, and is governed by a probability distribution P , which itself is uncertain and belongs to a set of distributions P , we get the following Distributionally Robust Stochastic Program (DRSP) aka Distributionally Robust Optimization [8, 10, 23], min x\u2208X (", "startOffset": 275, "endOffset": 286}, {"referenceID": 1, "context": "14 in [2], we have \u3008x\u0302\u2212PC(x), x \u2212PC(x)\u3009H \u2264 0, which further implies that \u2016PC(x)\u2212 x\u0302\u2016H + \u2016x\u2212PC(x)\u2016H \u2264 \u2016x\u2212 x\u0302\u2016H.", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "6 in [17], there exists a x\u2217 in K minimizing \u2211Tt=1 ft(x).", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "10 in [2], we have ft(xt)\u2212 ft(x) \u2264 \u3008\u2207ft(xt), xt \u2212 x\u3009H .", "startOffset": 6, "endOffset": 9}, {"referenceID": 15, "context": "The idea of gradient descent and variants in function space has appeared before [18, 19, 16], but all these works consider the unconstrained case only, i.", "startOffset": 80, "endOffset": 92}, {"referenceID": 16, "context": "The idea of gradient descent and variants in function space has appeared before [18, 19, 16], but all these works consider the unconstrained case only, i.", "startOffset": 80, "endOffset": 92}, {"referenceID": 13, "context": "The idea of gradient descent and variants in function space has appeared before [18, 19, 16], but all these works consider the unconstrained case only, i.", "startOffset": 80, "endOffset": 92}, {"referenceID": 14, "context": "6 in [17], there exists a x\u2217 in K minimizing \u2211Tt=1 ft(x).", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "10 in [2], we have ft(xt)\u2212 ft(x) \u2264 \u3008\u2207ft(xt), xt \u2212 x\u3009H .", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "1 Calculate the Projection Here, we introduce some exact formulas to calculate projections onto some common sets and refer Chapter 28 in [2] for a comprehensive study of the projection operator in Hilbert space.", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "44 in [2], we have \u2207lt(f) = 2 (f(xt)\u2212 yt) k(\u00b7, xt).", "startOffset": 6, "endOffset": 9}, {"referenceID": 4, "context": "Set \u03c1(X) = E(X) + c\u2016X \u2212 E(X)\u2016L2(\u03a9,\u03a3,\u03bc), it is proved in [6] (Chapter4 page128) that \u03c1 is convex.", "startOffset": 56, "endOffset": 59}, {"referenceID": 22, "context": "In particular, we propose an online learning algorithm called online functional gradient descent, which extends the well known online gradient descent algorithm of Zinkevich [25].", "startOffset": 174, "endOffset": 178}], "year": 2016, "abstractText": "In many problems in machine learning and operations research, we need to optimize a function whose input is a random variable or a probability density function, i.e. to solve optimization problems in an infinite dimensional space. On the other hand, online learning has the advantage of dealing with streaming examples, and better model a changing environment. In this paper, we extend the celebrated online gradient descent algorithm to Hilbert spaces (function spaces), and analyze the convergence guarantee of the algorithm. Finally, we demonstrate that our algorithms can be useful in several important problems.", "creator": "LaTeX with hyperref package"}}}