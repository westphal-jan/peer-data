{"id": "1512.02406", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2015", "title": "Learning Discrete Bayesian Networks from Continuous Data", "abstract": "Real data often contains a mixture of discrete and continuous variables, but many Bayesian network structure learning and inference algorithms assume all random variables are discrete. Continuous variables are often discretized, but the choice of discretization policy has significant impact on the accuracy, speed, and interpretability of the resulting models. This paper introduces a principled Bayesian discretization method for continuous variables in Bayesian networks with quadratic complexity instead of the cubic complexity of other standard techniques. Empirical demonstrations show that the proposed method is superior to the state of the art. In addition, this paper shows how to incorporate existing methods into the structure learning process to discretize all continuous variables and simultaneously learn Bayesian network structures.", "histories": [["v1", "Tue, 8 Dec 2015 11:12:04 GMT  (780kb,D)", "https://arxiv.org/abs/1512.02406v1", "This work has been submitted to Machine Learning (Springer journal)"], ["v2", "Tue, 15 Dec 2015 08:00:55 GMT  (817kb,D)", "http://arxiv.org/abs/1512.02406v2", "This work has been submitted to Machine Learning (Springer journal)"]], "COMMENTS": "This work has been submitted to Machine Learning (Springer journal)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["yi-chun chen", "tim allan wheeler", "mykel john kochenderfer"], "accepted": false, "id": "1512.02406"}, "pdf": {"name": "1512.02406.pdf", "metadata": {"source": "CRF", "title": "Learning Discrete Bayesian Networks from Continuous Data", "authors": ["Yi-Chun Chen", "Tim A. Wheeler", "Mykel J. Kochenderfer"], "emails": ["yichunc@stanford.edu", "wheelert@stanford.edu", "mykel@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "It is common to assume that the random variables on a Bayesian network are discrete, since many Bayesian networking methods are unable to handle continuous variables efficiently. Furthermore, many of the commonly used Bayesian network software packages, such as Neo (Norsys 1992-2009), SMILearn (Druzdzel 1999), and bnlearn (Scutari 2010), tend to be discrete variables. However, many applications require the use of continuous variables, such as position and velocity in dynamic systems (Kochenderfer et al 2010)."}, {"heading": "2 Preliminaries", "text": "This section discusses the notation used throughout the essay to describe discretization policies, and provides a brief overview of Bayesian networks."}, {"heading": "2.1 Discretization Policies", "text": "Let X be a continuous variable and let x be a specific instance of X. A discrediting policy \u0432X = < e1 < e2 <.. < ek \u2212 1 > for X is a mapping of R to {1, 2, 3,..., k} such that X (x) = 1, if x < e1i, if ei \u2212 1 \u2264 x < ei k, otherwise. (1) The discrediting policy discredits X in k intervals. Let the samples of X in a given dataset D sort in ascending order, x1: n = {x1 \u2264 x2 \u2264... < ei k}, and let the unique values u1: m = {u1 < u2 < < um}. The index of the last occurrence of ui in x1: n is displayed. < < < < < the discrepancy instances < are < the boundaries between the discrediting instances < < each within the discrediting 1."}, {"heading": "2.2 Bayesian Networks", "text": "A Bayesian network B over N random variables X1: N is defined by a directed acyclic graph G, whose nodes are the random variables and a conditional probability distribution for each node given its parents. Edges in a Bayesian network represent probable dependencies between the nodes and encode the Markov property: Each node Xi is independent of its non-descendants given its parents paXi in G. The children of the node Xi are referred to as chXi. Discussing the discretification of a certain continuous variable X, Pi is said to be the ith parent of X, Ci the ith child of X, and Si the group of spouses of X associated with the ith child."}, {"heading": "3 Single Variable Discretization", "text": "This section discusses the discretization of a single continuous variable X in a Bayesian network where all other variables are discrete. An optimal discretization policy for a dataset D maximizes P (\u044b) \u00b7 P (D) for a previous P (\u044b) and the probability P (D)."}, {"heading": "3.1 Priors and Objective Function", "text": "In fact, it is so that it is a way in which most people are able to survive themselves. (...) In fact, it is so that most of them are able to survive themselves. (...) In fact, it is so that most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "3.2 Single-Variable Discretization Algorithm", "text": "The approach to minimizing the objective function involves dynamic programming. Note that the objective function is cumulative beyond the intervals when dynamic programming can be used to more accurately solve the optimization problem."}, {"heading": "4 Multi-Variable Discretization", "text": "The discretization process for a single variable requires that all other variables be discrete. The iterative approach uses an initial discretization method to start the process that assigns k-wide intervals to each continuous variable, with k being the largest number of intervals of initially discrete variables within the mesh. After the initial discretization, the single-variable discretization method is applied iteratively over each continuous variable in reverse topoligical order, from the leaves to the root. The reverse topological sequence has the advantage that it relies on less initial discretizations of the continuous variables during the first pass. For example, in the network of Figure 2, if S2 is the only discrete variable, the discretization of P1 includes both P2 and X, while the discretization of C1 is only an X.The algorithm conversion within the discretization of the 2 and the continuous discretizations occur."}, {"heading": "5 Combining Discretization with Structure Learning", "text": "It is often necessary to derive the structure of a Bayesian network from data. (D) Three common approaches to Bayesian network structure learning are based on constraints, scorebased, and Bayesian model averaging (see Koller and Friedman 2009, Fig. 18). This paper uses the K2 structure learning algorithm (Cooper and Herskovits 1992), a commonly used, score-based structure learning method. (D, G, n, n, n, n) The DiscretizeAll (D, X) learning method for each X-level based structure learning method. (D) The dataset structure-based structure learning algorithm is according to the X-level Max ({S, X) has no corresponding X functions. (X) 5: for (X, X) such that X and X is the discrediting of the X-level. (X) The discrediting of the X-level is the discrediting of the X-level."}, {"heading": "6 Experiments", "text": "In this section, experiments are performed to evaluate the Bayean discretization method. All experiments were performed on data sets from the publicly available University of California (Irvine machine learning repository) (Lichman 2013), with variables presented alphabetically in the order of the data on the data page. In the following numbers, the shaded nodes initially correspond to discrete variables, and the subscriptions indicate the number of discrete instantiation intervals. The first experiment compares the performance of Bayesian and MDL discretization methods on a known Bayesian network structure; the structure was achieved by discretizing the individual continuous variables in k uniform width intervals, where the median number of instantiations of the discrete variables is equal, and uses the structure with the highest Bayesian score of 1,000 runs of the K2 algorithms with random topological orderings; the second experiment compares the methods used in simultaneous learning and discretization."}, {"heading": "6.1 Dataset 1: Auto MPG", "text": "The data set Auto MPG contains variables related to the fuel consumption of cars in urban traffic. The data set includes 392 samples of eight variables, excluding six cases of missing data. Three variables are discrete: B, G and H with 5, 13 and 3 instances, respectively."}, {"heading": "6.1.1 Discretization with Fixed Structure", "text": "The Bayesian and MDL discretization methods were tested using the auto-MPG data using the network shown in Figure 3. This structure was first achieved by discretizing each continuous variable into five equal-width intervals, with five being the mean cardinality of the discrete variable, and then the structure with the highest probability of 1,000 passes of K2. Table 1 lists the discretization edges and the mean log probability below 10x cross-validation of the discrete Bayesian network resulting from the two discretion methods. MDL does not generate discretization edges, assigns a continuous interval to each continuous variable, and produces the result with the lower probability. The reason why MDL generates fewer discretization edges is discussed in Section 6.4. Two examples of MDL producing results comparable to the Bayesian method are given in the Appendix 4. Figure 4 shows the marginal probability method A for the original discretization density A and G matching with the Bayesian variable under the MPG."}, {"heading": "6.1.2 Discretization while Learning Structure", "text": "Figure 5 shows a learned Bayesian network structure and the corresponding number of intervals after discretization for each continuous variable. This result was achieved by executing algorithm 3 fifty times using the Bayesian method and selecting the structure with the highest K2 score (equation 12). Figure 6 compares the Bayesian discretization policy for variables A and C in the learned network with the original auto-MPG data. The color of a discretized region indicates the marginal probability density of a sample of P (A, C) drawn from that region. Although there are fewer discretization limits for A and C in the learned network, the marginal distribution is still recorded."}, {"heading": "6.2 Dataset 2: Wine", "text": "The wine dataset contains variables related to the chemical analysis of wines of three Italian grape varieties. The dataset includes 178 samples across fourteen variables. Variable A is the only discrete variable and has three instances."}, {"heading": "6.2.1 Discretization with Fixed Structure", "text": "The discretization methods of Bayesian and MDL were tested on the basis of the wine data using the network shown in Figure 7. This structure was first achieved by discretizing each continuous variable into three equally wide intervals, with three being the mean cardinality of the discrete variable, and then the structure with the highest probability of 1000 passes of K. Table 2 lists the discretization margins and the mean log probability below 10-fold cross-validation of the Bayesian network resulting from each discretization method. Bayesian's method probably slightly exceeds the MDL method. The MDL method creates significantly fewer discretization margins. Some discretiring margins occur in all three discretion methods, such as 1.42 and 2.35 for variable C and 0.785 for variable L. This indicates that MDL does indeed find some important discretization margins of the WDL, but it is not sensitive enough for WDL 385 to find the variable WDL and 385 for WDL."}, {"heading": "6.2.2 Discretization while Learning Structure", "text": "A comparison of Figures 9 and 7 shows that the Bayesian network learned during the discretization process has more edges than the network learned from the originally discredited data. If a network is learned along with discretization, the algorithm has more freedom to adjust the structure and discretization policy simultaneously to identify useful correlations and produce a more dense structure.Figure 10 shows the discretization policy for variables E and K obtained using the Bayesian method. Moreover, the discretization edge E = 17.9 also appears in both discretization policies from the fixed network (Figure 8), suggesting that discretization edges may be robust to the network structure. Moreover, the discretization edge is not present at E = 23.5 in the case of the fixed structure, due to the fact that E has twice as many parents in the learned network structure as the number of expenditures required."}, {"heading": "6.3 Dataset 3: Housing", "text": "The Housing dataset contains variables related to the values of houses in the Boston suburbs. The dataset includes 506 samples of fourteen variables. Only variables D and I are rated discreetly, with 2 and 9 instances, respectively. Although they are continuous, several variables in the Housing dataset have many repeated values. The following experiments were conducted with a maximum of three parents per variable to prevent memory from running out when MDL is executed."}, {"heading": "6.3.1 Discretization with Fixed Structure", "text": "The Bayesian network structure in Figure 11 was achieved by first discretiring each continuous variable into five equinox intervals, and then executing the K2 algorithm 1000 times, selecting the network with the highest probability. Table 3 shows the number of intervals after discretization for each continuous variable and the log probability of the data set based on each discretization method. The MDL method does not generate discretization edges for most variables. The relative weighting of the objective function of each method is given in Section 6.4.Figures 12 and 13 show the discretization policy learned using the Bayesian approach to the fixed network shown in Figure 11. the scatter points in Figure 12 were trembled to indicate the number of repeated values for variables C and E. Each repeated point forms a single discrete region, thus promoting discretization. In contrast, the samples for Figure 11 are well distributed, resulting in less H leading to greater discretization and greater discretion."}, {"heading": "6.3.2 Discretization while Learning Structure", "text": "The variables C, E, J, and K, which are all connected by C, have many discretization intervals. Typically, the expected number of discretization intervals in the discretization of a variable is close to the highest cardinality among the variables in its Markov mantle. This naturally leads to aggregations of variables with many discretization intervals. Figures 15 and 16 show the discretization result for variables in the network shown in Figure 14. Again, variables C and E have many discretization edges due to repeated values. Although the number of intervals after discretization on H is lower than the number in Figure 13, it still captures the distribution of the raw data along with the discretization edges for E."}, {"heading": "6.4 Discussion", "text": "To qualitatively evaluate the sensitivity of each method to the number and position of discretization intervals, we look at the Bayesian network in Figure 17, where X is continuous and P1 and P2 are discrete. If paX = {P1, P2} and DX = {1, 2, 3,. n}, then the corresponding objective functions for the discretization methods are: fMDL = penalty condition Z1 \u00b7 k + ln (k) + ln (n + k \u2212 1) + edge positioning term ln (n) \u00b7 I (X, paX) fBayesian = Z2 \u00b7 k + ln (k) + JP \u2212 1 JP \u2212 1 \u2212 1) penalty condition i = 1 ln (n) of discrediting method (s)."}, {"heading": "7 Conclusion", "text": "This paper introduces a principal discretization method for continuous variables in Bayesian networks with quadratic complexity instead of the cubic complexity of other standard techniques. Empirical demonstrations show that the proposed method is superior to the state of the art. Furthermore, this paper shows how existing methods can be integrated into the structural learning process in order to discredit all continuous variables while learning Bayesian network structures at the same time. The proposed method has been adopted and its superior performance demonstrated empirically. Future work will examine edge positions at locations other than the centers between samples and expand the approach to clusters of categorical variables with very many levels. All software is publicly available at github.com / sisl / LearnDiscreteBayesNets.jl."}], "references": [{"title": "MODL: A Bayes optimal discretization method for continuous attributes", "author": ["Marc Boull\u00e9"], "venue": "Machine Learning,", "citeRegEx": "Boull\u00e9.,? \\Q2006\\E", "shortCiteRegEx": "Boull\u00e9.", "year": 2006}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["Gregory F. Cooper", "Edward Herskovits"], "venue": "Machine Learning,", "citeRegEx": "Cooper and Herskovits.,? \\Q1992\\E", "shortCiteRegEx": "Cooper and Herskovits.", "year": 1992}, {"title": "Supervised and unsupervised discretization of continuous features", "author": ["J. Dougherty", "R. Kohavi", "M. Sahami"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "Dougherty et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dougherty et al\\.", "year": 1995}, {"title": "SMILE: Structural modeling, inference, and learning engine and GeNIe: a development environment for graphical decision-theoretic models", "author": ["Marek J. Druzdzel"], "venue": "In National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Druzdzel.,? \\Q1999\\E", "shortCiteRegEx": "Druzdzel.", "year": 1999}, {"title": "Multi-interval discretization of continuous-valued attributes for classification learning", "author": ["U.M. Fayyad", "K.B. Irani"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Fayyad and Irani.,? \\Q1993\\E", "shortCiteRegEx": "Fayyad and Irani.", "year": 1993}, {"title": "Discretizing continuous attributes while learning Bayesian networks", "author": ["N. Friedman", "M. Goldszmidt"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "Friedman and Goldszmidt.,? \\Q1996\\E", "shortCiteRegEx": "Friedman and Goldszmidt.", "year": 1996}, {"title": "Bayesian network classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine Learning,", "citeRegEx": "Friedman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1997}, {"title": "Minimum Description Length Principle", "author": ["P.D. Gr\u00fcnwald"], "venue": null, "citeRegEx": "Gr\u00fcnwald.,? \\Q2007\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2007}, {"title": "Very simple classification rules perform well on most commonly used datasets", "author": ["R.C. Holte"], "venue": "Machine Learning,", "citeRegEx": "Holte.,? \\Q1993\\E", "shortCiteRegEx": "Holte.", "year": 1993}, {"title": "Nonparametric Bayesian network", "author": ["K. Ickstadt", "B. Bornkamp", "M. Grzegorczyk", "J. Wieczorek", "M.R. Sheriff", "H.E. Grecco", "E. Zamir"], "venue": "Bayesian Statistics,", "citeRegEx": "Ickstadt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ickstadt et al\\.", "year": 2010}, {"title": "ChiMerge: Discretization of numeric attributes", "author": ["R. Kerber"], "venue": "In National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Kerber.,? \\Q1992\\E", "shortCiteRegEx": "Kerber.", "year": 1992}, {"title": "Airspace encounter models for estimating collision risk", "author": ["Mykel J. Kochenderfer", "Matthew W.M. Edwards", "Leo P. Espindle", "James K. Kuchar", "Daniel J. Griffith"], "venue": "AIAA Journal of Guidance, Control, and Dynamics,", "citeRegEx": "Kochenderfer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kochenderfer et al\\.", "year": 2010}, {"title": "Nextgeneration airborne collision avoidance system", "author": ["Mykel J. Kochenderfer", "Jessica E. Holland", "James P. Chryssanthacopoulos"], "venue": "Lincoln Laboratory Journal,", "citeRegEx": "Kochenderfer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kochenderfer et al\\.", "year": 2012}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman.,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman.", "year": 2009}, {"title": "Nonuniform dynamic discretization in hybrid networks", "author": ["A.V. Kozlov", "D. Koller"], "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Kozlov and Koller.,? \\Q1997\\E", "shortCiteRegEx": "Kozlov and Koller.", "year": 1997}, {"title": "Application of an efficient Bayesian discretization method to biomedical data", "author": ["J.L. Lustgarten", "S. Visweswaran", "V. Gopalakrishnan", "G.F. Cooper"], "venue": "BMC Bioinformatics,", "citeRegEx": "Lustgarten et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lustgarten et al\\.", "year": 2011}, {"title": "A multivariate discretization method for learning Bayesian networks from mixed data", "author": ["S. Monti", "G.F. Cooper"], "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Monti and Cooper.,? \\Q1998\\E", "shortCiteRegEx": "Monti and Cooper.", "year": 1998}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q1988\\E", "shortCiteRegEx": "Pearl.", "year": 1988}, {"title": "Modeling by shortest data", "author": ["J. Rissanen"], "venue": "description. Automatica,", "citeRegEx": "Rissanen.,? \\Q1978\\E", "shortCiteRegEx": "Rissanen.", "year": 1978}, {"title": "Learning Bayesian networks with the bnlearn R package", "author": ["Marco Scutari"], "venue": "Journal of Statistical Software,", "citeRegEx": "Scutari.,? \\Q2010\\E", "shortCiteRegEx": "Scutari.", "year": 2010}, {"title": "Predictive discretization during model selection", "author": ["Harald Steck", "Tommi S Jaakkola"], "venue": "In Pattern Recognition,", "citeRegEx": "Steck and Jaakkola.,? \\Q2004\\E", "shortCiteRegEx": "Steck and Jaakkola.", "year": 2004}, {"title": "Minimum description length induction, Bayesianism, and Kolmogorov complexity", "author": ["Paul Vit\u00e1nyi", "Ming Li"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Vit\u00e1nyi and Li.,? \\Q2000\\E", "shortCiteRegEx": "Vit\u00e1nyi and Li.", "year": 2000}, {"title": "Correctness of belief propagation in gaussian graphical models of arbitrary topology", "author": ["Y. Weiss", "W.T. Freeman"], "venue": "Neural Computation,", "citeRegEx": "Weiss and Freeman.,? \\Q2001\\E", "shortCiteRegEx": "Weiss and Freeman.", "year": 2001}], "referenceMentions": [{"referenceID": 12, "context": "Bayesian networks (Pearl 1988; Koller and Friedman 2009) are often used to model uncertainty and causality, with applications ranging from decision-making systems (Kochenderfer et al. 2012) to medical diagnosis (Lustgarten et al.", "startOffset": 163, "endOffset": 189}, {"referenceID": 15, "context": "2012) to medical diagnosis (Lustgarten et al. 2011).", "startOffset": 27, "endOffset": 51}, {"referenceID": 11, "context": "However, many applications require the use of continuous variables, such as position and velocity in dynamic systems (Kochenderfer et al. 2010).", "startOffset": 117, "endOffset": 143}, {"referenceID": 9, "context": "The second approach is to use nonparametric distributions, such as particle representations and Gaussian processes (Ickstadt et al. 2010).", "startOffset": 115, "endOffset": 137}, {"referenceID": 2, "context": "Automated discretization methods have been studied in machine learning and statistics for many years (Dougherty et al. 1995; Kerber 1992; Holte 1993; Fayyad and Irani 1993), primarily for classification problems.", "startOffset": 101, "endOffset": 172}, {"referenceID": 6, "context": "Prior work exists for discretizing continuous variables in naive Bayesian networks and tree-augmented networks (Friedman et al. 1997), but only a few discretization methods for general Bayesian networks have been proposed (Friedman and Goldszmidt 1996; Kozlov and Koller 1997; Monti and Cooper 1998; Steck and Jaakkola 2004).", "startOffset": 111, "endOffset": 133}, {"referenceID": 2, "context": "Automated discretization methods have been studied in machine learning and statistics for many years (Dougherty et al. 1995; Kerber 1992; Holte 1993; Fayyad and Irani 1993), primarily for classification problems. These methods search for the best discretization policy for a continuous attribute by considering its interaction with a class variable. It is common to discretize all continuous variables before learning a Bayesian network structure to avoid having to consider variable interactions, as the interactions and dependencies between variables in Bayesian networks introduce complexity. Prior work exists for discretizing continuous variables in naive Bayesian networks and tree-augmented networks (Friedman et al. 1997), but only a few discretization methods for general Bayesian networks have been proposed (Friedman and Goldszmidt 1996; Kozlov and Koller 1997; Monti and Cooper 1998; Steck and Jaakkola 2004). A common discretization method for Bayesian networks is to split continuous variables into uniform-width intervals or by using field-specific expertise. A more principled method is the minimum description length (MDL) principle discretization (Friedman and Goldszmidt 1996). The MDL principle proposed by Rissanen (1978) states that the best model for a dataset is the one that minimizes the amount of information needed to describe it (Gr\u00fcnwald 2007).", "startOffset": 102, "endOffset": 1243}, {"referenceID": 2, "context": "Automated discretization methods have been studied in machine learning and statistics for many years (Dougherty et al. 1995; Kerber 1992; Holte 1993; Fayyad and Irani 1993), primarily for classification problems. These methods search for the best discretization policy for a continuous attribute by considering its interaction with a class variable. It is common to discretize all continuous variables before learning a Bayesian network structure to avoid having to consider variable interactions, as the interactions and dependencies between variables in Bayesian networks introduce complexity. Prior work exists for discretizing continuous variables in naive Bayesian networks and tree-augmented networks (Friedman et al. 1997), but only a few discretization methods for general Bayesian networks have been proposed (Friedman and Goldszmidt 1996; Kozlov and Koller 1997; Monti and Cooper 1998; Steck and Jaakkola 2004). A common discretization method for Bayesian networks is to split continuous variables into uniform-width intervals or by using field-specific expertise. A more principled method is the minimum description length (MDL) principle discretization (Friedman and Goldszmidt 1996). The MDL principle proposed by Rissanen (1978) states that the best model for a dataset is the one that minimizes the amount of information needed to describe it (Gr\u00fcnwald 2007). MDL methods trade off goodness-of-fit against model complexity to reduce generalization error. In the context of Bayesian networks, Friedman and Goldszmidt (1996) applied the MDL principle to determine the optimal number of discretization intervals for continuous variables and the optimal positions of their discretization edges.", "startOffset": 102, "endOffset": 1538}, {"referenceID": 0, "context": "MODL (Boull\u00e9 2006) is a Bayesian method for discretizing a continuous feature according to a class variable, which selects the model with maximum probability given the data. The MODL method uses dynamic programming to find the optimal discretization policy for a continuous variable given a discrete class variable, and has an O ( n3 + r \u00b7 n2 ) runtime, where r is the number of class variable instantiations. Lustgarten et al. (2011) suggest several formulations for the prior over models.", "startOffset": 6, "endOffset": 435}, {"referenceID": 0, "context": "MODL (Boull\u00e9 2006) is a Bayesian method for discretizing a continuous feature according to a class variable, which selects the model with maximum probability given the data. The MODL method uses dynamic programming to find the optimal discretization policy for a continuous variable given a discrete class variable, and has an O ( n3 + r \u00b7 n2 ) runtime, where r is the number of class variable instantiations. Lustgarten et al. (2011) suggest several formulations for the prior over models. The asymptotic equivalence between MDL and MODL on the single-variable, single-class problem was examined by Vit\u00e1nyi and Li (2000). This paper describes a new Bayesian discretization method for continuous variables in Bayesian networks, extending prior work on single-variable discretization methods from Boull\u00e9 (2006) and Lustgarten et al.", "startOffset": 6, "endOffset": 622}, {"referenceID": 0, "context": "MODL (Boull\u00e9 2006) is a Bayesian method for discretizing a continuous feature according to a class variable, which selects the model with maximum probability given the data. The MODL method uses dynamic programming to find the optimal discretization policy for a continuous variable given a discrete class variable, and has an O ( n3 + r \u00b7 n2 ) runtime, where r is the number of class variable instantiations. Lustgarten et al. (2011) suggest several formulations for the prior over models. The asymptotic equivalence between MDL and MODL on the single-variable, single-class problem was examined by Vit\u00e1nyi and Li (2000). This paper describes a new Bayesian discretization method for continuous variables in Bayesian networks, extending prior work on single-variable discretization methods from Boull\u00e9 (2006) and Lustgarten et al.", "startOffset": 6, "endOffset": 810}, {"referenceID": 0, "context": "MODL (Boull\u00e9 2006) is a Bayesian method for discretizing a continuous feature according to a class variable, which selects the model with maximum probability given the data. The MODL method uses dynamic programming to find the optimal discretization policy for a continuous variable given a discrete class variable, and has an O ( n3 + r \u00b7 n2 ) runtime, where r is the number of class variable instantiations. Lustgarten et al. (2011) suggest several formulations for the prior over models. The asymptotic equivalence between MDL and MODL on the single-variable, single-class problem was examined by Vit\u00e1nyi and Li (2000). This paper describes a new Bayesian discretization method for continuous variables in Bayesian networks, extending prior work on single-variable discretization methods from Boull\u00e9 (2006) and Lustgarten et al. (2011). The proposed method optimizes the discretization policy relative to the network and takes parents, children, and spouse variables into account.", "startOffset": 6, "endOffset": 839}, {"referenceID": 15, "context": "The four principles, which can be considered an extension of the priors in MODL and Lustgarten et al. (2011) to Bayesian networks, are:", "startOffset": 84, "endOffset": 109}, {"referenceID": 0, "context": "For faster methods with suboptimal results, see Boull\u00e9 (2006).", "startOffset": 48, "endOffset": 62}, {"referenceID": 5, "context": "For faster methods of MDL discretization with suboptimal results, see Friedman and Goldszmidt (1996). Note that the preliminary discretization and the discretization order of variables can be arbitrary for the MDL discretization method, as stated in the original work (Friedman and Goldszmidt 1996).", "startOffset": 70, "endOffset": 101}, {"referenceID": 5, "context": "Note that the third term of fMDL had been approximated using H(p) (see Equation 18) in the original work by Friedman and Goldszmidt (1996), but it is written here without that approximation.", "startOffset": 108, "endOffset": 139}], "year": 2015, "abstractText": "Real data often contains a mixture of discrete and continuous variables, but many Bayesian network structure learning and inference algorithms assume all random variables are discrete. Continuous variables are often discretized, but the choice of discretization policy has significant impact on the accuracy, speed, and interpretability of the resulting models. This paper introduces a principled Bayesian discretization method for continuous variables in Bayesian networks with quadratic complexity instead of the cubic complexity of other standard techniques. Empirical demonstrations show that the proposed method is superior to the state of the art. In addition, this paper shows how to incorporate existing methods into the structure learning process to discretize all continuous variables and simultaneously learn Bayesian network structures.", "creator": "TeX"}}}