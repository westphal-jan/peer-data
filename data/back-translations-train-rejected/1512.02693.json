{"id": "1512.02693", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2015", "title": "Reinforcement Control with Hierarchical Backpropagated Adaptive Critics", "abstract": "Present incremental learning methods are limited in the ability to achieve reliable credit assignment over a large number time steps (or events). However, this situation is typical for cases where the dynamical system to be controlled requires relatively frequent control updates in order to maintain stability or robustness yet has some action-consequences which must be established over relatively long periods of time. To address this problem, the learning capabilities of a control architecture comprised of two Backpropagated Adaptive Critics (BACs) in a two-level hierarchy with continuous actions are explored. The high-level BAC updates less frequently than the low-level BAC and controls the latter to some degree. The response of the low-level to high-level signals can either be determined a priori or it can emerge during learning. A general approach called Response Induction Learning is introduced to address the latter case.", "histories": [["v1", "Tue, 8 Dec 2015 23:11:54 GMT  (1002kb,D)", "http://arxiv.org/abs/1512.02693v1", "16 pages, 5 figures"]], "COMMENTS": "16 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG cs.SY", "authors": ["john w jameson"], "accepted": false, "id": "1512.02693"}, "pdf": {"name": "1512.02693.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Control with Hierarchical Backpropagated Adaptive Critics\u2217", "authors": ["John W. Jameson"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "There has recently been considerable interest in the application of reinforcement learning methods to adaptive (or intelligent) control problems. Perhaps the most recognized early work on this topic is a 1983 paper by Barto, Sutton, and Anderson [1], which showed the stabilization of the often-studied basket-pole problem (see Figure 2).In this paper, the authors introduced \"adaptive heuristic critique,\" whose function is to assess the current state in terms of expected future (cumulative) amplifications. Sutton's work on temporal differences [5] refined these terms and extended their application to nonlinear forward-oriented neural networks. His work is the basis for the current \"critical networks,\" which aim is to estimate the sum of future reinforcements (pt): pt = 0 krt + k + 1, (1) This work was supported by Jameson Robotics and the Lyndon Baynes Johnson Space Center."}, {"heading": "2 The Backpropagated Adaptive Critic (BAC)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Background", "text": "While their results were impressive, the discrete nature of the method is impractical for problems that either require high-level control or have a high-dimensional state space. Nevertheless, they achieved stabilization of the cart pole system within an average of seventy attempts - considerably better than any of the continuous approaches that appear in the literature (or here). Anderson [4] later used critical and action modules based on backward propagated multi-layered perceptrons to stabilize the cart pole problem. Although input to the modules was continuous, the associative learning method for training the action network still required (stochastic) binary actions. On average, about 8,000 attempts were required to stabilize the system. Note that an \"attempt\" begins with the cart and cart pole problems when the position of the two exceeds their limits."}, {"heading": "2.2 Architecture: Indirect BAC", "text": "A schematic representation of the BAC is shown in Fig. 1 and consists of three networks: the Action, Model, and Critic Network, indicated by the letters \"A,\" \"M,\" and \"C.\" This architecture, for reasons to be explained shortly, is referred to as the Indirect BAC. For a more detailed description of this architecture, see [3,11]. Note: Arrows that exceed the left boundary of the BAC in Fig. 1 correspond to signals to and from the same BAC, but for the previous time step, and similar arrows that exceed the right boundary refer to the next time. dashed lines represent feedback errors that may occur through a network with or without affecting its weights; if a dashed line within the network blocks the weights, otherwise they are not affected. For the experiments being discussed below all networks, a single hidden layer with (seven) Gaussian activation functions and an output layer with activation functions included."}, {"heading": "2.3 Direct BAC Architecture", "text": "Another form of the BAC, the so-called \"Direct BAC,\" is practically identical to the BAC just described, except that the input of the critic consists of the current state and action - no model network is required. In this case, the action is included as input to the critic network. [1] This particular form of the error gradient is referred to by Sutton [5] as TD (0), where the \"0\" refers to the fact that in Eq.4 no past values of the gradient are used."}, {"heading": "3 Single Level BAC Control with the Cart-Pole Prob-", "text": ""}, {"heading": "3.1 The Cart-pole System", "text": "The condition of this system is fully described by four sizes, which include the state vector xt: the position and speed of the wagon, as well as the angle position and speed of the pole. However, the aim is to keep the reverse pole in balance and to prevent the wagon from hitting on both sides of the track by controlling the force exerted on the wagon (proportional to yt).The equations of motion of the system are: x \ufffd = fc + mpL [\u03b8 2 sin asm] mc + mp (5) constant pole acceleration is balanced by the control of the force exerted on the wagon (proportional to yt).The equations of motion of the system are: x \ufffd = fc + mpL [\u03b8 2 sin asm] mc + mpc.p] (6), whereby, mc \ufffd cos \u03b8] mc + mp + mp (c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c.c..c..c..c...c...c...c....c.c...c.....c....c.........c.......c....c.c..c........c....c.c....c....c......c..c......c.....c....c.c....c.....c.......c......c......c.....c.........c.....c..c..c..........c..c...c.........c..........c..c............c......c......"}, {"heading": "3.2 Results for the Single-Level BAC with Different Servo Rates", "text": "The results of these experiments are presented in Table 1. One experiment consisted of letting the BAC learn until it either stabilized the system (referred to as \"success\" in Table 1) or until the number of experiments reached 1,200 (or 3,000 for the Direct BAC), which constituted a \"failure.\" The parameters for all experiments (except Case 4) were identical; the learning rates for the Action or Critic networks were.01 and 02, respectively. The model network was trained for 1,000 time steps each. Results in Table 1 show that both BACs (indirect and direct) had superior performance for the faster servo rates, largely for the reason that although the shortened pole for a slower servo rate is more difficult to balance (the gains must be subjected to finer fine tuning), less foresight is required by the critic to center the cart. The data for Case 4 in Table 1 comes from Direct and Jacobs, which is very similar to the architecture."}, {"heading": "4 The Two-Level BAC with Explicit Low-level Role", "text": "Simply put, the principle of the two-stage BAC architecture is this: the use of a low level (LL) BAC with a fast servo rate to directly control the actuators for short-term stability and a high level (HL) BAC with a slower servo rate to control the LL BAC tomaximize the (long-term) gain. 2The preferred form of the two-stage action BAC is shown in Figure 3. Only one HL BAC and one LL BAC are used in this architecture. HL BAC updates once for each N LL BAC update where N 40 was for the simulations. 2The HL BAC is virtually identical to the \"standard\" BAC shown in Fig. 2, except for the HL action, Yt, is used as part of the input into the LL, rather than determining the force on the art. Yt is referred to as a \"plan\"; in general it can be a vector, but for examples in this paper it is just a quantity work."}, {"heading": "4.1 Experimental Results", "text": "Table 2 presents results obtained for the cart pole system with a servo rate of 50 Hz, which corresponds to the most difficult of the cases presented in Table 1. Note that the performance of the HL controller in phases I to III was virtually identical to the performance of the system described in [11], where a proportional derived control loop essentially fulfilled the role of the LL controller in this example. Learning performance in phases I to III was very reliable, i.e. no local minimums were encountered over the ten experiments. Table 2 Results for two-stage BAC architectures with explicit low-level function over ten experiments (50 hz servo-rate) Architecture Phase Typical Noverage of Time Steps Indirect BAC I 800 10,000 \"\" \"\" II 130,000 \"III. BAC stages 400 150,000\" \"IV BAC-160 10,000 Direct BAC I 450 500,000 A way to make a direct comparison between the two levels."}, {"heading": "5 Response Induction (RI) Learning", "text": "What happens if, instead of forcing the pole to \"follow the HL plan by an internal LL gain,\" the LL controller gets the same (external) gain as the LL controller? What role does the HL plan, with its influence on the LL controller, play in this case, i.e., what \"sub-task\" does the LL controller require the same (external) gain as the LL controller? I have generally found that the effects of the LL plans on the LL actions have generally become negligible. Since there was no inclusion of the plans in the amplification provided to the LL controller, the plans became interferences to be rejected. One way to get the LL plans to respond to the signals of the HL plan without explicitly expressing how they respond to the weights of the action network is that the eachi-based measures Yi, i-P (9) some significant values are the Lq (see the output Lp)."}, {"heading": "5.1 Experimental Results with RI Learning", "text": "During the experiments, it was generally the case that a change in plan input caused a relatively rapid shift in the pole angle, after which the pole moved smoothly from the new (shifted) position in the direction from which it was started. However, in several experiments, it was observed that the pole was balanced at an angle to the plan signal, just as in the more closely monitored case in sec. 1. After performing a certain degree of (RI) parameter optimization, I performed fifteen experiments in which RI learning was used during Phase II for the two-stage indirect BAC; all other parameters were identical to those used to achieve the results indicated in Table 2. During Phase II of RI learning and Phase III of HL modeling, uniformly random HL actions were performed in the areas [-.3] and [-.7,.7]."}, {"heading": "5.2 Outlook on RI Learning", "text": "Getting the LL Action Network to simply react to the plan signals while maximizing its local amplification seems crude in that the type of learned response may not necessarily be useful for HL's performance. However, because it is so general, it gives the LL Action Network considerable leeway to maximize the amplification. This leeway could perhaps be used most effectively by adding factors such as space exploration and / or trajectory smoothing to the \"ecological\" amplification signal. It might be useful to construct separate critics solely for the purpose of reaction induction so that a new critic can respond to some of the above factors. In any case, for most applications it seems most desirable that the LL behavior in relation to HL actions is deterministic and that government space is adequately covered."}, {"heading": "6 Role Learning Based on Time Constant Identification", "text": "Another potential way to produce a meaningful LL role is more directly related to learning the \"explicit function\" in paragraph 3. This would mean that the intelligent controller would have to figure out for himself which components of the plant state variables are \"slow\" and which are \"fast\" and the assignment control is appropriate (e.g. how the control in paragraph 3 was assigned). This would not be as general as RI learning, since certain state variables need to be identified and directed to different levels of control, while in RI learning different functions of the state variables can be implicitly passed on. However, it seems that this might be more practical in the short term than RI learning."}, {"heading": "7 Conclusion", "text": "In this case, most of them are able to determine for themselves what they want and what they want to do."}], "references": [{"title": "Neuronlike Elements That Can Solve Difficult Control Problems", "author": ["A.G. Barto", "R.S. Sutton", "C.W. Anderson"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1983}, {"title": "Beyond Regression, New Tools for Prediction and Analysis in the Behavioral Sciences, Ph.D", "author": ["P.J. Werbos"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1989}, {"title": "A Menu of Designs for Reinforcement Learning Over Time, Neural Networks for Control", "author": ["P.J. Werbos"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "Strategy Learning with Multilayer Connectionist Representations", "author": ["C.W. Anderson"], "venue": "Tech. Report TR87-509.3,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Learning to Predict by the Methods of Temporal Differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "Temporal Credit Assignment in Reinforcement Learning, Ph.D", "author": ["R.S. Sutton"], "venue": "Thesis, Dept. of Computer and Information Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1984}, {"title": "Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming,", "author": ["Sutton", "R. S"], "venue": "Proc. 7th Int. Conf. on Machine Learning", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1990}, {"title": "The Truck Backer-Upper: An Example of Self-Learning", "author": ["D. Nguyen", "B. Widrow"], "venue": "Proc. IEEE Int. Conf. on Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Neural Network Based Process Optimization and Control", "author": ["Sofge", "D.A.D.A. White"], "venue": "Proc. 29th Conf. on Decision and Control,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "A Neurocontroller Based on Model Feedback and the Adaptive Heuristic Critic", "author": ["J.W. Jameson"], "venue": "Proc. IEEE Int. Conf. on Neural Networks, San Diego,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "Learning to Control an Unstable System with Forward Modeling", "author": ["M.I. Jordan", "R.A. Jacobs"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1989}, {"title": "Learning from Delayed Rewards, Ph.D", "author": ["C.J. Watkins"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "Real Time Application of Neural Networks for Sensor- Based Control of Robots with Vision", "author": ["W.T. Miller"], "venue": "IEEE Trans. on Sys., Man, and Cybernetics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1989}, {"title": "Making the World Differentiable: On Using Supervised Learning Fully Recurrent Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments, Report FKI-126-90", "author": ["J.H. Schmidhuber"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}], "referenceMentions": [{"referenceID": 0, "context": "Perhaps the most recognized early work on this subject is a 1983 paper by Barto, Sutton, and Anderson [1] which demonstrated stabilization of the oft-studied cart-pole problem (see Figure 2).", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "Sutton\u2019s work on temporal differences [5] refined these notions, and extended their application to nonlinear feedforward neural networks.", "startOffset": 38, "endOffset": 41}, {"referenceID": 11, "context": "Watkins [13] presented an interesting paradigm for hierarchical control based on ship navigation which has several parallels to, the present work.", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "Anderson [4] later used critic and action modules based on backpropagated multilayer perceptrons for stabilization of the cart-pole problem.", "startOffset": 9, "endOffset": 12}, {"referenceID": 1, "context": "Werbos [2, 3] introduced an architecture called the Backpropagated Adaptive Critic (BAC) for which a utility function similar to a critic is used as a guide for training the action module.", "startOffset": 7, "endOffset": 13}, {"referenceID": 2, "context": "Werbos [2, 3] introduced an architecture called the Backpropagated Adaptive Critic (BAC) for which a utility function similar to a critic is used as a guide for training the action module.", "startOffset": 7, "endOffset": 13}, {"referenceID": 10, "context": "Jordan and Jacobs [12] and Jameson [11] provided some of the first", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "Jordan and Jacobs [12] and Jameson [11] provided some of the first", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "For a more detailed description of this architecture, see [3,11].", "startOffset": 58, "endOffset": 64}, {"referenceID": 9, "context": "For a more detailed description of this architecture, see [3,11].", "startOffset": 58, "endOffset": 64}, {"referenceID": 4, "context": "which corresponds to the following equation for changing the weights of the critic network (see [5] for further clarification):", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "However, feedback is performed with activities corresponding to zero noise (see [3]).", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "This particular form of the error gradient is referred to as TD(0) by Sutton [5], where the \u201c0\u201d refers to the fact that no past values of the gradient are used in eq.", "startOffset": 77, "endOffset": 80}, {"referenceID": 9, "context": ", r = \u22121 when a failure occurs, or r = 0 otherwise (see [11] for more detailed performance comparisons).", "startOffset": 56, "endOffset": 60}, {"referenceID": 9, "context": "This fact motivated me to perform similar experiments as my earlier paper [11] but with a slower servo rate, i.", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "The data for case 4 in Table 1 was obtained from Jordan and Jacobs [12], who used an architecture very similar to the Direct BAC but with a different reinforcement signal (related to the time to failure).", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "of time steps prior to successful trial dfrom Jordan and Jacobs [12]", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "As Watkins pointed out [13], if the LL controller is subject to state transitions that are not entirely caused by its own actions then the Markov property does not hold for the LL controller.", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "Note that the performance of the HL controller in phase IV learning was virtually identical to the performance of the system described in [11], where a proportional derivative control loop essentially fulfilled the role of the LL controller in the present example.", "startOffset": 138, "endOffset": 142}, {"referenceID": 7, "context": ", the feedback in time method [8]).", "startOffset": 30, "endOffset": 33}], "year": 2015, "abstractText": "Present incremental learning methods are limited in the ability to achieve reliable credit assignment over a large number time steps (or events). However, this situation is typical for cases where the dynamical system to be controlled requires relatively frequent control updates in order to maintain stability or robustness yet has some action/consequences which must be established over relatively long periods of time. To address this problem, the learning capabilities of a control architecture comprised of two Backpropagated Adaptive Critics (BAC\u2019s) in a two-level hierarchy with continuous actions are explored. The high-level BAC updates less frequently than the low-level BAC and controls the latter to some degree. The response of the low-level to high-level signals can either be determined a priori or it can emerge during learning. A general approach called Response Induction Learning is introduced to address the latter case.", "creator": "LaTeX with hyperref package"}}}