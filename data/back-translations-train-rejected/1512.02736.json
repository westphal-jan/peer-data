{"id": "1512.02736", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Dec-2015", "title": "Window-Object Relationship Guided Representation Learning for Generic Object Detections", "abstract": "In existing works that learn representation for object detection, the relationship between a candidate window and the ground truth bounding box of an object is simplified by thresholding their overlap. This paper shows information loss in this simplification and picks up the relative location/size information discarded by thresholding. We propose a representation learning pipeline to use the relationship as supervision for improving the learned representation in object detection. Such relationship is not limited to object of the target category, but also includes surrounding objects of other categories. We show that image regions with multiple contexts and multiple rotations are effective in capturing such relationship during the representation learning process and in handling the semantic and visual variation caused by different window-object configurations. Experimental results show that the representation learned by our approach can improve the object detection accuracy by 6.4% in mean average precision (mAP) on ILSVRC2014. On the challenging ILSVRC2014 test dataset, 48.6% mAP is achieved by our single model and it is the best among published results. On PASCAL VOC, it outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolute mAP.", "histories": [["v1", "Wed, 9 Dec 2015 03:32:21 GMT  (402kb,D)", "http://arxiv.org/abs/1512.02736v1", "9 pages, including 1 reference page, 6 figures"]], "COMMENTS": "9 pages, including 1 reference page, 6 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["xingyu zeng", "wanli ouyang", "xiaogang wang"], "accepted": false, "id": "1512.02736"}, "pdf": {"name": "1512.02736.pdf", "metadata": {"source": "CRF", "title": "Window-Object Relationship Guided Representation Learning for Generic Object Detections", "authors": ["Xingyu ZENG", "Wanli OUYANG", "Xiaogang WANG"], "emails": ["xyzeng@ee.cuhk.edu.hk", "wlouyang@ee.cuhk.edu.hk", "xgwang@ee.cuhk.edu.hk"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that you are able to be in a position without being able to play by the rules."}, {"heading": "2. Relative Work", "text": "RCNN [7] is a widely used object detection pipeline based on CNN [17, 20, 14], which first educates representation by classifying 1.2 million images from ImageNet into 1000 categories and then fine-tuning them by classifying object detection boxes on the target detection dataset. People improved RCNN by suggesting better structures from CNN [20, 17]. Ouyang et al. [14] improved pre-training by classifying the bounding boxes of images from ImageNet instead of the entire images. All of this work presented the problem of representation without effort in exploring the window-object relationship. A group of papers attempted to solve detection with regression [21, 19, 23]. Given the entire image as input, Szegedy et al. [21] used DNN to reform the binary masks of an object-bounding box and its subboxes."}, {"heading": "3. Method", "text": "In order to give readers a clear picture of the overall framework, we will first explain the object detection pipeline in the test phase. The most important contributions come from presentation learning, the details of which are explained in Section 3.2 - Section 3.5."}, {"heading": "3.1. Object detection at the testing stage", "text": "As in Fig. 3, the object detection pipeline is treated as follows: 1) The selective search in [18] is used to obtain candidate windows. 2) A candidate window is used to extract properties as follows: 2.1) For a candidate window, the properties of a candidate are selected as follows: (x, y, H) with the size (W, H) and the center (x, y), the crop images and the candidate window have the same central location (x, y). \u03bb is the scale of a contextual region. The selection of the set scale characteristics is detailed in Section 3.2. 2.2) The crop image is rotated by degrees r and center (x, y) and covered with the surrounding context to I (r, Goobs), R = {0, gleector), R = = (45, gleector f)."}, {"heading": "3.2. Representation learning pipeline", "text": "Our proposed pipeline is structured as follows and in Fig. 5. a) Pretrain CNN using the ImageNet 1000 class classification and localization data. b) Train CNN, which was trained for initialization in the previous step. Train CNN by estimating the window-object relationship. Details are in Section 3.3. c) Use CNN, which was trained for initialization in the previous step. Train CNN by estimating the window-multi-object relationship. Details are in Section 3.4. d) Use CNN, which was trained for initialization in the previous step. Train CNN for the C + 1 classification problem. C is the number of object classes plus 1 for the background. C = 20 for PASCAL VOC and C = 200 for ILSVRC2014. Details are in Section 3.5. Since the above pipeline is used for displaying learning functions, except for the difference in the output layer, the network structures are treated as the last training layer for all of the above-mentioned responses to CNN."}, {"heading": "3.2.1 Window-object relationship label preparation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3. Learning the window-object relationship", "text": "The idea is that CNN distinguishes candidate windows that contain different parts of the same object or have different sizes. For example, one candidate window that contains a person's torso and another that contains the legs is classified as the same category (i.e., \"person\") of non-existent works, but in our approach is considered to be different configurations of the window-object relationship. To distinguish candidate windows of the same object class, we group training samples in each class into subsets with similar relative positions. Use bi, s = (xi, s, yi, s, candidate, s, Hi, s) to indicate the i-th candidate window at the training stage with center (xi, s, yi, s) and size (Wi, s, s, Hi, s)."}, {"heading": "3.3.1 Loss function of window-object relationship", "text": "Using the CNN parameters obtained as initialization from step a) in Section 3.3, we continue to train the CNN prediction of the window-object relationship; the CNN classification layer in step a) is replaced by two fully connected (fc) layers; the other layer, which predicts the location and size of the cluster n, is called the cluster prediction layer, using the last CNN feature extraction layer as the input layer; the output dimension of the localization layer is 4N; and the output dimension of the layer that n-i outputs is N. Softmax is used for the cluster prediction layer; both layers use the last CNN feature extraction layer as input.The output dimension of the localization layer is 4N and the output dimension of the layer that n-i outputs is N. Softmax is used for cluster prediction."}, {"heading": "3.3.2 Multi-context and multi-rotation", "text": "If the location and size of a candidate window differs from that of the floor truth limiting field, the candidate window will have only partial visual content of the object. The limited view causes difficulties for CNN to figure out the visual difference between the object classes. For example, it is hard to tell whether it is an iPod or a monitor if you can only see the screen, but it becomes much easier when the entire object and its contextual region is provided, as in Fig. 6 (top row). If occlusion happens, the floor truth limiting boxes may contain different amount of object parts and therefore have different sizes. Without a region that is larger than the floor truth as input, it is confusing for CNN to decide the interface field size. 6 (bottom row), the floor truth field for a standing uninvolved person should cover more parts of the human body than those covered with legs. If the image region cropped off by a candidate window only covers the upper body."}, {"heading": "3.4. Window-multi-object relationship prediction", "text": "The previous training step does not take into account the coexistence of multiple object instances in the same image, which often happens and forms layout configurations. For the example in Fig. 5, the person has a helmet on his head and a rugby ball in his arms. To further enrich the representation of the feature, we expand the training sequence by predicting the window-multi-objects relationship. The window-multi-objects relationship can be formulated in answering three basic questions about whether other instances exist in the neighborhood, where they are and what they are.We start with the relative position and size li, loc defined in (2), to describe the pairwise relationship between a candidate window and multiple objects.The li, loc for all ground truth boxes are used as characteristics to obtain K clusters that are used for describing the window's multi-objects layout.Given a candidate window, its surrounding truth objects are listed in their cluster type and are assigned to each of the corresponding K labels."}, {"heading": "4. Experimental results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Experimental setting", "text": "The implementation of our framework uses GoogleNet [20] as a CNN structure. The 1000-class pre-training is based on the ILSVRC2014 classification and localization dataset, and the learned representations are evaluated on the two following datasets. Most component analysis evaluations of our training pipeline are performed on ILSVRC2014, as this dataset is much larger and contains more object categories, and the overall results and state-of-the-art comparison are ultimately evaluated on both datasets.The ILSVRC2014 dataset contains 200 object categories and is divided into three subsets, i.e. train, validation and test datas.The validation subset is divided into Val1 and Val2 in [7].We follow the same settings.In training step d) we use both train and Val1 subsets, but in training steps b) and c) we use only Val1 subsets, as many of the positive subsets are not included in the window sample set, and they may be most common in the window sample set."}, {"heading": "4.2. Component analysis on the training pipeline", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Comparison with baselines", "text": "In order to evaluate the effectiveness of our key innovations, several baselines are compared based on ILSVRC2014 and the results are summarized in Table 1. (1) RCNN chooses GoogLeNet as the CNN structure. It corresponds to the distance of steps b) and c) in our training pipeline and only takes the single context of scale 1.2 without rotation as input into the test. (2) Since our method combines features from seven GoogLeNets, one can question the improvement from the model average. This baseline randomly initializes seven GoogLeNets, associates their features to train SVM and follow the RCNN pipeline. (3) Let's take multicontext and multirotation inputs (i.e. the use of the test pipeline in Figure 3) without monitoring the window-object relationship (i.e., the distance from step b) and c) from our training pipeline. (4) Our pipeline with a single context of scale 1.2 and without rotation (i.e., our relationship of the window-object)."}, {"heading": "4.2.2 Clustering window-object relationship", "text": "The window-object relationship is bundled in our approach as introduced in Section 3.3. Its effectiveness is evaluated in this section. Steps a), b) and d) are used. The cropped image has only one setting for rotation and scaling, i.e. (r, \u03bb) = (0 \u0445, 1,2), which is the default setting in [7, 6]. If only steps a) and d) are used, this corresponds to the RCNN baseline. If the window-object ratio is not used, the relative location designations and the object class are used for learning characteristics, which is the scheme in Fast RCNN [6], the improvement of mAP [6] is 0.2%. With clustering, the improvement of mAP is 1.2%. Step b) is less effective without clusters and brings only 0.2% improvement alone. Without clusters, a single regressor is learned for each class, relative places and sizes cannot be accurately predicted, and the learned characteristics are less effective."}, {"heading": "4.2.3 Investigation on using multiple scales", "text": "Based on the training pipeline with steps a) + b) + d) with the clustering of window-object relationships, Table 3 shows the impact of using multiple scales. The four-scale network has a MAP improvement of 45.5% compared to a single scale. Based on the 1.2 scale, the improvements in MAP are enhanced by an additional scale in descending order of 2.7, 1.8, and 0.8. This is common sense: A larger contextual region is more helpful in eliminating visual similarities between candidate fields of different categories. Other scales offer better performance, showing that the characteristic representations learned with different scales complement each other. To find out the effectiveness of using multiple contextual scales for feature learning, we also perform the configuration in which network parameters are divided and fixed for all four scales that are trained in Scale 1.2. If a common network is used that uses more contextual scales from Scale 1.2, we also perform the contextual use of multiple networks while showing different contextual uses."}, {"heading": "4.2.4 Investigation on rotation", "text": "Table 4 shows the experimental results when using multiple rotation degrees and scales. Table 4 shows that rotation improves card performance by 2.0% on one scale and 0.3% on multiple scales."}, {"heading": "4.3. Overall results", "text": "Ouyang et al. [14] showed that pre-training CNN with Bounding Boxes of objects instead of whole images in step a) could significantly improve detection accuracy. It is also known that using the Bounding Box Regression [7] to refine the positions of candidate windows in the final step of the detection pipeline is effective. To compete with the state of the art, we integrate the two existing technologies into our framework to increase performance in final evaluation. Table 5 summarizes the best placed results of V2 and test records from the ILSVRC2014 object thermal challenge and demonstrates the effectiveness of our training pipeline. Flair [22] was the winner of ILSCRC2013. GoogleNet, DeepID-Net, DeepInsight, UvA-Euvision and Berkeley Vision were the top-ranked participants of ILSVRC2014 and GoogleNet was the winner. Table 6 reports on the results of PASCAL-type VOC-12 of the training strategy (Fast VOC-Da-Strategy)."}, {"heading": "5. Conclusion", "text": "This paper proposes a training pipeline that uses the window-object relationship to enhance imaging learning. To help CNN assess these relationships, multiple scales of contextual information and rotations are used. Extensive experimental evaluations of the ILSVRC14 object recognition dataset confirm the improvement over the proposed training pipeline. Our approach exceeds the state-of-the-art for both ILSVRC14 and PASCAL VOC07 datasets."}], "references": [{"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "BMVC,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "ICML, pages 647\u2013655,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Clustering by passing messages between data points", "author": ["B.J. Frey", "D. Dueck"], "venue": "science, 315(5814):972\u2013976,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Object detection via a multiregion & semantic segmentation-aware cnn model", "author": ["S. Gidaris", "N. Komodakis"], "venue": "arXiv preprint arXiv:1505.01749,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast r-cnn", "author": ["R. Girshick"], "venue": "arXiv preprint arXiv:1504.08083,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ECCV.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "ICLR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Distinctive image features from scale-invarian keypoints", "author": ["D. Lowe"], "venue": "IJCV, 60(2):91\u2013110,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns", "author": ["T. Ojala", "M. Pietikainen", "T. Maenpaa"], "venue": "IEEE Trans. PAMI, 24(7):971\u2013987,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Deepid-net: multi-stage and deformable deep convolutional neural networks for object detection", "author": ["W. Ouyang", "P. Luo", "X. Zeng", "S. Qiu", "Y. Tian", "H. Li", "S. Yang", "Z. Wang", "Y. Xiong", "C. Qian"], "venue": "arXiv preprint arXiv:1409.3505,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "IJCV,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Segmentation as selective search for object recognition", "author": ["A. Smeulders", "T. Gevers", "N. Sebe", "C. Snoek"], "venue": "ICCV,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Object detection using deep neural networks, 2015. US Patent 20,150,170,002", "author": ["C. Szegedy", "D. Erhan", "A.T. Toshev"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for object detection", "author": ["C. Szegedy", "A. Toshev", "D. Erhan"], "venue": "NIPS,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Fisher and vlad with flair", "author": ["K.E.A. van de Sande", "C.G.M. Snoek", "A.W.M. Smeulders"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Attentionnet: Aggregating weak directions for accurate object detection", "author": ["D. Yoo", "S. Park", "J.-Y. Lee", "A. Paek", "I.S. Kweon"], "venue": "arXiv preprint arXiv:1506.07704,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Generic object detection with dense neural patterns and regionlets", "author": ["W.Y. Zou", "X. Wang", "M. Sun", "Y. Lin"], "venue": "BMVC,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "4% in mean average precision (mAP) on ILSVRC2014 [15].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "On the challenging ILSVRC2014 test dataset [15], 48.", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "On PASCAL VOC, it outperforms the state-of-the-art result of Fast RCNN [6] by 3.", "startOffset": 71, "endOffset": 74}, {"referenceID": 11, "context": "Initially, researchers employed manually designed features [12, 2, 13].", "startOffset": 59, "endOffset": 70}, {"referenceID": 1, "context": "Initially, researchers employed manually designed features [12, 2, 13].", "startOffset": 59, "endOffset": 70}, {"referenceID": 12, "context": "Initially, researchers employed manually designed features [12, 2, 13].", "startOffset": 59, "endOffset": 70}, {"referenceID": 8, "context": "Recent works [9, 10, 17, 11, 1] have demonstrated the power of learning features with deep neural networks from large-scale data.", "startOffset": 13, "endOffset": 31}, {"referenceID": 9, "context": "Recent works [9, 10, 17, 11, 1] have demonstrated the power of learning features with deep neural networks from large-scale data.", "startOffset": 13, "endOffset": 31}, {"referenceID": 16, "context": "Recent works [9, 10, 17, 11, 1] have demonstrated the power of learning features with deep neural networks from large-scale data.", "startOffset": 13, "endOffset": 31}, {"referenceID": 10, "context": "Recent works [9, 10, 17, 11, 1] have demonstrated the power of learning features with deep neural networks from large-scale data.", "startOffset": 13, "endOffset": 31}, {"referenceID": 0, "context": "Recent works [9, 10, 17, 11, 1] have demonstrated the power of learning features with deep neural networks from large-scale data.", "startOffset": 13, "endOffset": 31}, {"referenceID": 6, "context": "It advances the state-of-the-art of object detection substantially [7, 16, 24, 8, 20, 14].", "startOffset": 67, "endOffset": 89}, {"referenceID": 15, "context": "It advances the state-of-the-art of object detection substantially [7, 16, 24, 8, 20, 14].", "startOffset": 67, "endOffset": 89}, {"referenceID": 23, "context": "It advances the state-of-the-art of object detection substantially [7, 16, 24, 8, 20, 14].", "startOffset": 67, "endOffset": 89}, {"referenceID": 7, "context": "It advances the state-of-the-art of object detection substantially [7, 16, 24, 8, 20, 14].", "startOffset": 67, "endOffset": 89}, {"referenceID": 19, "context": "It advances the state-of-the-art of object detection substantially [7, 16, 24, 8, 20, 14].", "startOffset": 67, "endOffset": 89}, {"referenceID": 13, "context": "It advances the state-of-the-art of object detection substantially [7, 16, 24, 8, 20, 14].", "startOffset": 67, "endOffset": 89}, {"referenceID": 6, "context": "ered as a multi-class problem [7, 3], in which a candidate window is classified as containing an object of category c or background, decided by thresholding the overlap between the candidate window and the ground truth bounding box.", "startOffset": 30, "endOffset": 36}, {"referenceID": 2, "context": "ered as a multi-class problem [7, 3], in which a candidate window is classified as containing an object of category c or background, decided by thresholding the overlap between the candidate window and the ground truth bounding box.", "startOffset": 30, "endOffset": 36}, {"referenceID": 6, "context": "RCNN [7] is a widely used object detection pipeline based on CNN [17, 20, 14].", "startOffset": 5, "endOffset": 8}, {"referenceID": 16, "context": "RCNN [7] is a widely used object detection pipeline based on CNN [17, 20, 14].", "startOffset": 65, "endOffset": 77}, {"referenceID": 19, "context": "RCNN [7] is a widely used object detection pipeline based on CNN [17, 20, 14].", "startOffset": 65, "endOffset": 77}, {"referenceID": 13, "context": "RCNN [7] is a widely used object detection pipeline based on CNN [17, 20, 14].", "startOffset": 65, "endOffset": 77}, {"referenceID": 19, "context": "People improved RCNN by proposing better structures of CNN [20, 17].", "startOffset": 59, "endOffset": 67}, {"referenceID": 16, "context": "People improved RCNN by proposing better structures of CNN [20, 17].", "startOffset": 59, "endOffset": 67}, {"referenceID": 13, "context": "[14] improved pre-training by classifying the bounding boxes of the images from ImageNet instead of the whole images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21, 19, 23].", "startOffset": 0, "endOffset": 12}, {"referenceID": 18, "context": "[21, 19, 23].", "startOffset": 0, "endOffset": 12}, {"referenceID": 22, "context": "[21, 19, 23].", "startOffset": 0, "endOffset": 12}, {"referenceID": 20, "context": "[21] used DNN to regress the binary masks of an object bounding box and its subboxes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] used CNN to directly predict the coordinates of object bounding boxes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "AttentionNet [23] initially treated the whole image as a bounding box, and iteratively refined it.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "The recently proposed Fast RCNN [6] jointly predicted object categories and locations of candidate windows as multi-task learning.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "The related work [5] cropped multiple subregions as the input of CNN.", "startOffset": 17, "endOffset": 20}, {"referenceID": 17, "context": "3, the object detection pipeline is as follows: 1) Selective search in [18] is adopted to obtain candidate windows.", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "In the experiments, the structure of CNN is chosen as GoogleNet [20] for different settings of (r, \u03bb).", "startOffset": 64, "endOffset": 68}, {"referenceID": 6, "context": "The steps are similar to RCNN [7] except for multi-context and multi-rotation input.", "startOffset": 30, "endOffset": 33}, {"referenceID": 17, "context": "Candidate windows at the training stage are from selective search [18] and ground truth bounding boxes.", "startOffset": 66, "endOffset": 70}, {"referenceID": 3, "context": "With features li,loc, affinity propagation(AP) [4] is used to group candidate windows with similar window-object relationship into N clusters.", "startOffset": 47, "endOffset": 50}, {"referenceID": 6, "context": "2 is the only scale chosen in [7] and is set as default value in many existing works.", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "Once features are learned, we fix the CNN parameters and learn 200 classspecific linear SVMs for object detection as in [7].", "startOffset": 120, "endOffset": 123}, {"referenceID": 19, "context": "The implementation of our framework adopts GoogleNet [20] as CNN structure.", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "The validation subset is split into val1 and val2 in [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "Following the most commonly used approach in [7], we finetune the network with the trainval set and evaluate the performance on the test set.", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "2), which is the standard setting used in [7, 6].", "startOffset": 42, "endOffset": 48}, {"referenceID": 5, "context": "2), which is the standard setting used in [7, 6].", "startOffset": 42, "endOffset": 48}, {"referenceID": 5, "context": "If window-object relationship clustering is not used, the relative location and object class labels are used for learning features, which is the scheme in Fast RCNN [6], the mAP improvement is 0.", "startOffset": 165, "endOffset": 168}, {"referenceID": 21, "context": "approach Flair [22] RCNN[7] Berkeley Vision UvA-Euvision DeepInsight DeepID-Net GoogleNet ours val2(sgl) n/a 31.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "approach Flair [22] RCNN[7] Berkeley Vision UvA-Euvision DeepInsight DeepID-Net GoogleNet ours val2(sgl) n/a 31.", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "RCNN and FRCN results come from [6].", "startOffset": 32, "endOffset": 35}, {"referenceID": 13, "context": "[14] showed that pre-training CNN with bounding boxes of objects instead of whole images in step a) could improve the detection accuracy significantly.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "It is also well known that using the bounding box regression [7] to refine the locations of candidate windows in the last step of the detection pipeline is effective.", "startOffset": 61, "endOffset": 64}, {"referenceID": 21, "context": "Flair [22] was the winner of ILSCRC2013.", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "Since the state-of-art approach Fast RCNN (FRCN) [6] reported their performance of models trained on both VOC07 trainval and VOC12 trainval, we also evaluate our approach with the same training strategy.", "startOffset": 49, "endOffset": 52}], "year": 2015, "abstractText": "In existing works that learn representation for object detection, the relationship between a candidate window and the ground truth bounding box of an object is simplified by thresholding their overlap. This paper shows information loss in this simplification and picks up the relative location/size information discarded by thresholding. We propose a representation learning pipeline to use the relationship as supervision for improving the learned representation in object detection. Such relationship is not limited to object of the target category, but also includes surrounding objects of other categories. We show that image regions with multiple contexts and multiple rotations are effective in capturing such relationship during the representation learning process and in handling the semantic and visual variation caused by different window-object configurations. Experimental results show that the representation learned by our approach can improve the object detection accuracy by 6.4% in mean average precision (mAP) on ILSVRC2014 [15]. On the challenging ILSVRC2014 test dataset [15], 48.6% mAP is achieved by our single model and it is the best among published results. On PASCAL VOC, it outperforms the state-of-the-art result of Fast RCNN [6] by 3.3% in absolute mAP.", "creator": "LaTeX with hyperref package"}}}