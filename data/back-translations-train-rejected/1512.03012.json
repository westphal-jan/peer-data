{"id": "1512.03012", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Dec-2015", "title": "ShapeNet: An Information-Rich 3D Model Repository", "abstract": "We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.", "histories": [["v1", "Wed, 9 Dec 2015 19:42:48 GMT  (1179kb,D)", "http://arxiv.org/abs/1512.03012v1", null]], "reviews": [], "SUBJECTS": "cs.GR cs.AI cs.CG cs.CV cs.RO", "authors": ["angel x chang", "thomas funkhouser", "leonidas guibas", "pat hanrahan", "qixing huang", "zimo li", "silvio savarese", "manolis savva", "shuran song", "hao su", "jianxiong xiao", "li yi", "fisher yu"], "accepted": false, "id": "1512.03012"}, "pdf": {"name": "1512.03012.pdf", "metadata": {"source": "CRF", "title": "ShapeNet: An Information-Rich 3D Model Repository", "authors": ["Angel X. Chang", "Thomas Funkhouser", "Leonidas Guibas", "Pat Hanrahan", "Qixing Huang", "Zimo Li", "Silvio Savarese", "Manolis Savva", "Shuran Song", "Hao Su", "Jianxiong Xiao", "Li Yi", "Fisher Yu"], "emails": ["msavva@cs.stanford.edu", "haosu@cs.stanford.edu"], "sections": [{"heading": "1. Introduction", "text": "In recent years, the number of those who are able to significantly increase has multiplied; in recent years, the number of those who are able to increase has doubled; in recent years, the number of those who are able to increase has doubled; in recent years, the number of those who are able to skyrocket has skyrocketed; in recent years, the number of those who are able to skyrocket has skyrocketed; and in recent years, the number of those who are able to skyrocket has skyrocketed."}, {"heading": "2. Background and Related Work", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight,"}, {"heading": "3. ShapeNet: An Information-Rich 3D Model", "text": "RepositoryShapeNet is a large, informative repository of 3D models. It contains models that encompass a variety of semantic categories. Unlike previous 3D models, it provides extensive annotations on each model and links between models in the repository and other multimedia data outside the repository. Like ImageNet, ShapeNet provides an overview of the data contained in a hierarchical categorization by WordNet synsets (Figure 1). Unlike other models, ShapeNet also provides a variety of annotations on each shape and matches between shapes. The annotations include geometric attributes such as upright and anterior orientation vectors, parts and key points, form symmetries (reflective plane, other rotational symmetries) and scales of objects in real units. These attributes provide valuable resources for processing, understanding and visualizing 3D shapes in a way that is aware of hierarchical."}, {"heading": "3.1. Data Collection", "text": "The raw 3D model data for ShapeNet comes from online public repositories or existing research datasets. ShapeNet is supposed to be an evolving repository with regular updates as more 3D models become available as more and more people contribute annotations, and as the data captured with new 3D sensors prevails. We have collected 3D polygonal models from two popular public repositories: Trimble 3D Warehouse1 and Yobi3D2. The Trimble 3D Warehouse contains 2.4M custom 3D models and scenes. Yobi3D contains 350K additional models collected from a wide range of other online repositories. Together, they provide a diverse set of 1https: / / 3dwarehouse.sketchup.com / 2https: / yobi3d.com shapes shapes forms from a wide range of object and scene categories - e.g. many organic form categories (e.g. commodities, humans and mammals) that are abundant in Yobi3s."}, {"heading": "3.2. Annotation Types", "text": "This year it is more than ever before."}, {"heading": "3.3. Annotation Methodology", "text": "While at first glance it may seem reasonable to collect the annotations we describe solely through manual human effort, we will generally adopt a hybrid approach. If possible, we will first algorithmically predict the annotations for each model instance (e.g. global symmetry planes, consistent rigid alignments) for note types, which will then be verified by crowd-sourcing pipelines and inspections by human experts. This hybrid strategy makes sense in the context of 3D shape data, as there are already various algorithms we can use, and collecting corresponding annotations can be extremely labor-intensive. In particular, since objects in 3D representation are both purer and more complete than objects in images, we can expect better and simpler correspondences between 3D shapes, allowing an algorithmic transport of semantic annotations."}, {"heading": "3.4. Annotation Schema and Web API", "text": "To ensure convenient access to all the model and annotation data contained in ShapeNet, we create an index of all 3D models and their annotations using the Apache Solr framework. [3] Each annotation stored for a specific 3D model is included in the index as a separate attribute, which can be easily queried and filtered via a simple web-based user interface. To make the dataset easily accessible to researchers, we also offer a batch download function."}, {"heading": "4. Annotation Acquisition and Validation", "text": "A key challenge in creating ShapeNet is the methodology for collecting and validating notes. Our goal is to provide all notes with a high level of accuracy. In cases where full verification is not yet available, we strive to obtain a confidence metric for each note and to record its provenance, which will allow others to accurately assess the trustworthiness of the information we provide and use it for various applications."}, {"heading": "4.1. Category Annotation", "text": "As described in Section 3.2, we assign each 3D model to one or more synsets in the WordNet taxonomy. Annotation models are retrieved by textual query in the online repositories we collect, and the initial category comment is determined by the textual query used for each retrieval.3http: / / lucene.apache.org / solr / model. After retrieving these models, we use the popularity rating of each model in the repository to sort models and ask human staff to verify the category comment assigned to them. This makes sense, as the more popular models tend to be of high quality and to be retrieved correctly by the category keyword text query. We stop verifying category comments with humans when the positive ratio is less than 2%."}, {"heading": "4.2. Hierarchical Rigid Alignment", "text": "The goal of this step is to establish a unified canonical orientation for models within each category. Such orientation is important for various tasks such as visualizing shapes, shape classification and shape recognition. Figure 3 shows several categories in ShapeNet that have been consistently aligned. Although the concept of unified orientation seems natural, a problem needs to be addressed. We explain using an example. \"Armchair,\" \"chair\" and \"seat\" are three categories in our taxonomy, each being a subcategory of its successor. Consistent orientation can be well defined for shapes in the \"armchair\" category by checking arms, legs and back. However, it becomes difficult to define correct orientation for the \"chair\" category. For example, \"chair\" and \"swivel chair\" are both subcategories of \"chair,\" but swivel chairs have a very different leg structure than most side chairs. It becomes even more ambiguous for \"seat,\" the subcategories are \"hierarchical,\" \"and\" hierarchical. \""}, {"heading": "4.3. Parts and Keypoints", "text": "To get notes on parts and key points, we start with some curated notes on parts within each category. For parts, this capture can be accelerated by using algorithmically generated segmentation, which is then accepted or modified by users. We intend to experiment with both 2D and 3D interfaces for this task. We then use a number of different algorithmic techniques to transfer this information to other nearby shapes. Such methods can rely on rigid 3D annotations, feature descriptor alignments in a suitably defined feature space, or general form correspondences. We iterate this pipeline and use active learning to estimate the 3D models and regions of these models where further human annotations would be most informative, generate a number of new crowd-sourced annotation tasks, propagate their results algorithmically, and so on. In the end, we let users verify all the proposed parts and key points as direct annotation is much faster."}, {"heading": "4.4. Symmetry Estimation", "text": "Our method is a modified version of [22]. The basic idea is to determine the parameters of the symmetry plane using roughness transformation. Specifically, we create all combinations of corner pairs from the mesh. Each pair gives a voice of a possible symmetry plane in the discredited space of evenly distributed plane parameters. We then select the parameter with the most voices as symmetry plane candidate. In the last step, this candidate is verified to ensure that each vertex has a symmetrical counterpart."}, {"heading": "4.5. Physical Property Estimation", "text": "Before calculating annotations on physical attributes, the dimensions of the models must correspond to the real world. We estimate the absolute dimensions of models with previous work in the size estimation [25], followed by manual verification. With the given absolute dimensions, we now calculate the total volume of each model by completed voxelization. We use the space carving approach implemented by Binvox [23]. Categories of objects known as container-like (i.e. bottles, microwaves) are recorded as such and instead only the volume of surface voxelization is used. We then estimate the proportional material composition of each object category and use a table of material densities together with the volume of each model to calculate a rough total weight estimate in this case. Further details on acquiring these physical attributes notes are available separately [26]."}, {"heading": "5. Current Statistics", "text": "At the time of this technical report, ShapeNet indexed approximately 3,000,000 models. 220,000 models of these models are classified into 3,135 categories (WordNet synsets). Below, we provide detailed statistics for the currently commented models in ShapeNet as a whole, as well as details of the publicly available subsets of ShapeNet. Category Distribution Figure 4 shows the distributions of the number of forms per synset at different taxonomy levels for the current ShapeNetCore corpus. To our knowledge, ShapeNet is the largest data set available for clean forms, average number of forms per category, and number of categories. We note that overall, ShapeNet strongly ententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententententent"}, {"heading": "5.1. ShapeNetCore", "text": "ShapeNetCore is a subset of the complete ShapeNet dataset with individual clean 3D models and manually verified category and alignment annotations. It covers 55 general object categories with about 51,300 unique 3D models. The 12 object categories of PASCAL 3D + [35], a popular 3D benchmark dataset for computer vision, are all covered by ShapeNetCore. ShapeNetCore's category distribution is shown in Table 2."}, {"heading": "5.2. ShapeNetSem", "text": "ShapeNetSem is a smaller, more densely annotated subset consisting of 12,000 models spread over a broader set of 270 categories. In addition to manually verified category labels and consistent arrangements, these models are annotated with real dimensions, estimates of their material composition at category level, and estimates of their total volume and weight. The total number of models for the top 100 categories in this subset is listed in Table 3."}, {"heading": "6. Discussion and Future Work", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "7. Conclusion", "text": "We firmly believe that ShapeNet will prove to be an immensely useful resource for multiple research communities in several respects: Data-driven research By establishing ShapeNet as the first large-scale 3D form dataset of its kind, we can help move computer graphics research in a data-driven direction that follows recent developments in vision and NLP. In addition, we can help facilitate a more comprehensive quantitative analysis of proposed systems that can demonstrate the advantages of certain methodologies over a broader and more representative variety of 3D model data. Training resource By providing an extensive, richly annotated dataset, we can also promote a broad class of recently resurgent machine learning methods and neural networks for applications that have to do with geometric data. Similar to research in computer vision and understanding of natural languages, computational geometry and graphics will also support us in addressing these data-driven learning challenges in a broader way that we hope will benefit the public in the long term."}, {"heading": "A. Appendix", "text": "A.1. Hierarchical Rigid Alignment Below we describe in detail our hierarchical rigid alignment algorithms. As a pre-processing step, we first semi-automatically align the upright orientation of each shape. Fortunately, most shapes downloaded from the Web are placed in the upright orientation by default. For those who are not, we filter them out by manual inspection. We then convert models to show clouds through the widest collections of points, and perform PCA on the dot sets. Finally, we ask a person to select the vector of correct upright orientation of six candidates that include the PCA axes and their inverted arrangements. Starting from a leaf category in ShapeNet, we collectively align all shapes that follow previous work. [8] If a leaf category has more than 100 shapes, we further divide it into smaller, more coherent clusters, using formations of invasive global features."}], "references": [{"title": "The protein data bank", "author": ["Helen M Berman", "John Westbrook", "Zukang Feng", "Gary Gilliland", "TN Bhat", "Helge Weissig", "Ilya N Shindyalov", "Philip E Bourne"], "venue": "Nucleic Acids Res,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "A benchmark for 3D mesh segmentation", "author": ["Xiaobai Chen", "Aleksey Golovinskiy", "Thomas Funkhouser"], "venue": "ACM TOG,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Schelling points on 3D surface meshes", "author": ["Xiaobai Chen", "Abulhair Saparov", "Bill Pang", "Thomas Funkhouser"], "venue": "ACM TOG,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Example-based synthesis of 3D object arrangements", "author": ["Matthew Fisher", "Daniel Ritchie", "Manolis Savva", "Thomas Funkhouser", "Pat Hanrahan"], "venue": "ACM TOG,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Fine-grained semi-supervised labeling of large shape collections", "author": ["Qixing Huang", "Hao Su", "Leonidas Guibas"], "venue": "ACM TOG,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Developing an engineering shape benchmark for CAD models", "author": ["Subramaniam Jayanti", "Yagnanarayanan Kalyanaraman", "Natraj Iyer", "Karthik Ramani"], "venue": "Computer-Aided Design,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "A probabilistic model for component-based shape synthesis", "author": ["Evangelos Kalogerakis", "Siddhartha Chaudhuri", "Daphne Koller", "Vladlen Koltun"], "venue": "ACM TOG,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Mobius transformations for global intrinsic symmetry analysis", "author": ["Vladimir Kim", "Yaron Lipman", "Xiaobai Chen", "Thomas Funkhouser"], "venue": "Symposium on Geometry Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Learning part-based templates from large collections of 3D shapes", "author": ["Vladimir G. Kim", "Wilmot Li", "Niloy J. Mitra", "Siddhartha Chaudhuri", "Stephen DiVerdi", "Thomas Funkhouser"], "venue": "ACM TOG,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Exploring collections of 3D models using fuzzy correspondences", "author": ["Vladimir G. Kim", "Wilmot Li", "Niloy J. Mitra", "Stephen DiVerdi", "Thomas Funkhouser"], "venue": "ACM TOG,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "3D object representations for fine-grained categorization", "author": ["Jonathan Krause", "Michael Stark", "Jia Deng", "Li Fei-Fei"], "venue": "In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "PDBsum: A web-based database of summaries and analyses of all PDB structures", "author": ["Roman A Laskowski", "E Gail Hutchinson", "Alex D Michie", "Andrew C Wallace", "Martin L Jones", "Janet M Thornton"], "venue": "Trends Biochem. Sci.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "SHREC\u201912 track: generic 3D shape retrieval", "author": ["Bo Li", "Afzal Godil", "Masaki Aono", "X Bai", "Takahiko Furuya", "L Li", "R L\u00f3pez-Sastre", "Henry Johan", "Ryutarou Ohbuchi", "Carolina Redondo-Cabrera"], "venue": "In 5th Eurographics Conference on 3D Object Retrieval,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "SHREC\u201914 track:  Large scale comprehensive 3D shape retrieval", "author": ["Bo Li", "Yijuan Lu", "Chunyuan Li", "Afzal Godil", "Tobias Schreck", "Masaki Aono", "Qiang Chen", "Nihad Karim Chowdhury", "Bin Fang", "Takahiko Furuya"], "venue": "In Eurographics Workshop on 3D Object Retrieval,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Multi-view object class detection with a 3D geometric model", "author": ["Joerg Liebelt", "Cordelia Schmid"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Creating consistent scene graphs using a probabilistic grammar", "author": ["Tianqiang Liu", "Siddhartha Chaudhuri", "Vladimir G. Kim", "Qi- Xing Huang", "Niloy J. Mitra", "Thomas Funkhouser"], "venue": "ACM TOG,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Building a large annotated corpus of english: The Penn Treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1993}, {"title": "WordNet: a lexical database", "author": ["George A. Miller"], "venue": "for English. CACM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "Symmetry in 3D geometry: Extraction and applications", "author": ["Niloy J Mitra", "Mark Pauly", "Michael Wand", "Duygu Ceylan"], "venue": "In Computer Graphics Forum,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Simplification and repair of polygonal models using volumetric techniques", "author": ["Fakir S. Nooruddin", "Greg Turk"], "venue": "Visualization and Computer Graphics, IEEE Transactions on,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Building a database of 3D scenes from user annotations", "author": ["Bryan C Russell", "Antonio Torralba"], "venue": "In CVPR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "On being the right scale: Sizing large collections of 3D models", "author": ["Manolis Savva", "Angel X. Chang", "Gilbert Bernstein", "Christopher D. Manning", "Pat Hanrahan"], "venue": "In SIGGRAPH Asia 2014 Workshop on Indoor Scene Understanding: Where Graphics meets Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Semantically-Enriched 3D Models for Common-sense Knowledge", "author": ["Manolis Savva", "Angel X. Chang", "Pat Hanrahan"], "venue": "CVPR 2015 Workshop on Functionality, Physics, Intentionality and Causality,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "The Princeton shape benchmark", "author": ["Philip Shilane", "Patrick Min", "Michael Kazhdan", "Thomas Funkhouser"], "venue": "In Shape Modeling Applications. IEEE,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}, {"title": "Sliding shapes for 3D object detection in depth images", "author": ["Shuran Song", "Jianxiong Xiao"], "venue": "In ECCV,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "A large-scale shape benchmark for 3D object retrieval: Toyohashi shape benchmark", "author": ["Atsushi Tatsuma", "Hitoshi Koyanagi", "Masaki Aono"], "venue": "In Asia Pacific Signal and Information Processing Association,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "LabelMe: Online image annotation and applications", "author": ["Antonio Torralba", "Bryan C Russell", "Jenny Yuen"], "venue": "Proceedings of the IEEE,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "3D shape retrieval contest", "author": ["Remco C. Veltkamp", "FB ter Harr"], "venue": "SHREC", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Vrani\u0107. 3D model retrieval", "author": ["V Dejan"], "venue": "University of Leipzig, Germany, PhD thesis,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}, {"title": "A 3D shape benchmark for retrieval and automatic classification of architectural data", "author": ["Raoul Wessel", "Ina Bl\u00fcmel", "Reinhard Klein"], "venue": "In Eurographics 2009 Workshop on 3D Object Retrieval,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "3D ShapeNets: A Deep Representation for Volumetric Shapes", "author": ["Zhirong Wu", "Shuran Song", "Aditya Khosla", "Fisher Yu", "Linguang Zhang", "Xiaoou Tang", "Jianxiong Xiao"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Beyond PASCAL: A benchmark for 3D object detection in the wild", "author": ["Yu Xiang", "Roozbeh Mottaghi", "Silvio Savarese"], "venue": "In WACV,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "SUN3D: A database of big spaces reconstructed using SfM and object labels", "author": ["Jianxiong Xiao", "Andrew Owens", "Antonio Torralba"], "venue": "In ICCV,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}], "referenceMentions": [{"referenceID": 25, "context": "Scene understanding from 2D images is a grand challenge in vision that has recently benefited tremendously from 3D CAD models [28, 34].", "startOffset": 126, "endOffset": 134}, {"referenceID": 31, "context": "Scene understanding from 2D images is a grand challenge in vision that has recently benefited tremendously from 3D CAD models [28, 34].", "startOffset": 126, "endOffset": 134}, {"referenceID": 7, "context": "Mirroring this pattern, recent work in computer graphics has also applied similar approaches to specific problems in the synthesis of new shape variations [10] and new arrangements of shapes [6].", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "Mirroring this pattern, recent work in computer graphics has also applied similar approaches to specific problems in the synthesis of new shape variations [10] and new arrangements of shapes [6].", "startOffset": 191, "endOffset": 194}, {"referenceID": 17, "context": "Motivated by the far-reaching impact of dataset efforts such as the Penn Treebank [20], WordNet [21] and ImageNet [4], which collectively have tens of thousands of citations, we propose establishing ShapeNet: a large-scale 3D model dataset.", "startOffset": 82, "endOffset": 86}, {"referenceID": 18, "context": "Motivated by the far-reaching impact of dataset efforts such as the Penn Treebank [20], WordNet [21] and ImageNet [4], which collectively have tens of thousands of citations, we propose establishing ShapeNet: a large-scale 3D model dataset.", "startOffset": 96, "endOffset": 100}, {"referenceID": 3, "context": "Motivated by the far-reaching impact of dataset efforts such as the Penn Treebank [20], WordNet [21] and ImageNet [4], which collectively have tens of thousands of citations, we propose establishing ShapeNet: a large-scale 3D model dataset.", "startOffset": 114, "endOffset": 117}, {"referenceID": 14, "context": "However, those datasets are very small \u2014 the most recent SHREC iteration in 2014 [17] contains a \u201clarge\u201d dataset with around 9,000 models consisting of models from a variety of sources organized into 171 categories (Table 1).", "startOffset": 81, "endOffset": 85}, {"referenceID": 24, "context": "The Princeton Shape Benchmark is probably the most well-known and frequently used 3D shape collection to date (with over 1000 citations) [27].", "startOffset": 137, "endOffset": 141}, {"referenceID": 1, "context": "Other commonly-used datasets contain segmentations [2], correspondences [13, 12], hierarchies [19], symmetries [11], salient features [3], semantic segmentations and labels [36], alignments of 3D models with images [35], semantic ontologies [5], and other functional annotations \u2014 but again only for small size datasets.", "startOffset": 51, "endOffset": 54}, {"referenceID": 10, "context": "Other commonly-used datasets contain segmentations [2], correspondences [13, 12], hierarchies [19], symmetries [11], salient features [3], semantic segmentations and labels [36], alignments of 3D models with images [35], semantic ontologies [5], and other functional annotations \u2014 but again only for small size datasets.", "startOffset": 72, "endOffset": 80}, {"referenceID": 9, "context": "Other commonly-used datasets contain segmentations [2], correspondences [13, 12], hierarchies [19], symmetries [11], salient features [3], semantic segmentations and labels [36], alignments of 3D models with images [35], semantic ontologies [5], and other functional annotations \u2014 but again only for small size datasets.", "startOffset": 72, "endOffset": 80}, {"referenceID": 16, "context": "Other commonly-used datasets contain segmentations [2], correspondences [13, 12], hierarchies [19], symmetries [11], salient features [3], semantic segmentations and labels [36], alignments of 3D models with images [35], semantic ontologies [5], and other functional annotations \u2014 but again only for small size datasets.", "startOffset": 94, "endOffset": 98}, {"referenceID": 8, "context": "Other commonly-used datasets contain segmentations [2], correspondences [13, 12], hierarchies [19], symmetries [11], salient features [3], semantic segmentations and labels [36], alignments of 3D models with images [35], semantic ontologies [5], and other functional annotations \u2014 but again only for small size datasets.", "startOffset": 111, "endOffset": 115}, {"referenceID": 2, "context": "Other commonly-used datasets contain segmentations [2], correspondences [13, 12], hierarchies [19], symmetries [11], salient features [3], semantic segmentations and labels [36], alignments of 3D models with images [35], semantic ontologies [5], and other functional annotations \u2014 but again only for small size datasets.", "startOffset": 134, "endOffset": 137}, {"referenceID": 33, "context": "Other commonly-used datasets contain segmentations [2], correspondences [13, 12], hierarchies [19], symmetries [11], salient features [3], semantic segmentations and labels [36], alignments of 3D models with images [35], semantic ontologies [5], and other functional annotations \u2014 but again only for small size datasets.", "startOffset": 173, "endOffset": 177}, {"referenceID": 32, "context": "Other commonly-used datasets contain segmentations [2], correspondences [13, 12], hierarchies [19], symmetries [11], salient features [3], semantic segmentations and labels [36], alignments of 3D models with images [35], semantic ontologies [5], and other functional annotations \u2014 but again only for small size datasets.", "startOffset": 215, "endOffset": 219}, {"referenceID": 1, "context": "For example, the Benchmark for 3D Mesh Segmentation contains just 380 models in 19 object classes [2].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "For example, ImageNet [4] provides a set of 14M images organized into 20K categories associated with \u201csynsets\u201d of WordNet [21].", "startOffset": 22, "endOffset": 25}, {"referenceID": 18, "context": "For example, ImageNet [4] provides a set of 14M images organized into 20K categories associated with \u201csynsets\u201d of WordNet [21].", "startOffset": 122, "endOffset": 126}, {"referenceID": 21, "context": "LabelMe provides segmentations and label annotations of hundreds of thousands of objects in tens of thousands of images [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 31, "context": "Recent work demonstrated the benefit of a large dataset of 120K 3D CAD models in training a convolutional neural network for object recognition and next-best view prediction in RGB-D data [34].", "startOffset": 188, "endOffset": 192}, {"referenceID": 11, "context": ", [14, 18]) have revitalized data-driven algorithms for recognition, detection, and editing of images, which have revolutionized computer vision.", "startOffset": 2, "endOffset": 10}, {"referenceID": 15, "context": ", [14, 18]) have revitalized data-driven algorithms for recognition, detection, and editing of images, which have revolutionized computer vision.", "startOffset": 2, "endOffset": 10}, {"referenceID": 0, "context": "For example, the Protein Data Bank [1] provides a database with 100K protein 3D structures, each labeled with its source and links to structural and functional annotations [15].", "startOffset": 35, "endOffset": 38}, {"referenceID": 12, "context": "For example, the Protein Data Bank [1] provides a database with 100K protein 3D structures, each labeled with its source and links to structural and functional annotations [15].", "startOffset": 172, "endOffset": 176}, {"referenceID": 24, "context": "Source datasets from SHREC 2014: Princeton Shape Benchmark (PSB) [27], SHREC 2012 generic Shape Benchmark (SHREC12GTB) [16], Toyohashi Shape Benchmark (TSB) [29], Konstanz 3D Model Benchmark (CCCC) [32], Watertight Model Benchmark (WMB) [31], McGill 3D Shape Benchmark (MSB) [37], Bonn Architecture Benchmark (BAB) [33], Purdue Engineering Shape Benchmark (ESB) [9].", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "Source datasets from SHREC 2014: Princeton Shape Benchmark (PSB) [27], SHREC 2012 generic Shape Benchmark (SHREC12GTB) [16], Toyohashi Shape Benchmark (TSB) [29], Konstanz 3D Model Benchmark (CCCC) [32], Watertight Model Benchmark (WMB) [31], McGill 3D Shape Benchmark (MSB) [37], Bonn Architecture Benchmark (BAB) [33], Purdue Engineering Shape Benchmark (ESB) [9].", "startOffset": 119, "endOffset": 123}, {"referenceID": 26, "context": "Source datasets from SHREC 2014: Princeton Shape Benchmark (PSB) [27], SHREC 2012 generic Shape Benchmark (SHREC12GTB) [16], Toyohashi Shape Benchmark (TSB) [29], Konstanz 3D Model Benchmark (CCCC) [32], Watertight Model Benchmark (WMB) [31], McGill 3D Shape Benchmark (MSB) [37], Bonn Architecture Benchmark (BAB) [33], Purdue Engineering Shape Benchmark (ESB) [9].", "startOffset": 157, "endOffset": 161}, {"referenceID": 29, "context": "Source datasets from SHREC 2014: Princeton Shape Benchmark (PSB) [27], SHREC 2012 generic Shape Benchmark (SHREC12GTB) [16], Toyohashi Shape Benchmark (TSB) [29], Konstanz 3D Model Benchmark (CCCC) [32], Watertight Model Benchmark (WMB) [31], McGill 3D Shape Benchmark (MSB) [37], Bonn Architecture Benchmark (BAB) [33], Purdue Engineering Shape Benchmark (ESB) [9].", "startOffset": 198, "endOffset": 202}, {"referenceID": 28, "context": "Source datasets from SHREC 2014: Princeton Shape Benchmark (PSB) [27], SHREC 2012 generic Shape Benchmark (SHREC12GTB) [16], Toyohashi Shape Benchmark (TSB) [29], Konstanz 3D Model Benchmark (CCCC) [32], Watertight Model Benchmark (WMB) [31], McGill 3D Shape Benchmark (MSB) [37], Bonn Architecture Benchmark (BAB) [33], Purdue Engineering Shape Benchmark (ESB) [9].", "startOffset": 237, "endOffset": 241}, {"referenceID": 30, "context": "Source datasets from SHREC 2014: Princeton Shape Benchmark (PSB) [27], SHREC 2012 generic Shape Benchmark (SHREC12GTB) [16], Toyohashi Shape Benchmark (TSB) [29], Konstanz 3D Model Benchmark (CCCC) [32], Watertight Model Benchmark (WMB) [31], McGill 3D Shape Benchmark (MSB) [37], Bonn Architecture Benchmark (BAB) [33], Purdue Engineering Shape Benchmark (ESB) [9].", "startOffset": 315, "endOffset": 319}, {"referenceID": 6, "context": "Source datasets from SHREC 2014: Princeton Shape Benchmark (PSB) [27], SHREC 2012 generic Shape Benchmark (SHREC12GTB) [16], Toyohashi Shape Benchmark (TSB) [29], Konstanz 3D Model Benchmark (CCCC) [32], Watertight Model Benchmark (WMB) [31], McGill 3D Shape Benchmark (MSB) [37], Bonn Architecture Benchmark (BAB) [33], Purdue Engineering Shape Benchmark (ESB) [9].", "startOffset": 362, "endOffset": 365}, {"referenceID": 18, "context": "Models are organized under WordNet [21] noun \u201csynsets\u201d (synonym sets).", "startOffset": 35, "endOffset": 39}, {"referenceID": 3, "context": "multiple scales [4].", "startOffset": 16, "endOffset": 19}, {"referenceID": 18, "context": "As described in the previous section, we organize ShapeNet based on the WordNet [21] taxonomy.", "startOffset": 80, "endOffset": 84}, {"referenceID": 3, "context": "In particular, linking to ImageNet [4] will help transport information between images and shapes.", "startOffset": 35, "endOffset": 38}, {"referenceID": 10, "context": ", upright and front) for every model is important for various tasks such as visualizing shapes [13], shape classification [8] and shape recognition [34].", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": ", upright and front) for every model is important for various tasks such as visualizing shapes [13], shape classification [8] and shape recognition [34].", "startOffset": 122, "endOffset": 125}, {"referenceID": 31, "context": ", upright and front) for every model is important for various tasks such as visualizing shapes [13], shape classification [8] and shape recognition [34].", "startOffset": 148, "endOffset": 152}, {"referenceID": 19, "context": "Our method is a modified version of [22].", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "We estimate the absolute dimensions of models using prior work in size estimation [25], followed by manual verification.", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "We use the space carving approach implemented by Binvox [23].", "startOffset": 56, "endOffset": 60}, {"referenceID": 23, "context": "More details about the acquisition of these physical attribute annotations are available separately [26].", "startOffset": 100, "endOffset": 104}, {"referenceID": 27, "context": "This is in contrast to common image database statistics that contain more natural objects such as plants and animals [30].", "startOffset": 117, "endOffset": 121}, {"referenceID": 32, "context": "The 12 object categories of PASCAL 3D+[35], a popular computer vision 3D benchmark dataset, are all covered by ShapeNetCore.", "startOffset": 38, "endOffset": 42}], "year": 2015, "abstractText": "We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.", "creator": "LaTeX with hyperref package"}}}