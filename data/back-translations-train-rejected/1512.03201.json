{"id": "1512.03201", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Dec-2015", "title": "Gated networks: an inventory", "abstract": "Gated networks are networks that contain gating connections, in which the outputs of at least two neurons are multiplied. Initially, gated networks were used to learn relationships between two input sources, such as pixels from two images. More recently, they have been applied to learning activity recognition or multi-modal representations. The aims of this paper are threefold: 1) to explain the basic computations in gated networks to the non-expert, while adopting a standpoint that insists on their symmetric nature. 2) to serve as a quick reference guide to the recent literature, by providing an inventory of applications of these networks, as well as recent extensions to the basic architecture. 3) to suggest future research directions and applications.", "histories": [["v1", "Thu, 10 Dec 2015 10:31:13 GMT  (233kb)", "http://arxiv.org/abs/1512.03201v1", "Unpublished manuscript, 17 pages"]], "COMMENTS": "Unpublished manuscript, 17 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["olivier sigaud", "cl\\'ement masson", "david filliat", "freek stulp"], "accepted": false, "id": "1512.03201"}, "pdf": {"name": "1512.03201.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["olivier.sigaud@isir.upmc.fr", "clement.masson@ensta-paristech.fr", "david.filliat@ensta-paristech.fr", "freek.stulp@ensta-paristech.fr"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.03 201v 1 [cs.L G] 10 DGated networks are networks that contain gated connections in which the outputs of at least two neurons are multiplied. Initially, gated networks were used to learn relationships between two input sources, such as pixels from two images. More recently, they have been applied to the detection of learning activities or multimodal representations. The objectives of this paper are threefold: 1) to explain the basic calculations in gated networks to the layman, taking a standpoint that insists on their symmetrical nature. 2) to serve as a quick reference guide to the newer literature by providing an inventory of the applications of these networks as well as recent enhancements to the basic architecture. 3) to propose future research directions and applications."}, {"heading": "1 INTRODUCTION", "text": "Due to their many successful pattern recognition applications, deep learning functionality has become one of the most active research trends in the machine learning community, and machine learning networks (LeCun et al., 2015) are expected to be so detailed mainly because the main building blocks of in-depth learning literature have been designed in recent years to be limited by Boltzmann machines (RBMs) (Smolensky, 1986) and recurred neural networks (RNNNs) (Bengio, 2013). Most of these architectures are used to learn a relationship between a single input source and the corresponding output, by building a representation of the input domain that facilitates the extraction of the appropriate relationship. However, there are many domains where representation should be learned."}, {"heading": "2 STANDARD GATED NETWORK ARCHITECTURES", "text": "As shown in Figure 1, two types of connections between three neurons can be considered: in the first family, a neuron h is used as a switch that stops or does not stop the flow of information between two other neurons x and y; this functionality is similar to that of transistors as electronic circuits in digital circuits; this mechanism is used, among other things, in the LSTM network family (Hochreiter & Schmidhuber, 1997a; Srivastava et al., 2015); in the second family, the gating connection implements a multiplicative relationship between two inputs x and y.Note that the latter mechanism is more general than the former, since a value of 0 in h is gates y to 0; the most common view is that neuron h is the signal between x and y.In this paper, we focus on the specific family of neurons built on NMs and CNBs based on multiple networks."}, {"heading": "2.1 FROM GATED RBMS TO GATED AUTOENCODERS AND BEYOND", "text": "We face the questions of the people in a nutshell. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which people are able to identify themselves. (...) We are dealing with a system in which are dealing with a system in which people are able. (...) We are dealing with a system in which we are dealing with a system in which people are able to identify ourselves. (... \"(...) We are dealing with a system in which we are dealing with a system in which we are able to identify ourselves.\" (... \"(...\" We are dealing with a system in which we are dealing with a system in which we are dealing with a system in which we are able to identify ourselves. \"(...\" (...) We have done. \"(...\" We have done. \"We have done.\" (... \"We have done.\" We have done. \"We have done.\" (... \"We have done.\" We have done. \"We have done.\" We have done. \"We have done.\" (... \"We have done.\" We have done. \"We have done.\" We have done. \"We"}, {"heading": "2.2 REDUCING THE NUMBER OF MULTIPLICATIVE CONNECTIONS", "text": "Consider the network shown in Figure 2 (a), consisting of three layers x, y, and h1, whose respective cardinality is nx, ny, and nh. Predicting the output layer y = 1Wijkxihk) (1), where this (optional) non-linear activation function is described in more details in Section 2.4.Alternatively, you can calculate x = specified y and h = 1Wijkxihk or calculate h = 1Wijkxihk. (1), where this (optional) non-linear activation function is described in Section 2.4.4. (2) Alternatively, you can calculate x = specified y and h = 1Wijkxihk and using1With this document, bold lowercase symbols denote vectors, and bold uppercase symbols dennn denominated in SententeneS-i-teneaS-nihk and h = 1Wijkxihk and using1Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-Sc-i-SententententententententeneS-i-nteneaS-Sc-Sc-Sc-Sc-ec-Sc-ec-Sc-Sc-Sc-ec-Sc-Sc-Sc-ec-Sc-Sc-ec-Sc-ec-Sc-Sc-Sc-Sc-Sc-ec-Sc-Sc-Sc-ec-Sc-Sc-Sc-ec-Sc-Sc-Sc-"}, {"heading": "2.2.1 PROJECTING ONTO FACTOR LAYERS", "text": "One way to reduce the number of layers is to project the x, y, and h layers onto smaller layers, each of which is noted as fx, fy, and fh, before performing the product between these smaller layers. Given their multiplicative role, these layers are called \"factor layers.\" The corresponding approach is illustrated in Figure 2 (b). If the respective cardinality of factors is nfx, nfy, and nfh, the number of weights of the central 3-way tensor is nfx \u00d7 nfy \u00d7 nfh. To coordinate the entire network, additional weights must be added to this 3-way tensor nx \u00d7 nfx, ny \u00d7 nfx, and nfh for each layer, so that the total number of weights is nfy \u00d7 nfh (nfx \u00d7 nfy) + (nfx \u00d7 nfx) to reduce the number of layers used."}, {"heading": "2.2.2 CONSTRAINING THE 3-WAY TENSOR", "text": "Another possibility to reduce the number of parameters is to restrict the matrices Wx, Wy and Wh in the size of the input or factors.Let us consider again the case in which the y numbers of x and h. Equation (1) can be rewritten as follows: \"j,\" y \"j\" j \"j\" j \"j\" j \"j\" j \"j\" j \"j\" j \"j\" j \"j\" j \"j\" j \"j\" j \"i\" h \"h\" h. \""}, {"heading": "2.3 VARIATIONS ON THE CENTRAL TENSOR", "text": "The architecture outlined in Section 2.2.2 can be considered either as a specific way to parameterise the global 3-way tensor = for complex factors.eu by introducing features into its internal structure, or as a way to replace the central tensor of the approach outlined in Section 2.2.1 with an elementary product of factor layers. This approach to implementing the central 3-way tensor can be regarded as a degenerated case in which all its non-diagonal elements are zero and its diagonal elements are all set to 1. By this definition, the central product does not contain any adjustable parameters. Instead, representation learning is implemented by matching the weights of the Wx, Wy and Wh matrices linking the external layers to the factors. Note that the use of parameters instead of adjustments to the diagonals may increase the flexibility of the model for learning, but this effect may not enhance the expression of the Wx."}, {"heading": "2.4 ACTIVATION FUNCTIONS", "text": "If the content of an n layer is binary, there are two options. First, it can represent 2n elements of a discrete group using the usual binary encoding rules. In this case, either the model directly represents the probability of each binary value, as is the case in RBMs, or the binary values are obtained from real numbers using a threshold, the latter case being unusual due to the indistinguishability of the threshold function. Second, the option is to represent only n layers using a \"most uniform\" representation, where one bit is set to 1 and all others to 0. If the corresponding layer is used as input, this representation is easy to force the external world. In real layers, the role of the activation function is to limit the values of the output to one layer, and all others to 0. If the corresponding layer is used as input, this representation is simple (manufacturing)."}, {"heading": "3 LEARNING IN GATED NETWORKS", "text": "To explain this learning process, it is useful to reconstruct two output levels and one output level. One way to train such networks would be to use supervised learning: for a particular pair of input levels, one would provide the expected performance and then train the network to minimize a function between the expected and the obtained result. This is used in gated CNNs and gated RNNNs (see Section 5). But GRBMs and GAEs are not trained in this way. Instead, the training process is designed to perform unsupervised learning, but the differences between GRBMs and GAEs differ. In this paper, we do not cover the training of GRBMs based on the training of RBMs. We refer the reader (Swersky et al) for a clear explanation of the latter topic. Instead, we focus on the training of GAes.Given two input layers, GAEs are trained to reconstruct one of these learning processes."}, {"heading": "4 APPLICATIONS OF GATED NETWORKS", "text": "In light of what we have presented so far, there are three aspects in which the use of gated networks can vary: First, as described in Section 2.4, the content of the x, y, and h layers can be evaluated either binary, unilateral, or real; second, gated networks can be trained in different ways, as described in Section 3, using different training signals, regulatory functions, and cost functions; and third, different layers can be used as either input or output; all of these variations result in different functional roles for the respective networks. The aim of this section is to take stock of such uses in the literature, which is finally summarized in Table 1."}, {"heading": "4.1 FORMAT OF THE EXTERNAL LAYERS", "text": "In Section 2.4, we outlined the various activation functions used to deal with different formats of the external layers. Here, we review the use of these formats in different models. First, the h layer always uses a standard binary encoding in all GRBMs (Memisevic & Hinton, 2007; 2010).In addition, most models use pixels of two images as x- and y-inputs.The transformation between these images stored in h is either binary (Memisevic & Hinton, 2007; 2010) or real (Droniou & Sigaud, 2013; Dehghan et al., 2014).In both cases, what we learn is a multiplicity of pixels in the x layer due to those of the y layer, or vice versa (Memisevic & Hinton, 2007).There are two models in which the y layer is binary."}, {"heading": "4.2 TRAINING SIGNAL", "text": "As outlined in Section 3, GAEs can be trained to reconstruct either x or y functions. If the input data is binary, the cross-entropy loss function is the default selection (Rudy & Taylor, 2014). If evaluated in real terms, the default cost function is a square reconstruction error, so the first option is the one chosen in (Rudy & Taylor, 2014), which makes the connection to autoencoders more explicit because they take both x as input and x as output, but this contradicts the rest of the literature, where it is more common to reconstruct y units (Memisevic & Hinton, 2007; Memimmetric et al al al al, 2010; Droniou Sigaud, J-4b-A models)."}, {"heading": "4.3 INPUT-OUTPUT FUNCTION", "text": "This results in three permutations where two layers are between x, y and h inputs, with the third layer being the output layer. However, given the unattended training process described in Section 3, we see that in addition to the three options outlined above, the input layers can also be used to predict either x or y input layers. Under this perspective, learning the latent representation h is a side effect that is not used as input or output, but is \"squeezed\" into the network as input to reconstruct one of the input layers. The same fact applies mutatis mutandis to all other layers. The various possible output layers result in two main ways to use a gated network. The first, the predictive coding view is in inferring layers (or x layers) in input layers."}, {"heading": "4.3.1 SUMMARY: AN INVENTORY", "text": "Table 1 summarizes many uses of the standard gated networks listed above. Table 1 shows that there are a variety of ways to use gated networks, and this diversity is even greater if we also take into account the non-standard architectures discussed in the next section."}, {"heading": "5 BEYOND STANDARD GATED ARCHITECTURES", "text": "In this section we will describe some architectures that contain a gated network. First, we will list some architectures where the central tensor connects more than 3 layers, and then we will introduce some architectures whose connections are not limited to the central tensor."}, {"heading": "5.1 EXTENDED TENSORS", "text": "There are some architectures where the central tensor connects more than 3 external layers. Conditional RBMs (CRBMs) are RBMs where a portion of the memory data of the past is incorporated into the input layer so that the architecture can model time-dependent data (Taylor & Hinton, 2009). In (Taylor et al., 2011) a CRBM is used to model human motion data, but, as shown in Figure 5, it is augmented by an additional style layer to model different motion styles. The x layer corresponds to the motion input in the previous time step. The y layer, the output, corresponds to the predicted motion in the current time step. As in all GRBMs, the h layer is used to learn a representation of the transformation between x and y. Additionally, the z layer corresponds to the real stylistic characteristics sorted by discrete style labels (in the s layer)."}, {"heading": "5.2 CLUSTERING WITH GATED NETWORKS", "text": "In some architectures, the central 3-way tensor is not the only ingredient. For example, the architecture shown in Figure 6 uses an additional autocoding link in relation to a standard GAE (Droniou et al., 2015).The network aims to cluster input data in \"concepts\" that correspond to manifold input layers without using supervised learning. To do this, the input x is first fed into a standard autoencoder that uses a Softmax activation function that performs an uncontrolled clustering of input data. The Softmax activation function implements a competition between the bits of the class layer, resulting in a soft uniform representation of the corresponding class. Then, given the input and the obtained class, the h layer implements a parameterization of input data in relation to the class by using a soft plus layer, i.e. Dosis = the class is weighted directly to the input layer."}, {"heading": "5.3 RECURRENT GATED NETWORKS", "text": "Another architecture based on factoring gating connections is the \"Multiplicative RNN\" architecture (Sutskever et al., 2011), shown in Figure 7, which is a recurring architecture trained to deal with temporally organized information such as text or speech signals. The main requirement of the architecture is that the recurring connection responsible for the dynamics of the hidden variables should be a function of the input layer x. This would result in a complete 3-way tensor which the authors factorize as described in Section 2.2 to reduce the number of free parameters. With slightly adapted notations to emphasize the similarity with other architectures, the internal calculation of the multiplicative RNN is given by the following equations: ft = diag (Wfxxt).Wfhht \u2212 1 (20) ht = tanh (Whfft + Whxxt) (21) y t = W outhy + t."}, {"heading": "5.4 CONVOLUTIONAL GATED NETWORKS", "text": "Folding is a technique that involves editing a large image by moving a smaller filter to any position in the image and applying it to all positions; the same filter can be used, for example, to detect a pattern at any position in the image; folding gated networks apply the folding idea to gated networks; this was done in GRBMs (Taylor et al., 2010) to extract spatio-temporal features associated with the detection of human activity; and in GAEs (Konda & Memisevic, 2015) to perform visual odometry from stereopairs in a sequence of images taken by a moving camera."}, {"heading": "5.5 PREDICTION WITH A SEQUENCE OF GATED NETWORKS", "text": "Another architecture models temporal data using a sequence of GAEs (Michalski et al., 2014b) 3. Beyond a sequence, it even uses a hierarchy of GAEs to learn transformations of transformations. The model of (Michalski et al., 2014b), called Predictive Gating Pyramides (PGP), cascades two levels of GAEs to predict sequences. As the authors point out, the reconstruction error in their context is insufficient, so the model is trained to make predictions rather than reconstruct them. In fact, it is trained to predict over several steps. A strong assumption in PGP is that the high-order relational structure is constant in the sequence. It uses Back-Propagation Through Time (BPTT) to perform a gradient descent on the weights over time. However, the model is used to learn temporal characteristics, it does not predict sequences of large numbers of time, and it does not require many sequences of images."}, {"heading": "6 CONCLUSION", "text": "In this paper, we have based our presentation of gated networks on a perspective that persists in its symmetrical nature. Starting from this particular perspective, we could highlight its richness by taking stock of the various ways in which it has been used in the literature so far. Given this richness, we believe that gated networks still have a largely untapped potential as a unifying tool for many areas in which the relevant information is naturally expressed in tripartite relationships between three interdependent sources. Apart from the architectures proposed in this paper, we hope that many more areas of application for gated networks will emerge in the years to come. Furthermore, as outlined in Section 5, there are still quite a few non-standardized architectures based on the factored gating idea. We believe that the list of such architectures will expand in the future, and that gated networks should be incorporated into more general frameworks that may contain multiple examples of such networks, as is already the case (Michalski, 2014, 2014, or this Lerner)."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the Horizon 2020 research and innovation programme of the European Union as part of the DREAM project under grant agreement no. 640891."}], "references": [{"title": "Deep learning of representations: Looking forward", "author": ["Bengio", "Yoshua"], "venue": "In Statistical language and speech processing,", "citeRegEx": "Bengio and Yoshua.,? \\Q2013\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2013}, {"title": "Who do I look like? determining parent-offspring resemblance via gated autoencoders", "author": ["Dehghan", "Afshin", "Ortiz", "Enrique G", "Villegas", "Ruben", "Shah", "Mubarak"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Dehghan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dehghan et al\\.", "year": 2014}, {"title": "Mental rotation\u201d by optimizing transforming distance", "author": ["Ding", "Weiguang", "Taylor", "Graham W"], "venue": "arXiv preprint arXiv:1406.3010,", "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Apprentissage de repr\u00e9sentations et robotique d\u00e9veloppementale: quelques apports de l\u2019apprentissage profond pour la robotique autonome", "author": ["Droniou", "Alain"], "venue": "PhD thesis, UPMC-Paris", "citeRegEx": "Droniou and Alain.,? \\Q2015\\E", "shortCiteRegEx": "Droniou and Alain.", "year": 2015}, {"title": "Gated autoencoders with tied input weights", "author": ["Droniou", "Alain", "Sigaud", "Olivier"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Droniou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Droniou et al\\.", "year": 2013}, {"title": "Learning a repertoire of actions with deep neural networks", "author": ["Droniou", "Alain", "Ivaldi", "Serena", "Sigaud", "Olivier"], "venue": "In ICDL-Epirob, pp", "citeRegEx": "Droniou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Droniou et al\\.", "year": 2014}, {"title": "A deep unsupervised network for multimodal perception, representation and classification", "author": ["Droniou", "Alain", "Ivaldi", "Serena", "Sigaud", "Olivier"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Droniou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Droniou et al\\.", "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Reducing the Dimensionality of Data with", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Neural Networks. Science,", "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "LSTM can solve hard long time lag problems", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in Neural Information Processing Systems 9: Proceedings of the 1996 Conference,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "LSTM can solve hard long time lag problems", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in Neural Information Processing Systems 9: Proceedings of the 1996 Conference,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Estimation of non-normalized statistical models by score matching", "author": ["Hyv\u00e4rinen", "Aapo"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Hyv\u00e4rinen and Aapo.,? \\Q2005\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Aapo.", "year": 2005}, {"title": "Analyzing the dynamics of gated auto-encoders in practice", "author": ["Im", "Daniel Jiwoong", "Taylor", "Graham W"], "venue": "Unpublished manuscript?,", "citeRegEx": "Im et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Im et al\\.", "year": 2014}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Koller", "Daphne", "Friedman", "Nir"], "venue": "MIT press,", "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "Learning visual odometry with a convolutional network", "author": ["Konda", "Kishore", "Memisevic", "Roland"], "venue": "In International Conference on Computer Vision Theory and Applications,", "citeRegEx": "Konda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Konda et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Deep learning via hessian-free optimization", "author": ["Martens", "James"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Martens and James.,? \\Q2010\\E", "shortCiteRegEx": "Martens and James.", "year": 2010}, {"title": "Non-linear latent factor models for revealing structure in high-dimensional data", "author": ["Memisevic", "Roland"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "Memisevic and Roland.,? \\Q2008\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2008}, {"title": "Gradient-based learning of higher-order image features", "author": ["Memisevic", "Roland"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Memisevic and Roland.,? \\Q2011\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2011}, {"title": "On multi-view feature learning", "author": ["Memisevic", "Roland"], "venue": "In Proceedings of the 28th Annual International Conference on Machine Learning, pp", "citeRegEx": "Memisevic and Roland.,? \\Q2012\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2012}, {"title": "Learning to relate images", "author": ["Memisevic", "Roland"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Memisevic and Roland.,? \\Q2013\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2013}, {"title": "Unsupervised learning of image transformations", "author": ["Memisevic", "Roland", "Hinton", "Geoffrey E"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Memisevic et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Memisevic et al\\.", "year": 2007}, {"title": "Learning to represent spatial transformations with factored higher-order boltzmann machines", "author": ["Memisevic", "Roland", "Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Memisevic et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Memisevic et al\\.", "year": 2010}, {"title": "Gated softmax classification", "author": ["Memisevic", "Roland", "Zach", "Christopher", "Hinton", "Geoffrey E", "Pollefeys", "Marc"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Memisevic et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Memisevic et al\\.", "year": 2010}, {"title": "Modeling sequential data using higher-order relational features and predictive training", "author": ["Michalski", "Vincent", "Memisevic", "Roland", "Konda", "Kishore"], "venue": "arXiv preprint arXiv:1402.2333,", "citeRegEx": "Michalski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Michalski et al\\.", "year": 2014}, {"title": "Modeling deep temporal dependencies with recurrent \u201dgrammar cells", "author": ["Michalski", "Vincent", "Memisevic", "Roland", "Konda", "Kishore"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Michalski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Michalski et al\\.", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Factored four way conditional restricted boltzmann machines for activity recognition", "author": ["Mocanu", "Decebal Constantin", "Ammar", "Haitham Bou", "Lowet", "Dietwig", "Driessens", "Kurt", "Liotta", "Antonio", "Weiss", "Gerhard", "Tuyls", "Karl"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Mocanu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mocanu et al\\.", "year": 2015}, {"title": "Learning object affordances: From sensory\u2013motor coordination to imitation", "author": ["L. Montesano", "M. Lopes", "A. Bernardino", "J. Santos-Victor"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "Montesano et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Montesano et al\\.", "year": 2008}, {"title": "Principles of image representation in visual cortex", "author": ["Olshausen", "Bruno A"], "venue": "The visual neurosciences,", "citeRegEx": "Olshausen and A.,? \\Q2003\\E", "shortCiteRegEx": "Olshausen and A.", "year": 2003}, {"title": "Generative class-conditional autoencoders", "author": ["Rudy", "Jan", "Taylor", "Graham W"], "venue": "arXiv preprint arXiv:1412.7009,", "citeRegEx": "Rudy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rudy et al\\.", "year": 2014}, {"title": "Towards deep developmental learning", "author": ["Sigaud", "Olivier", "Droniou", "Alain"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Sigaud et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sigaud et al\\.", "year": 2016}, {"title": "Information processing in dynamical systems: foundations of harmony theory", "author": ["Smolensky", "Paul"], "venue": "In Parallel distributed processing: explorations in the microstructure of cognition,", "citeRegEx": "Smolensky and Paul.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky and Paul.", "year": 1986}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["Srivastava", "Nitish", "Mansimov", "Elman", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1502.04681,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "A tutorial on stochastic approximation algorithms for training restricted boltzmann machines and deep belief nets", "author": ["Swersky", "Kevin", "Chen", "Bo", "Marlin", "Benjamin", "De Freitas", "Nando"], "venue": "In Information Theory and Applications Workshop (ITA),", "citeRegEx": "Swersky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2010}, {"title": "Factored conditional restricted boltzmann machines for modeling motion style", "author": ["Taylor", "Graham W", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Taylor et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2009}, {"title": "Convolutional learning of spatio-temporal features", "author": ["Taylor", "Graham W", "Fergus", "Rob", "LeCun", "Yann", "Bregler", "Christoph"], "venue": "In ECCV\u201910,", "citeRegEx": "Taylor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2010}, {"title": "Two Distributed-State Models For Generating High-Dimensional Time Series", "author": ["Taylor", "Graham W", "Hinton", "Geoffrey E", "Roweis", "Sam T"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Taylor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2011}, {"title": "A connection between score matching and denoising autoencoders", "author": ["Vincent", "Pascal"], "venue": "Neural computation,", "citeRegEx": "Vincent and Pascal.,? \\Q2011\\E", "shortCiteRegEx": "Vincent and Pascal.", "year": 2011}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 42, "context": "The main building blocks in the deep learning literature are Restricted Boltzmann Machines (RBMs) (Smolensky, 1986), autoencoders (Hinton & Salakhutdinov, 2006; Vincent et al., 2008), Convolutional Neural Networks (CNNs) (LeCun et al.", "startOffset": 130, "endOffset": 182}, {"referenceID": 15, "context": ", 2008), Convolutional Neural Networks (CNNs) (LeCun et al., 1998) and Recurrent Neural Networks (RNNs) (Bengio, 2013).", "startOffset": 46, "endOffset": 66}, {"referenceID": 27, "context": "In order to deal with continuous states and actions, finding separately the adequate representations for states and actions to facilitate value function learning might be critical (Mnih et al., 2015; Lillicrap et al., 2015).", "startOffset": 180, "endOffset": 223}, {"referenceID": 16, "context": "In order to deal with continuous states and actions, finding separately the adequate representations for states and actions to facilitate value function learning might be critical (Mnih et al., 2015; Lillicrap et al., 2015).", "startOffset": 180, "endOffset": 223}, {"referenceID": 28, "context": "Initially they were mainly used to learn transformation between images (Memisevic & Hinton, 2007), but they have recently also been applied to human activity recognition from videos and moving skeleton data from the kinect sensor (Mocanu et al., 2015), or to recognize offspring relationship from pictures of faces (Dehghan et al.", "startOffset": 230, "endOffset": 251}, {"referenceID": 1, "context": ", 2015), or to recognize offspring relationship from pictures of faces (Dehghan et al., 2014).", "startOffset": 71, "endOffset": 93}, {"referenceID": 5, "context": "In robotics, gated networks have been used to learn to write numbers (Droniou et al., 2014), as well as to learn multimodal representations of numbers using images, vocal signal and articular movements with the iCub robot (Droniou et al.", "startOffset": 69, "endOffset": 91}, {"referenceID": 6, "context": ", 2014), as well as to learn multimodal representations of numbers using images, vocal signal and articular movements with the iCub robot (Droniou et al., 2015).", "startOffset": 138, "endOffset": 160}, {"referenceID": 29, "context": "At a higher level, the same tools could be used to learn affordances, that are often represented as object-action-effect complexes (Montesano et al., 2008).", "startOffset": 131, "endOffset": 155}, {"referenceID": 35, "context": "This mechanism is used in the LSTM family of networks (Hochreiter & Schmidhuber, 1997a; Srivastava et al., 2015), among others.", "startOffset": 54, "endOffset": 112}, {"referenceID": 42, "context": "We now briefly introduce Restricted Boltzmann Machines (RBMs) (Smolensky, 1986), autoencoders (Hinton & Salakhutdinov, 2006; Vincent et al., 2008), Convolutional Neural Networks (CNNs) (LeCun et al.", "startOffset": 94, "endOffset": 146}, {"referenceID": 15, "context": ", 2008), Convolutional Neural Networks (CNNs) (LeCun et al., 1998) and Recurrent Neural Networks (RNNs) (Bengio, 2013), and show how these networks have been extended to contain gated connections.", "startOffset": 46, "endOffset": 66}, {"referenceID": 42, "context": "This endow autoencoders with generative properties similar to those of RBMs (Vincent et al., 2008).", "startOffset": 76, "endOffset": 98}, {"referenceID": 39, "context": "This led to a shift from GRBMs to gated autoencoders (GAEs) (Memisevic, 2008; 2011; 2012) though research on GRBMs is still active (Taylor et al., 2010; Ding & Taylor, 2014).", "startOffset": 131, "endOffset": 173}, {"referenceID": 36, "context": "The gating idea was also applied to RNNs (Sutskever et al., 2011) and CNNs, either combined to GRBMs (Taylor et al.", "startOffset": 41, "endOffset": 65}, {"referenceID": 39, "context": ", 2011) and CNNs, either combined to GRBMs (Taylor et al., 2010) or to GAEs (Konda & Memisevic, 2015), as we outline in Section 5.", "startOffset": 43, "endOffset": 64}, {"referenceID": 15, "context": ", 2008), Convolutional Neural Networks (CNNs) (LeCun et al., 1998) and Recurrent Neural Networks (RNNs) (Bengio, 2013), and show how these networks have been extended to contain gated connections. An RBM is not a neural network but a particular probabilistic graphical model (PGM) (Koller & Friedman, 2009) whose graph is bipartite: one set (or layer) of nodes is called \u201cvisible\u201d and is used as the input of the model, whereas the other layer is \u201chidden\u201d and is interpreted as being the hidden cause explaining the input. Both layers are generally binary (though it is possible to extend them to real-valued units) and fully connected to each other. However, there are no connections within a layer, which facilitates inference and training. Training an RBM consists in finding the parameters (edge\u2019s weights and node\u2019s bias) which maximize the likelihood of the training data. Importantly, RBMs are generative models: they can model the probability density of the joint distribution of visible and hidden units, which enables them to generate samples similar to those of the training data onto the visible layer. The first instance of a gated network in the deep learning literature was a gated RBM (GRBM) (Memisevic & Hinton, 2007). However, this model was using a fully connected multiplicative network that required a lot of memory and computations for inference and training. In the next section, we present a solution to this issue, that was introduced by Memisevic & Hinton (2010) as a direct extension of (Memisevic & Hinton, 2007), still using GRBMs.", "startOffset": 47, "endOffset": 1489}, {"referenceID": 23, "context": "All figures are adapted from (Memisevic et al., 2010).", "startOffset": 29, "endOffset": 53}, {"referenceID": 7, "context": "One can also use the softplus function \u03c3+, defined as \u03c3+(x) = log(1+exp(x)), which is a smooth version of the rectified linear unit (Glorot et al., 2011).", "startOffset": 132, "endOffset": 153}, {"referenceID": 37, "context": "We refer the reader to (Swersky et al., 2010) for a clear presentation of the latter topic.", "startOffset": 23, "endOffset": 45}, {"referenceID": 1, "context": "The transformation between these images stored in h is either binary (Memisevic & Hinton, 2007; 2010) or real-valued (Droniou & Sigaud, 2013; Dehghan et al., 2014).", "startOffset": 117, "endOffset": 163}, {"referenceID": 23, "context": "the output \u0177 consisted of binary class labels, and the values of the h layer were also binary (Memisevic et al., 2010).", "startOffset": 94, "endOffset": 118}, {"referenceID": 1, "context": "Interestingly, a model recognizing offspring relationship from pictures of faces combines generative and discriminative training, using two training signals (Dehghan et al., 2014).", "startOffset": 157, "endOffset": 179}, {"referenceID": 1, "context": "The work of (Dehghan et al., 2014) is another instance of the transformation coding view, but where x and y are temporally independent images.", "startOffset": 12, "endOffset": 34}, {"referenceID": 23, "context": "training (Memisevic & Hinton, 2007; 2010) pixels(t) pixels(t+1) binary proba \u0177 (Memisevic et al., 2010) pixels binary binary proba \u0177 (Memisevic, 2011) pixels pixels = x soft 1-hot relu (x\u0302, \u0177) (Droniou & Sigaud, 2013) pixels(t) pixels(t+1) real softplus \u0177 (Rudy & Taylor, 2014) pixels 1-hot real relu x\u0302 (Dehghan et al.", "startOffset": 79, "endOffset": 103}, {"referenceID": 1, "context": ", 2010) pixels binary binary proba \u0177 (Memisevic, 2011) pixels pixels = x soft 1-hot relu (x\u0302, \u0177) (Droniou & Sigaud, 2013) pixels(t) pixels(t+1) real softplus \u0177 (Rudy & Taylor, 2014) pixels 1-hot real relu x\u0302 (Dehghan et al., 2014) face 1 face 2 soft 1-hot softmax hybrid", "startOffset": 208, "endOffset": 230}, {"referenceID": 40, "context": "In (Taylor et al., 2011), a CRBM is used to model human motion data but, as illustrated in Figure 5, it is extended with an additional style layer to model different styles of motion.", "startOffset": 3, "endOffset": 24}, {"referenceID": 40, "context": "Figure 5: Four layers can be connected by three tripartite connection blocks (adapted from (Taylor et al., 2011)).", "startOffset": 91, "endOffset": 112}, {"referenceID": 28, "context": "More recently, a \u201c4-way tensor\u201d and its factorization were introduced based in GRBMs (Mocanu et al., 2015).", "startOffset": 85, "endOffset": 106}, {"referenceID": 28, "context": "The models of (Taylor & Hinton, 2009) and (Mocanu et al., 2015) are both capable of representing sequential data in the limit of the N previous time steps included in the memory concatenated to the input layer.", "startOffset": 42, "endOffset": 63}, {"referenceID": 6, "context": "For instance, the architecture depicted in Figure 6 uses an additional autoencoding connection with respect to a standard GAE (Droniou et al., 2015).", "startOffset": 126, "endOffset": 148}, {"referenceID": 6, "context": "Figure 6: Gated network for unsupervised classification (adapted from (Droniou et al., 2015)).", "startOffset": 70, "endOffset": 92}, {"referenceID": 6, "context": "We do not further study this aspect here, see (Droniou et al., 2015) for more details.", "startOffset": 46, "endOffset": 68}, {"referenceID": 36, "context": "Another architecture based on factoring gating connections is the \u201cMultiplicative RNN\u201d architecture (Sutskever et al., 2011) depicted in Figure 7.", "startOffset": 100, "endOffset": 124}, {"referenceID": 39, "context": "This has been done in GRBMs (Taylor et al., 2010) so as to extract spatio-temporal features in the context of human activity recognition, and in GAEs (Konda & Memisevic, 2015) to perform visual odometry from stereo pairs in a sequence of images captured from a moving camera.", "startOffset": 28, "endOffset": 49}, {"referenceID": 5, "context": ", 2014b) or (Droniou et al., 2014).", "startOffset": 12, "endOffset": 34}], "year": 2015, "abstractText": "Gated networks are networks that contain gating connections, in which the outputs of at least two neurons are multiplied. Initially, gated networks were used to learn relationships between two input sources, such as pixels from two images. More recently, they have been applied to learning activity recognition or multimodal representations. The aims of this paper are threefold: 1) to explain the basic computations in gated networks to the non-expert, while adopting a standpoint that insists on their symmetric nature. 2) to serve as a quick reference guide to the recent literature, by providing an inventory of applications of these networks, as well as recent extensions to the basic architecture. 3) to suggest future research directions and applications.", "creator": "LaTeX with hyperref package"}}}