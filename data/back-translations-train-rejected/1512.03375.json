{"id": "1512.03375", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Dec-2015", "title": "Convolutional Monte Carlo Rollouts in Go", "abstract": "In this work, we present a MCTS-based Go-playing program which uses convolutional networks in all parts. Our method performs MCTS in batches, explores the Monte Carlo search tree using Thompson sampling and a convolutional network, and evaluates convnet-based rollouts on the GPU. We achieve strong win rates against open source Go programs and attain competitive results against state of the art convolutional net-based Go-playing programs.", "histories": [["v1", "Thu, 10 Dec 2015 19:32:48 GMT  (29kb)", "http://arxiv.org/abs/1512.03375v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["peter h jin", "kurt keutzer"], "accepted": false, "id": "1512.03375"}, "pdf": {"name": "1512.03375.pdf", "metadata": {"source": "CRF", "title": "Convolutional Monte Carlo Rollouts in Go", "authors": ["Peter H. Jin"], "emails": ["phj@eecs.berkeley.edu,", "keutzer@berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.03 375v 1 [cs.L G] 10 Dec 2In this paper, we present an MCTS-based go-play program that utilizes winding networks in all parts. Our methodology conducts MCTS in batches, explores the Monte Carlo search tree using Thompson samples and a winding network, and evaluates convent-based rollouts on the GPU. We achieve strong return rates over open source go programs and achieve competitive results over modern revolutionary network-based go-play programs."}, {"heading": "1 Introduction", "text": "The game of Go remains unsolved by computer algorithms despite advances in tree search in Monte Carlo over the past decade until tree function has been achieved (Kocsis & Szepesva \u0301 ri, 2006) Recent work in Convolutionary Networks for Playing Go (Tian & Zhu, 2015; Maddison et al., 2014; Clark & Storkey, 2015; Sutskever & Nair, 2008) has produced neural train predictors for Go with an accuracy of up to 57.3% on datasets of historical Go game records. However, a train predictor is a component in a Monte Carlo tree search loop.Previous work uses a precise train predictor built from a very deep revolutionary network to guide the study of the search tree in MCTS (Tian & Zhu, 2015; Maddison et al., 2014). Exploration is only one half of the MCTS search; the other half consists of simulations or rollouts that are random in their original form."}, {"heading": "2 Related Work", "text": "Sutskever & Nair (2008) trained two-tiered Convolutionary Networks to predict movements in Go. Clark & Storkey (2015) and Maddison et al. (2014) later expanded the results to include deep Convolutionary Networks. More recently, Tian & Zhu (2015) showed that multi-label long-term prediction training further improved the accuracy and gameplay of Deep Convolutional Move predictors. However, in the aforementioned work, the rollout part of MCTS consisted, if at all, of traditional pattern-based rollouts. Several non-revolutionary network-based methods for predicting or ranking movements were introduced in the past. MoGo, the pioneering go-play program, showed pattern-based rollouts (Gelly et al, 2006)."}, {"heading": "3 Terminology", "text": "In the Monte Carlo tree search literature, the terms rollout, playout and simulation are often used synonymously to define the randomly simulated games. To avoid confusion, we try to use only the term rollout.In the MCTS literature (see Figure 1 for pseudocode), neural networks can be used in two different parts: (1) during tree exploration and (2) during rollouts. Both of these parts are similar in that a classified neural network in a state s can be used to determine the probability of selecting from the available actions for s (for example, if the network has a softmax probability output layer).Since such a neural network is effectively the probability calculation for the guidelines, we call this network a political network as a supplementation network."}, {"heading": "4 Monte Carlo Tree Search", "text": "The most powerful modern Go game programs use all versions of MCTS. MCTS builds a search tree of game position nodes, with each node following the Monte Carlo values: the total number of attempts passing the node and the number of successful attempts (wins).The search tree is updated by three repetitive phases: exploration, rollout and backup.2The canonical version of MCTS is UCT (Kocsis & Szepesva) ri, where the selection criterion in the exploration phase is determined by the multi-armed bandit algorithm UCB1 (Auer et al., 2002).For a particular node in the MC search tree, the total number of studies by the j child of the node is called nj, the number of successful studies by all children of the node is called wj, and the total number of studies by all children of the node is called nj."}, {"heading": "5 Neural Move Prediction", "text": "Typically, the position of the board is pre-processed into a dense mask of relatively easy-to-calculate binary and real features, which are then provided as input into the Convolutionary Network. Common input features include the configuration of the board (stone positions), the co-point if it exists, the number of chain freedoms and the stone history or distance since the last move. Using our earlier terminology, a neural network-based train predictor is a kind of political network. The most accurate Convolutionary Network architectures for predicting steps in Go tend to be very deep, with at least a dozen layers (Tian & Zhu, 2015; Maddison et al., 2014). A typical architecture has a larger first revolutionary layer, followed by many 3 x 3 Convolutionary layers, which tend to have the ability to predict steps in Go very deep, with at least a dozen layers (Tian & Zhu, 2015; Maddison)."}, {"heading": "6 Batch Thompson Sampling Tree Search", "text": "In theory, it is also possible to use a political network to select the non-uniform random movements during rollouts, which can be called a rollout policy network and would replace traditional pattern-based rollout policies. However, there are two related obstacles that prevent revolutionary rollout policy networks from working effectively: (1) convolutions are computationally expensive, and (2) UCB1 is a deterministic algorithm. When conducting inferences with a Constitutional network, one prefers to evaluate the input in batches to maximize throughput on hardware platforms such as modern GPUs. While batching convolutions is a well-known technique and forms the basis of modern minibatch stochastic gradient methods, in order to be effective, the input states must be sufficiently unique to maximize throughput on hardware platforms such as modern GPUs."}, {"heading": "7 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Data", "text": "The record consists of 82,609 historical and modern games. We limited our experiments to a subset of games that met the following criteria: 19 x 19 board games, modern (played after 1950), \"standard\" komi (komi, {2,75, 3,75, 5,5, 6,5}) and no handicap stones. We did not differentiate between rule sets (most games followed Chinese or Japanese rules), and our somewhat strict criteria resulted in a training set of over 57,000 games.For validation, we used a subset of over 1000 high-level games from the KGS Go server (Go \ufffd rtz, 2015)."}, {"heading": "7.2 Architectures", "text": "For each time step and for each player, we have tracked (1) binary characteristics indicating the placement of stones on dots, and (2) uniform characteristics indicating whether a point belongs to a chain with 1, 2 or \u2265 3 freedoms. We have traced the last two time steps for a total of 16 attribute levels. In our experiments, we have used a total of 5 different network architectures described in Table 2. They consist of a first layer, followed by repeated inner layers and supplemented by a last layer before a softmax probability output layer; this is the same architectural pattern used by Maddison et al. (2014) and Tian & Zhu (2015). Three of them (A, B and C) are very deep nets, followed by repeated inner layers and are previous political networks. The other two (R-2 and R-3) consist of a flat and a minimally deep net, with the line and line used as a unit."}, {"heading": "7.3 Training", "text": "We found that Tian & Zhu (2015) trained deep convolutional motion predictors to predict k's next moves, and found that k = 3 yielded stronger networks than the use of single-stage labels (k = 1).For architectures A and B, we initialized the learning rate at 0.01 and annealed the learning rate by a factor of 0.1 every two epochs. For architecture C, we set the learning rate to 0.05 and did not adjust iterations.For architectures A, B, and C, we used zero dynamics and zero weight loss. For architectures R-2 and R-3, we followed a similar training protocol as in Sutskever & Nair, 2008. We initialized the training at a learning rate of 0.1 and then annealed the learning rate to 0.01 after 3,200 iterations, performing SGD for at least two epochs. We used dynamics of 0.9 and no weight loss."}, {"heading": "7.4 Benchmarking", "text": "We have compared the win rates of our Go program with the open source programs: GNU Go version 3.8, a traditional Go program, at level 10; and Pachi version 11.00 \"Retreat\" (Baudi Producs & Gailly, 2012), an MCTS program, with fixed 104 playouts per turn and thought disabled during the opponent's turn. The main reason we disable thinking in Pachi is that the kind of revolutionary rollouts we have implemented are quite expensive to run: the typical throughput is between 80 rollouts / s and 170 rollouts / s when executed on a single GPU. Parallel rollouts on multiple GPUs can greatly improve throughput: for example, a system consisting of 8x high-end NVIDIA Maxwell GPUs can achieve a peak throughput of about 1000 rollouts / s."}, {"heading": "7.5 Results", "text": "We show the profit rate of the different variants of our Go-Program against GNU Go, where the variants differed in their combinations of the previous Policy-Network and the Rollout-Policy-Network. We also tested variants in which we did not use MCTS and instead greedily played the best softmax activation of the previous Policy-Network; these lines are marked with \"none / greedy\" under the \"rollouts\" column. Our results are listed in Table 3. There are at least two interesting observations here. First, with a 6-layer-previous Policy-Network and 2-layer-rollout-Policy-Network, we are able to achieve comparable results for 12-layer-networks without MCTS (Tian & Zhu, 2015; Maddison et al.) This alone is a promising result and shows that the sophistication of the rollout-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Network is able to achieve comparable results for 12-layer-networks without MCTS (Tian & Zhu, 2015; Maddison et al.) This alone is a promising result and shows that the sophistication of the rollout-Policy-Policy-Policy-Network-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Policy-Promised in the rollout-Network."}, {"heading": "8 Discussion", "text": "In this work, we have demonstrated that combining Convolutionary Networks in the exploration phase of MCTS with Convolutionary Networks in the introductory phase is practicable and effective by using random MCTS in Thompson's Convolutionary Net-based implementations. We evaluated our Go program against the open source programs GNU Go and Pachi and found that the win rate of Convolutionary Net-based Go programs is relatively insensitive to reasonable batch size values, indicating that further scaling is possible. While we have compared the win rate of our own Go program with open source Go programs and the reported results of Convolutionary Net-based Go programs, there are commercial or otherwise closed Go programs that are much stronger: among them are the programs Zen, Crazy Stone (Coulom, 2007b) and Dol Baram, which include handicap games against high-level Net-based professional training programs that are best suited for long-term use."}, {"heading": "Acknowledgments", "text": "Thanks to Forrest Iandola for insightful discussions and Kostadin Ilov for supporting our computer systems."}], "references": [{"title": "Analysis of Thompson Sampling for the Multi-armed Bandit Problem", "author": ["Agrawal", "Shipra", "Goyal", "Navin"], "venue": "COLT \u201912,", "citeRegEx": "Agrawal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2012}, {"title": "Finite-time Analysis of the Multiarmed Bandit Problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Fischer", "Paul"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Bayesian Mixture Modeling and Inference based Thompson Sampling in Monte-Carlo Tree Search", "author": ["Bai", "Aijun", "Wu", "Feng", "Chen", "Xiaoping"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bai et al\\.", "year": 2013}, {"title": "Pachi: State of the Art Open Source Go Program", "author": ["Baud\u01d0s", "Petr", "Gailly", "Jean-loup"], "venue": "In Advances in Computer Games, pp", "citeRegEx": "Baud\u01d0s et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baud\u01d0s et al\\.", "year": 2012}, {"title": "A Parallel Monte-Carlo Tree Search Algorithm", "author": ["Cazenave", "Tristan", "Jouandeau", "Nicolas"], "venue": "In Computers and Games, pp", "citeRegEx": "Cazenave et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cazenave et al\\.", "year": 2008}, {"title": "An Empirical Evaluation of Thompson Sampling", "author": ["Chapelle", "Olivier", "Li", "Lihong"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Parallel Monte-Carlo Tree Search", "author": ["Chaslot", "Guillaume M.J.-B", "Winands", "Mark H.H", "van den Herik", "H. Jaap"], "venue": "In Computers and Games, pp", "citeRegEx": "Chaslot et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chaslot et al\\.", "year": 2008}, {"title": "Progressive strategies for Monte-Carlo tree search", "author": ["Chaslot", "Guillaume M.J.-B", "Winands", "Mark H.M", "van den Herik", "H. Jaap", "Uiterwijk", "Jos W.H.M", "Bouzy", "Bruno"], "venue": "New Mathematics and Neural Computation,", "citeRegEx": "Chaslot et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chaslot et al\\.", "year": 2008}, {"title": "Training Deep Convolutional Neural Networks to Play Go", "author": ["Clark", "Christopher", "Storkey", "Amos"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Clark et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2015}, {"title": "Computing Elo Ratings of Move Patterns in the Game of Go", "author": ["Coulom", "R\u00e9mi"], "venue": "In Computer Games Workshop,", "citeRegEx": "Coulom and R\u00e9mi.,? \\Q2007\\E", "shortCiteRegEx": "Coulom and R\u00e9mi.", "year": 2007}, {"title": "Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search", "author": ["Coulom", "R\u00e9mi"], "venue": "In Computers and Games, pp", "citeRegEx": "Coulom and R\u00e9mi.,? \\Q2007\\E", "shortCiteRegEx": "Coulom and R\u00e9mi.", "year": 2007}, {"title": "Combining Online and Offline Knowledge in UCT", "author": ["Gelly", "Sylvain", "Silver", "David"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Gelly et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gelly et al\\.", "year": 2007}, {"title": "Modification of UCT with Patterns in Monte-Carlo Go", "author": ["Gelly", "Sylvain", "Wang", "Yizao", "Munos", "R\u00e9mi", "Teytaud", "Olivier"], "venue": "Technical report,", "citeRegEx": "Gelly et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gelly et al\\.", "year": 2006}, {"title": "Efficiency of Static Knowledge Bias in Monte-Carlo Tree Search", "author": ["Ikeda", "Kokolo", "Viennot", "Simon"], "venue": "In Computers and Games, pp", "citeRegEx": "Ikeda et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ikeda et al\\.", "year": 2013}, {"title": "Enhancements in Monte Carlo Tree Search Algorithms for Biased Game Trees", "author": ["Imagawa", "Takahisa", "Kaneko", "Tomoyuki"], "venue": "In 2015 IEEE Conference on Computational Intelligence in Games,", "citeRegEx": "Imagawa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Imagawa et al\\.", "year": 2015}, {"title": "Thompson Sampling: An Asymptotically Optimal Finite-Time Analysis", "author": ["Kaufmann", "Emilie", "Korda", "Nathaniel", "Munos", "R\u00e9mi"], "venue": "In Proceedings of the 23rd International Conference on Algorithmic Learning Theory, pp", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Bandit Based Monte-Carlo Planning", "author": ["Kocsis", "Levente", "Szepesv\u00e1ri", "Csaba"], "venue": "In Proceedings of the 17th European Conference on Machine Learning,", "citeRegEx": "Kocsis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kocsis et al\\.", "year": 2006}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Move Evaluation in Go", "author": ["Maddison", "Chris J", "Huang", "Aja", "Sutskever", "Ilya", "Silver", "David"], "venue": "Using Deep Convolutional Neural Networks", "citeRegEx": "Maddison et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2014}, {"title": "Comparison of Different Selection Strategies in Monte-Carlo Tree Search for the Game of Tron", "author": ["Perick", "Pierre", "St-Pierre", "David L", "Maes", "Francis", "Ernst", "Damien"], "venue": "IEEE Conference on Computational Intelligence and Games, pp. 242\u2013249,", "citeRegEx": "Perick et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Perick et al\\.", "year": 2012}, {"title": "Monte-Carlo Simulation Balancing", "author": ["Silver", "David", "Tesauro", "Gerald"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Silver et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2009}, {"title": "Bayesian Pattern Ranking for Move Prediction in the Game of Go", "author": ["Stern", "David", "Herbrich", "Ralf", "Graepel", "Thore"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Stern et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Stern et al\\.", "year": 2006}, {"title": "Mimicking Go Experts with Convolutional Neural Networks", "author": ["Sutskever", "Ilya", "Nair", "Vinod"], "venue": "In Proceedings of the 18th International Conference on Artificial Neural Networks, Part II, pp", "citeRegEx": "Sutskever et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2008}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["Thompson", "William R"], "venue": null, "citeRegEx": "Thompson and R.,? \\Q1933\\E", "shortCiteRegEx": "Thompson and R.", "year": 1933}, {"title": "Better Computer Go Player with Neural Network and Long-term Prediction", "author": ["Tian", "Yuandong", "Zhu", "Yan"], "venue": null, "citeRegEx": "Tian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2015}, {"title": "Move Prediction in Go \u2014 Modeling Feature Interactions Using Latent Factors", "author": ["Wistuba", "Martin", "Schmidt-Thieme", "Lars"], "venue": "In KI 2013: Advances in Artificial Intelligence,", "citeRegEx": "Wistuba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wistuba et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 18, "context": "Recent work in convolutional networks for playing Go (Tian & Zhu, 2015; Maddison et al., 2014; Clark & Storkey, 2015; Sutskever & Nair, 2008) have produced neural move predictors for Go with up to 57.", "startOffset": 53, "endOffset": 141}, {"referenceID": 18, "context": "Previous work used an accurate move predictor built from a very deep convolutional network to guide the search tree exploration in MCTS (Tian & Zhu, 2015; Maddison et al., 2014).", "startOffset": 136, "endOffset": 177}, {"referenceID": 12, "context": "An early non-uniform rollout policy was pioneered in the Go-playing program MoGo (Gelly et al., 2006), which matched 3 \u00d7 3 board patterns, first in the local vicinity of the previous move, and then in the rest of the board.", "startOffset": 81, "endOffset": 101}, {"referenceID": 1, "context": "Our contribution is threefold: (1) we implement a MCTS-based Go-playing program that uses convolutional networks executed on the GPU in all parts; (2) we perform MCTS in batches to maximize the throughput of convolutions during rollouts; and (3) we demonstrate that Thompson sampling (Thompson, 1933) during exploration of the search tree in MCTS is a viable alternative to UCB1 (Auer et al., 2002).", "startOffset": 379, "endOffset": 398}, {"referenceID": 10, "context": "An early non-uniform rollout policy was pioneered in the Go-playing program MoGo (Gelly et al., 2006), which matched 3 \u00d7 3 board patterns, first in the local vicinity of the previous move, and then in the rest of the board. Coulom (2007a) extended the pattern features by weighting them by relative strength using a minorization-maximization algorithm and choosing patterns with probability determined by a Bradley-Terry model.", "startOffset": 82, "endOffset": 239}, {"referenceID": 12, "context": "The pioneering Go-playing program MoGo featured pattern-based rollouts (Gelly et al., 2006).", "startOffset": 71, "endOffset": 91}, {"referenceID": 13, "context": "Clark & Storkey (2015) and Maddison et al. (2014) later extended the results to deep convolutional networks.", "startOffset": 27, "endOffset": 50}, {"referenceID": 13, "context": "Clark & Storkey (2015) and Maddison et al. (2014) later extended the results to deep convolutional networks. More recently, Tian & Zhu (2015) showed that training with multiple labels for long term prediction further improved the accuracy and playing strength of deep convolutional move predictors.", "startOffset": 27, "endOffset": 142}, {"referenceID": 8, "context": "The pioneering Go-playing program MoGo featured pattern-based rollouts (Gelly et al., 2006). Stern et al. (2006) learned patterns and local features using Bayesian ranking.", "startOffset": 72, "endOffset": 113}, {"referenceID": 8, "context": "The pioneering Go-playing program MoGo featured pattern-based rollouts (Gelly et al., 2006). Stern et al. (2006) learned patterns and local features using Bayesian ranking. Coulom (2007a) computed the likelihood of patterns and local features with a Bradley-Terry model.", "startOffset": 72, "endOffset": 188}, {"referenceID": 8, "context": "The pioneering Go-playing program MoGo featured pattern-based rollouts (Gelly et al., 2006). Stern et al. (2006) learned patterns and local features using Bayesian ranking. Coulom (2007a) computed the likelihood of patterns and local features with a Bradley-Terry model. Wistuba & Schmidt-Thieme (2013) used latent factor ranking to achieve move prediction accuracy of 41%.", "startOffset": 72, "endOffset": 303}, {"referenceID": 8, "context": "The pioneering Go-playing program MoGo featured pattern-based rollouts (Gelly et al., 2006). Stern et al. (2006) learned patterns and local features using Bayesian ranking. Coulom (2007a) computed the likelihood of patterns and local features with a Bradley-Terry model. Wistuba & Schmidt-Thieme (2013) used latent factor ranking to achieve move prediction accuracy of 41%. Thompson sampling has been applied to Monte Carlo tree search in non-computer Go domains in Perick et al. (2012), Bai et al.", "startOffset": 72, "endOffset": 487}, {"referenceID": 2, "context": "(2012), Bai et al. (2013), and Imagawa & Kaneko (2015).", "startOffset": 8, "endOffset": 26}, {"referenceID": 2, "context": "(2012), Bai et al. (2013), and Imagawa & Kaneko (2015). In those previous works, Thompson sampling was compared with UCB1 or other bandit algorithms based on their performance on specific tasks (e.", "startOffset": 8, "endOffset": 55}, {"referenceID": 2, "context": "(2012), Bai et al. (2013), and Imagawa & Kaneko (2015). In those previous works, Thompson sampling was compared with UCB1 or other bandit algorithms based on their performance on specific tasks (e.g., maximizing reward), rather than our focus of using Thompson sampling guide batch parallelism. Cazenave & Jouandeau (2008) and Chaslot et al.", "startOffset": 8, "endOffset": 323}, {"referenceID": 2, "context": "(2012), Bai et al. (2013), and Imagawa & Kaneko (2015). In those previous works, Thompson sampling was compared with UCB1 or other bandit algorithms based on their performance on specific tasks (e.g., maximizing reward), rather than our focus of using Thompson sampling guide batch parallelism. Cazenave & Jouandeau (2008) and Chaslot et al. (2008a) introduced approaches for parallelizing MCTS, including \u201croot parallelism\u201d and \u201ctree parallelism.", "startOffset": 8, "endOffset": 350}, {"referenceID": 18, "context": "Method Exploration Rollouts Our method (PPN+RPN) 12 layer PPN, 128 filters 2- or 3-layer RPN, 16 filters (Maddison et al., 2014) 12 layer PPN, 128 filters pattern-based (Tian & Zhu, 2015) 12 layer PPN, 384 filters, 1\u20133 step lookahead pattern-based", "startOffset": 105, "endOffset": 128}, {"referenceID": 1, "context": "The canonical version of MCTS is UCT (Kocsis & Szepesv\u00e1ri, 2006), where the selection criterion in the exploration phase is determined by the UCB1 multi-armed bandit algorithm (Auer et al., 2002).", "startOffset": 176, "endOffset": 195}, {"referenceID": 12, "context": "However, it was quickly found that using nonuniformly random rollouts (for example, based on the probabilities of local features and patterns as in (Coulom, 2007a)) resulted in stronger play by MCTS-based Go-playing programs (Gelly et al., 2006).", "startOffset": 225, "endOffset": 245}, {"referenceID": 18, "context": "The most accurate convolutional network architectures for predicting moves in Go tend to be very deep, with at least a dozen layers (Tian & Zhu, 2015; Maddison et al., 2014).", "startOffset": 132, "endOffset": 173}, {"referenceID": 17, "context": "The layers also have hundreds of convolution filters, and like most modern deep convolutional nets use rectified linear units as their nonlinearity (Krizhevsky et al., 2012).", "startOffset": 148, "endOffset": 173}, {"referenceID": 15, "context": "Instead, we substitute for UCB1 the probabilistic bandit algorithm Thompson sampling (Thompson, 1933) as the search policy in MCTS, a choice justified by recent empirical evidence (Chapelle & Li, 2011), as well as proofs of its comparable regret bounds to those of UCB1 (Agrawal & Goyal, 2012; Kaufmann et al., 2012).", "startOffset": 270, "endOffset": 316}, {"referenceID": 18, "context": "They consist of a first layer, followed by repeated inner layers, and completed by a last layer before a softmax probability output layer; this is the same architectural pattern used by Maddison et al. (2014) and Tian & Zhu (2015).", "startOffset": 186, "endOffset": 209}, {"referenceID": 18, "context": "They consist of a first layer, followed by repeated inner layers, and completed by a last layer before a softmax probability output layer; this is the same architectural pattern used by Maddison et al. (2014) and Tian & Zhu (2015). Three of them (A, B, and C) are very deep nets and are prior policy networks.", "startOffset": 186, "endOffset": 231}, {"referenceID": 18, "context": "0 (Maddison et al., 2014) 36 5\u00d7 5\u00d7 128 3\u00d7 3\u00d7 128\u00d7 10 3\u00d7 3\u00d7 2 12 layers 55.", "startOffset": 2, "endOffset": 25}, {"referenceID": 18, "context": "The other two architectures (Maddison et al., 2014) and (Tian & Zhu, 2015) are state of the art deep convolutional nets for predicting moves in Go.", "startOffset": 28, "endOffset": 51}, {"referenceID": 18, "context": "First, with a 6-layer prior policy network and 2-layer or 3-layer rollout policy network, we are able to obtain comparable results to 12-layer networks without MCTS (Tian & Zhu, 2015; Maddison et al., 2014).", "startOffset": 165, "endOffset": 206}, {"referenceID": 18, "context": "3 GNU Go 12 layer PPN (Maddison et al., 2014) none/greedy 97.", "startOffset": 22, "endOffset": 45}, {"referenceID": 18, "context": "12-layer network of Maddison et al. (2014) without MCTS.", "startOffset": 20, "endOffset": 43}, {"referenceID": 18, "context": "12-layer network of Maddison et al. (2014) without MCTS. Our MCTS Go program with a 12-layer prior policy and 2-layer rollout policy is comparable to the larger 1-step darkforest 12-layer network without MCTS of Tian & Zhu (2015). Their network was trained using extra features, including ko point, stone history, and opponent rank, and also has 384 convolution filters compared to 128 filters in our network.", "startOffset": 20, "endOffset": 230}, {"referenceID": 18, "context": "12-layer network of Maddison et al. (2014) without MCTS. Our MCTS Go program with a 12-layer prior policy and 2-layer rollout policy is comparable to the larger 1-step darkforest 12-layer network without MCTS of Tian & Zhu (2015). Their network was trained using extra features, including ko point, stone history, and opponent rank, and also has 384 convolution filters compared to 128 filters in our network. The 3-step darkfores2 12-layer network with traditional pattern-based MCTS rollouts by Tian & Zhu (2015) performs the best against both GNU Go and Pachi, although they also employ an enhanced training method using 3-step lookahead for long-term prediction and tuned learning rate, whereas we only train our networks using single-step lookahead.", "startOffset": 20, "endOffset": 515}, {"referenceID": 18, "context": "5 Pachi 12 layer PPN (Maddison et al., 2014) 55.", "startOffset": 21, "endOffset": 44}], "year": 2015, "abstractText": "In this work, we present a MCTS-based Go-playing program which uses convolutional networks in all parts. Our method performs MCTS in batches, explores the Monte Carlo search tree using Thompson sampling and a convolutional network, and evaluates convnet-based rollouts on the GPU. We achieve strong win rates against open source Go programs and attain competitive results against state of the art convolutional net-based Go-playing programs.", "creator": "LaTeX with hyperref package"}}}