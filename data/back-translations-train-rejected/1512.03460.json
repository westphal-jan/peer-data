{"id": "1512.03460", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Dec-2015", "title": "Neural Self Talk: Image Understanding via Continuous Questioning and Answering", "abstract": "In this paper we consider the problem of continuously discovering image contents by actively asking image based questions and subsequently answering the questions being asked. The key components include a Visual Question Generation (VQG) module and a Visual Question Answering module, in which Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are used. Given a dataset that contains images, questions and their answers, both modules are trained at the same time, with the difference being VQG uses the images as input and the corresponding questions as output, while VQA uses images and questions as input and the corresponding answers as output. We evaluate the self talk process subjectively using Amazon Mechanical Turk, which show effectiveness of the proposed method.", "histories": [["v1", "Thu, 10 Dec 2015 21:58:46 GMT  (7020kb,D)", "http://arxiv.org/abs/1512.03460v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.RO", "authors": ["yezhou yang", "yi li", "cornelia fermuller", "yiannis aloimonos"], "accepted": false, "id": "1512.03460"}, "pdf": {"name": "1512.03460.pdf", "metadata": {"source": "CRF", "title": "Neural Self Talk: Image Understanding via Continuous Questioning and Answering", "authors": ["Yezhou Yang", "Yi Li", "Cornelia Fermuller", "Yiannis Aloimonos"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This is the question we have to ask ourselves in order to answer it, this is the question we have to ask ourselves in order to answer it, this is the question we have to ask ourselves in order to answer it, this is the question we have to ask ourselves in order to answer it."}, {"heading": "2 Related Work", "text": "Our work mainly relates to three lines of research on the understanding of natural images: 1) Generation, 2) Image captions, and 3) Visual questions to be answered.Question generation is one of the central challenges in natural languages. Previous approaches to natural language phrases have been mainly through template matching in a conservative manner [Brown et al., 2005; Heilman and Smith, 2010; Ali et al., 2010]. [Ren et al., 2015] suggested using a parsing-based approach to synthetically create question and answer pairs from image annotations. In this paper, we propose a visual question generation module using a technique adapted directly from the image description system [Karpathy and Li, 2014] that is data-driven and challenges the potential production space is much larger than previous parsing or template-based approaches, and the trained module takes the image only as input.In Image, in addition to capturing the deep neural networks mentioned in the art."}, {"heading": "3 Self talk: Theory and Practice", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Theory and Motivation", "text": "The phenomenon of \"self-speaking\" has been researched in the field of psychology for hundreds of years. It is defined as a specific form of intrapersonal communication: the internal use of language or thought by a communicator. Using the terms computer science and engineering, it might be useful to imagine intrapersonal communication taking place in the mind of the individual in a model that includes a sender, receiver and a potential feedback loop. This process occurs consciously or unconsciously in our minds. The ability to ask and answer questions for oneself is also crucial to learning. The question of raising and answering questions facilitates the learning process. For example, in the field of education, mutual questioning has been studied as a strategy in which students assume the role of teacher by formulating their own list of questions about reading material. In this essay, we consider this as another challenge for computers, and we believe that a key to intelligence raises the right questions."}, {"heading": "3.2 Our Approach", "text": "We have two hypotheses for validation in this work: 1) with current progress in caption, a system can be trained to generate reasonable and relevant questions, and 2) by combining it with a visual question-answering system, a system could be trained to generate people like \"self talk\" with promising success.In this section, we present a frustratingly simple policy to generate a sequence of questions that we guide through the VQA module a = V isualAnswer (q, I) to achieve an answer in such a way that we call the \"self talk\" question and answer pairs {(q1, a1), (qN, aN)}. The \"self talk\" is further evaluated."}, {"heading": "4 Experiments", "text": "We will test the presented approach using two visual questioners (VQA), namely DARQUAR [Malinowski and Fritz, 2014] and MSCOCO-VQA [Antol et al., 2015]. In the experiments on these two datasets, we will first report on the generation of questions using a standard language-based evaluation metric. In order to then evaluate the performance of the \"soliloquy,\" we will report on the AMT results and offer further discussions."}, {"heading": "4.1 Datasets", "text": "We first briefly describe the two test beds we use for the experiments.DAQUAR: Indoor Scenes: DAQUAR [Malinowski and Fritz, 2014] vqa dataset contains 12,468 human pairs of questions on 1,449 images of indoor scenes. The training set contains 795 images and 6,793 question pairs, and the test set contains 654 images and 5,675 question pairs. We conduct experiments for the complete dataset with all classes, instead of their reduced dataset, in which the output space is limited to only 37 object categories and a total of 25 test images. This is because the complete dataset is much more sophisticated and the results are more statistically meaningful. COCO: General Domain: MSCOCO-VQA [Antol et al., 2015] is the latest VQA dataset containing open questions about any images from the Internet."}, {"heading": "4.2 Question Generation Evaluation", "text": "We first trained our multimodal RNN to generate questions on full frames, with the aim of verifying whether the model is rich enough to support the mapping of image data to word sequences. We report on the BLEU [Papineni et al., 2002], METEOR [Lavie, 2014], ROUGE [Lin, 2004] and CIDER [Vedantam et al., 2014] results calculated using the coco-capture code [Chen et al., 2015]. Each method evaluates a question generated by the candidates by measuring how well it matches a number of reference questions (an average of eight questions for DAQUAR datasets and three questions for MSCOCO-VQA) written by humans. To further validate the performance of the question, we list the performance indicators reported in the status metrics of image caption work [KarpaLi and, 2014]."}, {"heading": "4.3 \u201cSelf talk\u201d Evaluation", "text": "In Table 2, we report on the average score and standard deviation for each measurement variable. We randomly drew 100 and 1000 test samples from DAQUAR and MSCOCO-VQA test sets for the human evaluation reported here. Human evaluation indicates that the questions generated have reached near-human readability. The accuracy of the generated \"soliloquy\" has, on average, some relevance to the image, and according to Turks, the imaginary companion robot behaves on average beyond \"a bit of a human,\" but below the category of \"half human, half machine.\" In addition, we asked Turks to choose between five immediate feelings based on the performance of their companion robot. Fig. 7 and Fig. 8 show the feedback we received from users. Given that the performance of the robot \"self-talk\" is still far from human performance, most Turks thought they liked such a robot or found it amusing."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we look at the problem of image understanding as a process of self-questioning and self-response, and present a primitive method of \"talking to oneself\" based on two deep neural network modules. Starting from the experimental assessment of both question performance and final \"talking to oneself,\" we show that the presented method has achieved an acceptable level of success. There are several potential ways to improve the performance of intelligent \"talking to oneself.\" The role of common sense plays a critical role in question-raising and answering processes for humans [Aditya et al., 2015]. The experimental result shows that by learning the model from large commented question pairs, our system implicitly encodes a certain level of common sense. The real challenge is to deal with situations where visual input collides with common sense learned from context data. In our experiment, it seems that the model tends to trust oneself more than its visual input."}], "references": [{"title": "From images to sentences through scene description graphs using commonsense reasoning and knowledge", "author": ["Somak Aditya", "Yezhou Yang", "Chitta Baral", "Cornelia Fermuller", "Yiannis Aloimonos"], "venue": "arXiv preprint arXiv:1511.03292,", "citeRegEx": "Aditya et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of QG2010: The Third Workshop on Question Generation", "author": ["Husam Ali", "Yllias Chali", "Sadid A Hasan. Automation of question generation from sentences"], "venue": "pages 58\u201367,", "citeRegEx": "Ali et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "The cognitive dialogue: A new model for vision implementing common sense reasoning", "author": ["Yiannis Aloimonos", "Cornelia Ferm\u00fcller"], "venue": "Image and Vision Computing, 34:42\u201344,", "citeRegEx": "Aloimonos and Ferm\u00fcller. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "Antol et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic question generation for vocabulary assessment", "author": ["Brown et al", "2005] Jonathan C Brown", "Gwen A Frishkoff", "Maxine Eskenazi"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "al. et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al. et al\\.", "year": 2005}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["Xinlei Chen", "C Lawrence Zitnick"], "venue": "arXiv preprint arXiv:1411.5654,", "citeRegEx": "Chen and Zitnick. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollar", "C Lawrence Zitnick"], "venue": "arXiv preprint arXiv:1504.00325,", "citeRegEx": "Chen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1411.4389,", "citeRegEx": "Donahue et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Image description using visual dependency representations", "author": ["Elliott", "Keller", "2013] Desmond Elliott", "Frank Keller"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Elliott et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2013}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Farhadi et al", "2010] Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth"], "venue": "In Proceedings of the 11th European Conference on Computer Vi-", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Are you talking", "author": ["Gao et al", "2015] Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Good question! statistical ranking for question generation", "author": ["Heilman", "Smith", "2010] Michael Heilman", "Noah A Smith"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Heilman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2010}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Framing image description as a ranking task: Data", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier"], "venue": "models and evaluation metrics. Journal of Artificial Intelligence Research, pages 853\u2013899,", "citeRegEx": "Hodosh et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "author": ["Justin Johnson", "Ranjay Krishna", "Michael Stark", "Jia Li", "Michael Bernstein", "Li Fei-Fei. Image retrieval using scene graphs"], "venue": "June", "citeRegEx": "Johnson et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Fei-Fei Li"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "Karpathy and Li. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "Kiros et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg"], "venue": "Proceedings of the 24th CVPR,", "citeRegEx": "Kulkarni et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Collective generation of natural image descriptions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume", "author": ["Kuznetsova et al", "2012] Polina Kuznetsova", "Vicente Ordonez", "Alexander C. Berg", "Tamara L. Berg", "Yejin Choi"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski Alon Lavie"], "venue": "ACL 2014, page 376,", "citeRegEx": "Lavie. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "Computer Vision\u2013ECCV 2014, pages 740\u2013755. Springer,", "citeRegEx": "Lin et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8,", "citeRegEx": "Lin. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning to answer questions from image using convolu", "author": ["Ma et al", "2015] Lin Ma", "Zhengdong Lu", "Hang Li"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Towards a visual turing challenge", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "arXiv preprint arXiv:1410.8027,", "citeRegEx": "Malinowski and Fritz. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neuralbased approach to answering questions about images", "author": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz"], "venue": "arXiv preprint arXiv:1505.01121,", "citeRegEx": "Malinowski et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L Yuille"], "venue": "arXiv preprint arXiv:1410.1090,", "citeRegEx": "Mao et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The uncanny valley [from the field", "author": ["Masahiro Mori", "Karl F MacDorman", "Norri Kageki"], "venue": "Robotics & Automation Magazine, IEEE, 19(2):98\u2013100,", "citeRegEx": "Mori et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Ordonez et al", "2011] Vicente Ordonez", "Girish Kulkarni", "Tamara L. Berg"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni et al", "2002] Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "al. et al\\.,? \\Q2002\\E", "shortCiteRegEx": "al. et al\\.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), pages 1532\u20131543,", "citeRegEx": "Pennington et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The turing test: Verbal behavior as the hallmark of intelligence edited by stuart shieber", "author": ["William J. Rapaport"], "venue": "Computational Linguistics, 31(3):407\u2013412, September", "citeRegEx": "Rapaport. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Exploring models and data for image question answering", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "arXiv preprint arXiv:1505.02074,", "citeRegEx": "Ren et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating semantically precise scene graphs from textual descriptions for improved image retrieval", "author": ["Schuster et al", "2015] Sebastian Schuster", "Ranjay Krishna", "Angel Chang", "Li Fei-Fei", "Christopher D. Manning"], "venue": "In Proceedings of the Fourth Workshop on Vision and Language,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "HAL\u2019s Legacy: 2001\u2019s Computer as Dream and Reality", "author": ["David G Stork"], "venue": "MIT Press,", "citeRegEx": "Stork. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Cider: Consensusbased image description evaluation", "author": ["Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "arXiv preprint arXiv:1411.5726,", "citeRegEx": "Vedantam et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Yang et al", "2011] Yezhou Yang", "Ching Lik Teo", "III Hal Daum\u00e9", "Yiannis Aloimonos"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "I2t: Image parsing to text description", "author": ["Benjamin Z. Yao", "Xiong Yang", "Liang Lin", "Mun Wai Lee", "Song Chun Zhu"], "venue": "Proceedings of the IEEE, 98(8):1485\u2013 1508,", "citeRegEx": "Yao et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Computer Vision (ICCV)", "author": ["Xiaodong Yu", "Cornelia Fermuller", "Ching Lik Teo", "Yezhou Yang", "Yiannis Aloimonos. Active scene recognition with vision", "language"], "venue": "2011 IEEE International Conference on, pages 810\u2013817. IEEE,", "citeRegEx": "Yu et al.. 2011", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 34, "context": "Acclaimed as \u201cone of the last cognitive tasks to be performed well by computers\u201d [Stork, 1998], exploring and analyzing novel visual scenes is a journey of continuous discovery, which requires not just passively detecting objects and segmenting the images, but arguably more importantly, actively asking the right questions and subsequently closing the semantic loop by answering the questions being asked.", "startOffset": 81, "endOffset": 94}, {"referenceID": 30, "context": "\u201d [Rapaport, 2005] ar X iv :1 51 2.", "startOffset": 2, "endOffset": 18}, {"referenceID": 3, "context": "Question Answering\u201d [Antol et al., 2015] problem recently becomes an important area in computer vision and machine learning, and sometimes it is referred as Visual Turing challenge [Malinowski and Fritz, 2014].", "startOffset": 20, "endOffset": 40}, {"referenceID": 23, "context": ", 2015] problem recently becomes an important area in computer vision and machine learning, and sometimes it is referred as Visual Turing challenge [Malinowski and Fritz, 2014].", "startOffset": 148, "endOffset": 176}, {"referenceID": 24, "context": "A few approaches [Malinowski et al., 2015; Ren et al., 2015] have shown that deep neural nets again can be trained to answer a related question for an arbitrary scene with promising success.", "startOffset": 17, "endOffset": 60}, {"referenceID": 31, "context": "A few approaches [Malinowski et al., 2015; Ren et al., 2015] have shown that deep neural nets again can be trained to answer a related question for an arbitrary scene with promising success.", "startOffset": 17, "endOffset": 60}, {"referenceID": 14, "context": "These limitations have been pointed out by several recent works [Johnson et al., 2015; Schuster et al., 2015; Aditya et al., 2015] and have been addressed partially by introducing middle layer knowledge representations.", "startOffset": 64, "endOffset": 130}, {"referenceID": 0, "context": "These limitations have been pointed out by several recent works [Johnson et al., 2015; Schuster et al., 2015; Aditya et al., 2015] and have been addressed partially by introducing middle layer knowledge representations.", "startOffset": 64, "endOffset": 130}, {"referenceID": 23, "context": "4, we report experiments on two publicly available datasets (DAQUAR for indoor domain [Malinowski and Fritz, 2014] and COCO for arbitrary domain [Antol et al.", "startOffset": 86, "endOffset": 114}, {"referenceID": 3, "context": "4, we report experiments on two publicly available datasets (DAQUAR for indoor domain [Malinowski and Fritz, 2014] and COCO for arbitrary domain [Antol et al., 2015]).", "startOffset": 145, "endOffset": 165}, {"referenceID": 1, "context": "Previous approaches of question generation from natural language sentences are mainly through template matching in a conservative manner [Brown et al., 2005; Heilman and Smith, 2010; Ali et al., 2010].", "startOffset": 137, "endOffset": 200}, {"referenceID": 31, "context": "[Ren et al., 2015] proposed to use parsing based approach to synthetically create question and answer pairs from image annotaFigure 2: The flow chart or our approach.", "startOffset": 0, "endOffset": 18}, {"referenceID": 15, "context": "In this paper, we propose a visual question generation module through a technique directly adapted from image captioning system [Karpathy and Li, 2014], which is data driven and the potential output questions space is significantly larger than previous parsing or template based approaches, and the trained module only takes in image as input.", "startOffset": 128, "endOffset": 151}, {"referenceID": 13, "context": "This includes the works that retrieves and ranks sentences from training sets given an image such as [Hodosh et al., 2013], [Farhadi et al.", "startOffset": 101, "endOffset": 122}, {"referenceID": 17, "context": "[Elliott and Keller, 2013], [Kulkarni et al., 2011], [Kuznetsova et al.", "startOffset": 28, "endOffset": 51}, {"referenceID": 38, "context": ", 2011], [Yao et al., 2010] are some of the works that have generated descriptions by stitching together annotations or applying templates on detected image content.", "startOffset": 9, "endOffset": 27}, {"referenceID": 3, "context": "In the filed of Visual Question Answering, very recently researchers spent a significant amount of efforts on both creating datasets and proposing new models [Antol et al., 2015; Malinowski et al., 2015; Gao et al., 2015; Ma et al., 2015].", "startOffset": 158, "endOffset": 238}, {"referenceID": 24, "context": "In the filed of Visual Question Answering, very recently researchers spent a significant amount of efforts on both creating datasets and proposing new models [Antol et al., 2015; Malinowski et al., 2015; Gao et al., 2015; Ma et al., 2015].", "startOffset": 158, "endOffset": 238}, {"referenceID": 3, "context": "Interestingly both [Antol et al., 2015] and [Gao et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 20, "context": ", 2015] adapted MS-COCO [Lin et al., 2014] images and created an open domain dataset with human generated questions and answers.", "startOffset": 24, "endOffset": 42}, {"referenceID": 24, "context": "Both [Malinowski et al., 2015] and [Gao et al.", "startOffset": 5, "endOffset": 30}, {"referenceID": 24, "context": "Specifically, [Malinowski et al., 2015] applies a single network to handle both encoding and decoding, while [Gao et al.", "startOffset": 14, "endOffset": 39}, {"referenceID": 31, "context": "More recently, the work from [Ren et al., 2015] reported state-of-the-art VQA performance using multiple benchmarks.", "startOffset": 29, "endOffset": 47}, {"referenceID": 39, "context": "For a specific task, such as scene category recognition, this formulation has been proven to be efficient [Yu et al., 2011].", "startOffset": 106, "endOffset": 123}, {"referenceID": 39, "context": "From a practical point of view, the revealing of \u201cself talk\u201d makes computers more human like, and the presented system has application potential in creating robotic companions [Yu et al., 2011].", "startOffset": 176, "endOffset": 193}, {"referenceID": 15, "context": "We adopted the method from [Karpathy and Li, 2014], where a simple but Algorithm 1 A Primitive \u201cSelf Talk\u201d Generation Algorithm", "startOffset": 27, "endOffset": 50}, {"referenceID": 31, "context": "We adopted the approach from [Ren et al., 2015], which introduced a model builds directly on top of the long short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997] sen-", "startOffset": 29, "endOffset": 47}, {"referenceID": 12, "context": ", 2015], which introduced a model builds directly on top of the long short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997] sen-", "startOffset": 94, "endOffset": 128}, {"referenceID": 33, "context": "The model uses the last hidden layer of the 19-layer Oxford VGG Conv Net [Simonyan and Zisserman, 2014] trained on ImageNet 2014 Challenge as the visual embeddings.", "startOffset": 73, "endOffset": 103}, {"referenceID": 29, "context": "The model also uses word embedding model from general purpose skip-gram embedding [Pennington et al., 2014].", "startOffset": 82, "endOffset": 107}, {"referenceID": 31, "context": "Please refer to [Ren et al., 2015] for the details.", "startOffset": 16, "endOffset": 34}, {"referenceID": 23, "context": "We test the presented approach on two visual question answering (VQA) datasets, namely, DARQUAR [Malinowski and Fritz, 2014] and MSCOCO-VQA [Antol et al.", "startOffset": 96, "endOffset": 124}, {"referenceID": 3, "context": "We test the presented approach on two visual question answering (VQA) datasets, namely, DARQUAR [Malinowski and Fritz, 2014] and MSCOCO-VQA [Antol et al., 2015].", "startOffset": 140, "endOffset": 160}, {"referenceID": 23, "context": "DAQUAR: Indoor Scenes: DAQUAR [Malinowski and Fritz, 2014] vqa dataset contains 12,468 human question answer pairs on 1,449 images of indoor scene.", "startOffset": 30, "endOffset": 58}, {"referenceID": 3, "context": "COCO: General Domain: MSCOCO-VQA [Antol et al., 2015] is the latest VQA dataset that contains openended questions about arbitrary images collect from the Internet.", "startOffset": 33, "endOffset": 53}, {"referenceID": 19, "context": ", 2002], METEOR [Lavie, 2014], ROUGE [Lin, 2004] and CIDEr [Vedantam et al.", "startOffset": 16, "endOffset": 29}, {"referenceID": 21, "context": ", 2002], METEOR [Lavie, 2014], ROUGE [Lin, 2004] and CIDEr [Vedantam et al.", "startOffset": 37, "endOffset": 48}, {"referenceID": 35, "context": ", 2002], METEOR [Lavie, 2014], ROUGE [Lin, 2004] and CIDEr [Vedantam et al., 2014] scores computed with the coco-caption code [Chen et al.", "startOffset": 59, "endOffset": 82}, {"referenceID": 6, "context": ", 2014] scores computed with the coco-caption code [Chen et al., 2015].", "startOffset": 51, "endOffset": 70}, {"referenceID": 15, "context": "To further validate the performance of question generation, we further list the performance metrics reported in the stateof-the-art image captioning work [Karpathy and Li, 2014].", "startOffset": 154, "endOffset": 177}, {"referenceID": 15, "context": "coco-Caption [Karpathy and Li, 2014] .", "startOffset": 13, "endOffset": 36}, {"referenceID": 26, "context": "And only very few of the users felt scared, which indicates that our image understanding performance is far from being trapped into the so-called \u201cuncanny valley\u201d [Mori et al., 2012] of machine intelligence.", "startOffset": 163, "endOffset": 182}, {"referenceID": 0, "context": "Common-sense knowledge has a crucial role in question raising and answering process for human beings [Aditya et al., 2015].", "startOffset": 101, "endOffset": 122}, {"referenceID": 2, "context": "This indicates a more sophisticated dialogue generation process (such as a cognitive dialogue [Aloimonos and Ferm\u00fcller, 2015]), and it can also potentially prevent self-contradictions happened in this paper\u2019s generated results (see last comment in Fig.", "startOffset": 94, "endOffset": 125}], "year": 2015, "abstractText": "In this paper we consider the problem of continuously discovering image contents by actively asking image based questions and subsequently answering the questions being asked. The key components include a Visual Question Generation (VQG) module and a Visual Question Answering module, in which Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are used. Given a dataset that contains images, questions and their answers, both modules are trained at the same time, with the difference being VQG uses the images as input and the corresponding questions as output, while VQA uses images and questions as input and the corresponding answers as output. We evaluate the self talk process subjectively using Amazon Mechanical Turk, which show effectiveness of the proposed method.", "creator": "LaTeX with hyperref package"}}}