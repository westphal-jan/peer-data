{"id": "1512.03465", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Dec-2015", "title": "Measuring Semantic Relatedness using Mined Semantic Analysis", "abstract": "Mined Semantic Analysis (MSA) is a novel distributional semantics approach which employs data mining techniques. MSA embraces knowledge-driven analysis of natural languages. It uncovers implicit relations between concepts by mining for their associations in target encyclopedic corpora. MSA exploits not only target corpus content but also its knowledge graph (e.g., \"See also\" link graph of Wikipedia). Empirical results show competitive performance of MSA compared to prior state-of-the-art methods for measuring semantic relatedness on benchmark data sets. Additionally, we introduce the first analytical study to examine statistical significance of results reported by different semantic relatedness methods. Our study shows that, top performing results could be statistically equivalent though mathematically different. The study positions MSA as one of state-of-the-art methods for measuring semantic relatedness.", "histories": [["v1", "Thu, 10 Dec 2015 22:15:10 GMT  (32kb)", "https://arxiv.org/abs/1512.03465v1", "7 pages"], ["v2", "Sat, 1 Oct 2016 03:46:23 GMT  (96kb,D)", "http://arxiv.org/abs/1512.03465v2", "8 pages, 1 figure"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["walid shalaby", "wlodek zadrozny"], "accepted": false, "id": "1512.03465"}, "pdf": {"name": "1512.03465.pdf", "metadata": {"source": "CRF", "title": "Measuring Semantic Relatedness using Mined Semantic Analysis", "authors": ["Walid Shalaby", "Wlodek Zadrozny"], "emails": ["wzadrozn}@uncc.edu"], "sections": [{"heading": null, "text": "Categories and Subject Descriptions H.3.1 [Storage and Retrieval of Information]: Content Analysis and Indexing - Language ProcessingGeneral Terms Algorithms, Scale Keywords Semantic Relationship, Concept Space Models, Conceptual Bag of Concepts, Association Rules Mining"}, {"heading": "1. INTRODUCTION", "text": "In this sense, it is important that people who are able to understand and understand themselves should also be put in the position in which they live."}, {"heading": "2. RELATED WORK", "text": "It is a question of what the future of the world is like, and what the future of the world is like, and what the future of the world is like. \"It is a question of the future of the world, the future of the world, the future of the world, the future of the world, the future of the world, the future of the world, the future of the world, the future of the world, the future of the world, the future of the world, the future of the world, the future of the world, the future of the world, the future of the world, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future"}, {"heading": "3. MINED SEMANTIC ANALYSIS", "text": "We call our approach Mined Semantic Analysis (MSA) because it uses data mining techniques to discover the conceptual space of textual structures. Therefore, the motivation behind our approach is to mitigate a notable gap in earlier conceptual space models limited to direct associations between words and concepts. Therefore, these models lack the ability to transfer the association relationship to other latent concepts that contribute to the meaning of these words.Figure 1 shows the architecture of MSA. In short, MSA generates the conceptual space of a given text by using two offline repositories: 1) a search index of Wikipedia articles, and 2) a concept-concept association repository created by clearing the \"See also\" link diagram of Wikipedia concepts (articles). First, the explicit conceptual space is constructed by retrieving concepts (title of articles)."}, {"heading": "3.1 The Search Index", "text": "MSA begins by constructing the conceptual space of terms (articles) by looking for an initial set of explicit candidate concepts. To this end, we create a search index of a conceptual corpus such as Wikipedia, in which each article represents a concept, similar to the idea of the reverse index introduced in ESA [Gabrilovich and Markovitch, 2007]. We create the index using Apache Lucene2, an open source indexing and search engine. For each article, we index the title, content, length and section \"See also.\" During the search, we use some parameters to optimize the search space. Specifically, we define the following parameters to ensure more control over the search: article length (L): minimum length of the Wikipedia article in characters that exclude sections such as \"References,\" \"Categories...\" etc. Number of terms (M): maximum number of concepts that are relevant to (the) (the) article (s) as (s) called."}, {"heading": "3.2 Association Rules Mining", "text": "To discover the implicit concepts, we use rule mining [Agrawal et al., 1993] to learn implicit relationships between concepts linking Wikipedia's \"See also.\" Formally, given a set of concepts C = {c1, c2,..., cN} of size N (i.e., all Wikipedia articles), we build a dictionary of transactions T = {t1, t2, t3,..., tM} of size M \u2264 N. Each transaction in T contains a subset of concepts in C. t is constructed from each article in Wikipedia that contains at least one entry in its \"See also\" section. If an article contains concept c1 with entries in its \"See also\" section, \"it refers to concepts {c2, c3, cn}, then a transaction in C is constructed."}, {"heading": "3.3 Constructing the Concept Space", "text": "In the search phase, t is presented as a search query and searched in the Wikipedia search index, resulting in a weighted set of articles that best fits the vector space model. We refer to the set of concepts representing these articles as Cs and are presented as in Equation 2: Cs = (ci, wi): ci, C and i < = N}, with the weight of ci, (ci), length (ci) > = L, | Cs | < = M (2) being noted that we search all articles whose content length and title grams meet the thresholds L and D. The weight of ci is denoted by wi and represents the correspondence between t and ci as supplied by the search engine. In the expansion phase, we use implicit rules to expand each concept."}, {"heading": "3.4 Relatedness Scoring", "text": "In order to calculate the relationship value between a pair of terms (t1, t2), we first save their concept vectors (Ct1, Ct2) to have the same length. Then, we apply the traditional measure of cosmic similarity to their respective weight vectors (Wt1, Wt2), as in Equation 5: Relcos (t1, t2) = Wt1. Wt2 | | | Wt2 | | (5) Similar to [Hassan and Mihalcea, 2011], we use a normalization factor \u03bb, since the cosine measure, due to its concept vector economy, provides low values for highly related terms. Other approaches for dealing with vector parity that are worth exploring in the future [Songand Roth, 2015]. Based on this, the final relationship value is adjusted as in Equation 6: Rel (t1, t2) = {1 Relcos (t1, t2) (Relt2)."}, {"heading": "4. EXPERIMENTS AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data Sets", "text": "Each data set is a collection of word pairs along with humanly judged similarity / kinship values for each pair.RG: a similarity data set created by [Rubenstein and Goodenough, 1965] containing 65 word pairs. Similarity judgments for each pair were performed by 51 subjects. Assessments range from 0 (very independent) to 4 (very related). [Pilehvar and Navigli, 2015] reported the highest performance on this data set by using a semantic network of Wiktionary.MC: a similarity data set created by [Miller and Charles, 1991] containing 30 word pairs 4 from the RG data set. Similarity judgments were performed by 38 subjects on the same scale as RG. [CamachoCollados et al al al, 2015] reports the highest performance on MC by integrating knowledge from Wikipedia and Wotstein."}, {"heading": "4.2 Experimental Setup", "text": "We followed an experimental setup similar to [Baroni et al., 2014]. In essence, we have implemented two types of experiments: First, we perform a grid search across the parameter space of the MSA to get the maximum powerful combination of parameters for each dataset. Second, we evaluate the MSA in a more realistic environment in which we use one of the datasets as a development set to tune the parameters of the MSA, and then use tuned parameters to evaluate the performance of the MSA on 3http: / / www.cs.cmu.edu / research / wordsim / EN-RG-65.txt 4http: / / www.cs.cmu.edu.edu / word / ~ mfaruqui / EN-MC-30.txt 5http: / / www.cs.cmu.edu / s / http: / / www.cs.cs.cs.cs.cmu.htm /.htm http: /.htm.htm http: /.chtm http.htm http: /.htm www.cs.muxt /.cs.muxt /.cs.cs.uu 30.tu / MC / 6x.htmtxt / 6x.htm.htm.htm http:.htm http: /.htm http: /.htm http: /.htm http: /.ttttttttttr /.htm http: /.htm http: /.c.htm http: /.c.htm.tr /.tr /.tr /.tr /.tr /.tr /.tr /.tr /.tr /.tr /.tr /.tr /.tr /.tr.tr /.tr.tr /.tr.tr /.tr /.tr.tr /.tr.tr /.tr.tr /.tr.tr /.tr /.tr /.tr /.tr /.tr.tr /.tr.tr /.tr /.tr /.tr.tr /.tr.tr"}, {"heading": "4.3 Evaluation", "text": "As in previous studies, we report on both Pearson correlation (r) [Hill and Lewicki, 2007] and Spearman ranking correlation (\u03c1) [Zwillinger and Kokoska, 1999]. We compare our results with those derived from three types of semantic representation models: First, with models of statistical incidence such as LSA [Landauer et al., 1997], CW and BOW [Agirre et al., 2009] and ADW [Pilehvar and Navigli, 2015]; second, with models of neural networks such as Collobert and Weston (CW) vectors [Collobert and Weston, 2008], Word2Vec [Baroni et al., 2014] and GloVe [Pennington et al., 2014]; third, with explicit sectional models such as ESA [Gabrilovich and Markodos], SARC [Baroni et al., 2014] and SARceo [ASC and Mihalal, 2014]."}, {"heading": "4.4 Results", "text": "We report on correlation values of MSA compared to other models in Tables 2, 3 and 4. Some models do not report their correlation values on all datasets, so we leave them blank. MSA (last line) represents values obtained by using WS as the development set for tuning MSA parameters and evaluating performance on the other datasets using the matched parameters. Parameter values obtained by tuning to WS were L = 5k, M = 800, \u03c4 = 2 for Cs, Cp and finally = 1.Table 2 shows Pearson correlation (r) of MSA on five benchmark datasets. Previous work results on the same datasets are also reported. For10https / walid-shalaby / wikimedia.org / enwiki / 20150304 / 11http: / medialab.di.unipi.it / wiki / Wikipedia _ Extractor 12ps: walid-shalabiek.wicttraxor.we"}, {"heading": "5. A STUDY ON STATISTICAL SIGNIFICANCE", "text": "First, the differences between the reported correlation values were very small. Second, the size of the data sets was not large enough to accommodate for such small differences. These two facts raise a question about the statistical significance of the improvement that can be hypothesized by using Method A compared to another, well-performing Method B.We hypothesize that the best method is not necessarily the one that provides the highest correlation value. In other words, it is not necessary to give the highest correlation, but to give a relatively high score that makes any other higher score statistically insignificant. To test our hypothesis, we decided to perform statistical significance tests on the highest reported correlations. First, we targeted Word2Vec, GloVe, ADW, and NASARI alongside MSA. We contacted several authors and some of them gratefully provided."}, {"heading": "6. CONCLUSION", "text": "In this case, it is only a pure formation, which is able to move without being able to hide itself. In this case, it is a pure formation, which is able to move without being able to hide."}, {"heading": "7. REFERENCES", "text": "[Agirre et al., 2009] Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., Pas-ca, M., and Soroa, A. (2009). A study onsimilarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 19-27. Association for Computational Linguistics. [Agrawal et al., 1993] Agrawal, R., Imielin \u0445 ski, T., and Swami, A. (1993). Mining association between rules of items in large databases. In ACM SIGMOD Record, volume 22, pages 207-216. ACM. [Banjade et al., 2015] Banjade et al., R., Maharjan, N., N. B., Rus, V., and Gautational, D. Lemsimiity."}], "references": [{"title": "Mining association rules between sets of items in large databases", "author": ["Agrawal et al", "R. 1993] Agrawal", "T. Imieli\u0144ski", "A. Swami"], "venue": "In ACM SIGMOD Record,", "citeRegEx": "al. et al\\.,? \\Q1993\\E", "shortCiteRegEx": "al. et al\\.", "year": 1993}, {"title": "Lemon and tea are not similar: Measuring word-to-word similarity by combining different methods", "author": ["Banjade et al", "R. 2015] Banjade", "N. Maharjan", "N.B. Niraula", "V. Rus", "D. Gautam"], "venue": "In Computational Linguistics and Intelligent Text Processing,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al", "M. 2014] Baroni", "G. Dinu", "G. Kruszewski"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al", "E. 2014] Bruni", "Tran", "N.-K", "M. Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Evaluating wordnet-based measures of lexical semantic relatedness", "author": ["Budanitsky", "Hirst", "A. 2006] Budanitsky", "G. Hirst"], "venue": "Computational Linguistics,", "citeRegEx": "Budanitsky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Budanitsky et al\\.", "year": 2006}, {"title": "Nasari: a novel approach to a semantically-aware representation of items", "author": ["Camacho-Collados et al", "J. 2015] Camacho-Collados", "M.T. Pilehvar", "R. Navigli"], "venue": "In Proceedings of NAACL,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston", "R. 2008] Collobert", "J. Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Indexing by latent semantic analysis", "author": ["Deerwester et al", "S.C. 1990] Deerwester", "S.T. Dumais", "T.K. Landauer", "G.W. Furnas", "R.A. Harshman"], "venue": "JAsIs,", "citeRegEx": "al. et al\\.,? \\Q1990\\E", "shortCiteRegEx": "al. et al\\.", "year": 1990}, {"title": "Concept-based information retrieval using explicit semantic analysis", "author": ["Egozi et al", "O. 2011] Egozi", "S. Markovitch", "E. Gabrilovich"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Placing search in context: The concept revisited", "author": ["Finkelstein et al", "L. 2001] Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "al. et al\\.,? \\Q2001\\E", "shortCiteRegEx": "al. et al\\.", "year": 2001}, {"title": "Computing semantic relatedness using wikipedia-based explicit semantic analysis", "author": ["Gabrilovich", "Markovitch", "E. 2007] Gabrilovich", "S. Markovitch"], "venue": "In IJCAI,", "citeRegEx": "Gabrilovich et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gabrilovich et al\\.", "year": 2007}, {"title": "Large-scale learning of word relatedness with constraints", "author": ["Halawi et al", "G. 2012] Halawi", "G. Dror", "E. Gabrilovich", "Y. Koren"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Semantic relatedness using salient semantic analysis", "author": ["Hassan", "Mihalcea", "S. 2011] Hassan", "R. Mihalcea"], "venue": null, "citeRegEx": "Hassan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hassan et al\\.", "year": 2011}, {"title": "Statistics: Methods and Applications", "author": ["Hill", "Lewicki", "T. 2007] Hill", "P. Lewicki"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2007}, {"title": "Roget\u00e2\u0102\u0179s thesaurus and semantic similarity1", "author": ["Jarmasz", "Szpakowicz", "M. 2004] Jarmasz", "S. Szpakowicz"], "venue": "Recent Advances in Natural Language Processing III: Selected Papers from RANLP,", "citeRegEx": "Jarmasz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jarmasz et al\\.", "year": 2004}, {"title": "How well can passage meaning be derived without using word order? a comparison of latent semantic analysis and humans", "author": ["Landauer et al", "T.K. 1997] Landauer", "D. Laham", "B. Rehder", "M.E. Schreiner"], "venue": "In Proceedings of the 19th annual meeting of the Cognitive Science Society,", "citeRegEx": "al. et al\\.,? \\Q1997\\E", "shortCiteRegEx": "al. et al\\.", "year": 1997}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Mikolov et al", "T. 2013] Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["Miller", "Charles", "G.A. 1991] Miller", "W.G. Charles"], "venue": "Language and cognitive processes,", "citeRegEx": "Miller et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1991}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington et al", "J. 2014] Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "From senses to texts: An all-in-one graph-based approach for measuring semantic similarity", "author": ["Pilehvar", "Navigli", "M.T. 2015] Pilehvar", "R. Navigli"], "venue": "Artificial Intelligence,", "citeRegEx": "Pilehvar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pilehvar et al\\.", "year": 2015}, {"title": "A word at a time: computing word relatedness using temporal semantic analysis", "author": ["Radinsky et al", "K. 2011] Radinsky", "E. Agichtein", "E. Gabrilovich", "S. Markovitch"], "venue": "In Proceedings of the 20th international conference on World wide web,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "Goodenough", "H. 1965] Rubenstein", "J.B. Goodenough"], "venue": "Commun. ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "On dataless hierarchical text classification", "author": ["Song", "Roth", "Y. 2014] Song", "D. Roth"], "venue": "In AAAI,", "citeRegEx": "Song et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Song et al\\.", "year": 2014}, {"title": "Unsupervised sparse vector densification for short text similarity", "author": ["Song", "Roth", "Y. 2015] Song", "D. Roth"], "venue": "In Proceedings of NAACL", "citeRegEx": "Song et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Song et al\\.", "year": 2015}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney et al", "P.D. 2010] Turney", "P Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Using wiktionary for computing semantic relatedness", "author": ["Zesch et al", "T. 2008] Zesch", "C. M\u00fcller", "I. Gurevych"], "venue": "In AAAI,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "CRC standard probability and statistics tables and formulae", "author": ["Zwillinger", "Kokoska", "D. 1999] Zwillinger", "S. Kokoska"], "venue": null, "citeRegEx": "Zwillinger et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Zwillinger et al\\.", "year": 1999}], "referenceMentions": [], "year": 2016, "abstractText": "Mined Semantic Analysis (MSA) is a novel concept space model which employs unsupervised learning to generate semantic representations of text. MSA represents textual structures (terms, phrases, documents) as a bag-of-concepts where concepts are derived from concept rich encyclopedic corpora. Traditional concept space models exploit only target corpus content to construct the concept space. MSA, alternatively, uncovers implicit relations between concepts by mining for their associations (e.g., mining Wikipedia\u2019s \"See also\" link graph). We evaluate MSA\u2019s performance on benchmark data sets for measuring lexical semantic relatedness. Empirical results show competitive performance of MSA compared to prior stateof-the-art methods. Additionally, we introduce the first analytical study to examine statistical significance of results reported by different semantic relatedness methods. Our study shows that, the nuances of results across top performing methods could be statistically insignificant. The study positions MSA as one of state-of-theart methods for measuring semantic relatedness.", "creator": "LaTeX with hyperref package"}}}