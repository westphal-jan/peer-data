{"id": "1512.03549", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2015", "title": "Words are not Equal: Graded Weighting Model for building Composite Document Vectors", "abstract": "Despite the success of distributional semantics, composing phrases from word vectors remains an important challenge. Several methods have been tried for benchmark tasks such as sentiment classification, including word vector averaging, matrix-vector approaches based on parsing, and on-the-fly learning of paragraph vectors. Most models usually omit stop words from the composition. Instead of such an yes-no decision, we consider several graded schemes where words are weighted according to their discriminatory relevance with respect to its use in the document (e.g., idf). Some of these methods (particularly tf-idf) are seen to result in a significant improvement in performance over prior state of the art. Further, combining such approaches into an ensemble based on alternate classifiers such as the RNN model, results in an 1.6% performance improvement on the standard IMDB movie review dataset, and a 7.01% improvement on Amazon product reviews. Since these are language free models and can be obtained in an unsupervised manner, they are of interest also for under-resourced languages such as Hindi as well and many more languages. We demonstrate the language free aspects by showing a gain of 12% for two review datasets over earlier results, and also release a new larger dataset for future testing (Singh,2015).", "histories": [["v1", "Fri, 11 Dec 2015 08:44:45 GMT  (57kb)", "http://arxiv.org/abs/1512.03549v1", "10 Pages, 2 Figures, 11 Tables"]], "COMMENTS": "10 Pages, 2 Figures, 11 Tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["pranjal singh", "amitabha mukerjee"], "accepted": false, "id": "1512.03549"}, "pdf": {"name": "1512.03549.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["pranjals16@gmail.com", "amit@cse.iitk.ac.in"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.03 549v 1 [cs.C L] 11 Dec 2Despite the success of distributional semantics, writing phrases from word vectors remains an important challenge. Several methods have been tried for benchmark tasks such as mood classification, including word vector evaluation, matrix vector approaches based on parsing, and the immediate learning of paragraph vectors. Some of these methods (especially tf-idf) generally result in a significant improvement in performance over previous state of the art. In addition, we consider several tiered schemes in which words are weighted according to their discriminatory relevance in terms of their use in the document (e.g. idf). Some of these methods (especially tf-idf) result in a significant improvement in performance over previous state of the art. Furthermore, combining such approaches results in an overall ensemble based on alternative classifiers such as the RNN model."}, {"heading": "1 Introduction", "text": "Language is a very crucial aspect in performing various NLP tasks and has been looked at in great detail lately. Language representation models are divided into roughly two categories: those that require hand-trained language databases such as Treebanks (e.g., (Socher et al., 2013)) and those that are language agnostic and work on raw corpora (e.g., LDA, BOW, SkipGram, NLM, etc.) Liu (2015) compare different language agnostic models on the subject of modeling. Language-independent models such as LDA and BOW have been quite effective for a long time. Variants of BOW such as tf-idf had changed the perception of researchers regarding these models when they were effective in various NLP tasks. LDA was able to model models between and intra documentary statistical and relative structures, but the semantic and syntactical dependencies of BOW were still ignored."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Neural Language Model", "text": "Neural networks tend to overcome the disadvantages of n-gram models because they can model continuous variables or distributed representation, which is a necessity if we want to find better generalizations about the most discrete word sequences (Bengio et al., 2003). Neural language models were introduced by Bengio et al., 2001 (revised in 2003), in the form of computer models. They build a mapping C from each word i of vocabulary V into a feature vector C (i) and Rm is the number of characters; a probability function g over words expressed with C; and finally, the word vector and parameters learn the probability function."}, {"heading": "2.2 Sentiment Analysis", "text": "The majority of existing work in this area is in English (Pang and Lee, 2008). Medagoda (2013) surveys sentiment analysis in non-English languages, while (Sharma et al., 2014) give a summary of work in Hindi in the field of opinion formation. However, heuristic based and machine learning models have been used in this area. Heuristic methods, in general, classify the text entities based on the total number of derived positive or negative mood-oriented fea-tures. But these models rely heavily on human constructed traits, which is a domain and language-dependent task. Several groups have tried to improve the situation by modelling the composition of words in larger contexts (Le and Micro, 2014; Johnson and Zhang, 2014)."}, {"heading": "3 Method", "text": "The algorithms and data structures used in this thesis were presented and discussed in the following."}, {"heading": "3.1 Distributed Representation", "text": "In the eeisn eaeasrrrrrVrree\u00fcgn rf\u00fc ide eeisrmtlrteeaeVnlrlhsrtee\u00fccnh hacu ide eeisrmtlrteeaeVnlrsrteeu ni rde eeisrmnlrteeVnlrteeeeVnlrrrmtlrteeVnrree\u00fcgn in the eeisn-eaeaJnlhsrdcnlSrteeaeaeaeaetnln-eaeaeaeaeHnlrrrrrrsrteeaeaeaeaeaeaeSriiiiceceSrrreSreSrrec-SrreeeSrreec-eaeSrluic-Srluic"}, {"heading": "3.2 Semantic Composition", "text": "The principle of compositivity is that the meaning of a complex expression is determined by the meaning of its parts or components and the rules that guide this combination. It is also known as Frege's principle. In our case, the components are word vectors and the expression in the hand is the sentence / document vector. For example, the movie is funny and the script is well analyzing the results from Table 2, we observed that when we are dealing with a large number of characteristics, there are a large number of zeros and the presence of a single zero in a characteristic that these characteristics contribute zero in the final vector, which happens in our case and therefore multiplicative composition fails. Therefore, we apply both simple and idf-weighted averages in our work. The advantage with the addition is that it does not increase the dimension of the vector and captures semantics at a high level with ease. In fact (Zou et al., 2013) we have used simple averages to find phrases to construct them later on to have the similarity between the words."}, {"heading": "3.2.1 Graded Weighting", "text": "We describe two approaches to integrate graduated weighting into word vectors to form document vectors. Let vwi be the vector representation of the ith word. Then, we propose a different scheme that weighs the contribution of each word while building document vectors with a graduated approach. We define idf (t, d) = log (| D | df (t), where t is the term, d is the document and other notations are the same as in the previous subsection. The new document vector representation taking into account this graduated scheme is: vdi = {0 idf (wk, di) = log (| D | df (t), where t is the term, d is the document and other notations are the same as in the previous subsection. The new document vector representation taking into account this graduated scheme is: vdi = {0 idf (wk, di) = (log | df) (D)."}, {"heading": "3.3 Composite Representation", "text": "This experiment redefined the representation of documents in NLP, which is used for the classification of emotions. It has the property of incorporating both syntactic and semantic properties of a text. The limitations of word vectors without grams were met by document vectors and therefore we achieve state-of-the-art results in IMDB movie rating records as well as Amazon's electronic verification datasets. We first created n-dimensional word vectors by training the skip gram model on the datasets. Then we assigned weights to the word vectors for each document to generate document vectors.This now functions as a feature set for this particular document. We then created tf-idf vectors for each document. This can be considered as a vector representation of this particular document. Subsequently, we linked these document vectors with document vectors, which, after training the desired datasets, were created separately with the one proposed in Le and Micolov 2014."}, {"heading": "3.4 Dimensionality Reduction", "text": "The reduction of dimensionality is the process of reducing the number of random variables in such a way that the remaining variables effectively reproduce most of the variability of the dataset. The reason for using such techniques is the curse of dimensionality, a phenomenon that occurs in high dimensions but does not occur in low dimensions. Table 3 summarizes how the selection of characteristics has improved the classification accuracy in the 700 movie review dataset. With ANOVA-F we have selected about 4k characteristics, but with PCA it was only 50. The low accuracy with PCA can therefore be attributed to the fact that we have lost some important characteristics in low dimensions. PCA also cannot work with the size of the dimension d > size of the learning dataset. This strong decrease in accuracy happens in both cases because ANOVA-F selects characteristics with greater variance between groups and thus reduces the noise to a greater extent, while PCA decreases the data angle, which is more effective in this case due to higher distributions."}, {"heading": "4 Experiment", "text": "In this section we describe the experiments and analyze the results."}, {"heading": "4.1 Datasets", "text": "We have used 3 data sets for experiments in Hindi and 5 for English. All data sets, including our own Hindi data set, are described below. We have experimented with two Hindi data sets. One is the Product Review Data Set (LTG, IIIT Hyderabad), which contains 350 positive reviews and 350 negative reviews; the other is a Movie Review Data Set (CFILT, IIT Bombay), which contains 127 positive reviews and 125 negative reviews. Each review is about 1-2 sentences long and the sentences focus mainly on the mood, either positive or negative. Our 700 Movie Review Corpus in Hindi contains film reviews from websites such as Dainik Jagran and Navbharat Times. The film reviews are longer than the previous corpus and contain topics other than sentiment. In total, there are 697 negative film reviews from both websites. The statistics produced are described below."}, {"heading": "4.2 SkipGram or CBOW", "text": "We present an interesting experiment to show that Skipgram actually performs better than CBOW. The SkipGram model tends to predict a context, while the CBOW model predicts a word that has a context. It appears intuitive and also from observation (Mikolov et al., 2013a) that SkipGram performs better on semantic tasks and CBOW on syntactic tasks. We now try to assess how they differ in the classification accuracy of the two datasets: Watches and MP3. Figure 2 shows that Skipgram performs better than CBOW in the task of sentiment classification."}, {"heading": "4.3 Results", "text": "Table 5 summarizes the results obtained by others and by us based on the IMDB Movie Review dataset. We have surpassed previous bests (Le and Mikolov, 2014) by a 1.33% margin using discrimination weighting. The main contribution to improving the results is our new document vector, which overcomes the weaknesses of BOW and separately captured document vectors. Table 6 shows the effectiveness of our proposed graded weighting methodology. Without tfidf features, our proposed method performs better in the case of graded idf weighting, and when we include tf-idf features, the 0-1 weighting performs better than idf-graded technique, and both perform better than the previous grading technique. We see that with higher weights, there is a decrease in accuracy, and that is because we are now filtering more words that are important as we build document vectors. Table 7 is a further improvement of the RM, and we anticipate the results as soon as we get the RM."}, {"heading": "5 Conclusion", "text": "In this paper, we present an early experiment on the possibilities of distributional semantics (word vectors) for resource-poor, highly inflected languages such as Hindi. What is interesting is that our word vector requires a mean method along with tf-idf results in improvements in accuracy compared to existing methods in Hindi (from 80.2% to 90.3% on IITB Movie Review Dataset). Also from Table 1, we can see that the paragraph vector of (Le and Mikolov, 2014) does not work well due to the fact that the Hindi dataset only contains single sentences highlighting the weakness of this model. Also, the size of the corpus is small to learn paragraph vectors. Thus, our model overcomes these weaknesses with better document representation."}, {"heading": "6 Future Work", "text": "Distributional semantic approaches remain relatively unexplored for Indian languages, and our results suggest that there could be significant advantages in exploring these approaches for Indian languages. Although this work has focused on the classification of feelings, it could also improve a number of tasks, from verbal analogy tests to ontological learning, as has been reported for other languages. In future work, we may examine various compositional models - a) weighted averages - where weights are determined based on cosmic distances in vector space; b) weighted multiplicative models. Identifying morphological variants would be another direction to explore for greater accuracy. In terms of emotion analysis, the idea of aspect-based models (or part-based sentiment analysis), which consider the components in a document and classify their feeling polarity separately, has led our composite document in English to open up a new area for our work that may include many ensembles."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Despite the success of distributional semantics, composing phrases from word vectors remains an important challenge. Several methods have been tried for benchmark tasks such as sentiment classification, including word vector averaging, matrix-vector approaches based on parsing, and on-the-fly learning of paragraph vectors. Most models usually omit stop words from the composition. Instead of such an yes-no decision, we consider several graded schemes where words are weighted according to their discriminatory relevance with respect to its use in the document (e.g., idf). Some of these methods (particularly tf-idf) are seen to result in a significant improvement in performance over prior state of the art. Further, combining such approaches into an ensemble based on alternate classifiers such as the RNN model, results in an 1.6% performance improvement on the standard IMDB movie review dataset, and a 7.01% improvement on Amazon product reviews. Since these are language free models and can be obtained in an unsupervised manner, they are of interest also for underresourced languages such as Hindi as well and many more languages. We demonstrate the language free aspects by showing a gain of 12% for two review datasets over earlier results, and also release a new larger dataset for future testing (Singh, 2015).", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}