{"id": "1512.03950", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Dec-2015", "title": "A Hidden Markov Model Based System for Entity Extraction from Social Media English Text at FIRE 2015", "abstract": "This paper presents the experiments carried out by us at Jadavpur University as part of the participation in FIRE 2015 task: Entity Extraction from Social Media Text - Indian Languages (ESM-IL). The tool that we have developed for the task is based on Trigram Hidden Markov Model that utilizes information like gazetteer list, POS tag and some other word level features to enhance the observation probabilities of the known tokens as well as unknown tokens. We submitted runs for English only. A statistical HMM (Hidden Markov Models) based model has been used to implement our system. The system has been trained and tested on the datasets released for FIRE 2015 task: Entity Extraction from Social Media Text - Indian Languages (ESM-IL). Our system is the best performer for English language and it obtains precision, recall and F-measures of 61.96, 39.46 and 48.21 respectively.", "histories": [["v1", "Sat, 12 Dec 2015 18:57:11 GMT  (315kb)", "http://arxiv.org/abs/1512.03950v1", "FIRE 2015 Task:Entity Extraction from Social Media Text - Indian Languages (ESM-IL) - See more at:this http URLarXiv admin note: substantial text overlap witharXiv:1405.7397"]], "COMMENTS": "FIRE 2015 Task:Entity Extraction from Social Media Text - Indian Languages (ESM-IL) - See more at:this http URLarXiv admin note: substantial text overlap witharXiv:1405.7397", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kamal sarkar"], "accepted": false, "id": "1512.03950"}, "pdf": {"name": "1512.03950.pdf", "metadata": {"source": "CRF", "title": "A Hidden Markov Model Based System for Entity Extraction from Social Media English Text at FIRE 2015", "authors": ["Kamal Sarkar"], "emails": ["jukamal2001@yahoo.com"], "sections": [{"heading": null, "text": "University participating in FIRE 2015: Entity Extraction from Social Media Text - Indian Languages (ESM-IL) task. The tool we developed for this task is based on the Trigram Hidden Markov Model, which uses information such as Gazetteer list, POS tag and some other vocabulary features to improve the probabilities of observation of known tokens as well as unknown tokens. We submitted runs only for English. In implementing our system, a statistical HMM model (Hidden Markov Models) was used, and the system was trained and tested on the data sets published for FIRE 2015 task: Entity Extraction from Social Media Text - Indian Languages (ESM-IL). Our system is the best for the English language and it achieves precision, recall and F measures of 61.96, 39.46 and 48.21 Respect categories and subject description.H.3 [Search retrieval and search retrieval]: H.3 and H.3;"}, {"heading": "1. INTRODUCTION", "text": "The aim of the designated entity recognition is to identify and classify each word / term in a document into some predefined categories such as person name, location name, organization name, various names (date, time, percentage and monetary terms, etc.)."}, {"heading": "2. TRAINING DATA PREPARATION", "text": "The training data published for the joint task of FIRE contain two files: One file contains the raw text file and another file contains the NE annotation file, in which each line has 6 columns: tweet-id, user-id, NE-tag, NE-raw string, NE-start index and NE _ length. The index column is the initial position of NE, which is calculated for each tweet. Participants are instructed to generate the output in the same format after testing the system based on the test data. Our system uses the two files supplied for the training data and converts the data into the IOB format before training, and the data is converted to the IOB format (Inside, Outside and Beginning) (a format used for the joint task of CoNLL-2003 on NER). IOB format uses a B \u2212 XXX tag, which specifies the first word of a unit of type XXX and I \u2212 XXX, which is used for the subsequent words of a unit."}, {"heading": "3. HMM BASED NAMED ENTITY TAGGING", "text": "A named company based on Hidden Markov Model (HMM) finds the best sequence of NE tags 1 > that is optimized for a particular observation sequence. () So the tagging issue is not only a question of probability, but also a question of probability. () It is a question of probability. () It is a question of probability. () It is a question of probability. () It is a question of probability. () It is a question of probability. () It is a question of probability. () It is a question of probability. () It is a question of probability. () It is a question of probability. () It is a question of probability. () It is a question of probability. () It is a question of probability."}, {"heading": "3.1 Computing Tag Transition Probabilities", "text": "As we can see from Equation (4), in order to find the most likely tag sequence for an observation sequence, we have to calculate two types of probabilities: transit probabilities and word probabilities or observation probabilities. Our developed trigram HMM tagger requires the calculation of the diurnal trigram probability, 1 2 (|,) i i iP t t t, which is calculated by the maximum probability estimate from the number of diurnal trigrams. To overcome the problem of data economy, the diurnal trigram probability is smoothed using the technique of deleted interpolation [13] [15], which uses the maximum probability estimate from the number of diurnal trigrams, ditagbigrams and diurnal unigrams."}, {"heading": "3.2 Computing Observation Probabilities", "text": "The probability of observing an observed triplet < word, X-day, meta-day >, which in our case is the observed symbol, is calculated with the following equation [12] [13]. (,) (|) C o tC o P o t (7)"}, {"heading": "3.3 Viterbi Decoding", "text": "The task of a decoder is to find the best hidden state sequence in relation to an input HMM and a sequence of observations.The Viterbi algorithm is the most common decoding algorithm for HMM-based tagging task. This is a standard application of the classic dynamic programming algorithm [16].In relation to a tag transition probability matrix and the observation probability matrix, Viterbi (used in the test phase) accepts a tweet in the Indian language and finds the most likely tag sequence for the test tweet, which is also X-tagged and meta-tagged. Here, a tweet of the Viterbi is presented as an observation sequence of triplexes."}, {"heading": "4. SPECIAL TAGS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Meta Tag", "text": "To gather this information, we use meta tags that we use for our task, described below. Meta tag for a token is determined by the following rules that we will assign to the token: \"Meta tag\" = \"YYYYYYY\" (default), if the first letter of the token is a capital letter, then it is described below. Meta tag for a token is determined by the following rules: \"Meta tag =\" YYYYYYYYYYYY \"(default), if the first letter of the token is a capital letter, then metatag =\" ICAP \"end ifif is the first token, then metatag =\" ABBR \"End Ifif contains.\""}, {"heading": "4.2 Gazetteer tag", "text": "In previous sections, we have mentioned that the POS identification for a token is replaced by a label identification that refers to a specific label identification list.,,,, \"We\" -Identification \"-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identification-Identity-Identity-Identification-Identification-Identification-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-Identity-"}, {"heading": "6. CONCLUSION", "text": "This paper describes a named entity recognition system for Entity Extraction from Social Media Text in English. Features such as Gazetteer list, POS tag and some other vocabulary features have been introduced into the HMM model. Experimental results show that our system performs best among the systems participating in the ESM-IL task for the English language. The named entity recognition system has been developed using the Visual Basic platform so that a suitable user interface can be designed for beginners. The system has been designed so that only changing the training corpus in a file can make the system portable to a new Indian language."}], "references": [{"title": "The New York University System MUC- 6 or Where\u2019s the syntax", "author": ["R. Grishman"], "venue": "In Proceedings of the Sixth Message Understanding Conference", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Internal and external evidence in the identification and semantic categorization of proper names", "author": ["D.D. McDonald"], "venue": "In B. Boguraev and J. Pustejovsky, editors, Corpus Processing for Lexical Acquisition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Evaluationof an algorithm for the recognition and classification of proper names", "author": ["T. Wakao", "R. Gaizauskas", "Y. Wilks"], "venue": "In Proceedings of COLING-96", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Nymble: A High Performance Learning Name-finder", "author": ["D.M. Bikel", "S. Miller", "R. Schwartz", "R. Weischedel"], "venue": "In Proceedings of the Fifth Conference on Applied Natural Language Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "A Maximum Entropy Approach to Named Entity Recognition", "author": ["A. Borthwick"], "venue": "Ph.D. thesis,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Named Entity Recognition in Hindi using MEMM", "author": ["N. Kumar", "P. Bhattacharyya"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Rapid Development of Hindi Named Entity Recognition using Conditional Random Fields and Feature Induction (Short Paper)", "author": ["Li", "Wei", "A. McCallum"], "venue": "In ACM Transactions on Computational Logic", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "A Hybrid Approach for Named Entity and Sub-Type Tagging", "author": ["R. Srihari", "C. Niu", "Li", "Wei"], "venue": "In Proceedings of the sixth conference on Applied natural language processing", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Language Independent Named Entity Recognition Combining Morphological and Contextual Evidence", "author": ["S. Cucerzan", "D. Yarowsky"], "venue": "In Proceedings of the Joint SIGDAT Conference on EMNLP and VLC,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Rapid Development of Hindi Named Entity Recognition using Conditional Random Fields and Feature Induction (Short Paper)", "author": ["Li", "Wei", "A. McCallum"], "venue": "Vira - Charotar", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "A conditional random field approach for named entity recognition in Bengali and Hindi", "author": ["Ekbal A", "S. Bandyopadhyay"], "venue": "Linguistic Issues in Language Technology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "A practical part-of-speech tagger for Bengali", "author": ["Sarkar K", "V. Gayen"], "venue": "In Proceedings of the third International conference on Emerging Applications of Information Technology (EAIT),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "An HMM based named entity recognition system for indian languages: the JU system at ICON 2013.", "author": ["V. Gayen", "K. Sarkar"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "MontyLingua: an end to end natural language processor with common sense", "author": ["H. Liu"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "TnT \u2013 A statistical part-of-speech tagger", "author": ["T. Brants"], "venue": "In proceedings of the 6 Applied NLP Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Speech and Language Processing: An Intoduction to Natural Language Processing, Computational Linguistics and Speech Recognition, Preason Education Series", "author": ["D. Jurafsky", "J.H. Martin"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "The rule based approaches typically use a set of hand crafted rules [1][2][3].", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "The rule based approaches typically use a set of hand crafted rules [1][2][3].", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "The rule based approaches typically use a set of hand crafted rules [1][2][3].", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "Several ML techniques have already been applied for the NER tasks such as Markov Model (HMM) [4], Maximum Entropy (MaxEnt) [5][6], Conditional Random Field (CRF)[7] etc.", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "Several ML techniques have already been applied for the NER tasks such as Markov Model (HMM) [4], Maximum Entropy (MaxEnt) [5][6], Conditional Random Field (CRF)[7] etc.", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "Several ML techniques have already been applied for the NER tasks such as Markov Model (HMM) [4], Maximum Entropy (MaxEnt) [5][6], Conditional Random Field (CRF)[7] etc.", "startOffset": 126, "endOffset": 129}, {"referenceID": 6, "context": "Several ML techniques have already been applied for the NER tasks such as Markov Model (HMM) [4], Maximum Entropy (MaxEnt) [5][6], Conditional Random Field (CRF)[7] etc.", "startOffset": 161, "endOffset": 164}, {"referenceID": 7, "context": "(2000) [8] combines MaxEnt, Hidden Markov Model (HMM) and handcrafted rules to build an NER system.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "Both the linguistic approach [1][3] and the ML based approach[5][8] may use gazetteer lists.", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "Both the linguistic approach [1][3] and the ML based approach[5][8] may use gazetteer lists.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "Both the linguistic approach [1][3] and the ML based approach[5][8] may use gazetteer lists.", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "Both the linguistic approach [1][3] and the ML based approach[5][8] may use gazetteer lists.", "startOffset": 64, "endOffset": 67}, {"referenceID": 8, "context": "The NER tasks for Hindi have been presented in [9][10][11].", "startOffset": 47, "endOffset": 50}, {"referenceID": 9, "context": "The NER tasks for Hindi have been presented in [9][10][11].", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "The NER tasks for Hindi have been presented in [9][10][11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "In general, HMM based sequence labeling tasks such as POS tagging use words in a sentence as an observation sequence [12] 13].", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "But, we use MontyTagger [14] to assign POS tags to the data released for the task, that is, some additional information such as POS for each token in a tweet becomes now available.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "Considering a special tag tn+1 to indicate the end sentence boundary and two special tags t-1 and t0 at the starting boundary of the sentence and adding these three special tags to the tag set [15], gives the following equation for NE tagging:", "startOffset": 193, "endOffset": 197}, {"referenceID": 12, "context": "To overcome the data sparseness problem, tag trigram probability is smoothed using deleted interpolation technique [13][15] which uses the maximum likelihood estimates from counts for tag trigram, tag bigram and tag unigram.", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": "To overcome the data sparseness problem, tag trigram probability is smoothed using deleted interpolation technique [13][15] which uses the maximum likelihood estimates from counts for tag trigram, tag bigram and tag unigram.", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": "The observation probability of a observed triplet <word, X-tag, meta-tag >, which is the observed symbol in our case, is computed using the following equation [12][13].", "startOffset": 159, "endOffset": 163}, {"referenceID": 12, "context": "The observation probability of a observed triplet <word, X-tag, meta-tag >, which is the observed symbol in our case, is computed using the following equation [12][13].", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "This is a standard application of the classic dynamic programming algorithm[16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "We have used the Viterbi algorithm presented in [16] for finding the most likely tag sequence for a given observation sequence.", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "We find the observation probabilities of such unknown pseudo words using suffix analysis of all rare pseudo words (frequency <=2) in the training corpus for the concerned language [13][15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "We find the observation probabilities of such unknown pseudo words using suffix analysis of all rare pseudo words (frequency <=2) in the training corpus for the concerned language [13][15].", "startOffset": 184, "endOffset": 188}], "year": 2015, "abstractText": "This paper presents the experiments carried out by us at Jadavpur University as part of the participation in FIRE 2015 task: Entity Extraction from Social Media Text Indian Languages (ESM-IL). The tool that we have developed for the task is based on Trigram Hidden Markov Model that utilizes information like gazetteer list, POS tag and some other word level features to enhance the observation probabilities of the known tokens as well as unknown tokens. We submitted runs for English only. A statistical HMM (Hidden Markov Models) based model has been used to implement our system. The system has been trained and tested on the datasets released for FIRE 2015 task: Entity Extraction from Social Media Text Indian Languages (ESM-IL). Our system is the best performer for English language and it obtains precision, recall and F-measures of 61.96, 39.46 and 48.21 respectively.", "creator": "Microsoft\u00ae Office Word 2007"}}}