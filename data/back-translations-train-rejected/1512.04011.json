{"id": "1512.04011", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2015", "title": "L1-Regularized Distributed Optimization: A Communication-Efficient Primal-Dual Framework", "abstract": "Despite the importance of sparsity in many big data applications, there are few existing methods for efficient distributed optimization of sparsely-regularized objectives. In this paper, we present a communication-efficient framework for L1-regularized optimization in distributed environments. By taking a non-traditional view of classical objectives as part of a more general primal-dual setting, we obtain a new class of methods that can be efficiently distributed and is applicable to common L1-regularized regression and classification objectives, such as Lasso, sparse logistic regression, and elastic net regression. We provide convergence guarantees for this framework and demonstrate strong empirical performance as compared to other state-of-the-art methods on several real-world distributed datasets.", "histories": [["v1", "Sun, 13 Dec 2015 06:49:00 GMT  (278kb,D)", "https://arxiv.org/abs/1512.04011v1", null], ["v2", "Thu, 2 Jun 2016 23:45:03 GMT  (422kb,D)", "http://arxiv.org/abs/1512.04011v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["virginia smith", "simone forte", "michael i jordan", "martin jaggi"], "accepted": false, "id": "1512.04011"}, "pdf": {"name": "1512.04011.pdf", "metadata": {"source": "CRF", "title": "L1-Regularized Distributed Optimization: A Communication-Efficient Primal-Dual Framework", "authors": ["Virginia Smith", "Simone Forte", "Michael I. Jordan"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this paper, we consider standard regulated loss minimization problems, including as our main focus L1-regulated optimization problems of formmin \u03b1 Rn f (A\u03b1) + \u03bb-regulated classification and regression models where \u03b1-Rn is the weight vector, A-Rd-n is a pre-defined data matrix, and \u03bb is a regularization parameter. This formulation includes many popular L1-regulated classification and regression models, such as lasso and sparse logistic regression, and can easily be extended to other separable regulators such as elastic mesh. Models of this form are particularly useful in high-dimensional constellations because they tend to favor economical solutions. However, despite their importance, few methods exist to efficiently deploy such sparse-inducing models in the distributed environment."}, {"heading": "1.1 Contributions", "text": "Generalized Framework. Building on the COCOA + framework, PROXCOCOA + offers several advantages, including the use of any local solver on each machine and the analysis and ability to solve partial problems with arbitrary accuracy. However, unlike COCOCOA +, we are looking at a much broader class of optimization problems. This leads to a more general framework that: (1) specifically includes the case of L1 regulation; (2) allows the flexibility of the distribution of data either by characteristics or data points; and (3) can be executed either on the original or dual formulation, which has significant theoretical and practical implications; the analysis of non-strongly convective derivable derivatives; we derive convergence rates for the general class of problems considered in this paper; and the use of a novel approach to the analysis of Ur-dual dates for non-strongly convective derivatives. The proposed technique is a significant improvement over niche."}, {"heading": "2 Setup", "text": "A wide variety of methods in machine learning and signal processing is presented as the minimization of a weighted sum of two convex functions, the first term being a convex function of a linear predictor and the second term a regularizer: min \u03b1-Rn f (A\u03b1) + g (\u03b1). (A) Here \u03b1-Rn is the parameter vector, and A: = [x1;..; xn] is a data matrix with column vectors xi-Rd, i- [n] and row vectors yTj-Rn, j-Rn, j-D. Our central assumption will be that g (\u00b7) is separable, which means that g (\u03b1) = n-i = 1 gi (\u03b1i) for convex functions gi: R \u2192 R. In addition, we assume that f: Rd \u2192 R for (1 / 2) -smooth."}, {"heading": "3 The PROXCOCOA+ Algorithmic Framework", "text": "The PROXCOCOA + Framework is given in algorithm 1. This framework builds on the current COCOA + Framework (12, 17], although with a more general goal, a modified partial problem, and where we allow the method to be applied to either the primary or dual formulation. To distribute the method, we assign each machine to work only on local coordinates of the weight vector, and access only data stored locally. Machines share the state by the vector v: = A\u03b1. This vector is communicated in each turn after using local solution approaches to find (possibly) approximate solutions to the partial problems defined in (2). The solution of the primary problem (A) will be done directly with PROXCOCOCOA + in the distribution of the data in columns (according to feature), and have the vector v of the length equal to the number of data points. This can significantly reduce the communication costs as number of features (see section 6)."}, {"heading": "3.1 Primal-Dual Context", "text": "Using the primary dual structure is not a prerequisite for optimization (A); indeed, we have shown above how this optimization problem can be solved directly. However, establishing the relationship between primary and dual objectives has many advantages, including calculating the duality gap that allows us to have a certificate of approximation quality. It is also useful as an analysis tool and helps to relate this work to the previous work of [30, 12, 17]. To use this structure, we can start from our original formulation (A) with objective function D (\u03b1): = f (A\u03b1) + \u2211 n i = 1 gi (\u03b1i), the dual problem to the previous work of [30, 12, 17]."}, {"heading": "4 Convergence Analysis", "text": "In this section, we provide convergence rates for the proposed framework and introduce an important theoretical technique for the analysis of non-strongly convex terms in the primary-dual setting. To simplify the representation, we assume in the analysis that the data partition is balanced, i.e. nk = n / K for all k. Furthermore, we assume that the columns of A meet the requirements for all i [n]. We present results for the case in which \u03b3: = 1 is present in algorithm 1 and in which the partial problems (2) are defined using the associated safe limit \u03c3: = K. This case provides the fastest convergence rates in the distributed setting, which do not deteriorate in particular if the number of machines K grows and n remains unchanged."}, {"heading": "4.1 General Convex gi", "text": "Our first main theorem provides convergence guarantees for targets with non-strongly convex regulators, including models such as lasso and sparse logistic regression. Providing primary dual rates and globally defined primary dual accuracy certificates requires a theoretical technique that we present below in which we show how we can meet the following notion of L-bounded support. (7) As we explain in Section F of the appendix, our adoption of L-bounded support for the GI functions can be interpreted as assuming that their conjugates are global L-Lipschitz.Theorem 1. Consider algorithm 1 with 3, and let's consider the quality of the local solution in Assumption 1 that L-bounded support for the GI functions is global L-Lipschitz.Theorem"}, {"heading": "4.2 Strongly Convex gi", "text": "Theorem 2. Let us consider algorithm 1 with \u03b3: = 1, and let us allow the quality of the local solvent as in assumption 1. Let gi be as strongly convex as in assumption 1. Let gi be as strongly convex as i [n], and let f be (1 / \u03c4) -smooth. Then we have that T iterations are sufficient for sub-optimality D, with T \u2265 1\u03b3 (1 \u2212 \u0432), with T [P (w (T)))) + n around Log n D. (9) Further, after T iterations with T \u2265 1\u03b3 (1 \u2212 \u0432) + n around Log (1\u03b3 (1 \u2212 \u0432) + n around Log g g), E [P (w (T)))) \u2212 (\u2212 D (\u03b1 (T))). We provide evidence for both theorem 1 and theorem 2 in the appendix (section E)."}, {"heading": "5 Related Work", "text": "For strongly converged coordination rates, our knowledge has only been obtained for strongly converged data. [34] While the current state of the art for empirical loss minimization based on the randomized coordinate ascension method (SDCA) is largely secured, the SDCA family is often preferred because it is free of learning rate parameters and has faster (geometric) convergence guarantees. Interestingly, a similar trend among coordinate solvers has recently been observed in the lasso literature, but with the roles of primary and dual reverse communication. For these problems, coordinate descend methods at the primary level have become explicitly state-of-the-art, as in GLMNET [11] and extensions [34]; see e.g. the overview in [33]. Primal-dual convergence rates for unmodified coordination algorithms have only been obtained for our knowledge only for strongly converged data."}, {"heading": "6 Experimental Results", "text": "In this section, we compare PROXCOCOA + to numerous state-of-the-art methods for large-scale communication of L1-regulated optimization, including: MB-SGD: various methods to improve the performance of each system, with the first three methods being optimized and implemented in Apache's MLLP (v1.5.0). We employ an alternating method of multipliers \u2022 MB-CD: mini-batch descending in parallel, incl. SHOTGUNThe first three methods are optimized and implemented in Apache's MLP (v1.0). We coordinate the descent as a local solver for PROXCOCOCOA +, and apply PROXCOCOA + directly to the primary formulation of lasso and elastic meshes, mapping the problem directly."}, {"heading": "Acknowledgments", "text": "We thank Michael P. Friedlander and Martin Tak\u00e1c for fruitful discussions."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Definitions", "text": "Definition 2 (L-Lipschitz continuity). A function f: Rd \u2192 R is L-Lipschitz continuous if it is a, b-Lipschitz continuity. (10) Definition '1 (L-Bounded Support). A function f: Rd \u2192 R has L-limited support if its effective domain is limited by L, i.e., f (u) < + 3 (L-Smoothness). A function f: Rd \u2192 R is called L-smooth, for L > 0 if it is differentiable and its derivative L-Lipschitz continuous or equivalent f (u) \u2264 f (w) + < f (w), u \u2212 w > + L-smooth if it is distinguishable and its derivative L-Lipschitz continuous or equivalent f (u)."}, {"heading": "B Convex Conjugates", "text": "The convex conjugate of a function f: Rd \u2192 R is defined as f: 1 (v): = maxu: Rd vTu \u2212 f (u). (14) Below we list several useful properties of conjugates (see e.g. [6, Section 3.3.2]: \u2022 Double conjugate: (f: 0) * = f: f (v) = f: f (v) = f: f (v). \u2022 Value scaling: (for \u03b1 > 0) f (v: v) = f: f: f (w: 0) = \u03b1g: f (v: 0) f: 0 (v) = g (\u03b1v) \u21d2 f: f: f: 0 (w: 1). \u2022 Conjugate of a divisible sum: f (v: v) = v: f: p: vi (vi) \u21d2 f: f: f (w: 0) f: i (wi).Lemma 3 (duality between Lipschitzness and L: Bounded Support, [24, Korb: 3]."}, {"heading": "C Applications", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 L1 and General Non-Strongly Convex Regularizers", "text": "L1 regularization is achieved in target (A) by leaving gi (\u00b7): = \u03bb | \u00b7 |. Primary dual convergence can be achieved by using the modification introduced in Section 4, which guarantees L-limited support. Formally, we replace gi (\u00b7) = | \u00b7 | byg convergence (\u03b1): = | \u03b1 |: \u03b1 [\u2212 B, B] + \u221e: otherwise. For sufficiently large B, this problem provides the same solution as the original L1 target. We provide a detailed proof and description of this technique in Section D.3. Note that this applies only to convergence theory, as it allows us to present a strong primal dual rate (theorem 1 for L = B)."}, {"heading": "C.2 Elastic Net and General Strongly Convex Regularizers", "text": "Another application that we can consider is the regulation of the elastic mesh, \u03b72-2-2-2 + (1 \u2212 \u03b7) -1, for fixed parameters \u03b7-1 (0, 1], which is achieved by setting gi (\u03b1): = \u03bb [\u03b7 2\u03b1 2 + (1 \u2212 \u03b7) | \u03b1 |] in (A). In the special case \u03b7 = 0 we get the L1 standard. For elastic-mesh regulated problems of form (A), theorem 2 yields a global linear (geometric) convergence rate, since gi is strongly convex, as long as the data pass function is smooth (see section C.4) and directly yields a primary-dual algorithm and corresponding rate."}, {"heading": "C.3 Local Solvers for L1 and Elastic Net", "text": "For the L1 regularizer in the primary situation, the local partial problem (2) becomes a simple quadratic problem on the local data, whereby the regularization can only be applied to local variables \u03b1 [k]. Therefore, existing fast L1 solvers for the individual case, such as the GLMNET variants [11] or BLITZ [13], can be applied within algorithm 1 directly to each local partial problem G\u03c3 \u2032 k (\u00b7; v, \u03b1 [k]). The economy induced on the partial problem solutions of each machine naturally translates into the economy of the global solution, since the local variables \u03b1 [k] are linked. With regard to the approximation parameter for the local problems (assumption 1), we can apply existing current convergence results from the individual case of the machine."}, {"heading": "C.4 Smooth Data-Fit Functions", "text": "To illustrate the role of f as a smooth data pass function in this section - as opposed to its role as a regulator in the traditional COCOA +, as we discuss in Section F - consider the following examples: loss of the smallest squares. Let b-Rd be labels or response values, and consider the smallest squares as a target f (v): = 12 x v-b-2, which is 1-smooth. We get the familiar regression object of the smallest squares in our optimization problem (A) using f (A\u03b1): = 12 x-A\u03b1 \u2212 b-2. Let's consider that the gradient of f-b-v \u2212 v \u2212 b is the dual to-primary mapping resulting from: w (\u03b1): = f (v (\u03b1)) = A\u03b1 \u2212 b, which is well known as a residual vector in the smallest squares regression.Logistic regression loss. For classification problems, let's play a jj logistic regression model with jj = 1."}, {"heading": "D Proofs of Primal-Dual Relationship", "text": "In the following sections we provide derivatives of the primary-dual relationship of the general goals (A) and (B) and then show how this primary-dual structure can be derived for different applications."}, {"heading": "D.1 Primal-Dual Relationship", "text": "The relationship of our original formulation (A) to its dual formulation (B) is standard in convex analysis and is a special case of the concept of fennel duality. Using the combination with linear map A as in our case, the relationship is called fennel-rockafellar duality, see e.g. [4, theorem 4.4.2] or [2, proposition 15.18]. For completeness, we illustrate this correspondence with a self-contained derivative of duality. Starting from the original formulation (A), we introduce a helper variable vector v-Rd representing v = A\u03b1. Then, the optimization problem (A) becomes: min \u03b1-Rn f (v) + g (\u03b1) so that v = A\u03b1. (16) Introductive Lagrange multipliers w-Rd, the Lagrange multipliers w-Rd, the Lagrangian variable vector-Rd to the representation of Aww-T = T-\u03b1."}, {"heading": "D.2 Conjugates and Smoothness of f -Functions of Interest", "text": "Lemma 5 (Conjugate and smoothness of logistic loss).The logistic classifier loss (A\u03b1): = d = 1 log (1 + exp (\u2212 bjyTj \u03b1) (see also (15) above) is the conjugate of f *, where f * (w): = d \u00b2 j = 1 ((((1 + wjbj) log (1 + wjbj) \u2212 wjbj log (\u2212 wjbj)))), (18) with the box constraint \u2212 wjbj [0, 1]. Furthermore, f \u00b2 (w) is over its range 1 \u2212 strongly convex, if the labels bj \u00b2 [\u2212 1, 1] are satisfactory. Proof Lemma 5. By the separability of f \u00b2, the conjugate of f \u00b2 (v) = f \u00b2 bjugat (vj \u00b2), wj \u00b2 bj \u00b2 s (wj \u00b2), wj \u00b2 wj \u00b2 wj, jj \u00b2 1 = 1."}, {"heading": "D.3 Conjugates of Common Regularizers", "text": "Lemma 6 (Conjugate of the elastic net regularizer) = let's project this g (let's project this g) = let's project this g (let's project this g) = let's project this g (let's project this g) = let's project this g (let's project the g) = let's project the x (let's project the x) = let's project the x-project the x-project the x-project the x-project the x-project the x-project the x-project the x-project the x-project the x-project the x-project the x-project the x-project the x-project the values we projected the x-project the x-project the x-project the x-project-the x-project-the x-project-the R = let's project-the x-project-the x-the x-project-the R = let's project-the x-project-the x-the x-project-the values we projected:"}, {"heading": "E Convergence Proofs", "text": "In this section, we provide evidence for our most important convergence results. The results are motivated by [17], but where we have significantly generalized the problem of interest and where we derive a separate meaning by applying the problem directly to (A). We provide full details on Lemon 8 as proof of the concept, but omit details in later evidence that can be derived from the arguments in [17] or earlier work of [26], and instead outline the evidence strategy and highlight sections where the theory differs."}, {"heading": "E.1 Approximation of D(\u00b7) by the Local Subproblems G\u03c3\u2032k (\u00b7)", "text": "We start with a definition of the data-dependent aggregation parameter for PROXCOCOA = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K (K = K = K = K = K = K = K (K = K = K = K = K = K = K = K = K = K = K = K = K = K = K (K = K = K = K = K = K = K = K = K = K = K = K = K = K = K): K + K (K = K): K + K (K + K) (K): K + K + K (K + K) (K): K + K + K) + K (K = K) 2. (21) Simple convergence selection applies to (21), i.e. K = K = K + K + K + K) + K (K + K): K + K (K): (K): K + K)."}, {"heading": "E.2 Proof of Main Convergence Result (Theorem 1)", "text": "Before demonstrating the most important convergence results, we present several useful quantities, including the following problem, which describes the effect of the iterations of algorithm 1 on the duality gap for each selected local solver of approximation quality. Let's leave gi = = two convex problems with convexity parameters \u00b5 0 (relative to the norm of algorithm 1 (relative to the norm of algorithm 1), then all iterations of algorithm 1 under assumption 1, and all s problems [0, 1], it applies to E [D (\u03b1 (t))). \u2212 D (t) (relative to the norm of algorithm 1). \u2212 D (relative to all iterations of algorithm 1). \u2212 \u2212 \u2212 Then applies R (t). (24), where R (t): = \u2212 s)."}, {"heading": "E.3 Proof of Convergence Result for Strongly Convex gi", "text": "Our second main theorem follows the reasoning in [26] and is a generalization of [17, Korollar11]. We first introduce a dilemma to simplify the proof."}, {"heading": "F Recovering COCOA+ as a Special Case", "text": "As a special case, PROXCOCOA + refers directly to all L2-regulated loss minimization problems, including those presented in [12, 17]. In this context, the original machine learning problem is mapped to what we here refer to as the \"dual\" problem formulation (B): min w-Rd [P (w): = f-Rd (w) + n-Rd i = 1 g-Rd i (\u2212 xTi w)], where f-Ra (\u00b7) = 2-Rd is the regulator, and g-Rd [P (w) i assumes the role of the loss function and to a linear predictor xTi w (remember that xi is a column of the data matrix A). In other words, the PROXCOCOCOA + algorithm is applied in this case to (A) as a dual factor of the original input function (which relates to the original linear problem (which is mapped to (the following) remark) that (the Rometric)."}, {"heading": "G Experiment Details", "text": "This year it is more than ever before in the history of the city."}], "references": [{"title": "Scalable training of L1-regularized log-linear models", "author": ["G. Andrew", "J. Gao"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces. CMS Books in Mathematics", "author": ["H.H. Bauschke", "P.L. Combettes"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Parallel coordinate descent newton method for efficient g1-regularized minimization", "author": ["Y. Bian"], "venue": "arXiv.org,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Techniques of Variational Analysis and Nonlinear Optimization. Canadian Mathematical Society Books in Math", "author": ["J.M. Borwein", "Q. Zhu"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Parallel coordinate descent for l1-regularized loss minimization", "author": ["J.K. Bradley"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Primal-Dual Rates and Certificates", "author": ["C. D\u00fcnner"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Accelerated, Parallel, and Proximal Coordinate Descent", "author": ["O. Fercoq", "P. Richt\u00e1rik"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Distributed Optimization for Non-Strongly Convex Regularizers", "author": ["S. Forte"], "venue": "Master\u2019s thesis, ETH Zu\u0308rich,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Statistical Software,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["M. Jaggi"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Blitz: A Principled Meta-Algorithm for Scaling Sparse Optimization", "author": ["T. Johnson", "C. Guestrin"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization", "author": ["S.M. Kakade", "S. Shalev-Shwartz", "A. Tewari"], "venue": "Technical report,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Data/feature distributed stochastic coordinate descent for logistic regression", "author": ["Kang"], "venue": "In CIKM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "On the complexity analysis of randomized block-coordinate descent methods", "author": ["Z. Lu", "L. Xiao"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Adding vs. averaging in distributed primal-dual optimization", "author": ["C. Ma"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "A distributed block coordinate descent method for training l1 regularized linear classifiers", "author": ["D. Mahajan", "S.S. Keerthi", "S. Sundararajan"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Ad click prediction: a view from the trenches", "author": ["H.B. McMahan"], "venue": "In KDD,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Mllib: Machine learning in apache spark", "author": ["X. Meng"], "venue": "arXiv.org,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Parallel Random Coordinate Descent Method for Composite Minimization: Convergence Analysis and Error Bounds", "author": ["I. Necoara", "D. Clipici"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "SDNA: Stochastic dual newton ascent for empirical risk", "author": ["Z. Qu"], "venue": "minimization. arXiv.org,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Convex Analysis", "author": ["R.T. Rockafellar"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "Stochastic methods for l1-regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Mathematical Programming, Series", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "On the complexity of parallel coordinate descent", "author": ["R. Tappenden", "P. Richt\u00e1rik"], "venue": "arXiv.org,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Distributed coordinate descent for l1-regularized logistic regression", "author": ["I. Trofimov", "A. Genkin"], "venue": "arXiv.org,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["T. Yang"], "venue": "In NIPS,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "A Dual Augmented Block Minimization Framework for Learning with Limited Memory", "author": ["I.E.-H. Yen", "S.-W. Lin", "S.-D. Lin"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "A quasi-newton approach to nonsmooth convex optimization problems in machine learning", "author": ["J. Yu", "S. Vishwanathan", "S. G\u00fcnter", "N.N. Schraudolph"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "A comparison of optimization methods and software for large-scale l1-regularized linear classification", "author": ["G.-X. Yuan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "An improved glmnet for l1-regularized logistic regression", "author": ["G.-X. Yuan", "C.-H. Ho", "C.-J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization", "author": ["Y. Zhang", "X. Lin"], "venue": "In ICML,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "One promising distributed method is COCOA+ [12, 17], a recently proposed primal-dual framework that demonstrates competitive performance, provides a flexible communication scheme, and enables the use of off-the-shelf single-machine solvers internally.", "startOffset": 43, "endOffset": 51}, {"referenceID": 16, "context": "One promising distributed method is COCOA+ [12, 17], a recently proposed primal-dual framework that demonstrates competitive performance, provides a flexible communication scheme, and enables the use of off-the-shelf single-machine solvers internally.", "startOffset": 43, "endOffset": 51}, {"referenceID": 25, "context": "However, by solving the problem in the dual, COCOA+ (like SDCA, prox-SDCA, and numerous other primal-dual methods [26, 27, 30, 35, 36]) is only equipped to handle strongly convex regularizers, which prevents it from being directly applied to L1-regularized objectives.", "startOffset": 114, "endOffset": 134}, {"referenceID": 26, "context": "However, by solving the problem in the dual, COCOA+ (like SDCA, prox-SDCA, and numerous other primal-dual methods [26, 27, 30, 35, 36]) is only equipped to handle strongly convex regularizers, which prevents it from being directly applied to L1-regularized objectives.", "startOffset": 114, "endOffset": 134}, {"referenceID": 29, "context": "However, by solving the problem in the dual, COCOA+ (like SDCA, prox-SDCA, and numerous other primal-dual methods [26, 27, 30, 35, 36]) is only equipped to handle strongly convex regularizers, which prevents it from being directly applied to L1-regularized objectives.", "startOffset": 114, "endOffset": 134}, {"referenceID": 34, "context": "However, by solving the problem in the dual, COCOA+ (like SDCA, prox-SDCA, and numerous other primal-dual methods [26, 27, 30, 35, 36]) is only equipped to handle strongly convex regularizers, which prevents it from being directly applied to L1-regularized objectives.", "startOffset": 114, "endOffset": 134}, {"referenceID": 9, "context": "\u2020Parts of this work appear in SF\u2019s Master\u2019s Thesis [10].", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": ", [22, 27, 35] that enforce strong convexity by adding a small L2 term to the objective.", "startOffset": 2, "endOffset": 14}, {"referenceID": 26, "context": ", [22, 27, 35] that enforce strong convexity by adding a small L2 term to the objective.", "startOffset": 2, "endOffset": 14}, {"referenceID": 34, "context": ", [22, 27, 35] that enforce strong convexity by adding a small L2 term to the objective.", "startOffset": 2, "endOffset": 14}, {"referenceID": 11, "context": "This framework builds on the recent COCOA+ framework [12, 17], though with a more general objective, a modified subproblem, and where we allow the method to be applied to either the primal or dual formulation.", "startOffset": 53, "endOffset": 61}, {"referenceID": 16, "context": "This framework builds on the recent COCOA+ framework [12, 17], though with a more general objective, a modified subproblem, and where we allow the method to be applied to either the primal or dual formulation.", "startOffset": 53, "endOffset": 61}, {"referenceID": 0, "context": "k=1 G \u2032 k (\u2206\u03b1[k];v,\u03b1[k]) = L+ f(v) +\u2207f(v)A\u2206\u03b1 + \u03c3\u2032 2\u03c4 \u2206\u03b1 \uf8ef\uf8f0A T [1]A[1] 0 .", "startOffset": 62, "endOffset": 65}, {"referenceID": 0, "context": "k=1 G \u2032 k (\u2206\u03b1[k];v,\u03b1[k]) = L+ f(v) +\u2207f(v)A\u2206\u03b1 + \u03c3\u2032 2\u03c4 \u2206\u03b1 \uf8ef\uf8f0A T [1]A[1] 0 .", "startOffset": 66, "endOffset": 69}, {"referenceID": 16, "context": "Assumption 1 (\u0398-approximate solution, see [17]).", "startOffset": 42, "endOffset": 46}, {"referenceID": 29, "context": "It is also useful as an analysis tool and helps relate this work to the prior work of [30, 12, 17].", "startOffset": 86, "endOffset": 98}, {"referenceID": 11, "context": "It is also useful as an analysis tool and helps relate this work to the prior work of [30, 12, 17].", "startOffset": 86, "endOffset": 98}, {"referenceID": 16, "context": "It is also useful as an analysis tool and helps relate this work to the prior work of [30, 12, 17].", "startOffset": 86, "endOffset": 98}, {"referenceID": 7, "context": "This \u201cLipschitzing\u201d trick will make the conjugates g\u2217 i globally defined and Lipschitz [8], as we prove in Section 4.", "startOffset": 87, "endOffset": 90}, {"referenceID": 10, "context": "Further, it allows us to directly leverage state-of-the-art coordinate-wise primal methods, such as GLMNET [11] and extensions [34, 13].", "startOffset": 107, "endOffset": 111}, {"referenceID": 33, "context": "Further, it allows us to directly leverage state-of-the-art coordinate-wise primal methods, such as GLMNET [11] and extensions [34, 13].", "startOffset": 127, "endOffset": 135}, {"referenceID": 12, "context": "Further, it allows us to directly leverage state-of-the-art coordinate-wise primal methods, such as GLMNET [11] and extensions [34, 13].", "startOffset": 127, "endOffset": 135}, {"referenceID": 21, "context": "To address this, existing approaches typically use a simple smoothing technique as in [22]: by adding a small amount of L2 to the L1-norm, it becomes strongly convex; see, e.", "startOffset": 86, "endOffset": 90}, {"referenceID": 26, "context": ", [27].", "startOffset": 2, "endOffset": 6}, {"referenceID": 7, "context": "3, and illustrate how to leverage it for a variety of applications (see Section C of the appendix and also [8]).", "startOffset": 107, "endOffset": 110}, {"referenceID": 25, "context": "For strongly convex regularizers, current state-of-the-art for empirical loss minimization is randomized coordinate ascent on the dual (SDCA) [26] and its accelerated variants, e.", "startOffset": 142, "endOffset": 146}, {"referenceID": 26, "context": ", [27].", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "For those problems, coordinate descent methods on the primal have become state-of-the-art, as in GLMNET [11] and extensions [34]; see, e.", "startOffset": 104, "endOffset": 108}, {"referenceID": 33, "context": "For those problems, coordinate descent methods on the primal have become state-of-the-art, as in GLMNET [11] and extensions [34]; see, e.", "startOffset": 124, "endOffset": 128}, {"referenceID": 32, "context": ", the overview in [33].", "startOffset": 18, "endOffset": 22}, {"referenceID": 26, "context": "However, primal-dual convergence rates for unmodified coordinate algorithms have to our knowledge been obtained only for strongly convex regularizers to date [27, 35].", "startOffset": 158, "endOffset": 166}, {"referenceID": 34, "context": "However, primal-dual convergence rates for unmodified coordinate algorithms have to our knowledge been obtained only for strongly convex regularizers to date [27, 35].", "startOffset": 158, "endOffset": 166}, {"referenceID": 10, "context": "In the single-coordinate update case, this is at the core of GLMNET [11, 33], and widely used in, e.", "startOffset": 68, "endOffset": 76}, {"referenceID": 32, "context": "In the single-coordinate update case, this is at the core of GLMNET [11, 33], and widely used in, e.", "startOffset": 68, "endOffset": 76}, {"referenceID": 24, "context": ", solvers based on the primal formulation of L1-regularized objectives [25, 34, 3, 9, 28].", "startOffset": 71, "endOffset": 89}, {"referenceID": 33, "context": ", solvers based on the primal formulation of L1-regularized objectives [25, 34, 3, 9, 28].", "startOffset": 71, "endOffset": 89}, {"referenceID": 2, "context": ", solvers based on the primal formulation of L1-regularized objectives [25, 34, 3, 9, 28].", "startOffset": 71, "endOffset": 89}, {"referenceID": 8, "context": ", solvers based on the primal formulation of L1-regularized objectives [25, 34, 3, 9, 28].", "startOffset": 71, "endOffset": 89}, {"referenceID": 27, "context": ", solvers based on the primal formulation of L1-regularized objectives [25, 34, 3, 9, 28].", "startOffset": 71, "endOffset": 89}, {"referenceID": 10, "context": "When changing more than one coordinate at a time, again employing a quadratic upper bound on the smooth part, this results in a two-loop method as in GLMNET [11] for the special case of logistic regression.", "startOffset": 157, "endOffset": 161}, {"referenceID": 6, "context": "Parallel coordinate descent for L1-regularized objectives (with and without using mini-batches) was proposed in [7] (Shotgun) and generalized in [3] , and is among the best performing solvers in the parallel setting.", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "Parallel coordinate descent for L1-regularized objectives (with and without using mini-batches) was proposed in [7] (Shotgun) and generalized in [3] , and is among the best performing solvers in the parallel setting.", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "Other parallel coordinate descent methods on the L1-objective have recently been analyzed in [9, 28, 21], but not in the communication-efficient or distributed setting.", "startOffset": 93, "endOffset": 104}, {"referenceID": 27, "context": "Other parallel coordinate descent methods on the L1-objective have recently been analyzed in [9, 28, 21], but not in the communication-efficient or distributed setting.", "startOffset": 93, "endOffset": 104}, {"referenceID": 20, "context": "Other parallel coordinate descent methods on the L1-objective have recently been analyzed in [9, 28, 21], but not in the communication-efficient or distributed setting.", "startOffset": 93, "endOffset": 104}, {"referenceID": 17, "context": "The methods most closely related to our approach are distributed variants of GLMNET as in [18].", "startOffset": 90, "endOffset": 94}, {"referenceID": 33, "context": "Inspired by GLMNET and [34], the work of [3, 18] introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context.", "startOffset": 23, "endOffset": 27}, {"referenceID": 2, "context": "Inspired by GLMNET and [34], the work of [3, 18] introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context.", "startOffset": 41, "endOffset": 48}, {"referenceID": 17, "context": "Inspired by GLMNET and [34], the work of [3, 18] introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context.", "startOffset": 41, "endOffset": 48}, {"referenceID": 28, "context": "The later work of [29] specialized this approach to sparse logistic regression.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "If hypothetically each of our quadratic subproblems G\u03c3 k (\u2206\u03b1[k]) as defined in (2) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer [18, 34, 23].", "startOffset": 289, "endOffset": 301}, {"referenceID": 33, "context": "If hypothetically each of our quadratic subproblems G\u03c3 k (\u2206\u03b1[k]) as defined in (2) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer [18, 34, 23].", "startOffset": 289, "endOffset": 301}, {"referenceID": 22, "context": "If hypothetically each of our quadratic subproblems G\u03c3 k (\u2206\u03b1[k]) as defined in (2) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer [18, 34, 23].", "startOffset": 289, "endOffset": 301}, {"referenceID": 17, "context": "While [18] allows a fixed accuracy for these subproblems\u2014but not arbitrary approximation quality \u0398 as in our framework\u2014the work of [29, 34, 31] assumes that the quadratic subproblems are solved exactly.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "While [18] allows a fixed accuracy for these subproblems\u2014but not arbitrary approximation quality \u0398 as in our framework\u2014the work of [29, 34, 31] assumes that the quadratic subproblems are solved exactly.", "startOffset": 131, "endOffset": 143}, {"referenceID": 33, "context": "While [18] allows a fixed accuracy for these subproblems\u2014but not arbitrary approximation quality \u0398 as in our framework\u2014the work of [29, 34, 31] assumes that the quadratic subproblems are solved exactly.", "startOffset": 131, "endOffset": 143}, {"referenceID": 30, "context": "While [18] allows a fixed accuracy for these subproblems\u2014but not arbitrary approximation quality \u0398 as in our framework\u2014the work of [29, 34, 31] assumes that the quadratic subproblems are solved exactly.", "startOffset": 131, "endOffset": 143}, {"referenceID": 17, "context": "On the theoretical side, the rate results provided by [18, 29, 34] are not explicit convergence rates but only asymptotic, as the quadratic upper bounds are not explicitly controlled for safety as with our \u03c3\u2032.", "startOffset": 54, "endOffset": 66}, {"referenceID": 28, "context": "On the theoretical side, the rate results provided by [18, 29, 34] are not explicit convergence rates but only asymptotic, as the quadratic upper bounds are not explicitly controlled for safety as with our \u03c3\u2032.", "startOffset": 54, "endOffset": 66}, {"referenceID": 33, "context": "On the theoretical side, the rate results provided by [18, 29, 34] are not explicit convergence rates but only asymptotic, as the quadratic upper bounds are not explicitly controlled for safety as with our \u03c3\u2032.", "startOffset": 54, "endOffset": 66}, {"referenceID": 4, "context": "ADMM [5], proximal gradient descent, and quasi-Newton methods such as L-BFGS and are also often used in distributed environments because of their relatively low communication requirements.", "startOffset": 5, "endOffset": 8}, {"referenceID": 18, "context": "The works of [19] and [15] have obtained encouraging results for distributed systems employing coordinate descent variants on L1-problems.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "The works of [19] and [15] have obtained encouraging results for distributed systems employing coordinate descent variants on L1-problems.", "startOffset": 22, "endOffset": 26}, {"referenceID": 0, "context": "We include experimental comparisons with ADMM, prox-GD, and orthant-wise limited memory quasi-Newton (OWL-QN) [1], an L-BFGS variant that can handle L1 regularization [32], but which has no convergence rate.", "startOffset": 110, "endOffset": 113}, {"referenceID": 31, "context": "We include experimental comparisons with ADMM, prox-GD, and orthant-wise limited memory quasi-Newton (OWL-QN) [1], an L-BFGS variant that can handle L1 regularization [32], but which has no convergence rate.", "startOffset": 167, "endOffset": 171}, {"referenceID": 19, "context": "0) [20].", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "As expected, naively distributing SHOTGUN [7] (single coordinate updates per machine) does not perform well, as it is tailored to shared-memory systems and requires communicating too frequently.", "startOffset": 42, "endOffset": 45}, {"referenceID": 16, "context": "Finally, we point out several important ways in which PROXCOCOA+ improves upon the COCOA+ framework [17].", "startOffset": 100, "endOffset": 104}, {"referenceID": 26, "context": ", [27, 35] \u2014 adding a small amount of strong convexity \u03b4\u2016\u03b1\u20162 to the objective for Lasso regression.", "startOffset": 2, "endOffset": 10}, {"referenceID": 34, "context": ", [27, 35] \u2014 adding a small amount of strong convexity \u03b4\u2016\u03b1\u20162 to the objective for Lasso regression.", "startOffset": 2, "endOffset": 10}, {"referenceID": 16, "context": "1COCOA+ in [17] is in fact limited to the case where the regularizer is equal to the L2 norm 12\u2016 \u00b7 \u2016 2 2, though the extension to strongly convex regularizers is covered as a special case in our analysis.", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "References [1] G.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] O.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Kang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Z.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] X.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Z.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Therefore, existing fast L1-solvers for the single-machine case, such as GLMNET variants [11] or BLITZ [13] can be directly applied to each local subproblem G\u03c3 k ( \u00b7 ;v,\u03b1[k]) within Algorithm 1.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "Therefore, existing fast L1-solvers for the single-machine case, such as GLMNET variants [11] or BLITZ [13] can be directly applied to each local subproblem G\u03c3 k ( \u00b7 ;v,\u03b1[k]) within Algorithm 1.", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "For example, for randomized coordinate descent (as part of GLMNET), [16, Theorem 1] gives a O(1/t) approximation quality for any separable regularizer, including L1 and elastic net; see also [28, 25].", "startOffset": 191, "endOffset": 199}, {"referenceID": 24, "context": "For example, for randomized coordinate descent (as part of GLMNET), [16, Theorem 1] gives a O(1/t) approximation quality for any separable regularizer, including L1 and elastic net; see also [28, 25].", "startOffset": 191, "endOffset": 199}, {"referenceID": 0, "context": "with the box constraint \u2212wjbj \u2208 [0, 1].", "startOffset": 32, "endOffset": 38}, {"referenceID": 0, "context": "For the losses, the conjugate pairs are \u03c6j(u) = log(1 + exp(\u2212bju)), and \u03c6j (wj) = \u2212wjbj log(\u2212wjbj) + (1 + wjbj) log(1 + wjbj) with \u2212wjbj \u2208 [0, 1], see e.", "startOffset": 139, "endOffset": 145}, {"referenceID": 0, "context": "\u1e21(\u03b1) = sup x\u2208[0,1] \u03b1x ,", "startOffset": 13, "endOffset": 18}, {"referenceID": 16, "context": "The results are motivated by [17], but where we have significantly generalized the problem of interest, and where we derive separate meaning by applying the problem directly to (A).", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "We provide full details of Lemma 8 as a proof of concept, but omit details in later proofs that can be derived using the arguments in [17] or earlier work of [26], and instead outline the proof strategy and highlight sections where the theory deviates.", "startOffset": 134, "endOffset": 138}, {"referenceID": 25, "context": "We provide full details of Lemma 8 as a proof of concept, but omit details in later proofs that can be derived using the arguments in [17] or earlier work of [26], and instead outline the proof strategy and highlight sections where the theory deviates.", "startOffset": 158, "endOffset": 162}, {"referenceID": 0, "context": "Then for all iterations t of Algorithm 1 under Assumption 1, and any s \u2208 [0, 1], it holds that E[D(\u03b1)\u2212D(\u03b1)] \u2265 \u03b3(1\u2212\u0398) ( sG(\u03b1)\u2212 \u03c3 \u2032s2 2\u03c4 R ) , (24) where R := \u2212 \u03c4\u03bc(1\u2212s) \u03c3\u2032s \u2016u (t) \u2212\u03b1\u2016 + \u2211K k=1\u2016A(u \u2212\u03b1)[k]\u2016 , (25) for u \u2208 R with u (t) i \u2208 \u2202g \u2217 i (\u2212xi w(\u03b1)) .", "startOffset": 73, "endOffset": 79}, {"referenceID": 0, "context": "s = 1 1 + 12\u03b3(1\u2212\u0398)(t\u2212 t0) \u2208 [0, 1] , (40)", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "s = 1 (T \u2212 T0)\u03b3(1\u2212\u0398) \u2208 [0, 1] (43)", "startOffset": 23, "endOffset": 29}, {"referenceID": 25, "context": "Our second main theorem follows reasoning in [26] and is a generalization of [17, Corollary 11].", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "Assume that gi(0) \u2208 [0, 1] for all i \u2208 [n], then for the zero vector \u03b1 := 0 \u2208 R, we have D(\u03b1)\u2212D(\u03b1) = D(0)\u2212D(\u03b1) \u2264 n .", "startOffset": 20, "endOffset": 26}, {"referenceID": 0, "context": "s = \u03c4\u03bc \u03c4\u03bc+ \u03c3max\u03c3\u2032 \u2208 [0, 1] (50)", "startOffset": 20, "endOffset": 26}, {"referenceID": 11, "context": "F Recovering COCOA+ as a Special Case As a special case, PROXCOCOA+ directly applies to any L2-regularized loss-minimization problem, including those presented in [12, 17].", "startOffset": 163, "endOffset": 171}, {"referenceID": 16, "context": "F Recovering COCOA+ as a Special Case As a special case, PROXCOCOA+ directly applies to any L2-regularized loss-minimization problem, including those presented in [12, 17].", "startOffset": 163, "endOffset": 171}, {"referenceID": 11, "context": "In other words, the PROXCOCOA + algorithm will in this case apply to (A) as the dual of the original input problem (which will be mapped to (B)), as described in [12, 17].", "startOffset": 162, "endOffset": 170}, {"referenceID": 16, "context": "In other words, the PROXCOCOA + algorithm will in this case apply to (A) as the dual of the original input problem (which will be mapped to (B)), as described in [12, 17].", "startOffset": 162, "endOffset": 170}, {"referenceID": 4, "context": "ADMM Alternating Direction Method of Multipliers (ADMM) [5] is a popular method that lends itself naturally to the distributed environment.", "startOffset": 56, "endOffset": 59}, {"referenceID": 24, "context": "Mini-batch CD Mini-batch CD aims to improve mini-batch SGD by employing coordinate descent, which has encouraging theoretical and practical backings [25, 9, 28].", "startOffset": 149, "endOffset": 160}, {"referenceID": 8, "context": "Mini-batch CD Mini-batch CD aims to improve mini-batch SGD by employing coordinate descent, which has encouraging theoretical and practical backings [25, 9, 28].", "startOffset": 149, "endOffset": 160}, {"referenceID": 27, "context": "Mini-batch CD Mini-batch CD aims to improve mini-batch SGD by employing coordinate descent, which has encouraging theoretical and practical backings [25, 9, 28].", "startOffset": 149, "endOffset": 160}, {"referenceID": 6, "context": "Shotgun As a special case of mini-batch CD, Shotgun [7] is a popular method for parallel optimization.", "startOffset": 52, "endOffset": 55}, {"referenceID": 31, "context": "OWL-QN OWN-QN [32] is a quasi-Newton method optimized in Spark\u2019s spark.", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "We note that since the framework and theory allow any internal solver to be used, PROXCOCOA+ could benefit even beyond the results shown, by using existing fast L1-solvers for the single-machine case, such as GLMNET variants [11] or BLITZ [13].", "startOffset": 225, "endOffset": 229}, {"referenceID": 12, "context": "We note that since the framework and theory allow any internal solver to be used, PROXCOCOA+ could benefit even beyond the results shown, by using existing fast L1-solvers for the single-machine case, such as GLMNET variants [11] or BLITZ [13].", "startOffset": 239, "endOffset": 243}], "year": 2016, "abstractText": "Despite the importance of sparsity in many large-scale applications, there are few methods for distributed optimization of sparsity-inducing objectives. In this paper, we present a communication-efficient framework for L1-regularized optimization in the distributed environment. By viewing classical objectives in a more general primal-dual setting, we develop a new class of methods that can be efficiently distributed and applied to common sparsity-inducing models, such as Lasso, sparse logistic regression, and elastic net-regularized problems. We provide theoretical convergence guarantees for our framework, and demonstrate its efficiency and flexibility with a thorough experimental comparison on Amazon EC2. Our proposed framework yields speedups of up to 50\u00d7 as compared to current state-of-the-art methods for distributed L1-regularized optimization.", "creator": "LaTeX with hyperref package"}}}