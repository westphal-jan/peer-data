{"id": "1512.04087", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2015", "title": "True Online Temporal-Difference Learning", "abstract": "The temporal-difference methods TD($\\lambda$) and Sarsa($\\lambda$) form a core part of modern reinforcement learning. Their appeal comes from their good performance, low computational cost, and their simple interpretation, given by their forward view. Recently, new versions of these methods were introduced, called true online TD($\\lambda$) and true online Sarsa($\\lambda$), respectively (van Seijen and Sutton, 2014). Algorithmically, these true online methods only make two small changes to the update rules of the regular methods, and the extra computational cost is negligible in most cases. However, they follow the ideas underlying the forward view much more closely. In particular, they maintain an exact equivalence with the forward view at all times, whereas the traditional versions only approximate it for small step-sizes. We hypothesize that these true online methods not only have better theoretical properties, but also dominate the regular methods empirically. In this article, we put this hypothesis to the test by performing an extensive empirical comparison. Specifically, we compare the performance of true online TD($\\lambda$)/Sarsa($\\lambda$) with regular TD($\\lambda$)/Sarsa($\\lambda$) on random MRPs, a real-world myoelectric prosthetic arm, and a domain from the Arcade Learning Environment. We use linear function approximation with tabular, binary, and non-binary features. Our results suggest that the true online methods indeed dominate the regular methods. Across all domains/representations the learning speed of the true online methods are often better, but never worse than that of the regular methods. An additional advantage is that no choice between traces has to be made for the true online methods. We show that new true online temporal-difference methods can be derived by making changes to the real-time forward view and then rewriting the update equations.", "histories": [["v1", "Sun, 13 Dec 2015 17:13:33 GMT  (1951kb,D)", "https://arxiv.org/abs/1512.04087v1", null], ["v2", "Thu, 8 Sep 2016 18:56:23 GMT  (1646kb,D)", "http://arxiv.org/abs/1512.04087v2", "This is the published JMLR version. It is a much improved version. The main changes are: 1) re-structuring of the article; 2) additional analysis on the forward view; 3) empirical comparison of traditional and new forward view; 4) added discussion of other true online papers; 5) updated discussion for non-linear function approximation"]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["harm van seijen", "a rupam mahmood", "patrick m pilarski", "marlos c machado", "richard s sutton"], "accepted": false, "id": "1512.04087"}, "pdf": {"name": "1512.04087.pdf", "metadata": {"source": "CRF", "title": "True Online Temporal-Difference Learning", "authors": ["Harm van Seijen", "A. Rupam Mahmood", "Patrick M. Pilarski", "Marlos C. Machado", "Richard S. Sutton", "Rupam Mahmood"], "emails": ["harm.vanseijen@maluuba.com", "ashique@ualberta.ca", "patrick.pilarski@ualberta.ca", "machado@ualberta.ca", "sutton@cs.ualberta.ca"], "sections": [{"heading": null, "text": "Keywords: Forward-view equivalencec \u00a9 2016 Harm van Seijen, A. Rupam Mahmood, Patrick M. Pilarski, Marlos C. Machado and Richard S. Sutton.ar Xiv: 151 2.04 087v 2"}, {"heading": "1. Introduction", "text": "This is one of the most important challenges in the field of enhancing learning is to make predictions online about the (discounted) sum of future rewards, the return, based on the currently observed characteristics and a certain behavioral policy, in a previously unknown environment. TD learning makes it possible to quickly learn good estimates of the expected return by bootstrapping from other expected return estimates. TD (\u03bb) (Sutton, 1988) is a popular TD algorithm that combines basic TD learning methods with eligibility. TD (\u03bb) popularity can be explained by its simple implementation, its low computational complexity, and its straightforward interpretation given by its forward-looking vision."}, {"heading": "2. Background", "text": "Here we present the most important learning framework: as a convention we refer to scalar random variables by uppercase letters (e.g. St, Rt), vectors by bold lowercase letters (e.g. \u03b8, \u03c6), functions by non-bold lowercase letters (e.g. v) and sets by calligraphic lettering (e.g. S, A).1"}, {"heading": "2.1 Markov Decision Processes", "text": "Reinforcement learning (RL) problems are often formalized as Markov decision processes (MDPs = MDPs), which can be described as 5-tuples of the form < S, A, p, r, \u03b3 >, where S indicates the set of all states; A indicates the set of all actions; p (s \"| s, a) is indicates the probability of a transition to state s,\" when action a \"s taken in state s\" S; r (s, a \"s) is indicates the expected reward for a transition from state s\" under action a \"; the discount factor \u03b3 is specified how future rewards are in terms of the immediate reward.Actions are taken at discrete time steps t = 0, 1, 2,... according to a Phillips policy: S\" A \"\u2192 [0, 1], which defines the selection probability is conditioned on the state.\" The return at time t is defined as the discounted sum of rewards observed after t: Gt: S. \""}, {"heading": "2.2 Temporal-Difference Learning", "text": "Consider the task of learning an estimate V of the value function v\u03c0 from samples, where v\u03c0 = > is projected by linear function approximation. That is, V is the inner product between a feature vector \u03c6 (s) \u0445 Rn of s and a weight vector \u03b8 Rn: V (s, \u03b8) = \u03b8 (s). If s is a terminal state, then by definition (s): = 0, and therefore V (s, \u03b8) = 0. We can formulate the problem of estimating v\u03c0 as an error minimization problem, where the error is a weighted mean of the square difference between the value of a state and its estimate: E (\u03b8): = 12 \u2211 i d\u03c0 (si) (si) \u2212 Prognosticated (si) > Prognosticated (si))))) 2, where litution is a problem where the error is a weighted mean of the quadrified difference between the value of a state and its estimate."}, {"heading": "2.3 TD(\u03bb)", "text": "The TD algorithm (\u03bb) implements the following update equations: \u03b4t = Rt + 1 + \u03b3\u03b8 > t \u03c6t + 1 \u2212 \u03c6 > t \u03c6t, (2) \u2190 et = \u03b3\u03bbet \u2212 1 + \u03c6t, (3) \u03b8t + 1 = \u03b8t + \u03b1\u03b4t et, (4) for t \u2265 0, and with e \u2212 1 = 0. The scalar error is called a TD error, and the vector et is called a permission trace vector, which is discussed below. While these updates differ from the general gradient-descend update rule e, we refer to this version of TD (\u03bb) as an \"accumulated TD (\u03bb)\" to distinguish it from a slightly different version, which is discussed below. While these updates differ from the general gradient-descend update rule e, there is a close connection to this update rule from TD (the one that gives a close connection to this update rule)."}, {"heading": "3. The Online Forward View", "text": "Traditional foresight refers the TD (\u03bb) update equations to the general update rule shown in Equation (1). Specifically, for small increments, the weight vector at the end of an episode computed by accumulated TD (\u03bb) is roughly the same as the weight vector resulting from a sequence of Equation (1) updates (one for each visited state), using a specific multi-stage update target known as \u03bb return (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996).The \u03bb return for the state of St is defined as: = (1 \u2212 \u03bb) T \u2212 1 \u2211 n = 1 \u03bbn \u2212 1G (n) t \u2212 1Gt, (5) where T is the time step in which the final state is reached, and G (n) t is the n-step return, defined as the online value: n-T return on an end yield."}, {"heading": "3.1 The Online \u03bb-Return Algorithm", "text": "The concept of an online preview contains a paradox. On the one hand, multi-stage update targets require data from time steps far beyond the time a state is visited; on the other hand, the online aspect requires that the value of a visited state be immediately updated. The solution to this paradox is to assign a sequence of update targets to each visited state. The first updated target in this sequence contains data only from the next time step, the second contains data from the next two time steps, the third from the next three time steps, and so on. Now, there is an initial weight vector and a sequence of visited states, a new weight vector can be constructed by updating each visited state with an update target containing data up to the current time step. Below, we define the preliminary return for the state Sk with horizon h > k as follows."}, {"heading": "3.2 Comparison to Accumulate TD(\u03bb)", "text": "While accumulated TD (\u03bb) actually behaves like the online progression algorithm - as predicted by Theem - the difference increases with small steps, so small steps often lead to slow learning. Therefore, higher steps are desirable \u2212 \u2212 For larger steps, however, the behavior of the accumulated TD (\u03bb) can be very different from that of the online progression algorithm. And, as we show in the empirical section of this article, the behavior of the online progression algorithm at larger steps can be almost exclusively in favor of the online progression algorithm. In this section, we will analyze why the online progression algorithm can outperform the accumulated TD (\u03bb) by using the one-state example shown in Figure 2 on the left. The RMS error over the first 10 episodes of the one-state example for different step sizes and progression = 1. While for small steps, accumulated TD (\u03bb) behaves like the online progression algorithm in fact."}, {"heading": "3.3 Comparison to Replace TD(\u03bb)", "text": "The sensitivity of the accumulated TD (\u03bb) to divergence, as demonstrated in the previous subsection, has been known for a long time. In fact, the replacement of TD (\u03bb) was designed to deal with this. However, while the replacement of TD (\u03bb) is much more robust in terms of divergence, it also has its limitations. An obvious limitation is that it refers only to binary characteristics, so it is not universally applicable, but even in areas where the replacement of TD (\u03bb) can be applied, it can work poorly, the reason being that the replacement of previous trace values, rather than complementing them, reduces the multistage characteristics of TD (\u03bb). To illustrate this, let us consider the two-step example shown in left Figure 3. It is easy to see that the value of the farthest left state is 2 and the other state is 0. The state representation consists of only one single feature, which is the final state, and it is exactly in both states."}, {"heading": "4. True Online TD(\u03bb)", "text": "Fortunately, it is possible to rewrite the update equations of the online \u03bb return algorithm into another set of update equations that can be implemented with a time-independent computational complexity. In fact, this alternative set of update equations differs from the update equations of accumulated TD (\u03bb) only in two additional terms, each of which can be efficiently calculated.The algorithm that implements these equations is called online true TD (\u03bb) and is discussed below. Algorithm 2: true online TD (\u03bb) INPUT: \u03b1, \u03bb, \u03b3, \u03b3, successinit. This algorithm can only be realized with two additional terms, each of which can be calculated efficiently. The algorithm that implements these equations is called real online TD (\u03bb) INPUT."}, {"heading": "4.1 The Algorithm", "text": "The length of this sequence, and thus the calculation per time step, increases over time. However, it is possible to calculate the weight vector resulting from the sequence in time step t + 1 directly from the weight vector resulting from the sequence in time step t, resulting in the following update equations (see Appendix B for the online derivative): \u03b4t = Rt + 1 + vault code > t + 1 \u2212 V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V (10) et = V-V-V-V-V-V-V-V-V (e > t \u2212 V-V-V)."}, {"heading": "4.2 When Can a Performance Difference be Expected?", "text": "In Section 3, a number of examples were shown where the online return algorithm has a difference that replaces / replaces actual performance. As true online TD (\u03bb) is simply an efficient implementation of the online return algorithm, the difference between the online return algorithm / true online TD (\u03bb) and the accumulated TD (\u03bb) will not always be a performance difference either. In this section, we will identify two other factors that influence whether or not there will be a performance difference. While the focus of this section is on performance differences and not performance benefits, our experiments show that true online TD (\u03bb) is always at least as good as accumulated TD (\u03bb) and replace TD (\u03bb)."}, {"heading": "4.3 True Online Sarsa(\u03bb)", "text": "From a learning perspective, the main difference is that the prediction of the expected return should be made dependent on the state and the plot, and not just the state. This means that an estimate of the action value function q\u03c0 is learned, rather than the state value function v\u03c0.Another difference is that instead of a fixed policy that generates the behavior, politics depends on the action value estimates. Since these estimates typically improve over time, so does politics. The (on-policy) control counterpart to TD (\u03bb) is the popular Sarsa (\u03bb) algorithm. The control counterpart to a real online strategy is \"real online sarsa (\u03bb)\" real online sarsa. \"Algorithm 3 shows pseudo code for real online measures (\u03bb)."}, {"heading": "5. Empirical Study", "text": "This section contains our most important empirical study, in which both TD (\u03bb) and Sarsa (\u03bb) are compared with their real online counterparts. For each method and domain, a scan is performed on the step size \u03b1 and the trace degradation parameter \u03bb, so that the optimal performance can be compared. In Section 5.4 we discuss the results."}, {"heading": "5.1 Random MRPs", "text": "For our first series of experiments, we used randomly constructed Markov reward processes (MRPs).4 An MRP can be interpreted as an MDP with only one action per state. Consequently, there is only one policy that is possible. We represent a random MRP as a 3-tuple (k, b, \u03c3) consisting of k, the number of states; b, the branching factor (i.e., the number of next states with a non-zero transition probability); and \u03c3, the standard deviation of the reward. An MRP is constructed as follows: The b potential next states for a given state are drawn from the total number of states randomly and without substitution; the transition probabilities to these states are also randomized (by partitioning the unit interval at b \u2212 1 random intersections). The expected value of the reward for a transition is drawn from a normal distribution with zero unit and unit variance."}, {"heading": "5.2 Predicting Signals From a Myoelectric Prosthetic Arm", "text": "In this experiment, we compared the performance of true online TD (\u03bb) and TD (\u03bb) on a real-world dataset consisting of sensorimotor signals measured during human control of an electromechanical robotic arm. The source of the data is a series of manipulation tasks performed by a participant with an amputation, as represented by Pilarski et al. (2013). In this study, an amputated participant used signals taken from the muscles of the remaining arm to control a robotic arm with several degrees of freedom (Figure 5). Such interactions are known as myoelectric control (see, for example, Parker et al., 2006). For consistency and comparison of the results, we used the same source data and prediction architecture as published in Pilarski et al. (2013) Overall, two signals are predicted: adhesive force and motor angular signals from the hand of the robot."}, {"heading": "5.3 Control in the ALE Domain Asterix", "text": "In this last experiment, we compared the performance of real online Sarsa (\u03bb) with that of accumulated Sarsa (\u03bb) and replaced Sarsa (\u03bb) in an area of the Arcade Learning Environment (ALE). We compared the performance of real online Sarsa (\u03bb) with that of accumulated three actions. We compared the performance of real online Sarsa (\u03bb) with that of accumulated three actions and actions. We compared the performance of Sarsa (\u03bb et al., 2013; Defazio & Graepel, 2014; Mnih et al., 2015), called Asterix. \"The ALE is a general test bed that interfaces hundreds of Atari 2600 games.5In the Asterix domain, the agent controls a yellow avatar who must collect objects while avoiding\" harp \"objects while avoiding\" objects (see Figure 7 for a screenshot). Both compositions and Harp's screen move across the Asterix domain a total of 50 life points, and each time he receives one harp of each horizontal."}, {"heading": "5.4 Discussion", "text": "Figure 9 summarizes the performance of the various versions of TD (\u03bb) across all weighting ranges. Specifically, it shows the error for each method in its best settings of \u03b1 and \u03bb. The error is normalized by dividing it by the error at \u03bb = 0 (remember that all versions of TD (\u03bb) behave the same for \u03bb = 0). Since \u03bb = 0 is within the parameter range that will be optimized over time, the normalized error can never be higher than 1. However, if for a method / domain the normalized error is equal to 1, this means that setting higher than 0 either has no effect or that the error gets worse. In both cases, credentials are not effective for this method / domain. Overall, real online TD (\u03bb) is clearly better than accumulated TD (\u03bb) and replace TD (\u03bb) and replace the optimal performance."}, {"heading": "6. Other True Online Methods", "text": "Appendix B shows that the true online TD (\u03bb) equations can be derived directly from the online forward equations. By using different online forward views, new real online methods can be derived. Sometimes, small changes in forward view, such as the use of a time-dependent step size, can lead to surprising changes in the true online equations. In this section, we will consider a number of such variations."}, {"heading": "6.1 True Online TD(\u03bb) with Time-Dependent Step-Size", "text": "If we use a time-dependent step size in the basic equation of the Preview (Eq. 7) and derive the updated equations from Appendix B, it turns out that a slightly different trace definition appears. We show this new trace with a \"+\" high sentence: e +. For fixed step size, this new trace definition is the same: e + t = \u03b1et, for all. Naturally, the use of e + violent instead of violent also slightly changes the weight vector update. In the following, the complete set of updated equations is presented: violent = Re + 1 + \u03b3t > violent +, e + violent = Wallace > violent - and violent - and violent - and violent - and violent - and violent - and violent - and violent - and violent - and violent - and violent - and violent - and violent - ((e + violent - and violent - and) - (1)"}, {"heading": "6.2 True Online Version of Watkins\u2019s Q(\u03bb)", "text": "So far, we have only looked at \"on-policy\" methods, i.e. methods that evaluate a policy that is the same as the policy that generates the samples. However, the true online principle can also be applied to \"off-policy\" methods, for which the evaluation policy differs from the behavioral policy. Let's take a simple example of Watkins \"Q (\u03bb) (Watkins, 1989), an off-policy method that evaluates greedy policies based on arbitrary behavioral policy.This is done by combining tracks with a TD error that uses the maximum state action value of the successor state: \u03b4t = Rt + 1 + max aQ (St, a) \u2212 Q (St, At). Furthermore, tracks are reset to 0 when a non-greedy action occurs. From an online preview perspective, the strategy of Watkins\" Qgorithm method, which does not equate the Qalgorithm-action measures, is very true: the Qalgorithm method can be taken as an updated target action that stops growth when a greedy action is interpreted."}, {"heading": "6.3 Tabular True Online TD(\u03bb)", "text": "Tabular features are a special case of linear function approximation. Therefore, the updated equations for real online TD (\u03bb) that are presented so far can also be applied to the tabular case. However, we will discuss them here separately, because the simplicity of this special case may provide additional insights. Algorithm 6 tabular real online TD (\u03bb) initializes v (s) for all s loop (via episodes): initializes S e (s) \u2190 0 for all s Vold \u2190 0 While S is not endless, applies: Get the next state S \u2032 and reward R \u0445 V (S) \u2212 Vold Vold \u2190 V (S \u2032) GOP (S \u2032) GOP V (S \u2032) \u2212 V (S) \u2190 (S) + 1 for all s: V (S) + 1 for all s: V (s) \u2190 V (s) + update of V (s) + \u03b1 (V\u03b1) e (s) e (s). \u2212 St \u2212 St \u2212 S \u2212 V (V)."}, {"heading": "7. Related Work", "text": "Since the first publication on true online TD (\u03bb) (van Seijen & Sutton, 2014), several related papers have been published that expand the underlying concepts and improve presentation, which are reviewed in sections 7.1, 7.2 and 7.3 and discussed in section 7.4 other variants of TD (\u03bb)."}, {"heading": "7.1 True Online Learning and Dutch Traces", "text": "As already mentioned, the traditional forward view based on the \u03bb return is per se an offline forward view = > forward view, since the \u03bb return is constructed from data until the end of an episode. Consequently, work on the equivalence between a forward view and a backward view is traditionally focused on the final weight vector PhenomenT. This changed in 2014 when two papers introduced an online forward view with a corresponding backward view, which has an exact equivalence at any moment in time (van Seijen & Sutton, 2014; Sutton et al., 2014) While both papers introduced an online forward view, the two forward views differ greatly from each other. One difference is that the forward view introduced by van Seijen & Sutton is a political forward view, while the forward view by Sutton et al. It is, however, a non-political forward view."}, {"heading": "7.2 Backward View Derivation", "text": "The task of finding an efficient forward view that matches a particular online forward view is not an easy one. Furthermore, there is no guarantee that there will be an efficient implementation of a particular online forward view. Often, minor changes in forward view determine whether or not an efficient backward view can be constructed, thus creating the desire to somehow automate the process of constructing an efficient backward view.Van Seijen & Sutton (2014) were the first to attempt to develop a general strategy for deriving a backward view (although for forward view based on Eq.19). Van Hasselt et al. (2014) simply took the approach of providing a theorem that proves the equivalence between a general forward view and a corresponding general backward view. They showed that the forward / backward view of true online TDs (previous year) is a special case of this general forward / backward view."}, {"heading": "7.3 Extension to Non-Linear Function Approximation", "text": "The linear update equations of the online forward view presented in Section 3.1 can easily be extended to the case of nonlinear function approximation. Unfortunately, it seems impossible to construct an efficient backward view with exact equivalence in the case of nonlinear function approximation, because the derivative in Appendix B takes advantage of the fact that the gradient in relation to the value function is independent of the weight vector; this does not apply to nonlinear function approximation. Fortunately, van Seijen (2016) shows that many of the benefits of true online learning can also be achieved in the case of nonlinear function approximation through an alternative forward view (but still based on Equation 18)."}, {"heading": "7.4 Other Variations on TD(\u03bb)", "text": "Schapire & Warmuth (1996) introduced a variant of TD (\u03bb) for which upper and lower performance limits can be derived and verified. Konidaris et al. (2011) introduced TD\u03b3, a parameter-free alternative to TD (\u03bb) based on a multi-stage update start.7 The span of prediction refers to the time difference between the first prediction and the time at which its target is known (e.g. for episodic tasks corresponding to the length of an episode).The TD\u03b3 is an offline algorithm whose calculation costs are proportional to episode length. Furthermore, Thomas et al. (2015) proposed a method based on a multi-stage update target, which they call \"Return.\" The return method can take into account the correlation between different length returns, something that concerns both the return forecast and the return forecast."}, {"heading": "8. Conclusions", "text": "We tested the hypothesis that true online TD (\u03bb) (and true online Sarsa (\u03bb)) dominate the TD (\u03bb) (and Sarsa (\u03bb)) both when accumulating and replacing tracks through experiments over a wide range of areas. Our extensive results support this hypothesis. In terms of learning speed, real online TD (\u03bb) was often better, but never worse than TD (\u03bb) with the accumulation or replacement of tracks in all areas / representations that we have tried. Our analysis showed that, especially in areas with no sparse features and a relatively small deviation in yield, a big difference in learning speed is to be expected. More generally, real online TD (\u03bb) has the advantage that it can be used with non-binary features, and it has the advantage over TD (\u043c) in the accumulation of tracks that it is less sensitive in terms of its parameters. In terms of computing time, T\u03bb may be a slightly more expensive case than the typical TD, which is only slightly more expensive."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Hado van Hasselt for the in-depth discussions that led to the refinement of these ideas, as well as the anonymous critics for their valuable suggestions, which resulted in a much improved presentation, supported by grants from Alberta Innovates - Technology Futures and the National Science and Engineering Research Council of Canada. Computing Canada provided the computing resources through WestGrid."}, {"heading": "Appendix A. Proof of Theorem 1", "text": "Theorem 1: We always use weights with duplicate indices for the online return algorithm and weights with single indices for the cumulative TD."}, {"heading": "Appendix B. Derivation Update Equations", "text": "In this subsection we derive the updated equations of the actual online TD (\u03bb) = > > > Preview, defined by equations (6) and (7) (and preliminary efficiency). \u2212 \u2212 \u2212 The derivative is based on the expression of the factor + 1t + > error. We start, of course, with the notation of the vector of action and the intermediate effectiveness. Let us first consider (7) as: force \u2212 \u2212 \u2212 1 = (I \u2212 force + 1 = k) force + force > k) force of action + force > k) force of action of k + t, with the identity matrix. Now we consider force tk for k = 1 and k = 2: force \u2212 \u2212 force of action > 0 force + force of action > k) force + force of action of the factor > k) force + force of action of k = (I \u2212 force of action of k + force of action of the factor)."}, {"heading": "Appendix C. Detailed Results Random MRPs", "text": "van Seijen, Mahmood, Pilarski, Machado and SuttonAppendix D. Detailed Results for Myoelectric Prothetic Armvan Seijen, SuttonANGLE PREDICTION FORCE PREDICTIONBESTOTDRTracesATracesFigure 5: Analysis of TOTD in terms of accumulating and replacing traces on prosthetic data from the single amputee subject described in Pilarski et al. (2013), for the prediction of servo motor angle (left column) and grip force (right column) as recorded from the amputee's myoelectrically controlled robot arm during a grasping task.16van Seijen, Sutton ANGLE PREDICTION FORCE PREDICTION PREDICTION PREDICTION PREDICTION PREDICTION PREDICTION PREDICTION PREDICTION PREDICTION PREDICTION PREDICTION PREDICTION PREDICTION PREDICTION PREDICON PREDICTION PREDICON PREDICTION PREDICTION PREDICTION PREDICTION PREDICON PREDICON PREDICON PREDICON PREDICTION PREDICON PREDICTION PREDICTION PREDICON PREDICON PREDICON PREDICTION PREDICON PREDICON PREDICON PREDICON PREDICON PREDICON PREDICON PREDICON PREDICON PREDICON PREDICCE PREDICCE PREDICCE PREDICCE PREDICCE PREDICCE PREDICCE PREDICCE: PREDICCE PREDICON PREDICON PREDICON PREDICTION PREDICTION PREDICTION."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "The temporal-difference methods TD(\u03bb) and Sarsa(\u03bb) form a core part of modern reinforcement learning. Their appeal comes from their good performance, low computational cost, and their simple interpretation, given by their forward view. Recently, new versions of these methods were introduced, called true online TD(\u03bb) and true online Sarsa(\u03bb), respectively (van Seijen & Sutton, 2014). Algorithmically, these true online methods only make two small changes to the update rules of the regular methods, and the extra computational cost is negligible in most cases. However, they follow the ideas underlying the forward view much more closely. In particular, they maintain an exact equivalence with the forward view at all times, whereas the traditional versions only approximate it for small step-sizes. We hypothesize that these true online methods not only have better theoretical properties, but also dominate the regular methods empirically. In this article, we put this hypothesis to the test by performing an extensive empirical comparison. Specifically, we compare the performance of true online TD(\u03bb)/Sarsa(\u03bb) with regular TD(\u03bb)/Sarsa(\u03bb) on random MRPs, a real-world myoelectric prosthetic arm, and a domain from the Arcade Learning Environment. We use linear function approximation with tabular, binary, and non-binary features. Our results suggest that the true online methods indeed dominate the regular methods. Across all domains/representations the learning speed of the true online methods are often better, but never worse than that of the regular methods. An additional advantage is that no choice between traces has to be made for the true online methods. Besides the empirical results, we provide an in-depth analysis of the theory behind true online temporal-difference learning. In addition, we show that new true online temporaldifference methods can be derived by making changes to the online forward view and then rewriting the update equations.", "creator": "LaTeX with hyperref package"}}}