{"id": "1512.04092", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2015", "title": "Stack Exchange Tagger", "abstract": "The goal of our project is to develop an accurate tagger for questions posted on Stack Exchange. Our problem is an instance of the more general problem of developing accurate classifiers for large scale text datasets. We are tackling the multilabel classification problem where each item (in this case, question) can belong to multiple classes (in this case, tags). We are predicting the tags (or keywords) for a particular Stack Exchange post given only the question text and the title of the post. In the process, we compare the performance of Support Vector Classification (SVC) for different kernel functions, loss function, etc. We found linear SVC with Crammer Singer technique produces best results.", "histories": [["v1", "Sun, 13 Dec 2015 17:52:44 GMT  (455kb)", "http://arxiv.org/abs/1512.04092v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sanket mehta", "shagun sodhani"], "accepted": false, "id": "1512.04092"}, "pdf": {"name": "1512.04092.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Shagun Sodhani"], "emails": ["sanketmehta.iitr@gmail.com", "sshagunsodhani@gmail.com."], "sections": [{"heading": null, "text": "Our problem is an example of the more general problem of developing accurate classifiers for large-format text records. We are addressing the multi-label classification problem, where each element (in this case question) can belong to several classes (in this case tags). We are predicting the tags (or keywords) for a particular Stack Exchange post, specifying only the question text and title of the post, comparing the performance of support vector classification (SVC) for different core functions, loss functions, etc. We have found that linear SVC with Crammer Singer technology delivers the best results."}, {"heading": "1. Main Objectives", "text": "- Use SVC with different core functions (rbf, linear, polynomial, sigmoid) - Compare performance in terms of number of iterations, loss function, regularization term."}, {"heading": "2. Status and other details", "text": "(https: / / github.com / shagunsodhani / StackExchange-tagger)"}, {"heading": "3. Major stumbling blocks", "text": "- Stack Exchange Dataset: We needed time to scratch the entire data set. - Computational Power Limitation: The time complexity for determining the SingularValue Decomposition (SVD) for an mxnmatrix is (2 + 3). - Error selection: Since classification with multiple labels differs from classification with multiple classes, we need to change accuracy, precision and callback for classifiers with multiple labels."}, {"heading": "4. Introduction", "text": "In fact, most of them will be able to play by the rules."}, {"heading": "5. Related Work", "text": "Also, they focus only on stackoverflow.com and not on other member pages of the StackExchange network. Our work differs from the existing work because none of the existing work performs survey analysis. [10] In addition, most of the related work focuses on getting good results for a particular member page of the StackExchange network, while in our case we keep all methods very general, making them applicable to all member pages. [10] uses a random model that predicts keywords based on the words in the message and their relationship (randomness) to keywords. They have developed a model for the StackOverflow dataset by limiting the next word to just keywords. Its random model has a 47% classification accuracy that predicts one day per post. Our experimental results show that we beat its accuracy, as mentioned in Section 7."}, {"heading": "6. Proposed Approach", "text": "Figure 6.1 shows the proposed workflow of our system. We explain each step in detail in the following section."}, {"heading": "6.1 Data Collection \u2013 Stack Exchange Data", "text": "StackExchange Network provides all community-contributed content under the Creative Commons BY-SA 3.0 License. A quarterly dump of all of this data (after cleanup) is updated in the Internet archive. Apart from this method, all data is available via the StackExchange API. We have used both the dumps and the API to obtain our data. This data included information about posts, users, voices, comments, badges, posthistory and postlinks, of which we kept the information related to the issue and tags and filtered out the remaining information. Figure 6.2 shows a snapshot of an example from the stackoverflow.com website for members."}, {"heading": "6.2 Data Preprocessing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1 Parsing and Removing Noise", "text": "The content extracted from Stack Exchange archives and from scrapes is in HTML format. Therefore, we first analyze the text by filtering HTML tags. Next, we remove any snippets of code that users may have added with their question, and retain only the words used in the question itself."}, {"heading": "6.2.2 Removing Stop Words", "text": "Stopwords refer to words such as \"and,\" \"or,\" \"the,\" etc., which do not contain specific information about the context of text. These words are usually removed as part of pre-processing. There is not a single universal list of stopwords that can be used in all contexts. In many cases, developers have to come up with their own list of stopwords. Also, what is a stopword in one context cannot be a stopword in another context. For example, we usually treat mathematical symbols as stopwords, but they become relevant when our text contains words such as C + +."}, {"heading": "6.2.3 Stemming", "text": "Stemming [6], [7] refers to the process of reducing words to their root, also known as root, and hence the name Stemming. A program that can execute Stemming is called Stemming. For example, words \"fishing,\" \"fished,\" and \"fisherman\" would be traced back to the word \"fish.\" Most information retraction systems use Stemming as a pre-processing step before storing data or before applying more sophisticated techniques to user data. Many algorithms are available for Stemming. Among the most well-known are the Porter, the Snowball and the Lancaster Controller. The Porter Controller is the most convenient algorithm and consists of 5 phases of word reduction, which are applied successively."}, {"heading": "6.2.4 Lemmatization", "text": "Lemmatization is the process of grouping different forms of a word in order to treat them as a single word. This single word is called Lemmatization, hence the name Lemmatization. For example, the verb \"to eat\" can appear as \"to eat,\" \"to eat,\" \"to eat,\" etc., although all these words can be reduced to a general problem, i.e. \"to eat.\" In our implementation, we have used the \"Ordering Lemmetizer.\""}, {"heading": "6.2.5 Tf-Idf based filtering", "text": "tf-idf [9] (Term Frequency-inverse document frequency) is defined for a word that contains a collection of documents (also called a corpus), indicating how important the word is for the given corpus. We have used it as a balancing factor to remove some words that do not convey information about the context of the problem at hand. Meaning varies proportionally with the number of times the word appears in the document and inversely proportional to the frequency of the word in the corpus. (,) = 0.5 + 0.5% (,) max. (,): (,): (,) refers to the term, (,) refers to the document, (,) is the rough frequency of a term in a document. (,) = (,) = (,) is the total frequency of a term in a document."}, {"heading": "6.3 Feature Extraction", "text": "Character extraction refers to the process of deriving attributes / values from the given dataset, so that the derived attributes are more informative and less redundant than the parameters in the given dataset. This is closely related to dimensionality reduction, where we reduce the number of dimensions of the given dataset to make calculations feasible. Some important techniques are SVD (Singular Value Decomposition) and PCA (Principal Component Analysis). We have used SVD and will explain it further. SVD [9] is a dimensionality reduction technology that produces a factorization of each matrix, whether real or complex. SVD connects the rows and columns of a matrix by defining a small number of \"concepts.\" Let us be an m \u00d7 n matrix, and let us be the rank of r matrix. The rank of a matrix is the largest number of rows (or columns) we can select."}, {"heading": "6.4 Building Tag Predictor", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.4.1 Support Vector Classification (SVC)", "text": "Considering the formation vectors, = 1,..., in two classes and one vector () (1, \u2212 1), our primary problem formulation is as follows: min 12 () + () 1 \u2212, 0, = 1,..., its dual is as follows: min 12 (,) (,) () is the vector of all ones and is the regulation parameter, is a positive semi-defined matrix, (,) (), (), (), (), ())) is the core. The decision function, as defined in [11], [13]: (,,"}, {"heading": "6.4.2 Linear Support Vector Classification (Linear SVC)", "text": "Linear SVC is SVC with a linear kernel. We are performing further optimizations with linear SVC, as our previous results have shown that linear SVC behaves better than SVC with other cores. When using linear SVC, we are experimenting with both the loss function and the optimization technique - namely, the traditional multi-class optimization technique or the Crammer-Singer approach. We were playing around with the \"hinge\" loss function and the \"square hinge\" loss function. Next, we are reverting to the traditional multi-class optimization technique versus the Crammer-Singer approach. The primary approach to solving multi-class problems using support vector machines focused on reducing a single multi-class problem into multiple binary problems. For example, we can build a series of binary classifiers to differentiate between terms. This approach is more commonly known as the One-vs-Rest approach Crammer's Combined Kernel Problems. An alternative method to solving multi-class problems with the help of 12 Singer's Combined Optimizer set has been proposed."}, {"heading": "6.5 Testing Tag Predictor", "text": "The error metrics we use are proposed in [14] for multi-class classification problems. Let it be a data set with multiple labels, consisting of examples with multiple labels (,), = 1. |,. Let it be a classifier with multiple labels and () be those of predicted labels. The following metrics are used to evaluate and evaluate: (,) = 1 | (,) = 1 (,) = 1 (,) = 1 (,) = 1 (,) = 1 (,,,,,"}, {"heading": "7. Experimental Results", "text": "erD rf\u00fc ide rf\u00fc die f \u00fc die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f der die f die f die f der die f die f der die f die f die f der die f die f die f der die f die f die f die f die f die f der die f die f der die f die f die f die f die f die f der die f die f die f der die f die f die f der die f e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e"}, {"heading": "8. Conclusion", "text": "We conclude that linear SVC performs better than all other core functions on both soft and hard edge problems. In the case of linear SVC, linear SVC with Crammer Singer soft edge technique performs better than ome-vs-rest technique. The best accuracy achieved in our case is 54.75%."}, {"heading": "9. Future Scope", "text": "For our analysis, too, we looked only at the text portion of the data and ignored any segments of code or user information present in the system. Also, many tags occur jointly. http: / / vizibility.net / blog / tag-youreit-3 Reasons why tags are important. / 2. Diakopoulos, Nicholas A., and David A. Shamma. \"Characterizingdebate performance via aggregated twitter sentiments.\" Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1. ACM, 2010. 3. Stanley, Clayton, and Michael D. Byrne. \"4. forproceedings of the SIGCHI Conference on Human Factors in Computing Systems."}], "references": [{"title": "Characterizing debate performance via aggregated twitter sentiment.", "author": ["Diakopoulos", "Nicholas A", "David A. Shamma"], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Predicting tags for stackoverflow posts.", "author": ["Stanley", "Clayton", "Michael D. Byrne"], "venue": "Proceedings of ICCM. Vol", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "On word prediction methods", "author": ["Kuo", "Darren"], "venue": "Technical report, EECS Department,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Support-vector networks.\" Machine learning", "author": ["Cortes", "Corinna", "Vladimir Vapnik"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "On the Algorithmic Implementation of Multiclass Kernel-based  Vector Machines", "author": ["Koby Crammer", "Yoram Singer"], "venue": "(PDF). J. of Machine Learning Research", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "A tutorial on support vector machines for pattern recognition.\" Data mining and knowledge discovery", "author": ["Burges", "Christopher JC"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Multi-label classification: An overview.", "author": ["Tsoumakas", "Grigorios", "Ioannis Katakis"], "venue": "Dept. of Informatics, Aristotle University of Thessaloniki,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Tags are also used as a form of query based search for information retrieval [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "Some work has already been done around this problem to address tag prediction but it still remains a challenge [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "[3] focuses on mining user interest from their behavior on stackoverflow.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[10] uses a cooccurrence model that predicts tags based on the words in the post and their relation (cooccurrence) to tags.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "The decision function as defined in [11], [13] is: sgn(\u2211 yi\u03b1iK(xi , x n i=1 ) + \u03c1 ), Where, \u03c1 is intercept.", "startOffset": 36, "endOffset": 40}, {"referenceID": 5, "context": "The decision function as defined in [11], [13] is: sgn(\u2211 yi\u03b1iK(xi , x n i=1 ) + \u03c1 ), Where, \u03c1 is intercept.", "startOffset": 42, "endOffset": 46}, {"referenceID": 4, "context": "An alternate method was proposed by Crammer and Singer [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "The error metrices that we have used are proposed in [14] for multi-label classification problems.", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "Also rbf kernel is able to beat the method in [10].", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "Also rbf kernel is able to beat the method in [10].", "startOffset": 46, "endOffset": 50}], "year": 2015, "abstractText": "The goal of our project is to develop an accurate tagger for questions posted on Stack Exchange. Our problem is an instance of the more general problem of developing accurate classifiers for large scale text datasets. We are tackling the multilabel classification problem where each item (in this case, question) can belong to multiple classes (in this case, tags). We are predicting the tags (or keywords) for a particular Stack Exchange post given only the question text and the title of the post. In the process, we compare the performance of Support Vector Classification (SVC) for different kernel functions, loss function, etc. We found linear SVC with Crammer Singer technique produces best results.", "creator": "Microsoft\u00ae Word 2013"}}}