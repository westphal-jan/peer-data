{"id": "1512.04105", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2015", "title": "Policy Gradient Methods for Off-policy Control", "abstract": "Off-policy learning refers to the problem of learning the value function of a way of behaving, or policy, while following a different policy. Gradient-based off-policy learning algorithms, such as GTD and TDC/GQ, converge even when using function approximation and incremental updates. However, they have been developed for the case of a fixed behavior policy. In control problems, one would like to adapt the behavior policy over time to become more greedy with respect to the existing value function. In this paper, we present the first gradient-based learning algorithms for this problem, which rely on the framework of policy gradient in order to modify the behavior policy. We present derivations of the algorithms, a convergence theorem, and empirical evidence showing that they compare favorably to existing approaches.", "histories": [["v1", "Sun, 13 Dec 2015 19:20:14 GMT  (39kb,D)", "http://arxiv.org/abs/1512.04105v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["lucas lehnert", "doina precup"], "accepted": false, "id": "1512.04105"}, "pdf": {"name": "1512.04105.pdf", "metadata": {"source": "META", "title": "Policy Gradient Methods for Off-policy Control", "authors": ["Lucas Lehnert", "Doina Precup"], "emails": ["lucas.lehnert@mail.mcgill.ca", "dprecup@cs.mcgill.ca"], "sections": [{"heading": "1 Introduction", "text": "A basic concept in Reinforcement Learning (RL) is Temporary Difference (TD) Learning introduced by Sutton [9]. In TD Learning, methods such as TD (0) are used for policy evaluation, where one tries to learn the value of a particular state under a fixed policy. However, the extension of the control case is called Q-Learning, where the value function is defined on government action packages. Control policy is then calculated from these action points. One of the first Q-Learning algorithms was proposed by Watkins and Dayan [14], which simultaneously seeks and evaluates a policy by estimating its action value. Watkins and Dayan's Q-Learning algorithm is an off-policy algorithm, as the policy that is sought and evaluated is strictly greedy in relation to current action values, but is used for the control of the agent."}, {"heading": "2 Q-learning with Policy Gradients", "text": "We consider an MDP M = < S, A, t, r, \u03b3 > where S is a finite state space and A a finite action space. As in [13, 5] we consider the case of linear function approximation with a basic function: S \u00b7 A \u2192 Rk and define the state value function asQ\u03b8 (s, a) = \u03b8 > \u03c6 (s, a). As in [13, 5] we consider the case of linear function approximation with a basic function: S \u00b7 A \u2192 Rk and define the state value function asQ\u03b8 (s, a) > \u03c6 (s, a) \u2248 Q (s, a)."}, {"heading": "2.1 Gradient Derivation", "text": "To get the gradient of the MSPBE target, [13] we have MSPBE > > JavaSIS (JavaScript) > JavaScript (JavaScript) > JavaScript (JavaScript) > JavaScript (JavaScript) = JavaScript (JavaScript) > (JavaScript) \u2212 JavaScript (JavaScript) \u2212 JavaScript (JavaScript) \u2212 JavaScript (JavaScript) \u2212 JavaScript (JavaScript) > (JavaScript) > (JavaScript) > (JavaScript) > (JavaScript) > (JavaScript) > (JavaScript) > (JavaScript) = JavaScript (JavaScript)"}, {"heading": "2.2 Sampling the Gradient", "text": "To derive a stochastic course (6), we (5) > > Expectations by (5) > Expectations (5) > Expectations (4) > Expectations (5), (5), Letw = (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, 5, (5), (5), (5), (5, 5, 5, (5), (5), (5), (5, 5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5, (5), (5), (5, (5), 5, (5), (5), (5, (5), (5), (5), (5, (5), (5), (5, 5, (5), (5, (5), (5, 5), (5, 5, 5, (5), (5, 5, (5), (5), (5, (5), (5, (5, 5, 5), (5, (5, 5, 5, 5, 5, 5, 5), (5, (5), (5, 5, (5, 5), (5, (5), (5, 5, 5, (5, 5, 5, 5, (5), (5, (5), (5), (5), (5, (5), (5, (5), ("}, {"heading": "3 Baird Counter Example", "text": "We tested our method on the \"Stern\" Baird counterexample [1] and compared it with Q-Learning and GQ [5]. For this 7-state version, the divergence of Q-Learning is known to be monotonous and GQ converges [7]. We initialize the parameter vector error corresponding to the action, the transitions to the 7th central state with (1, 1, 1, 1, 1, 10) and the remaining parameter entries with 1. The discount factor is set at \u03b3 = 0.99. In our experiments, we do not assume a hard-coded policy that ensures uniform exploration via state action pairs, but consider the control case in which actions are selected by means of a Boltzmann policy in which the probability of selecting a specific action target (a | s) = exp (s, a) / s) = expectorial (s) / 4).b (exp, exp, exp (expec), amporator (s), amporator (exp)."}, {"heading": "4 Conclusion", "text": "The resulting algorithm is similar to the GQ / TDC, but also has a corrective concept in the direction of the gradient of the target policy. Our analysis assumes that the Markov chain depends on the parameter vector \u03b8 through the target policy. This enables our algorithm to correctly exceed a sequence of different Markov chains and to take into account the drift in the distribution from which the transition data is scanned due to changes in the parameter vector. A next direction of research is to extend this method to the nonlinear case of functional approximation. Maei [6] presents the first gradient-based TD algorithm that converges in this case. We can draw on its results for our work. To derive our algorithm, we only assume that the Bellman operator is parametric in the parameter estimation, which leads to the additional political gradients."}], "references": [{"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["Leemon Baird"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Policy evaluation with temporal differences: A survey and comparison", "author": ["Christoph Dann", "Gerhard Neumann", "Jan Peters"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Chattering in sarsa(lambda) - a cmu learning lab internal report", "author": ["Geoffrey J. Gordon"], "venue": "Technical report,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Reinforcement learning with function approximation converges to a region", "author": ["Geoffrey J. Gordon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Gq(lambda): A general gradient algorithm for temporal-difference prediction learning with eligibility traces", "author": ["H.R. Sutton R.S. Maei"], "venue": "Proceedings of the Third Conference on Artificial General Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Convergent temporaldifference learning with arbitrary smooth function approximation", "author": [], "venue": "In Advances in Neural Information Processing Systems 22,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Toward off-policy learning control with function approximation", "author": [], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "A convergent form of approximate policy iteration", "author": ["Theodore J. Perkins", "Doina Precup"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Richard S. Sutton"], "venue": "Mach. Learn.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S. Sutton", "David McAllester", "Satinder Singh", "Yishay Mansour"], "venue": "In IN ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["Richard S. Sutton", "Hamid Reza Maei", "Doina Precup", "Shalabh Bhatnagar", "David Silver", "Csaba Szepesv\u00e1ri", "Eric Wiewiora"], "venue": "Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}], "referenceMentions": [{"referenceID": 11, "context": "Gradient-based off-policy learning algorithms, such as GTD and TDC/GQ [13], converge even when using function approximation and incremental updates.", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "One fundamental concept in Reinforcement Learning (RL) is Temporal Difference (TD) learning introduced by Sutton [9].", "startOffset": 113, "endOffset": 116}, {"referenceID": 11, "context": "Recently, gradient-based off-policy learning algorithms were introduced such as GTD [11] and TDC [13] which are also proven to be convergent under off-policy learning with linear value function approximation.", "startOffset": 97, "endOffset": 101}, {"referenceID": 4, "context": "The extension to Q-learning, GQ(\u03bb) [5], is also convergent under off-policy learning but only if the control policy is fixed.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "SARSA also suffers from this problem and is only guaranteed to converge to a sub-space of policies [4, 3].", "startOffset": 99, "endOffset": 105}, {"referenceID": 2, "context": "SARSA also suffers from this problem and is only guaranteed to converge to a sub-space of policies [4, 3].", "startOffset": 99, "endOffset": 105}, {"referenceID": 10, "context": "Similar to the policy gradient framework [12] we directly analyze the interaction between the policy gradient and the distribution from which transitions are sampled.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "methods such as [8].", "startOffset": 16, "endOffset": 19}, {"referenceID": 11, "context": "As in [13, 5] we consider the linear function approximation case with a basis function \u03c6 : S \u00d7A \u2192 R and define the state-action value function as Q\u03b8(s, a) = \u03b8 >\u03c6(s, a) \u2248 Q(s, a) = E [ \u221e \u2211", "startOffset": 6, "endOffset": 13}, {"referenceID": 4, "context": "As in [13, 5] we consider the linear function approximation case with a basis function \u03c6 : S \u00d7A \u2192 R and define the state-action value function as Q\u03b8(s, a) = \u03b8 >\u03c6(s, a) \u2248 Q(s, a) = E [ \u221e \u2211", "startOffset": 6, "endOffset": 13}, {"referenceID": 11, "context": "The Mean Squared Projected Bellman Error introduced by [13] is MSPBE(\u03b8) = ||Q\u03b8 \u2212\u03a0T\u03b8Q\u03b8||D, (2) where \u03a0 = \u03a6(\u03a6D\u03a6)\u22121\u03a6>D is the projection matrix and the Bellman operator applied to the action value function is defined as T\u03b8Q\u03b8 def = R+ \u03b3P\u03b8Q\u03b8.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "1 Gradient Derivation To obtain the gradient of the MSPBE objective, [13] have shown", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "We have tested our method on the \u201dstar\u201d Baird counter example [1] and compared it with Q-learning and GQ [5].", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "We have tested our method on the \u201dstar\u201d Baird counter example [1] and compared it with Q-learning and GQ [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "For this 7 state version divergence of Q-learning is monotonic and GQ is known to converge [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Figure 2 shows the MSPBE and the Mean Squared TD-error (MSTDE) defined in [2] of the parameter vector \u03b8 at each step of the simulation.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "Maei [6] present the first gradient based TD algorithm that converges in this case.", "startOffset": 5, "endOffset": 8}], "year": 2015, "abstractText": "Off-policy learning refers to the problem of learning the value function of a way of behaving, or policy, while following a different policy. Gradient-based off-policy learning algorithms, such as GTD and TDC/GQ [13], converge even when using function approximation and incremental updates. However, they have been developed for the case of a fixed behavior policy. In control problems, one would like to adapt the behavior policy over time to become more greedy with respect to the existing value function. In this paper, we present the first gradient-based learning algorithms for this problem, which rely on the framework of policy gradient in order to modify the behavior policy. We present derivations of the algorithms, a convergence theorem, and empirical evidence showing that they compare favorably to existing approaches.", "creator": "LaTeX with hyperref package"}}}