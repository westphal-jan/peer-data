{"id": "1512.04114", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2015", "title": "Building and Measuring Privacy-Preserving Predictive Blacklists", "abstract": "Collaborative approaches to network defense are being increasingly advocated, aiming to proactively predict and speed up detection of attacks. In particular, a lot of attention has recently been given to the problem of predictive blacklisting, i.e., forecasting attack sources based on Intrusion Detection Systems (IDS) alerts contributed by different organizations. While collaboration allows the discovery of groups of correlated attacks targeting similar victims, it also raises important privacy and security challenges, thus motivating privacy-preserving approaches to the problem. Although recent work provides encouraging results on the feasibility of collaborative predictive blacklisting via limited data sharing, a number of open problems remain unaddressed, which this paper sets to address. We introduce a privacy-friendly system for predictive blacklisting featuring a semi-trusted authority that clusters organizations based on the similarity of their logs, without access to these logs. Entities in the same cluster then securely share relevant logs with each other, and build predictive blacklists.", "histories": [["v1", "Sun, 13 Dec 2015 20:05:53 GMT  (234kb,D)", "https://arxiv.org/abs/1512.04114v1", null], ["v2", "Fri, 19 Feb 2016 08:45:45 GMT  (204kb,D)", "http://arxiv.org/abs/1512.04114v2", null], ["v3", "Thu, 9 Jun 2016 08:59:38 GMT  (228kb,D)", "http://arxiv.org/abs/1512.04114v3", null], ["v4", "Wed, 1 Mar 2017 16:08:02 GMT  (151kb,D)", "http://arxiv.org/abs/1512.04114v4", null]], "reviews": [], "SUBJECTS": "cs.CR cs.AI", "authors": ["luca melis", "apostolos pyrgelis", "emiliano de cristofaro"], "accepted": false, "id": "1512.04114"}, "pdf": {"name": "1512.04114.pdf", "metadata": {"source": "CRF", "title": "Building and Measuring Privacy-Preserving Predictive Blacklists", "authors": ["Luca Melis", "Apostolos Pyrgelis", "Emiliano De Cristofaro"], "emails": [], "sections": [{"heading": null, "text": "Unfortunately, the CPB's proposals so far focus only on improving hit numbers, but overlook the impact of collaboration on false positives and false negatives. Moreover, threat information sharing often leads to important privacy, confidentiality and liability issues. In this paper, we first provide a comprehensive measurement analysis of two modern CPB systems: one where a trusted central party collects warnings [Soldo et al., Infocom '10] and a peer-to-peer system based on controlled data exchange [Freudiger et al., DIMVA' 15], examining the impact of collaboration on accurate and false predictions. Then, we present a novel privacy-friendly approach that significantly improves the work done so far and achieves a better balance between true and false data sharing, while minimizing the impact of collaboration on accurate and false predictions."}, {"heading": "1 Introduction", "text": "Because it is impossible to perform expensive classification tasks for each connection, filtering is typically done using periodically updated lists of suspicious hosts - or black lists. However, these can be created locally or obtained from alarmed repositories, such as DShield.org.Zhang et al. [36] are the first to introduce the concept of collaborative blacklisting (CPB), where different companies send their logs to a trusted entity, which in turn provides tailored blacklists based on relevance rankings. Intuition is that attacks are often correlated by the same sources against different networks. In fact, Katti et al. [22] show that the attack correlation persists over time, suggesting that collaboration between victims can be significantly improved."}, {"heading": "1.1 Roadmap", "text": "First, we offer an experimental evaluation of existing collaborative predictive blacklisting (CPB) proposals (Section 4). We use alarms transmitted to DShield.org by 70 organizations that report an average of 4,000 daily events within a 15-day window. We implement and compare the centralized technology of Soldo et al. [33] compared to the peer-topeer method, which relies on controlled data transmission by Freudiger et al. [16] We find that the former achieves high hit rates (almost doubling of correct predictions compared to no collaboration) and relatively high recall rates (58%), but their accuracy is ultimately quite low (F1 = 14%) due to a significant increase in false positives (8% precision). While the latter only marginally improves hit rates compared to no collaboration, it also leads to fewer false predictions, leading to overall better accuracy (up to 29%), we suggest a novel approach to the two (5)."}, {"heading": "1.2 Contributions", "text": "In summary, we present two main contributions: First, we present a measurement study of existing CPB approaches, which aim to capture the overall effects of collaboration, highlighting important outstanding issues; second, we introduce a novel, privacy-friendly, and scalable approach to CPB, which achieves a better balance between hit numbers and false predictions; and third, our system minimizes the amount of information disclosed in the process and achieves scalability in the presence of a large number of cooperating companies."}, {"heading": "2 Related Work", "text": "Katti et al. [22] are among the first to measure correlated attacks (i.e., attacks mounted against different networks by the same sources are found to be very frequent yet highly targeted, showing that the attack correlation persists over time and indicating that collaboration between victims could significantly improve the detection time of malicious IP attacks. In [36], Zhang et al., highly predictive blacklisting practices are introduced, where different organizations contribute alerts to a central repository, such as DShield.org, which in turn provides them with personalized (predictive) blacklists on a daily basis. The prediction uses a relevance ranking scheme similar to PageRank, which measures an attacker's correlation to their history as well as the recent protocol production patterns of the attacker. Then, Soldo et al. [33] improves the use of an implied recommendation system to detect similar victims as well as clusters of correlated victims."}, {"heading": "3 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Background", "text": "Considering a signal over time r (t), we use r (t + 1) to give the predicted value of r (t + 1), taking into account past observations r (t) at a point in time t. \"The predicted signal is calculated as: r\" (t + 1) = t \"(1 \u2212 \u03b1) t\" \u00b7 r \"(t\") (1), where \u03b1 \"(0, 1) is a smoothing coefficient, t.\" t \"} denotes the prediction algorithm c\" and c \"notations notation notation. To evaluate the performance of the predictionP, we use cryptic and false positive values in comparison to NPR (we), NF1 / NPR (we remember the Tsac to evaluate the Tsac results)."}, {"heading": "3.2 Dataset", "text": "With the aim of designing a meaningful empirical analysis of CPB, we collect a dataset with black lists of IP addresses from DShield.org, a collaborative firewall log correlation system to which various organizations provide voluntary daily notifications. Each entry in the logs consists of a pseudonymised post ID (the target), the source IP address (the attacker), the source and destination port number, and the timestamp. With DShield's permission, we have collected protocols that use a JavaScript web crawler, collecting an average of 10 million logs per day. We exclude entries for untitles for invalid or non-routine IP addresses, and discard port numbers, then we extract its / 24 subnet for each IP address and use / 24 addresses for all experiments, following experimental decisions we use both the test sets for invalid or non-routine IP addresses, and then we extract its / 24 addresses for each IP address as training sets and use the school sets as training sets as well. We also use the school sets as the school sets, [16]"}, {"heading": "4 Existing Collaborative Predictive Blacklisting Approaches", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Soldo et al. [33]\u2019s Implicit Recommendation (No Privacy)", "text": "We first evaluate Soldo et al. [33] \"s CPB approach based on implicit recommendations. We do so with a twofold goal: (1) to evaluate false positives and false negatives that have not been taken into account in [33], and (2) to compare them against privacy-friendly approaches that will be presented later. In essence, Soldo et al.\" s work builds on [36], which is based on a relevance ranking scheme that resembles PageRank and the correlation of an attacker with a contributor based on their history as well as the recent protocol production patterns of the attacker. Soldo et al. Improvement [36] by using an implicit recommendation system to detect similar victims as well as groups of correlated victims and attackers. The presence of attacks carried out at the same time leads to greater victim similarity, and a neighborhood model (kNN) is applied to similar victims. Cross Association (CA) Co-Clustering [7] is used to identify victims and attackers within groups, and correlate victims and attackers."}, {"heading": "4.2 Freudiger et al. [16]\u2019s Peer-to-Peer Controlled Data Sharing", "text": "Next, we evaluate the system of Freudiger et al. [16], where organizations interact in pairs and aim to estimate the benefits of collaboration privately and then share data with companies that are likely to bring the most benefit. [11] The authors also use DShield data and make predictions using EWMA. They find that (1) the number of common attacks is the best predictive value that can be estimated privately, using Private Set Intersection Cardinality (PSICA) [11] and (2) sharing only the intersection of attacks - which can be done privately with Private Set Intersection (PSI) [12] - is almost as beneficial as sharing of all kinds. Their goal is to evaluate benefit ratings / sharing strategies rather than focusing on deployment. They start from a network of 100 organizations, select the \"Top 50\" from all possible 4950 pairs (in terms of estimated benefits), and just experiment with it."}, {"heading": "5 A Novel Approach to PrivacyFriendly CPB", "text": "In this section, we present a novel privacy-friendly system based on a reasonably trusted authority, the STA, which acts as a coordinator to facilitate clustering without having access to the raw data. In other words, the STA bundles contributors based on the similarity of their protocols (without access to these protocols) and helps organizations in the same cluster share relevant protocols."}, {"heading": "5.1 Overview", "text": "Our system consists of four steps: (1) First, organizations interact in pairs to privately calculate a similarity measurement of their protocols based on the number of common attacks. Then, (2) the STA captures the similarity measurements of each organization to a matrix presented as O2O and then clusters. Next, the STA reports to each organization the identifiers of other organizations in the same cluster (if any) so that they share protocols jointly but privately to increase the accuracy of their predictions by exchanging either common attacks (intersections), correlated attacks (IP2IP) or mutual attacks (if any)."}, {"heading": "5.2 Results", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "5.3 Discussion", "text": "In fact, most of them will be able to play by the rules that they have adopted in recent years."}, {"heading": "5.4 Comparison to Soldo et al. and Freudiger et al.", "text": "We observe that [33] achieves a higher maximum of TPimpr (0.95) than our hybrid approach (0.61 with k averages, k = 1). However, our privacy preservation techniques perform better in terms of both recall (TPPR) and precision (PPV). For example, our hybrid model with k-NN achieves a TPR of 0.77 (compared to 0.66 for [33]). Likewise, k = 5 reaches the PPV of our system 0.19, whereas [33] the best precision is 0.08. As a result, our hybrid model delivers greater (up to 2 x) F1 values (e.g. 0.3 with k-NN, k = 15) than [33] (0.14). Furthermore, our novel hybrid approach delivers better results than [16] - (A) in terms of TPimpr and TPR."}, {"heading": "6 Implementing At Scale", "text": "As discussed above, there are four steps: (1) secure calculation of pairwise similarity, (2) clustering, (3) secure data transfer within clusters, and (4) time series prediction. (4) To assess their scalability, we need to evaluate the computational and communication complexities that arise from each step. (1) and (3) it has complex relationships, requiring a number of cryptographic protocols that depend on the number of organizations involved. (4) Clustering is actually a negligible overhead: depending on hardware to perform clustering with 1,000 organizations, it needs 6.1ms for agglomerative means, and 5.2ms for the number of organizations involved. (k = 2) Time series EWMA prediction also requires 4.6\u00b5s per IP, so it needs 4.6ms for 1,000 IPs."}, {"heading": "7 Conclusion", "text": "In this paper, we first presented the results of a comprehensive measurement study on collaborative predictive blacklisting techniques (CPB), in particular one based on trusted central parties from Soldo et al. [33] and another based on private data sharing from Freudiger et al. [16], and then introduced a novel, hybrid approach that improves on the first two. Our experiments evaluated correct and incorrect predictions, as well as the effects of collaboration on the real world (e.g. improving on real positives and increasing the number of false positives / negatives). We found that access to more (attack) protocols does not necessarily lead to better predictions overall - in fact, the approach proposed by Soldo et al. [33], although considered to be the state of the art in CPB, leads to high hit rates (almost doubling the number of correct predictions), but also with very poor accuracy due to very high error rates in the private sphere [2], which speaks to the improvement in our system (433)."}, {"heading": "A Cryptography Background", "text": "In fact, most of them are unable to abide by the rules they have imposed on themselves."}, {"heading": "B Clustering Algorithms", "text": "Agglomerative clustering. Hierarchical clustering algorithms form nested clusters by merging or splitting the individual samples one by one. Hierarchy is represented as a tree, with the root being the unique cluster that collects all samples and leaves the clusters with only one sample. Agglomerative clustering performs hierarchical clustering using a bottomup approach: each observation begins in its own cluster, and clusters are fused successively. k-means. k-means clustering determines the actual metric used to merge, for example, the average linkage minimizes the average of distances between all observations of cluster pairs, while the full linkage minimizes the maximum distance between observations of cluster pairs. k-means clustering separates samples into groups of equal variance, minimizes inertia or within the cluster sum of squares."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Collaborative security initiatives are increasingly often ad-<lb>vocated to improve timeliness and effectiveness of threat mit-<lb>igation. Among these, collaborative predictive blacklisting<lb>(CPB) aims to forecast attack sources based on alerts con-<lb>tributed by multiple organizations that might be targeted in<lb>similar ways. Alas, CPB proposals thus far have only fo-<lb>cused on improving hit counts, but overlooked the impact of<lb>collaboration on false positives and false negatives. More-<lb>over, sharing threat intelligence often prompts important pri-<lb>vacy, confidentiality, and liability issues. In this paper, we<lb>first provide a comprehensive measurement analysis of two<lb>state-of-the-art CPB systems: one that uses a trusted cen-<lb>tral party to collect alerts [Soldo et al., Infocom\u201910] and a<lb>peer-to-peer one relying on controlled data sharing [Freudi-<lb>ger et al., DIMVA\u201915], studying the impact of collaboration<lb>on both correct and incorrect predictions. Then, we present<lb>a novel privacy-friendly approach that significantly improves<lb>over previous work, achieving a better balance of true and<lb>false positive rates, while minimizing information disclosure.<lb>Finally, we present an extension that allows our system to<lb>scale to very large numbers of organizations.", "creator": "LaTeX with hyperref package"}}}