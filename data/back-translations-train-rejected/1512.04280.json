{"id": "1512.04280", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2015", "title": "Small-footprint Deep Neural Networks with Highway Connections for Speech Recognition", "abstract": "For speech recognition, deep neural network (DNN) have significantly improved the recognition accuracy in most of benchmark datasets and application domains. However, compared to the conventional Gaussian mixture models(GMMs), DNN-based acoustic models usually have much larger number of model parameters, making it challenging for their applications in resource constrained platforms, e.g. mobile devices. In this paper, we study the application of the recently proposed highway network to train small-footprint DNNs, which are thinner and deeper and have significantly smaller number of model parameters compared to conventional DNNs. We investigated this approach on the AMI meeting speech transcription corpus which has around 70 hours of audio data. The highway neural networks constantly outperformed their plain DNN counterparts, and the number of model parameters can be reduced significantly without sacrificing the recognition accuracy.", "histories": [["v1", "Mon, 14 Dec 2015 12:29:32 GMT  (86kb,D)", "http://arxiv.org/abs/1512.04280v1", "5 pages, 2 figures"], ["v2", "Thu, 3 Mar 2016 12:14:06 GMT  (110kb,D)", "http://arxiv.org/abs/1512.04280v2", "5 pages, 3 figures"], ["v3", "Mon, 20 Jun 2016 10:30:54 GMT  (110kb,D)", "http://arxiv.org/abs/1512.04280v3", "5 pages, 3 figures, accepted by Interspeech 2016"], ["v4", "Wed, 14 Jun 2017 15:17:27 GMT  (110kb,D)", "http://arxiv.org/abs/1512.04280v4", "5 pages, 3 figures, fixed typo, accepted by Interspeech 2016"]], "COMMENTS": "5 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["liang lu", "steve renals"], "accepted": false, "id": "1512.04280"}, "pdf": {"name": "1512.04280.pdf", "metadata": {"source": "CRF", "title": "Small-footprint Deep Neural Networks with Highway Connections for Speech Recognition", "authors": ["Liang Lu", "Steve Renals"], "emails": ["s.renals}@ed.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people are able to decide whether they will be able to play by the rules or whether they will be able to play by the rules."}, {"heading": "2. Highway Deep Neural Network", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Deep neural networks", "text": "A DNN is a forward-facing neural network with multiple hidden layers that performs cascaded, layer-by-layer nonlinear transformations of the input. For a network with hidden layers of L, the model ash1 = f (x, \u03b81) (1) hl = f (hl \u2212 1, \u03b8l), for l = 2,.., L (2) y = g (hL or tanh) (3) can be represented, where x is an input vector to the network; f (hl \u2212 1, \u03b8l) denotes the transformation of the input hl \u2212 1 with the parameter successl followed by a nonlinear activation function (e.g. sigmoid or tanh); g (\u00b7, \u0439) is the output function (e.g. softmax) parameterized by \u0456 in the output layer. Given the objective of soil truth, the network is usually trained by gradients to minimize a loss function L (y) (e.g. transverse drop)."}, {"heading": "2.2. Highway connections for deep networks", "text": "There have been numerous studies to overcome the difficulties of forming very deep neural networks, including pre-training [17, 18], normalized initialization [22], deeply monitored networks [23], etc. Recently, Srivastav and al. [19] proposed the motorway network and showed the impressive results of using this approach to train very deep networks, e.g. up to 100 hidden layers. In the motorway network, the hidden layers are enhanced by two gating functions, which are ashl = f (hl \u2212 1, \u03b8l) T (hl \u2212 1, WT) + hl \u2212 C (hl \u2212 1, Wc) (4), where T (\u00b7) is the transformation gate that scales the original hidden activations; C (\u00b7) is the carry gate that scales the input before it is passed directly to the next hidden layer; it designates elementary (Hadamad) the product; the outputs of T () and C () are limited."}, {"heading": "2.3. Small-footprint networks", "text": "Equation (4) shows that the motorway network is not directly suitable for this purpose, since it introduces additional computational costs and model parameters into the two gating functions, on the grounds that the computational complexity and number of model parameters for each layer in a tightly connected network are in the order of O (n2), where n is the size of hidden units. Increasing the depth of the network only increases the computational costs and model size linearly, while decreasing the width leads to a square reduction. Motorway connections make it possible to train very thin and deep networks, and therefore the overall model sizes are much smaller. In order to save the model parameters further, in this work we shared the two gates for all hidden layers, so that the additional number of model parameters for T (\u00b7) and C (\u00b7) is relatively small."}, {"heading": "3. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. System setup", "text": "The amount of training data is about 70 hours, which corresponds to about 28 million frames. This data set is much larger than most of the data sets (e.g. MNIST, CIFAR, etc.) in which other types of thin and deep networks were evaluated [10, 19]. We used 40-dimensional fMLLR-adapted feature vectors that were normalized at speaker level, which were then connected through a context window of 15 frames (i.e. \u00b1 7) for all systems. The number of bound HMM states is 3972, and all DNN systems were trained with the same orientation. The results reported in this paper were obtained with the CNTK toolkit [27] with the Kaldi decoder [28], and the networks were trained using the cross-entropy criterion (CE), with no predefined parameters. We set the dynamic to 0.9 after the first layers were activated, and the hidden 0.5 layers for the distribution."}, {"heading": "3.2. Depth vs. Width", "text": "Table 1 shows the word error rate (WERs) of simple DNNs and highway networks (HDNNs) with different configurations. As the number of hidden units decreases, the accuracy of simple DNNs decreases rapidly, and the loss of accuracy cannot be compensated by an increase in network depth. We found it difficult to train thin and deep networks directly without RBM pre-training, where the CE loss has not decreased at all after many eras. However, we did not have this difficulty with highway connections. HDNNs achieved consistently lower WERs compared to pure DNN counterparts, and the profit margin increases even as the number of hidden units decreases, as shown in Figure 1. With highway connections, we can reduce the number of model parameters by about 80%, with slight loss of accuracy and with less than 1 million model parameters, the CE-trained HDNN can achieve a comparable or slightly higher accuracy of school lines compared to GMAT based models (MAT)."}, {"heading": "3.3. Transform gate vs. Carry gate", "text": "The results are shown in Table 2, where we disabled one of the two gates. We observed that the HDNN with only one of the two gates can still achieve a lower WER compared to the normal DNN, but the best results were achieved with both gates, suggesting that the two gates complement each other. Figure 2 shows the convergence curve of the formation of HDNN with and without transformation and carry gates. We observed that it converged quickly when both gates were switched on, and with only the Transform gate the convergence rate was much slower. As previously discussed, the carry gate can be regarded as a certain type of skip connection, and it was observed that it delivers a much faster convergence rate compared to the Transform gate."}, {"heading": "3.4. Constrained carry gate", "text": "This approach reduces computing costs as matrix vector multiplication is not required for the carry gate. We evaluated this configuration with 10-layer neural networks, and the results are in Table 3. Contrary to our expectations, we achieved worse results with the confined carry gate when the networks were relatively wide, while the accuracy gap was reduced when the number of hidden units was smaller. This could be because the Transform Gate T (\u00b7) learns the scaling function for both the input and output simultaneously in the confined environment. As regulation of the training of wide and deep networks may be more important, this cannot be achieved by using a single gating function. For example, both the input and output of a hidden layer may require larger or smaller scaling weights, which is impossible in the constrained setting."}, {"heading": "4. Conclusions", "text": "Our study builds on the recently proposed neural highway network, which introduces an additional transformation and carry gate for each hidden layer. Our experiments suggest that highway connections can facilitate the flow of information and reduce the difficulty of forming very deep feedback networks; the thin and deep architecture with highway junctions consistently achieved lower VERs compared to simple DNNs; and by reducing the number of hidden units, we can significantly reduce the total number of model parameters with negligible loss of accuracy; we have also evaluated the specific role of the transform and carry gate, and the carry gate was more important to accelerate the convergence rate in our experiment; and the small-scale highway networks could be further improved by training teachers and students, which will be investigated in our future work."}, {"heading": "5. References", "text": "[1] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Li, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury, 2012. [2] G. Dahl, D. Yu, L. Deng, and A. Acero, \"Contextdependent pre-trained deep neural networks for largevocabulary speech recognition,\" IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 30-42, 2012. [3] H. A. Bourlard and N. Morgan, Connectionist speech recognition: a hybrid approach."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Contextdependent pre-trained deep neural networks for largevocabulary speech recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Connectionist speech recognition: a hybrid approach", "author": ["H.A. Bourlard", "N. Morgan"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Neural networks for statistical recognition of continuous speech", "author": ["N. Morgan", "H.A. Bourlard"], "venue": "Proceedings of the IEEE, vol. 83, no. 5, pp. 742\u2013772, 1995.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Connectionist probability estimators in HMM speech recognition", "author": ["S. Renals", "N. Morgan", "H. Bourlard", "M. Cohen", "H. Franco"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 2, no. 1, pp. 161\u2013174, 1994.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition.", "author": ["J. Xue", "J. Li", "Y. Gong"], "venue": "in Proc. INTERSPEECH,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "Proc. ICASSP. IEEE, 2013, pp. 6655\u20136659.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning smallsize dnn with output-distribution-based criteria", "author": ["J. Li", "R. Zhao", "J.-T. Huang", "Y. Gong"], "venue": "Proc. INTERSPEECH, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Do deep nets really need to be deep?", "author": ["J. Ba", "R. Caruana"], "venue": "in Proc. NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Fitnets: Hints for thin deep nets", "author": ["R. Adriana", "B. Nicolas", "K. Samira Ebrahimi", "C. Antoine", "G. Carlo", "B. Yoshua"], "venue": "Proc. ICLR, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Model compression", "author": ["C. Bucilu\u01ce", "R. Caruana", "A. Niculescu-Mizil"], "venue": "Proc. ACM SIGKDD, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "Proc. NIPS Deep Learning and Representation Learning Workshop, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Structured transforms for small-footprint deep learning", "author": ["V. Sindhwani", "T.N. Sainath", "S. Kumar"], "venue": "Proc. NIPS, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Fastfood-approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarl\u00f3s", "A. Smola"], "venue": "Proc. ICML, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep fried convnets", "author": ["Z. Yang", "M. Moczulski", "M. Denil", "N. de Freitas", "A. Smola", "L. Song", "Z. Wang"], "venue": "Proc. ICCV, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "ACDC: A Structured Efficient Linear Layer", "author": ["M. Moczulski", "M. Denil", "J. Appleyard", "N. de Freitas"], "venue": "arXiv preprint arXiv:1511.05946, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Proc. NIPS, vol. 19, 2007, p. 153.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "Proc. NIPS, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition", "author": ["Y. Zhang", "G. Chen", "D. Yu", "K. Yao", "S. Khudanpur", "J. Glass"], "venue": "arXiv preprint arXiv:1510.08983, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["D. Erhan", "P.-A. Manzagol", "Y. Bengio", "S. Bengio", "P. Vincent"], "venue": "International Conference on artificial intelligence and statistics, 2009, pp. 153\u2013160.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, 2010, pp. 249\u2013256.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Deeply-supervised nets", "author": ["C.-Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "arXiv preprint arXiv:1409.5185, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1929}, {"title": "Recognition and understanding of meetings the AMI and AMIDA projects", "author": ["S. Renals", "T. Hain", "H. Bourlard"], "venue": "Proc. ASRU. IEEE, 2007, pp. 238\u2013247.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["D. Yu", "A. Eversole", "M. Seltzer", "K. Yao", "Z. Huang", "B. Guenter", "O. Kuchaiev", "Y. Zhang", "F. Seide", "H. Wang"], "venue": "Tech. Rep. MSR, Microsoft Research, Tech. Rep., 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motl\u0131cek", "Y. Qian", "P. Schwarz", "J. Silovsk\u00fd", "G. Semmer", "K. Vesel\u00fd"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Modern state-of-the-art speech recognition systems are based on the neural network acoustic models [1, 2, 3, 4, 5].", "startOffset": 99, "endOffset": 114}, {"referenceID": 1, "context": "Modern state-of-the-art speech recognition systems are based on the neural network acoustic models [1, 2, 3, 4, 5].", "startOffset": 99, "endOffset": 114}, {"referenceID": 2, "context": "Modern state-of-the-art speech recognition systems are based on the neural network acoustic models [1, 2, 3, 4, 5].", "startOffset": 99, "endOffset": 114}, {"referenceID": 3, "context": "Modern state-of-the-art speech recognition systems are based on the neural network acoustic models [1, 2, 3, 4, 5].", "startOffset": 99, "endOffset": 114}, {"referenceID": 4, "context": "Modern state-of-the-art speech recognition systems are based on the neural network acoustic models [1, 2, 3, 4, 5].", "startOffset": 99, "endOffset": 114}, {"referenceID": 0, "context": "A typical architecture is the deep neural network (DNN) [1, 2], which is a feedforward neural network with multiple hidden layers (e.", "startOffset": 56, "endOffset": 62}, {"referenceID": 1, "context": "A typical architecture is the deep neural network (DNN) [1, 2], which is a feedforward neural network with multiple hidden layers (e.", "startOffset": 56, "endOffset": 62}, {"referenceID": 5, "context": "[6] and Sainath et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] approximate the weight matrix between two hidden layers by a product of two low-rank matrices, which may be equivalent to insert a bottleneck layer in between without the nonlinear activation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Another branch of studies are based on the teacher-student architecture [8, 9, 10], which is also refereed to as model compression [11] or knowledge distillation [12].", "startOffset": 72, "endOffset": 82}, {"referenceID": 8, "context": "Another branch of studies are based on the teacher-student architecture [8, 9, 10], which is also refereed to as model compression [11] or knowledge distillation [12].", "startOffset": 72, "endOffset": 82}, {"referenceID": 9, "context": "Another branch of studies are based on the teacher-student architecture [8, 9, 10], which is also refereed to as model compression [11] or knowledge distillation [12].", "startOffset": 72, "endOffset": 82}, {"referenceID": 10, "context": "Another branch of studies are based on the teacher-student architecture [8, 9, 10], which is also refereed to as model compression [11] or knowledge distillation [12].", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "Another branch of studies are based on the teacher-student architecture [8, 9, 10], which is also refereed to as model compression [11] or knowledge distillation [12].", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "As discussed in [12], the soft targets provided by the teacher encode the generalisation power of the teacher, and the student model trained using these labels is observed to perform better than a small model trained in the usual way [8, 9, 10].", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "As discussed in [12], the soft targets provided by the teacher encode the generalisation power of the teacher, and the student model trained using these labels is observed to perform better than a small model trained in the usual way [8, 9, 10].", "startOffset": 234, "endOffset": 244}, {"referenceID": 8, "context": "As discussed in [12], the soft targets provided by the teacher encode the generalisation power of the teacher, and the student model trained using these labels is observed to perform better than a small model trained in the usual way [8, 9, 10].", "startOffset": 234, "endOffset": 244}, {"referenceID": 9, "context": "As discussed in [12], the soft targets provided by the teacher encode the generalisation power of the teacher, and the student model trained using these labels is observed to perform better than a small model trained in the usual way [8, 9, 10].", "startOffset": 234, "endOffset": 244}, {"referenceID": 12, "context": "Recently, [13] investigated the use of low rank displacement of structured matrices (e.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "This work is in line with the argument that the neural networks with dense connections are overparameterised, and the linear layer may be replaced by structured efficient linear layers (SELLs) [14, 15, 16].", "startOffset": 193, "endOffset": 205}, {"referenceID": 14, "context": "This work is in line with the argument that the neural networks with dense connections are overparameterised, and the linear layer may be replaced by structured efficient linear layers (SELLs) [14, 15, 16].", "startOffset": 193, "endOffset": 205}, {"referenceID": 15, "context": "This work is in line with the argument that the neural networks with dense connections are overparameterised, and the linear layer may be replaced by structured efficient linear layers (SELLs) [14, 15, 16].", "startOffset": 193, "endOffset": 205}, {"referenceID": 16, "context": "One approach is to pre-train the neural network by unsupervised [17] or greedy layer-wise fashion [18].", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "One approach is to pre-train the neural network by unsupervised [17] or greedy layer-wise fashion [18].", "startOffset": 98, "endOffset": 102}, {"referenceID": 9, "context": "the FitNet [10], but it requires the additional effort to train the teacher model beforehand.", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": "Our work in this paper builds on the recently proposed highway networks [19], where the transform gate is used to scale the output of a hidden layer and the carry gate is used to pass through the input directly after elementwise rescaling.", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "Similar idea also has been tried on long short-term memory recurrent neural networks (LSTM-RNN) for speech recognition [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "However, as the number of hidden layers increases, the error surface becomes increasingly non-convex, and it is more possible to find a poor local minima using gradient-based optimisation algorithms with random initialisation [21].", "startOffset": 226, "endOffset": 230}, {"referenceID": 21, "context": "Furthermore, [22] showed that the variance of the back-propagated gradients may become small in the lower layers if the model parameters are not initialised properly.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "There have been numerous studies on overcoming the difficulties in training very deep neural networks, including pre-training [17, 18], normalised initialisation [22], deeply-supervised networks [23], etc.", "startOffset": 126, "endOffset": 134}, {"referenceID": 17, "context": "There have been numerous studies on overcoming the difficulties in training very deep neural networks, including pre-training [17, 18], normalised initialisation [22], deeply-supervised networks [23], etc.", "startOffset": 126, "endOffset": 134}, {"referenceID": 21, "context": "There have been numerous studies on overcoming the difficulties in training very deep neural networks, including pre-training [17, 18], normalised initialisation [22], deeply-supervised networks [23], etc.", "startOffset": 162, "endOffset": 166}, {"referenceID": 22, "context": "There have been numerous studies on overcoming the difficulties in training very deep neural networks, including pre-training [17, 18], normalised initialisation [22], deeply-supervised networks [23], etc.", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "[19] proposed the highway network and demonstrated the impressive results of using this approach to train very deep networks, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "where T (\u00b7) is the transform gate that scales the original hidden activations; C(\u00b7) is the carry gate, which scales the input before passing it directly to the next hidden layer; \u25e6 denotes elementwise (Hadamad) product; The outputs of T (\u00b7) and C(\u00b7) are constrained to be [0, 1], and we use sigmoid functions for both gates parameterised by WT and Wc respectively.", "startOffset": 272, "endOffset": 278}, {"referenceID": 18, "context": "Unlike [19], in this work, we do not use any bias vector in the two gate functions.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "In [19], the carry gate is constrained to be C(\u00b7) = 1\u2212T (\u00b7), while in this work, we evaluate the generalisation ability of highway networks with and without this constraint.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "It looks similar to the dropout regularisation for neural networks [24], which may be represented as", "startOffset": 67, "endOffset": 71}, {"referenceID": 23, "context": "where p( i) is a Bernoulli distribution for each element in as originally proposed in [24], while it was shown later that using a continuous distribution with well designed mean and variance works as well or better [25].", "startOffset": 86, "endOffset": 90}, {"referenceID": 24, "context": "where p( i) is a Bernoulli distribution for each element in as originally proposed in [24], while it was shown later that using a continuous distribution with well designed mean and variance works as well or better [25].", "startOffset": 215, "endOffset": 219}, {"referenceID": 25, "context": "Our experiments were performed on the AMI meeting speech transcription dataset [26].", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": ") where other types of thin and deep networks were evaluated [10, 19].", "startOffset": 61, "endOffset": 69}, {"referenceID": 18, "context": ") where other types of thin and deep networks were evaluated [10, 19].", "startOffset": 61, "endOffset": 69}, {"referenceID": 26, "context": "The results reported in this paper were obtained using the CNTK toolkit [27] with the Kaldi decoder [28], and the networks were trained using the cross entropy (CE) criterion without pre-training unless specified otherwise.", "startOffset": 72, "endOffset": 76}, {"referenceID": 27, "context": "The results reported in this paper were obtained using the CNTK toolkit [27] with the Kaldi decoder [28], and the networks were trained using the cross entropy (CE) criterion without pre-training unless specified otherwise.", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "We also evaluated using the constrained carry gate in our experiments, where C(\u00b7) = 1 \u2212 T (\u00b7) as studied in [19].", "startOffset": 108, "endOffset": 112}], "year": 2017, "abstractText": "For speech recognition, deep neural network (DNN) have significantly improved the recognition accuracy in most of benchmark datasets and application domains. However, compared to the conventional Gaussian mixture models(GMMs), DNN-based acoustic models usually have much larger number of model parameters, making it challenging for their applications in resource constrained platforms, e.g. mobile devices. In this paper, we study the application of the recently proposed highway network to train small-footprint DNNs, which are thinner and deeper and have significantly smaller number of model parameters compared to conventional DNNs. We investigated this approach on the AMI meeting speech transcription corpus which has around 70 hours of audio data. The highway neural networks constantly outperformed their plain DNN counterparts, and the number of model parameters can be reduced significantly without sacrificing the recognition accuracy.", "creator": "LaTeX with hyperref package"}}}