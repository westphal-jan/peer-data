{"id": "1512.04455", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2015", "title": "Memory-based control with recurrent neural networks", "abstract": "Partially observed control problems are a challenging aspect of reinforcement learning. We extend two related, model-free algorithms for continuous control -- deterministic policy gradient and stochastic value gradient -- to solve partially observed domains using recurrent neural networks trained with backpropagation through time.", "histories": [["v1", "Mon, 14 Dec 2015 18:44:48 GMT  (494kb,D)", "http://arxiv.org/abs/1512.04455v1", "NIPS Deep Reinforcement Learning Workshop 2015"]], "COMMENTS": "NIPS Deep Reinforcement Learning Workshop 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nicolas heess", "jonathan j hunt", "timothy p lillicrap", "david silver"], "accepted": false, "id": "1512.04455"}, "pdf": {"name": "1512.04455.pdf", "metadata": {"source": "CRF", "title": "Memory-based control with recurrent neural networks", "authors": ["Nicolas Heess", "Jonathan J Hunt", "Timothy P Lillicrap", "David Silver"], "emails": [], "sections": [{"heading": null, "text": "Partially observed control problems are a challenging aspect of reinforcement learning. We are expanding two interconnected, model-free continuous control algorithms - deterministic policy gradients and stochastic value gradients - to solve partially observed domains using recurrent neural networks that are trained with back propagation over time. We show that this approach, coupled with long-term memory, is capable of solving a variety of physical control problems that have different memory requirements, including short-term integration of information from noisy sensors and identification of system parameters, as well as long-term memory problems that require the preservation of information over many time steps. We also show success with a combined exploration and memory problem in the form of a simplified version of the well-known Morris Water Labyrinth task. Finally, we show that our approach can deal with high-dimensional observations by learning directly from pixels."}, {"heading": "1 Introduction", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in"}, {"heading": "2 Background", "text": "We model our environment as discrete time, partly observing Markov decision-making processes (POMDP). A POMDP describes a set of environmental states S and a set of measures A, an initial state distribution p0 (s0), a transitional function p (st + 1 | st, at) and a reward function r (st, at). This underlying MDP is partially observed when the actor is unable to observe the state st directly and instead receives observations from the sentence O tied to the underlying state p (ot | st). The actor observes the underlying state of the MDP only indirectly through the observations. An optimal actor can in principle rely on access to the entire story = (o1, a1, o2, a2, o2, ot). Therefore, the goal of the actor is to learn (ht) a policy that maximizes maps from history to a distribution through actions P (A)."}, {"heading": "3 Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Recurrent DPG", "text": "We are expanding the Deterministic Policy Gradient (DPG) algorithm for MDPs introduced in [24] to deal with partially observed domains and pixels. Therefore, the basic idea of the DPG algorithm for the fully observed case is that for a deterministic policy using strategies using strategies using strategies using strategies using strategies using strategies using strategies using strategies using strategies using strategies using strategies using strategies abusive, and allowing access to the true action value function in the context of current policy Q\u00b5, policy can be effectively updated through backpropagation. (3), where the expectation regarding the (discounted) state visitation distribution is misunderstood. [24] Similar ideas have previously been exploited in NFQCA [4] and in the ADP [13] community. In practice, the exact action value function of QMP is replaced by the action value function."}, {"heading": "3.1.1 Off-policy learning and experience replay", "text": "DPG is typically used in a non-political environment, because politics is deterministic, but exploration is needed to learn Q's gradient in relation to actions (see algorithms 1, 2). In addition, data efficiency and stability can be greatly improved in practice by using experience reproduction (e.g. [4, 5, 14, 16, 6]), and we use the same approach here (see algorithms 1, 2). Thus, during learning, we store experienced trajectories in a database and then replace expectation in an equivalent. (4) With functional approximation, this can lead to a distortion in the trajectories sampled from the databases. One consequence is a distortion in the state distribution in an equivalent. (3, 5), which no longer corresponds to the state distribution induced by current policies. With functional approximation, this can lead to a distortion in the learned policies, although this is typically ignored in practice. RDPG and SVG (0) can no longer be observed in the state's action context, but only in the policy as a whole (Q)."}, {"heading": "3.1.2 Target networks", "text": "A second algorithmic function that has significantly improved the stability of neural amplification algorithms that rely on bootstrapping for learning value functions is the use of target networks [4, 14, 16, 6]: The algorithm manages two copies of the value function Q and the policy summary, each with the parameters \u03b8 and \u03b8, and 270 and 270 are the parameters updated by the algorithm. \u2212 In this work, we use \"soft updates\" as in [14] (see algorithms 1 and 2 below). \u2212 Algorithm 1 RDPG algorithms initiate the criterion values for the Q function update and use the \"target values\" for the Q function update. \u2212 In this work, we use \"soft updates\" as in [14] (see algorithms 1 and 2 below). \u2212 Algorithm 1 RDPG algorithm initiates the critical network QII (at, ht) and actualization (we use network parameters) as in this case. \u2212 In this case, we use network algorithms \u2212 in this case."}, {"heading": "4 Results", "text": "We tested our algorithms on a variety of partially observed environments, covering different types of storage problems. Videos of the learned guidelines for all areas are included in our supplementary videos2, we recommend watching them as they can provide a better intuition for the environment. All physical control problems except the simulated water maze (Section 4.3) were simulated in MuJoCo [28]. We tested both standard recurring networks and LSTM networks."}, {"heading": "4.1 Sensor integration and system identification", "text": "Physical control problems with noisy sensors are one of the paradigm examples for partially observed environments. A large amount of research has focused on how to efficiently integrate noisy sensory information over multiple time levels to derive accurate estimates of the system state, or to estimate derivatives of important characteristics of the system. Here, we are looking at two simple, standard control problems commonly used in affirming learning, the under-actuated pendulum and the upward cart rod vibration. We modify these standard measurements so that the agent does not receive direct information about the velocity of one of the components, i.e. for the pendulum oscillation task, which observes only the boundary of the pendulum and the cart rod vibration. and2Video of all learned policies is available at https: be / youtu.be / V4 _ vb1NQfor the velocity of one of the components, i.e. for the pendulum oscillation task, which covers only the boundary of the pendulum oscillation task up to the level of the pendulum task, which includes the level of observation and the policy of the level of the pendulum observation only."}, {"heading": "4.2 Memory tasks", "text": "Another type of partially observed task, which has been less studied in the context of reinforcement learning, involves the need to memorize explicit information over several steps. We constructed two tasks like this: One consisted of a 3-hinged Reacher that must reach for a randomly positioned target, but the location of the target is not made available to the agent until the initial observation (the entire episode lasts for 80 periods). As a harder variant of this task, we constructed a 5-hinged gripper that must reach for a (fully observed) payload from a randomized initial configuration and then return the payload to the starting position of its \"hand\" (T = 100). Note that even in the fully observed case, this is a challenging control problem. Results for both tasks are shown in Figure 2, RDPG agents with LSTM networks reliably perform both tasks, while purely forward-facing agents fail at the memory components of the task, as can be seen in the supplementary video."}, {"heading": "4.3 Water maze", "text": "We tested our algorithms on a simplified version of the task. The agent moves in a two-dimensional circular space in which a small area of space is an invisible \"platform\" on which the agent receives a positive reward. At the beginning of the episode, the agent and platform are randomly positioned in the tank. The platform position is not visible to the agent, but he \"sees\" when he is on the platform. The agent must search for the platform's position and stay on it in order to receive a reward by controlling its acceleration. After 5 steps on the platform, the agent is randomly reset to a new position in the tank, but the platform remains in place for the rest of the episode (T = 200). The agent must remember the platform's position in order to quickly return there. It is sometimes assumed that a stochastic policy is needed to solve problems like this one that require learning a search strategy. Although there are some variables in the results, we found that both RPG and RPG platform were quick to find solutions (RPG-RPG)."}, {"heading": "4.4 High-dimensional observations", "text": "We also tested our agents, using convoluted networks, in solving tasks directly from high-dimensional pixel spaces. We tested the pendulum task (but now the agent is only given a static representation of the pendulum in each timeframe) and a two-selection task where the target disappears after 5 frames (and the agent is not allowed to move during the first 5 frames to prevent it encoding the target position in its original trajectory). We found that RDPG was able to learn effective strategies from high-dimensional observations that integrate information from multiple timeframes to estimate the speed and memorize the visually pending target for the entire length of the episode (in the assignment). Figure 4 shows the results."}, {"heading": "5 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Variants", "text": "In the experiments presented here, actor and critic networks are completely separate, but especially when learning deep, revolutionary networks, the filters required in the early layers can be similar between the policy and the actor. Sharing these early layers could improve computing power and learning speed. Similar arguments apply to the recurring part of the network that could be shared between actor and critic, but this sharing can also lead to instabilities, as updates to one network can unwittingly damage or shift the other network. Therefore, we have not shared here, although this is a potential topic for further investigation."}, {"heading": "5.2 Related work", "text": "Several groups [15, 1, 5] have investigated the use of model-free algorithms with recurring networks to solve POMDPs with discrete action spaces. [1] They focused on relatively long-term (\"deep\") memory problems in small state action spaces. In contrast, [5] they modified the Atari DQN architecture [16] (i.e. they perform the control of high-dimensional pixel inputs) and showed that recurring Q-learning processes can perform the information integration required to solve short-term partial observation (e.g. to estimate speeds) achieved by stacks of frames in the original DQN architecture. Continuous action problems with relatively low observation spaces have been identified e.g. in [30, 31, 29, 32]."}, {"heading": "6 Conclusion", "text": "We have shown that two related model-free approaches can be expanded to effectively learn with relapsing neural networks on a variety of partially observed problems, including directly from pixel observations. As these algorithms learn over time using standard back propagation, we can benefit from innovations in monitored relapsing neural networks such as long-term short-term memory networks [7] to solve challenging memory problems such as the Morris water maze."}, {"heading": "7 Supplementary", "text": "algorithm 2 RSVG (0) algorithm 2 RSVG (0) algorithm 2 RSVG (1) algorithm 2 RSVG (1) algorithm 2 RSVG (1) algorithm 2 RSVG (1) algorithm 3 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (1) algorithm 4 (4) algorithm 4 (4) (4) 4) 4 (4) 4 (4) 4) 4 (4) 4 (4) 4) 4 (4) 4 (4) 4) 4 (4) 4 (4) 4) 4 (4) 4 (4) 4) 4 (4) 4 (4) 4 (4) 4) 4 (4) 4 (4) 4) 4 (4) 4 (4) 4) 4 (1) 4 (4) 4) 4 (4) 4 (4) 4) 4 (1) 4 (4) 4) 4 (4) 4 (4) 4) 4 (1) 4 (4) 4) 4 (1) 4 (4) 4) 4 (1) 4 (4) 4) 4 (4) 4 (4) (4) 4) 4 (1) 4) 4 (4) 4 (4) 4) 4 (4) 4) 4 (4) (4) 4 (4) 4) (4) 4 (4) 4) 4 (4) 4) 4 (4) 4 (4) 4) (4) 4) 4 (4) (4) (4) (4) (4) 4) (4) (4) 4 (4) (4) 4) 4) (4) 4 (4) (4) 4) 4) 4 (4 (4) 4) 4 (4) 4) (4) 4 (4) (4)"}], "references": [{"title": "Reinforcement learning with long short-term memory", "author": ["B. Bakker"], "venue": "NIPS", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Compatible value gradients for reinforcement learning of continuous deep policies", "author": ["D. Balduzzi", "M. Ghifary"], "venue": "arXiv preprint arXiv:1509.03005", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Applications of the morris water maze in the study of learning and memory", "author": ["R. DHooge", "P.P. De Deyn"], "venue": "Brain research reviews, 36(1):60\u201390", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Reinforcement learning in feedback control", "author": ["R. Hafner", "M. Riedmiller"], "venue": "Machine learning, 84(1-2):137\u2013169", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M. Hausknecht", "P. Stone"], "venue": "arXiv preprint arXiv:1507.06527", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["N. Heess", "G. Wayne", "D. Silver", "T. Lillicrap", "T. Erez", "Y. Tassa"], "venue": "NIPS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u2013 1780", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial intelligence, 101(1):99\u2013134", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "CoRR, abs/1312.6114", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning and adaptive dynamic programming for feedback control", "author": ["F.L. Lewis", "D. Vrabie"], "venue": "Circuits and Systems Magazine, IEEE, 9(3):32\u201350", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning with hidden states", "author": ["L.-J. Lin", "T.M. Mitchell"], "venue": "J.-A. Meyer, H. L. Roitblat, and S. W. Wilson, editors, From animals to animats 2, pages 271\u2013280. MIT Press, Cambridge, MA, USA", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1993}, {"title": "et al", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning policies with external memory", "author": ["L. Peshkin", "N. Meuleau", "L.P. Kaelbling"], "venue": "ICML", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on, pages 512\u2013519. IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 1278\u20131286", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Reinforcement learning for factored markov decision processes", "author": ["B. Sallans"], "venue": "PhD thesis, Citeseer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "ICML", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M.I. Jordan", "P. Abbeel"], "venue": "CoRR, abs/1506.02438", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey of point-based pomdp solvers", "author": ["G. Shani", "J. Pineau", "R. Kaplow"], "venue": "Autonomous Agents and Multi-Agent Systems, 27(1):1\u201351", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Deterministic policy gradient algorithms", "author": ["D. Silver", "G. Lever", "N. Heess", "T. Degris", "D. Wierstra", "M. Riedmiller"], "venue": "ICML", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning without state-estimation in partially observable markovian decision processes", "author": ["S.P. Singh"], "venue": "ICML", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1994}, {"title": "Bias in natural actor-critic algorithms", "author": ["P. Thomas"], "venue": "Proceedings of The 31st International Conference on Machine Learning, pages 441\u2013448", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic robotics", "author": ["S. Thrun", "W. Burgard", "D. Fox"], "venue": "MIT press", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Mujoco: A physics engine for model-based control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": "Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026\u20135033. IEEE", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Contextual behaviors and internal representations acquired by reinforcement learning with a recurrent neural network in a continuous state and action space task", "author": ["H. Utsunomiya", "K. Shibata"], "venue": "M. Kppen, N. Kasabov, and G. Coghill, editors, Advances in Neuro-Information Processing, volume 5507 of Lecture Notes in Computer Science, pages 970\u2013978. Springer Berlin Heidelberg", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Solving deep memory pomdps with recurrent policy gradients", "author": ["D. Wierstra", "A. F\u00f6rster", "J. Peters", "J. Schmidhuber"], "venue": "ICANN", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Policy gradient critics", "author": ["D. Wierstra", "J. Schmidhuber"], "venue": "ECML", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Policy learning with continuous memory states for partially observed robotic control", "author": ["M. Zhang", "S. Levine", "Z. McCarthy", "C. Finn", "P. Abbeel"], "venue": "CoRR, abs/1507.01273", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12].", "startOffset": 206, "endOffset": 228}, {"referenceID": 5, "context": "Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12].", "startOffset": 206, "endOffset": 228}, {"referenceID": 13, "context": "Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12].", "startOffset": 206, "endOffset": 228}, {"referenceID": 20, "context": "Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12].", "startOffset": 206, "endOffset": 228}, {"referenceID": 21, "context": "Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12].", "startOffset": 206, "endOffset": 228}, {"referenceID": 11, "context": "Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12].", "startOffset": 206, "endOffset": 228}, {"referenceID": 7, "context": "This approach has two major disadvantages: The first is the need for a model, and the second is the computational cost that is typically associated with the update of the belief state [8, 23].", "startOffset": 184, "endOffset": 191}, {"referenceID": 22, "context": "This approach has two major disadvantages: The first is the need for a model, and the second is the computational cost that is typically associated with the update of the belief state [8, 23].", "startOffset": 184, "endOffset": 191}, {"referenceID": 15, "context": "In practice, partial observability is often solved by hand-crafting a solution such as providing multiple-frames at each timestep to allow velocity estimation [16, 14].", "startOffset": 159, "endOffset": 167}, {"referenceID": 13, "context": "In practice, partial observability is often solved by hand-crafting a solution such as providing multiple-frames at each timestep to allow velocity estimation [16, 14].", "startOffset": 159, "endOffset": 167}, {"referenceID": 23, "context": "We primarily consider the Deterministic Policy Gradient algorithm (DPG) [24], which is an off-policy policy gradient algorithm that has recently produced promising results on a broad range of difficult, highdimensional continuous control problems, including direct control from pixels [14].", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "We primarily consider the Deterministic Policy Gradient algorithm (DPG) [24], which is an off-policy policy gradient algorithm that has recently produced promising results on a broad range of difficult, highdimensional continuous control problems, including direct control from pixels [14].", "startOffset": 285, "endOffset": 289}, {"referenceID": 5, "context": "We also consider DPG\u2019s stochastic counterpart, SVG(0) ([6]; SVG stands for \u201cStochastic Value Gradients\u201d) which similarly updates the policy via backpropagation of action-value gradients from an action-value critic but learns a stochastic policy.", "startOffset": 55, "endOffset": 58}, {"referenceID": 23, "context": "We extend the Deterministic Policy Gradient (DPG) algorithm for MDPs introduced in [24] to deal with partially observed domains and pixels.", "startOffset": 83, "endOffset": 87}, {"referenceID": 23, "context": "where the expectation is taken with respect to the (discounted) state visitation distribution \u03c1 induced by the current policy \u03bc [24].", "startOffset": 128, "endOffset": 132}, {"referenceID": 3, "context": "Similar ideas had previously been exploited in NFQCA [4] and in the ADP [13] community.", "startOffset": 53, "endOffset": 56}, {"referenceID": 12, "context": "Similar ideas had previously been exploited in NFQCA [4] and in the ADP [13] community.", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "These networks, with convolutional layers have proven effective at many sensory processing tasks [11, 18], and been demonstrated to be effective for scaling reinforcement learning to large state spaces [14, 16].", "startOffset": 97, "endOffset": 105}, {"referenceID": 17, "context": "These networks, with convolutional layers have proven effective at many sensory processing tasks [11, 18], and been demonstrated to be effective for scaling reinforcement learning to large state spaces [14, 16].", "startOffset": 97, "endOffset": 105}, {"referenceID": 13, "context": "These networks, with convolutional layers have proven effective at many sensory processing tasks [11, 18], and been demonstrated to be effective for scaling reinforcement learning to large state spaces [14, 16].", "startOffset": 202, "endOffset": 210}, {"referenceID": 15, "context": "These networks, with convolutional layers have proven effective at many sensory processing tasks [11, 18], and been demonstrated to be effective for scaling reinforcement learning to large state spaces [14, 16].", "startOffset": 202, "endOffset": 210}, {"referenceID": 13, "context": "[14] proposed modifications to DPG necessary in order to learn effectively with deep neural networks which we make use of here (cf.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "As discussed in the literature [25, 20] it is possible to construct examples where deterministic policies perform poorly under partial observability.", "startOffset": 31, "endOffset": 39}, {"referenceID": 19, "context": "As discussed in the literature [25, 20] it is possible to construct examples where deterministic policies perform poorly under partial observability.", "startOffset": 31, "endOffset": 39}, {"referenceID": 5, "context": "We therefore also investigate a recurrent version of the stochastic counterpart to DPG: SVG(0) [6] (DPG can be seen as the deterministic limit of SVG(0)).", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "[10, 19]) of the stochastic policy: The stochastic policy is represented in terms of a fixed, independent noise source and a parameterized deterministic function that transforms a draw from that noise source, i.", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[10, 19]) of the stochastic policy: The stochastic policy is represented in terms of a fixed, independent noise source and a parameterized deterministic function that transforms a draw from that noise source, i.", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "See [6] for more details.", "startOffset": 4, "endOffset": 7}, {"referenceID": 3, "context": "[4, 5, 14, 16, 6]) and we use the same approach here (see Algorithms 1, 2).", "startOffset": 0, "endOffset": 17}, {"referenceID": 4, "context": "[4, 5, 14, 16, 6]) and we use the same approach here (see Algorithms 1, 2).", "startOffset": 0, "endOffset": 17}, {"referenceID": 13, "context": "[4, 5, 14, 16, 6]) and we use the same approach here (see Algorithms 1, 2).", "startOffset": 0, "endOffset": 17}, {"referenceID": 15, "context": "[4, 5, 14, 16, 6]) and we use the same approach here (see Algorithms 1, 2).", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "[4, 5, 14, 16, 6]) and we use the same approach here (see Algorithms 1, 2).", "startOffset": 0, "endOffset": 17}, {"referenceID": 3, "context": "A second algorithmic feature that has been found to greatly improve the stability of neural-network based reinforcement learning algorithms that rely on bootstrapping for learning value functions is the use of target networks [4, 14, 16, 6]: The algorithm maintains two copies of the value function Q and of the policy \u03c0 each, with parameters \u03b8 and \u03b8\u2032, and \u03c9 and \u03c9\u2032 respectively.", "startOffset": 226, "endOffset": 240}, {"referenceID": 13, "context": "A second algorithmic feature that has been found to greatly improve the stability of neural-network based reinforcement learning algorithms that rely on bootstrapping for learning value functions is the use of target networks [4, 14, 16, 6]: The algorithm maintains two copies of the value function Q and of the policy \u03c0 each, with parameters \u03b8 and \u03b8\u2032, and \u03c9 and \u03c9\u2032 respectively.", "startOffset": 226, "endOffset": 240}, {"referenceID": 15, "context": "A second algorithmic feature that has been found to greatly improve the stability of neural-network based reinforcement learning algorithms that rely on bootstrapping for learning value functions is the use of target networks [4, 14, 16, 6]: The algorithm maintains two copies of the value function Q and of the policy \u03c0 each, with parameters \u03b8 and \u03b8\u2032, and \u03c9 and \u03c9\u2032 respectively.", "startOffset": 226, "endOffset": 240}, {"referenceID": 5, "context": "A second algorithmic feature that has been found to greatly improve the stability of neural-network based reinforcement learning algorithms that rely on bootstrapping for learning value functions is the use of target networks [4, 14, 16, 6]: The algorithm maintains two copies of the value function Q and of the policy \u03c0 each, with parameters \u03b8 and \u03b8\u2032, and \u03c9 and \u03c9\u2032 respectively.", "startOffset": 226, "endOffset": 240}, {"referenceID": 13, "context": "In this work we use \u201csoft updates\u201d as in [14] (see Algorithms 1 and 2 below).", "startOffset": 41, "endOffset": 45}, {"referenceID": 8, "context": "Update actor and critic using Adam [9] Update the target networks \u03c9\u2032 \u2190 \u03c4\u03c9 + (1\u2212 \u03c4)\u03c9\u2032 \u03b8\u2032 \u2190 \u03c4\u03b8 + (1\u2212 \u03c4)\u03b8\u2032 end for", "startOffset": 35, "endOffset": 38}, {"referenceID": 27, "context": "3) were simulated in MuJoCo [28].", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "A large amount of research has focused on how to efficiently integrate noisy sensory information over multiple timesteps in order to derive accurate estimates of the system state, or to estimate derivatives of important properties of the system [27].", "startOffset": 245, "endOffset": 249}, {"referenceID": 2, "context": "The Morris water maze has been used extensively in rodents for the study of memory [3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 14, "context": "Several groups [15, 1, 5] have studied the use of model-free algorithms with recurrent networks to solve POMDPs with discrete action spaces.", "startOffset": 15, "endOffset": 25}, {"referenceID": 0, "context": "Several groups [15, 1, 5] have studied the use of model-free algorithms with recurrent networks to solve POMDPs with discrete action spaces.", "startOffset": 15, "endOffset": 25}, {"referenceID": 4, "context": "Several groups [15, 1, 5] have studied the use of model-free algorithms with recurrent networks to solve POMDPs with discrete action spaces.", "startOffset": 15, "endOffset": 25}, {"referenceID": 0, "context": "[1] focused on relatively long-horizon (\u201ddeep\u201d) memory problems in small state-action spaces.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "In contrast, [5] modified the Atari DQN architecture [16] (i.", "startOffset": 13, "endOffset": 16}, {"referenceID": 15, "context": "In contrast, [5] modified the Atari DQN architecture [16] (i.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "they perform control from high-dimensional pixel inputs) and demonstrated that recurrent Q learning [15] can perform the required information integration to resolve short-term partial observability (e.", "startOffset": 100, "endOffset": 104}, {"referenceID": 29, "context": "in [30, 31, 29, 32].", "startOffset": 3, "endOffset": 19}, {"referenceID": 30, "context": "in [30, 31, 29, 32].", "startOffset": 3, "endOffset": 19}, {"referenceID": 28, "context": "in [30, 31, 29, 32].", "startOffset": 3, "endOffset": 19}, {"referenceID": 31, "context": "in [30, 31, 29, 32].", "startOffset": 3, "endOffset": 19}, {"referenceID": 29, "context": "[30] trained LSTM-based stochastic policies using Reinforce; [31, 29, 32] used actor-critic architectures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[30] trained LSTM-based stochastic policies using Reinforce; [31, 29, 32] used actor-critic architectures.", "startOffset": 61, "endOffset": 73}, {"referenceID": 28, "context": "[30] trained LSTM-based stochastic policies using Reinforce; [31, 29, 32] used actor-critic architectures.", "startOffset": 61, "endOffset": 73}, {"referenceID": 31, "context": "[30] trained LSTM-based stochastic policies using Reinforce; [31, 29, 32] used actor-critic architectures.", "startOffset": 61, "endOffset": 73}, {"referenceID": 30, "context": "The algorithm of [31] can be seen as a special case of DPG where the deterministic policy produces the parameters of an action distribution from which the actions are then sampled.", "startOffset": 17, "endOffset": 21}, {"referenceID": 31, "context": "All works mentioned above, except for [32], consider the memory to be internal to the policy and learn the RNN parameters using BPTT, back-propagating either TD errors or policy gradients.", "startOffset": 38, "endOffset": 42}, {"referenceID": 31, "context": "[32] instead take the view of [17] and consider memory as extra state dimensions that can can be read and set by the policy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[32] instead take the view of [17] and consider memory as extra state dimensions that can can be read and set by the policy.", "startOffset": 30, "endOffset": 34}, {"referenceID": 11, "context": "They optimize the policy using guided policy search [12] which performs explicit trajectory optimization along reference trajectories and, unlike our approach, requires a well defined full latent state and access to this latent state during training.", "startOffset": 52, "endOffset": 56}, {"referenceID": 6, "context": "are able to benefit from innovations in supervised recurrent neural networks, such as long-short term memory networks [7], to solve challenging memory problems such as the Morris water maze.", "startOffset": 118, "endOffset": 121}], "year": 2015, "abstractText": "Partially observed control problems are a challenging aspect of reinforcement learning. We extend two related, model-free algorithms for continuous control \u2013 deterministic policy gradient and stochastic value gradient \u2013 to solve partially observed domains using recurrent neural networks trained with backpropagation through time. We demonstrate that this approach, coupled with long-short term memory is able to solve a variety of physical control problems exhibiting an assortment of memory requirements. These include the short-term integration of information from noisy sensors and the identification of system parameters, as well as long-term memory problems that require preserving information over many time steps. We also demonstrate success on a combined exploration and memory problem in the form of a simplified version of the well-known Morris water maze task. Finally, we show that our approach can deal with high-dimensional observations by learning directly from pixels. We find that recurrent deterministic and stochastic policies are able to learn similarly good solutions to these tasks, including the water maze where the agent must learn effective search strategies.", "creator": "LaTeX with hyperref package"}}}