{"id": "1512.04483", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2015", "title": "Dropout Training of Matrix Factorization and Autoencoder for Link Prediction in Sparse Graphs", "abstract": "Matrix factorization (MF) and Autoencoder (AE) are among the most successful approaches of unsupervised learning. While MF based models have been extensively exploited in the graph modeling and link prediction literature, the AE family has not gained much attention. In this paper we investigate both MF and AE's application to the link prediction problem in sparse graphs. We show the connection between AE and MF from the perspective of multiview learning, and further propose MF+AE: a model training MF and AE jointly with shared parameters. We apply dropout to training both the MF and AE parts, and show that it can significantly prevent overfitting by acting as an adaptive regularization. We conduct experiments on six real world sparse graph datasets, and show that MF+AE consistently outperforms the competing methods, especially on datasets that demonstrate strong non-cohesive structures.", "histories": [["v1", "Mon, 14 Dec 2015 19:38:14 GMT  (501kb)", "http://arxiv.org/abs/1512.04483v1", "Published in SDM 2015"]], "COMMENTS": "Published in SDM 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuangfei zhai", "zhongfei zhang"], "accepted": false, "id": "1512.04483"}, "pdf": {"name": "1512.04483.pdf", "metadata": {"source": "CRF", "title": "Dropout Training of Matrix Factorization and Autoencoder for Link Prediction in Sparse Graphs", "authors": ["Shuangfei Zhai", "Zhongfei (Mark) Zhang"], "emails": ["szhai2@binghamton.edu", "zhongfei@cs.binghamton.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.04 483v 1 [cs.L G] 14 Dec 201 5Matrix factorization (MF) and autoencoder (AE) are among the most successful approaches to unattended learning. While MF-based models have been widely used in the literature for graph modeling and linking predictions, the AE family has not received much attention. In this paper, we examine both MF and the application of AE to the link prediction problem in sparse diagrams. We show the link between AE and MF from the perspective of multiview learning and also propose MF + AE: a model training of MF and AE together with common parameters. We apply MF + AE to the training of both the MF and the AE parts and show that it can significantly prevent overfitting by acting as adaptive regulation. We conduct experiments on six sparse graphs showing that MF + E are superior to the MF and MF structures, in particular, and that MF + structures are not superior."}, {"heading": "1 Introduction", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "2 Model", "text": "We are the only thing we are able to understand when we see ourselves in the W1, W 2, J 2, J J) (2.1), where W1, W2, W2, W2, W2, and W1, W2), W2, and W1, W2, and W1, W2, W2 and W2, W1 and W2, W1 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2 and W2, W2 and W2, W2 and W2 and W2, W2 and W2, W2 and W2, W2 and W2 and W2, W2 and W2 and W2, W2 and W2, W2 and W2, W2 and W2 and W2, W2 and W2 and W2, W2 and W2, W2 and W2 and W2, W2 and W2, W2 and W2 and W2, W2 and W2 and W2, W2 and W2, W2 and W2 and W2, W2 and W2, W2 and W2, W2 and W2 and W2 and W2, W2 and W2, W2 and W2 and W2, W2 and W2, W2 and W2, W2 and W2, W2 and W2 and W2 and W2, W2, W2 and W2, W2 and W2 and W2, W"}, {"heading": "3 Experiments", "text": "This year, it will be ready to leave the country in which it is located."}, {"heading": "4 Related Work", "text": "Salakhutdinov et al. [30] apply RBM initially to film recommendations. Recently, Chen and Zhang [7] propose a variant of linear AE with marginalized character noise for link predictions, and Li et al. [11] use RBM to link predictions in dynamic graphs.MF + AE is also related to monitored learning-based methods [3, 13]. While these approaches train a classifier directly on manually collected characteristics, MF + AE learn the corresponding characteristics directly from the adjacent matrix. The use of dropout training as implicit regulation is also at odds with Bayesian models [2, 15]. While both Dropout and Bayesian inference are designed to reduce overadaptation of metrics, their approaches are essentially orthogonal to mutually formatting in order to learn future work."}, {"heading": "5 Conclusion", "text": "We propose a novel model of MF + AE that trains MF and AE together with common parameters. We show that dropouts can significantly improve the generalizability of MF and AE by acting as adaptive regulation of weight matrices. We conduct experiments on six sparse real-world diagrams and show that MF + AE outperforms all competing methods, especially on datasets with strong, non-cohesive structures."}, {"heading": "Acknowledgements", "text": "This work is partially supported by NSF CCF1017828, China's National Basic Research Programme (2012CB316400) and the Zhejiang Provincial Engineering Center on Media Data Cloud Processing and Analysis in China."}], "references": [{"title": "Friends and neighbors on the web", "author": ["L.A. Adamic", "E. Adar"], "venue": "SOCIAL NETWORKS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Mixed membership stochastic blockmodels", "author": ["E.M. Airoldi", "D.M. Blei", "S.E. Fienberg", "E.P. Xing"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1981}, {"title": "Link prediction using supervised learning", "author": ["M. Al Hasan", "V. Chaoji", "S. Salem", "M. Zaki"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Supervised random walks: predicting and recommending links in social networks", "author": ["L. Backstrom", "J. Leskovec"], "venue": "In WSDM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Marginalized denoising auto-encoders for nonlinear representations", "author": ["M. Chen", "K. Q Weinberger", "F. Sha", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "A marginalized denoising method for link prediction in relational data", "author": ["Z. Chen", "W. Zhang"], "venue": "In SDM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. E Hinton", "R. R Salakhutdinov"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Modeling homophily and stochastic equivalence in symmetric relational data", "author": ["P. Hoff"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Learning the parts of objects by nonnegative matrix factorization", "author": ["D.D. Lee", "H. Sebastian Seung"], "venue": "Nature, 401:788\u2013791,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "A deep learning approach to link prediction in dynamic networks", "author": ["X. Li", "N. Du", "H. Li", "K. Li", "J. Gao", "A. Zhang"], "venue": "In SDM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "The link-prediction problem for social networks", "author": ["D. Liben-Nowell", "J. Kleinberg"], "venue": "J. Am. Soc. Inf. Sci. Technol.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "New perspectives and methods in link prediction", "author": ["R.N. Lichtenwalter", "J.T. Lussier", "N.V. Chawla"], "venue": "In KDD,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Link prediction via matrix factorization", "author": ["A. Krishna Menon", "C. Elkan"], "venue": "In ECML PKDD Proceedings, Part II,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Nonparametric latent feature models for link prediction", "author": ["K.T. Miller", "T.L. Griffiths", "M.I. Jordan"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "In KDD ,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Probabilistic matrix factorization", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1929}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. A Manzagol"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Dropout training as adaptive regularization", "author": ["S. Wager", "S. Wang", "P. Liang"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Overlapping community detection at scale: a nonnegative matrix factorization approach", "author": ["J. Yang", "J. Leskovec"], "venue": "In WSDM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In COLT,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Distance metric learning using dropout: A structured regularization approach", "author": ["Q. Qian", "J. Hu", "R. Jin", "J. Pei", "S. Zhu"], "venue": "In KDD ,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Relational learning via collective matrix factorization", "author": ["A. P Singh", "G. J Gordon"], "venue": "In KDD,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Feature noising for log-linear structured prediction", "author": ["S. Wang", "Mengqiu Wang", "S. Wager", "P. Liang", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Detecting cohesive and 2-mode communities indirected and undirected networks", "author": ["J. Yang", "J. McAuley", "J. Leskovec"], "venue": "In WSDM ,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Restricted Boltzmann Machines for Collaborative Filtering in ICML, pages", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}], "referenceMentions": [{"referenceID": 11, "context": "Link prediction is one of the fundamental problems of network analysis, as pointed out in [12], \u201da network model is useful to the extent that it can support meaningful inferences from observed network data.", "startOffset": 90, "endOffset": 94}, {"referenceID": 26, "context": "Among the large number of models proposed over the decade, Matrix Factorization (MF) is one of the most popular ones in network modeling and relational learning in general [27, 24, 14, 22].", "startOffset": 172, "endOffset": 188}, {"referenceID": 23, "context": "Among the large number of models proposed over the decade, Matrix Factorization (MF) is one of the most popular ones in network modeling and relational learning in general [27, 24, 14, 22].", "startOffset": 172, "endOffset": 188}, {"referenceID": 13, "context": "Among the large number of models proposed over the decade, Matrix Factorization (MF) is one of the most popular ones in network modeling and relational learning in general [27, 24, 14, 22].", "startOffset": 172, "endOffset": 188}, {"referenceID": 21, "context": "Among the large number of models proposed over the decade, Matrix Factorization (MF) is one of the most popular ones in network modeling and relational learning in general [27, 24, 14, 22].", "startOffset": 172, "endOffset": 188}, {"referenceID": 26, "context": "Training of such a model is usually conducted with stochastic gradient descent, which makes it easily scalable to large datasets [27, 24].", "startOffset": 129, "endOffset": 137}, {"referenceID": 23, "context": "Training of such a model is usually conducted with stochastic gradient descent, which makes it easily scalable to large datasets [27, 24].", "startOffset": 129, "endOffset": 137}, {"referenceID": 1, "context": "Bayesian models, such as MMSB [2, 15, 26] are another family of latent factor models for studying network structures.", "startOffset": 30, "endOffset": 41}, {"referenceID": 14, "context": "Bayesian models, such as MMSB [2, 15, 26] are another family of latent factor models for studying network structures.", "startOffset": 30, "endOffset": 41}, {"referenceID": 25, "context": "Bayesian models, such as MMSB [2, 15, 26] are another family of latent factor models for studying network structures.", "startOffset": 30, "endOffset": 41}, {"referenceID": 4, "context": "Autoencoder (AE) together with its variants such as Restricted Boltzman Machine (RBM) has recently achieved great success in various machine learning applications, and is well recognized as the building block of Deep Learning [5, 8].", "startOffset": 226, "endOffset": 232}, {"referenceID": 7, "context": "Autoencoder (AE) together with its variants such as Restricted Boltzman Machine (RBM) has recently achieved great success in various machine learning applications, and is well recognized as the building block of Deep Learning [5, 8].", "startOffset": 226, "endOffset": 232}, {"referenceID": 6, "context": "Surprisingly, it is not until recently that AE finds its application to modeling graphs [7, 11].", "startOffset": 88, "endOffset": 95}, {"referenceID": 10, "context": "Surprisingly, it is not until recently that AE finds its application to modeling graphs [7, 11].", "startOffset": 88, "endOffset": 95}, {"referenceID": 18, "context": "To prevent overfitting, we train the model with Dropout [19] combined with stochastic gradient descent.", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "The idea is similar to co-training [23] in the semisupervised learning setting, where one trains two classifiers on two sufficient views such that the two", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "Instead of putting explicit constraints on the parameters or hidden units, we use dropout training [19, 21] as an implicit regularization.", "startOffset": 99, "endOffset": 107}, {"referenceID": 20, "context": "Instead of putting explicit constraints on the parameters or hidden units, we use dropout training [19, 21] as an implicit regularization.", "startOffset": 99, "endOffset": 107}, {"referenceID": 19, "context": "For the AE module, randomly dropping out the input can be considered as a \u201ddenoising\u201d technique, which was exploited by the previous work [20, 6], and also was applied to link prediction [7].", "startOffset": 138, "endOffset": 145}, {"referenceID": 5, "context": "For the AE module, randomly dropping out the input can be considered as a \u201ddenoising\u201d technique, which was exploited by the previous work [20, 6], and also was applied to link prediction [7].", "startOffset": 138, "endOffset": 145}, {"referenceID": 6, "context": "For the AE module, randomly dropping out the input can be considered as a \u201ddenoising\u201d technique, which was exploited by the previous work [20, 6], and also was applied to link prediction [7].", "startOffset": 187, "endOffset": 190}, {"referenceID": 20, "context": "Previously, [21, 6] used the second order Taylor expansion to explain the effect of feature noising in generalized linear models and AE, respectively.", "startOffset": 12, "endOffset": 19}, {"referenceID": 5, "context": "Previously, [21, 6] used the second order Taylor expansion to explain the effect of feature noising in generalized linear models and AE, respectively.", "startOffset": 12, "endOffset": 19}, {"referenceID": 3, "context": "Following the experimental protocol suggested in [4], we first split the observed links into a training graph Gtrain and a testing graph Gtest.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "We then only consider the nodes that are 2-hops from the target node as the candidate nodes [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "The methods we evaluate are: Adamic-Adar Score (AA) [1].", "startOffset": 52, "endOffset": 55}, {"referenceID": 11, "context": "Random Walk with restart (RW)[12].", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": "3 used in [4]) works slightly better on our problem .", "startOffset": 10, "endOffset": 13}, {"referenceID": 13, "context": "This is a variant of the model proposed in [14].", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "Marginalized Denoising Model (MDM)[7].", "startOffset": 34, "endOffset": 37}, {"referenceID": 28, "context": "non-cohesive distinction of graph structure has previously been investigated in [29, 9].", "startOffset": 80, "endOffset": 87}, {"referenceID": 8, "context": "non-cohesive distinction of graph structure has previously been investigated in [29, 9].", "startOffset": 80, "endOffset": 87}, {"referenceID": 26, "context": "The link prediction problem can be considered as a special case of relational learning and recommender systems [27, 24], and a lot of techniques proposed are directly applicable to link prediction as well.", "startOffset": 111, "endOffset": 119}, {"referenceID": 23, "context": "The link prediction problem can be considered as a special case of relational learning and recommender systems [27, 24], and a lot of techniques proposed are directly applicable to link prediction as well.", "startOffset": 111, "endOffset": 119}, {"referenceID": 29, "context": "[30] first apply RBM to movie recommendation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Recently, Chen and Zhang [7] propose a variant of linear AE with marginalized feature noise for link prediction, and Li et al.", "startOffset": 25, "endOffset": 28}, {"referenceID": 10, "context": "[11] apply RBM to link prediction in dynamic graphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "MF+AE is also related to the supervised learning based methods [3, 13].", "startOffset": 63, "endOffset": 70}, {"referenceID": 12, "context": "MF+AE is also related to the supervised learning based methods [3, 13].", "startOffset": 63, "endOffset": 70}, {"referenceID": 1, "context": "The utilization of dropout training as an implicit regularization also contrasts with Bayesian models [2, 15].", "startOffset": 102, "endOffset": 109}, {"referenceID": 14, "context": "The utilization of dropout training as an implicit regularization also contrasts with Bayesian models [2, 15].", "startOffset": 102, "endOffset": 109}, {"referenceID": 20, "context": "Dropout has also been applied to training generalized linear models [21], log linear models with structured output [28], and distance metric learning [25].", "startOffset": 68, "endOffset": 72}, {"referenceID": 27, "context": "Dropout has also been applied to training generalized linear models [21], log linear models with structured output [28], and distance metric learning [25].", "startOffset": 115, "endOffset": 119}, {"referenceID": 24, "context": "Dropout has also been applied to training generalized linear models [21], log linear models with structured output [28], and distance metric learning [25].", "startOffset": 150, "endOffset": 154}, {"referenceID": 15, "context": "[16] propose to learn node embeddings by predicting the path of a random walk, and they show that the learned representation can boost the performance of the classification task on", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Matrix factorization (MF) and Autoencoder (AE) are among the most successful approaches of unsupervised learning. While MF based models have been extensively exploited in the graph modeling and link prediction literature, the AE family has not gained much attention. In this paper we investigate both MF and AE\u2019s application to the link prediction problem in sparse graphs. We show the connection between AE and MF from the perspective of multiview learning, and further propose MF+AE: a model training MF and AE jointly with shared parameters. We apply dropout to training both the MF and AE parts, and show that it can significantly prevent overfitting by acting as an adaptive regularization. We conduct experiments on six real world sparse graph datasets, and show that MF+AE consistently outperforms the competing methods, especially on datasets that demonstrate strong non-cohesive structures.", "creator": "LaTeX with hyperref package"}}}