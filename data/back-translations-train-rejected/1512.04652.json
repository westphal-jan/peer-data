{"id": "1512.04652", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2015", "title": "Hyper-Heuristic Algorithm for Finding Efficient Features in Diagnose of Lung Cancer Disease", "abstract": "Background: Lung cancer was known as primary cancers and the survival rate of cancer is about 15%. Early detection of lung cancer is the leading factor in survival rate. All symptoms (features) of lung cancer do not appear until the cancer spreads to other areas. It needs an accurate early detection of lung cancer, for increasing the survival rate. For accurate detection, it need characterizes efficient features and delete redundancy features among all features. Feature selection is the problem of selecting informative features among all features. Materials and Methods: Lung cancer database consist of 32 patient records with 57 features. This database collected by Hong and Youngand indexed in the University of California Irvine repository. Experimental contents include the extracted from the clinical data and X-ray data, etc. The data described 3 types of pathological lung cancers and all features are taking an integer value 0-3. In our study, new method is proposed for identify efficient features of lung cancer. It is based on Hyper-Heuristic. Results: We obtained an accuracy of 80.63% using reduced 11 feature set. The proposed method compare to the accuracy of 5 machine learning feature selections. The accuracy of these 5 methods are 60.94, 57.81, 68.75, 60.94 and 68.75. Conclusions: The proposed method has better performance with the highest level of accuracy. Therefore, the proposed model is recommended for identifying an efficient symptom of Disease. These finding are very important in health research, particularly in allocation of medical resources for patients who predicted as high-risks", "histories": [["v1", "Tue, 15 Dec 2015 05:15:07 GMT  (251kb)", "http://arxiv.org/abs/1512.04652v1", null], ["v2", "Sun, 24 Jan 2016 11:07:25 GMT  (251kb)", "http://arxiv.org/abs/1512.04652v2", "Published in the Journal of Basic and Applied Scientific Research, 2013"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mitra montazeri", "mahdieh soleymani baghshah", "ahmad enhesari"], "accepted": false, "id": "1512.04652"}, "pdf": {"name": "1512.04652.pdf", "metadata": {"source": "CRF", "title": "Hyper-Heuristic Algorithm for Finding Efficient Features in Diagnose of Lung Cancer Disease", "authors": ["Mitra Montazeri", "MahdiehSoleymani Baghshah", "Ahmad Enhesari"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Preliminaries", "text": "In this section, we first present some basic concepts of the Pearson correlation coefficient (as we will use it later as a criterion for evaluation) and then the concept of attribute selection."}, {"heading": "2.1 Pearson Correlation Coefficient", "text": "Pearson's linear correlation coefficient is very popular in statistics and represents the normalized measure of the strength of the linear relationship between variables [18]. For random variable X with values x and random variable Y with values y, while a vector of d data xi, yi, i = 1,..., d is defined as: (1) Where, as usual, x is the mean of xi variables, y is the mean of yi variables equal \u00b1 1, if X and Y are linearly dependent and zero if they are completely uncorrelated (random variables may be positively or negatively correlated). If a group of k characteristics already exists, the correlation coefficient r is equal to \u00b1 1 if X and Y are linearly dependent and zero if they are completely uncorrelated (random variables may be positively or negatively correlated)."}, {"heading": "2.2 Feature Selection Problem", "text": "We can formulate the problem of selecting a subset of features with superior classificatory power as follows: Let F represent the original group of features with cardinality N and m the number of features in the selected subset X, X F. Let J X represent the function of the feature selection for set X (a higher value of J indicates a better feature subset). Formally, the problem of selecting features is to find a subset X F which has the following two important properties: m N J X J NIt has been shown that the search for the minimal feature subset NP is hard [22]."}, {"heading": "3 Proposed Method", "text": "Since each region of the solution space can have its own characteristics, a suitable LLH should be selected and applied to the current solution. GA's role is as a supervisor who manages the selection of LLHs that should be applied at all times. GA selects an LLH based on the existing functional history of LLHs. This GA is not a direct GA, in fact, every person in the population of GA consists of a sequence of integer numbers. Each number is an LLH choice that tells us which LLHH must be applied, and each individual tells in what order LLHs should be applied. LLHs perform local searches for the current solution to improve it. Local search is a neighborhood search algorithm. They cooperate in each chromosome by sharing their best solutions to combine their efforts and improve the quality of solutions that each of them can exploit."}, {"heading": "3.1 Chromosome encoding", "text": "In our proposed GA, each chromosome Ci consists of a sequence of integer numbers, each number being an LLH choice that tells us which LLH must be used. Each integer is lied to Number of LLHs (NLLH) at interval 1. Fig. 1 shows the encoding representation of a chromosome for NLLH = 12. These integers are generated randomly. As you can see in Fig. 1, for example, integer number 10 causes the call of the 10th LLH.10 9 1 6 2 3 8 11 4 5 7 12Abb.1: Example of the encoding representation of a chromosome as an integer string in the proposed method."}, {"heading": "3.2 Fitness Function", "text": "To judge the quality of each feature subset produced by a chromosome, i.e. a sequence of LLHs, we use the accuracy of the classification. It can be defined as follows: () Fit J fs (3) Cause for calling 10thLLHWhere fs denotes the corresponding selected feature subset encoded in the solution, and J calculates the quality of the feature subset. In our method, J shows the accuracy of the classification. Note that if two solutions have the same suitability, each one is selected that has a smaller number of features."}, {"heading": "3.3 Evolutionary Operations", "text": "Genetic operations are very important for the success of GA applications. [24] In the following, we present the genetic operations used in the proposed method. Selection: In the proposed method, the selection operation is based on the fitness of the chromosomes. It should ensure that fitter chromosomes have a better chance of survival. In our method, we use rank-based elitism roulette wheel selection. Assuming the population has Np chromosomes, for each chromosome Ci, 1 \u2264 i \u2264 Np, the probability of selection p (Ci) is calculated as follows: 1 () () () i i i Npj jFit Cp C Fit C C (4) A chromosome Ci is selected if it meets the following inequality: 10 0 () () i i i i i i i i i i i j crossover p C r C (5), whereby r is a random number with equal distribution in chromosomes Cp C Fit C Fit C (4) A chromosome Ci is selected if it meets the following imbalance: 10 0 () () () () i i i i i i i i i i i i i i i i i j crossover C r C r C (5), where r is a random number with equal distribution in chromosomes with one chromosomes [1] probability of two, one chromosomes are selected, and one chromosomes with two: one each."}, {"heading": "3.4 Low Level Heuristics", "text": "In the proposed method, 12 LLHs (NLLH is equal to 12) are used to change the current solution. All of our LLHs are divided into two groups: Explorer Heuristics and Explorer Heuristics. We use domain knowledge about the problem of feature selection in designing the local search to enable a more effective search through the HH approach. 8 of these are exploiters, searching between existing traits, non-existent and all traits. Their searches are similar to the mountaineer [26, 27]. The last four LLHs seek a high diversity of searches. In these LLHs, different focus points of probability are used, i.e. random probability, equal probability and mutation probability in GA. These probability variations due to each region of the solution space have their own characteristics, in addition to the diversity of the Explorer LLLHs we use different types of explorer LHs."}, {"heading": "3.4.1 Solution Encoding and Evaluation", "text": "In the proposed method, the coded solution used by the LLHs is a binary length chain that corresponds to the total number of characteristics, each of which encodes a single characteristic so that the length of the chromosome is N. As shown in Figure 2, \"1\" implies that the corresponding characteristic is selected and \"0\" that it is excluded. In order to provide an efficient and effective method, we use the criteria in Equation (2) as evaluation criteria."}, {"heading": "4 Experimental Result and Discussion", "text": "In this section, several experiments are performed to evaluate the performance of our method. In the following subsections, a brief description of the lung cancer dataset and experimental setup is presented, and then our results are presented and compared with the results of some literature work. 4.1 Database description and pre-processing of the lung cancer database consists of 32 patient records with 57 characteristics. This database was collected by Hong and Young [29] and indexed in the repository of the University of California Irvine (UCI) [30]. The experimental content includes data extracted from clinical data and X-ray data, etc. It should be noted that the number of samples is less than the number of characteristics. The data described 3 types of pathological lung cancer and all characteristics are nominal and take an integer value of 0-3. Since this dataset has missing values or continues values in uncontrolled areas, they need a pre-processing step before they are used for the W1 missing values."}, {"heading": "4.2 Performance Evaluation Setup", "text": "In our method, the following metrics are used: Result validation: We use the accuracy of a nearest neighborhood classifier (1NN) for result validation. It is stable and insensitive to the initial setting. Other classifiers, such as neural networks due to different output by changing the initial weights, may not be suitable for performance comparisons. k-Fold cross-validation: It is used to estimate how accurately a predictive model will work in practice and represents the probability that an instance will be correctly classified. In k-Fold cross-validation, the original sample is randomly divided into k-subsample samples. A single subsample is retained as test data and the remaining k-1 subsample samples are used as training data. The cross-validation process is then repeated k-times, with each of the k-subsample samples being used exactly once as validation data. K-results from the folds can then be averaged to generate a random 10-fold-to-10-to-to-10-to-to-to-to-to-10-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to"}, {"heading": "4.3 Performance of the proposed method", "text": "In this section we present an experimental study of the proposed method of the Lung Cancer Database. We set the population size (Np) to 30 and the number of generations to 200. Crossover rate (Pc) and mutation rate (Pm) to 0.7 and 0.1, respectively. Table 1 shows the best and average accuracy of 10 passes of our method of the Lung Cancer Database. Since the proposed method is a random search algorithm, different results can be obtained from different runs. Therefore, we run this algorithm ten times and report their average. In addition to the accuracy, the number of selected features is also given. As shown in this table, this number decreases considerably and with a small number of features we see a higher accuracy. Our method can achieve these excellent effects in low generations (200). In fact, heuristic algorithms for a high generation, e.g. at least 10,000, should be executed to find a global optimum, but our method finds its results in 200 generations."}, {"heading": "4.4 Comparison Of literature Works", "text": "We compare the proposed method with the five most common machine learning characteristics (for more information see Ref. [34-38]), which are shown in Table 2.We compare the mean and the best value below 10 passes of the proposed method. According to the results shown in Table 2, our method shows the best results overall (both average and best results in 10 passes). Furthermore, the results obtained with the proposed method are substantially better than the other methods. Although two methods have lower characteristics, their accuracies are substantially lower than those of the proposed method. In predicting and classifying, the main problem is the accuracy and number of characteristics the second important problem. Through the proposed method, the reported value is the best value below 10 passes, the value in parentheses is the mean of these 10 passes and the average number of selected characteristics. The proposed method 80.63 (75.06) 11 (18.8) Based on the chi-squared algorithm, the gain-ratio algorithm, the relay algorithm and the 3rd algorithm,"}, {"heading": "5. Conclusion and Discussion", "text": "Lung cancer is a deadly cancer that kills a large number of people each year.Survival rates for cancer are around 15%.Statistics show that overall, victims of breast, prostate and colon cancer are less likely than victims of lung cancer.If undiagnosed, this growth can spread beyond the lungs to nearby tissues, in a process called metastasis, and gradually to other parts of the body. Early detection of lung cancer is the leading factor in survival rates, making it more difficult for physicians to work.A physician often makes decisions by evaluating a patient's current test results and pointing to the previous decisions he has made to exploiting the same condition.It takes accurate early detection of lung cancer to increase survival rate.For accurate detection, it must characterize efficient features and erase redundancy features among all characteristics. Characteristics selection is the problem of informative characteristics among all characteristics."}], "references": [{"title": "Asbestos-related lung disease", "author": ["KM O'Reilly", "Mclaughlin AM", "Beckett WS", "Sime PJ"], "venue": "American Family Physician75", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Lung cancer occurrence in never-smokers: an analysis of 13 cohorts and 22 cancer registry studies", "author": ["MJ Thun", "Adams-Campbell LL Hannan LM"], "venue": "PMID", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Estimates of worldwide burden of cancer in 2008: GLOBOCAN 2008", "author": ["J Ferlay", "Bray F Shin HR"], "venue": "International Journal of Cancer", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Principles component analysis", "author": ["P. Kemal", "G. Salih"], "venue": "fuzzy weighting pre-processing and artificial immune recognition system based diagnostic system for diagnosis of lung cancer, Expert Systems with Applications", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Cancer statistics", "author": ["A. Jemal", "R. Siegel", "E. Ward", "T. Murray", "J. Xu", "M.J. Thun"], "venue": "CA Cancer J. Clin.57:43", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature Extraction Foundations and Applications", "author": ["I Guyon"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Less Is More\", in Feature Extraction, Construction and Selection: A Data Mining Perspective H.L.a", "author": ["H. Liu", "H. Motoda"], "venue": "H, Editor", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "A Novel Memetic Feature Selection Algorithm, 5th International Conference on Information and Knowledge Technology (IKT", "author": ["M. Montazeri", "H.R. Naji"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Cooperating Of Local Searches Based Hyper-Heuristic Approach For Solving Travelling Salesman Problem, in International Conference on Evolutionary Computation Theory and Application (ECTA)2011: Paris, France", "author": ["M Montazeri"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Selecting Efficient Features via a Hyper-Heuristic Approach, in The 5th Iran Data Mining Conference (IDMC) 2011: Amirkabir", "author": ["M. Montazeri", "M.S. Baghshah", "A. Niknafs"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Exploring and Exploiting Effectively Based Hyper-Heuristic Approach for Solving Travelling Salesman Problem, Iranian Data Mining Conference (IDMC\u201911)", "author": ["M. Montazeri", "H. Nezamabadi-pour", "A. Bahrololoum"], "venue": "Amirkabir university,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Numerical recipes in C", "author": ["Press", "W.H"], "venue": "The art of scientific computing", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1988}, {"title": "Methods for aggregating opinions. Decision making and change in human affairs 1977: Dordrecht, Holland: Reidel Publishing Co", "author": ["R.M. Hogarth"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1977}, {"title": "Correlation-based Feature Subset Selection for Machine Learning, in Department of Computer Science1999, University of Waikato: Waikato, N.Z", "author": ["M.A. Hall"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "On the approximation of minimizing non zero variables or unsatisfied relations in linear systems", "author": ["E. Amaldi", "V. Kann"], "venue": "Theoretical Computer Science 1998", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "A hyperheuristic approach to scheduling a sales summit", "author": ["P. Cowling", "G. Kendall", "E. Soubeiga"], "venue": "In Selected papers of Proceedings of the 3rd International Conference on the Practice and Theory of Automated Timetabling", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "Automatically integrating multiple rule sets in a distributed-knowledge environment", "author": ["T.P.H.C.H. Wang", "S.S. Tseng", "C.M. Liao"], "venue": "IEEE Transaction Systems, Man, and Cybernetics, Part C: Applications and Reviews,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Fitness Landscapes Royal Road Functions", "author": ["M. Mitchell", "S. Forrest"], "venue": "Handbook of Evolutionary Computation,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1997}, {"title": "Bit Climbing, Representational Bias, and Test Suite Design", "author": ["L. Davis"], "venue": "Proceedings of the 4th Int. Conference on Genetic Algorithms", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1991}, {"title": "Analyzing microarray gene expression data. 2005: Wiley Series in Probability and Statistics", "author": ["M. GJ", "K.A. Do", "C. Ambroise"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "Optimal Discriminate Plane for a Small Number of Samples and Design Method of Classifier on the Plane", "author": ["Z.Q. Hong", "J.Y. Yang"], "venue": "Pattern Recognition,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1991}, {"title": "Automatic extraction of eye field from a grayintensity image using intensity filtering and hybrid projection function, InternationalConference on Communications, Computing and Control Applications", "author": ["M. Montazeri", "H. Nezamabadi-pour"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Eye detection in digital images :challenges and solutions, 2th National Conference of Electrical EngineeringNEEC2011), Iran", "author": ["M. Montazeri", "S. Saryazdi"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Automatically Eye Detectionwith Different Gray Intensity Image Conditions", "author": ["M. Montazeri", "H. Nezamabadi-pour"], "venue": "International Journal of ComputerTechnology and Application,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Attribute selection based on information gain ratio in fuzzy rough set theory with application to tumor classification", "author": ["Dai", "Jianhua", "Xu", "Qing"], "venue": "Applied Soft Computing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "A Practical Approach to Feature Selection", "author": ["Kenji Kira", "Larry A. Rendell"], "venue": "Ninth International Workshop on Machine Learning,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1992}, {"title": "DataMining: Practical Machine Learning Tools and Techniques", "author": ["Witten", "Ian H", "Frank", "Eibe"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "A guide to chi-squared testing", "author": ["P.E. Greenwood", "M.S. Nikulin"], "venue": "New York: Wiley,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "80\u201390% of lung cancers [1] in reason of long-term exposure to tobacco smoke,[2] these cases are often attributed to a combination of genetic factors,[3]radon gas,[3]asbestos,[4] and air pollution[4] including secondhand smoke while nonsmokers account is only 10\u201315% of lung cancer cases.", "startOffset": 174, "endOffset": 177}, {"referenceID": 0, "context": "80\u201390% of lung cancers [1] in reason of long-term exposure to tobacco smoke,[2] these cases are often attributed to a combination of genetic factors,[3]radon gas,[3]asbestos,[4] and air pollution[4] including secondhand smoke while nonsmokers account is only 10\u201315% of lung cancer cases.", "startOffset": 195, "endOffset": 198}, {"referenceID": 1, "context": "[5] Symptoms of lung cancer are coughing up blood.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "physician often makes decisions by evaluating the current test results of a patient and by referring to the previousdecisions he made on other patience with the same condition [9].", "startOffset": 176, "endOffset": 179}, {"referenceID": 4, "context": "On the other, all symptoms of lung cancer do not appear until the cancer spreads to other areas, thus leading to cancer detection of only 24% in early stages [10, 11].", "startOffset": 158, "endOffset": 166}, {"referenceID": 21, "context": "Features are useful in many aspect of applications [31-33] and feature selection is the problem of selecting informative features among all features such that the selected feature subset has lower cardinality and provides higher accuracy.", "startOffset": 51, "endOffset": 58}, {"referenceID": 22, "context": "Features are useful in many aspect of applications [31-33] and feature selection is the problem of selecting informative features among all features such that the selected feature subset has lower cardinality and provides higher accuracy.", "startOffset": 51, "endOffset": 58}, {"referenceID": 23, "context": "Features are useful in many aspect of applications [31-33] and feature selection is the problem of selecting informative features among all features such that the selected feature subset has lower cardinality and provides higher accuracy.", "startOffset": 51, "endOffset": 58}, {"referenceID": 5, "context": "[12, 13].", "startOffset": 0, "endOffset": 8}, {"referenceID": 6, "context": "[12, 13].", "startOffset": 0, "endOffset": 8}, {"referenceID": 7, "context": "Until now many heuristic algorithms with different strategies have been proposed [14], but they have their own problem.", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "Hyper-heuristic (HH) approach is a newest heuristic algorithm which was introduced in 2000 [23], solve these problems, properly.", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "solutions but also search more efficiently than their conventional counterparts [15-17].", "startOffset": 80, "endOffset": 87}, {"referenceID": 9, "context": "solutions but also search more efficiently than their conventional counterparts [15-17].", "startOffset": 80, "endOffset": 87}, {"referenceID": 10, "context": "solutions but also search more efficiently than their conventional counterparts [15-17].", "startOffset": 80, "endOffset": 87}, {"referenceID": 5, "context": "In contrast to information theoretic and decision tree approaches, they avoid problems with probability density estimation and discretization of continuous features and therefore they are treated first [12].", "startOffset": 202, "endOffset": 206}, {"referenceID": 11, "context": "The linear correlation coefficient of Pearson is very popular in statistics and represents the normalized measure of the strength of linear relationship between variables [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 12, "context": "These ideas have been discussed in the theory of psychological measurements [19] and in the literature on decision making and aggregating opinions [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 13, "context": "It has been used in the Correlation-based Feature Selection (CFS) algorithm [21].", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "It has been demonstrated that searching for the minimum feature subset is NP-hard [22].", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "3 Evolutionary Operations Genetic operations are very important to the success of GA applications [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 17, "context": "Their searches are similar to hill climber[26, 27].", "startOffset": 42, "endOffset": 50}, {"referenceID": 18, "context": "Their searches are similar to hill climber[26, 27].", "startOffset": 42, "endOffset": 50}, {"referenceID": 20, "context": "This database collected by Hong and Young [29] and indexed in the University of California Irvine (UCI) repository [30].", "startOffset": 42, "endOffset": 46}, {"referenceID": 19, "context": "In our study, due to being randomness, run 10 times and at each time a 10-fold cross validation which is commonly used [28] is used, and the final results were their average values (10-10 fold CV).", "startOffset": 119, "endOffset": 123}, {"referenceID": 24, "context": "[34-38]) which are listed in Table 2.", "startOffset": 0, "endOffset": 7}, {"referenceID": 25, "context": "[34-38]) which are listed in Table 2.", "startOffset": 0, "endOffset": 7}, {"referenceID": 26, "context": "[34-38]) which are listed in Table 2.", "startOffset": 0, "endOffset": 7}, {"referenceID": 27, "context": "[34-38]) which are listed in Table 2.", "startOffset": 0, "endOffset": 7}], "year": 2015, "abstractText": "Background: Lung cancer was known as primary cancers and the survival rate of cancer is about 15%. Early detection of lung cancer is the leading factor in survival rate. All symptoms (features) of lung cancer do not appear until the cancer spreads to other areas. It needs an accurate early detection of lung cancer, for increasing the survival rate. For accurate detection, it need characterizes efficient features and delete redundancy features among all features.Feature selection is the problem of selecting informative features among all features. Materialsand Methods: Lung cancer database consist of 32 patient records with 57 features. This database collected by Hong and Youngand indexed in the University of California Irvine repository. Experimental contents include the extracted from the clinical data and X-ray data, etc. The data described 3 types of pathological lung cancers and all features are taking an integer value 0-3. In our study, new method is proposed for identify efficient features of lung cancer. It is based on Hyper-Heuristic. Results:We obtained an accuracy of 80.63% using reduced 11 feature set.The proposed method compare to the accuracy of 5 machine learning feature selections.The accuracy of these 5 methods are 60.94, 57.81, 68.75, 60.94 and 68.75. Conclusions: The proposed method has better performance with the highest level of accuracy. Therefore, the proposed model is recommended for identifying an efficient symptom of Disease. These finding are very important in health research, particularly in allocation of medical resources for patients who predicted as high-risks", "creator": null}}}