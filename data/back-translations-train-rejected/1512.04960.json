{"id": "1512.04960", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2015", "title": "A Light Touch for Heavily Constrained SGD", "abstract": "Projected stochastic gradient descent (SGD) is often the default choice for large-scale optimization in machine learning, but requires a projection after each update. For heavily-constrained objectives, we propose an efficient extension of SGD that stays close to the feasible region while only applying constraints probabilistically at each iteration. Theoretical analysis shows a good trade-off between per-iteration work and the number of iterations needed, indicating compelling advantages on problems with a large number of constraints onto which projecting is expensive. In MATLAB experiments, our algorithm successfully handles a large-scale real-world video ranking problem with tens of thousands of linear inequality constraints that was too large for projected SGD and stochastic Frank-Wolfe.", "histories": [["v1", "Tue, 15 Dec 2015 21:07:02 GMT  (55kb)", "https://arxiv.org/abs/1512.04960v1", null], ["v2", "Mon, 24 Oct 2016 20:30:25 GMT  (74kb)", "http://arxiv.org/abs/1512.04960v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew cotter", "maya gupta", "jan pfeifer"], "accepted": false, "id": "1512.04960"}, "pdf": {"name": "1512.04960.pdf", "metadata": {"source": "CRF", "title": "A Light Touch for Heavily Constrained SGD", "authors": ["Andrew Cotter", "Maya Gupta", "Jan Pfeifer"], "emails": ["acotter@google.com", "mayagupta@google.com", "janpf@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.04 960v 2 [cs.L G] 24 Oct 2"}, {"heading": "1 Introduction", "text": "In this paper, we set out a new strategy for solving problems that exist on a large scale, by introducing appropriate constraints to guarantee or encourage positive derivatives that affect all other countries. (...) We have to get used to the fact that it is not only about solving problems, but also about solving problems. (...) We have to get involved in solving problems that we cannot solve. (...) We have to get involved in solving problems that we cannot solve. (...) We have to get involved in solving problems that we cannot solve. (...) We have to get involved in solving problems that we cannot solve. (...) We have to get involved in solving problems. (...) We have to get involved. (...) We have to get involved. (...) We have to get involved. (...) We have to get involved. (...) We have to get involved. (...) We have to get involved in solving problems. (...) We have to get involved. (...) We have to get involved."}, {"heading": "2 Heavily Constrained SGD", "text": "Consider the limited optimization problem: min w: W: W: W: W: W: W: R and all gi: W: R are convex (our spelling is summarized in Table 1). We assume that W: Rd is a simple object, e.g. a sphere on which it is inexpensive to project, and that the more \"difficult\" aspects of the domain are specified by the gi: W: 0 constraints. Note that we consider constraints written in the form of arbitrary convex functions and not limited to e.g. linear or square constraints."}, {"heading": "2.1 FullTouch: A Relaxation with a Feasible Minimizer", "text": "(2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012. (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (. (2012). (2012. (2012). (2012).). (2012. (2012. (2012. (2012. (2012.).).)."}, {"heading": "2.2 Constraint-Dependence of \u03b3", "text": "The condition for answering this question is that the question of the d and m dependence of most theoretical questions in practice, so the question of the d and m dependencies of most theoretical questions.Box Consider the m = 2d box (w).Box Conseil (w).Box Conseil (s).Box Conseil (s).Box Conseil (s).Box Conseil (s).Box Conseil (s).Box Conseil (s).Box Conseil (s).Box Conseil (s).Box Conseil (s).Box Conseil (s).Box Conseil (s).Box Conseil (s).Box Conseil (s).Box Conseil (s)."}, {"heading": "3 A Light Touch", "text": "This section presents the main contribution of this paper: an algorithm that stochastically scans a small subset of m constraints for each SGD iteration, updates the parameters based on the subgradients of the constraints captured, and carefully learns the distribution over the constraints to achieve a net performance gain. We motivate the approach first by considering an oracle, then explain the algorithm and present convergence results for the convex (Section 3.2) and strongly convex (Section 3.3) cases."}, {"heading": "3.1 Wanted: An Oracle For the Most Violated Constraint", "text": "Since FullTouch assumes only a limited probability distribution (9 lines), it follows that if one had access to an oracle that identifies the most violated constraint, the overall convergence rate (including the cost of each iteration) could depend only on m (1), it motivates us to predict the most violated constraint, ideally better than linear-in-m rate.To this end, we ease the problem of minimizing h (w) (defined in Lemma 1) by replacing the maximum (0, g) distribution with a probability distribution (as in Clarkson et al)."}, {"heading": "3.2 LightTouch: Stochastic Constraint Handling", "text": "In order to achieve a good rate of convergence, we must both be expensive. (i.e.).................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3.3 MidTouch: Strong Convexity", "text": "(1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1 (1) (1) (1) (1) (1) (1) (1) (1 (1) (1) (1) (1 (1) (1 (1) (1) (1) (1 (1) (1 (1) (1) (1 (1) (1) (1) (1 (1) (1) (1) (1 (1) (1 (1) (1) (1 (1) (1) (1 (1) (1) ("}, {"heading": "4 Theoretical Comparison", "text": "Table 3 contains the same comparison (without Online Frank Wolfe) for all of these algorithms, which must apply all of these algorithms. - Table 3 contains the same comparison (without Online Frank Wolfe) for all of these algorithms. - Table 3 contains the same comparison (without Online Frank Wolfe) for all of these algorithms. - Table 3 contains the same comparisons (without Online Frank Wolfe) for all of the algorithms we refer to as LLO-FW, and the references cited for the complete statements. - Table 3 contains the same comparisons (without Online Frank Wolfe) for all of the algorithms. - Table 3 contains the same comparisons (without Online Frank Wolfe) for all of the algorithms."}, {"heading": "5 Practical Considerations", "text": "Algorithms 2 and 3 are primarily designed to be easy to analyze, but in real-world applications we recommend making a few tweaks to improve performance, the first of which is trivial: when optimizing a non-strongly convex lens, we use a decreasing w-update step size \u03b7 (t) w = \u03b7w / t, and for a strongly convex lens \u03b7 (t) w = \u03b7w / t. In both cases, we continue to use a constant p-update step size throup. This change is also included in Algorithm 4, as described in Section 5.2."}, {"heading": "5.1 Constraint Aggregation", "text": "A natural concern with algorithms 2 and 3 is that O (m) calculations are performed per iteration, even if only a few constraints are checked. If each constraint is expensive, this is a minor problem, as these costs are \"drowned out\" by checking the constraints. Our solution to this problem is simple: if the constraints are very cheap and the O (m) calculation costs are unfavorably compared with the cost of checking a handful of constraints, it can become a bottleneck. Our solution to this problem is simple: Turn a problem with a large number of cheap constraints into one with a smaller number of more expensive constraints. To this end, we divide the constraints 1,... m in m \u00b2 sets the size {Mi} to a maximum of m / m \u00b2, defining g (w) = maxj (w) = Mi gj (w), and then apply LightTouch or Touch to the constraints."}, {"heading": "5.2 Automatic Minibatching", "text": "It is not only a question of how we have behaved in recent years, but also of how we have behaved in recent years. (...) It is the question of how we have behaved in recent years. (...) It is the question of how we have behaved in recent years. (...) It is the question of how we have behaved in recent years. (...) It is the question of how we have behaved in recent years. (...) It is the question of how we have behaved in recent years. (...) It is the question of how we have behaved in recent years. (...) It is the question of how we have behaved in recent years. (...) It is the question of how we have behaved in recent years. \"(...) It is the question of how we have behaved in recent years."}, {"heading": "6 Experiments", "text": "We validated the performance of our practical variant of LightTouch (algorithm 4) on a YouTube ranking problem in the style of Joachims [2002], in which the task is to predict what a user will see next because he has just watched a particular video. In this setting, a user has just viewed a video a, was presented with a list of videos to watch next, and clicked b +, with b \u2212 the video immediately preceding b + in the list (if b + was the first list element, then the example is thrown out). We used an anonymous proprietary dataset consisting of n = 612 587 training pairs of feature vectors (x +, x \u2212), where x \u2212 is a vector of 12 features that summarizes the similarity between a and b +, and x \u2212 between a and b \u2212.We treat this as a default ranking problem where the goal is to estimate a function (f) (x) (x)."}, {"heading": "6.1 Implementations", "text": "We implemented all the algorithms in C + +. Before conducting our main experiments, we performed raw parameter searches on a power-of-four grid (i.e., 1 / 16, 1 / 4, 1, 4, 16,..). For each candidate value, we selected approximately 10,000 iterations and selected the parameter that seemed to lead to the fastest convergence in terms of objective functionality. (LightTouch Our implementation of LightTouch included all the proposed changes to Section 5, including the constraint aggregation approach of Section 5.1, although we did not use aggregation until our timing comparison (Section 6.3). For automatic minibatching, we used weighted averages of variance estimates as v + 1), including the constraint aggregation approaches of Section 5.1, although we did not use aggregation until our timing comparison. (Section 6.3) For the automatic minibatching, we used weighted averages of variance estimates as v + 1), including the constraint aggregation approaches of Section 5.1, although we did not use aggregation until our timing comparison. (We used the automated minibatch estimates as we felt the most recent estimates of the most recent value estimates). (Section 6.3)"}, {"heading": "6.2 Constraint-check Comparison", "text": "This year is the highest in the history of the country."}, {"heading": "7 Conclusions", "text": "We have proposed an efficient strategy for large-scale, highly constrained optimization, building on the work of Mahdavi et al. [2012], and analyzing their performance and showing that, asymptotically, our approach requires much less constraint testing in order to be able to converge. We are building on these theoretical results to suggest a practical variant. To this end, the most significant of these improvements is based on the observation that our algorithm takes steps based on three separate stochastic gradients and that it is advantageous to exchange the deviations in the computational costs of these three components. To this end, we propose heuristics for the dynamic selection of minibatch sizes to promote faster convergence at lower computational costs. Experiments on a real 4096-dimensional machine learning problem with 24,576 constraints and 612,587 training examples - too large for a QP-based implementation of projected effectiveness, as we have previously stated, that our method of optimization - in particular demonstrated that our practice is less effective than previously proposed."}, {"heading": "Acknowledgments", "text": "We thank Kevin Canini, Mahdi Milani Fard, Andrew Frigyik, Michael Friedlander and Seungil You for helpful discussions and proofreading of earlier drafts."}, {"heading": "A Mirror Descent", "text": "Descent of Mirrors [Nemirovski and Yudin, 1983, Beck and Teboulle, 2003] is a meta-algorithm for stochastic optimization (general, online repentance minimization), which performs gradient updates in relation to a meta-parameter that performs distance-generating function (d.g.f.). (The two most commonly used d.g.f.s are the square Euclidean norm and negative Shannon entropy, for which the resulting MD indicators are stochastic gradients (SGD) and a multiplicative updating algorithm (w). These are exactly the two d.g.f.s that our limited algorithm uses for the updates of w and p. Here we give a number of results that differ only slightly from the \"standard\" entries, starting with a statement about an online MD boundary that refracts from Sbro \u00b7 2011 and Theorem \u00b7 3."}, {"heading": "By Ho\u0308lder\u2019s inequality, \u3008w\u2032, w\u3009 \u2264 \u2016w\u2032\u2016 \u2016w\u2016\u2217. Also, \u03a8(w) = supv(\u3008v, w\u3009\u2212\u03a8\u2217(v)) is maximized when\u2207\u03a8\u2217(v) = w,", "text": "So the answer to the question of whether the question is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a"}, {"heading": "B SGD for Strongly-Convex Functions", "text": "We can achieve a faster convergence rate for SGD by using the increments (T + 1). (T + 1). (T + 1). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T). (T. (T). (T). (T). (T). (T). (T. (T). (T). (T. (T). (T. (T. (T). (T. (T. (T). (T. (T). (T. (T). (T). (T. (T). (T). (T. (T. (T. (T). (T). (T. (T). (T). (T). (T. (T). (T. (T. (T. (T.). (T. (T).. (T. (T.). (T"}, {"heading": "C Analyses of FullTouch and LightTouch", "text": "We start by proving that when we are sufficiently large, we optimize the relaxed target and project the resulting solution, we approach the optimum of the limited target. \u2212 W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) -W (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w) (w (w) (w) (w) (w) (w (w) (w (w) (w) (w (w) (w) (w (w) (w) (w (w) (w) (w (w) (w) (w (w (w) (w (w) (w) (w (w) (w (w (w) (w) (w) (w (w) (w (w) (w (w (w) (w) (w) (w (w (w) (w) (w) (w (w (w) (w (w) (w) (w (w) (w (w) (w (w) (w) (w (w (w) (w (w) (w (w) (w) (w ("}, {"heading": "D Analysis of MidTouch", "text": "We now move on to the analysis of our LightTouch variant for the two phases: Lemma 6. We assume that the conditions of Lemma 1 (w) = maxi (w) = maxi (w) = maxi (w) = maxi (w) = maxi (w) = maxi (w)."}], "references": [{"title": "Application of the back propagation neural network algorithm with monotonicity constraints for two-group classification problems", "author": ["N.P. Archer", "S. Wang"], "venue": "Decision Sciences,", "citeRegEx": "Archer and Wang.,? \\Q1993\\E", "shortCiteRegEx": "Archer and Wang.", "year": 1993}, {"title": "Projection Algorithms and Monotone Operators", "author": ["H.H. Bauschke"], "venue": "Ph.D. Thesis, Simon Fraser University,", "citeRegEx": "Bauschke.,? \\Q1996\\E", "shortCiteRegEx": "Bauschke.", "year": 1996}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "author": ["A. Beck", "M. Teboulle"], "venue": "Oper. Res. Lett.,", "citeRegEx": "Beck and Teboulle.,? \\Q2003\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2003}, {"title": "Sublinear optimization for machine learning", "author": ["K.L. Clarkson", "E. Hazan", "D.P. Woodruff"], "venue": "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Clarkson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Clarkson et al\\.", "year": 2010}, {"title": "Monotone and partially monotone neural networks", "author": ["H. Daniels", "M. Velikova"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "Daniels and Velikova.,? \\Q2010\\E", "shortCiteRegEx": "Daniels and Velikova.", "year": 2010}, {"title": "On Bernstein-type inequalities for martingales", "author": ["K. Dzhaparidze", "J.H. van Zanten"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "Dzhaparidze and Zanten.,? \\Q2001\\E", "shortCiteRegEx": "Dzhaparidze and Zanten.", "year": 2001}, {"title": "Playing non-linear games with linear oracles. In FOCS, pages 420\u2013428", "author": ["D. Garber", "E. Hazan"], "venue": "IEEE Computer Society,", "citeRegEx": "Garber and Hazan.,? \\Q2013\\E", "shortCiteRegEx": "Garber and Hazan.", "year": 2013}, {"title": "Optimized regression for efficient function evaluation", "author": ["E.K. Garcia", "R. Arora", "M. Gupta"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "Garcia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Garcia et al\\.", "year": 2012}, {"title": "Monotonic calibrated interpolated look-up", "author": ["M.R. Gupta", "A. Cotter", "J. Pfeifer", "K. Voevodski", "K. Canini", "A. Mangylov", "W. Moczydlowski", "A. van Esbroeck"], "venue": "tables. JMLR,", "citeRegEx": "Gupta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2016}, {"title": "Projection-free online learning", "author": ["E. Hazan", "S. Kale"], "venue": "In ICML\u201912,", "citeRegEx": "Hazan and Kale.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2012}, {"title": "Revisiting Frank-Wolfe: Projection-free sparse convex optimization", "author": ["M. Jaggi"], "venue": "In ICML\u201913,", "citeRegEx": "Jaggi.,? \\Q2013\\E", "shortCiteRegEx": "Jaggi.", "year": 2013}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "In KDD\u201902,", "citeRegEx": "Joachims.,? \\Q2002\\E", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In NIPS\u201913,", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "Stochastic gradient descent with only one projection", "author": ["M. Mahdavi", "T. Yang", "R. Jin", "S. Zhu", "J. Yi"], "venue": "In NIPS\u201912,", "citeRegEx": "Mahdavi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2012}, {"title": "Lecture notes: Interior point polynomial time methods in convex programming", "author": ["A. Nemirovski"], "venue": null, "citeRegEx": "Nemirovski.,? \\Q2004\\E", "shortCiteRegEx": "Nemirovski.", "year": 2004}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Generalized constraint neural network regression model subject to linear priors", "author": ["Y. Qu", "B. Hu"], "venue": "IEEE Trans. on Neural Networks,", "citeRegEx": "Qu and Hu.,? \\Q2011\\E", "shortCiteRegEx": "Qu and Hu.", "year": 2011}, {"title": "Optimization, learning, and games with predictable sequences", "author": ["A. Rakhlin", "K. Sridharan"], "venue": "In NIPS\u201913,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2013}, {"title": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Least squares isotonic regression in two dimensions", "author": ["J. Spouge", "H. Wan", "W.J. Wilbur"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Spouge et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Spouge et al\\.", "year": 2003}, {"title": "On the universality of online mirror descent", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "In NIPS\u201911,", "citeRegEx": "Srebro et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2011}, {"title": "On the implementation of a primal-dual interior point filter line search algorithm for large-scale nonlinear programming", "author": ["A. W\u00e4chter", "L.T. Biegler"], "venue": "Mathematical Programming,", "citeRegEx": "W\u00e4chter and Biegler.,? \\Q2006\\E", "shortCiteRegEx": "W\u00e4chter and Biegler.", "year": 2006}, {"title": "Random Multi-Constraint Projection: Stochastic Gradient Methods for Convex Optimization with Many Constraints", "author": ["M. Wang", "Y. Chen", "J. Liu", "Y. Gu"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML\u201903,", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Archer and Wang, 1993, Sill, 1998, Spouge et al., 2003, Daniels and Velikova, 2010, Gupta et al., 2016]. Submodular functions can often be learned from noisy examples by imposing constraints to ensure submodularity holds. Another example occurs when one wishes to guarantee that a classifier will correctly label certain \u201ccanonical\u201d examples, which can be enforced by constraining the function values on those examples. See Qu and Hu [2011] for some other examples of constraints useful in machine learning.", "startOffset": 0, "endOffset": 441}, {"referenceID": 0, "context": "Archer and Wang, 1993, Sill, 1998, Spouge et al., 2003, Daniels and Velikova, 2010, Gupta et al., 2016]. Submodular functions can often be learned from noisy examples by imposing constraints to ensure submodularity holds. Another example occurs when one wishes to guarantee that a classifier will correctly label certain \u201ccanonical\u201d examples, which can be enforced by constraining the function values on those examples. See Qu and Hu [2011] for some other examples of constraints useful in machine learning. However, these practical uses of constraints in machine learning are impractical in that the number of constraints may be very large, and scale poorly with the number of features d or number of training samples n. In this paper we propose a new strategy for tackling such heavily-constrained problems, with guarantees and compelling convergence rates for large-scale convex problems. A standard approach for large-scale empirical risk minimization is projected stochastic gradient descent [e.g. Zinkevich, 2003, Nemirovski et al., 2009]. Each SGD iteration is computationally cheap, and the algorithm converges quickly to a solution good enough for machine learning needs. However, this algorithm requires a projection onto the feasible region after each stochastic gradient step, which can be prohibitively slow if there are many non-trivial constraints, and is not easy to parallelize. Recently, Frank-Wolfe-style algorithms [e.g. Hazan and Kale, 2012, Jaggi, 2013] have been proposed that remove the projection, but require a constrained linear optimization at each iteration. We propose a new strategy for large-scale constrained optimization that, like Mahdavi et al. [2012], moves the constraints into the objective and finds an approximate solution of the resulting unconstrained problem, projecting the (potentially-infeasible) result onto the constraints only once, at the end.", "startOffset": 0, "endOffset": 1688}, {"referenceID": 13, "context": "Despite this, LightTouch was roughly as fast as the Mahdavi et al. [2012]-like algorithm FullTouch.", "startOffset": 52, "endOffset": 74}, {"referenceID": 12, "context": "p \u03bc Remembered gradient coordinates [Johnson and Zhang, 2013] k Minibatch size in LightTouch\u2019s p-update w\u0304 Average iterate w\u0304 = ( \u2211T t=1 w )/T consider constraints written in terms of arbitrary convex functions, and are not restricted to e.", "startOffset": 36, "endOffset": 61}, {"referenceID": 12, "context": "p \u03bc Remembered gradient coordinates [Johnson and Zhang, 2013] k Minibatch size in LightTouch\u2019s p-update w\u0304 Average iterate w\u0304 = ( \u2211T t=1 w )/T consider constraints written in terms of arbitrary convex functions, and are not restricted to e.g. only linear or quadratic constraints. 2.1 FullTouch: A Relaxation with a Feasible Minimizer We build on the approach of Mahdavi et al. [2012] to relax Equation 1.", "startOffset": 37, "endOffset": 385}, {"referenceID": 13, "context": "This algorithm\u2014our starting point\u2014is similar to those proposed by Mahdavi et al. [2012], and like their algorithms only contains a single projection, at the end, projecting the potentially-infeasible result vector w\u0304.", "startOffset": 66, "endOffset": 88}, {"referenceID": 13, "context": "This algorithm\u2014our starting point\u2014is similar to those proposed by Mahdavi et al. [2012], and like their algorithms only contains a single projection, at the end, projecting the potentially-infeasible result vector w\u0304. Hyperparameters: T , \u03b7 1 Initialize w \u2208 W arbitrarily 2 For t = 1 to T : 3 Sample \u2206\u030c // stochastic subgradient of f(w) 4 Let \u2206\u030c w = \u2206\u030c + \u03b3\u2207\u030cmax{0, g(w(t))} 5 Update w = \u03a0w(w \u2212 \u03b7\u2206\u030c w ) // \u03a0w projects its argument onto W w.r.t. \u2016\u00b7\u20162 6 Average w\u0304 = ( \u2211T t=1 w )/T 7 Return \u03a0g(w\u0304) // optional if small constraint violations are acceptable If \u03b3 > Lf/\u03c1, then for any infeasible w (i.e. for which g(w) > 0): h (w) > h (\u03a0g (w)) = f (\u03a0g (w)) and \u2016w \u2212\u03a0g (w)\u20162 \u2264 h (w)\u2212 h (\u03a0g (w)) \u03b3\u03c1\u2212 Lf , where \u03a0g (w) is the projection of w onto the set {w \u2208 W : g(w) \u2264 0} w.r.t. the Euclidean norm. Proof. In Appendix C. The strategy of applying SGD to h(w), detailed in Algorithm 1, which we call FullTouch, has the same \u201cflavor\u201d as the algorithms proposed by Mahdavi et al. [2012], and we use it as a baseline comparison point for our other algorithms.", "startOffset": 66, "endOffset": 976}, {"referenceID": 1, "context": "This requirement is related to the linear regularity assumption introduced by Bauschke [1996], and considered recently by Wang et al.", "startOffset": 78, "endOffset": 94}, {"referenceID": 1, "context": "This requirement is related to the linear regularity assumption introduced by Bauschke [1996], and considered recently by Wang et al. [2015]. 3 A Light Touch This section presents the main contribution of this paper: an algorithm that stochastically samples a small subset of the m constraints at each SGD iteration, updates the parameters based on the subgradients of the sampled constraints, and carefully learns the distribution over the constraints to produce a net performance gain.", "startOffset": 78, "endOffset": 141}, {"referenceID": 1, "context": "This requirement is related to the linear regularity assumption introduced by Bauschke [1996], and considered recently by Wang et al. [2015]. 3 A Light Touch This section presents the main contribution of this paper: an algorithm that stochastically samples a small subset of the m constraints at each SGD iteration, updates the parameters based on the subgradients of the sampled constraints, and carefully learns the distribution over the constraints to produce a net performance gain. We first motivate the approach by considering an oracle, then explain the algorithm and present convergence results for the convex (Section 3.2) and strongly convex (Section 3.3) cases. 3.1 Wanted: An Oracle For the Most Violated Constraint Because FullTouch only needs to differentiate the most violated constraint at each iteration, it follows that if one had access to an oracle that identified the most-violated constraint, then the overall convergence rate (including the cost of each iteration) could only depend on m through \u03b3. This motivates us to learn to predict the most-violated constraint, ideally at a significantly better than linear-in-m rate. To this end, we further relax the problem of minimizing h(w) (defined in Lemma 1) by replacing \u03b3max(0, g(w)) with maximization over a probability distribution (as in Clarkson et al. [2010]), yielding the equivalent convex-linear 5", "startOffset": 78, "endOffset": 1337}, {"referenceID": 12, "context": "For this reason, in addition to minibatching, we center the stochastic gradients, as is done by the well-known SVRG algorithm [Johnson and Zhang, 2013], by storing a gradient estimate \u03b3\u03bc with \u03bc \u2208 R, at each iteration sampling a set S of size |S| = k uniformly without replacement, and computing: \u2206\u0302p = \u03b3\u03bc+ \u03b3m k \u2211", "startOffset": 126, "endOffset": 151}, {"referenceID": 6, "context": "4 Theoretical Comparison Table 2 compares upper bounds on the convergence rates and per-iteration costs when applied to a convex (but not necessarily strongly convex) problem for LightTouch, FullTouch, projected SGD, the online Frank-Wolfe algorithm of Hazan and Kale [2012], and a Frank-Wolfe-like online algorithm for optimization over a polytope [Garber and Hazan, 2013].", "startOffset": 349, "endOffset": 373}, {"referenceID": 6, "context": "3], and Garber and Hazan [2013, Theorem 2]. Notice that because this table compares upper bounds to upper bounds, subsequent work may improve these bounds further. #Iterations to achieve #Constraint checks to achieve \u01eb-suboptimality \u01eb-suboptimality FullTouch \u03b3 D w \u01eb2 m\u03b3D w \u01eb2 LightTouch (lnm)\u03b3 D w \u01eb2 (lnm)\u03b3D w \u01eb2 + m(lnm) /2\u03b3 /2D3 w \u01eb/2 Projected SGD D 2 w \u01eb2 N/A (projection) Online Frank-Wolfe D 3 w \u01eb3 N/A (linear optimization) LLO-FW d\u03bd D w \u01eb2 N/A (local linear oracle) then: E [ \u2016\u03a0g(w\u0304)\u2212 w\u20162 ] \u2264 E [ \u2016w\u0304 \u2212 w\u20162 ] \u2264 \u01eb, where w\u2217 = argmin{w\u2208W:\u2200i.gi(w)\u22640} f(w) is the optimal constraint-satisfying reference vector. Proof. In Appendix D. Notice that the above theorem bounds not the suboptimality of \u03a0g(w\u0304), but rather its squared Euclidean distance from w\u2217, for which reason the denominator of the highest order term depends on \u03bb rather than \u03bb. Like Theorem 1 in the non-strongly convex case, the dominant terms above, both in terms of the total number of iterations and number of constraint checks, match the usual 1/\u01eb convergence rate for unconstrained strongly-convex SGD with an additional \u03b3 lnm factor, while the lower-order terms have a worse m-dependence. As before, fewer constraint checks will be performed per iteration as \u01eb shrinks, reaching a constant number (on average) once \u01eb is on the order of 1/m. 4 Theoretical Comparison Table 2 compares upper bounds on the convergence rates and per-iteration costs when applied to a convex (but not necessarily strongly convex) problem for LightTouch, FullTouch, projected SGD, the online Frank-Wolfe algorithm of Hazan and Kale [2012], and a Frank-Wolfe-like online algorithm for optimization over a polytope [Garber and Hazan, 2013].", "startOffset": 8, "endOffset": 1593}, {"referenceID": 11, "context": "6 Experiments We validated the performance of our practical variant of LightTouch (Algorithm 4) on a YouTube ranking problem in the style of Joachims [2002], in which the task is to predict what a user will watch next, given that they have just viewed a certain video.", "startOffset": 141, "endOffset": 157}, {"referenceID": 21, "context": "ProjectedSGD We implemented Euclidean projections onto lattice monotonicity constraints using IPOPT [W\u00e4chter and Biegler, 2006] to optimize the resulting sparse 4096-dimensional quadratic program.", "startOffset": 100, "endOffset": 127}, {"referenceID": 7, "context": "ApproxSGD This is an approximate projected SGD implementation using the fast approximate update procedure described in Gupta et al. [2016], which is an active set method that, starting from the current iterate, moves along the boundary of the feasible region, adding constraints to the active set as they are encountered, until the desired step is exhausted (this is reminiscent of the local linear oracles considered by Garber and Hazan [2013]).", "startOffset": 119, "endOffset": 139}, {"referenceID": 6, "context": "[2016], which is an active set method that, starting from the current iterate, moves along the boundary of the feasible region, adding constraints to the active set as they are encountered, until the desired step is exhausted (this is reminiscent of the local linear oracles considered by Garber and Hazan [2013]).", "startOffset": 289, "endOffset": 313}, {"referenceID": 13, "context": "7 Conclusions We have proposed an efficient strategy for large-scale heavily constrained optimization, building on the work of Mahdavi et al. [2012], and analyze its performance, demonstrating that, asymptotically, our approach requires many fewer constraint checks in order to converge.", "startOffset": 127, "endOffset": 149}, {"referenceID": 2, "context": "\u2225 \u03b1\u2217 1\u2212 \u03b4\u03c3 Probability that \u03c3 bound holds 1\u2212 \u03b4\u03c3w Probability that \u03c3w bound holds 1\u2212 \u03b4\u03c3\u03b1 Probability that \u03c3\u03b1 bound holds A Mirror Descent Mirror descent [Nemirovski and Yudin, 1983, Beck and Teboulle, 2003] is a meta-algorithm for stochastic optimization (more generally, online regret minimization) which performs gradient updates with respect to a meta-parameter, the distance generating function (d.g.f.). The two most widely-used d.g.f.s are the squared Euclidean norm and negative Shannon entropy, for which the resulting MD instantiations are stochastic gradient descent (SGD) and a multiplicative updating algorithm, respectively. These are precisely the two d.g.f.s which our constrained algorithm will use for the updates of w and p. We\u2019ll here give a number of results which differ only slightly from \u201cstandard\u201d ones, beginning with a statement of an online MD bound adapted from Srebro et al. [2011]: Theorem 3.", "startOffset": 181, "endOffset": 910}, {"referenceID": 14, "context": "The convergence rate will be determined from a saddle-point bound, which we derive from Corollary 2 by following Nemirovski et al. [2009], Rakhlin and Sridharan [2013], and simply applying it twice: Corollary 3.", "startOffset": 113, "endOffset": 138}, {"referenceID": 14, "context": "The convergence rate will be determined from a saddle-point bound, which we derive from Corollary 2 by following Nemirovski et al. [2009], Rakhlin and Sridharan [2013], and simply applying it twice: Corollary 3.", "startOffset": 113, "endOffset": 168}, {"referenceID": 14, "context": "This is a convex-concave saddle-point problem, which we will optimize by playing two convex optimization algorithms against each other, as in Nemirovski et al. [2009], Rakhlin and Sridharan [2013].", "startOffset": 142, "endOffset": 167}, {"referenceID": 14, "context": "This is a convex-concave saddle-point problem, which we will optimize by playing two convex optimization algorithms against each other, as in Nemirovski et al. [2009], Rakhlin and Sridharan [2013]. By Corollary 2, with probability 1\u2212 \u03b4\u03c3w \u2212 \u03b4 and 1\u2212 \u03b4\u03c3\u03b1 \u2212 \u03b4, respectively: 1 T T \u2211", "startOffset": 142, "endOffset": 197}, {"referenceID": 13, "context": "Like the algorithm itself, the convergence rate is little different from that found by Mahdavi et al. [2012] (aside from the bound on \u2016w\u0304 \u2212\u03a0g(w\u0304)\u20162), and is included here only for completeness.", "startOffset": 87, "endOffset": 109}, {"referenceID": 12, "context": "2 Analysis of LightTouch Because we use the reduced-variance algorithm of Johnson and Zhang [2013], and therefore update the remembered gradient \u03bc one random coordinate at a time, we must first bound the maximum number of iterations over which a coordinate can go un-updated: Lemma 4.", "startOffset": 74, "endOffset": 99}], "year": 2016, "abstractText": "Minimizing empirical risk subject to a set of constraints can be a useful strategy for learning restricted classes of functions, such as monotonic functions, submodular functions, classifiers that guarantee a certain class label for some subset of examples, etc. However, these restrictions may result in a very large number of constraints. Projected stochastic gradient descent (SGD) is often the default choice for large-scale optimization in machine learning, but requires a projection after each update. For heavily-constrained objectives, we propose an efficient extension of SGD that stays close to the feasible region while only applying constraints probabilistically at each iteration. Theoretical analysis shows a compelling trade-off between per-iteration work and the number of iterations needed on problems with a large number of constraints.", "creator": "LaTeX with hyperref package"}}}