{"id": "1512.04973", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2015", "title": "An Operator for Entity Extraction in MapReduce", "abstract": "Dictionary-based entity extraction involves finding mentions of dictionary entities in text. Text mentions are often noisy, containing spurious or missing words. Efficient algorithms for detecting approximate entity mentions follow one of two general techniques. The first approach is to build an index on the entities and perform index lookups of document substrings. The second approach recognizes that the number of substrings generated from documents can explode to large numbers, to get around this, they use a filter to prune many such substrings which do not match any dictionary entity and then only verify the remaining substrings if they are entity mentions of dictionary entities, by means of a text join. The choice between the index-based approach and the filter &amp; verification-based approach is a case-to-case decision as the best approach depends on the characteristics of the input entity dictionary, for example frequency of entity mentions. Choosing the right approach for the setting can make a substantial difference in execution time. Making this choice is however non-trivial as there are parameters within each of the approaches that make the space of possible approaches very large. In this paper, we present a cost-based operator for making the choice among execution plans for entity extraction. Since we need to deal with large dictionaries and even larger large datasets, our operator is developed for implementations of MapReduce distributed algorithms.", "histories": [["v1", "Tue, 15 Dec 2015 21:23:20 GMT  (51kb,D)", "http://arxiv.org/abs/1512.04973v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.DB cs.CL", "authors": ["ndapandula nakashole"], "accepted": false, "id": "1512.04973"}, "pdf": {"name": "1512.04973.pdf", "metadata": {"source": "CRF", "title": "An Operator for Entity Extraction in MapReduce", "authors": ["Ndapandula Nakashole"], "emails": ["ndapa@cs.cmu.edu"], "sections": [{"heading": null, "text": "Categories and topic descriptions H.1.0 [Information Systems]: Models and Principles - GeneralKeywords Approximate Entity Extraction, MapReduce, Text Analytics Optimization"}, {"heading": "1. INTRODUCTION", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "2. SEMANTICS OF DICTIONARY ENTITY EXTRACTION", "text": "In this section we formally define the similarity as: Jaccard sim (e, ds (i)), different similarity functions can be used depending on the desired semantics."}, {"heading": "3. APPROXIMATE MENTION ALGORITHMS", "text": "IN MAPREDUCEOnce we have defined the semantics, we can introduce algorithms for approximate mention of the extraction. For each algorithm, we briefly describe the individual machine version before explaining how adaptation to MapReduce is realized by using MapReduce constructs. Then, we explain the influence of MapReduce constructs on the performance of each algorithm."}, {"heading": "3.1 MapReduce SSJoin", "text": "In this case it is so that it concerns a kind of conspiracy theatrical, which is based on a multiplicity of similarity functions. Thus, for example, it is possible to decouple the links between the links and discuss the availability of the links. The linking of the links with the links between the links of the links and the linking of the links with the linking of the links and the linking of the links is such that the linking between the linking of the links and the linking of the linking with the linking of the links and the linking of the linking of the linking and the linking of the linking of the linking of the linking of the linking of the linking of the linking of the linking and the linking of the linking of the linking of the linking of the linking of the linking of the linking and the linking of the linking of the linking of the linking of the linking of the linking of the linking of the linking and the linking of the linking of the linking of the linking of the linking of the"}, {"heading": "3.2 MapReduce Index on Entities", "text": "The Entity Approach Index generates an index for words from the dictionary entities. It then generates all substrings and queries the index for similar entities. We adjust the index-based approach to entities as follows: First, we create the index for the entities as a separate MapReduce job. The index is then sent to each mapper node. The index used may vary by application domain. Suppose it is a base index with inverted lists per word and then for each query substring q retrieves all lists that match the words in q. Unifying the lists are the candidate entities mentioned by q. Each of the candidate entities is then verified to determine whether they are genuine mentions. The MapReduce algorithm is outlined in Figure 3. Although the index is created in separate MapReduce, it is not a significant part of the document as the execution time is much shorter."}, {"heading": "3.3 MapReduce ISHFilter & SSJoin", "text": "In fact, it is so that it will be able to hide and that it will be able to hide, \"he said.\" It is very important for us that we are able to be able to find ourselves in a position, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "3.4 MapReduce Index on Documents", "text": "The fourth approach to the approximate extraction of entities on MapReduce is to create an index for the entire document collection. Input to the mappers are partitions of the document index, the dictionary of entities is sent to each mapper. Each dictionary is treated as a query placed on the index. Each mapper searches its part of the document index for all entity queries. The full list of mentions is the union of the mentions found by each mapper. If the dictionary of entities does not fit into memory, the algorithm performs several transitions over the document index. Creating an index on the entire corpus is a costly operation, unlike the approach that constructs the index on the dictionary of entities, since the dictionary of entities is usually orders of magnitude smaller than the document collection. As the index on the document collection is not normally available in advance, and the rest of the study is not expensive."}, {"heading": "3.5 Operator Algorithms", "text": "We have already removed the Index on Documents algorithm from the algorithms eligible for the EEJoin operator and removed the MapReduce SSJoin algorithm due to its limitation on generating all substrings; instead, we retain the optimized version of SSJoin, the MapReduce ISHFilter & SSJoin approach; we also retain the MapReduce Index on Entities, but instead of generating all substrings from documents, we use the filter to effectively apply the MapReduce ISHFilter & Index on Entities approach."}, {"heading": "4. COST MODELS", "text": "After describing the algorithms for the EEJoin operator, we now present the cost model that is used to estimate the performance of each algorithm. On the basis of a cost model, we can automatically determine which of the algorithms performs best for a given input dictionary and document capture. We consider two objective functions, one for the entire work done and another for the processing time. The processing time of the index-on-entities approach is composed of two main components, the first being the part-time search time specified by Clookup in Definition 3. The total processing time is evenly distributed among the mappers, due to the mapping burden distribution, i.e. the total processing time for the signature. The second is the number of iterations that are made over the entire processing time."}, {"heading": "5. OPTIMIZATION", "text": "The planning space consists of two core algorithms. However, each of the algorithms can be instantiated with several signature schemes. Furthermore, the dictionary is partitioned in such a way that a fraction of the units is processed by one of the core approaches and the rest by another core approach. Furthermore, any combination of the different signature schemes forms a possible hybrid approach, thereby creating a large space of possible plans."}, {"heading": "5.1 Plan space", "text": "The EE operator optimizes entity extraction by dividing entities into mention frequency categories. Intuition is that each of the algorithms for entities of particular mention frequencies performs better than the other approach. Therefore, for a certain input dictionary and text collection, a fraction of entities are processed by the index-based approach, for a certain signature scheme the remaining fraction is processed by the filter and verification approach, for a certain signature scheme, not necessarily the same as for the index-based approach. Therefore, a plan for the EE operator is a combination of the index-based approach and the filter and verification approach. The cost of a plan where an \u03b1 fraction of entities is processed by the index-based approach, and \u03b2-portions are used by the filter and verification approach Y is a combination of the index-based approach and the filter and verification approach. The cost of a plan where an \u03b1-portion of entities is processed by the index-based approach, and \u03b2-portions are processed by the filter and verification approach Y is processed by the filter and Y are processed by the Y are processed by the filter and verification approach."}, {"heading": "5.2 Searching the Plan Space", "text": "This year it has come to the point where it will be able to retaliate, \"he says.\" It's as if it will be able to retaliate, \"he says.\" It's as if it will be able to retaliate, \"he says."}, {"heading": "6. RELATED WORK", "text": "The main difference to our approach is that we do not stick to the index approach, but instead allow the operator to make cost-based decisions when selecting the best implementation of approximate units. However, in terms of optimizing text-centric tasks, [14] introduced an optimizer to choose between query-based and crawl-based methods for various text analytic tasks, and the optimizer adaptively selects the best execution strategy, while the EE operator is specifically geared toward implementing MapReduce implementations.Afrati and Ullman [2] investigated the problem of efficient connections in MapReduce. Vernica et al. [24] developed algorithms to optimize connections in Mapce systems to reduce costs."}, {"heading": "7. REFERENCES", "text": "Twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twel"}], "references": [{"title": "HadoopDB: An architectural hybrid of mapreduce and dbms technologies for analytical workloads", "author": ["A. Abouzeid", "K. Bajda-Pawlikowski", "D.J. Abadi", "A. Rasin", "A. Silberschatz"], "venue": "In PVLDB,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Optimizing joins in a map-reduce environment", "author": ["Foto N. Afrati", "Jeffrey D. Ullman"], "venue": "In Proceedings of EDBT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "On indexing error-tolerant set containment", "author": ["Parag Agrawal", "Arvind Arasu", "Raghav Kaushik"], "venue": "In Proceedings of the ACM SIGMOD,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Efficient exact set similarity joins", "author": ["A Arasu", "V Ganti", "R. Kaushik"], "venue": "In Proceedings of VLDB,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "An efficient filter for approximate membership checking", "author": ["Kaushik Chakrabarti", "Surajit Chaudhuri", "Venkatesh Ganti", "Dong Xin"], "venue": "In Proceedings of the ACM SIGMOD,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Efficient Batch Top-k Search for Dictionary-based Entity Recognition", "author": ["Amit Chandel", "P.C. Nagesh", "Sunita Sarawagi"], "venue": "In Proceedings of ICDE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Exploiting dictionaries in named entity extraction: combining semi-markov extraction processes and data integration methods", "author": ["William W. Cohen", "Sunita Sarawagi"], "venue": "In Proceedings of KDD,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Mining Document Collections to Facilitate Accurate Approximate Entity Matching", "author": ["Surajit Chaudhuri", "Venkatesh Ganti", "Dong Xin"], "venue": "In Proceedings of VLDB,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "A Primitive Operator for Similarity Joins in Data Cleaning", "author": ["Surajit Chaudhuri", "Venkatesh Ganti", "Raghav Kaushik"], "venue": "In Proceedings of ICDE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat Mapreduce"], "venue": "In Proceedings of the OSDI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Hadoop++: Making a yellow elephant run like a cheetah, without it even noticing", "author": ["Jens Dittrich", "Jorge-Arnulfo Quian\u00e9-Rui", "Alekh Jindal", "Yagiz Kargin", "Vinay Setty", "J\u00f6rg Schad"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In Proceedings of VLDB,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Efficient approximate search on string collections", "author": ["Hadjieleftheriou M", "C. Li"], "venue": "In Proceedings of PVLDB,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "To search or to crawl?: towards a query optimizer for text-centric tasks", "author": ["Panagiotis G. Ipeirotis", "Eugene Agichtein", "Pranay Jain", "Luis Gravano"], "venue": "In Proceedings of the ACM SIGMOD,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Automatic Optimization for MapReduce", "author": ["Eaman Jahani", "Michael J. Cafarella", "Christopher Re"], "venue": "Programs PVLDB,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Record linkage: similarity measures and algorithms", "author": ["Nick Koudas", "Sunita Sarawagi", "Divesh Srivastava"], "venue": "In Proceedings of the ACM SIGMOD,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Efficient processing of joins on set-valued attributes", "author": ["N. Mamoulis"], "venue": "In Proceedings of the ACM SIGMOD,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Adaptive algorithms for set containment joins", "author": ["S. Melnik", "H. Garcia-Molina"], "venue": "In ACM Trans. Database Syst.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Scalable knowledge harvesting with high precision and high recall", "author": ["Ndapandula Nakashole", "Martin Theobald", "Gerhard Weikum"], "venue": "In Proceedings of the Forth International Conference on Web Search and Web Data Mining, WSDM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Fine-grained Semantic Typing of Emerging Entities", "author": ["N. Nakashole", "T. Tylenda", "G. Weikum"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Real-time population of knowledge bases: opportunities and challenges", "author": ["N. Nakashole", "G. Weikum"], "venue": "In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Efficient set joins on similarity predicates", "author": ["S. Sarawagi", "A. Kirpal"], "venue": "In Proceedings of the ACM SIGMOD,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Efficient parallel set-similarity joins using MapReduce", "author": ["Rares Vernica", "Michael J. Carey", "Chen Li"], "venue": "In Proceedings of the ACM SIGMOD,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Big Data Methods for Computational Linguistics In IEEE Data", "author": ["Gerhard Weikum", "Johannes Hoffart", "Ndapandula Nakashole", "Marc Spaniol", "Fabian M. Suchanek", "Mohamed Amir Yosef"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Map-reduce-merge: simplified relational data processing on large clusters", "author": ["Hung-chih Yang", "Ali Dasdan", "Ruey-Lung Hsiao", "D. Stott Parker"], "venue": "In Proceedings of the ACM SIGMOD,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Improving mapreduce performance in heterogeneous environments", "author": ["Matei Zaharia", "Andy Konwinski", "Anthony D. Joseph", "Randy Katz", "Ion Stoica"], "venue": "In Proceedings of the OSDI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "A mention in a document may miss some of the words of the entity or it may have extra words not found in the entity name in the dictionary, therefore it is crucial for algorithms to detect approximate mentions of entities in addition to exact mentions [3, 8, 7] Finding entity mentions entails finding substrings in a document sequence such that the substring matches an entity in the dictionary.", "startOffset": 252, "endOffset": 261}, {"referenceID": 7, "context": "A mention in a document may miss some of the words of the entity or it may have extra words not found in the entity name in the dictionary, therefore it is crucial for algorithms to detect approximate mentions of entities in addition to exact mentions [3, 8, 7] Finding entity mentions entails finding substrings in a document sequence such that the substring matches an entity in the dictionary.", "startOffset": 252, "endOffset": 261}, {"referenceID": 6, "context": "A mention in a document may miss some of the words of the entity or it may have extra words not found in the entity name in the dictionary, therefore it is crucial for algorithms to detect approximate mentions of entities in addition to exact mentions [3, 8, 7] Finding entity mentions entails finding substrings in a document sequence such that the substring matches an entity in the dictionary.", "startOffset": 252, "endOffset": 261}, {"referenceID": 2, "context": "The first approach is to build an index on the entities and perform index lookups of document substrings [3, 6].", "startOffset": 105, "endOffset": 111}, {"referenceID": 5, "context": "The first approach is to build an index on the entities and perform index lookups of document substrings [3, 6].", "startOffset": 105, "endOffset": 111}, {"referenceID": 4, "context": "The second approach recognizes that the number of substrings generated from documents can explode to large numbers, to get around this, they use a filter to prune many such substrings which do not match any dictionary entity [5] [8] and then only verify the remaining substrings if they are entity mentions of dictionary entities, by means of a text join.", "startOffset": 225, "endOffset": 228}, {"referenceID": 7, "context": "The second approach recognizes that the number of substrings generated from documents can explode to large numbers, to get around this, they use a filter to prune many such substrings which do not match any dictionary entity [5] [8] and then only verify the remaining substrings if they are entity mentions of dictionary entities, by means of a text join.", "startOffset": 229, "endOffset": 232}, {"referenceID": 12, "context": "A commonly used similarity measure is Jaccard similarity [13, 16], defined as: JaccSim(e, ds(i)) = e\u2229ds(i) e\u222ads(i) .", "startOffset": 57, "endOffset": 65}, {"referenceID": 15, "context": "A commonly used similarity measure is Jaccard similarity [13, 16], defined as: JaccSim(e, ds(i)) = e\u2229ds(i) e\u222ads(i) .", "startOffset": 57, "endOffset": 65}, {"referenceID": 8, "context": "[9] introduced the notion of set similarity join (SSJoin) for identifying similar strings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] to prune a large number of substrings that are obvious non-mentions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] eveloped an operator-centric approach for for set-similarity joins.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] uses an invertedindex approach to compute set overlap string similarities.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "In terms of optimization for text-centric tasks, [14] introduced an optimizer for choosing between query-based and crawl-based method for various text-analytics tasks in a cots-based way.", "startOffset": 49, "endOffset": 53}, {"referenceID": 1, "context": "Afrati and Ullman [2] investigated the problem of efficient joins in MapReduce.", "startOffset": 18, "endOffset": 21}, {"referenceID": 22, "context": "[24] developed algorithms for set similarty joins in MapReduce.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] extended MapReduce to Map-Reduce-Merge, in order to allow users to express different join types and algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The Manimal system [15] analyses MapReduce programs to do general database-style", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "Dittrich, et al [11].", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "doopDB [1] combines relational and MapReduce qualities into one system.", "startOffset": 7, "endOffset": 10}], "year": 2015, "abstractText": "Dictionary-based entity extraction involves finding mentions of dictionary entities in text. Text mentions are often noisy, containing spurious or missing words. Efficient algorithms for detecting approximate entity mentions follow one of two general techniques. The first approach is to build an index on the entities and perform index lookups of document substrings. The second approach recognizes that the number of substrings generated from documents can explode to large numbers, to get around this, they use a filter to prune many such substrings which do not match any dictionary entity and then only verify the remaining substrings if they are entity mentions of dictionary entities, by means of a text join. The choice between the index-based approach and the filter & verification-based approach is a case-to-case decision as the best approach depends on the characteristics of the input entity dictionary, for example frequency of entity mentions. Choosing the right approach for the setting can make a substantial difference in execution time. Making this choice is however non-trivial as there are parameters within each of the approaches that make the space of possible approaches very large. In this paper, we present a cost-based operator for making the choice among execution plans for entity extraction. Since we need to deal with large dictionaries and even larger large datasets, our operator is developed for implementations of MapReduce distributed algorithms.", "creator": "LaTeX with hyperref package"}}}