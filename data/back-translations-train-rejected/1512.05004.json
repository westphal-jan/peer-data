{"id": "1512.05004", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2015", "title": "Towards Evaluation of Cultural-scale Claims in Light of Topic Model Sampling Effects", "abstract": "In this preliminary study, we examine whether random samples from within given Library of Congress Classification Outline areas yield significantly different topic models. We find that models of subsamples can equal the topic similarity of models over the whole corpus. As the sample size increases, topic distance decreases and topic overlap increases. The requisite subsample size differs by field and by number of topics. While this study focuses on only five areas, we find significant differences in the behavior of these areas that can only be investigated with large corpora like the Hathi Trust.", "histories": [["v1", "Tue, 15 Dec 2015 23:07:58 GMT  (155kb,D)", "http://arxiv.org/abs/1512.05004v1", "HTRC ACS Technical Report; 8 pages, 3 figures"], ["v2", "Wed, 3 Feb 2016 21:12:17 GMT  (157kb,D)", "http://arxiv.org/abs/1512.05004v2", "HTRC ACS Technical Report; 10 pages, 3 figures"], ["v3", "Mon, 13 Feb 2017 15:48:16 GMT  (110kb,D)", "http://arxiv.org/abs/1512.05004v3", "2016 International Conference on Computational Social Science (IC2S2), June 23-26, 2016. 3 pages"]], "COMMENTS": "HTRC ACS Technical Report; 8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.DL cs.CL cs.IR", "authors": ["jaimie murdock", "jiaan zeng", "colin allen"], "accepted": false, "id": "1512.05004"}, "pdf": {"name": "1512.05004.pdf", "metadata": {"source": "CRF", "title": "Towards Cultural-Scale Models of Full Text", "authors": ["Jaimie Murdock", "Jiaan Zeng", "Colin Allen"], "emails": ["jammurdo@indiana.edu"], "sections": [{"heading": null, "text": "Keywords - digital libraries; topic modeling; topic alignment; random sampling; Hathi Trust; Library of Congress Classification Outline (LCCO).Large-scale digital libraries, such the Hathi Trust1, give a window into a much larger quantity of textual data than ever before [1] These data raise new challenges for analysis and interpretation.The constant, dynamic addition and revision of works in digital libraries means that any study aimed at characterizing the evolution of culture through large digital libraries must have an awareness of the implications of corpus sampling. Not recognizing that every large digital library is merely a sample of a larger set of books published within culture can lead to unintentionally strong claims about socio-linguistics [2]. New methods also require careful consideration of the humanistic implications [3].A methodology of modeling cultural enhancement is likely."}, {"heading": "Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "LCCO Sampling", "text": "We have implemented a random sampling web service that provides the following query interfaces: \u2022 Samples (Category, Number): It takes a category and the number of random samples as input and returns a list of randomly generated book IDs. For example, given the categories \"DQ78-210\" and 3 \"gri.ark: / 13960 / t50g6cm2v - uc1.31822038210555 - uva.x030577307\" where the book ID is separated by the tube symbol; \u2022 id (Category): It takes a category as input and returns a list of book IDs that all books have under such a category; \u2022 idTotal (Category): It takes a category as input and returns the total number of books under such a category. Figure 1 shows an example of the LCCO hierarchy stored in the HathiTrust Solr Index."}, {"heading": "Corpus Download", "text": "Each topic area was downloaded from HathiTrust via the HathiTrust Data API via the InPhO Topic Explorer interface on October 19, 2015. Selected areas can be found in Table 1."}, {"heading": "Topic Modeling", "text": "The LDA theme modeling [10] represents the current state of the art in extracting meaningful data from digitized texts. We are using the implementation of the LDA embedded within the InPhO Topic Explorer [11], which uses collapsed Gibbs samples for topic assessment [12]. All bodies have been removed from the NLTK stop list. Additionally, all words occurring more than 50,000 times and less than 15 times from the corpus are removed. A reference model is trained across the entire field. Several other cross-disciplinary models are trained across the entire field. For this preliminary study, we do not select the reference model from the cross-disciplinary models, but model verification studies [4] and model selection [13] provide guidelines for further research. Finally, several sub-body models are trained on different randomly selected parts of the entire corpus."}, {"heading": "Topic alignment", "text": "A topic orientation is a function that maps a topic in M1 to a topic in M2. For this analysis, M2 is always the reference model, while M1 is either an exciting or a partial body model. The orientation function is not required to be bijective: Multiple topics from M1 can be mapped to the same topic (not injectively) in M2, and not all topics in M2 must have an analogue in M1 (not surjective). Basic orientation - The basic orientation simply calculates the Jensen-Shannon distance (JSD) between each topic and all topics in M2. Each topic is simply mapped to the next topic, regardless of whether that topic in M2 coincides with another topic in M1. Mathematically, this function is neither injective nor surjective, so an inverse orientation from M2 to M1 does not guarantee identical results."}, {"heading": "Results", "text": "In this preliminary study, we examined five areas of the LCCO. These five areas were selected to provide examples in the arts, humanities, natural sciences and technology at different levels of granularity. For each subject area, three classes of models were trained on k = {20, 40, 60, 80}: 1. LDA subject modeling was applied to all volumes of the subject area to generate the reference model. 2. Additional models were trained across all volumes to generate multiple cross-disciplinary models. 3. Models were trained on random samples of the subject area to generate subbody models. A basic alignment was performed between each stress or subbody model on the reference model. Figure 3 shows the alignment for each alignment as an average Jensen-Shannon distance [14] between the word distribution of each subject pair. Results of alignment are plotted against the sample size in Figure 2 80 for nb: Art Sculpture Reference 80. Figure 3 shows the alignment for all five subject areas = 20."}, {"heading": "Discussion and Conclusions", "text": "In this preliminary study, we wanted to answer: \"Do random samples within certain LCCO categories yield significantly different TMs?\" We note that the principle-driven sample approach may work, based on the existence of subcorpus models with thematic distances of less than or equal to a reference model. We highlight several phenomena for further investigation: 1. Thematic distance decreases with sample size. (See Figures 2. and 3.) 2. Thematic overlaps increase with sample size. (See Figure 4.) 3. Different areas may have different properties, so that a single sample size portion may not be possible. (See Figures 3. and 4.) In particular, the behavior of different areas may be linked to the \"cognitive extent\" of the discipline. [15] While we used only five subject areas, Figure 4 indicates different levels of cognitive extent. (See Figures 3. and 4.) In particular, the behavior of different areas may be linked to the \"cognitive extent\" of the discipline."}, {"heading": "Acknowledgments", "text": "The work in this report was supported by a Hathi Trust Research Center (HTRC) Advanced Collaborative Support (ACS) Grant. We thank Miao Chen for her project management of the ACS Fellowships. We thank Justin Stamets for his support in the body curation."}, {"heading": "Art Sculpture topic alignments", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Quantitative Analysis of Culture", "author": ["Jean-Baptiste Michel", "Yuan Kui Shen", "Aviva Presser Aiden", "Adrian Veres", "Matthew K Gray", "The Google Books Team", "Joseph P Pickett", "Dale Hoiberg", "Dan Clancy", "Peter Norvig", "Jon Orwant", "Steven Pinker", "Martin A Nowak", "Erez Lieberman Aiden"], "venue": "Using Millions of Digitized Books. Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Characterizing the Google Books Corpus: Strong Limits to Inferences of Socio-Cultural and Linguistic Evolution", "author": ["Eitan Adam Pechenick", "Christopher M Danforth", "Peter Sheridan Dodds"], "venue": "PLoS ONE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Theorizing Research Practices We Forgot to Theorize", "author": ["Ted Underwood"], "venue": "Twenty Years Ago. Representations,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Probabilistic topic models", "author": ["David M. Blei"], "venue": "Communications of the ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "The Emergence of Literary Diction", "author": ["Ted Underwood", "Jordan Sellers"], "venue": "Journal of Digital Humanities,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "The Quiet Transformations of Literary Studies: What Thirteen Thousand Scholars Could Tell Us", "author": ["Andrew Goldstone", "Ted Underwood"], "venue": "New Literary History,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Trawling in the Sea of the Great Unread: Sub-corpus topic modeling and Humanities research", "author": ["Timothy R. Tangherlini", "Peter Leonard"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Cloud computing data capsules for non-consumptiveuse of texts", "author": ["Jiaan Zeng", "Guangchen Ruan", "Alexander Crowell", "Atul Prakash", "Beth Plale"], "venue": "In Proceedings of the 5th ACM Workshop on Scientific Cloud Computing, ScienceCloud", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Topic Exploration with the HTRC Data Capsule for Non-Consumptive Research", "author": ["Jaimie Murdock", "Jiaan Zeng", "Robert H McDonald"], "venue": "In JCDL \u201915 Proceedings of the 15th ACM/IEEE-CS joint conference on Digital libraries,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Visualization techniques for topic model checking", "author": ["Jaimie Murdock", "Colin Allen"], "venue": "Proceedings of 29th Association for the Advancement of Artificial Intelligence", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Finding scientific topics", "author": ["Thomas L Griffiths", "Mark Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Navigating the Local Modes of Big Data: The Case of Topic Models. In Data Analytics in Social Science, Government, and Industry", "author": ["Margaret Roberts", "Brandon Stewart", "Dustin Tingley"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Divergence Measures Based on the Shannon Entropy", "author": ["Jianhua Lin"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1991}], "referenceMentions": [{"referenceID": 0, "context": "Large-scale digital libraries, such as the Hathi Trust1, give a window into a much greater quantity of textual data than ever before [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "Failing to recognize that any large digital library is merely a sample of a larger set of books published within the culture can lead to unintentionally strong claims about socio-linguistics [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 2, "context": "New methodologies also require careful consideration for humanistic implications [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "One methodology with rapid uptake in the study of cultural evolution is probabilistic topic modeling [4].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "Topic modeling has been used to characterize the evolution of literary diction [5], the evolution of literary studies [6], and to search large corpora for \u201cthe great unread\u201d [7].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "Topic modeling has been used to characterize the evolution of literary diction [5], the evolution of literary studies [6], and to search large corpora for \u201cthe great unread\u201d [7].", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "Topic modeling has been used to characterize the evolution of literary diction [5], the evolution of literary studies [6], and to search large corpora for \u201cthe great unread\u201d [7].", "startOffset": 174, "endOffset": 177}, {"referenceID": 7, "context": "Moreover, topic modeling is an integral part of the Hathi Trust Research Center (HTRC)\u2019s Data Capsule [8, 9].", "startOffset": 102, "endOffset": 108}, {"referenceID": 8, "context": "Moreover, topic modeling is an integral part of the Hathi Trust Research Center (HTRC)\u2019s Data Capsule [8, 9].", "startOffset": 102, "endOffset": 108}, {"referenceID": 9, "context": "Topic Modeling LDA topic modeling [10] represents the current state of the art for extracting meaningful data from digitized texts.", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "We use the implementation of LDA embedded within the InPhO Topic Explorer [11], which uses collapsed Gibbs sampling for topic estimation [12].", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "We use the implementation of LDA embedded within the InPhO Topic Explorer [11], which uses collapsed Gibbs sampling for topic estimation [12].", "startOffset": 137, "endOffset": 141}, {"referenceID": 3, "context": "For this preliminary study, we do not select the reference model from the spanning models, but research on model checking [4] and model selection [13] provide guidelines for further research.", "startOffset": 122, "endOffset": 125}, {"referenceID": 12, "context": "For this preliminary study, we do not select the reference model from the spanning models, but research on model checking [4] and model selection [13] provide guidelines for further research.", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "Each alignment\u2019s fitness score is calculated as the average Jensen-Shannon distance [14] between the word distribution of each topic pair.", "startOffset": 84, "endOffset": 88}], "year": 2017, "abstractText": "In this preliminary study, we examine whether random samples from within given Library of Congress Classification Outline areas yield significantly different topic models. We find that models of subsamples can equal the topic similarity of models over the whole corpus. As the sample size increases, topic distance decreases and topic overlap increases. The requisite subsample size differs by field and by number of topics. While this study focuses on only five areas, we find significant differences in the behavior of these areas that can only be investigated with large corpora like the Hathi Trust. Keywords\u2014 digital libraries; topic modeling; topic alignment; random sampling; Hathi Trust; Library of Congress Classification Outline (LCCO). Large-scale digital libraries, such as the Hathi Trust1, give a window into a much greater quantity of textual data than ever before [1]. These data raise new challenges for analysis and interpretation. The constant, dynamic addition and revision of works in digital libraries mean that any study aiming to characterize the evolution of culture using large-scale digital libraries must have an awareness of the implications of corpus sampling. Failing to recognize that any large digital library is merely a sample of a larger set of books published within the culture can lead to unintentionally strong claims about socio-linguistics [2]. New methodologies also require careful consideration for humanistic implications [3]. One methodology with rapid uptake in the study of cultural evolution is probabilistic topic modeling [4]. Topic modeling has been used to characterize the evolution of literary diction [5], the evolution of literary studies [6], and to search large corpora for \u201cthe great unread\u201d [7]. Moreover, topic modeling is an integral part of the Hathi Trust Research Center (HTRC)\u2019s Data Capsule [8, 9]. Researchers need confidence in sampling methods used to construct topic models intended to represent very large portions of the HathiTrust collection. For example, topic modeling every book categorized under the Library of Congress Classification Outline (LCCO)2 as \u201cPhilosophy\u201d (call numbers B1-5802) is impractical, as any library will be incomplete. However, if it can be shown that models built from different random samples are highly similar to one another, then the project of having a topic model that is sufficiently representative of the entire HT collection may become tractable. http://hathitrust.org/ https://www.loc.gov/catdir/cpso/lcco/ 1 ar X iv :1 51 2. 05 00 4v 1 [ cs .D L ] 1 5 D ec 2 01 5 Methods LCCO Sampling We implemented a random sampling web service that provides the following query interfaces: \u2022 sampling(Category, number): It takes a category and the number of random samples as input, and returns a list of book ID which are randomly generated. For example, given category \u201cDQ78-210\u201d and 3, the web service returns \u201cgri.ark:/13960/t50g6cm2v\u2014uc1.31822038210555\u2014uva.x030577307\u201d where the book ID is separated by the pipe symbol; \u2022 id(Category): It takes a category as input and returns a list of book ID which has all the books under such a category; \u2022 idTotal(Category): It takes a category as input and returns the total number of books under such a category. Figure 1 shows an example of the LCCO hierarchy stored in the HathiTrust Solr Index. Corpus Download Each subject area was downloaded from the HathiTrust on 19 October 2015 using the HathiTrust Data API through the InPhO Topic Explorer interface. The selected areas can be found in Table 1. Topic Modeling LDA topic modeling [10] represents the current state of the art for extracting meaningful data from digitized texts. We use the implementation of LDA embedded within the InPhO Topic Explorer [11], which uses collapsed Gibbs sampling for topic estimation [12]. All corpuses have the NLTK English stoplist removed. Additionally, all words occurring more than 50000 times and less than 15 times removed from the corpus. A reference model is trained on the whole subject area. Multiple other spanning models are trained on the whole subject area. For this preliminary study, we do not select the reference model from the spanning models, but research on model checking [4] and model selection [13] provide guidelines for further research. Finally, multiple subcorpus models are trained on different portions of the whole corpus, selected randomly. Topic alignment A topic alignment is a function that maps one topic in M1 to a topic in M2. For this analysis, M2 is always the reference model, while M1 is either a spanning or subcorpus model. The alignment function is not LCCO Subject Heading # HT Vols nb Art Sculpture 801 tg Bridge Engineering 799 bj71-1185 History of Ethics 898 hd6050-6305 Classes of Labor 1255 tn600-799 Metallurgy 938 Table 1: LCCO Areas Sampled \u2014 Areas in the Library of Congress Classification Outline (LCCO) and their representation in the HathiTrust Digital Library.", "creator": "LaTeX with hyperref package"}}}