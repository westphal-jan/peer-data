{"id": "1512.05244", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2015", "title": "Learning Games and Rademacher Observations Losses", "abstract": "It has recently been shown that supervised learning with the popular logistic loss is equivalent to optimizing the exponential loss over sufficient statistics about the class: Rademacher observations (rados). We first show that this unexpected equivalence can actually be generalized to other example / rado losses, with necessary and sufficient conditions for the equivalence, exemplified on four losses that bear popular names in various fields: exponential (boosting), mean-variance (finance), Linear Hinge (on-line learning), ReLU (deep learning), and unhinged (statistics). Second, we show that the generalization unveils a surprising new connection to regularized learning, and in particular a sufficient condition under which regularizing the loss over examples is equivalent to regularizing the rados (with Minkowski sums) in the equivalent rado loss. This brings simple and powerful rado-based learning algorithms for sparsity-controlling regularization, that we exemplify on a boosting algorithm for the regularized exponential rado-loss, which formally boosts over four types of regularization, including the popular ridge and lasso, and the recently coined slope --- we obtain the first proven boosting algorithm for this last regularization. Through our first contribution on the equivalence of rado and example-based losses, Omega-R.AdaBoost~appears to be an efficient proxy to boost the regularized logistic loss over examples using whichever of the four regularizers. Experiments display that regularization consistently improves performances of rado-based learning, and may challenge or beat the state of the art of example-based learning even when learning over small sets of rados. Finally, we connect regularization to differential privacy, and display how tiny budgets can be afforded on big domains while beating (protected) example-based learning.", "histories": [["v1", "Wed, 16 Dec 2015 16:56:02 GMT  (1510kb)", "http://arxiv.org/abs/1512.05244v1", null], ["v2", "Sat, 13 Feb 2016 00:33:22 GMT  (1510kb)", "http://arxiv.org/abs/1512.05244v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["richard nock"], "accepted": false, "id": "1512.05244"}, "pdf": {"name": "1512.05244.pdf", "metadata": {"source": "CRF", "title": "Learning Games and Rademacher Observations Losses", "authors": ["Richard Nock"], "emails": ["richard.nock@nicta.com.au"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.05 244v 1 [cs.L G] 16 Dec 2"}, {"heading": "1 Introduction", "text": "A recent result has shown that there are many more problems in practice than in practice, both in practice and in practice. \"It is not as if,\" he says, \"but it is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is as if.\" \"It is.\" \"It is as if.\" \"\" It is. \"\" \"It is.\" \"\" It. \"\" \"It.\" \"\" \"\" It. \"\" \"\". \"\" \"\" \"It.\" \"\". \"\" \"\". \"\". \"\" \".\" \"\". \"\" \"\". \"\" \"\". \"\" \"\". \"\" \".\" \"\". \"\". \"\" \".\" \".\" \".\" \".\" \".\" \".\" \".\" \".\" \".\". \"\". \".\". \"\" \".\". \".\" \"\". \".\". \".\". \"\" \"\". \"\". \"\". \"\". \".\" \".\" \"\". \".\". \".\". \"\" \".\" \".\". \"\". \"\". \".\". \".\". \".\" \".\". \".\" \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\"............................................. \"...........................\" \"\"................. \"\" \"\" \"\" \"\" \"\"..............................."}, {"heading": "2 Games and equivalent example/rado losses", "text": "To avoid the notation load, we will not immediately question the learning situation, considering that the learner fits on a general z-Rm vector that depends both on the data (examples or rados) and on the classifiers. Let's [m]. We define the functions Le: Rm, Rm, Rm, Rm, R2, Rm, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2, R2."}, {"heading": "In the case where \u03d5e, \u03d5r are differentiable, they are proportionate iff p\u2217(z) = Gmq\u2217(z).", "text": "(Proof in Appendix, Section 9.2) Theorem 2 gives a necessary and sufficient condition for two generators to be proportional. It says nothing about how to construct one from the other side if possible. We now show that it is actually possible and that the search space (0, 1) is proportional to some others, then it must be a \"symmetrized\" version of r (1 \u2212 z) the symmetrized version of r, if there is such a definition. We call s (r) (z) (z). = r (z) the symmetrized version of r (1 \u2212 z) the symmetrized version of r.Lemma 4 If e and r are proportioned, then e (z) = (r / \u00b5e) s (r) s (r) s (z) + (b / \u00b5e), where b) is in eq. (Proof r.Lemma 4)."}, {"heading": "3 Learning with (rado) regularized losses", "text": "The learner has given a number of examples, all of which he assumed to be based on a given value. (...) There are a number of losses and losses that we can obtain with the corresponding example and the losses in Table 1. Losses are conveniently simplified if H5s linear classifier, h (x). In this case, we can describe the loss using edge vectors Se. = {yi \u00b7 xi xi xi xi xi xi xi, i = 1, 2..., m} since zi = empirical (yi \u00b7 xi), and the rado losses are described."}, {"heading": "4 Boosting with (rado) regularized losses", "text": "We are exploring four ways to improve our learning capability. (http: / / www.dapd-online.org) We are exploring four ways to improve our learning capability. (http: / / www.dapd-online.org) We are exploring four ways. (http: / / www.dapd-online.org) We are exploring four ways. (http: / / www.dapd-online.org) We are exploring four ways. (http: / / www.dapd-online.org) We are exploring four ways. (http: / / www.dapd-online.org) We are exploring four ways. (http: / / www.dapd-online.org) We are exploring four ways. (http: / www.dapd-online.org) We are exploring four ways. (http: / / / www.dapd-online.org) We are exploring four ways. (http: / / / www.dapd-online.org) We are exploring four ways. (.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.orf.or"}, {"heading": "5 Regularized losses and differential privacy", "text": "We show here that the standard differential privacy mechanism (DP) (Dwork & Roth, 2014) to protect examples in Rados - not examined in (Nock et al., 2015) - amounts to a surrogate form of randomized regulation versus clean examples. We let Lap (z | b). = (1 / 2b) exp (\u2212 | z | / b) denote the pdf of the Laplace distribution. DP-RADOS algorithm indicates the protection mechanism. Let's define two training samples Se and S \u00b2 e as neighbors, Se \u00b2 S \u2032 e notes when they differ from an example. We show how the Laplace 10 mechanism of DP-RADOS can lead to a Rado-DP and also minimize a Rado loss against protected radars to minimize an optimistic example bound to regulation (mac \u00b2 export)."}, {"heading": "6 Experiments", "text": "We have the possibility that we have not sorted out the weak learners for the weak learners, so that the weak learners are limited to the framebox in WL4. We tested two types of random learning effects, the rerandom rados, the rerandom rados et al., and class-wise rados, for which we first randomly select a class and then select a subset of its examples to calculate a rado (and repeat for n rados). \"We have two types of random rados, the rerandom rados, and class-wise rados, for which we randomly select a class and then a rado (and repeat for n rados)."}, {"heading": "7 Conclusion", "text": "We have shown that the equivalence between the protocol loss via examples and the exponential loss via rados, as shown in (Nock et al., 2015), can be generalized to other losses through a principal representation of a loss function in a zero-sum game for two players. Furthermore, we have shown that this equivalence extends to regulated losses, where the regulation of wheel loss via the rados themselves is carried out with Minkowski sums. Since regulation with rados has such a simple form, it is relatively easy to derive efficient learning algorithms that work with various forms of regulation, such as First, Lasso, and SLOPE regulations in a formal boosting algorithm we have introduced. Experiments confirm that this freedom in the choice of regulation represents a clear strength of the algorithm, and that regulation dramatically improves performance compared to unregulated Rado learning."}, {"heading": "8 Acknowledgments", "text": "Thanks are also due to Stephen Hardy and Giorgio Patrini for many stimulating discussions and feedback on this topic. NICTA is funded by the Australian government through the Ministry of Communications and by the Australian Research Council through the ICT Center of Excellence Program."}, {"heading": "Appendix \u2014 Table of contents", "text": "Proofs S. 16 Proof of Theorem 2 S. 16 Proof of Lemma 4 S. 18 Proof of Lemma 6 S. g 19 Proof of Corloary 7 S. g 20 Proof of Lemma 8 S. g 21 Proof of Corloary 9 S. g 22 Proof of Lemma 10 S. g 24 Proof of Corloary 11 S. g 24 Proof of Lemma 12 S. g 25 Proof of Corloary 13 S. g 25 Proof Theorem 14 S. 25 Proof Theorem 17 Pg 26 Proof Theorem 18 S. 28 Proof Theorem 19 S. g 30 Proof Theorem 20 S. 31Additional Experiments S. 33 Supports for rados (in addition to Table 2) S. 33 Experiments on class-wise rados S. 33 Test errors and evidence for rados (comparison of last vs. best empirical classics) S. g 3315"}, {"heading": "9 Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Proof of Theorem 2", "text": "We divide the proof into two parts, the first of which concerns the case in which the two generators are distinguishable, since some of the derivatives are to be distinguished below, and then the case in which they are indistinguishable. Note: Due to term 4, we do not have to cover the case in which only one of the two generators would be distinguishable. (43) Solving the two generators. (3) and (4) each bring: p \u00b2 i (z) = p \u00b2 e \u2212 1 (\u2212 1 \u00b2 zi), (44) q \u00b2 I (z) = 1 (\u2212 1 \u00b5r \u00b7 i)."}, {"heading": "9.2 Proof of Lemma 4", "text": "Take m = 1 and replace z with real z1. We have Le (p, z1) = pz1 + e (z1) and Lr (q, z) = q {1} z1 + r (q {1}) + r (q). Note that we can drop the constraint q H2 since then q = 1 \u2212 q {1}. Thus, we get L r (q) = min q R qz1 + r (q) + r (1 \u2212 q) + r (1 \u2212 q) + r (1 \u2212 q) = min q R qz1 + s (r) (q) = \u2212 \u00b5rs (r) (\u2212 \u00b5rs \u00b7 z1), whereas L e (p) = \u2212 \u00b5er (\u2212 1 \u00b5e) (\u2212 1 \u00b5e) (\u2212 1 \u00b5e) (\u2212 1 \u00b5e) and since e and r r (e) (e) = min q (r) (r) (\u2212 1 \u00b5rs (r) (r) = \u2212 1 \u00b5rs (r) (r) (\u2212 1 \u00b5rs), 18 whereas L e (p) = \u2212 \u00b5er (\u2212 1 \u00b5e) (\u2212 1 \u00b5e) (\u2212 1 \u00b5e) (\u2212 1 \u00b5e) (\u2212 1 \u00b5e) and since e and r r r r (e) (e and r r r r r r r (e) (e and r r r r r r r r r r (e) are proportionate, thenr (\u2212 1 \u00b5rs) = \u00b5rs (r) (r) = \u2212 1 \u00b5rs (r \u00b7 zr (r) (0.1) (0.1) (0.1) (0.1) (0.1) (0.1) (0.1 (0.1) (0.1) (0.1) (0.1) (0.1 (0.1) (0.1) (0.1) (0.1 (0.1) (0.1) 0.1 (0.1) (0.1 (0.1) (0.1) (0.1) (0.1) (0.1) (0.1 0.1) (0.1) (0.1) (0.1 (0.1) (0.1 0.1) (0.1) (0.1) (0.1) (0.1 (0.1) 0.1 (0.1) (0.1"}, {"heading": "9.3 Proof of Lemma 6", "text": "We use the fact that whenever there is differentiability, this (z). = z \u00b7 q = J \u00b7 \u2212 1 (z) \u2212 1 (z). We therefore have the Lagrange multiplier \u03bb in (46). \u2212 \u2212 r \u00b7 [m] exp (\u2212 m] exp (\u2212 1 \u00b5r \u00b7 p), (62), which from (54): q \u00b2 I (z) = exp (\u2212 1 \u00b5r \u00b7 p) exp (\u2212 1 \u00b5r \u00b7 p) exp (\u2212 m) exp (\u2212 1 \u00b5r \u00b7 J zj), (m].On the other hand, we also have this (z) = log (z / (1 \u2212 z) exp (i), we have this (z) exp (z)."}, {"heading": "9.4 Proof of Corollary 7", "text": "Consider r (z). = z log z \u2212 z and e = s. We get from eq. (47): \u2212 L \u00b2 e (z) = fe \u00b2 e [m] log (1 + exp (\u2212 1 \u00b5e \u00b7 zi)), with fe (z) = \u00b5e \u00b7 z + \u00b5em. We also have i \u00b2 r (z) = exp (z), and so we get \u2212 L \u00b2 r (z) = \u00b5r \u00b7 log (z) I [m] exp (\u2212 1 \u00b5r \u00b7 x) i i zi) + \u00b5r \u00b7 exp (\u03bb\u00b5r)."}, {"heading": "9.5 Proof of Lemma 8", "text": "\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2."}, {"heading": "9.6 Proof of Corollary 9", "text": "Consider the following numbers: (47): (L) (z) = (1 / 2) \u2212 \u2212 \u2212 \u2212 (2) \u2212 (2) \u2212 (2) \u2212 (2) \u2212 (2) \u2212 (2) and (4) \u2212 (4) \u00b7 z (4). We also have (2) \u2212 (1 / 2) \u2212 (2) \u2212 z2, and so we get \u2212 L (z) = (2) \u2212 (2). (67), we get \u2212 L (z) = \u2212 (2) (m) (1) (m). (1), (2) \u2212 z \u2212 (2)."}, {"heading": "9.7 Proof of Lemma 10", "text": "Define the d-dimensional probability simplex. Then it comes to this choice of r (qI): minq H2m Lr (q, z) = min q-dimensional probability simplex. Then it comes to this choice of r (qI): minq H2m Lr (q, z) = min q-m-i [m] qI-i-Izi = {0, if i-i zi > 0, I-6 = \u2205 i < 0 zi otherwise, (72) because whenever no zi is negative, the minimum is reached by setting all mass (1) to q-i, and if some are negative, the minimum is reached by setting all mass to the smallest of all I-i-i-i-i-zi, which is the one that is the one that has all the indices of the negative coordinates at the moment. On the other hand, the specification of e. = s is still e (z) = 0.1 (z) = 0.1)) (z = (z) that is the one that is the one who has all the indices of the negative coordinates at the moment."}, {"heading": "9.8 Proof of Corollary 11", "text": "We get from term 10 that \u2212 L \u0445 r (z) = fr (\u03c6r (z, \u00b5r)) with fr (z) = \u00b5r \u00b7 z and: \u03c6r (z, \u00b5r) = max {0, max I [m] {\u2212 1 \u00b5r \u00b7 \u2211 i Izi}. (74) On the other hand, it comes from eq. (73) that \u2212 L \u0445 e (z) = fr (\u0395e (z, \u00b5e))) with fe (z) = \u00b5e \u00b7 z and: \u0432 e (z, \u00b5e) = \u2211 i [m] max {0, \u2212 1 \u00b5e \u00b7 zi}. (75) From this results the proof of inference 11,24."}, {"heading": "9.9 Proof of Lemma 12", "text": "The choice of r (z) = [12m, 1 2] (z), (76) under the condition that q-H2m, q-I = 1 / 2 / 2 / I [m] is forced. Furthermore, the definition of e = s actually results in the yields of e = 1 [12m, 1 2] (z) + 1 [12m, 1 2] (1 \u2212 z) = 1 {12} (z), (77), which prevails p-i = 1 / 2, i-i. Since each i belongs to exactly 2 \u2212 1 subsets of [m], we get p-m (z) = Gmq (z), for each \u00b5e, \u00b5r, and thus r is proportional to e = s for each \u00b5e, \u00b5r."}, {"heading": "9.10 Proof of Corollary 13", "text": "We get from term 12 that \u2212 L \u0445 r (z) = fr (\u0435r (z, \u00b5r)) with fr (z) = z and: \u0442 r (z, \u00b5r) = EI [\u2212 1 \u00b5r \u00b7 \u2211 i \u0442Izi]. On the other hand, it comes from eq. (73) that \u2212 L \u0445 e (z) = fr (\u0435e (z, \u00b5e)) with fe (z) = (1 / 2) \u00b7 z and: \u0435e (z, \u00b5e) = \u0432 i \u2212 1 \u00b5e \u00b7 zi. Thus, the proof for the 11th sequence is provided."}, {"heading": "9.11 Proof of Theorem 14", "text": "Da fe (z) = ae \u00b7 z + be, results from eq. (7) that ae \u00b7 e (z) + be \u2212 ae (z) = ae \u00b7 (z) + b = [m] q \"I\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\""}, {"heading": "9.12 Proof of Theorem 17", "text": "The proof of the theorem contains two parts from which the first part follows ADABOOST's exponential convergence rate (79) and the second part deviates from this proof in order to grasp the weight distribution. \u2212 \u2212 The first part follows ADABOOST's exponential convergence rate (79), and the second part deviates from this proof in order to unravel the weight distribution as: wTj = w (T \u2212 1) jZT \u00b7 exp (T) (T) empirically (T) + empirically p (T \u2212 1) empirically p (empirically p \u2212 1) empirically p (empirically p \u2212 p) empirically p empirically empirically p (empirically p) empirically p (empirically p) empirically p (empirically p) empirically p (empirically p) empirically p (empirically p) empirically p (empirically p) empirically p (empirically p) empirically p (empirically p) empip."}, {"heading": "9.13 Proof of Theorem 18", "text": "We assume that it is an isolated case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case. (...) We are looking at the case."}, {"heading": "9.14 Proof of Theorem 19", "text": "We use the proof of theorem 18, since when \"k\" = \"k\" = \"k\" = \"k\" = \"k\" = \"k\" = \"k\" = \"k.\" We use the proof of \"k\" = \"k\" = \"k\" = \"k.\" We use the proof of theorem 18, since when \"k\" = \"k\" = \"k\" = \"k\" = \"k.\" (91) \"k\" = \"k\" = \"k\" = \"p.\" We use the proof of theorem 18 \"\u2212 k.\""}, {"heading": "9.15 Proof of Theorem 20", "text": "Let us assume that the example index on which Se and S'e differ is m = b = b = b = b = b = b = b = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c c = c c c = c c c = c c c = c c = c c = c c = c c c = c c c = c c = c = c = c c = c = c = c = c = c = c c c = c c = c = c c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c c = c = c c = c = c = c = c = c = c c c = c c c c c c c c c c c c c c"}, {"heading": "10 Additional experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10.1 Supports for rados (complement to Table 2)", "text": "Table 4 in this appendix contains the tools used to summarize Table 2.10.2. Experiments on class-wise Rados Tables 5 and 6 provide the test errors and aids for the ADOOST-R.ADABOOST when trained with class-wise Rados, i.e. Rados that summarize examples of the same class. Experiments do not show that class-wise Rados allow better training of ADABOOST-R.ADABOOST since test errors are equated with ADABOOST-R.ADOOST-R.ADOOST-R.R.R.ADOOST-R.OOST-R.OOST trained with purely random Rados (see Table 2)."}, {"heading": "10.3 Test errors and supports for rados (comparison last vs best empirical classifier)", "text": "In the main experiments, the classifier is kept out of the sequence, both for the sequence and for the sequence. This setting makes sense if the goal is only to minimize the test error, and it is also applicable in a sequence in which the data and the student are far apart. (In this case, the student sends the sequence of classifiers in which the party holds the data, which then select the best ones in the sequence.) However, one may wonder how the algorithms are returned the last ones in the sequence, that is, the sequence of classifiers that adapts to the sequence. (In the sequence of the sequence, the sequence of sequencing of the sequence is sequenced.)"}], "references": [{"title": "Optimization with sparsity-inducing penalties", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2011}, {"title": "SLOPE \u2013 adaptive variable selection via convex optimization", "author": ["M Bogdan", "E. van den Berg", "C. Sabatti", "W. Su", "Cand\u00e8s", "E.-J"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Bogdan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bogdan et al\\.", "year": 2015}, {"title": "Efficient learning using forward-backward splitting", "author": ["Duchi", "J.-C", "Y. Singer"], "venue": "In NIPS*22,", "citeRegEx": "Duchi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2009}, {"title": "The algorithmic foudations of differential privacy", "author": ["C. Dwork", "A. Roth"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Dwork and Roth,? \\Q2014\\E", "shortCiteRegEx": "Dwork and Roth", "year": 2014}, {"title": "Linear hinge loss and average margin", "author": ["C. Gentile", "M. Warmuth"], "venue": "In NIPS*11,", "citeRegEx": "Gentile and Warmuth,? \\Q1998\\E", "shortCiteRegEx": "Gentile and Warmuth", "year": 1998}, {"title": "Differential privacy: An economic method for choosing epsilon", "author": ["J. Hsu", "M. Gaboardi", "A. Haeberlen", "S. Khanna", "A. Narayan", "Pierce", "B.-C", "A. Roth"], "venue": "In Proc. of the 27 IEEE CSFS,", "citeRegEx": "Hsu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2014}, {"title": "On the boosting ability of top-down decision tree learning algorithms", "author": ["M.J. Kearns", "Y. Mansour"], "venue": "J. Comp. Syst. Sc.,", "citeRegEx": "Kearns and Mansour,? \\Q1999\\E", "shortCiteRegEx": "Kearns and Mansour", "year": 1999}, {"title": "Computational implications of reducing data to sufficient statistics", "author": ["A. Montanari"], "venue": "Technical Report 201412, Stanford U.,", "citeRegEx": "Montanari,? \\Q2014\\E", "shortCiteRegEx": "Montanari", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G. Hinton"], "venue": "ICML, pp", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "On the efficient minimization of classification-calibrated surrogates", "author": ["R. Nock", "F. Nielsen"], "venue": "In NIPS*21,", "citeRegEx": "Nock and Nielsen,? \\Q2008\\E", "shortCiteRegEx": "Nock and Nielsen", "year": 2008}, {"title": "Generalized mixability via entropic duality", "author": ["Reid", "M.-D", "Frongillo", "R.-M", "Williamson", "R.-C", "Mehta", "N.-A"], "venue": "In 28 th COLT, pp", "citeRegEx": "Reid et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reid et al\\.", "year": 2015}, {"title": "The boosting approach to machine learning: An overview", "author": ["Schapire", "R.-E"], "venue": "Notes in Statistics,", "citeRegEx": "Schapire and R..E.,? \\Q2003\\E", "shortCiteRegEx": "Schapire and R..E.", "year": 2003}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["R.E. Schapire", "Y. Singer"], "venue": "MLJ,", "citeRegEx": "Schapire and Singer,? \\Q1999\\E", "shortCiteRegEx": "Schapire and Singer", "year": 1999}, {"title": "SLOPE is adaptive to unkown sparsity and asymptotically minimax", "author": ["W. Su", "Cand\u00e8s", "E.-J"], "venue": "CoRR, abs/1503.08393,", "citeRegEx": "Su et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Su et al\\.", "year": 2015}, {"title": "A primal-dual convergence analysis of boosting", "author": ["M. Telgarsky"], "venue": "JMLR, 13:561\u2013606,", "citeRegEx": "Telgarsky,? \\Q2012\\E", "shortCiteRegEx": "Telgarsky", "year": 2012}, {"title": "Learning with symmetric label noise: The importance of being unhinged", "author": ["B. van Rooyen", "A. Menon", "Williamson", "R.-C"], "venue": "In NIPS*28,", "citeRegEx": "Rooyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rooyen et al\\.", "year": 2015}, {"title": "Privacy for free: Posterior sampling and stochastic gradient Monte Carlo", "author": ["Wang", "Y.-X", "S.E. Fienberg", "Smola", "A.-J"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Speed and sparsity of regularized boosting", "author": ["Xi", "Y.-T", "Xiang", "Z.-J", "Ramadge", "P.-J", "Schapire", "R.-E"], "venue": "AISTATS, pp", "citeRegEx": "Xi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Xi et al\\.", "year": 2009}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Zou and Hastie,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie", "year": 2005}], "referenceMentions": [{"referenceID": 7, "context": "It is known that sufficient statistics carry the intractability of certain processes that would otherwise be easy with data (Montanari, 2014).", "startOffset": 124, "endOffset": 141}, {"referenceID": 14, "context": "The technique we use exploits a two-player zero sum game representation of convex losses, that has been very useful to analyse boosting algorithms (Schapire, 2003; Telgarsky, 2012), with one key difference: payoffs are non-linear convex, eventually non-differentiable.", "startOffset": 147, "endOffset": 180}, {"referenceID": 10, "context": "These also resemble the entropic dual losses (Reid et al., 2015), with the difference that we do not enforce conjugacy over the simplex.", "startOffset": 45, "endOffset": 64}, {"referenceID": 14, "context": "It turns out that the losses involved bear popular names in different communities, even when not all of them are systematically used as losses per se: exponential, logistic, square, mean-variance, ReLU, linear Hinge, and unhinged losses (Nair & Hinton, 2010; Gentile & Warmuth, 1998; Nock & Nielsen, 2008; Telgarsky, 2012; Vapnik, 1998; van Rooyen et al., 2015) (and many others).", "startOffset": 237, "endOffset": 361}, {"referenceID": 0, "context": "Regularizing a loss is common in machine learning (Bach et al., 2011).", "startOffset": 50, "endOffset": 69}, {"referenceID": 1, "context": "ADABOOST, that learns a classifier from rados using the exponential regularized rado loss, with regularization choice belonging to the ridge, lasso, l\u221e, or the recently coined SLOPE (Bogdan et al., 2015).", "startOffset": 182, "endOffset": 203}, {"referenceID": 0, "context": "= le(Se,\u03b8) + \u03a9(\u03b8) , (21) with \u03a9 a regularizer (Bach et al., 2011).", "startOffset": 46, "endOffset": 65}, {"referenceID": 0, "context": "(Bach et al., 2011; Bogdan et al., 2015; Duchi & Singer, 2009; Su & Cand\u00e8s, 2015).", "startOffset": 0, "endOffset": 81}, {"referenceID": 1, "context": "(Bach et al., 2011; Bogdan et al., 2015; Duchi & Singer, 2009; Su & Cand\u00e8s, 2015).", "startOffset": 0, "endOffset": 81}, {"referenceID": 1, "context": "ADABOOST, for two reasons: it matches the original definition (Bogdan et al., 2015) and furthermore it unveils an interesting connection between boosting and the SLOPE properties (Su & Cand\u00e8s, 2015).", "startOffset": 62, "endOffset": 83}, {"referenceID": 1, "context": "14) Constraint (ii) on q is interesting in the light of the properties of SLOPE (Bogdan et al., 2015; Su & Cand\u00e8s, 2015).", "startOffset": 80, "endOffset": 120}, {"referenceID": 17, "context": "Table 2: Best result of ADABOOST/l1-ADABOOST (Schapire & Singer, 1999; Xi et al., 2009), vs \u03a9R.", "startOffset": 45, "endOffset": 87}, {"referenceID": 17, "context": "ADABOOST to ADABOOST/l1-ADABOOST (Schapire & Singer, 1999; Xi et al., 2009).", "startOffset": 33, "endOffset": 75}, {"referenceID": 17, "context": "To obtain very sparse solutions for l1-ADABOOST, we pick its \u03c9 (\u03b2 in (Xi et al., 2009)) in {10\u22124, 1, 104}.", "startOffset": 69, "endOffset": 86}, {"referenceID": 5, "context": "The results are a clear advocacy in favor of using rados against examples for the straight DP protection: with plain random rados, test errors that compete with clean data can be observed for privacy budget \u03b5 \u2248 10\u22124, that is, more than a hundred times smaller than most reported studies (Hsu et al., 2014).", "startOffset": 287, "endOffset": 305}, {"referenceID": 16, "context": "In addition to \u201ccoming for free\u201d (Wang et al., 2015) in machine learning, DP may thus also be a worthwhile companion to improve learning.", "startOffset": 33, "endOffset": 52}, {"referenceID": 17, "context": "Table 7: Best result of ADABOOST/l1-ADABOOST Schapire & Singer (1999); Xi et al. (2009), vs \u03a9-R.", "startOffset": 71, "endOffset": 88}, {"referenceID": 17, "context": "Table 7: Best result of ADABOOST/l1-ADABOOST Schapire & Singer (1999); Xi et al. (2009), vs \u03a9-R.ADABOOST (with or without regularization, trained with n = m random rados (above bold horizontal line) / n = 10000 rados (below bold horizontal line)), according to the expected true error of \u03b8T , when the classifier \u03b8T returned is the last classifier of the sequence (\u201d\u2020\u201d; \u03b8T = \u03b81000), or when it is the classifier minimizing the empirical risk in the sequence (\u201d\u201d; \u03b8T = \u03b8\u22641000). Table shows the best result over all \u03c9s, as well as the difference between the worst and best (\u2206). Shaded cells display the best result of \u03a9-R.ADABOOST. All domains but Kaggle are UCI Bache & Lichman (2013). 39", "startOffset": 71, "endOffset": 684}], "year": 2015, "abstractText": "It has recently been shown that supervised learning with the popular logistic loss is equivalent to optimizing the exponential loss over sufficient statistics about the class: Rademacher observations (rados). We first show that this unexpected equivalence can actually be generalized to other example / rado losses, with necessary and sufficient conditions for the equivalence, exemplified on four losses that bear popular names in various fields: exponential (boosting), mean-variance (finance), Linear Hinge (on-line learning), ReLU (deep learning), and unhinged (statistics). Second, we show that the generalization unveils a surprising new connection to regularized learning, and in particular a sufficient condition under which regularizing the loss over examples is equivalent to regularizing the rados (with Minkowski sums) in the equivalent rado loss. This brings simple and powerful rado-based learning algorithms for sparsity-controlling regularization, that we exemplify on a boosting algorithm for the regularized exponential rado-loss, which formally boosts over four types of regularization, including the popular ridge and lasso, and the recently coined SLOPE \u2014 we obtain the first proven boosting algorithm for this last regularization. Through our first contribution on the equivalence of rado and example-based losses, \u03a9R.ADABOOST appears to be an efficient proxy to boost the regularized logistic loss over examples using whichever of the four regularizers (and any linear combination of them, e.g., for elastic net regularization). We are not aware of any regularized logistic loss formal boosting algorithm with such a wide spectrum of regularizers. Experiments display that regularization consistently improves performances of rado-based learning, and may challenge or beat the state of the art of example-based learning even when learning over small sets of rados. Finally, we connect regularization to \u03b5-differential privacy, and display how tiny budgets (e.g. \u03b5 < 10) can be afforded on big domains while beating (protected) example-based learning.", "creator": "gnuplot 4.6 patchlevel 5"}}}