{"id": "1512.05467", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2015", "title": "Unsupervised Feature Construction for Improving Data Representation and Semantics", "abstract": "Feature-based format is the main data representation format used by machine learning algorithms. When the features do not properly describe the initial data, performance starts to degrade. Some algorithms address this problem by internally changing the representation space, but the newly-constructed features are rarely comprehensible. We seek to construct, in an unsupervised way, new features that are more appropriate for describing a given dataset and, at the same time, comprehensible for a human user. We propose two algorithms that construct the new features as conjunctions of the initial primitive features or their negations. The generated feature sets have reduced correlations between features and succeed in catching some of the hidden relations between individuals in a dataset. For example, a feature like $sky \\wedge \\neg building \\wedge panorama$ would be true for non-urban images and is more informative than simple features expressing the presence or the absence of an object. The notion of Pareto optimality is used to evaluate feature sets and to obtain a balance between total correlation and the complexity of the resulted feature set. Statistical hypothesis testing is used in order to automatically determine the values of the parameters used for constructing a data-dependent feature set. We experimentally show that our approaches achieve the construction of informative feature sets for multiple datasets.", "histories": [["v1", "Thu, 17 Dec 2015 05:18:05 GMT  (6915kb,D)", "http://arxiv.org/abs/1512.05467v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["marian-andrei rizoiu", "julien velcin", "st\\'ephane lallich"], "accepted": false, "id": "1512.05467"}, "pdf": {"name": "1512.05467.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Feature Construction for Improving Data Representation and Semantics", "authors": ["Marian-Andrei Rizoiu", "Julien Velcin", "St\u00e9phane Lallich"], "emails": ["Marian-Andrei.Rizoiu@univ-lyon2.fr", "Julien.Velcin@univ-lyon2.fr", "Stephane.Lallich@univ-lyon2.fr"], "sections": [{"heading": null, "text": "Keywords Unsupervised feature construction \u00b7 Feature evaluation \u00b7 Nonparametric statistics \u00b7 Data Mining \u00b7 Clustering \u00b7 Representations \u00b7 Algorithms for data and knowledge management \u00b7 Heuristic methods \u00b7 Pattern analysis \u00b7 Marian-Andrei Rizoiu \u00b7 Julien Velcin \u00b7 Ste'phane Lallich ERIC Laboratory, University Lumie de Lyon 2 5, avenue Pierre Mende's France, 69676 Bron Cedex, France Tel. + 33 (0) 4 78 77 31 54 Fax. + 33 (0) 4 78 77 23 75Marian-Andrei Rizoiu Email: Marian-Andrei.Rizoiu @ univ-lyon2.frJulien Velcin Email: Julien.Velcin @ univ-lyon2.frSte phane Lallich Email: Stephane.Lallich @ univ-lyon2.frar Xiv: 151 2.05 467v 1 [cs.A] 701"}, {"heading": "1 Introduction", "text": "This year, the time has come for an agreement to be reached in just a few days."}, {"heading": "2 uFRINGE - adapting FRINGE for unsupervised learning", "text": "This year it is more than ever before."}, {"heading": "3 uFC - a greedy heuristic", "text": "In fact, it is the case that one sees oneself in a position to surpass oneself, both in the question, as well as in the question, to what extent it is in fact a purely problem, and in the question, to what extent it is actually a problem. (afu) In the question, to what extent it is actually a problem, which has come to a head in recent years, there is only one way out. (afu) In the question, to what extent it is actually a problem, the question is whether it is a problem at all. (afu)"}, {"heading": "4 Evaluation of a feature set", "text": "To our knowledge, there are no generally accepted measures for assessing the general correlation between the characteristics of a character set (1). We propose a measure inspired by the principle of \"inclusion exclusion\" (Feller, 1950). In set theory, this principle allows to express the cardinality of the finite reunion of finite ensembles by taking into account the cardinality of these characteristics and their intersections. In the Boolean form, it is used to evaluate the probability of a clause (disjunction of letters) as a function of its composing concepts (conjunctions of letters). Given the characteristics that F = {f1, fm}, we have the following characteristics: p (f1, f2, fm)."}, {"heading": "5 Initial Experiments", "text": "In fact, most of them will be able to move to another world in which they will be able to move."}, {"heading": "6 Improving the uFC algorithm", "text": "The main difficulty of the uFC, as shown by the initial experiments, is the determination of the values of the parameters. An unfortunate choice would lead either to an excessively complex set of characteristics or a set of characteristics in which characteristics are still correlated, but both parameters \u03bb and limiter depend on the data sets and finding the appropriate values would prove to be a process of testing and error for each new corpus. The \"closest\" heuristic method achieves an acceptable balance between complexity and performance, but requires several executions with wide choices for parameters and the construction of the pareto front, which may not always be desirable or even possible. We propose a new method to choose the best solution based on statistical hypotheses and apply a new stop criterion inspired by the \"next point\" heuristic. These are integrated into a new \"risk-based\" heuristic \"method, which is approximate to the best solution, while the only parameter is avoided from the signistic front."}, {"heading": "7 Further Experiments", "text": "In fact, most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves."}, {"heading": "8 Conclusion and future work", "text": "In this article, we propose two approaches to the construction of characteristics. In contrast to the other design algorithms for characteristics proposed in the literature so far, our proposals work in a paradigm of unattended learning. uFRINGE is an unattended adaptation of the FRINGE algorithm, while uFC is a new approach that replaces linearly correlated characteristics with conjunctions of literals. We demonstrate that our approaches successfully reduce the general correlation in the characteristic set, while finding an appropriate balance between quality and complexity and avoiding time-consuming multiple executions, followed by a pareto front construction. We use statistical hypotheses tests and confidence levels for parameter adjustments and argumentation approaches in the pareto front of evaluation solutions."}], "references": [{"title": "A step-down multiple hypotheses testing procedure that controls the false discovery rate under independence", "author": ["Y Benjamini", "W Liu"], "venue": "Journal of Statistical Planning and Inference", "citeRegEx": "Benjamini and Liu,? \\Q1999\\E", "shortCiteRegEx": "Benjamini and Liu", "year": 1999}, {"title": "Top-down induction of clustering trees", "author": ["H Blockeel", "L De Raedt", "J Ramon"], "venue": "Proceedings of the 15th International Conference on Machine Learning,", "citeRegEx": "Blockeel et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Blockeel et al\\.", "year": 1998}, {"title": "Data-driven constructive induction", "author": ["E Bloedorn", "RS Michalski"], "venue": "Intelligent Systems and their Applications", "citeRegEx": "Bloedorn and Michalski,? \\Q1998\\E", "shortCiteRegEx": "Bloedorn and Michalski", "year": 1998}, {"title": "Support-vector networks. Machine learning", "author": ["C Cortes", "V Vapnik"], "venue": null, "citeRegEx": "Cortes and Vapnik,? \\Q1995\\E", "shortCiteRegEx": "Cortes and Vapnik", "year": 1995}, {"title": "An introduction to probability theory and its applications", "author": ["W Feller"], "venue": null, "citeRegEx": "Feller,? \\Q1950\\E", "shortCiteRegEx": "Feller", "year": 1950}, {"title": "Resampling-based multiple testing for microarray data analysis", "author": ["Y Ge", "S Dudoit", "TP Speed"], "venue": null, "citeRegEx": "Ge et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2003}, {"title": "Automatic feature construction and a simple rule induction algorithm for skin detection", "author": ["G Gomez", "E Morales"], "venue": "Proc. of the ICML workshop on Machine Learning in Computer Vision,", "citeRegEx": "Gomez and Morales,? \\Q2002\\E", "shortCiteRegEx": "Gomez and Morales", "year": 2002}, {"title": "A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics pp", "author": ["S Holm"], "venue": null, "citeRegEx": "Holm,? \\Q1979\\E", "shortCiteRegEx": "Holm", "year": 1979}, {"title": "A survey of manifold-based learning methods. Mining of Enterprise Data pp", "author": ["X Huo", "XS Ni", "AK Smith"], "venue": null, "citeRegEx": "Huo et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Huo et al\\.", "year": 2005}, {"title": "Fast feature selection using partial correlation for multivalued attributes", "author": ["S Lallich", "R Rakotomalala"], "venue": "Proceedings of the 4th European Conference on Principles of Data Mining and Knowledge Discovery, LNAI Springer-Verlag,", "citeRegEx": "Lallich and Rakotomalala,? \\Q2000\\E", "shortCiteRegEx": "Lallich and Rakotomalala", "year": 2000}, {"title": "Statistical inference and data mining: false discoveries control. In: COMPSTAT: proceedings in computational statistics: 17th symposium", "author": ["S Lallich", "O Teytaud", "E Prudhomme"], "venue": null, "citeRegEx": "Lallich et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lallich et al\\.", "year": 2006}, {"title": "Feature extraction, construction and selection: A data mining perspective", "author": ["H Liu", "H Motoda"], "venue": null, "citeRegEx": "Liu and Motoda,? \\Q1998\\E", "shortCiteRegEx": "Liu and Motoda", "year": 1998}, {"title": "Adding domain knowledge to sbl through feature construction", "author": ["CJ Matheus"], "venue": "Proceedings of the Eighth National Conference on Artificial Intelligence,", "citeRegEx": "Matheus,? \\Q1990\\E", "shortCiteRegEx": "Matheus", "year": 1990}, {"title": "A theory and methodology of inductive learning", "author": ["RS Michalski"], "venue": "Artificial Intelligence", "citeRegEx": "Michalski,? \\Q1983\\E", "shortCiteRegEx": "Michalski", "year": 1983}, {"title": "Feature selection based on inference correlation. Intelligent Data Analysis", "author": ["D Mo", "SH Huang"], "venue": null, "citeRegEx": "Mo and Huang,? \\Q2011\\E", "shortCiteRegEx": "Mo and Huang", "year": 2011}, {"title": "Feature selection, extraction and construction", "author": ["H Motoda", "H Liu"], "venue": "Communication of IICM (Institute of Information and Computing", "citeRegEx": "Motoda and Liu,? \\Q2002\\E", "shortCiteRegEx": "Motoda and Liu", "year": 2002}, {"title": "Id2-of-3: Constructive induction of m-of-n concepts for discriminators in decision trees", "author": ["PM Murphy", "MJ Pazzani"], "venue": "Proceedings of the Eighth International Workshop on Machine Learning,", "citeRegEx": "Murphy and Pazzani,? \\Q1991\\E", "shortCiteRegEx": "Murphy and Pazzani", "year": 1991}, {"title": "Boolean feature discovery in empirical learning. Machine learning", "author": ["G Pagallo", "D Haussler"], "venue": null, "citeRegEx": "Pagallo and Haussler,? \\Q1990\\E", "shortCiteRegEx": "Pagallo and Haussler", "year": 1990}, {"title": "Discovery, analysis, and presentation of strong rules. Knowledge discovery in databases", "author": ["G Piatetsky-Shapiro"], "venue": null, "citeRegEx": "Piatetsky.Shapiro,? \\Q1991\\E", "shortCiteRegEx": "Piatetsky.Shapiro", "year": 1991}, {"title": "Induction of decision trees. Machine learning", "author": ["JR Quinlan"], "venue": null, "citeRegEx": "Quinlan,? \\Q1986\\E", "shortCiteRegEx": "Quinlan", "year": 1986}, {"title": "C4.5: programs for machine learning", "author": ["JR Quinlan"], "venue": null, "citeRegEx": "Quinlan,? \\Q1993\\E", "shortCiteRegEx": "Quinlan", "year": 1993}, {"title": "Labelme: a database and webbased tool for image annotation", "author": ["BC Russell", "A Torralba", "KP Murphy", "WT Freeman"], "venue": "International Journal of Computer Vision", "citeRegEx": "Russell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Russell et al\\.", "year": 2008}, {"title": "A direct approach to false discovery rates. Journal of the Royal Statistical Society: Series B (Statistical Methodology", "author": ["JD Storey"], "venue": null, "citeRegEx": "Storey,? \\Q2002\\E", "shortCiteRegEx": "Storey", "year": 2002}, {"title": "A scheme for feature construction and a comparison of empirical methods", "author": ["DS Yang", "L Rendell", "G Blix"], "venue": "Proceedings of the Twelfth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Yang et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Yang et al\\.", "year": 1991}, {"title": "Constructing nominal x-of-n attributes", "author": ["Z Zheng"], "venue": "Proceedings of International Joint Conference On Artificial Intelligence,", "citeRegEx": "Zheng,? \\Q1995\\E", "shortCiteRegEx": "Zheng", "year": 1995}, {"title": "A comparison of constructive induction with different types of new attribute", "author": ["Z Zheng"], "venue": "Tech. rep., School of Computing and Mathematics,", "citeRegEx": "Zheng,? \\Q1996\\E", "shortCiteRegEx": "Zheng", "year": 1996}, {"title": "Constructing conjunctions using systematic search on decision trees. Knowledge-Based Systems", "author": ["Z Zheng"], "venue": null, "citeRegEx": "Zheng,? \\Q1998\\E", "shortCiteRegEx": "Zheng", "year": 1998}], "referenceMentions": [{"referenceID": 3, "context": ", SVM (Cortes and Vapnik, 1995), PCA (Dunteman, 1989) etc.", "startOffset": 6, "endOffset": 31}, {"referenceID": 18, "context": "In the same way that frequent itemsets (Piatetsky-Shapiro, 1991) help users to understand the patterns in transactions, our goal with the new features is to help understand relations between individuals of datasets.", "startOffset": 39, "endOffset": 64}, {"referenceID": 11, "context": "Liu and Motoda (1998) collects some of them and divides them into three categories: feature selection, feature extraction and feature construction.", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "Feature selection (Lallich and Rakotomalala, 2000; Mo and Huang, 2011) seeks to filter the original feature set in order to remove redundant features.", "startOffset": 18, "endOffset": 70}, {"referenceID": 14, "context": "Feature selection (Lallich and Rakotomalala, 2000; Mo and Huang, 2011) seeks to filter the original feature set in order to remove redundant features.", "startOffset": 18, "endOffset": 70}, {"referenceID": 15, "context": "Feature extraction is a process that extracts a set of new features from the original features through functional mapping (Motoda and Liu, 2002).", "startOffset": 122, "endOffset": 144}, {"referenceID": 3, "context": "For example, the SVM algorithm (Cortes and Vapnik, 1995) constructs a kernel function that changes the description space into a new separable one.", "startOffset": 31, "endOffset": 56}, {"referenceID": 15, "context": "Feature Construction is a process that discovers missing information about the relationships between features and augments the space of features by inferring or creating additional features (Motoda and Liu, 2002).", "startOffset": 190, "endOffset": 212}, {"referenceID": 13, "context": "Constructive induction (Michalski, 1983) is a process of constructing new features using two intertwined searches (Bloedorn and Michalski, 1998): one in the representation space (modifying the feature set) and another in the hypothesis space (using classical learning methods).", "startOffset": 23, "endOffset": 40}, {"referenceID": 2, "context": "Constructive induction (Michalski, 1983) is a process of constructing new features using two intertwined searches (Bloedorn and Michalski, 1998): one in the representation space (modifying the feature set) and another in the hypothesis space (using classical learning methods).", "startOffset": 114, "endOffset": 144}, {"referenceID": 24, "context": "Supervised feature construction can also be applied in other domains, like decision rule learning (Zheng, 1995).", "startOffset": 98, "endOffset": 111}, {"referenceID": 6, "context": "Algorithm 1, presented in Gomez and Morales (2002); Yang et al (1991), represents the general schema followed by most constructive induction algorithms.", "startOffset": 26, "endOffset": 51}, {"referenceID": 6, "context": "Algorithm 1, presented in Gomez and Morales (2002); Yang et al (1991), represents the general schema followed by most constructive induction algorithms.", "startOffset": 26, "endOffset": 70}, {"referenceID": 17, "context": "FRINGE (Pagallo and Haussler, 1990) creates new features using a decision tree that it builds at each iteration.", "startOffset": 7, "endOffset": 35}, {"referenceID": 12, "context": "CITRE (Matheus, 1990) adds other search strategies like root (selects first two nodes in a positive path) or root-fringe (selects the first and last node in the path).", "startOffset": 6, "endOffset": 21}, {"referenceID": 26, "context": "CAT (Zheng, 1998) is another example of a hypothesis-driven constructive algorithm similar to FRINGE.", "startOffset": 4, "endOffset": 17}, {"referenceID": 16, "context": "The algorithm ID2\u2212o f \u22123 (Murphy and Pazzani, 1991) uses M-of-N representations for the newly-created features.", "startOffset": 25, "endOffset": 51}, {"referenceID": 24, "context": "The Xo f N (Zheng, 1995) algorithm functions similarly, except that it uses the X-of-N representation.", "startOffset": 11, "endOffset": 24}, {"referenceID": 12, "context": "CITRE (Matheus, 1990) adds other search strategies like root (selects first two nodes in a positive path) or root-fringe (selects the first and last node in the path). It also introduces domain-knowledge by applying filters to prune the constructed features. CAT (Zheng, 1998) is another example of a hypothesis-driven constructive algorithm similar to FRINGE. It also constructs conjunctive features based on the output of decision trees. It uses a dynamic-path based approach (the conditions used to generate new features are chosen dynamically) and it includes a pruning technique. There are alternative representations, other than conjunctive and disjunctive. The M-ofN and X-of-N representations use feature-value pairs. An feature-value pair AVk(Ai = Vi j) is true for an instance if and only if the feature Ai has the value Vi j for that instance. The difference between M-of-N and X-of-N is that, while the second one counts the number of true feature-value pairs, the first one uses a threshold parameter to assign a value of truth for the entire representation. The algorithm ID2\u2212o f \u22123 (Murphy and Pazzani, 1991) uses M-of-N representations for the newly-created features. It has a specialization and a generalization construction operator and it does not need to construct a new decision tree at each step, but instead integrates the feature construction in the decision tree construction. The Xo f N (Zheng, 1995) algorithm functions similarly, except that it uses the X-of-N representation. It also takes into account the complexity of the features generated. Comparative studies like Zheng (1996) show that conjunctive and disjunctive representations have very similar performances in terms of prediction accuracy and theoretical complexity.", "startOffset": 7, "endOffset": 1612}, {"referenceID": 17, "context": "FRINGE (Pagallo and Haussler, 1990) is a framework algorithm (see Section 1.", "startOffset": 7, "endOffset": 35}, {"referenceID": 19, "context": "It creates new features using a logical decision tree, created using a traditional algorithm like ID3 (Quinlan, 1986) or C4.", "startOffset": 102, "endOffset": 117}, {"referenceID": 20, "context": "5 (Quinlan, 1993).", "startOffset": 2, "endOffset": 17}, {"referenceID": 4, "context": "We propose a measure inspired from the \u201cinclusionexclusion\u201d principle (Feller, 1950).", "startOffset": 70, "endOffset": 84}, {"referenceID": 0, "context": "Setting aside the Bonferroni correction, often considered too simplistic and too drastic, one has the option of using sequential rejection methods (Benjamini and Liu, 1999; Holm, 1979), the q-value method of Storey (Storey, 2002) or making use of bootstrap (Lallich et al, 2006).", "startOffset": 147, "endOffset": 184}, {"referenceID": 7, "context": "Setting aside the Bonferroni correction, often considered too simplistic and too drastic, one has the option of using sequential rejection methods (Benjamini and Liu, 1999; Holm, 1979), the q-value method of Storey (Storey, 2002) or making use of bootstrap (Lallich et al, 2006).", "startOffset": 147, "endOffset": 184}, {"referenceID": 22, "context": "Setting aside the Bonferroni correction, often considered too simplistic and too drastic, one has the option of using sequential rejection methods (Benjamini and Liu, 1999; Holm, 1979), the q-value method of Storey (Storey, 2002) or making use of bootstrap (Lallich et al, 2006).", "startOffset": 215, "endOffset": 229}], "year": 2015, "abstractText": "Feature-based format is the main data representation format used by machine learning algorithms. When the features do not properly describe the initial data, performance starts to degrade. Some algorithms address this problem by internally changing the representation space, but the newly-constructed features are rarely comprehensible. We seek to construct, in an unsupervised way, new features that are more appropriate for describing a given dataset and, at the same time, comprehensible for a human user. We propose two algorithms that construct the new features as conjunctions of the initial primitive features or their negations. The generated feature sets have reduced correlations between features and succeed in catching some of the hidden relations between individuals in a dataset. For example, a feature like sky\u2227\u00acbuilding\u2227 panorama would be true for non-urban images and is more informative than simple features expressing the presence or the absence of an object. The notion of Pareto optimality is used to evaluate feature sets and to obtain a balance between total correlation and the complexity of the resulted feature set. Statistical hypothesis testing is used in order to automatically determine the values of the parameters used for constructing a data-dependent feature set. We experimentally show that our approaches achieve the construction of informative feature sets for multiple datasets.", "creator": "LaTeX with hyperref package"}}}