{"id": "1512.05484", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2015", "title": "Deep Active Object Recognition by Joint Label and Action Prediction", "abstract": "An active object recognition system has the advantage of being able to act in the environment to capture images that are more suited for training and that lead to better performance at test time. In this paper, we propose a deep convolutional neural network for active object recognition that simultaneously predicts the object label, and selects the next action to perform on the object with the aim of improving recognition performance. We treat active object recognition as a reinforcement learning problem and derive the cost function to train the network for joint prediction of the object label and the action. A generative model of object similarities based on the Dirichlet distribution is proposed and embedded in the network for encoding the state of the system. The training is carried out by simultaneously minimizing the label and action prediction errors using gradient descent. We empirically show that the proposed network is able to predict both the object label and the actions on GERMS, a dataset for active object recognition. We compare the test label prediction accuracy of the proposed model with Dirichlet and Naive Bayes state encoding. The results of experiments suggest that the proposed model equipped with Dirichlet state encoding is superior in performance, and selects images that lead to better training and higher accuracy of label prediction at test time.", "histories": [["v1", "Thu, 17 Dec 2015 07:33:45 GMT  (5400kb,D)", "http://arxiv.org/abs/1512.05484v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mohsen malmir", "karan sikka", "deborah forster", "ian fasel", "javier r movellan", "garrison w cottrell"], "accepted": false, "id": "1512.05484"}, "pdf": {"name": "1512.05484.pdf", "metadata": {"source": "CRF", "title": "Deep Active Object Recognition by Joint Label and Action Prediction", "authors": ["Mohsen Malmira", "Karan Sikkab", "Deborah Forsterc", "Ian Faseld", "Javier R. Movelland", "Garrison W. Cottrella"], "emails": ["mmalmir@eng.ucsd.edu", "ksikka@eng.ucsd.edu", "forster@ucsd.edu", "ian@emotient.com", "javier@emotient.com", "gary@eng.ucsd.edu"], "sections": [{"heading": null, "text": "An active object recognition system has the advantage that it can act in the environment to capture images that are better suited for training and lead to better performance at the time of testing. In this work, we propose a deep Convolutionary Neural Network for Active Object Recognition, which simultaneously predicts the object label and selects the next action to be performed on the object with the aim of improving detection performance. We treat active object recognition as an intensified learning problem and derive the cost function to train the network to jointly predict the object label and action. We demonstrate empirically that the proposed network is capable of predicting both the object label and the actions on the GERMS Dataset for active object detection."}, {"heading": "1. Introduction", "text": "A robot that is able to control its environment is an example of an active object detection (AOR) that can be performed by a robot to detect an object in its environment."}, {"heading": "2. Literature Review", "text": "In fact, most of them will be able to move to another world in which they are able, in which they are able to move, in which they are able, and in which they are able to move."}, {"heading": "3. The GERMS Dataset", "text": "The GERMS dataset was collected as part of the RUBI project, which aims to develop robots that interact with infants in early childhood learning environments [13, 14, 15]. This dataset consists of 1365 video recordings of give-and-take experiments with 136 different objects, which are soft toys representing different types of human cells, microbes and disease-related organisms. Figure 1 shows the entire set of these toys. Each video consists of the robot (RUBI), which brings the seized object into its center of vision, rotates it 180 \u00b0 and then returns it. The dataset was recorded by the RUBI head camera at 30 frames per second. Data for GERMS was collected over two days. On the first day, each object was handed over to RUBI in one of 6 pre-defined poses, 3 to each arm, whereupon RUBI grabbed the object and took the images during filming."}, {"heading": "4. Proposed Network", "text": "In this thesis, we try to solve both problems simultaneously in order to reduce the training time of an AOR model. By including the errors from the prediction of actions in the extraction of visual characteristics, we hope to obtain properties that are suitable both for labeling and for predicting actions. The proposed network is presented in Figure 2. The input image is first transformed by a classification network into a series of beliefs about different object designations. Faith is then combined with the previously used faith vectors to produce a coding of the state of the system. This is done through the Mixture Faith Actualization Layer in the network. The accumulated faith is then transformed into action values that are used to select the next input screen.Next, we will describe each part of the network in detail and describe the challenges and their corresponding solution.First, we will deal with the transformation of images into beliefs via object classes followed by actuation followed by actuation."}, {"heading": "4.1. Single Image Classification", "text": "The aim of this part of the network is to transform a single image into beliefs about different object designations. The stage of feature extraction consists of 3 folding layers followed by 3 completely connected layers. The dimensions of each layer are shown in Figure 2. The processes of each layer are inspired by the model proposed in [17]. Each folding layer is followed by rectification, normalization across channels and maximum merging over a neighborhood of 2 x 2 with stride of 1. The drop-out for ReLU1 and ReLU2 uses P = 0.5. We will designate the GERMS data sets with D = {Ii, yi, Pi} Ni = 1, with Ideniz, R64 x 64 x 3 being the image captured by the robot camera, Part CRU2 = CR object, the cost and classification into cost category 1 (CR)."}, {"heading": "4.2. Action Value Prediction", "text": "Active object detection can be treated as an intensified learning problem, the aim of which is to learn an optimal policy approach by observing the initial observation system of the robot. (S \u2192 A from States S to Actions A. The optimal policy is to maximize the total reward for each interaction sequence. (s1) The optimal policy is to maximize the total reward for each interaction sequence. (s2) The total reward for an interaction sequence. (sT \u2212 1) \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 sTwhere si (si) \u2212 si + 1 is the transition from si to si + 1 by performing the action ai = (si). The total reward for an interaction sequence. \u2212 S is T R (s) = 0: T = 0 \u03b3 tR (st), where R (st) is a reward function and si + 1 is the transition from si + 1 by performing the action. (si) The total reward of actions for an overall interaction."}, {"heading": "4.3. State Encoding", "text": "Based on the current state of the system, an action is selected that is expected to reduce the ambiguity about the object label. An appealing choice is to transform the images into views of different target classes and use them as the state of the system. Based on the target label, the system will decide to perform an action to improve its target label. What we expect from the AOR system is to guide the robot to views that are more differentiated between the target classes. We first transform the input image Ii into a faith vector Bi = [Bi j] Cj = 1 that uses the first 7 levels of the network, combining Bi j 0, C and the beliefs generated as a vector with the previously observed faith vectors to shape the state of the system. The motivation for this encoding is that the combined faith encodes the ambiguity of the system about the target class and the views used."}, {"heading": "4.4. Training Network for Joint Label and Action Prediction", "text": "We have achieved this by minimizing the total cost of action value and label prediction. The total cost function for action value and label prediction is cost = CRL + CCL (10) The weights of the network in the visual feature extraction layers (Conv1, Conv2, Conv3, ReLU1, ReLU2, LU1) are trained on the basis of back propagation to (10), while the action layers (ReLU3, ReLU4 and LU2) are trained on the action prediction error by gradient descent (3). To learn the parameters of faith actualization, that is, we use gradient descent to the maximum probability of the data. The maximum probability of a dilette deviation on the action prediction error (3) is trained on the action prediction error (Div3). To determine the parameters of faith actualization, that is, we use the maximum probability of gradient ascent."}, {"heading": "4.5. Reward Function", "text": "Another component that has an important impact on the performance of our AOR system is the reward function, which maps the state of the system (4) in rewards. A simple choice for reward function is R (st) = {+ 1 if argmaxi [Bt] i = target label (It) \u2212 1 otherwise (12) We call this a reward function with correct label. A reward of + 1 (\u2212 1) is given to the system if the action at level t puts the object in a pose for which the predicted label is correct (wrong)."}, {"heading": "4.6. Action Coding", "text": "In order to be able to reach any position in the robot's articulated gripper, we use a series of relative rotations as actions of the system. Specifically, we use 10 actions to rotate the gripper from its current position by one of the following offset values: {\u00b1 \u03c04, \u00b1 \u03c0 8, \u00b1 \u03c0 16, \u00b1 \u03c0 32, \u00b1 \u03c0 64}. The total rotation range for each of the grippers of the robot is \u00b1 \u03c0. The actions are selected so that they are fine-grained enough that the robot can reach any position with a minimum number of possible movements. This coding is simple and flexible in the range of positions the robot can reach, but we have found that the policy can get stuck with a few actions without trying the rest. Coding the states with the Dirichlet faith update helps alleviate this problem to a certain degree, but it does not completely eliminate the problem. We solve this problem by forcing the algorithm to select the next best action, when the best action has already been seen."}, {"heading": "5. Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Training Details", "text": "We trained the network by minimizing the costs of classification, action value prediction (3) and negative log probability of the Dirichlet distributions (11); we used backpropagation with minibatches of size 128 to train the network; for Q (\u03bb), we used an initial learning rate of 0.1, multiplied by 0.5 after iterations, and then remained constant; the total number of training iterations is 4000; for each iteration, an interaction sequence of length 5 follows; the complete training algorithm is represented in algorithm 1; for Q (\u03bb), we used the policy of \"obesity\" in the training phase, gradually decreasing from 0.9 to 0.1; and we found that using a \"0\" in the test phase affects performance, so during the test phase we used \u03b5 = 0. The number of actions is 10 as described above, and there are a total of 136 object classes, resulting in a total of 139 coding states."}, {"heading": "5.2. Learning the Parameters of Dirichlet Distributions", "text": "Figure 4 shows the average negative log probability of the data among the Dirichlet distributions for the training of a DN model. This figure shows that the probability of data decreases after the first 1000 iterations, after which the change rate is reduced but not stopped. Algorithm 1 Training of the network for common labels and action forecasts. 1: Procedure TRAIN 2: R \u2190 1 3: for iteration = 1 To N do 4: I1, y \u2190 NextImage (Iteration) 5: s0 [0] 6: Actions \u2190 RandomActions (NumActions) 7: for t = 1 To NumMoves do 8: st, predictedActions \u2190 FeedForward (It, st \u2212 1, Actions) 9: It + 1, \u2190 NextImage (It, predictedActions) 7: for t = 1 To NumMoves do 8: st, predictedActions \u2190 FeedForward (It, 1 Ast, Ac9, AcIt = 1, It \u2190), (NextImage + 1) \u2190: It (LooktedActions)."}, {"heading": "5.3. Label Prediction Accuracy", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1. Comparing Naive Bayes and Dirichlet State Encoding", "text": "In the first experiment, we compare the effectiveness of the Dirichlet and Naive Bayes status encodings with respect to the accuracy of the label predictions. In the case of Naive Bayes models (NB), the state of the system is updated using (4), while the size and configuration of the rest of the network remains the same. Dirichlet status encoding is implemented using (9). We refer to Dirichlet models as (DR). For each encoding and for each arm, we create 10 different models and report the average precision of the test labels depending on the number of images observed, comparing Deep Active Object Recognition (DAOR) and Random (Rnd) action guidelines. Figure 5 shows the performance for these models. It is obvious that the Dirichlet model is superior to Naive Bayes in label prediction accuracy. The first point to be noted in Figure 5 is the difference in performance between Naive and Diffuse."}, {"heading": "5.3.2. Removing Duplicate Visits", "text": "We train a number of models using the Dirichlet state coding and force the policy to select non-duplicated joint positions for each action of an interaction sequence. This approach is easy to implement by keeping a history of the joint positions visited during an interaction sequence and selecting actions with the highest action value that do not lead to the joint positions visited. We call this model a Dirichlet with Non-Repeated Visits (DN). Comparison between DN and DR for Rnd and DAOR strategies (both of which are forced to perform new poses) is shown in Figure 6. Comparison between the models mentioned above is shown in Table 3. We see that the most powerful model is DN-DAOR, with the exception of Action 1 for the right arm, the DR-DAOR performs best. For both arms, Dirichlet models perform significantly better than Naive Bayes and improve the performance of the model by an average of 10% for the right arm and 14% for the left arm."}, {"heading": "5.3.3. Visualizing Policies", "text": "It can help us to understand the weakness and strength of different models by taking a closer look at the learned strategies. To this end, we visualize the successive actions in the interaction sequences of length 5, as shown for training data in Figure 7 and for test data in Figure 8. Each diagram represents actions in different lines, with the order of magnitude and orientation of the action represented by the length and direction of the corresponding arrow on the left. Each step of the interaction sequence is represented as a numbered column. The colored lines in each diagram link one action in column i with another action in column i + 1 only when these actions appear successively in interaction sequences at these time steps. The thickness of the lines describes the relative frequency with which two actions were observed on the data. Figure 7 visualizes the strategies DN-DAOR and NB-DAOR on the training data. This figure helps to illustrate the lower performance of the NB models as described above."}, {"heading": "6. CONCLUSIONS", "text": "The visual characteristics in the early stages of this network were trained by minimizing the cost of storing and labeling predictions. [20] The difference between the work presented here and the deeply monitored networks [20] is that in the latter, the training is done by minimizing the classification error in different layers, while in our approach we minimized the cost of storing and labeling predictions. We also adapted an alternative to the usual Naive Bayes beliefs that update the rule for government coding of the system. Naive Bayes has the potential to exceed subsets of training images, which could lead to lower accuracy during the test period. We used a generative model based on the Dirichlet distribution to model belief about the target classes and the actions carried out on them."}, {"heading": "7. Acknowledgments", "text": "The research presented here was funded in part by NSF IIS 0968573 SoCS, IIS INT2Large 0808767 and NSF SBE-0542013, and in part by NSF ACI-1541349 and OCI-1246396, the Office of the President of the University of California and the California Institute for Telecommunications and Information Technology (Calit2)."}, {"heading": "8. References", "text": "[1] J. Aloimonos, J. I. Weiss, and A. Bandyopadhyay International object, Active vision, International J. Computer Vision, vol. 1, no. 4, pp. 333-356, 1988. [2] R. Bajcsy, Active perception, Proceedings of the IEEE, vol. 76, no. 8, pp. 966-1005, 1988. [3] D. Wilkes, Adaptive 3-D object recognition from multiple views, Proceedings CVPR '92., 1992 IEEE Computer Society Conference on, pp. 136-141. IEEE, 1992. [4] M. Seibert and A. Waxman, Adaptive 3-D object recognition from multiple views, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 14, no., pp. 107-124, 1992. [5] B. Schiele and J. Crowley, Transformation for active object recognition, Sixth International on image, ppaytl."}], "references": [{"title": "Active vision", "author": ["J. Aloimonos", "J.I. Weiss", "A. Bandyopadhyay"], "venue": "International J. Computer Vision, vol. 1, no. 4, pp. 333-356", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1988}, {"title": "Active perception", "author": ["R. Bajcsy"], "venue": "Proceedings of the IEEE, vol. 76, no. 8, pp. 966-1005", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1988}, {"title": "Active object recognition", "author": ["D. Wilkes", "J.K. Tsotsos"], "venue": "Proceedings CVPR\u201992., 1992 IEEE Computer Society Conference on, pp. 136-141. IEEE", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "Adaptive 3-D object recognition from multiple views", "author": ["M. Seibert", "A.M. Waxman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 14, no. 2, pp. 107-124", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Transinformation for active object recognition", "author": ["B. Schiele", "J.L. Crowley"], "venue": "Computer Vision, Sixth International Conference on, pp. 249-254. IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Columbia object image library (COIL-100)", "author": ["S.A. Nene", "S.K. Nayar", "H. Murase"], "venue": "Technical Report CUCS-006-96, Columbia University", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "Appearance-based active object recognition", "author": ["H. Borotschnig", "L. Paletta", "M. Prantl", "A. Pinz"], "venue": "Image and Vision Computing, vol. 18, no. 9, pp. 715-727", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Active object recognition by view integration and reinforcement learning", "author": ["L. Paletta", "A. Pinz"], "venue": "Robotics and Autonomous Systems, vol. 31, no. 1, pp. 71-86", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Active object recognition: Looking for differences", "author": ["F.G. Callari", "F.P. Ferrie"], "venue": "International J. Computer Vision, vol. 43, no. 3, pp. 189-204", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Active object recognition on a humanoid robot", "author": ["B. Browatzki", "V. Tikhanoff", "G. Metta", "H.H. Bulthoff", "C. Wallraven"], "venue": "Robotics and Automation (ICRA), 2012 IEEE International Conference on, pp. 2021-2028, IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Active In-Hand Object Recognition on a Humanoid Robot", "author": ["B. Browatzki", "V. Tikhanoff", "G. Metta", "H.H. Bulthoff", "C. Wallraven"], "venue": "Robotics, IEEE Transactions on , vol. 30, no. 99, pp. 1-9", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonmyopic View Planning for Active Object Classification and Pose Estimation", "author": ["N. Atanasov", "B. Sankaran", "J.L. Ny", "G.J. Pappas", "K. Daniilidis"], "venue": "Robotics, IEEE Transactions on , vol. 30, no. 99, pp. 1078-1090", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Home Alone: Social Robots for Digital Ethnography of Toddler Behavior", "author": ["M. Malmir", "D. Forster", "K. Youngstrom", "L. Morrison", "J.R. Movellan"], "venue": "Computer Vision Workshops (ICCVW), 2013 IEEE International Conference on, pp. 762-768", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "HRI as a tool to monitor socioemotional development in early childhood education", "author": ["J.R. Movellan", "M. Malmir", "D. Forester"], "venue": "proc. HRI 2nd Workshop on Applications for Emotional Robots, Bielefeld, Germany", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Qlearning for Active Recognition of GERMS: Baseline performance on a standardized dataset for active learning", "author": ["M. Malmir", "K. Sikka", "D. Forster", "J. Movellan", "G.W. Cottrell"], "venue": "Proceedings of the British Machine Vision Conference (BMVC),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Learning from Delayed Rewards", "author": ["Watkins", "C.J.C. H"], "venue": "Ph.D. thesis,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1989}, {"title": "Imagenet classification with deep convolutional neural networks.", "author": ["Krizhevsky", "Alex", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Infomax control for acoustic exploration of objects by a mobile robot.", "author": ["Rebguns", "Antons", "Daniel Ford", "Ian R. Fasel"], "venue": "In Workshops at the Twenty- Fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Optimal camera parameter selection for state estimation with applications in object recognition.", "author": ["Denzler", "Joachim", "Christopher M. Brown", "Heinrich Niemann"], "venue": "In Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "There are a variety of approaches to active object recognition, the goal of which is to re-position sensors or change the environment so that the new inputs to the system become less ambiguous for label prediction [1, 2, 19].", "startOffset": 214, "endOffset": 224}, {"referenceID": 1, "context": "There are a variety of approaches to active object recognition, the goal of which is to re-position sensors or change the environment so that the new inputs to the system become less ambiguous for label prediction [1, 2, 19].", "startOffset": 214, "endOffset": 224}, {"referenceID": 18, "context": "There are a variety of approaches to active object recognition, the goal of which is to re-position sensors or change the environment so that the new inputs to the system become less ambiguous for label prediction [1, 2, 19].", "startOffset": 214, "endOffset": 224}, {"referenceID": 14, "context": "An issue with previous approaches to active object recognition is that they mostly used small simplistic datasets, which were not reflective of challenges in real-world applications [15].", "startOffset": 182, "endOffset": 186}, {"referenceID": 14, "context": "This paper extends our previous work, Deep Q-learning [15], where an action selection network was trained on top of a pre-trained convolutional neural network.", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": "Given this belief state, the control module produces actions that will affect the images observed in the future [19].", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "One of the earliest active systems for object recognition was developed by Wilkes and Tsotsos [3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "Seibert and Waxman explicitly model the views of an object by clustering the images acquired from the view-sphere of the object into aspects [4].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "Schiele and Crowley developed a framework for active object recognition by making an analogy between object recognition and information transmission [5].", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "They used the COIL-100 dataset for their experiments, which consists of 7200 images of 100 toy objects rotated in depth [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "formulate the observation planning in terms of maximization of the expected entropy loss over actions [7].", "startOffset": 102, "endOffset": 105}, {"referenceID": 8, "context": "Callari and Ferrie take into account the object modeling error and search for actions that simultaneously minimize both modeling variance and uncertainty of belief over objects [9].", "startOffset": 177, "endOffset": 180}, {"referenceID": 9, "context": "use a particle filter approach to determine the viewing pose of an object held in-hand by an iCub humanoid robot [10, 11].", "startOffset": 113, "endOffset": 121}, {"referenceID": 10, "context": "use a particle filter approach to determine the viewing pose of an object held in-hand by an iCub humanoid robot [10, 11].", "startOffset": 113, "endOffset": 121}, {"referenceID": 11, "context": "focus on the comparison of myopic greedy action selection that looks ahead only one step and non-myopic action selection which considers several time steps into the future [12].", "startOffset": 172, "endOffset": 176}, {"referenceID": 17, "context": "used acoustic properties of objects to learn an infomax controller to recognize a set of 10 objects [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 7, "context": "Paletta & Pinz [8] treat active object recognition as an instance of a reinforcement learning problem, using Q-learning to find the optimal policy.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "There are medium sized datasets such as COIL100, which consists of 7200 images of 100 toy objects rotated in depth [6].", "startOffset": 115, "endOffset": 118}, {"referenceID": 12, "context": "The GERMS dataset was collected in the context of the RUBI project, whose goal is to develop robots that interact with toddlers in early childhood education environments [13, 14, 15].", "startOffset": 170, "endOffset": 182}, {"referenceID": 13, "context": "The GERMS dataset was collected in the context of the RUBI project, whose goal is to develop robots that interact with toddlers in early childhood education environments [13, 14, 15].", "startOffset": 170, "endOffset": 182}, {"referenceID": 14, "context": "The GERMS dataset was collected in the context of the RUBI project, whose goal is to develop robots that interact with toddlers in early childhood education environments [13, 14, 15].", "startOffset": 170, "endOffset": 182}, {"referenceID": 16, "context": "The operations of each layer are inspired by the model proposed in [17].", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "integers [15].", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "In order to learn the optimal policy, we use the Q(\u03bb )algorithm to train the network to predict actions for improved classification [16].", "startOffset": 132, "endOffset": 136}, {"referenceID": 17, "context": "We use a generative model similar to [18] to calculate the state of the system given a set of images.", "startOffset": 37, "endOffset": 41}], "year": 2015, "abstractText": "An active object recognition system has the advantage of being able to act in the environment to capture images that are more suited for training and that lead to better performance at test time. In this paper, we propose a deep convolutional neural network for active object recognition that simultaneously predicts the object label, and selects the next action to perform on the object with the aim of improving recognition performance. We treat active object recognition as a reinforcement learning problem and derive the cost function to train the network for joint prediction of the object label and the action. A generative model of object similarities based on the Dirichlet distribution is proposed and embedded in the network for encoding the state of the system. The training is carried out by simultaneously minimizing the label and action prediction errors using gradient descent. We empirically show that the proposed network is able to predict both the object label and the actions on GERMS, a dataset for active object recognition. We compare the test label prediction accuracy of the proposed model with Dirichlet and Naive Bayes state encoding. The results of experiments suggest that \u2217Corresponding Author Email addresses: mmalmir@eng.ucsd.edu (Mohsen Malmir), ksikka@eng.ucsd.edu (Karan Sikka), forster@ucsd.edu (Deborah Forster), ian@emotient.com (Ian Fasel), javier@emotient.com (Javier R. Movellan), gary@eng.ucsd.edu (Garrison W. Cottrell) Preprint submitted to Computer Vision and Image Understanding December 18, 2015 ar X iv :1 51 2. 05 48 4v 1 [ cs .A I] 1 7 D ec 2 01 5 the proposed model equipped with Dirichlet state encoding is superior in performance, and selects images that lead to better training and higher accuracy of label prediction at test time.", "creator": "LaTeX with hyperref package"}}}