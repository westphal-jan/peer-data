{"id": "1512.05509", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2015", "title": "An Empirical Comparison of Neural Architectures for Reinforcement Learning in Partially Observable Environments", "abstract": "This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long Short-Term Memory, Gated Recurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures. A variant of fitted Q iteration, based on Advantage values instead of Q values, is also explored. The results show that GRU performs significantly better than LSTM and MUT1 for most of the problems considered, requiring less training episodes and less CPU time before learning a very good policy. Advantage learning also tends to produce better results.", "histories": [["v1", "Thu, 17 Dec 2015 09:45:51 GMT  (120kb,D)", "http://arxiv.org/abs/1512.05509v1", "Presented at the 27th Benelux Conference on Artificial Intelligence"]], "COMMENTS": "Presented at the 27th Benelux Conference on Artificial Intelligence", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["denis steckelmacher", "peter vrancx"], "accepted": false, "id": "1512.05509"}, "pdf": {"name": "1512.05509.pdf", "metadata": {"source": "CRF", "title": "An Empirical Comparison of Neural Architectures for Reinforcement Learning in Partially Observable Environments", "authors": ["Denis Steckelmacher", "Peter Vrancx"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Reinforcement Learning was originally developed for Markov Decision Processes (MDPs). It allows an agent to learn a policy to maximize a potentially delayed reward signal in a stochastic environment, and guarantees convergence to an optimal policy, provided the agent can experiment sufficiently and the environment in which he operates is Markovian. However, in many real-world problems the agent cannot perceive the full state of his environment and must make decisions based on incomplete observations of the system state. This partial observability leads to uncertainty about the actual state of the environment and makes the problem non-Markovian from the agent's point of view. One way to deal with partially observable environments is to equip the agent with a memory of past observations and actions in order to help him figure out what the current state of the environment is. This memory can be implemented in a variety of ways, including explicit history windows, but these networks can only function remotely by means of internal learning. [10]"}, {"heading": "2 Background", "text": "Discrete time enhancement learning consists of an agent who repeatedly observes his environment and performs actions. After each action on board, the state changes to st + 1, and the agent receives a reward rt + 1 = R (st, at, st + 1) and an observation ot + 1, o = f (st + 1).ar Xiv: 151 2.05 509v 1 [cs.N E] 17 Dec 2The agent has no knowledge of R (s, a, s) and f (s) and must interact with his environment to learn a policy \u03c0 (ot) that indicates the probability distribution of performing each action for a given observation. The optimal policy is that, when pursued by the agent, maximizes the cumulative discounted reward r = \u0441\u0442trt, whereby the reward on board does not take place but depends exclusively on his current observation and action."}, {"heading": "2.1 Q-Learning and Advantage Learning", "text": "Q-Learning [13] and Advantage Learning [6] allow an agent to learn a policy that approaches the optimal policy Q, since an infinite amount of time and in discrete areas is available. Q-Learning appreciates the Q (o, a) function that maps each state-action pair to the expected optimal cumulative discounted reward by performing a specific observation. At each step, the agent ot observes that the samples used are on and observed rt + 1 and ot + 1. Equation 1 is used to update the Q function after each time step, with 0 \u2264 \u2264 1 representing the learning factor.\u03b4t = rt + 1 + \u03b3max aQk (ot, at) = Qk (ot, at) + expect (2) Advantage Learning [6] refers to Q-Learning, but artificially reduces the value of non-optimal actions."}, {"heading": "2.2 Long Short Term Memory", "text": "An LSTM [7] cell stores a value. An output gate allows the cell to modulate its output strength, while an input gate controls the intensity of the input signal that is continuously added to the cell content. a forget, when set to zero, erases the cell content. ijt = \u03c3 (Wixt + Uiht \u2212 1) j (5) f jt = \u03c3 (Wfxt + Ufht \u2212 1) j (6) ojt = \u03c3 (Woxt + Uoht \u2212 1) c-jt = tanh \u2212 1) j (8) cjt = f-jt \u2212 1 (5) f-jt = Ufxt \u2212 1 (Wfj + 7) j (Wfj \u2212 1) \u2212 xt \u2212 1 (1) j \u2212 xt (1) j \u2212 xt (1) j \u2212 xt (1) j \u2212 xt (1) j \u2212 xt (1) j (Wfj + 1) j \u2212 xt (1) j \u2212 xt (1) j \u2212 xt (1) j \u2212 xt (1) j \u2212 xt (1) j \u2212 xt (1) j \u2212 xt (1) j \u2212 xt (1) j \u2212 xt = (1) j \u2212 xt = 1 = 1) j \u2212 xt \u2212 xt (1) j \u2212 xt \u2212 xt = xt = 1 \u2212 xt \u2212 xt = 1 = 1 = xt \u2212 xt \u2212 xt = xt \u2212 xt = xt = xt = xt \u2212 xt = xt = xt = xt \u2212 xt = xt = xt = xt = xt \u2212 xt = xt = xt = xt = xt \u2212 xt = xt = xt = xt = xt = xt \u2212 xt = xt \u2212 xt = xt = xt \u2212 xt = xt = xt = xt = xt = xt = xt = xt = xt = xt \u2212 xt = xt = xt = xt = xt = xt = xt = xt = xt = xt = xt = xt = xt = xt = xt = xt = xt = xt"}, {"heading": "2.3 Gated Recurrent Unit", "text": "GRU was recently introduced and follows a completely different design from LSTM [3, 4]. Instead of storing a value in a memory cell and updating it using input and forget-me-not, a GRU unit calculates an activation h gate for candidates based on their input and then generates an output that represents a mixture of past output and candidate activation; Equations 11 and 12 show how the Z (modulation) and R (reset) gates are calculated; Equations 13 and 14 show how the input is mixed with the last activation to generate candidate activation; and Equation 15 shows how the last activation and candidate activation are mixed to produce the new activation. \u2212 1The models themselves are available on Keras http: / / keras.io /, a python library that provides neural network primitives based on Theano [2]. Keras provides LM, GRMU (among other things), weighted layers (GRMUT)."}, {"heading": "2.4 MUT1", "text": "Jo \u0301 zefowicz et al. observed that GRU and LSTM are very different from each other, and wondered if other relapsing neural architectures could be used. To discover them, they developed a genetic algorithm that evaluated thousands of relapsing neural architectures. Upon completion of the experiment, they identified three architectures that performed as well or better on their test vectors than LSTM and GRU: MUT1, MUT2, and MUT3 [3]. In this paper, only MUT1 is taken into account, which yielded the best results in preliminary experiments.Equations 16 and 17 show that to calculate the value of the Z and R gates, equations 18 and 19 show how candidate activation can be calculated, and Equation 20 shows that the output of MUT1 uses the same type of mixing as the one used by GRU.zjt = 1 h (Wzxt) jh (jh = 1 Wh) (1)."}, {"heading": "3 Experimental Setup", "text": "To keep the training time manageable, the neural networks are trained to associate values with the last 10 observations, rather than with the entire history. LSTM, GRU, and MUT1 are able to associate values of any length sequences of inputs, but Keras requires all sequences it is trained on to have the same length (possibly with padding).The training must be done carefully, because one does not want the model to forget past experiences when learning a new group of episodes. The network has been configured to perform two training periods on the data, using a batch size of 10 (batches of 10 O10 x R | A | Samples are used to calculate an average gradient in performing the reverse propagation).The small number of epochs prevents the model from missing certain episodes."}, {"heading": "3.1 Environments", "text": "Three environments are used to evaluate the models of neural networks: the first is a simple, complete, observable 10 \u00d7 5 grid world with the initial position at (0, 2), the target at (9, 2) and an obstacle at (5, 2) (see Figure 2); the agent can observe his (x, y) coordinates; he receives a reward of \u2212 1, \u2212 5 at each step when he hits a wall or obstacle, and 10 when he reaches the target; the second environment is based on the same grid world as the first, but the agent can only observe his x-coordinates; the y-coordinate is masked to zero; the last environment is also based on the grid world, but the agent can only observe its orientation (whether it is directed up, down, left, or right, expressed as a 0 to 3 integer number) and the distance between it and the wall in front. This agentric environment is very close to what actual robots can represent neural."}, {"heading": "3.2 Experiments", "text": "Each experiment consists of 5000 episodes with a maximum of 500 time steps. During the episodes, the neural network is not trained on new data, but Qk + 1 (s, a) values are calculated on the basis of Qk (s, a) and stored in a list. After each batch of 10 episodes, the neural networks are trained on the basis of the Qk + 1 values as described in [11] and in Figure 3.The experiments themselves consist of trying to reach the target in one of the environments described in Section 3.1. Each experiment is executed 15 times for each combination of the following parameters: \u2022 Value rating: Q-Learning and Advantage Learning, \u03b1 = 0.2, \u03b3 = 0.9 and \u0445 = 0.3 \u2022 Architecture of neural networks: Forward-oriented perceptron with a single hidden layer (nnet), LSTM (lstm), GRU (gru) and MUT1 (mut1) \u2022 World: Grid world (Grid temperature), Grid world (partially observable) and Fixworld (starting point)."}, {"heading": "4 Empirical Results", "text": "Each experiment (see Section 3.2) is performed 15 times. The first step, in which the agent can maintain a mean (over the next 1000 steps) reward of more than \u2212 15 with a standard deviation, was performed again without this hint, without changing the results. The agent learns to look to the left, then up and uses these observations as a starting point. Results are presented in a medium / stddev format.Advantage Learning results in shorter learning times and standard deviations in the various neural networks for all experiments."}, {"heading": "5 Conclusion", "text": "LSTM, GRU, and MUT1 were compared using simple gain problems. It was shown that agents using LSTM and GRU to approximate Q- or Advantage-values perform significantly better than those using MUT1, receive higher rewards, and learn faster. GRU and LSTM offer comparable performance, with GRU often significantly better than LSTM. LSTM is never significantly better than GRU. Looking at the rewards that agents receive once they learn, rather than the time it takes to learn, GRU always achieves better results than LSTM. This shows that the use of GRU instead of LSTM should be considered when solving gain problems. In addition, the simpler GRU cell (compared to LSTM) allowed GRU-based agents to complete their 5000 episodes about two times faster than LSTM-based agents."}], "references": [{"title": "Reinforcement Learning with Long Short-Term Memory", "author": ["B. Bakker"], "venue": "Advances in Neural Information Processing Systems 14, pages 1475\u20131482", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Theano: a CPU and GPU Math Expression Compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde- Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "B", "author": ["K. Cho"], "venue": "van Merrienboer, \u00c7. G\u00fcl\u00e7ehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1724\u20131734", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "author": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1412.3555", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Tree-Based Batch Mode Reinforcement Learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel"], "venue": "Journal of Machine Learning Research, 6:503\u2013556", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Multi-player residual advantage learning with general function approximation", "author": ["M.E. Harmon", "L.C. Baird III"], "venue": "Wright Laboratory, WL/AACF, Wright-Patterson Air Force Base, OH", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u2013 1780", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["R. J\u00f3zefowicz", "W. Zaremba", "I. Sutskever"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, pages 2342\u20132350", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement Learning with Hidden States", "author": ["L.-J. Lin", "T. Mitchell"], "venue": "From animals to animats 2: Proceedings of the second international conference on simulation of adaptive behavior, volume 2, page 271. MIT Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "Learning to use selective attention and short-term memory in sequential tasks", "author": ["A.K. McCallum"], "venue": "From animals to animats 4: proceedings of the fourth international conference on simulation of adaptive behavior, volume 4, page 315. MIT Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method", "author": ["M.A. Riedmiller"], "venue": "Machine Learning: ECML 2005, 16th European Conference on Machine Learning, pages 317\u2013328", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "GNU Parallel - The Command-Line Power Tool", "author": ["O. Tange"], "venue": ";login: The USENIX Magazine,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, 8(3-4):279\u2013292", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 6, "context": "Abstract This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long ShortTerm Memory [7], Gated Recurrent Unit [3] and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures [8].", "startOffset": 221, "endOffset": 224}, {"referenceID": 2, "context": "Abstract This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long ShortTerm Memory [7], Gated Recurrent Unit [3] and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures [8].", "startOffset": 247, "endOffset": 250}, {"referenceID": 7, "context": "Abstract This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long ShortTerm Memory [7], Gated Recurrent Unit [3] and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures [8].", "startOffset": 358, "endOffset": 361}, {"referenceID": 5, "context": "A variant of fitted Q iteration, based on Advantage values [6, 1] instead of Q values, is also explored.", "startOffset": 59, "endOffset": 65}, {"referenceID": 0, "context": "A variant of fitted Q iteration, based on Advantage values [6, 1] instead of Q values, is also explored.", "startOffset": 59, "endOffset": 65}, {"referenceID": 8, "context": "This memory can be implemented in a variety of ways, including explicit history windows [9, 10], but this article only focuses on reinforcement learning using recurrent neural networks for function approximation.", "startOffset": 88, "endOffset": 95}, {"referenceID": 9, "context": "This memory can be implemented in a variety of ways, including explicit history windows [9, 10], but this article only focuses on reinforcement learning using recurrent neural networks for function approximation.", "startOffset": 88, "endOffset": 95}, {"referenceID": 6, "context": "These cycles give rise to dynamic temporal behavior, which can function as an internal memory that allows these networks to model values associated with sequences of observations [7, 4, 1].", "startOffset": 179, "endOffset": 188}, {"referenceID": 3, "context": "These cycles give rise to dynamic temporal behavior, which can function as an internal memory that allows these networks to model values associated with sequences of observations [7, 4, 1].", "startOffset": 179, "endOffset": 188}, {"referenceID": 0, "context": "These cycles give rise to dynamic temporal behavior, which can function as an internal memory that allows these networks to model values associated with sequences of observations [7, 4, 1].", "startOffset": 179, "endOffset": 188}, {"referenceID": 12, "context": "Q-Learning [13] and Advantage Learning [6] allow an agent to learn a policy that converges to the optimal policy given an infinite amount of time and in discrete domains.", "startOffset": 11, "endOffset": 15}, {"referenceID": 5, "context": "Q-Learning [13] and Advantage Learning [6] allow an agent to learn a policy that converges to the optimal policy given an infinite amount of time and in discrete domains.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "Advantage Learning [6] is related to Q-Learning, but artificially decreases the value of non-optimal actions.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "Equation 3 is used to update the Advantage values at each time step [1].", "startOffset": 68, "endOffset": 71}, {"referenceID": 10, "context": "It has been shown, however, that on-line Q-Learning can diverge, or converge very slowly, when used in combination with function approximation [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 10, "context": "The method used in this paper is the neural fitted Q iteration described in [11], an adaptation of fitted Q iteration [5] using neural networks.", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "The method used in this paper is the neural fitted Q iteration described in [11], an adaptation of fitted Q iteration [5] using neural networks.", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "2 Long Short Term Memory An LSTM [7] cell stores a value.", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "Figure 1: Cyclic architecture proposed by [1] (left), and architecture used in this paper (right).", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "In [1], Bakker proposes a neural network architecture tailored for reinforcement learning.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "GRU has been introduced recently and follows a design completely different from LSTM [3, 4].", "startOffset": 85, "endOffset": 91}, {"referenceID": 3, "context": "GRU has been introduced recently and follows a design completely different from LSTM [3, 4].", "startOffset": 85, "endOffset": 91}, {"referenceID": 1, "context": "io/, a Python library providing neural network primitives based on Theano [2].", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "The connection scheme in [1] makes the network layer graph cyclic, and hence impossible to build using the current version of Keras.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "Once the experiment was finished, they identified three architectures that performed as good as or better than LSTM and GRU on their test vectors: MUT1, MUT2 and MUT3 [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 10, "context": "After every batch of 10 episodes, the neural networks are trained on theQk+1 values, as described in [11] and shown in Figure 3.", "startOffset": 101, "endOffset": 105}], "year": 2015, "abstractText": "This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long ShortTerm Memory [7], Gated Recurrent Unit [3] and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures [8]. A variant of fitted Q iteration, based on Advantage values [6, 1] instead of Q values, is also explored. The results show that GRU performs significantly better than LSTM and MUT1 for most of the problems considered, requiring less training episodes and less CPU time before learning a very good policy. Advantage learning also tends to produce better results.", "creator": "LaTeX with hyperref package"}}}