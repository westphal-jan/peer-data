{"id": "1512.05808", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2015", "title": "Successive Ray Refinement and Its Application to Coordinate Descent for LASSO", "abstract": "Coordinate descent is one of the most popular approaches for solving Lasso and its extensions due to its simplicity and efficiency. When applying coordinate descent to solving Lasso, we update one coordinate at a time while fixing the remaining coordinates. Such an update, which is usually easy to compute, greedily decreases the objective function value. In this paper, we aim to improve its computational efficiency by reducing the number of coordinate descent iterations. To this end, we propose a novel technique called Successive Ray Refinement (SRR). SRR makes use of the following ray continuation property on the successive iterations: for a particular coordinate, the value obtained in the next iteration almost always lies on a ray that starts at its previous iteration and passes through the current iteration. Motivated by this ray-continuation property, we propose that coordinate descent be performed not directly on the previous iteration but on a refined search point that has the following properties: on one hand, it lies on a ray that starts at a history solution and passes through the previous iteration, and on the other hand, it achieves the minimum objective function value among all the points on the ray. We propose two schemes for defining the search point and show that the refined search point can be efficiently obtained. Empirical results for real and synthetic data sets show that the proposed SRR can significantly reduce the number of coordinate descent iterations, especially for small Lasso regularization parameters.", "histories": [["v1", "Thu, 17 Dec 2015 21:47:02 GMT  (880kb,D)", "http://arxiv.org/abs/1512.05808v1", "26 pages, 6 figures, 6 tables"]], "COMMENTS": "26 pages, 6 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jun liu", "zheng zhao", "ruiwen zhang"], "accepted": false, "id": "1512.05808"}, "pdf": {"name": "1512.05808.pdf", "metadata": {"source": "CRF", "title": "Successive Ray Refinement and Its Application to Coordinate Descent for LASSO", "authors": ["Jun Liu", "Zheng Zhao", "Ruiwen Zhang"], "emails": ["junliu.nt@gmail.com."], "sections": [{"heading": null, "text": "Such an update, which is usually easy to calculate, greedily reduces the value of the objective function. In this work, we aim to improve its computing power by reducing the number of iterations of the coordinate descent. To this end, we propose a novel technique called Successive Ray Refinement (SRR). Motivated by this beam continuation characteristic, we propose that the coordinate descent cannot be performed directly on the previous iteration, but on a refined search point that has the following properties: on the one hand, it lies on a beam that begins with its previous iteration and passes through the current iteration. Motivated by this beam continuation characteristic, we propose that the coordinate descent cannot be performed directly on the previous iteration, but on a refined search point that has the following properties: on one beam that begins with a historical solution and the previous iteration runs clearly, on the other hand, the points are searched for and the two points are suggested for finer."}, {"heading": "1 Introduction", "text": "rrrrrrcnlrrrrrrrrrrrrrrrrrrrrrrrrrrrrf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the retef\u00fc the retef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "2 Coordinate Descent For Lasso", "text": "In this section we first check the method of coordinate descent to solve lasso and then analyze the adjacent iterations to motivate the proposed SRR technique. Let \u03b2ki denote the ith element of \u03b2, which is achieved at the lowest iteration of the coordinate descent. In the coordinate descent we compute \u03b2ki, while we compute \u03b2j = \u03b2 k j, 1 \u2264 j < i, and \u03b2j = \u03b2k \u2212 1j, i < j \u2264 p. Specifically \u03b2ki is calculated as a minimizer for the following universal optimization problem: \u03b2ki = arg min \u03b2 f ([\u03b2k1,. \u2212 k \u2212 1, \u03b2 k \u2212 1 + 1,. \u2212 k \u2212 1 p] T). It can be computed in a closed form: \u03b2ki = S (xTi y \u2212 k)."}, {"heading": "30 -0.083806 -0.141752 0.468749 0.033883 0.218827 0.000082 ...", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "29 -0.082490 -0.142044 0.468368 0.032406 0.218288 0.000093", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "28 -0.081090 -0.142355 0.467964 0.030835 0.217715 0.000106", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 -0.031899 -0.149927 0.454929 -0.020507 0.197895 0.000950 ...", "text": "We demonstrate algorithm 1 using the following randomly generated X and y: X = \u2212 0,204708 \u2212 0,478943 \u2212 0,519439 \u2212 0,55\u03b2 = 1,965781 1,393406 \u2212 0,281746 0,76\u03b223 1,246435 1,007189 \u2212 1,296221 0,274992 0,228913 1,352917 0,886429 \u2212 2.001637 \u2212 0,371843 1,669025 \u2212 0,438570 \u2212 0,539741 0,476985 3,248944 \u2212 1,021228 \u2212 0,577087, (7) y = [0,124121, 0,302614, 0,523772, 0,00940, 1,343810] T. (8) We show the iterations of the coordinate origin for lasso with \u03bb = 0 in Table 1 and Figure 1 (a). We insert f = 0 to facilitate eigenvalue analysis in section 5."}, {"heading": "3 Successive Ray Refinement", "text": "For the proposed SRR technique, we use the radiation expiration property shown in Figure 1, Table 1 and Table 2. Our idea is as follows: To obtain \u03b2k + 1, we perform a coordinate descent on the basis of a refined search point sk and not on the basis of its previous solution \u03b2k. We propose to specify the refined search point as follows: sk = (1 \u2212 \u03b1k) hk + \u03b1k\u03b2k, (11)"}, {"heading": "10 1.897021 1.240529 2.128716 2.068764 1.924043 ...", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "28 1.939534 1.939949 1.939646 1.939628 1.939556", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "29 1.939545 1.939821 1.939620 1.939608 1.939560", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "30 1.939552 1.939736 1.939602 1.939594 1.939562", "text": "Where hk is a correctly chosen historical solution, \u03b2k is the current solution, and \u03b2 is an optimal refinement factor that optimizes the following universal optimization problem: \u03b1k = arg min \u03b1 {g (\u03b1) = f (1 \u2212 \u03b1) hk + \u03b1\u03b2k)}. (12) The attitude of hk to one of the historical solutions is based on the following two considerations. First, we want to use the proposed continuation property to reduce the number of iterations. Second, we need to ensure that the universal optimization problem in Equation (12) can be efficiently calculated. We discuss the calculation of Equation (12) in Section 4.Figure 2 illustrates the proposed SRR technique. If we have sk = \u03b2k; this is the refined search point that becomes the current solution \u03b2k. If we equate = 0, wehave sk = hk; this is the refined search point."}, {"heading": "3.1 Successive Ray Refinement Chain", "text": "In the first scheme, the coordinate update is based on the search point sk \u2212 1. (18) That is, the progression point is set to the most recent refined search point = 1. Figure 3 demonstrates this scheme. Since the generated points follow a chain structure \u2212 \u2212 we call this scheme the Successive Beam Refinement Chain (SRRK). In SRC, sk \u2212 1, \u03b2k and sk are on the same line. Furthermore, the coordinate descent (CD) controls the direction of the chain. In this figure, the optimal refinement factor \u03b1k is assumed to be greater than 1 in each step. According to Theorem 1, \u03b1k > 0. If the coordinate descent (0, 1), sk is between sk \u2212 1 and \u03b2k, sk can be used with \u03b2k.In algorithm 2, we apply the proposed SRRC to the coordinate descent for lasso. Compared to the traditional coordinate descent in algorithm 1, the coordinate update is based on the search point sk \u2212 1 and not on the previous solution \u2212 1."}, {"heading": "3.2 Successive Ray Refinement Triangle", "text": "In the second scheme sethk = \u03b2k \u2212 1. (19) Figure 4 illustrates this scheme. Since the generated points follow a triangular structure, we call this scheme the Successive Ray Refinement Triangle (SRRT). SRRT is less greedy than SRRRT because \u03b2k \u2212 1 results in a higher objective function value than sk \u2212 1. However, SRRT can sometimes be better at solving the lasso than SRRC. Algorithm 3 demonstrates the application of the proposed SRRT technique to the coordinate descent for lasso. Similar to algorithm 2, if\u03b1k is set to 1 in line 9, algorithm 3 to the traditional coordinate descent in algorithm 1. Table 4 illustrates algorithm 3. Similar to SRRC, SRRT significantly reduces the number of iterations used in the coordinate descent for lasso."}, {"heading": "3.3 Convergence of CD plus SRR", "text": "In this subsection we show that both the combination of CD and SRRC (CD + SRK = \u03b2k = \u03b2\u03b2k = \u03b2k = \u03b2\u03b2\u03b2\u03b2k = \u03b2\u03b2a = \u03b2a = \u03b2a = \u03b2a = \u03b2a (\u03b2 \u03b2 \u03b2 + SRRT) and the combination of CD + SRRT (\u03b2 \u03b2 + SRRT) are guaranteed to lead to convergence. Theorem 3 \u2212 \u2212 \u2212 \u03b2a For the sequence s0, \u03b21, s1, \u03b22, \u03b23,. (SRRRT + SRRT) the objective function value is decreased monotonously until convergence is reached; that is, f (sk \u2212 1). (\u03b2k). (\u03b2k). (\u03b2k). (\u03b2k). (\u03b21). (\u03b21). (20) Algorithm 3 + SRRT (CD + SRRT)."}, {"heading": "4 Efficient Refinement Factor Computation", "text": "In this section we will discuss how to use the refinement factor hi hi hi hi hi hi i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i"}, {"heading": "4.1 An Algorithm Based on Sorting", "text": "To calculate the refinement factor, an approach is to sort wi as follows: First, we sort wi, i-ig (hk, \u03b2k) and assume that wi0 \u2264 wi1 \u2264... \u2264 wi-ig (hk, \u03b2k) |. Second, we evaluate for j = 1, 2,.., |. (hk, \u03b2k) | with the following three cases: 1. If 0% ig is g (wij), we have \u03b1k = wij and finish the search. 2. If an element in g (wij) is positive, \u03b1 k lies in the piecewise line, starting \u03b1 = wij \u2212 1 and ending \u03b1 = wij, and it can be calculated analytically. 3. If all elements in g (wij) are negative, we set j = j + 1 and continue the search."}, {"heading": "4.2 An Algorithm Based on Bisection", "text": "A second approach is to use the improved two-cut proposed in [7], the idea being to 1) establish a first estimate of the interval [\u03b11, \u03b12] to which the root belongs, with all elements in \u2202 g (\u03b11) being negative and all elements in \u2202 g (\u03b12) being positive, 2) to evaluate the interval at \u03b1 = \u03b11 + \u03b122 and to update the interval to [\u03b11, \u03b1) if all elements in \u2202 g (\u03b1) are positive or to update to [\u03b1, \u03b12) if all elements in \u2202 g (\u03b1) are negative, 3) to set the value of \u03b1 to the largest value of wi, which satisfies wi < \u03b1 if all elements in \u0445 g (\u03b1) are positive or to the smallest value of wi, which satisfies wi > \u03b1, if all elements in \u0445 g (\u03b1) are negative, and 4) to repeat 2) and 3) until the root of wave g (\u03b1) is determined."}, {"heading": "5 An Eigenvalue Analysis on the Proposed SRR", "text": "Let us write A = XXT = L + D + U, (36) where D is the diagonal part of A, L = \u03b2k = \u03b2y method (52) is A's strictly lower triangular part, and U is A's strictly upper triangular part. It is easy to see that Lij = {xTi xj i < j0 i \u2265 j, (37) Dij = {xTi xi i = j0 i = j, (38) Uij = {xTi xj i > j0 i \u2264 j. (39) We can rewrite the equation (2) asDii\u03b2 k i = S (x T i \u2212 Li: \u03b2k \u2212 Ui: \u03b2k \u2212 1, \u03bb), (40) where Li: and Ui: denote the ith series of L and U, respectively. Therefore, we can write the equation of H as an equation: equality of H = zero."}, {"heading": "5.1 An Eigenvalue Analysis on CD+SRRC", "text": "For CD SRRC in algorithm 2, if \u03bb = 0 we have\u03b2k = (L + D) \u2212 1 [XTy \u2212 Usk \u2212 1] (55) sk = (1 \u2212 \u03b1k) sk \u2212 1 + \u03b1k\u03b2k. (56) It can be shown that sk \u2212 1 = \u03b1k [\u2212 (L + D) \u2212 1U + 1 \u2212 \u03b1 k \u2212 1\u03b1k \u2212 1 I] (sk \u2212 1 \u2212 sk \u2212 2), (57) \u03b2k \u2212 \u03b2k \u2212 1 = \u2212 (L + D) \u2212 1U (sk \u2212 1 \u2212 sk \u2212 2). (58) If k \u2265 2, we denote Ak = \u03b1k [\u2212 (L + D) \u2212 1U \u2212 1 \u2212 \u03b1 k \u2212 1 \u2212 1 I]. (59) It can be shown that Ak = P\u0441kP \u2212 1, (60) where we find the equation (D = diag = diag (D = 30,."}, {"heading": "5.2 An Eigenvalue Analysis on CD+SRRT", "text": "For coordinate descent with SRRT in algorithm 3, if \u03bb = 0 we have\u03b2k = (L + D) \u2212 1 [XTy \u2212 Usk \u2212 1] (66) sk = (1 \u2212 \u03b1k) \u03b2k \u2212 1 + \u03b1k\u03b2k. (67) If k \u2265 3, it can be shown that \u03b2k \u2212 \u03b2k \u2212 1 = G [(1 \u2212 \u03b1k \u2212 2) (\u03b2k \u2212 2 \u2212 \u03b2k \u2212 3) + \u03b1k \u2212 1 (\u03b2k \u2212 1 \u2212 \u03b2k \u2212 2)]. (68) If k = 2, we have \u03b22 \u2212 \u03b21 (\u03b21 \u2212 \u03b20). (69) With recursion in equation (68) we can \u03b23 \u2212 \u03b22 = [(1 \u2212 1 \u2212 \u03b21) G + \u03b2k] (\u03b21 \u2212 \u03b20)."}, {"heading": "6 Related Work", "text": "In this section, we compare our proposed SRR with successive overrelaxation [17] and the method of accelerated gradient descent [9]."}, {"heading": "6.1 Relationship between SRRC and Successive Over-Relaxation", "text": "Successive overrelaxation (SOR) is a classic approach to accelerating the GaussSeidel approach. Our discussion in this section only considers \u03bb = 0 because SOR aims at accelerating the Gauss-Seidel approach. Of equation (43) we have (wL + D) \u03b2 = wXTy \u2212 (w \u2212 1) D\u03b2, \u0394w > 0. (78) The iteration used in successive overrelaxation is: \u03b2k = (wL + D) \u2212 1 [wU + (w \u2212 1) D] \u03b2k \u2212 1, (79) which can be achieved by inserting \u03b2k and \u03b2k \u2212 1 into equation (78). Equation (79) can be rewritten as: \u03b2k = \u03b2k \u2212 1 \u2212 w (wL + D) \u2212 1 [XTXTXTXTk \u2212 1]."}, {"heading": "6.2 Relationship between SRRT and the Nesterov\u2019s Method", "text": "The SRRT scheme shown in Figure 4 is similar to the Nesterov method in that both use a search point in the iterations, and both specify the search point usingsk = (1 \u2212 \u03b1k) \u03b2k \u2212 1 + \u03b1k\u03b2k. (82) The crucial difference, however, is that the \u03b1k used in the Nesterov method is predefined according to a given formula, while the \u03b1k used in the SRRT is intended to optimize the objective function shown in Equation (12). If the Nesterov method sets \u03b1k to optimize the objective function, it is reduced to the traditional steepest descent method, thereby losing the good acceleration property of the Nesterov method."}, {"heading": "7 Experiments", "text": "In this section we report on experimental results for synthetic and real datasets and examine the number of iterations of CD, CD + SRRC and CD + SRRT to solve lasso. The computing time consumed is proportional to the number of iterations. Synthetic datasets We generate the synthetic data as follows: The entries in n \u00b7 p design matrix X and n \u00b7 1 response y are drawn from a Gaussian distribution. We try the following three settings of n and p: 1) n = 500, p = 1000, 2) n = 1000, and 3) n = CD = 1000, p = 500. Real datasets We use the following three real datasets provided in [2]: Leukemia, Kolon, and Gisette. The dataset Leukemia has n = 38 samples and p = 7129 variables. The dataset Kolon has n = 62 samples and p = 2000 variables."}, {"heading": "8 Conclusion", "text": "In this thesis, we propose a novel technique called successive beam refinement. Our proposed SRR is motivated by an interesting beam continuation characteristic in coordinate descend siterations: for a given coordinate, the value obtained in the next iteration is almost always on a beam that starts at its previous iteration and passes through the current iteration. We propose two schemes for the SRR and apply them to the solution of lasso with coordinate descend. Empirical results for real and synthetic datasets show that the proposed SRR can significantly reduce the number of coordinate descend siterations, especially when the regulation parameter is small. We have identified the convergence of CD + SRR, and it is interesting to study the convergence rate. We focus on a minimal square loss function in (1), and we plan to apply the SRR technique to the solution of generalized models."}], "references": [{"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J.H. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Statistical Software,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Safe feature elimination in sparse supervised learning", "author": ["L. Ghaoui", "V. Viallon", "T. Rabbani"], "venue": "Pacific Journal of Optimization,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "An interior-point method for large-scale l1regularized logistic regression", "author": ["K. Koh", "S. Kim", "S. Boyd"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Efficient euclidean projections in linear time", "author": ["J. Liu", "J. Ye"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Safe screening with variational inequalities and its application to lasso", "author": ["J. Liu", "Z. Zhao", "J. Wang", "J. Ye"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Introductory lectures on convex optimization : a basic course", "author": ["Y. Nesterov"], "venue": "Applied optimization. Kluwer Academic Publ.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Safe screening of non-support vectors in pathwise SVM computation", "author": ["K. Ogawa", "Y. Suzuki", "I. Takeuchi"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Stochastic methods for `1 regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Strong rules for discarding predictors in lasso-type problems", "author": ["R. Tibshirani", "J. Bien", "J.H. Friedman", "T. Hastie", "N. Simon", "J. Taylor", "R.J. Tibshirani"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng", "Communicated O.L. Mangasarian"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Lasso screening rules via dual polytope projection", "author": ["J. Wang", "B. Lin", "P. Gong", "P. Wonka", "J. Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R.D. Nowak", "M.A.T. Figueiredo"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Iterative methods for solving partial difference equations of elliptical type", "author": ["D.M. Yong"], "venue": "PhD thesis,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1950}, {"title": "An improved glmnet for l1-regularized logistic regression", "author": ["G.X. Yuan", "C.H. Ho", "C.J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Learning sparse representations of high dimensional data on large scale dictionaries", "author": ["J.X. Zhen", "X. Hao", "J.R. Peter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}], "referenceMentions": [{"referenceID": 11, "context": "Lasso [12] is an effective technique for analyzing high-dimensional data.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "Least Angle Regression (LARS) [3] is one of the most well-known homotopy approaches for Lasso.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.", "startOffset": 52, "endOffset": 59}, {"referenceID": 17, "context": "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.", "startOffset": 52, "endOffset": 59}, {"referenceID": 0, "context": "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.", "startOffset": 89, "endOffset": 96}, {"referenceID": 15, "context": "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.", "startOffset": 89, "endOffset": 96}, {"referenceID": 5, "context": "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.", "startOffset": 124, "endOffset": 127}, {"referenceID": 10, "context": "These methods include the coordinate descent method [4, 18], the gradient descent method [1, 16], the interior-point method [6], the stochastic method [11], and so on.", "startOffset": 151, "endOffset": 155}, {"referenceID": 4, "context": "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].", "startOffset": 131, "endOffset": 153}, {"referenceID": 7, "context": "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].", "startOffset": 131, "endOffset": 153}, {"referenceID": 9, "context": "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].", "startOffset": 131, "endOffset": 153}, {"referenceID": 12, "context": "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].", "startOffset": 131, "endOffset": 153}, {"referenceID": 14, "context": "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].", "startOffset": 131, "endOffset": 153}, {"referenceID": 18, "context": "To improve the efficiency of optimizing the Lasso problem in Equation (1), the screening technique has been extensively studied in [5, 8, 10, 13, 15, 19].", "startOffset": 131, "endOffset": 153}, {"referenceID": 13, "context": "Since the non-smooth `1 penalty in Equation (1) is separable, the algorithm is guaranteed to converge [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "If \u03b1 < 0, due to the convexity of g(\u03b1), we have g((1\u2212 \u03b8)\u03b1 + \u03b8) \u2264 (1\u2212 \u03b8)g(\u03b1) + \u03b8g(1),\u2200\u03b8 \u2208 [0, 1].", "startOffset": 89, "endOffset": 95}, {"referenceID": 0, "context": "Setting \u03b8 = \u03b1 k \u03b1k\u22121 , we have g(0) \u2264 \u22121 \u03b1k \u2212 1 g(\u03b1) + \u03b1 \u03b1k \u2212 1 g(1),\u2200\u03b8 \u2208 [0, 1].", "startOffset": 74, "endOffset": 80}, {"referenceID": 6, "context": "2 An Algorithm Based on Bisection A second approach is to make use of the improved bisection proposed in [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "With a similar implementation as in [7], the improved bisection approach has a time complexity of O(p).", "startOffset": 36, "endOffset": 39}, {"referenceID": 16, "context": "In this section, we compare our proposed SRR with successive over-relaxation [17] and the accelerated gradient descent method [9].", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "In this section, we compare our proposed SRR with successive over-relaxation [17] and the accelerated gradient descent method [9].", "startOffset": 126, "endOffset": 129}, {"referenceID": 16, "context": "When the design matrix has some special structures, it has been shown in [17] that the optimal value of w can be found for SOR.", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "Real Data Sets We make use of the following three real data sets provided in [2]: leukemia, colon, and gisette.", "startOffset": 77, "endOffset": 80}], "year": 2015, "abstractText": "Coordinate descent is one of the most popular approaches for solving Lasso and its extensions due to its simplicity and efficiency. When applying coordinate descent to solving Lasso, we update one coordinate at a time while fixing the remaining coordinates. Such an update, which is usually easy to compute, greedily decreases the objective function value. In this paper, we aim to improve its computational efficiency by reducing the number of coordinate descent iterations. To this end, we propose a novel technique called Successive Ray Refinement (SRR). SRR makes use of the following ray continuation property on the successive iterations: for a particular coordinate, the value obtained in the next iteration almost always lies on a ray that starts at its previous iteration and passes through the current iteration. Motivated by this ray-continuation property, we propose that coordinate descent be performed not directly on the previous iteration but on a refined search point that has the following properties: on one hand, it lies on a ray that starts at a history solution and passes through the previous iteration, and on the other hand, it achieves the minimum objective function value among all the points on the ray. We propose two schemes for defining the search point and show that the refined search point can be efficiently obtained. Empirical results for real and synthetic data sets show that the proposed SRR can significantly reduce the number of coordinate descent iterations, especially for small Lasso regularization parameters.", "creator": "LaTeX with hyperref package"}}}