{"id": "1512.05919", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2015", "title": "A Planning based Framework for Essay Generation", "abstract": "Generating an article automatically with computer program is a challenging task in artificial intelligence and natural language processing. In this paper, we target at essay generation, which takes as input a topic word in mind and generates an organized article under the theme of the topic. We follow the idea of text planning \\cite{Reiter1997} and develop an essay generation framework. The framework consists of three components, including topic understanding, sentence extraction and sentence reordering. For each component, we studied several statistical algorithms and empirically compared between them in terms of qualitative or quantitative analysis. Although we run experiments on Chinese corpus, the method is language independent and can be easily adapted to other language. We lay out the remaining challenges and suggest avenues for future research.", "histories": [["v1", "Fri, 18 Dec 2015 12:10:42 GMT  (540kb,D)", "https://arxiv.org/abs/1512.05919v1", null], ["v2", "Wed, 6 Jan 2016 07:49:07 GMT  (540kb,D)", "http://arxiv.org/abs/1512.05919v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bing qin", "duyu tang", "xinwei geng", "dandan ning", "jiahao liu", "ting liu"], "accepted": false, "id": "1512.05919"}, "pdf": {"name": "1512.05919.pdf", "metadata": {"source": "CRF", "title": "A Planning based Framework for Essay Generation", "authors": ["Bing Qin", "Duyu Tang", "Xinwei Geng", "Dandan Ning", "Jiahao Liu", "Ting Liu"], "emails": ["jhliu@ir.hit.edu.cn", "tliu@ir.hit.edu.cn"], "sections": [{"heading": null, "text": "Automatically generating an article with computer programs is a challenging task in the field of artificial intelligence and natural language processing. In this work, we aim to generate essays that take a topic word as input and generate an organized article on the topic. We follow the idea of text planning (Reiter and Dale, 1997) and develop a framework for generating essays. The framework consists of three components, including topic understanding, sentence extraction and sentence assignment. For each component, we have examined several statistical algorithms and compared them empirically with each other for qualitative or quantitative analysis. Although we conduct experiments on Chinese corpus, the method is language-independent and can be easily adapted to other languages. We outline the remaining challenges and suggest ways forward for future research."}, {"heading": "1 Introduction", "text": "In general, the tasks of natural language processing are divided into natural language understanding and natural language generation (Manning and Sch\u00fctze, 1999; Jurafsky and Martin, 2000), the former taking a piece of text as input and putting the syntactic / semantic information into the text, the latter concentrating on generating a text from the mind or from a large collection of text corpus. In this paper, we focus on natural language generation (Reiter et al., 2000)."}, {"heading": "2 The Framework", "text": "In this section, we describe the planning-based framework. As illustrated in Figure 1, the framework consists of three steps: topic understanding, sentence selection, and sentence organization. We also add a feedback mechanism to enrich the supporting words of each argument. We describe the details of these components."}, {"heading": "2.1 Topic Understanding", "text": "When a person writes an article under the theme of a particular topic, he typically finds some arguments to support his main idea. For example, an article about \"cell phone\" could have three paragraphs that indicate the ratings in the direction of \"call quality,\" \"appearance\" and \"battery life.\" These arguments are some important characteristics of the topic from some aspects. The evidence about each argument makes the whole article coherent. Based on these considerations, we consider the topic understanding as the first component of the framework. Given a topic topic word as input, topic understanding analyzes its semantic meaning and gives several arguments to support the topic. Each argument is presented as a collection of words, each of which is semantically related to the topic from some aspects. We consider topic understanding as two cascaded steps: topic extension and topic clustering of the topic is shown in Figure 2. The first step finds a collection of words with similar semantic meanings."}, {"heading": "2.2 Sentence Selecting", "text": "In fact, it is the case that most of them are able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able"}, {"heading": "2.3 Sentence Organizing", "text": "In this part, we describe the sentence that summarizes a series of chaotic sentences in a single sentence. It can be considered a structural predictive problem, and the aim is to foresee a desirable structure for a collection of sentences. In this sense, it is possible to create a list of sentences that are summarized in a single sentence."}, {"heading": "3 Experiment", "text": "We compare different methods empirically for each component. For the parts \"Topic Understanding\" and \"Sentence Selecting,\" we evaluate methods only qualitatively, because we do not have the basic truth. For the part \"Sentence Organizing,\" we also evaluate different methods quantitatively, because the original sentence sequence in a document could be considered a basic truth. We conduct experiments with a Chinese data set, which we crawl out of the web containing 6,683 documents with 193,210 sentences."}, {"heading": "3.1 Evaluation on Topic Understanding", "text": "Since there are two steps in this part, we evaluate them separately. Results on youth are presented in Table 1, where Thes is learned on the basis of Thesaurus (Mikolov et al., 2013). In Thesaurus-based methods, we filter out the words whose length is 1, because most of them have no concrete meaning. Despite the application of this filter rule, we find that the results of Thes are worse than others. Supporting words are formal, are not often used in user-generated articles. Furthermore, the meanings of supporting words are topic-focused and do not go beyond the literal meaning of the input word. This is partly caused by reporting on the Thesaurus. We find that the results of TM and WE are comparable and better than those of ThAP in Chinese examples that are not related to similar words."}, {"heading": "3.2 Evaluation on Sentence Selecting", "text": "The best selected results (in Chinese) can be found in Table 2. We can find that the sentences obtained in the counting method are typically longer, as they prefer the sentences with more keywords. We believe that these results are better suited to act as prompt sentences because they contain more specific evidence. On the contrary, the results of the embedding-based method are typically shorter and more coherent, partly caused by the way we have composed sentence vectors. Such results could be regarded as thematic or final sentences that are more abstract. In both methods, it is somewhat disappointing that they prefer the selection of sentences that contain thematic words such as \"youth,\" which is less diverse than we expected."}, {"heading": "3.3 Evaluation on Sentence Organizing", "text": "In this part, we will use two experimental settings to compare between different four coherence functions. In the first setting, we will take greedy frameworks as a case study and qualitatively evaluate them in the real system by showing the sentence orders generated from different coherence functions, including BOW (Boolean), BOW (Frequency), embedding (Avg) and recursive NN. In the second setting, we will evaluate them quantitatively on a hold-out data set consisting of several documents. Input for each coherence model is the same, namely the sentences of a document and the first sentence. The output is the order generated from each coherence model. As we have the original sentence sequence, we can consider it as the basic truth and evaluate the quantitative performance in terms of bigrams of sentences similar to Bleu (Papineni et al., 2002)."}, {"heading": "4 Related Work", "text": "We will briefly discuss some related work in the literature on natural language generation and essay writing in this section. Natural language generation (NLG) is a fundamental and challenging task in the processing of natural language and computer linguistics (Manning et al., 1999; Jurafsky and Martin, 2000; Reiter et al., 2000). The task of essay generation could be considered a special way of generating natural language. Existing NLG approaches can be divided into three categories: template-based methods, gram-based methods, and statistical methods. Template-based methods typically use manually designed templates with some slots and replace words to generate new articles. Grammar-based methods go a step further by designing some structured templates and composing an article with a computer program. Statistical-based methods focus on learning the elaborate patterns from the web and generating articles automatically."}, {"heading": "5 Conclusion and Future Directions", "text": "In summary, we are developing a planning-based framework to generate an article by using a topic word as an input; the framework consists of three components: a topic understanding component, a sentence selection component, and a sentence organization component; we are also adding a feedback mechanism to improve the results of the topic understanding; for each component, we are examining several methods and conducting a case study on a Chinese corpus; we are showing that topic understanding and word embedding-based methods work better than thesaurus-based methods; the recursive neural network-based model works better than bag-of-word and the embedding of average similarity-driven methods for sentence organization; there remains a lot of challenges in this line of research; one direction is how to quantitatively evaluate the effectiveness of each internal component, as well as the final generated article; in this work, we are evaluating only the sentence by treating the part as the gold part to organize it."}], "references": [{"title": "A simple domain-independent probabilistic approach to generation", "author": ["Angeli et al.2010] Gabor Angeli", "Percy Liang", "Dan Klein"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Angeli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Angeli et al\\.", "year": 2010}, {"title": "Probabilistic generation of weather forecast texts. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics", "author": ["Anja Belz"], "venue": "Proceedings of the Main Conference,", "citeRegEx": "Belz.,? \\Q2007\\E", "shortCiteRegEx": "Belz.", "year": 2007}, {"title": "Clustering by passing messages between data points", "author": ["Frey", "Dueck2007] Brendan J Frey", "Delbert Dueck"], "venue": null, "citeRegEx": "Frey et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Frey et al\\.", "year": 2007}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Salakhutdinov2006] G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Lcsts: A large scale chinese short text summarization dataset", "author": ["Hu et al.2015] Baotian Hu", "Qingcai Chen", "Fangze Zhu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Hu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "Generating chinese couplets using a statistical MT approach", "author": ["Jiang", "Zhou2008] Long Jiang", "Ming Zhou"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics (Coling", "citeRegEx": "Jiang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2008}, {"title": "Speech & language processing. Pearson Education India", "author": ["Jurafsky", "Martin2000] Dan Jurafsky", "James H Martin"], "venue": null, "citeRegEx": "Jurafsky et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Jurafsky et al\\.", "year": 2000}, {"title": "A model of coherence based on distributed sentence representation", "author": ["Li", "Hovy2014] Jiwei Li", "Eduard Hovy"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Recursive deep models for discourse parsing", "author": ["Li et al.2014] Jiwei Li", "Rumeng Li", "Eduard Hovy"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Li et al.2015] Jiwei Li", "Thang Luong", "Dan Jurafsky"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In Text summarization branches out: Proceedings of the ACL-04 workshop,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["Mann", "Thompson1988] William C Mann", "Sandra A Thompson"], "venue": "TextInterdisciplinary Journal for the Study of Discourse,", "citeRegEx": "Mann et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Mann et al\\.", "year": 1988}, {"title": "Foundations of statistical natural language processing", "author": ["Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Manning et al\\.", "year": 1999}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "The penn discourse treebank 2.0", "author": ["Prasad et al.2008] Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind K Joshi", "Bonnie L Webber"], "venue": "In LREC. Citeseer", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Building applied natural language generation systems", "author": ["Reiter", "Dale1997] Ehud Reiter", "Robert Dale"], "venue": "Natural Language Engineering,", "citeRegEx": "Reiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Reiter et al\\.", "year": 1997}, {"title": "Building natural language generation systems, volume 33", "author": ["Reiter et al.2000] Ehud Reiter", "Robert Dale", "Zhiwei Feng"], "venue": null, "citeRegEx": "Reiter et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Reiter et al\\.", "year": 2000}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural responding machine for shorttext conversation", "author": ["Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H Huang", "Jeffrey Pennington", "Andrew Y Ng", "Christopher D Manning"], "venue": "The Conference on Neural Information Processing Sys-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Cliff C Lin", "Andrew Ng", "Chris Manning"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Discourse element identification in student essays based on global and local cohesion", "author": ["Song et al.2015] Wei Song", "Ruiji Fu", "Lizhen Liu", "Ting Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Song et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Song et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Zhang", "Lapata2014] Xingxing Zhang", "Mirella Lapata"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "In this paper, we focus on natural language generation (Reiter et al., 2000).", "startOffset": 55, "endOffset": 76}, {"referenceID": 16, "context": "The coherence/discourse relationship (Prasad et al., 2008; Li and Hovy, 2014; Li et al., 2014) between sentences is a crucial element to improve the readability of a document and to guarantee the structured nature of a document in terms of lexicalization and semantic.", "startOffset": 37, "endOffset": 94}, {"referenceID": 8, "context": "The coherence/discourse relationship (Prasad et al., 2008; Li and Hovy, 2014; Li et al., 2014) between sentences is a crucial element to improve the readability of a document and to guarantee the structured nature of a document in terms of lexicalization and semantic.", "startOffset": 37, "endOffset": 94}, {"referenceID": 18, "context": "work (Reiter and Dale, 1997; Reiter et al., 2000) for essay generation.", "startOffset": 5, "endOffset": 49}, {"referenceID": 14, "context": "Words with similar semantic meanings and grammatical usages will be mapped into closed vectors in the embedding space (Mikolov et al., 2013).", "startOffset": 118, "endOffset": 140}, {"referenceID": 14, "context": "We first map each word in a low-dimensional word embedding (Mikolov et al., 2013).", "startOffset": 59, "endOffset": 81}, {"referenceID": 3, "context": "We leave more sophisticated unsupervised approaches like Deep Boltzmann Machine (Hinton and Salakhutdinov, 2006), Denoising Autoencoder (Glorot et al., 2011), Unfolding Recursive Autoencoder (Socher et al.", "startOffset": 136, "endOffset": 157}, {"referenceID": 10, "context": ", 2011a), LSTM Encoder (Li et al., 2015) as future work.", "startOffset": 23, "endOffset": 40}, {"referenceID": 23, "context": "Therefore, we use an automatically discourse element labeling algorithm (Song et al., 2015) to decide what role does a sentence acts as, such as \u201cIntroduction\u201d, \u201cPrompt\u201d and \u201cConclusion\u201d.", "startOffset": 72, "endOffset": 91}, {"referenceID": 14, "context": "The word embeddings used in WE are learned with Skipgram method (Mikolov et al., 2013).", "startOffset": 64, "endOffset": 86}, {"referenceID": 15, "context": "As we have the original sentence order, we can regard it as the ground truth, and evaluate the quantitative performance in terms of accuracy on bigrams of sentences, which is similar with Bleu (Papineni et al., 2002) in machine translation and Rough (Lin, 2004) in summarization.", "startOffset": 193, "endOffset": 216}, {"referenceID": 11, "context": ", 2002) in machine translation and Rough (Lin, 2004) in summarization.", "startOffset": 41, "endOffset": 52}, {"referenceID": 18, "context": "Natural language generation (NLG) is a fundamental and challenge task in natural language processing and computational linguistics (Manning and Sch\u00fctze, 1999; Jurafsky and Martin, 2000; Reiter et al., 2000).", "startOffset": 131, "endOffset": 206}, {"referenceID": 24, "context": "Image captioning (Xu et al., 2015) can also be viewed as a kind of text generation which takes a picture as the input.", "startOffset": 17, "endOffset": 34}, {"referenceID": 0, "context": "For example, Belz (2007) generate weather report texts with probabilistic generation method.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "For example, Belz (2007) generate weather report texts with probabilistic generation method. Jiang and Zhou (2008) generate Chinese couplets with statistical machine translation approach.", "startOffset": 13, "endOffset": 115}, {"referenceID": 0, "context": "Angeli et al. (2010) develop a domain-independent method with a sequence of local decisions, and evaluate the method on Robocup sportscasting and technical weather forecasts.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Angeli et al. (2010) develop a domain-independent method with a sequence of local decisions, and evaluate the method on Robocup sportscasting and technical weather forecasts. Zhang and Lapata (2014) generate Chinese poetry with recurrent neural network.", "startOffset": 0, "endOffset": 199}, {"referenceID": 0, "context": "Angeli et al. (2010) develop a domain-independent method with a sequence of local decisions, and evaluate the method on Robocup sportscasting and technical weather forecasts. Zhang and Lapata (2014) generate Chinese poetry with recurrent neural network. Shang et al. (2015) generate short-text conversation with neural responding machine.", "startOffset": 0, "endOffset": 274}, {"referenceID": 0, "context": "Angeli et al. (2010) develop a domain-independent method with a sequence of local decisions, and evaluate the method on Robocup sportscasting and technical weather forecasts. Zhang and Lapata (2014) generate Chinese poetry with recurrent neural network. Shang et al. (2015) generate short-text conversation with neural responding machine. Li et al. (2015) generate a paragraph/document with attention based neural network.", "startOffset": 0, "endOffset": 356}, {"referenceID": 0, "context": "Angeli et al. (2010) develop a domain-independent method with a sequence of local decisions, and evaluate the method on Robocup sportscasting and technical weather forecasts. Zhang and Lapata (2014) generate Chinese poetry with recurrent neural network. Shang et al. (2015) generate short-text conversation with neural responding machine. Li et al. (2015) generate a paragraph/document with attention based neural network. Rush et al. (2015) and Hu et al.", "startOffset": 0, "endOffset": 442}, {"referenceID": 0, "context": "Angeli et al. (2010) develop a domain-independent method with a sequence of local decisions, and evaluate the method on Robocup sportscasting and technical weather forecasts. Zhang and Lapata (2014) generate Chinese poetry with recurrent neural network. Shang et al. (2015) generate short-text conversation with neural responding machine. Li et al. (2015) generate a paragraph/document with attention based neural network. Rush et al. (2015) and Hu et al. (2015) use attention based recurrent neural network to generate abstractive summarization.", "startOffset": 0, "endOffset": 463}], "year": 2016, "abstractText": "Generating an article automatically with computer program is a challenging task in artificial intelligence and natural language processing. In this paper, we target at essay generation, which takes as input a topic word in mind and generates an organized article under the theme of the topic. We follow the idea of text planning (Reiter and Dale, 1997) and develop an essay generation framework. The framework consists of three components, including topic understanding, sentence extraction and sentence reordering. For each component, we studied several statistical algorithms and empirically compared between them in terms of qualitative or quantitative analysis. Although we run experiments on Chinese corpus, the method is language independent and can be easily adapted to other language. We lay out the remaining challenges and suggest avenues for future research.", "creator": "LaTeX with hyperref package"}}}