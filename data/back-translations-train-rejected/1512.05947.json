{"id": "1512.05947", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2015", "title": "Complexity and Approximation of the Fuzzy K-Means Problem", "abstract": "The fuzzy $K$-means problem is a generalization of the classical $K$-means problem to soft clusterings, i.e. clusterings where each points belongs to each cluster to some degree. Although popular in practice, prior to this work the fuzzy $K$-means problem has not been studied from a complexity theoretic or algorithmic perspective. We show that optimal solutions for fuzzy $K$-means cannot, in general, be expressed by radicals over the input points. Surprisingly, this already holds for very simple inputs in one-dimensional space. Hence, one cannot expect to compute optimal solutions exactly. We give the first $(1+\\epsilon)$-approximation algorithms for the fuzzy $K$-means problem. First, we present a deterministic approximation algorithm whose runtime is polynomial in $N$ and linear in the dimension $D$ of the input set, given that $K$ is constant, i.e. a polynomial time approximation algorithm given a fixed $K$. We achieve this result by showing that for each soft clustering there exists a hard clustering with comparable properties. Second, by using techniques known from coreset constructions for the $K$-means problem, we develop a deterministic approximation algorithm that runs in time almost linear in $N$ but exponential in the dimension $D$. We complement these results with a randomized algorithm which imposes some natural restrictions on the input set and whose runtime is comparable to some of the most efficient approximation algorithms for $K$-means, i.e. linear in the number of points and the dimension, but exponential in the number of clusters.", "histories": [["v1", "Fri, 18 Dec 2015 13:35:59 GMT  (56kb)", "http://arxiv.org/abs/1512.05947v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["johannes bl\\\"omer", "sascha brauer", "kathrin bujna"], "accepted": false, "id": "1512.05947"}, "pdf": {"name": "1512.05947.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Kathrin Bujna"], "emails": ["kathrin.bujna}@uni-paderborn.de"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.05 947v 1 [cs.L G] 18 Dec 201 5"}, {"heading": "1 Introduction", "text": "This problem occurs in a wide range of practical applications in many areas, such as image analysis, information retrieval, and bioinformatics. We call a grouping of objects in a certain number of clusters when each object is associated with a particular cluster. There is a continuous generalization of the K mean problem that leads to such a soft clustering problem, known as the fuzzy K mean problem. 1.1 Fuzzy K means was the first to present a fuzzy K mean objective function, which was later expanded to include [2]. Today, Fuzzy K means has found a wide range of practical applications, such as image segmentation and biological data analysis."}, {"heading": "1.2 Related Work", "text": "Although the blurred K-mean problem appears in a wide range of practical applications, there has been no classification of complexity. To our knowledge, there are no hardness results for the blurred K-mean problem. It is not even known whether it lies in NP. The same is true for other soft cluster problems, such as the problem of maximum probability for the Gaussian mixture [6] or the problem of soft clustering [7]. Two problems closely related to the blurred K-mean problem are the K-means and the K-median problem. The complexity of the K-mean problem is well studied."}, {"heading": "1.3 Overview", "text": "The following technical part of the paper is divided into three parts. In Section 2 we give an overview of our results. In Section 2.1 we formally announce our result that the blurred K-mean problem cannot be solved by radicals. In Section 2.2 we present our results on approximation algorithms. In Section 3 we give an overview of our algorithmic techniques. In Section 4 we outline the analysis of our approximation algorithms."}, {"heading": "2 Our Contribution", "text": "We initiate the complexity-theoretical and algorithmic investigation of the blurred K-mean problem."}, {"heading": "2.1 Complexity", "text": "Theorem 1. The blurred K mean problem for m = 2, K = 2, D \u2265 1, X-N and | X | \u2265 6 is generally not optimal for radicals above Q. This means that neither the coordinates of the mean vectors nor the member values can be expressed in terms (+, \u2212, \u00b7, /, q). This result is an application of the technique used by Bajaj [15], which has proven the same result for the K median problem. It is noteworthy that our result already applies to m = 2 and in one-dimensional space. Thus, we show that an optimal solution of the blurred 2 mean problem (with m = 2) for the set X = {\u2212 3, \u2212 2, 1, 2, 3} for the rational solution is only for radicals above Qain and for the efficient 2-mean K."}, {"heading": "2.2 Approximation Algorithms", "text": "We provide the first (1 + 1) approximation time for the fuzzy K averages (1 + 1).D (1 + 1).D (1 + 1).D (1 + 1).D (1).D (1).D (1).D (1).D (1).D (2).D (2).D (2).D (2).D (2).D (2).D).D (2).D (2).D (2).D (2).D (2).D (2).D (2).D).D (2).D (2).D (2).D (2).D (2).D (3).D (2).D (3).D (3).D (3).D (2).D (2).D (2).D (2).D (2).D (2).D (3).D (3).D (3).D (3).D (3).D (2).D (2).D (2).D (2).D (2).D (2).D (2).D (2).D (2).D (2.D (2).D (2).D (2).D (2).D (2).D (3 (3).D (3).D (3 (3).D (3).D (3).D (3 (3).D (3).D (3).D (3 (3).D (3).D (3 (3).D (2).D (2).D (2).D (2).D (.D (2).D (2).D (2).D (.D (2).D (2).D (2).D (.D (2).D (2).D (.D (2).D (.D (2).D (2).D (.D (2).D (.D).D (.D).D (.D (.D).D (.D ("}, {"heading": "3 Our Main Techniques", "text": "In this section, we describe the techniques we use to prove theorems 2, 4, and 3. (For this purpose, we use the following notation.Definition 2 (induced solution).Let's leave X-RD \u00b7 R. Membership values R induce the solution (C-K) where C-K contains the corresponding optimal averages (cf.).Let's name the cost of induced solutions by (M) X (R) and (M) X-RD (R).We observe that for all remedies C-K contains the corresponding optimal membership values (cf. Equation (1).Let's specify the cost of induced solutions by (M) X (R) and (M)."}, {"heading": "3.2 Sampling Techniques", "text": "It is not as if there are any unclear clusters that prove to be insoluble. (...) It is as if there were no insoluble clusters. (...) It is as if the insoluble clusters were an insoluble problem. (...) It is as if the insoluble clusters were an insoluble problem. (...) It is as if the insoluble clusters were an insoluble problem. (...) It is as if the insoluble clusters were an insoluble problem. (...) It is as if the insoluble clusters could be an insoluble problem. (...) It is as if the insoluble clusters could be an insoluble problem. (...) It is as if the insoluble clusters are an insoluble problem. (...) It is as if the insoluble clusters are an insoluble problem. (...) It is as if the insoluble clusters are an insoluble problem. (...) It is as if the insoluble clusters are an insoluble problem."}, {"heading": "4 Proof Sketches", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Relating Fuzzy to Hard Clusters (Theorems 2 and 4)", "text": "The following statement is the basis for the results of theorems 2 and 4.Proposition 1. There is a randomized algorithm which, since X = {xn, wn] n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n, n (n (n) n n (n) n (n) n (n) n (n) n (n) n (n) n (n) n, n n (n (n) n (n (n) n (n) n (n) n (n (n) n (n) n (n) n (n (n) n (n) n (n (n) n (n) n (n (n) n n (n) n (n) n, n (n (n (n n n n (n) n (n (n) n n n (n) n n n n (n) n (n) n (n n n n) n (n n n n (n n) n n n (n n n n n n (n) n (n n n n) n, n (n n n n n n n (n n n (n n n) n n (n n n n n n (n) n (n) n (n n n n n) n, n (n n n (n (n n n n n) n n (n) n n (n) n (n n n n n) n (n n n) n n (n) n (n n n"}, {"heading": "4.2 Candidate Set Search for Mean Vectors (Proof of Theorem 3)", "text": "The algorithm that creates and tests all these candidates and finally selects the best candidates fulfils the properties of theorem 3. Theorem 7 (candidate set). Let's have X-RD, K-N and E-RD (0, 1). There is a set G-RD with the size | G | = O (KmD + 1,000 \u2212 Dm log (mK\u0442) log (N))), which contains a protocol (N) containing (m) K-K-G with\u03c6 (m) X ({\u00b5k} k-K]) \u2264 (1 + \u0432) throuOPT (X, K, m). The set G can be calculated in time O (N (log (N) K-2K2D + NKD | G |). The proof (sketch of the evidence in Section 7.5). The idea behind the corset problem of [19] can be used to construct a partial K factor (2K2K)."}, {"heading": "5 Future Work & Open Problems", "text": "The aim of further research is to investigate whether theorem 5 can be applied to other soft cluster problems. In particular, we hope to obtain a constant factor approximation algorithm for the problem of maximum probability estimation for mixtures of Gaussian distributions, e.g. using the results from [24]. It is an open question whether we are able to classify the hardness of the approximation of blurred Kmeans. We suspect that, just as with the classical K mean problem, if P 6 = NP, there is no PTAS for the blurred K mean problem for any K and D."}, {"heading": "6 Full Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Preliminaries", "text": "In this section, we present some notations and lemmings used throughout the remainder of this appendix. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "7 Stochastic Fuzzy Clustering (Proof of Theorem 5)", "text": "In this section, we first describe a random process that generates K-hard clusters in the face of some blurred K-mean clusters. We define different quantities that describe the different clusters and deduce probabilistic boundaries for similarity between them in relation to these quantities."}, {"heading": "7.1 Setting and Random Process", "text": "Below we will consider arbitrary but fixed memberships {rnk} n, k. These membership values induce unclear clusters. We say that the kth unclear cluster has weight Rk, mean \u00b5k and cost \u03c6 (m) X, k ({rnk} n), where the \u00b5k is the optimal mean in relation to the given memberships. Remember that by Equation (2), Equation (3) and definition 4 we necessarily haveRk = N = 1rmnkwn, \u00b5k = \u2211 N = 1 r m nkwnxn. We will consider the following random process aimed at imitating the fuzzy clustering. In view of fixed memberships {rnk} n, k."}, {"heading": "7.2 Proximity", "text": "After definition, the binary variables znk = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K ="}, {"heading": "7.3 Proof of Theorem 5", "text": "We apply Corollary 1 to the given membership values {rnk} n, k and dot setX = {(xn, wn wmax) n # 1rmnk (wn wmax) = 1 wmax # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "7.4 Superset Sampling (Proof of Theorem 6)", "text": "Sampling-Sampled-Sampling-Sampled-Sampled-Sampling-Sampled-Sampled-Sampling-Sampled-Sampled-Sampled-Sampling-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Sampled-Samp"}, {"heading": "7.5 Candidate Set of Means (Proof of Theorem 7)", "text": "The following construction is used by [20], [19], [23], and [23] to obtain a candidate set containing a set of means that cause a (1 +) approach to the K mean problem. \u2212 Construction We then get X = {xn} n, [N] n, RD, and K N, let A = {ak} k, (K] -RD, an (n) approach to the K mean problem with respect to X, i.e.kmX (A), kmX, K. (22) In addition, the letR: = kmX (A) n, (23) B (x, r): {y-RD, (2), (24)."}, {"heading": "7.6 Unsolvability by Radicals (Proof of Theorem 1)", "text": "Consider the fuzzy 2-middle instance with m = 2 and X = 3. Q = 1. Q = 1. Q = 1. Q x x x x x x. Q = 2. Q = 2. Q = 2. Q = 3. Q = 3. Q = 3. Q = 3. Q = 3. Q = 4. Q = 4. Q = 4. Q = 4. Q = 4. Q = 4. Q = 4. Q = 4. Q = 4. Q = 4. Q = 4. Q = 4. Q = 4. Q = 6. Q = 6. Q = 1. Q = 4. Q = 4. Q = 6. Q = 6. Q = 6. \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 4. Q = 6. Q = 6. Q = Q = 4. Q = 4. Q = 6. Q = 4. Q = 4. Q = 4. Q = 4. Q = 3. Q = 4. Q = 4. Q = 4. Q = 4. Q = 3. Q = 4. Q = 4. Q = 4."}, {"heading": "7.8 Arbitrarily Poor Local Minima (Proof of Observation 1)", "text": "It is known that the FM algorithm converges to a stationary point of objective function, which is either a saddle point or a (local) minimum (5). We show that there are cases for which this point is arbitrarily bad compared to an optimal solution."}], "references": [{"title": "A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact WellSeparated Clusters", "author": ["J.C. Dunn"], "venue": "Journal of Cybernetics 3(3)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1973}, {"title": "FCM: The fuzzy c-means clustering algorithm", "author": ["J. Bezdek", "R. Ehrlich", "W. Full"], "venue": "Computers & Geosciences 10(2)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1984}, {"title": "A multiresolution image segmentation technique based on pyramidal segmentation and fuzzy clustering", "author": ["M. Rezaee", "P. van der Zwet", "B. Lelieveldt", "R. van der Geest", "J. Reiber"], "venue": "Image Processing, IEEE Transactions on 9(7)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Fuzzy C-means method for clustering microarray data", "author": ["D. Demb\u00e9l\u00e9", "P. Kastner"], "venue": "Bioinformatics 19(8)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Convergence theory for fuzzy c-means: Counterexamples and repairs", "author": ["J. Bezdek", "R. Hathaway", "M. Sabin", "W. Tucker"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on 17(5)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1987}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C. Bishop"], "venue": "SpringerVerlag New York, Inc., Secaucus, NJ, USA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Information Theory, Inference, and Learning Algorithms", "author": ["D. Mackay"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Applications of Weighted Voronoi Diagrams and Randomization to Variance-based K-clustering: (Extended Abstract)", "author": ["M. Inaba", "N. Katoh", "H. Imai"], "venue": "Proceedings of the Tenth Annual Symposium on Computational Geometry. SCG \u201994, New York, NY, USA, ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "The hardness of k-means clustering", "author": ["S. Dasgupta"], "venue": "Technical report, Department of Computer Science and Engineering, University of California, San Diego", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "The Hardness of Approximation of Euclidean k-means", "author": ["P. Awasthi", "M. Charikar", "R. Krishnaswamy", "A. Sinop"], "venue": "31st Annual Symposium on Computational Geometry, SOCG\u201915.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Proceedings of the Eighteenth Annual Symposium on Computational Geometry. SCG \u201902, ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "A simple linear time (1+ \u03b5)-approximation algorithm for geometric k-means clustering in any dimensions", "author": ["A. Kumar", "Y. Sabharwal", "S. Sen"], "venue": "Proceedings-Annual Symposium on Foundations of Computer Science, IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "On the Complexity of Some Common Geometric Location Problems", "author": ["N. Megiddo", "K. Supowit"], "venue": "SIAM J. Comput. 13(1)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1984}, {"title": "The Algebraic Degree of Geometric Optimization Problems", "author": ["C. Bajaj"], "venue": "Discrete Comput. Geom. 3(2)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1988}, {"title": "A contribution to convergence theory of fuzzy c-means and derivatives", "author": ["F. Hoppner", "F. Klawonn"], "venue": "Fuzzy Systems, IEEE Transactions on 11(5)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Optimality tests for fixed points of the fuzzy c-means algorithm", "author": ["T. Kim", "J. Bezdek", "R. Hathaway"], "venue": "Pattern Recognition 21(6)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1988}, {"title": "Local convergence of the fuzzy c-means algorithms", "author": ["R. Hathaway", "J. Bezdek"], "venue": "Pattern Recognition 19(6)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1986}, {"title": "Coresets for k-Means and k-Median Clustering and their Applications", "author": ["S. Har-peled", "S. Mazumdar"], "venue": "In Proc. 36th Annu. ACM Sympos. Theory Comput.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "On coresets for k-median and k-means clustering in metric and euclidean spaces and their applications", "author": ["K. Chen"], "venue": "SIAM J. Comput. 39(3)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Clustering for Metric and Nonmetric Distance Measures", "author": ["M. Ackermann", "J. Bl\u00f6mer", "C. Sohler"], "venue": "ACM Trans. Algorithms 6(4)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "K-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms. SODA \u201907, Society for Industrial and Applied Mathematics", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "On approximate geometric k -clustering", "author": ["J. Matousek"], "venue": "Discrete & Computational Geometry 24(1)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "A theoretical and experimental comparison of the EM and SEM algorithm", "author": ["J. Bl\u00f6mer", "K. Bujna", "D. Kuntze"], "venue": "22nd International Conference on Pattern Recognition, ICPR 2014, Stockholm, Sweden, August 24-28, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Algebra", "author": ["T. Hungerford"], "venue": "Graduate Texts in Mathematics. Springer", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1974}], "referenceMentions": [{"referenceID": 0, "context": "1 Fuzzy K-Means [1] was the first to present a fuzzy K-means objective function, which was later extended by [2].", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "1 Fuzzy K-Means [1] was the first to present a fuzzy K-means objective function, which was later extended by [2].", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "Today, fuzzy K-means has found a wide range of practical applications, for example in image segmentation [3] and biological data analysis [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 3, "context": "Today, fuzzy K-means has found a wide range of practical applications, for example in image segmentation [3] and biological data analysis [4].", "startOffset": 138, "endOffset": 141}, {"referenceID": 0, "context": "In a fuzzy clustering, each data point xn belongs to each cluster, represented by a \u03bck, with a certain membership value rnk \u2208 [0, 1].", "startOffset": 126, "endOffset": 132}, {"referenceID": 0, "context": "Given X = {(xn, wn)}n\u2208[N ] \u2282 R \u00d7 R\u22650, K \u2265 1 and m \u2265 2, find C = {\u03bck}k\u2208[K] \u2282 R and R = {rnk}n\u2208[N ],k\u2208[K] \u2282 [0, 1] minimizing", "startOffset": 106, "endOffset": 112}, {"referenceID": 1, "context": "Our problem definition is a generalization of the original definition presented in [2] in that we consider weighted data sets.", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "It is defined by the following first-order optimality conditions [5]: Fixing the means {\u03bck}k\u2208[K], optimal memberships are given by", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "The same holds for other soft clustering problems, such as the maximum-likelihood estimation problem for Gaussian mixture models [6] or the soft-clustering problem [7].", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "The same holds for other soft clustering problems, such as the maximum-likelihood estimation problem for Gaussian mixture models [6] or the soft-clustering problem [7].", "startOffset": 164, "endOffset": 167}, {"referenceID": 7, "context": "For fixed K and D, there is a polynomial time algorithm solving the problem optimally [8].", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "The K-means problem is NP-complete, even if K or D is fixed to 2 [9] [10].", "startOffset": 65, "endOffset": 68}, {"referenceID": 9, "context": "Furthermore, assuming P 6=NP, there is no PTAS for the K-means problem for arbitrary K and D [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 10, "context": "However, there are several approximation algorithms known, such as a polynomial-time constant-factor approximation algorithm [12] and a (1 + \u01eb)-approximation algorithm with runtime polynomial in N and D [13].", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "However, there are several approximation algorithms known, such as a polynomial-time constant-factor approximation algorithm [12] and a (1 + \u01eb)-approximation algorithm with runtime polynomial in N and D [13].", "startOffset": 203, "endOffset": 207}, {"referenceID": 12, "context": "Just as the K-means problem, the K-median problem is NP-hard, even for D = 2 [14].", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "Even in the plane, optimal solutions of the 1-median problem are in general not expressable by radicals over Q [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 1, "context": "However, [2] and [5] proved convergence of the FM algorithm to a local minimum or a saddle point of the objective function.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "However, [2] and [5] proved convergence of the FM algorithm to a local minimum or a saddle point of the objective function.", "startOffset": 17, "endOffset": 20}, {"referenceID": 14, "context": "Among others, [16] and [17] address the problem of determining and distinguishing whether the algorithm has reached a local minimum or a saddle point.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "Among others, [16] and [17] address the problem of determining and distinguishing whether the algorithm has reached a local minimum or a saddle point.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "started sufficiently close to a minimizer, the iteration sequence converges to that particular minimizer [18].", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "This result is an application of the technique used by Bajaj [15] who proved the same result for the K-median problem.", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "A more detailed discussion of the implications of unsolvability by radicals can be found in [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "That is, for any given \u01eb \u2208 [0, 1], our algorithm computes an (1+ \u01eb)-approximation to the fuzzy K-means problem in time polynomial in the number of points N and dimension D.", "startOffset": 27, "endOffset": 33}, {"referenceID": 17, "context": "The idea behind our algorithm from Theorem 3 is the same as behind the coreset construction of [19] as it is used by [20].", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": "The idea behind our algorithm from Theorem 3 is the same as behind the coreset construction of [19] as it is used by [20].", "startOffset": 117, "endOffset": 121}, {"referenceID": 19, "context": "Observe that the running time basically coincides with the running time of an algorithm that applies the superset sampling technique to the K-means problem [21].", "startOffset": 156, "endOffset": 160}, {"referenceID": 17, "context": "We use this result when transfering the ideas behind the coreset construction of [19] in order to obtain a candidate set of means.", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "Let X = {(xn, wn)}n\u2208[N ] \u2282 R \u00d7 R, m \u2208 N, K \u2208 N, and \u01eb \u2208 [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 7, "context": "In the next section, we describe how the superset sampling technique [8] [13] can be used approximate the means \u03bc(Ck) well.", "startOffset": 69, "endOffset": 72}, {"referenceID": 11, "context": "In the next section, we describe how the superset sampling technique [8] [13] can be used approximate the means \u03bc(Ck) well.", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "For instance, we presume that the sample and prune technique from [21] and the K-means++ algorithm [22] require that the convex hulls of clusters do not overlap.", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "For instance, we presume that the sample and prune technique from [21] and the K-means++ algorithm [22] require that the convex hulls of clusters do not overlap.", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "The superset sampling technique introduced by [8] [13] can be used to find such means.", "startOffset": 46, "endOffset": 49}, {"referenceID": 11, "context": "The superset sampling technique introduced by [8] [13] can be used to find such means.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "2 Candidate Set Search for Mean Vectors (Proof of Theorem 3) Using ideas behind the coreset construction of [19], we can construct a candidate set of mean vectors.", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "The idea behind the coreset construction of [19] can be used to construct a candidate set of mean vectors.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "To this end, we use the deterministic algorithm presented in [23], which requires time O ( N(log(N))K\u01eb\u22122K D ) .", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "by using the results from [24].", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "used in the proof of Theorem 2 in [8]).", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "Let \u01eb \u2208 [0, 1], c > 1, and m \u2208 N.", "startOffset": 8, "endOffset": 14}, {"referenceID": 7, "context": "If we assume that C contains at least a constant fraction of the points of X , then this problem can be solved via the superset sampling technique [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 19, "context": "Formally, using [21], we directly obtain the following lemma.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Lemma 11 (Superset Sampling [21]).", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": "The following we use the coreset construction used by [20], [19], and [23] to obtain a candidate set that contains a set of means which induce an (1+ \u01eb)-approximation to the fuzzy K-means problem.", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "The following we use the coreset construction used by [20], [19], and [23] to obtain a candidate set that contains a set of means which induce an (1+ \u01eb)-approximation to the fuzzy K-means problem.", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "The following we use the coreset construction used by [20], [19], and [23] to obtain a candidate set that contains a set of means which induce an (1+ \u01eb)-approximation to the fuzzy K-means problem.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "Upper Bound on the Size of G In the following, we upper bound |G| analogously to [20].", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "Lemma 14 ([15]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "Lemma 15 ([25]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "Lemma 16 ([25]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "8 Arbitrarily Poor Local Minima (Proof of Observation 1) It is known that the FM algorithm converges to a stationary point of the objective function that is either a saddlepoint or a (local) minimum [5].", "startOffset": 199, "endOffset": 202}], "year": 2015, "abstractText": "The fuzzy K-means problem is a generalization of the classical K-means problem to soft clusterings, i.e. clusterings where each points belongs to each cluster to some degree. Although popular in practice, prior to this work the fuzzy K-means problem has not been studied from a complexity theoretic or algorithmic perspective. We show that optimal solutions for fuzzy K-means cannot, in general, be expressed by radicals over the input points. Surprisingly, this already holds for very simple inputs in one-dimensional space. Hence, one cannot expect to compute optimal solutions exactly. We give the first (1+ \u01eb)-approximation algorithms for the fuzzy K-means problem. First, we present a deterministic approximation algorithm whose runtime is polynomial in N and linear in the dimension D of the input set, given that K is constant, i.e. a polynomial time approximation algorithm given a fixed K. We achieve this result by showing that for each soft clustering there exists a hard clustering with comparable properties. Second, by using techniques known from coreset constructions for the K-means problem, we develop a deterministic approximation algorithm that runs in time almost linear in N but exponential in the dimension D. We complement these results with a randomized algorithm which imposes some natural restrictions on the input set and whose runtime is comparable to some of the most efficient approximation algorithms for Kmeans, i.e. linear in the number of points and the dimension, but exponential in the number of clusters.", "creator": "LaTeX with hyperref package"}}}