{"id": "1512.05986", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2015", "title": "Can Pretrained Neural Networks Detect Anatomy?", "abstract": "Convolutional neural networks demonstrated outstanding empirical results in computer vision and speech recognition tasks where labeled training data is abundant. In medical imaging, there is a huge variety of possible imaging modalities and contrasts, where annotated data is usually very scarce. We present two approaches to deal with this challenge. A network pretrained in a different domain with abundant data is used as a feature extractor, while a subsequent classifier is trained on a small target dataset; and a deep architecture trained with heavy augmentation and equipped with sophisticated regularization methods. We test the approaches on a corpus of X-ray images to design an anatomy detection system.", "histories": [["v1", "Fri, 18 Dec 2015 15:16:31 GMT  (7kb)", "http://arxiv.org/abs/1512.05986v1", "NIPS 2015 Workshop on Machine Learning in Healthcare"]], "COMMENTS": "NIPS 2015 Workshop on Machine Learning in Healthcare", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.NE", "authors": ["vlado menkovski", "zharko aleksovski", "axel saalbach", "hannes nickisch"], "accepted": false, "id": "1512.05986"}, "pdf": {"name": "1512.05986.pdf", "metadata": {"source": "CRF", "title": "Can Pretrained Neural Networks Detect Anatomy?", "authors": ["Vlado Menkovski"], "emails": ["vlado.menkovski@philips.com", "zharko.aleksovski@philips.com", "axel.saalbach@philips.coom", "hannes.nickisch@philips.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.05 986v 1 [cs.C V] 1"}, {"heading": "1 Introduction and Motivation", "text": "Deep Learning [9] (DL) is rapidly gaining momentum in computer vision [5], speech recognition [3] and medical imaging applications due to its record-breaking empirical performance and promise to learn the low-threshold features and high-level decisions directly from the raw data. A major open research challenge is transferring the excellent predictive power of DL methods, especially Convolutionary Neural Networks (CNN), from the \"data-laden\" regime of computer vision1 to the \"data-limited\" regime of medical imagination. As the typical number of parameters in DL models is very large, the biggest concern for operation in \"data-limited\" systems is the inability to generalize and shape robust features because the small number of examples does not capture the variability in the input space well. There are various approaches to deal with this challenge. Data augmentation can be used to increase the effective size of the training set and to drive training invariability."}, {"heading": "2 Experiments and Results", "text": "To assess the performance of specifically trained and commercially available networks, X-rays from the ImageClef 2009 - Medical Image Annotation task2 were used. This challenge database consists of a wide range of X-rays from clinical routine, with a detailed anatomical classification. For convenience, we flattened the hierarchical class representation and ignored classes with fewer than 50 sample images, ending up with 24 unique classes. For evaluation, the entire data corpus (of 14,676 images) was divided into a training and test set covering 90% and 10% of the data. As a first baseline, the OverFeat network (see [6]) was used in combination with a (linear) multi-class SVM. OverFeat is a revolutionary neural network trained on 1.2 million non-medical images."}, {"heading": "3 Conclusions and Perspectives", "text": "We investigated when, where, and how a convolutionary neural network used for anatomy recognition can be (re) trained. Our results suggest that pre-trained networks are good image descriptors even outside of their training area, and that last-shift retraining, given limited training data, is a viable alternative to performing transfer learning. We also demonstrated that sufficiently meaningful models such as a very deep neural network can be trained even with a relatively small number of annotations, if appropriate augmentation and regulation is implemented. Our work is a first step toward a multimodal anatomical expert system whose components are trained to maximize data efficiency."}], "references": [{"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, 20(3):273\u2013297", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1502.01852", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "G", "author": ["G. Hinton", "L. Deng", "D. Yu"], "venue": "E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):82\u2013 97", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25 (NIPS)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Overfeat: Integrated recognition", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "localization and detection using convolutional networks. In International Conference on Learning Representations ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "A baseline for visual instance retrieval with deep convolutional networks", "author": ["A. Sharif Razavian", "J. Sullivan", "A. Maki", "S. Carlsson"], "venue": "International Conference on Learning Representations, May 7-9, 2015, San Diego, CA. ICLR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning", "author": ["Y.B. Yann LeCun", "G. Hinton"], "venue": "Nature, 521:436\u2013444", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Deep learning [9] (DL) is rapidly gaining momentum in computer vision [5], speech recognition [3] and medical imaging applications due to their record-breaking empirical performance and their promise to jointly learn the low-level features and the high-level decisions directly from raw data.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "Deep learning [9] (DL) is rapidly gaining momentum in computer vision [5], speech recognition [3] and medical imaging applications due to their record-breaking empirical performance and their promise to jointly learn the low-level features and the high-level decisions directly from raw data.", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "Deep learning [9] (DL) is rapidly gaining momentum in computer vision [5], speech recognition [3] and medical imaging applications due to their record-breaking empirical performance and their promise to jointly learn the low-level features and the high-level decisions directly from raw data.", "startOffset": 94, "endOffset": 97}, {"referenceID": 7, "context": "Sophisticated regularization methods such as dropout [8] and batch normalization (BN) [4] can work to counter overfitting.", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "Sophisticated regularization methods such as dropout [8] and batch normalization (BN) [4] can work to counter overfitting.", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "combined with a SVM [1] classifier trained on the medical images.", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "As a first baseline, the OverFeat network (see [6]) in combination with a (linear) multi-class SVM was employed.", "startOffset": 47, "endOffset": 50}, {"referenceID": 6, "context": "[7], using dropout and batch normalization with leaky ReLU activation functions [2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[7], using dropout and batch normalization with leaky ReLU activation functions [2].", "startOffset": 80, "endOffset": 83}], "year": 2015, "abstractText": "Convolutional neural networks demonstrated outstanding empirical results in computer vision and speech recognition tasks where labeled training data is abundant. In medical imaging, there is a huge variety of possible imaging modalities and contrasts, where annotated data is usually very scarce. We present two approaches to deal with this challenge. A network pretrained in a different domain with abundant data is used as a feature extractor, while a subsequent classifier is trained on a small target dataset; and a deep architecture trained with heavy augmentation and equipped with sophisticated regularization methods. We test the approaches on a corpus of X-ray images to design an anatomy detection system.", "creator": "LaTeX with hyperref package"}}}