{"id": "1512.06216", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2015", "title": "Poseidon: A System Architecture for Efficient GPU-based Deep Learning on Multiple Machines", "abstract": "Deep learning (DL) has achieved notable successes in many machine learning tasks. A number of frameworks have been developed to expedite the process of designing and training deep neural networks (DNNs), such as Caffe, Torch and Theano. Currently they can harness multiple GPUs on a single machine, but are unable to use GPUs that are distributed across multiple machines; as even average-sized DNNs can take days to train on a single GPU with 100s of GBs to TBs of data, distributed GPUs present a prime opportunity for scaling up DL. However, the limited bandwidth available on commodity Ethernet networks presents a bottleneck to distributed GPU training, and prevents its trivial realization.", "histories": [["v1", "Sat, 19 Dec 2015 09:55:37 GMT  (1535kb,D)", "http://arxiv.org/abs/1512.06216v1", "14 pages, 8 figures, 6 tables"]], "COMMENTS": "14 pages, 8 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.DC", "authors": ["hao zhang", "zhiting hu", "jinliang wei", "pengtao xie", "gunhee kim", "qirong ho", "eric xing"], "accepted": false, "id": "1512.06216"}, "pdf": {"name": "1512.06216.pdf", "metadata": {"source": "CRF", "title": "Poseidon: A System Architecture for Efficient GPU-based Deep Learning on Multiple Machines", "authors": ["Hao Zhang", "Zhiting Hu", "Jinliang Wei", "Pengtao Xie", "Gunhee Kim", "Qirong Ho", "Eric P. Xing"], "emails": ["epxing}@cs.cmu.edu,", "gunhee@snu.ac.kr,", "hoqirong@gmail.com"], "sections": [{"heading": null, "text": "A number of software \"frameworks\" have been developed to speed up the process of designing and forming deep neural networks, such as Caffe [11], Torch [4] and Theano [1]. Currently, these frameworks can use multiple GPUs on the same machine, but are unable to use GPUs spread across multiple machines; even medium-sized deep networks can take days to train on a single GPU when confronted with 100s of GBs on TB of data, distributed GPUs represent a prime opportunity to scale deep learning. However, the limited cross-machine bandwidth available on commodity Ethernet networks represents a bottleneck that requires distributed GPU education."}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Related Work", "text": "As the demand for faster training of neural networks on ever larger data sets grows, several frameworks have been proposed that use multiple GPUs on a single machine. Thus, for example, the number of GPUs that are able to move has increased. Similarly, the number of GPUs that are able to evolve in the same way as they have in the past has increased. Similarly, the number of GPUs that have relied on the parallelism of data has increased."}, {"heading": "3 Background", "text": "Poseidon builds on an existing general-purpose system for distributed machine learning algorithms, Petuum, and adds new contributions that specifically improve the performance of GPU-based deep learning. To clearly outline our contributions, we start with a brief overview of the Petuum features on which we build, and list some mathematical notations that will be useful in characterizing Poseidon."}, {"heading": "3.1 Petuum for Iterative-Convergent ML", "text": "Poseidon builds on Petuum, a distributed Big Machine Learning Framework that provides a generic interface to a wide range of ML programs. [28] Its design philosophy is rooted in iterative-convergent solutions to minimize loss functions. A number of ML algorithms are formulated in this way, thereby repeatedly executing update equations that reduce some error functions. Notable examples include stochastic gradient descendence in optimization programs, MCMC and variation methods for graphical models, and proximal optimization for structured savings problems, among others. In mathematical form, the iterative-convergent algorithm can be represented as follows. Given Data D and a loss function, a typical ML problem can be solved by executing the update equation until model parameters A meet some holding criteria."}, {"heading": "3.2 Stale Synchronous Parallel PS", "text": "A Parameter Server (PS) is a distributed shared memory system that provides a systematic abstraction for iterative-convergent algorithms in the field of distributed machine learning. Typically, each worker can access the global model parameters A via network communication. In particular, the training data is distributed to a large number of clients (i.e. workers).The distributed data system can be easily transferred to the PS architecture by restricting the execution of the update to individual workers only. Applying the updates to model parameters A is done on the server, and a consistency scheme coordinates the synchronization between the servers and the clients."}, {"heading": "3.3 Data-parallel Distributed Training of Convolutional Neural Networks", "text": "The basic computational unit in each layer is called a neuron, which is normally composed of a vector of weights corresponding to a series in the weight matrix and a nonlinear function to introduce rich model expression power. Each neuron takes outputs (activations) from its previous layer as input, applies both linear and nonlinear transformations to produce its own activation, which is then transferred to its fol-logical layers as their input. At the bottom of a neural network, an input layer is used as inputs for reading and vectorizing various types of data, while at the top of the network is usually a loss layer specified by an optimization goal."}, {"heading": "4 Poseidon Architecture", "text": "In fact, most of them will be able to play by the rules."}, {"heading": "4.1 Overview: A Three-level Structure", "text": "For example, in previous CPU-based distributed DL systems, a two-level server architecture has been established in which the first level has server machines that record worker histories and support complex cluster configurations (for example, a cluster of GPU nodes in which each node is distributed over multiple levels and generates gradient updates), and the implementation needs to be greatly customized to support cluster configurations (for example, a cluster of GPU nodes in which each node has multiple GPU nodes)."}, {"heading": "4.2 Distributed Wait-free Backpropagation", "text": "Backpropagation (BP) [21] is the basic algorithm for the formation of neural networks. Specifically, the BP algorithm runs as a chain with many forward and backward propagation phases. During the backpass, an error message E is propagated from top to bottom of the network, forming a message transmission chain. Figure.3. (a) shows the process of the original BP in distributed settings on a neural network with L-layers {li} Li = 1 and layer parameters as A = {Ai} Li = 1. For each iteration t, each worker p {1, \u00b7, P} performs the BP calculation separately. Only when the propagation reaches the lowest layer l1 (i.e. all gradients are generated as A = {Ai} Li = 1), is each worker p {1, \u00b7 \u00b7, P} ready to start the communication. The worker sends out local parameter updates, waits for the remote node to be updated to {3. Li and then the next layer}."}, {"heading": "At iteration t on worker p:", "text": "In fact, it is such that most of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "4.3 Structure-Aware Message Passing Protocol", "text": "Most ML models, such as neural networks, fall into the family of Matrix Parametrized Models (MPM), which represent their parameters as a series of matrices. Let's take AlexNetto as an example, the weights between the two FC layers fc6 and fc7 are represented as 4,096 matrix W, as are their gradients. At each iteration, each worker sends a current W and synchronizes two 4,096 Fl matrix W, which are represented as 4,096 Fl matrix W."}, {"heading": "4.3.1 Sufficient Factor-based Communication", "text": "Some MPMs, including neural networks, enjoy the following structural property: if the training with SGD = three fully equipped SGD = three fully equipped SFS = 7 is a fully equipped matrix that can be sent as an outer product of two levels u and v, where u and v are designated as sufficient factors (SFS); consider the training of CNNs in which W is an M & N weight matrix between two levels li and li + 1. In advance, a data sample is fed into the network and the activation of layers li is produced as ai. While the loss of M & M is propagated, and an error message from Ei + 1, which is an M-dimensional vector, is passed from li + 1 to li. Thus, the gradients of W can be accurately reconstructed."}, {"heading": "At iteration t on worker p:", "text": "Input: Level li, M x N gradients, number of workers P, lot size K. Task: Gradients, Api, and then update A p i. 1, if li is not a FC level, then 2 Send the updated Ai to the master node. 3 Synchronize the updated Ai from the master node. 4 otherwise 5 Reast Api in two SFs, i.e. A p i = u p i v p i p i >; 6 if (P \u2212 1) 2K (M + N) \u2264 PK (M + N) + PMN then 7 broadcast upi, v p i to all other workers. 8 Receive the SFs u ji, v j i, j 6 = p from all other workers. 9 Update Ai: Ai x Ai + j i v j i > + j (Ai).10 otherwise 11 Send upi, v p i to the master node. 12 Synchronize the updated Ai from the master node."}, {"heading": "4.3.2 Structure-Aware Communication Protocol", "text": "We propose the Structure-aware Communication Protocol (SACP), which hybridizes the client-server-PS schema with the P2P-SFB schema for GPU-based, distributed deep learning. SACP is structure-aware because it determines the optimal communication method before communicating the parameters according to the work level, the SGD batch size, and the number of workers. In particular, for CONV layers, where layer parameters are sparse, SACP chooses the centralized server-client-PS schema to communicate the parameters directly via the parameter server. On the other hand, for FC layers, where the layer parameters are dense and enjoy the low property of MPMs, SACP chooses the centralized server-client-PS schema to directly reduce the parameters according to the batch size and number of workers. Algorithm 3 clearly controls the cost of communication, such as the SACP communication cost to SSF."}, {"heading": "4.3.3 Bandwidth Management", "text": "Poseidon also uses the bus-based communication strategy [26], a key component of Petuum, which maximizes network efficiency on a given network bandwidth budget (especially for commodity Ethernet) while minimizing parallel errors. In collaboration with DWBP and SACP, who are aware of the model and cluster structures, the bandwidth manager also integrates knowledge of low network bandwidth and maximizes communication efficiency by prioritizing network bandwidth for the most important messages for algorithm progress. Specifically, it communicates model updates and dirty model parameters as quickly as possible without overloading the network bandwidth budget (full network utilization), and distributes network bandwidth according to the contribution of messages to convergence. In Poseidon, the bandwidth manager is at the lower end of DWBP and SACP (as shown in Figure 2), and manages message delivery to servers and message types independently from SFS."}, {"heading": "4.4 Other Features", "text": "Poseidon includes features that enhance the usability of the deep learning software system by addressing issues such as distributed storage and error tolerance. Although not critical to the performance of distributed GPU-based training, they help improve the user experience. Distributed storage. Poseidon allows both shared and private file systems for multiple cluster nodes, so training data can be stored either in a shared file system that all cluster nodes can access at the same time, or in separate file systems where each node has a separate data partition to avoid overloading the I / Os. Error tolerance. Poseidon provides error tolerance by checking the model states of all clients. Either in the event of a failure or as specified by the user, the entire distributed CNN system can be restarted from the last checkpoint, leaving all model / solver states and database pointers unchanged."}, {"heading": "5 Evaluation", "text": "First, we evaluate Poseidon on image classification tasks using benchmark datasets from CIFAR-10 [12] and ILSVRC2012 [22] and show that Poseidon significantly accelerates the formation of modern CNN structures while ensuring the correct convergence that is important for distributed deep learning. Furthermore, we use Poseidon on the ImageNet 22K classification and compare its performance with previously published results such as Adam [2]. Finally, we perform some internal comparisons to justify the effectiveness of the DWBP and SACP cluster configuration. We perform all experiments with the PRObE Susitna cluster [17], where each node has over 4 x 16 core 2.1GHz AMD Opteron 6272 CPUs, 128GB RAM and NVIDIA Tesla K20C GPU with 4799 MB of memory. All cluster nodes share access to an NFS HDUachi 1.0 Uachi Hitteron HDD 3.0 and HitOD 40D GPU."}, {"heading": "5.1 Image Classification", "text": "We show Poseidon's performance on three benchmark datasets ranging from small to large, including the CIFAR-10 [12], the ILSVRC2012, and the ImageNet22K [22]. Data set statistics are briefly summarized in Table 4."}, {"heading": "5.1.1 Classification on CIFAR-10", "text": "First, we evaluate our Poseidon on the CIFAR-10 dataset, which contains 32 \u00d7 32 images from 10 classes, with 6K images per class. An official train / test split is intended to use 50K images for training and 10K for testing. Although CIFAR-10 is a relatively small dataset, we are experimenting to demonstrate Poseidon's ability to achieve better accuracy than a single machine simultaneously, we are accelerating the formation of small CNNs. Settings. We are using the built-in Cifar10 Quick Train network structure in Caffe4, consisting of 3 CONV layers and 1 FC layers, followed by a 10-way Softmax classifier, in a total of 145,578 parameters. It compares with a 70% test accuracy with 4 epochs of training in a single machine, without reducing the learning rate."}, {"heading": "5.1.2 Classification on ILSVRC 2012", "text": "We then experiment with ImageNet ILSVRC 2012, consisting of 1.28 million training images and 50K validation images across 1,000 categories. Following the standards, we download all images to 256 x 256 x 3 before feeding them into the networks, and report the top-1 accuracy on the validation plate. These experiments show that Poseidon significantly accelerates the formation of modern states while guaranteeing correct convergence in a distributed GPU cluster. We rate Poseidon with AlexNet [14] and GoogLeNet [24]. Alexnet is a de facto standard CNN architecture with 5 CONV layers, 2 FC layers and a 1000-class softeifier, covering a total of 61.3 million parameters. GoogLeNet is a structural and deeper education (22 layers) CNN with only 5 million parameters."}, {"heading": "5.1.3 Classification on ImageNet 22K", "text": "ImageNet 22K is the largest public image classification dataset, including 14,197,087 labeled images from 21,841 categories, seldom touched by the research community due to its massive data size and complexity. We are experimenting with ImageNet 22K to demonstrate the scalability of Poseidon. However, since no official test data exists for evaluation, we downsize all images to 256 x 256 and report the top-1 test accuracy. Settings: We design an AlexNet-like CNN architecture and use the first 7.1 million images for training and remain for the test. Similar to ILSVRC 2012, we downsize all images to 256 x 256 and scale down the top-1 test accuracy. Settings: We design an AlexNet-like CNN architecture; in particular, CNN takes a random 227 x 227 snippet from the original image and passes it in 5 CONV layers and 2 FC layers before making a prediction."}, {"heading": "5.2 Internal Comparisons", "text": "In this section, we perform internal comparisons to examine the effectiveness of DWBP and SACP in improving GPU utilization and reducing communication costs for GPU-based, distributed deep learning. Also, in Fig.8, we report on the acceleration in throughput (i.e. the number of images carried per second) when AlexNet and GoogLeNet use Poseidon on 8 GPU nodes with different posture settings, compared to single machine caffe."}, {"heading": "5.2.1 DWBP and SACP", "text": "It is difficult to directly monitor how communication and calculation intersect. Instead, to measure DWBP and SACP improvement, we evaluate the acceleration in throughput, which is defined as the number of images processed per second, by specifying a model and batch size relative to each machine."}, {"heading": "5.2.2 SSP Consistency Model", "text": "In this section, we examine the effectiveness of the outdated synchronous parallel consistency model (SSP), which is a unique feature of Petuum in scaling distributed deep learning. Specifically, we compare the acceleration in throughput of training with AlexNet and GoogLeNet using Poseidon by varying the shelf life threshold value while leaving all other settings unchanged. Setting shelf life values to zero (i.e. s = 0) results in consistency management being mass synchronous parallelization (BSP), where the calculation uses local model copies that are synchronized only at the end of each iteration and the next iteration before all machines have received current model parameters. Therefore, at BSP, the learning speed is limited by the slowest machine. Compared to BSP, a positive shelf life value generates a short grace period for parameter synchronization."}, {"heading": "6 Conclusion", "text": "We present Poseidon, a highly scalable and efficient system architecture for large-scale deep learning on GPU clusters. Poseidon builds on Petuum, inheriting many of the functionalities and benefits of Petuum. Its design focuses on efficiently leveraging multiple distributed GPUs on hardware and Ethernet to maximize existing single-machine DL frameworks with a fully data-parallel distributed deep learning scheme. We evaluate Poseidon empirically in terms of throughput, convergence and accuracy in image classification tasks with multiple standard datasets, and demonstrate Poseidon's ability to achieve state-of-the-art accelerations in accelerating the training of modern CNN structures while ensuring correct convergence."}], "references": [{"title": "Project Adam: Building an Efficient and Scalable Deep Learning Training System", "author": ["T. CHILIMBI", "Y.S.J. APACIBLE", "K. KALYANARAMAN"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Deep Learning with COTS HPC Systems", "author": ["A. COATES", "B. HUVAL", "T. WANG", "D.J. WU", "A.Y. NG", "B. CATANZARO"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "AND FARABET, C. Torch7: A Matlab-like Environment for Machine Learning", "author": ["R. COLLOBERT", "K. KAVUKCUOGLU"], "venue": "NIPSW", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Analysis of high-performance distributed ml at scale through parameter server consistency models", "author": ["W. DAI", "A. KUMAR", "J. WEI", "Q. HO", "G. GIBSON", "E.P. XING"], "venue": "In Proceedings of the 29th AAAI Conference on Artificial Intelligence", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Recent Advances in Deep Learning for Speech Research at Microsoft", "author": ["L. DENG", "J. LI", "HUANG", "YAO J.-T", "YU K", "F. SEIDE", "M.L. SELTZER", "G. ZWEIG", "X. HE", "J. WILLIAMS", "Y. GONG", "A. ACERO"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server", "author": ["Q. HO", "J. CIPAR", "H. CUI", "J.K. KIM", "S. LEE", "P.B. GIBBONS", "G.A. GIBSON", "G.R. GANGER", "E.P. XING"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. IOFFE", "C. SZEGEDY"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Y. JIA", "E. SHELHAMER", "J. DONAHUE", "S. KARAYEV", "J. LONG", "R. GIRSHICK", "S. GUADARRAMA", "T. DARRELL"], "venue": "In MM", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["A. KRIZHEVSKY"], "venue": "Master\u2019s thesis, University of Toronto,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "One Weird Trick for Parallelizing Convolutional Neural Networks", "author": ["A. KRIZHEVSKY"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. KRIZHEVSKY", "I. SUTSKEVER", "G.E. HINTON"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Building High-level Features Using Large Scale Unsupervised Learning", "author": ["Q.V. LE", "R. MONGA", "M. DEVIN", "K. CHEN", "G.S. CORRADO", "J. DEAN", "NG", "A. Y"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Stronger Semantics for Low-Latency Geo-Replicated Storage", "author": ["W. LLOYD", "M.J. FREEDMAN", "M. KAMINSKY", "D.G. ANDER- SEN"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. MIKOLOV", "K. CHEN", "G. CORRADO", "J. DEAN"], "venue": "ICLRW", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Sparknet: Training deep networks in spark", "author": ["P. MORITZ", "R. NISHIHARA", "I. STOICA", "M.I. JORDAN"], "venue": "arXiv preprint arXiv:1511.06051", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Learning internal representations by error propagation", "author": ["D.E. RUMELHART", "G.E. HINTON", "R.J. WILLIAMS"], "venue": "Tech. rep., DTIC Document,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1985}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. SIMONYAN", "A. ZISSERMAN"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "VANHOUCKE, V., AND RABI- NOVICH, A. Going deeper with convolutions", "author": ["C. SZEGEDY", "W. LIU", "Y. JIA", "P. SERMANET", "S. REED", "D. ANGUELOV", "D. ERHAN"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Fast Convolutional Nets With fbfft: A GPU Performance Evaluation", "author": ["N. VASILACHE", "J. JOHNSON", "S. CHINTALA", "S. PIANTINO", "Y. LECUN"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Managed Communication and Consistency for Fast Data-parallel Iterative Analytics", "author": ["J. WEI", "W. DAI", "A. QIAO", "Q. HO", "H. CUI", "G.R. GANGER", "P.B. GIBBONS", "G.A. GIBSON", "E.P. XING"], "venue": "SoCC", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Distributed Machine Learning via Sufficient Factor Broadcasting", "author": ["XIE P", "KIM J. K", "ZHOU Y", "HO Q", "KUMAR A", "YU Y", "XING E"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Petuum: A New Platform for Distributed Machine Learning on Big Data", "author": ["XING E. P", "HO Q", "DAI W", "KIM J. K", "WEI J", "LEE S", "ZHENG X", "XIE P", "KUMAR A", "YU"], "venue": "KDD", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Multi-GPU Training of ConvNets", "author": ["O. YADAN", "K. ADAMS", "Y. TAIGMAN", "M. RANZATO"], "venue": "ICLRW", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Tencent Deep Learning Platform and its Applications", "author": ["ZOU Y", "JIN X", "LI Y", "GUO Z", "WANG E", "XIAO", "Mariana B"], "venue": "In VLDB Endowment", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "A number of software \u201cframeworks\u201d have been developed to expedite the process of designing and training deep neural networks, such as Caffe [11], Torch [4], and Theano [1].", "startOffset": 140, "endOffset": 144}, {"referenceID": 2, "context": "A number of software \u201cframeworks\u201d have been developed to expedite the process of designing and training deep neural networks, such as Caffe [11], Torch [4], and Theano [1].", "startOffset": 152, "endOffset": 155}, {"referenceID": 7, "context": "In order to assess Poseidon\u2019s effectiveness, we integrate Poseidon into the Caffe [11] framework and evaluate its performance at training convolutional neural networks for object recognition in images.", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "On the much larger ImageNet 22K dataset, Poseidon with 8 nodes achieves better speedup and competitive accuracy to recent CPU-based distributed deep learning systems such as Adam [2] and Le et al.", "startOffset": 179, "endOffset": 182}, {"referenceID": 11, "context": "[16], which use 10s to 1000s of nodes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Modern deep learning models, such as convolutional neural networks (CNNs), have achieved notable successes in a wide spectrum of machine learning tasks, including speech recognition [7], visual recognition [14] and language understanding [18].", "startOffset": 182, "endOffset": 185}, {"referenceID": 10, "context": "Modern deep learning models, such as convolutional neural networks (CNNs), have achieved notable successes in a wide spectrum of machine learning tasks, including speech recognition [7], visual recognition [14] and language understanding [18].", "startOffset": 206, "endOffset": 210}, {"referenceID": 13, "context": "Modern deep learning models, such as convolutional neural networks (CNNs), have achieved notable successes in a wide spectrum of machine learning tasks, including speech recognition [7], visual recognition [14] and language understanding [18].", "startOffset": 238, "endOffset": 242}, {"referenceID": 7, "context": "The explosive prosperity and rapid adoption of CNNs by research community are largely attributed to high performance computing hardware, such as GPUs, as well as a wide range of easy-to-use open source frameworks based on GPUs, including Caffe [11], Torch [4], Theano [1].", "startOffset": 244, "endOffset": 248}, {"referenceID": 2, "context": "The explosive prosperity and rapid adoption of CNNs by research community are largely attributed to high performance computing hardware, such as GPUs, as well as a wide range of easy-to-use open source frameworks based on GPUs, including Caffe [11], Torch [4], Theano [1].", "startOffset": 256, "endOffset": 259}, {"referenceID": 23, "context": "[30] report the Tencent deep learning platform named as Mariana, which distributes neural network training onto CPU clusters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[16] later scale up to a cluster of 16,000 CPU cores by exploiting model parallelism and asynchronous SGD.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Recently, Microsoft\u2019s Adam [2] achieved state-of-the-art results on the ImageNet22K classification task, by leveraging distributed systems techniques such as a global parameter server, ar X iv :1 51 2.", "startOffset": 27, "endOffset": 30}, {"referenceID": 9, "context": "Compared to CPU-based distributed deep learning, parallelization of deep networks on GPU-equipped clusters is more readily available to researchers, since satisfactory speedups could potentially be achieved with a smaller number of GPU cards [13].", "startOffset": 242, "endOffset": 246}, {"referenceID": 1, "context": "First, Infiniband networking, which has been responsible for past successes in distributed DL [3], is not available on most cloud computing platforms and lab clusters, where only commodity hardware with limited network bandwidth is installed.", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "On the other hand, in contrast to systems that require specialized hardware [3], we want our solution to effectively harness distributed GPUs installed on commodity servers and con-", "startOffset": 76, "endOffset": 79}, {"referenceID": 21, "context": "We implemented Poseidon\u2019s distributed layer upon the Petuum distributed ML framework [28], which provides a bounded stale synchronous parallel (SSP) parameter server [9] that preserves data-parallel convergence guarantees, and prioritized network bandwidth allocation [26].", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "We implemented Poseidon\u2019s distributed layer upon the Petuum distributed ML framework [28], which provides a bounded stale synchronous parallel (SSP) parameter server [9] that preserves data-parallel convergence guarantees, and prioritized network bandwidth allocation [26].", "startOffset": 166, "endOffset": 169}, {"referenceID": 19, "context": "We implemented Poseidon\u2019s distributed layer upon the Petuum distributed ML framework [28], which provides a bounded stale synchronous parallel (SSP) parameter server [9] that preserves data-parallel convergence guarantees, and prioritized network bandwidth allocation [26].", "startOffset": 268, "endOffset": 272}, {"referenceID": 8, "context": "ent sizes: CIFAR-10 [12], ILSVRC2012, and ImageNet 22K [22].", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "30% training time and 13% cluster nodes compared to Adam [2]).", "startOffset": 57, "endOffset": 60}, {"referenceID": 21, "context": "We summarize our main contributions as follows: (1) We propose Poseidon, a scalable system architecture as a general purpose solution for any single-machine DL framework to be efficiently distributed on GPU clusters with commodity Ethernet, by leveraging the Petuum framework [28] as well as three components: a threelevel architecture, distributed wait-free backpropagation, and structure-aware communication protocol.", "startOffset": 276, "endOffset": 280}, {"referenceID": 22, "context": "[29] show that mixed parallelism yields better speedups over model-only or data-only parallelism in ImageNet classification with 4 GPUs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Similarly, Krizhevsky [13] also implements mixed parallelism for AlexNet [14] with 8 GPUs which", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "Similarly, Krizhevsky [13] also implements mixed parallelism for AlexNet [14] with 8 GPUs which", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "Facebook\u2019s fbcunn [8, 25] implements both model- and dataparallelism on multiple GPUs.", "startOffset": 18, "endOffset": 25}, {"referenceID": 1, "context": "[3] demonstrated that they could train a 11-billion parameter network on a cluster of 16 GPU nodes using model-parallelism, but their implementation required specialized hardware, such as Infiniband networking.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Also of note are several efforts to port Caffe onto the Spark platform, such as SparkNet [19], which reports a 4-5 times speedup with 10 machines (and hence less scalability than our results herein), as well as a recent, non-peer-reviewed, effort by Yahoo which exclusively uses Infiniband RDMA.", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "Poseidon builds upon Petuum, a distributed big machine learning framework that provides a generic interface to a broad spectrum of ML programs [28].", "startOffset": 143, "endOffset": 147}, {"referenceID": 5, "context": "they still execute and converge correctly even when their model parameters A experience synchronization delays, provided that those delays are strictly bounded [9, 5, 15].", "startOffset": 160, "endOffset": 170}, {"referenceID": 3, "context": "they still execute and converge correctly even when their model parameters A experience synchronization delays, provided that those delays are strictly bounded [9, 5, 15].", "startOffset": 160, "endOffset": 170}, {"referenceID": 3, "context": "For stochastic gradient descent algorithms, SSP has very attractive theoretical properties [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 19, "context": "Poseidon\u2019s distributed layer is derived from B\u00f6sen [26], a parameter server implementation that supports SSP consistency model.", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "It allows computations to use stale model parameters (to reduce synchronization overheads), but strictly upper-bounds the number of missing iterations, restoring formal convergence guarantees [9].", "startOffset": 192, "endOffset": 195}, {"referenceID": 0, "context": "ample, in previous CPU-based distributed DL systems [2, 6], a two-level parameter server architecture was built, where the first level has server machines collecting gradients and distributing newly updated model parameters to workers, and the second level has worker nodes (threads) taking batches of training data and generating", "startOffset": 52, "endOffset": 58}, {"referenceID": 7, "context": "At the beginning of training, every worker thread starts its Caffe engine [11] to", "startOffset": 74, "endOffset": 78}, {"referenceID": 5, "context": "2), during which they communicate asynchronously following a consistency model of the bounded stale synchronous scheme [9], as we briefly introduced in section .", "startOffset": 119, "endOffset": 122}, {"referenceID": 19, "context": "In the lower level, the communications are further monitored and operated by a bandwidth manager provided by Petuum B\u00f6sen [26], as we explain in section 4.", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "Backpropagation (BP) [21] is the principle algorithm for training neural networks.", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": "fully-connected layers in AlexNet [14] and VGG-16 [23].", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "fully-connected layers in AlexNet [14] and VGG-16 [23].", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "The DWBP is even more effective in GPU clusters with state-of-the-art CNN architectures, such as AlexNet [14] and VGG-16 [23], which stack convolutional (CONV) layers at the bottom, followed by fullyconnected (FC) layers at the top.", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "The DWBP is even more effective in GPU clusters with state-of-the-art CNN architectures, such as AlexNet [14] and VGG-16 [23], which stack convolutional (CONV) layers at the bottom, followed by fullyconnected (FC) layers at the top.", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "In data-parallel distributed settings, learning MPMs using iterative-convergent algorithms, as in [2, 6], usually needs to repeatedly push out and pull in the whole parameter matrices.", "startOffset": 98, "endOffset": 104}, {"referenceID": 20, "context": "In this section, we first introduce a novel communication approach of Petuum for distributed machine learning, namely sufficient factor broadcasting (SFB) [27], which exchanges parameters following a P2P scheme.", "startOffset": 155, "endOffset": 159}, {"referenceID": 20, "context": "Sufficient factor broadcasting (SFB) [27] is designed to", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "Microsoft Adam [2] employs a different SF-based strategy.", "startOffset": 15, "endOffset": 18}, {"referenceID": 19, "context": "Poseidon also exploits the B\u00f6sen-based communication strategy [26], a key component of Petuum that maximizes the network efficiency under a given network bandwidth budget (especially in commodity Ethernet ) while minimizing parallel errors.", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "We first evaluate Poseidon on image classification tasks with benchmark datasets of CIFAR-10 [12] and ILSVRC2012 [22], and show that Poseidon significantly accelerates the training of modern CNN structures, while guaranteeing the correct convergence, which is important", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "Moreover, we deploy Poseidon on the ImageNet 22K classification, and compare its performance with previously published results such as Adam [2].", "startOffset": 140, "endOffset": 143}, {"referenceID": 12, "context": "We conduct all experiments on the PRObE Susitna cluster [17], where each node has 4\u00d7 16-core 2.", "startOffset": 56, "endOffset": 60}, {"referenceID": 8, "context": "We demonstrate Poseidon\u2019s performance on three benchmark datasets ranging from small to large, including the CIFAR-10 [12], the ILSVRC2012 and the ImageNet22K[22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "We evaluate Poseidon using AlexNet [14] and GoogLeNet [24].", "startOffset": 35, "endOffset": 39}, {"referenceID": 17, "context": "We evaluate Poseidon using AlexNet [14] and GoogLeNet [24].", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "GoogLeNet is a more structural and deeper (22-layer) CNN with only 5 million of parameters by stacking inception modules [24].", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "As no official test data exists for evaluation, following previous settings in [2, 6, 16], we randomly split the whole set into two parts, and use the first 7.", "startOffset": 79, "endOffset": 89}, {"referenceID": 11, "context": "As no official test data exists for evaluation, following previous settings in [2, 6, 16], we randomly split the whole set into two parts, and use the first 7.", "startOffset": 79, "endOffset": 89}, {"referenceID": 0, "context": "Table 6 compares our result to those of previous work on ImageNet 22K, Adam [2], MXNet, and Le et al.", "startOffset": 76, "endOffset": 79}, {"referenceID": 11, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Compared to Adam [2], we only use 30% training time and 13% machines to achieve 23.", "startOffset": 17, "endOffset": 20}, {"referenceID": 6, "context": "1 million images to train an inception-BN structure [10] using 4 GPUs in a single machine without network communication, and reports 37.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "7% Adam [2] 7.", "startOffset": 8, "endOffset": 11}, {"referenceID": 11, "context": "[16] w/ pretrain 7.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Deep learning models, which learn high-level feature representations from raw data, have become popular for machine learning and artificial intelligence tasks that involve images, audio, and other forms of complex data. A number of software \u201cframeworks\u201d have been developed to expedite the process of designing and training deep neural networks, such as Caffe [11], Torch [4], and Theano [1]. Currently, these frameworks can harness multiple GPUs on the same machine, but are unable to use GPUs that are distributed across multiple machines; because even average-sized deep networks can take days to train on a single GPU when faced with 100s of GBs to TBs of data, distributed GPUs present a prime opportunity for scaling up deep learning. However, the limited inter-machine bandwidth available on commodity Ethernet networks presents a bottleneck to distributed GPU training, and prevents its trivial realization. To investigate how existing software frameworks can be adapted to efficiently support distributed GPUs, we propose Poseidon, a scalable system architecture for distributed inter-machine communication in existing deep learning frameworks. In order to assess Poseidon\u2019s effectiveness, we integrate Poseidon into the Caffe [11] framework and evaluate its performance at training convolutional neural networks for object recognition in images. Poseidon features three key contributions that improve the training speed of deep neural networks on clusters: (i) a three-level hybrid architecture that allows Poseidon to support both CPU-only clusters as well as GPU-equipped clusters, (ii) a distributed wait-free backpropagation (DWBP) algorithm to improve GPU utilization and to balance communication, and (iii) a dedicated structure-aware communication protocol (SACP) to minimize communication overheads. We empirically show that Poseidon converges to the same objective value as a single machine, and achieves state-of-the-art training speedup across multiple models and well-established datasets, using a commodity GPU cluster of 8 nodes (e.g. 4.5\u00d7 speedup on AlexNet, 4\u00d7 on GoogLeNet, 4\u00d7 on CIFAR-10). On the much larger ImageNet 22K dataset, Poseidon with 8 nodes achieves better speedup and competitive accuracy to recent CPU-based distributed deep learning systems such as Adam [2] and Le et al. [16], which use 10s to 1000s of nodes.", "creator": "LaTeX with hyperref package"}}}