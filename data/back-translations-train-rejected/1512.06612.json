{"id": "1512.06612", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2015", "title": "Backward and Forward Language Modeling for Constrained Sentence Generation", "abstract": "Recent language models, especially those based on recurrent neural networks (RNNs), make it possible to generate natural language from a learned probability. Language generation has wide applications including machine translation, summarization, question answering, conversation systems, etc. Existing methods typically learn a joint probability of words conditioned on additional information, which is (either statically or dynamically) fed to RNN's hidden layer. In many applications, we are likely to impose hard constraints on the generated texts, i.e., a particular word must appear in the sentence. Unfortunately, existing methods could not solve this problem. In this paper, we propose a backbone language model (backbone LM) for constrained language generation. Provided a specific word, our model generates previous words and future words simultaneously. In this way, the given word could appear at any position in the sentence. Experimental results show that the generated texts are coherent and fluent.", "histories": [["v1", "Mon, 21 Dec 2015 13:07:31 GMT  (222kb,D)", "https://arxiv.org/abs/1512.06612v1", null], ["v2", "Sun, 3 Jan 2016 20:15:44 GMT  (445kb,D)", "http://arxiv.org/abs/1512.06612v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["lili mou", "rui yan", "ge li", "lu zhang", "zhi jin"], "accepted": false, "id": "1512.06612"}, "pdf": {"name": "1512.06612.pdf", "metadata": {"source": "CRF", "title": "Backward and Forward Language Modeling for Constrained Sentence Generation", "authors": ["Lili Mou", "Rui Yan", "Ge Li", "Lu zhang", "Zhi Jin"], "emails": ["doublepower.mou@gmail.com", "rui.yan.peking@gmail.com", "lige@sei.pku.edu.cn", "zhanglu@sei.pku.edu.cn", "zhijin@sei.pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year, it will be able to come out on top without breaking away."}, {"heading": "2 Background and Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Language Modeling", "text": "Faced with a corpus w = w1, \u00b7 \u00b7 \u00b7, wm, language modeling aims to minimize the common distribution of w, i.e. p (w) forward. Inspired by the observation that people always say a sentence from beginning to end, we would like to further simplify the above equation to estimate the common probability as2p (w) = m \u00b2 t = 1 p (wt | wt \u2212 11) (1) Parameterization by multinomial distributions, in order to estimate the parameters. A Markov assumption - a word depends only on previous n \u2212 1 words and regardless of its position - results in the classic n \u2212 gram model, where the common probability by p (w) \u2248 m \u00b2 t = 1 p (wt = 1t \u2212 n + 1) the Markov assumption - a word depends only on previous n \u2212 1 words and regardless of its position - results in the classic n \u2212 gram model, where the common probability by wt = 1 p = 1 t \u2212 n is a common probability (equal wt = 1 t \u2212 n)."}, {"heading": "2.2 Language Generation", "text": "Using RNNs to model the common probability of language, it is possible to generate new sentences. An early experiment generated texts using a character-level RNN language model [14]; recently, RNN-based language generation has used several real-world applications.The sequence for sequencing machine translation models [15] uses one RNN to encode a source sentence (in foreign language) into one or more fixed-size vectors; another RNN then decrypts the vector (s) to the target sentence. Such a network can be considered a language model, depending on the source sentence. In each step, the RNN predicts the most likely word as output; the embedding of the word is fed to the input layer at the next step. The process continues until the RNN produces a special symbol < eos > that indicates the end of the sequence. Beam search [15] or text sampling methods [16] can be used to improve diversity and diversity."}, {"heading": "3 The Proposed B/F Language Model", "text": "In this part we introduce our B / F models: \"We have the chance that the sentence will be divided into two subsequences: s, ws \u2212 s\" 2, ws \u2212 s \"2, ws \u2212 s\" 2, ws \u2212 s \"2, ws \u2212 s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\""}, {"heading": "4 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 The Dataset and Settings", "text": "To evaluate our B / F-LMs, we prefer a vertical domain corpus with interesting application nuggets rather than generic texts such as Wikipedia. Specifically, we have decided to build a language model on scientific paper titles on arXiv.6. Building a language model on paper titles can help researchers prepare their drafts. Provided that a topic (designated by a particular word) is created, the limited generation of natural language could also act as a form of brainstorming. 7We removed computer science-related paper titles from January 2014 to November 2015.8. Each word has been capitalized but no containment has been performed. Rare words (\u2264 10 occurrences) were grouped as a single token (relative to unknown). We removed non-English titles and those with more than three. We note that these words occur frequently, but a large number of them refer to acronyms, and therefore are largely uniform in semantics."}, {"heading": "4.2 Results", "text": "In fact, it is not as if we are embarking on a search for a solution, as is the case in practice. (...) It is not as if we are embarking on a solution. (...) It is not as if we are embarking on a solution. (...) It is not as if we are embarking on a solution. (...) It is not as if we are embarking on a solution. (...) It is as if we are embarking on a solution. (...) It is as if we are embarking on a solution. (...) It is as if we are embarking on a solution. \"(...) It is as if we are embarking on a solution.\" (...) It is as if we are embarking on a solution. (...) It is as if we are embarking on a solution."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a backward and forward language model (B / F LM) for the restricted generation of natural language. Given a particular word, our model can generate earlier words and future words either synchronously or asynchronously. Experiments show a similar helplessness as sequential LM, if we disregard the helplessness caused by random columning. Our case study shows that asynchronous B / F LM can generate sentences that contain the given word and are qualitatively comparable to sequential LM."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Bidirectional recurrent neural networks as generative models", "author": ["M. Berglund", "T. Raiko", "M. Honkala", "L. K\u00e4rkk\u00e4inen", "A. Vetek", "J.T. Karhunen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Speech and Language Processing", "author": ["D. Jurafsky", "J.H. Martin"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "F.-F. Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Convolutional neural networks over tree structures for programming language processing", "author": ["L. Mou", "G. Li", "L. Zhang", "T. Wang", "Z. Jin"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["L. Mou", "H. Peng", "G. Li", "Y. Xu", "L. Zhang", "Z. Jin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "A comparative study on regularization strategies for embedding-based neural networks", "author": ["H. Peng", "L. Mou", "G. Li", "Y. Chen", "Y. Lu", "Z. Jin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking", "author": ["T.-H. Wen", "M. Gasic", "D. Kim", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young"], "venue": "In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Y. Xu", "L. Mou", "G. Li", "Y. Chen", "H. Peng", "Z. Jin"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Attention with intention for a neural network conversation model", "author": ["K. Yao", "G. Zweig", "B. Peng"], "venue": "arXiv preprint arXiv:1510.08565 (NIPS Workshop),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Neural generative question answering", "author": ["J. Yin", "X. Jiang", "Z. Lu", "L. Shang", "H. Li", "X. Li"], "venue": "arXiv preprint arXiv:1512.01337,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "It has long been the core of natural language processing (NLP) [8], and has inspired a variety of other models, e.", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": ", the n-gram model, smoothing techniques [4], as well as various neural networks for NLP [2, 6, 17].", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": ", the n-gram model, smoothing techniques [4], as well as various neural networks for NLP [2, 6, 17].", "startOffset": 89, "endOffset": 99}, {"referenceID": 4, "context": ", the n-gram model, smoothing techniques [4], as well as various neural networks for NLP [2, 6, 17].", "startOffset": 89, "endOffset": 99}, {"referenceID": 15, "context": ", the n-gram model, smoothing techniques [4], as well as various neural networks for NLP [2, 6, 17].", "startOffset": 89, "endOffset": 99}, {"referenceID": 1, "context": "In particular, the renewed prosperity of neural models has made groundbreaking improvement in many tasks, including language modeling per se [2], part-of-speech tagging, named entity recognition, semantic role labeling [6], etc.", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "In particular, the renewed prosperity of neural models has made groundbreaking improvement in many tasks, including language modeling per se [2], part-of-speech tagging, named entity recognition, semantic role labeling [6], etc.", "startOffset": 219, "endOffset": 222}, {"referenceID": 5, "context": "Compared with traditional n-gram models, RNNs are more capable of learning long range features\u2014especially with long short term memory (LSTM) units [7] or gated recurrent units (GRU) [5]\u2014and hence are better at capturing the nature of sentences.", "startOffset": 147, "endOffset": 150}, {"referenceID": 13, "context": "On such a basis, it is even possible to generate a sentence from an RNN language model, which has wide applications in NLP, including machine translation [15], abstractive summarization [13], question answering [19], and conversation systems [18].", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "On such a basis, it is even possible to generate a sentence from an RNN language model, which has wide applications in NLP, including machine translation [15], abstractive summarization [13], question answering [19], and conversation systems [18].", "startOffset": 186, "endOffset": 190}, {"referenceID": 17, "context": "On such a basis, it is even possible to generate a sentence from an RNN language model, which has wide applications in NLP, including machine translation [15], abstractive summarization [13], question answering [19], and conversation systems [18].", "startOffset": 211, "endOffset": 215}, {"referenceID": 16, "context": "On such a basis, it is even possible to generate a sentence from an RNN language model, which has wide applications in NLP, including machine translation [15], abstractive summarization [13], question answering [19], and conversation systems [18].", "startOffset": 242, "endOffset": 246}, {"referenceID": 13, "context": ", the vector representation of the source sentence in a machine translation system [15]).", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "Unfortunately, using existing language models to generate a sentence with a given word is non-trivial: adding additional information [16, 19] about a word does not guarantee that the wanted word will appear; generic probabilistic samplers (e.", "startOffset": 133, "endOffset": 141}, {"referenceID": 17, "context": "Unfortunately, using existing language models to generate a sentence with a given word is non-trivial: adding additional information [16, 19] about a word does not guarantee that the wanted word will appear; generic probabilistic samplers (e.", "startOffset": 133, "endOffset": 141}, {"referenceID": 6, "context": "We refer interested readers to textbooks like [8] for n-gram models and their variants.", "startOffset": 46, "endOffset": 49}, {"referenceID": 2, "context": "With recent efforts in [3].", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "[2] propose to use feed-forward neural networks to estimate the probability in Equation 2.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In practice, the vanilla RNN is difficult to train due to the gradient vanishing or exploding problem; long short term (LSTM) units [7] and gated recurrent units (GRU) [5] are proposed to better balance between the previous state and the current input.", "startOffset": 132, "endOffset": 135}, {"referenceID": 12, "context": "An early attempt generates texts by a character-level RNN language model [14]; recently, RNN-based language generation has made breakthroughs in several real applications.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "The sequence to sequence machine translation model [15] uses an RNN to encode a source sentence (in foreign language) into one or a few fixed-size vectors; another RNN then decodes the vector(s) to the target sentence.", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "Beam search [15] or sampling methods [16] can be used to improve the quality and diversity of generated texts.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "Beam search [15] or sampling methods [16] can be used to improve the quality and diversity of generated texts.", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "If the source sentence is too long to fit into one or a few fixed-size vectors, an attention mechanism [1] can be used to dynamically focus on different parts of the source sentence during target generation.", "startOffset": 103, "endOffset": 106}, {"referenceID": 14, "context": "use an RNN to generate a sentence based on some abstract representations of semantics; they feed a one-hot vector, as additional information, to the RNN\u2019s hidden layer [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 17, "context": "leverage a soft logistic switcher to either generate a word from the vocabulary or copy the candidate answer [19].", "startOffset": 109, "endOffset": 113}, {"referenceID": 7, "context": "Following [9], we applied rmsprop for optimization (embeddings excluded), which is more suitable for training RNNs than n\u00e4\u0131ve stochastic gradient descent, and less sensitive to hyperparameters compared with momentum methods.", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": "As word embeddings are sparse in use [12], they were optimized asynchronously by pure stochastic gradient descent with learning rate being divided by \u221a .", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "\u2022 Info-all: Built upon sequential LM, Info-all takes the wanted word\u2019s embedding as additional input at each time step during sequence generation, similar to [16].", "startOffset": 158, "endOffset": 162}, {"referenceID": 13, "context": "\u2022 Info-init: The wanted word\u2019s embedding is only added at the first step (sequence to sequence model [15]).", "startOffset": 101, "endOffset": 105}, {"referenceID": 8, "context": "The implementation was based on [10, 11].", "startOffset": 32, "endOffset": 40}, {"referenceID": 9, "context": "The implementation was based on [10, 11].", "startOffset": 32, "endOffset": 40}, {"referenceID": 14, "context": "The problem is also addressed in [16].", "startOffset": 33, "endOffset": 37}], "year": 2016, "abstractText": "Recent language models, especially those based on recurrent neural networks (RNNs), make it possible to generate natural language from a learned probability. Language generation has wide applications including machine translation, summarization, question answering, conversation systems, etc. Existing methods typically learn a joint probability of words conditioned on additional information, which is (either statically or dynamically) fed to RNN\u2019s hidden layer. In many applications, we are likely to impose hard constraints on the generated texts, i.e., a particular word must appear in the sentence. Unfortunately, existing approaches could not solve this problem. In this paper, we propose a novel backward and forward language model. Provided a specific word, we use RNNs to generate previous words and future words, either simultaneously or asynchronously, resulting in two model variants. In this way, the given word could appear at any position in the sentence. Experimental results show that the generated texts are comparable to sequential LMs in quality.", "creator": "LaTeX with hyperref package"}}}