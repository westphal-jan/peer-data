{"id": "1512.06643", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2015", "title": "The 2015 Sheffield System for Transcription of Multi-Genre Broadcast Media", "abstract": "We describe the University of Sheffield system for participation in the 2015 Multi-Genre Broadcast (MGB) challenge task of transcribing multi-genre broadcast shows. Transcription was one of four tasks proposed in the MGB challenge, with the aim of advancing the state of the art of automatic speech recognition, speaker diarisation and automatic alignment of subtitles for broadcast media. Four topics are investigated in this work: Data selection techniques for training with unreliable data, automatic speech segmentation of broadcast media shows, acoustic modelling and adaptation in highly variable environments, and language modelling of multi-genre shows. The final system operates in multiple passes, using an initial unadapted decoding stage to refine segmentation, followed by three adapted passes: a hybrid DNN pass with input features normalised by speaker-based cepstral normalisation, another hybrid stage with input features normalised by speaker feature-MLLR transformations, and finally a bottleneck-based tandem stage with noise and speaker factorisation. The combination of these three system outputs provides a final error rate of 27.5% on the official development set, consisting of 47 multi-genre shows.", "histories": [["v1", "Mon, 21 Dec 2015 14:31:31 GMT  (44kb,D)", "http://arxiv.org/abs/1512.06643v1", "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2015), 13-17 Dec 2015, Scottsdale, Arizona, USA"]], "COMMENTS": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2015), 13-17 Dec 2015, Scottsdale, Arizona, USA", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["oscar saz", "mortaza doulaty", "salil deena", "rosanna milner", "raymond w m ng", "madina hasan", "yulan liu", "thomas hain"], "accepted": false, "id": "1512.06643"}, "pdf": {"name": "1512.06643.pdf", "metadata": {"source": "CRF", "title": "THE 2015 SHEFFIELD SYSTEM FOR TRANSCRIPTION OF MULTI\u2013GENRE BROADCAST MEDIA", "authors": ["Oscar Saz", "Mortaza Doulaty", "Salil Deena", "Rosanna Milner", "Raymond W.M. Ng", "Madina Hasan", "Yulan Liu", "Thomas Hain"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Multi-genre broadcasts, automatic speech recognition, data selection, speech segmentation, acoustic adjustment, speech adjustment."}, {"heading": "1. INTRODUCTION", "text": "The ability to search through vast media archives, navigate through thousands of hours of recordings, or structure a media company's entire resources would greatly increase the efficiency of these organizations and the services they provide. Automatic Voice Recognition (ASR) significantly reduced error rates in the early 1990 \"s."}, {"heading": "2. MGB CHALLENGE - TASK 1", "text": "The MGB Challenge 2015 consisted of four different tasks dealing with the topics of broadcast transcription of broadcasts of different genres, easily monitored alignment, longitudinal transcription of broadcasts and longitudinal tacticisation of speakers. The focus of this work was on task 1: transcription of broadcasts from language to text, although aspects of the system presented here were used in submissions for other tasks. A complete description of these and the other tasks in the task can be found in [10], but a brief description of the task is here.Participation in this task required the automatic transcription of a series of broadcasts broadcast by the British Broadcasting Corporation (BBC), which were selected to cover the various genres of broadcast television, categorised by 8 genres: counselling, children, comedy, competition, documentary, drama, events and news."}, {"heading": "2.1. Common system description", "text": "This year it was so far that it was able to the aforementioned rf\u00fc the mentioned rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green"}, {"heading": "3. DATA SELECTION AND TRAINING", "text": "One of the main difficulties for transcription in the MGB challenge was the efficient use of the acoustic training data provided because the use of previous models or other data was not permitted; the transcription of the training data was not made for ASR training purposes; only the subtitle text that was broadcast with each show could be used, which is of varying quality for a variety of reasons.The transcripts provided for training showed that the timestamps of the subtitles do not always correspond to the actual spoken words. [10, 21] After this process, 1,196.73 consultation hours were provided for the training; the transcripts for the training were unreliable."}, {"heading": "4. AUTOMATIC SEGMENTATION", "text": "As a matter of fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "5. ACOUSTIC BACKGROUND MODELLING", "text": "Dealing with acoustic variability is one of the most important questions that arise when transcribing broadcasts of different genres, and the existence of a wide variety of possible recording conditions and acoustic background environments poses a real challenge for ASR systems. In this paper, two approaches were examined to compensate for this variability: the first aimed at normalizing background variability in the input to DNNs for hybrid systems, while the second aimed at using asynchronous transformations with maximum likelihood linear regression (aCMLLR) to compensate for dynamic background noise in bottleneck systems [27]."}, {"heading": "5.1. Domain adaptation of hybrid systems", "text": "The adaptation of DNN-based ASR systems is currently one of the most intensively researched areas of speech recognition technology. While several approaches have been evaluated in the past, normalization of the input characteristics is most commonly applied, e.g. by direct transformations of the input characteristics using feature MLLR transformations [28] or by using additional input characteristics representing a speaker characteristic, such as i-vectors [29, 30]. Latent dirichlet allocation models (LDA) have recently been used to model hidden acoustic categories in audio data. [31] showed that LDA is a suitable model for structuring acoustic data from unknown origin into unattended categories that could be used to provide domain adaptations in ASR. In this work, 64 hidden acoustic domains were found in the acoustic models in which the LDA model was applied in different ways according to each domain's procedure [31] in each domain's structured areas."}, {"heading": "5.2. Dynamic noise adaptation of bottleneck systems", "text": "One of the advantages of tandem (DNN-GMM-HMM) systems is that techniques for adaptation such as Maximum A Posteriori (MAP) or MLLR [32] can be used. In our previous work, a new HMM topology for asynchronous adaptation of GMM-HMM systems was proposed and demonstrated that ASR improvement can be achieved in the presence of dynamic background conditions [27]. This setup was applied to this task and extended by the use of asynchronous Noise Adaptive Training (aNAT) [33, 27]. Firstly, a global aCMLR transformation with 8 parallel paths was trained on the entire training data to characterize the most common background conditions in these data."}, {"heading": "6. MULTI\u2013GENRE LANGUAGE MODELLING", "text": "In fact, most people who are able to move are able to move, to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move, to move, to move, to move, to move, to move and to move, to move, to move, to move and to move, to move, to move and to move."}, {"heading": "7. SYSTEM DESCRIPTION", "text": "The final system processing, as submitted for the MGB challenge, followed the diagram in Figure 1. Each node in the diagram was implemented as a composition of individual modules, each of which performed specific calculations on the speech data. The input ion was divided into speech segments using a DNN segmentator, based on the SNS2 strategy, as in Section 4. These segments were then decoded by an initial, non-customized hybrid ASR system: ASR-P1, trained on TRN1. Segmentation was then used using trust measures in the ASR output as in Section 4. After re-segmentation, the loudspeaker cluster was performed on the basis of a Bayean information criterion (BIC) [36] to assign each language segment to a given loudspeaker. From here, three different decoding passes were used in the ASR output: ASR-P2-1, ASR-2-PASR-2, and PASR-2-PASR-2-PASR-2."}, {"heading": "7.1. System implementation", "text": "The implementation of the system is based on the Resource Optimisation Toolkit (ROTK), developed by the University of Sheffield team and first presented in [25]. ROTK allows the formulation of functional modules that can be executed asynchronously using a computational grid infrastructure. Systems are defined as modules connected by directed links that transfer data of certain types. This is informally shown in a diagram in Figure 1; the modules actually used are more specific. The system uses metadata to organize how data is efficiently processed in parallel by the diagram. Each module can divide its own tasks into several subtasks based on data, which can then be processed in parallel."}, {"heading": "8. RESULTS", "text": "The results of all intermediate rounds and the final results are presented in Table 10. Since the results leading to the development of the proposed system have already been presented and discussed throughout the essay, this section reviews only the final results achieved by the complete system on the stage of development. In assessing the results per genre, the results vary considerably from news broadcasts with 13.2% WHO to comedy broadcasts with 40.9% WHO. This highlights the considerable influence of the acoustic variability that exists in television broadcasts. In terms of profit, the biggest improvement in children's broadcasts is from the initial unadjusted system, 36.5%, to the final result, 27.7%. This shows how the various techniques proposed to compensate for variability functioned complementarily in one of the most challenging conditions, i.e. where children and adults can appear on the same show and large amounts of music and other backgrounds."}, {"heading": "9. CONCLUSION", "text": "The final result, 27.5% WHO, reflects the complexity of the task, especially in the most challenging genres such as comedy or drama shows. It is important to note that these results are achieved without the availability of high-quality training data normally available for other related evaluation campaigns. The proposed system uses the complementarity of DNN-HMM and DNN-GMM-HMM systems using different adaptation strategies. Several techniques have been proposed and evaluated. In terms of data selection techniques for acoustic model training, the results have shown that the addition of higher-quality data can lead to improvements in both hybrid and bottleneck models. Refinement of automatic language segmentation through the output of an ASR level is a major contribution to the adaptation of this system, with the results showing that this higher-quality data can bring improvements in both hybrid and bottleneck models."}, {"heading": "10. ACKNOWLEDGEMENTS AND DATA", "text": "We would also like to thank our partners in the NST programme at the Universities of Cambridge and Edinburgh for the many discussions that have helped us greatly in the development of this system. Audio and subtitle data used in these experiments were distributed under a licence with the BBC as part of the MGB Challenge (www.mgb-challenge.org) and the system output and results for the presented system are also available to participants as part of the Challenge results."}, {"heading": "11. REFERENCES", "text": "In this context, it should be noted that the two are two people who are able to position themselves in public, namely people who are able to position themselves in public and people who are able to position themselves in public."}], "references": [{"title": "Broadcast news transcription using HTK", "author": ["P.C. Woodland", "M.J.F. Gales", "D. Pye", "S.J. Young"], "venue": "Proceedings of  the International Conference on Acoustics, Speech and Signal Processing (ICASSP), Munich, Germany, 1997, pp. 719\u2013722.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "The LIMSI broadcast news transcription system", "author": ["J.L. Gauvain", "L. Lamel", "G. Adda"], "venue": "Speech Communication, vol. 37, no. 1\u20132, pp. 89\u2013108, 2002.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Progress in the CU- HTK broadcast news transcription system", "author": ["M.J.F. Gales", "D.Y. Kim", "P.C. Woodland", "H.Y. Chan", "D. Mrva", "R. Sinha", "S.E. Tranter"], "venue": "IEEE Trans. on Audio, Speech and Language Processing, vol. 14, no. 5, pp. 1513\u20131525, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Automatic transcription of multi\u2013genre media archives", "author": ["P. Lanchantin", "P. Bell", "M. Gales", "T. Hain", "X. Liu", "Y. Long", "J. Quinnell", "S. Renals", "O. Saz", "M. Seigel"], "venue": "Proceedings of First Workshop on Speech, Language and Audio in Multimedia, Marseille, France, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "1995 Hub\u20134 dry run broadcast materials benchmark test", "author": ["D. Pallett", "J. Fiscus", "J. Garofalo", "M. Przybocki"], "venue": "Proceedings of 1996 DARPA Speech Recognition Workshop, 1996.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "The TDT-2 text and speech corpus", "author": ["C. Cieri", "D. Graff", "M. Liberman", "N. Martey", "S. Strassel"], "venue": "Proceedings of the 1999 DARPA Broadcast News Workshop, Herndon, VA, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Corpus description of the ES- TER evaluation campaign for the rich transcription of french broadcast news", "author": ["S. Galliano", "E. Geoffrois", "G. Gravier", "J.F. Bonastre", "D. Mostefa", "K. Choukri"], "venue": "Proceedings of Language Resources and Evaluation Conference, Genoa, Italy, 2006, pp. 139\u2013142.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Speaker diarization of broadcast news in albayzin 2010 evaluation campaign", "author": ["M. Zelenak", "H. Schulz", "J. Hernando"], "venue": "EURASIP Journal on Audio, Speech and Music Processing, vol. 19, pp. 1\u20139, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "The MGB challenge: Evaluating multi\u2013genre broadcast media transcription", "author": ["P. Bell", "M.J.F. Gales", "T. Hain", "J. Kilgour", "P. Lanchantin", "X. Liu", "A. McParland", "S. Renals", "O. Saz", "M. Webster", "P.C. Woodland"], "venue": "Proceedings of IEEE workshop on Automatic Speech Recognition and Understanding, Scottsdale, AZ, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "The Kaldi Speech Recognition Toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "G. Ondrej", "G. Nagendra", "M. Hanneman", "P. Motlicek", "Q. Yanmin", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "Proceedings of IEEE workshop on Automatic Speech Recognition and Understanding, Big Island, HA, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Boosted MMI for model and feature space discriminative training", "author": ["D. Povey", "D. Kanevsky", "B. Kingsbury", "B. Ramabhadran", "G. Saon", "K. Visweswariah"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), Las Vegas, NV, 2008, pp. 4057\u20134060.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Scalable minimum Bayes risk training of deep neural network acoustic models using distributed Hessian-free optimization", "author": ["B. Kingsbury", "T.N. Sainath", "H. Soltau"], "venue": "Proceeding of ISCA Interspeech, Portland, OR, 2012, pp. 10\u201313.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Hypothesis Spaces For Minimum Bayes Risk Training In Large Vocabulary Speech Recognition", "author": ["Matthew Gibson", "Thomas Hain"], "venue": "Proceedings of ISCA Interspeech, Pittsburgh, PA, 2006, pp. 2406\u20132409.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Parallel training of neural networks for speech recognition", "author": ["K. Vesely", "L. Burget", "F. Grezl"], "venue": "Proceedings of ISCA Interspeech, Makuhari, Japan, 2010, pp. 2934\u20132937.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "On generating combilex pronunciations via morphological analysis", "author": ["K. Richmond", "R. Clark", "S. Fitt"], "venue": "Proceedings of ISCA Interspeech, Makuhari, Japan, 2010, pp. 1974\u20131977.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "WSFT\u2013based grapheme\u2013to\u2013phoneme conversion: Open source tools for alignment, model\u2013building and decoding", "author": ["J.R. Novak", "N. Minematsu", "K. Hirose"], "venue": "Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, San Sebasti\u00e1n, Spain, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Implicit modelling of pronunciation variation in automatic speech recognition", "author": ["T. Hain"], "venue": "Speech Communication, vol. 46, pp. 171\u2013188, 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "SRILM \u2013 An Extensible Language Modeling Toolkit", "author": ["A. Stolcke"], "venue": "Proceedings of International Conference on Spoken Language Processing (ICSLP), Denver, CO, 2002, pp. 901\u2013904.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Improving lightly supervised training for broadcast transcriptions", "author": ["Y. Long", "M.J.F. Gales", "P. Lanchantin", "X. Liu", "M.S. Seigel", "P.C. Woodland"], "venue": "Proceedings of ISCA Interspeech, Lyon, France, 2013, pp. 2187\u20132191.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi\u2013supervised DNN training in meeting recognition", "author": ["P. Zhang", "Y. Liu", "T. Hain"], "venue": "Proceedings of IEEE Workshop on Spoken Language Technologies, South Lake Tahoe, CA, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "G. Dahl", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, November 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "The segmentation of multichannel meeting recordings for automatic speech recognition", "author": ["J. Dines", "J. Vepa", "T. Hain"], "venue": "Proceedings of ISCA Interspeech, Pittsburgh, PA, 2006, pp. 1213\u20131216.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Transcribing meetings with the AMIDA systems", "author": ["T. Hain", "L. Burget", "J. Dines", "P.N. Garner", "F. Grezl", "A.E. Hannani", "M. Huijbregts", "M. Karafiat", "M. Lincoln", "V. Wan"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 2, pp. 486\u2013498, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech activity detection on youtube using deep neural networks", "author": ["N. Ryant", "M. Liberman"], "venue": "Proceedings of ISCA Interspeech, Lyon, France, 2013, pp. 728\u2013731.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Asynchronous factorisation of speaker and background with feature transforms in speech recognition", "author": ["O. Saz", "T. Hain"], "venue": "Proceedings of ISCA Interspeech, Lyon, France, 2013, pp. 1238\u20131242.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum likelihood linear transformations for HMM-based speech recognition", "author": ["M.J.F. Gales"], "venue": "Computer Speech & Language, vol. 12, no. 2, pp. 75 \u2013 98, 1998.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "I\u2013vector estimation using informative priors for adaptation of deep neural networks", "author": ["P. Karanasou", "M. Gales", "P. Woodland"], "venue": "Proceedings of ISCA Interspeech, Dresden, Germany, 2015, pp. 2872\u20132876.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "An investigation into speaker informaed DNN front\u2013end for LVCSR", "author": ["Y. Liu", "P. Karanasou", "T. Hain"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brisbane, Australia, 2015, pp. 4300\u20134304.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised domain discovery using latent Dirichlet allocation for acoustic modelling in speech recognition", "author": ["M. Doulaty", "O. Saz", "T. Hain"], "venue": "Proceedings of ISCA Interspeech, Dresden, Germany, 2015, pp. 3640\u20133644.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Mean and variance adaptation within the MLLR framework", "author": ["M.J.F. Gales", "P.C. Woodland"], "venue": "Computer, Speech and Language, vol. 10, no. 4, pp. 249\u2013264, 1996.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "Noise Adaptive Training for Robust Automatic Speech Recognition", "author": ["O. Kalinli", "M.L. seltzer", "J. Droppo", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 18, no. 8, pp. 1889\u20131901, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1889}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J. Cernocky", "S. Khundapur"], "venue": "Proceedings of ISCA Interspeech, Makuhari, Japan, 2010, pp. 1045\u20131048.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrent neural network language model adaptation for multi-genre broadcast speech recognition", "author": ["X. Chen", "T. Tan", "X. Liu", "P. Lanchantin", "M. Wan", "M.J.F. Gales", "P.C. Woodland"], "venue": "Proceedings of ISCA Interspeech, Dresden, Germany, 2015, pp. 3511\u20133515.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Clustering via the Bayesian information criterion with applications in speech recognition", "author": ["S. Shaobing Chen", "P.S. Gopalakrishnan"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Seattle, WA, 1998, pp. 645\u2013648.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1998}, {"title": "A post\u2013processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER)", "author": ["J. Fiscus"], "venue": "Proceedings of IEEE workshop on Automatic Speech Recognition and Understanding, Santa Barbara, CA, 1997, pp. 347\u2013 354.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "From the point of view of Automatic Speech Recognition (ASR), work on transcription of broadcast news has achieved significant reduction in error rates since the early works in the 1990s [1, 2], with word error rates falling below 10% for traditional broadcast new programmes [3].", "startOffset": 187, "endOffset": 193}, {"referenceID": 1, "context": "From the point of view of Automatic Speech Recognition (ASR), work on transcription of broadcast news has achieved significant reduction in error rates since the early works in the 1990s [1, 2], with word error rates falling below 10% for traditional broadcast new programmes [3].", "startOffset": 187, "endOffset": 193}, {"referenceID": 2, "context": "From the point of view of Automatic Speech Recognition (ASR), work on transcription of broadcast news has achieved significant reduction in error rates since the early works in the 1990s [1, 2], with word error rates falling below 10% for traditional broadcast new programmes [3].", "startOffset": 276, "endOffset": 279}, {"referenceID": 3, "context": "The transcription of multigenre data is a complex task due to the large amounts of variability arising from multiple, diverse speakers, the variety of acoustic and recording conditions and the lexical and linguistic diversity of the topics covered [4].", "startOffset": 248, "endOffset": 251}, {"referenceID": 4, "context": "Evaluation series such as the NIST-organised Hub4 tasks [6] helped start the earlier efforts in broadcast news transcriptions in English, while the Topic Detection and Tracking (TDT) campaign [7] expanded this work to other tasks related to broadcast news.", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "Evaluation series such as the NIST-organised Hub4 tasks [6] helped start the earlier efforts in broadcast news transcriptions in English, while the Topic Detection and Tracking (TDT) campaign [7] expanded this work to other tasks related to broadcast news.", "startOffset": 192, "endOffset": 195}, {"referenceID": 6, "context": "More recently, the Ester campaigns [8] have created increased interest in the transcription of French broadcast news and the Albayzin campaigns [9] have pushed the efforts in audio processing of Spanish broadcast news.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "More recently, the Ester campaigns [8] have created increased interest in the transcription of French broadcast news and the Albayzin campaigns [9] have pushed the efforts in audio processing of Spanish broadcast news.", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": "Following these efforts, the Multi-Genre Broadcast (MGB) challenge [10] aimed to take on several tasks of an increasing complexity in broadcast media.", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "A full description of this and the other tasks in the challenge can be found in [10], but a brief description of the task is given here.", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "The first types of systems used were Hybrid DNN-HMM systems, built using the Kaldi toolkit [11].", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "Features were transformed using a boosted Maximum Mutual Information (bMMI) discriminative transformation [12], unless otherwise stated.", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "State-level Minimum Bayes Risk (sMBR) [13, 14] as target functions, unless otherwise mentioned, and Stochastic Gradient Descent (SGD) was used as the optimisation method.", "startOffset": 38, "endOffset": 46}, {"referenceID": 12, "context": "State-level Minimum Bayes Risk (sMBR) [13, 14] as target functions, unless otherwise mentioned, and Stochastic Gradient Descent (SGD) was used as the optimisation method.", "startOffset": 38, "endOffset": 46}, {"referenceID": 13, "context": "The second system types used are so-called Bottleneck DNNGMM-HMM systems built using the TNet toolkit [15] for DNN training and the HTK toolkit [16] for Gaussian Mixture Model (GMM) and Hidden Markov Model (HMM) training and decoding.", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "Pronunciations were obtained using the Combilex pronunciation dictionary[17], which was provided to the challenge participants.", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "When a certain word was not contained in the lexicon, automatically generated pronunciations were obtained using the Phonetisaurus toolkit [18].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "These pronunciations were expanded to incorporate pronunciation probabilities, learnt from the alignment of the AM training data [19].", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "LM training was performed with the SRILM toolkit [20].", "startOffset": 49, "endOffset": 53}, {"referenceID": 8, "context": "An aligned version of the subtitles was provided where the time stamps of the subtitles had been corrected in a lightly supervised manner [10, 21].", "startOffset": 138, "endOffset": 146}, {"referenceID": 18, "context": "An aligned version of the subtitles was provided where the time stamps of the subtitles had been corrected in a lightly supervised manner [10, 21].", "startOffset": 138, "endOffset": 146}, {"referenceID": 8, "context": "An initial selection strategy was based on selecting segments for training based on their Word Matching Error Rate (WMER), a by-product of the semi-supervised alignment process that measures how similar the text in the subtitle matched the output of a lightly supervised ASR system for that segment [10, 21].", "startOffset": 299, "endOffset": 307}, {"referenceID": 18, "context": "An initial selection strategy was based on selecting segments for training based on their Word Matching Error Rate (WMER), a by-product of the semi-supervised alignment process that measures how similar the text in the subtitle matched the output of a lightly supervised ASR system for that segment [10, 21].", "startOffset": 299, "endOffset": 307}, {"referenceID": 19, "context": "The scores were obtained from the posterior probabilities given by a 4-layer DNN trained on the initial selection of data whose targets were 144 monophone states [22].", "startOffset": 162, "endOffset": 166}, {"referenceID": 20, "context": "1, in this case using Cross-Entropy (CE) training [23].", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "NNs have been used extensively for speech segmentation of meetings [24, 25] and naturally DNNs are equally useful for this task [26].", "startOffset": 67, "endOffset": 75}, {"referenceID": 22, "context": "NNs have been used extensively for speech segmentation of meetings [24, 25] and naturally DNNs are equally useful for this task [26].", "startOffset": 67, "endOffset": 75}, {"referenceID": 23, "context": "NNs have been used extensively for speech segmentation of meetings [24, 25] and naturally DNNs are equally useful for this task [26].", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "The first aimed to normalise the background variability in the input to DNNs for hybrid systems, while the second one aimed to use asynchronous Constrained Maximum Likelihood Linear Regression (aCMLLR) transformations [27] for the compensation of dynamic background noises in bottleneck systems.", "startOffset": 218, "endOffset": 222}, {"referenceID": 25, "context": "transforming the input features via feature MLLR (fMLLR) transformations [28] or by using additional input features representing some characteristic of the speaker, like i-Vectors [29, 30].", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "transforming the input features via feature MLLR (fMLLR) transformations [28] or by using additional input features representing some characteristic of the speaker, like i-Vectors [29, 30].", "startOffset": 180, "endOffset": 188}, {"referenceID": 27, "context": "transforming the input features via feature MLLR (fMLLR) transformations [28] or by using additional input features representing some characteristic of the speaker, like i-Vectors [29, 30].", "startOffset": 180, "endOffset": 188}, {"referenceID": 28, "context": "In [31], it was shown that LDA is a suitable model for structuring acoustic data from unknown origin, into unsupervised categories, that could be used to provide domain adaptation in ASR.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "In this work, 64 hidden acoustic domains were found in the acoustic model training data using the LDA model following the procedure in [31]; these domains were found in a unsupervised manner and internally structured the different acoustic conditions of the data.", "startOffset": 135, "endOffset": 139}, {"referenceID": 29, "context": "One of the advantages of tandem (DNN-GMM-HMM) systems is that techniques for adaptation such as Maximum A Posteriori (MAP) or MLLR [32] can be employed.", "startOffset": 131, "endOffset": 135}, {"referenceID": 24, "context": "In our previous works, a new HMM topology for asynchronous adaptation of GMM-HMM systems was proposed and shown to produce ASR improvement in the presence of dynamic background conditions [27].", "startOffset": 188, "endOffset": 192}, {"referenceID": 30, "context": "This setup was applied to this task and expanded through the use of asynchronous Noise Adaptive Training (aNAT) [33, 27].", "startOffset": 112, "endOffset": 120}, {"referenceID": 24, "context": "This setup was applied to this task and expanded through the use of asynchronous Noise Adaptive Training (aNAT) [33, 27].", "startOffset": 112, "endOffset": 120}, {"referenceID": 31, "context": "was based on a Recurrent Neural Network (RNN) LM [34], initially trained on the full LM1 and LM2 training data.", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "This is consistent with the experiments reported on the same BBC data in [35].", "startOffset": 73, "endOffset": 77}, {"referenceID": 32, "context": "The main difference, however is that in [35], instead of LM1 as background language model, another corpus of 1 billion words was used for language modelling, and different topic models including LDA, were used to classify the text into a set of different genres.", "startOffset": 40, "endOffset": 44}, {"referenceID": 33, "context": "After resegmentation, speaker clustering based on Bayesian Information Criterion (BIC) [36] was performed to assign each speech segment to a given speaker.", "startOffset": 87, "endOffset": 91}, {"referenceID": 34, "context": "The output of these three passes was finally combined via a Recognition Output Voting Error Reduction (ROVER) [37] procedure.", "startOffset": 110, "endOffset": 114}, {"referenceID": 22, "context": "The implementation of the system is based on the Resource Optimisation Toolkit (ROTK), which is developed by the team at the University of Sheffield and was presented initially in [25].", "startOffset": 180, "endOffset": 184}], "year": 2015, "abstractText": "We describe the University of Sheffield system for participation in the 2015 Multi\u2013Genre Broadcast (MGB) challenge task of transcribing multi\u2013genre broadcast shows. Transcription was one of four tasks proposed in the MGB challenge, with the aim of advancing the state of the art of automatic speech recognition, speaker diarisation and automatic alignment of subtitles for broadcast media. Four topics are investigated in this work: Data selection techniques for training with unreliable data, automatic speech segmentation of broadcast media shows, acoustic modelling and adaptation in highly variable environments, and language modelling of multi\u2013 genre shows. The final system operates in multiple passes, using an initial unadapted decoding stage to refine segmentation, followed by three adapted passes: a hybrid DNN pass with input features normalised by speaker\u2013based cepstral normalisation, another hybrid stage with input features normalised by speaker feature\u2013MLLR transformations, and finally a bottleneck\u2013based tandem stage with noise and speaker factorisation. The combination of these three system outputs provides a final error rate of 27.5% on the official development set, consisting of 47 multi\u2013genre shows.", "creator": "LaTeX with hyperref package"}}}