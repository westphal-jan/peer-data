{"id": "1512.06658", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2015", "title": "Deep Learning for Surface Material Classification Using Haptic And Visual Information", "abstract": "When a user scratches a hand-held rigid tool across an object surface, an acceleration signal can be captured, which carries relevant information about the surface. More importantly, such a haptic signal is complementary to the visual appearance of the surface, which suggests the combination of both modalities for the recognition of the surface material. In this paper, we present a novel deep learning method dealing with the surface material classification problem based on a Fully Convolutional Network (FCN), which takes as input the aforementioned acceleration signal and a corresponding image of the surface texture. Compared to previous surface material classification solutions, which rely on a careful design of hand-crafted domain-specific features, our method automatically extracts discriminative features utilizing the advanced deep learning methodologies. Experiments performed on the TUM surface material database demonstrate that our method achieves state-of-the-art classification accuracy robustly and efficiently.", "histories": [["v1", "Mon, 21 Dec 2015 15:22:16 GMT  (8593kb,D)", "http://arxiv.org/abs/1512.06658v1", "8 pages, under review as a paper at Transactions on Multimedia"], ["v2", "Sun, 1 May 2016 07:00:56 GMT  (14540kb,D)", "http://arxiv.org/abs/1512.06658v2", "8 pages, under review as a paper at Transactions on Multimedia"]], "COMMENTS": "8 pages, under review as a paper at Transactions on Multimedia", "reviews": [], "SUBJECTS": "cs.RO cs.CV cs.LG", "authors": ["haitian zheng", "lu fang", "mengqi ji", "matti strese", "yigitcan ozer", "eckehard steinbach"], "accepted": false, "id": "1512.06658"}, "pdf": {"name": "1512.06658.pdf", "metadata": {"source": "CRF", "title": "DEEP LEARNING FOR SURFACE MATERIAL CLASSIFICATION USING HAPTIC AND VISUAL INFORMATION", "authors": ["Haitian Zheng", "Lu Fang", "Mengqi Ji", "Matti Strese", "Yigitcan \u00d6zer", "Eckehard Steinbach"], "emails": ["zhenght@mail.ustc.edu.cn", "mji}@ust.hk", "eckehard.steinbach}@tum.de"], "sections": [{"heading": null, "text": "In fact, it is the case that most of the people who have decided to take such a step do not know what to do, but what to do. \"I do not believe that they should do it,\" he says. \"But I believe that they must do it.\" \"I do not believe that they must do it.\" \"I do not believe,\" he says, \"I do not believe that they must do it.\" \"I do not believe.\" \"\" No. \"\" No. \"\" No. \"\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\""}, {"heading": "II. RELATED WORK", "text": "However, the main components of a CNN can be summarized briefly as follows: \u2022 Convolution layer extract feature maps from the input, applying successive convolution operations between the input layers and trainable cores, followed by a non-linear activation function. \u2022 Pooling layers usually follow convolution layers, which aim to reduce both the dimensionality and translation sensitivity of the input characteristics. \u2022 Fully connected layers are commonly used in the final stage of a CNN. \u2022 Flexible layers for detecting spectrograms lead to temporary frequency inventory. \u2022 Fully connected layers are commonly used in the final stage of a CNN to provide more flexible functionality."}, {"heading": "III. PROPOSED CNN SCHEME WITH HYBRID INPUT", "text": "In this context, it is important that people in the region are not only interested in themselves, but also in themselves, for themselves, for others, for themselves, for others, for others, for themselves, for others, for themselves, for themselves, for themselves, for others, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves, for themselves,"}, {"heading": "IV. EXPERIMENTAL RESULTS AND DISCUSSION", "text": "In fact, most of them are able to survive on their own."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "We introduce a method for classifying surface materials using fully revolutionary neural networks. To predict individual haptic input or image input, we apply FCN with a maximum selection frame. We then design a fusion network that deals with both haptic and image input. Experiments with TUM's haptic texture database show that our proposed system can achieve a competitive classification accuracy compared to existing systems. In our future work, we aim to extend the current \"FCN + maximum selection and hybrid classification schemes\" to other types of input (image input, haptic acceleration signal, haptic speed signal, sound signal) in order to further improve the flexibility and robustness of our system."}, {"heading": "VI. REFERENCES", "text": "[1] L. Liu, and P. Fieguth, Texture classification from random features, in Pattern Analysis and Machine Intelligence, IEEE Transactions on 34, no. 3 (2012): 574-586 M. [2] T. Ojala, P. Matti, and M. Topi, Multiresolution grey scale and rotation invariant texture classification with local binary patterns, in Pattern Analysis and Machine Intelligence, IEEE Transactions on 24, no. (2002): 971- 987. [3] Arvis, Vincent, Christophe Debain, Michel Berducat, and Albert Benassi, Generalization of the cooccurrence matrix for color images: application to color texture classification, in Image Analysis and Stereology 23, no. 1 (2011): 63-72. [4] M. Varma and A. Zisserman, Texture Classification: Are Filter Banks Necessary, in Proc."}], "references": [{"title": "Texture classification from random features", "author": ["L. Liu", "P. Fieguth"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 34, no. 3 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiresolution grayscale and rotation invariant texture classification with local binary patterns", "author": ["T. Ojala", "P. Matti", "M. Topi"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 24, no. 7 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Generalization of the cooccurrence matrix for colour images: application to colour texture classification, in Image Analysis and Stereology", "author": ["Arvis", "Vincent", "Christophe Debain", "Michel Berducat", "Albert Benassi"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Texture Classification: Are Filter Banks Necessary", "author": ["M. Varma", "A. Zisserman"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 691-698", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Creating realistic virtual textures from contact acceleration data", "author": ["Romano", "Joseph M", "Kuchenbecker", "Katherine J"], "venue": "IEEE Transactions on Haptics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Should Haptic Texture Vibrations Respond to User Force and Speed in IEEE World", "author": ["Romano", "Joseph M", "Kuchenbecker", "Katherine J"], "venue": "Haptics Conference,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Dimensional reduction of high-frequency accelerations for haptic rendering", "author": ["N. Landin", "J.M. Romano", "W. McMahan", "K.J. Kuchenbecker"], "venue": "Haptics: Generating and Perceiving Tangible Sensations, Springer, pp. 79- 86", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Strese and E.Steinbach, Preprocessing-free Surface Material Classification Using Convolutional Neural Networks Pretrained by Sparse Autoencoder (ACNN), in Proc. of Machine Learning  for Signal Processing", "author": ["M. Ji", "L. Fang", "M.H. Zheng"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Surface Classification Using Acceleration Signals Recorded During Human Free hand Movement", "author": ["M. Strese", "C. Schuwerk", "E. Steinbach"], "venue": "in Proc. of IEEE World Haptics Conference,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Methods for Robotic Tool-Mediated Haptic Surface Recognition, in IEEE Haptics Symposium (HAPTICS)", "author": ["J.M. Romano", "K.J. Kuchenbecker"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Bayesian exploration for intelligent identification of textures, in Frontiers in neurorobotics", "author": ["J.A. Fishel", "G.E. Loeb"], "venue": "vol. 6,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks, in Advances in neural information processing systems (NIPS)", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint 1409.1556 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint 1409.4842 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Dropout training as adaptive regularization, in Advances in Neural Information", "author": ["S. Wager", "S. Wang", "P. Liang"], "venue": "Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Texture classification using convolutional neural network", "author": ["F.H.C. Tivive", "A. Bouzerdoum"], "venue": "TENCON, 2006 IEEE Region 10 Conference, pp. 1-4. IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "An Analysis of Deep Neural Networks for Texture Classification", "author": ["L.G. Hafemann"], "venue": "M.Sc. Dissertation, Retrieved from http://dspace.c3sl.ufpr.br:8080/dspace/handle/1884/36967", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for speech recognition, in Audio, Speech, and Language Processing", "author": ["Abdel-Hamid", "Ossama", "Abdel-rahman Mohamed", "Hui Jiang", "Li Deng", "Gerald Penn", "Dong Yu"], "venue": "IEEE/ACM Transactions on 22, no", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition", "author": ["O. Abdel-Hamid", "A. Mohamed", "J. Hui", "G. Penn"], "venue": "Acous-  tics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pp. 4277-4280. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep Convolutional Neural Networks for LVCSR", "author": ["T.N. Sainath", "A. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 8614 - 8618. IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional Neural Networks for Distant Speech Recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "Hands-free Speech Communication and Microphone Arrays (HSCMA), 2014 4th Joint Workshop on, pp. 172- 176. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploring Convolutional Neural Network Structures and Optimization Techniques for Speech Recognition", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu"], "venue": "INTERSPEECH", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Music Genre Classification Using Convolutional Neural Network, Retrieved from http://www.terasoft.com.tw/conf/ismir2014/ LBD%5CLBD17.pdf", "author": ["Q. Kong", "X. Feng", "Y. Li"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Improved musical onset detection with Convolutional Neural Networks, in Acoustics", "author": ["J. Schluter", "S.Bock"], "venue": "Speech and Signal Processing (ICASSP),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Improved musical onset detection with convolutional neural networks", "author": ["J. Schluter", "B. Sebastian"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pp. 6979-6983. IEEE", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "B. Leon", "L. Ivan", "S. Sivic"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 1717-1724. IEEE", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "DeCAF: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv preprint 1310.1531 ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision (ECCV)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long"], "venue": "Girshick and et. al., Caffe: Convolutional architecture for fast feature embedding, in Proceedings of the ACM International Conference on Multimedia, ACM, pp. 675- 678", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "There has been increasing interest to recognize surface materials using robots [5][9][11][12] and to recreate the haptic feel of real surfaces.", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "There has been increasing interest to recognize surface materials using robots [5][9][11][12] and to recreate the haptic feel of real surfaces.", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "There has been increasing interest to recognize surface materials using robots [5][9][11][12] and to recreate the haptic feel of real surfaces.", "startOffset": 89, "endOffset": 93}, {"referenceID": 7, "context": "Before surface classification using acceleration data (captured while interacting with the surface) emerged [5][9][10][11][12], a significant number of previous works have focused on surface texture classification using images of the surface.", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "Before surface classification using acceleration data (captured while interacting with the surface) emerged [5][9][10][11][12], a significant number of previous works have focused on surface texture classification using images of the surface.", "startOffset": 114, "endOffset": 118}, {"referenceID": 9, "context": "Before surface classification using acceleration data (captured while interacting with the surface) emerged [5][9][10][11][12], a significant number of previous works have focused on surface texture classification using images of the surface.", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "Before surface classification using acceleration data (captured while interacting with the surface) emerged [5][9][10][11][12], a significant number of previous works have focused on surface texture classification using images of the surface.", "startOffset": 122, "endOffset": 126}, {"referenceID": 1, "context": "These approaches mainly rely on handcrafted image features including LBP features [2], filter Fig.", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "bank features [1][4], co-occurrence matrix based features [3] etc, combined with appropriate machine learning tools to distinguish the different texture types.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "bank features [1][4], co-occurrence matrix based features [3] etc, combined with appropriate machine learning tools to distinguish the different texture types.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "bank features [1][4], co-occurrence matrix based features [3] etc, combined with appropriate machine learning tools to distinguish the different texture types.", "startOffset": 58, "endOffset": 61}, {"referenceID": 17, "context": "In the context of surface classification, [20][21] aim to classify texture image patches by training CNNs, while our previous work [9] proposes an auto-encoder pre-trained CNN for classifying surface materials using acceleration segments1.", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "In the context of surface classification, [20][21] aim to classify texture image patches by training CNNs, while our previous work [9] proposes an auto-encoder pre-trained CNN for classifying surface materials using acceleration segments1.", "startOffset": 46, "endOffset": 50}, {"referenceID": 7, "context": "In the context of surface classification, [20][21] aim to classify texture image patches by training CNNs, while our previous work [9] proposes an auto-encoder pre-trained CNN for classifying surface materials using acceleration segments1.", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "1[9], a preliminary version of this work, has appeared at MLSP 2015.", "startOffset": 1, "endOffset": 4}, {"referenceID": 14, "context": "FCN [16] is a special type of convolutional neural network which replaces fully connected layers with convolutional layers with 1 \u00d7 1 convolution kernels.", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "Different from previous approaches in adapting FCN, we propose an efficient and systematic FCN scheme for recognizing hybrid data \u2013 haptic and image: the FCN for the haptic data is trained using concepts developed for speech recognition, as the used acceleration signals share similar characteristics with speech data [5] and [10]; the FCN for the image-based surface material recognition is trained by fine-tuning the network weights from [13], inspired by transfer learning [30].", "startOffset": 326, "endOffset": 330}, {"referenceID": 11, "context": "Different from previous approaches in adapting FCN, we propose an efficient and systematic FCN scheme for recognizing hybrid data \u2013 haptic and image: the FCN for the haptic data is trained using concepts developed for speech recognition, as the used acceleration signals share similar characteristics with speech data [5] and [10]; the FCN for the image-based surface material recognition is trained by fine-tuning the network weights from [13], inspired by transfer learning [30].", "startOffset": 440, "endOffset": 444}, {"referenceID": 27, "context": "Different from previous approaches in adapting FCN, we propose an efficient and systematic FCN scheme for recognizing hybrid data \u2013 haptic and image: the FCN for the haptic data is trained using concepts developed for speech recognition, as the used acceleration signals share similar characteristics with speech data [5] and [10]; the FCN for the image-based surface material recognition is trained by fine-tuning the network weights from [13], inspired by transfer learning [30].", "startOffset": 476, "endOffset": 480}, {"referenceID": 15, "context": "The activation function of the CNN can be chosen among sigmoid function, tanh function, and rectified linear (ReLu) function [17], among which, ReLu becomes more and more popular due to its efficiency for training and effectiveness for improving the classification performance.", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "Additionally, dropout regularization [18] is commonly used in the fullyconnected layer, which significantly reduces co-adaptation between features, and hence prevents over-fitting and boosts the classification performance significantly.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": ", ImageNet), CNN methods [13] [14] [15] have taken over the lead in large scale visual recognition challenges (ILSVRC) since 2012.", "startOffset": 25, "endOffset": 29}, {"referenceID": 12, "context": ", ImageNet), CNN methods [13] [14] [15] have taken over the lead in large scale visual recognition challenges (ILSVRC) since 2012.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": ", ImageNet), CNN methods [13] [14] [15] have taken over the lead in large scale visual recognition challenges (ILSVRC) since 2012.", "startOffset": 35, "endOffset": 39}, {"referenceID": 27, "context": "CNNs, however, still successfully show their power \u2013 studies on transfer learning [30] [31] [32] show that the trained CNN model for one specific vision task usually learns a good representation of natural images, which works for other visual recognition tasks as well.", "startOffset": 82, "endOffset": 86}, {"referenceID": 28, "context": "CNNs, however, still successfully show their power \u2013 studies on transfer learning [30] [31] [32] show that the trained CNN model for one specific vision task usually learns a good representation of natural images, which works for other visual recognition tasks as well.", "startOffset": 87, "endOffset": 91}, {"referenceID": 29, "context": "CNNs, however, still successfully show their power \u2013 studies on transfer learning [30] [31] [32] show that the trained CNN model for one specific vision task usually learns a good representation of natural images, which works for other visual recognition tasks as well.", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "Recent works [22] [23] [24] [25] [26] show that CNNs notably outperform fully-connected deep neural network (DNN).", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "Recent works [22] [23] [24] [25] [26] show that CNNs notably outperform fully-connected deep neural network (DNN).", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "Recent works [22] [23] [24] [25] [26] show that CNNs notably outperform fully-connected deep neural network (DNN).", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "Recent works [22] [23] [24] [25] [26] show that CNNs notably outperform fully-connected deep neural network (DNN).", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "Recent works [22] [23] [24] [25] [26] show that CNNs notably outperform fully-connected deep neural network (DNN).", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "The superior performance is attributed to the property of temporal-frequency translation invariance inherent with CNN [24].", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "In addition to speech recognition, CNNs are applied to acoustic recognition tasks such as music genre classification [27] and music onset detection [28].", "startOffset": 117, "endOffset": 121}, {"referenceID": 25, "context": "In addition to speech recognition, CNNs are applied to acoustic recognition tasks such as music genre classification [27] and music onset detection [28].", "startOffset": 148, "endOffset": 152}, {"referenceID": 8, "context": "Motivated by the previous work on 1-D speech signal recognition, and evidence in [5][10] which show that the acceleration signals captured during the interaction with", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "noted in [16], both training and inference of FCN can be performed by standard neural network approaches, leading to an efficient and systematic scheme.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "Acceleration data trace, reproduced from [10].", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "These scan-time parameters strongly influence the nature of the recorded acceleration signals [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "However, a user study in [7] reveals that the exerted normal force and tangential velocity are not equally relevant.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "In our work, we use the haptic stylus from [10], which is a free-to-wield object with a stainless steel tooltip, shown in Fig.", "startOffset": 43, "endOffset": 47}, {"referenceID": 8, "context": "In [10], a three-axis LIS344ALH accelerometer (ST Electronics) with a range of \u00b16 g was applied to collect the raw acceleration data traces.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "All three axis were combined to one using DFT321 (see [8]).", "startOffset": 54, "endOffset": 57}, {"referenceID": 21, "context": "As demonstrated by many speech recognition works [24][25][26][27], converting a 1-D signal into the spectral domain is helpful for the CNN to achieve translation invariance between the temporal and frequency domains.", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "As demonstrated by many speech recognition works [24][25][26][27], converting a 1-D signal into the spectral domain is helpful for the CNN to achieve translation invariance between the temporal and frequency domains.", "startOffset": 53, "endOffset": 57}, {"referenceID": 23, "context": "As demonstrated by many speech recognition works [24][25][26][27], converting a 1-D signal into the spectral domain is helpful for the CNN to achieve translation invariance between the temporal and frequency domains.", "startOffset": 57, "endOffset": 61}, {"referenceID": 24, "context": "As demonstrated by many speech recognition works [24][25][26][27], converting a 1-D signal into the spectral domain is helpful for the CNN to achieve translation invariance between the temporal and frequency domains.", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "The structure of our VisualNet is motivated by the widely used CNN structure AlexNet proposed in [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 30, "context": "Our implementation is based on the Caffe deep learning framework [33], using a computer with 8GB RAM and Nvidia GPU GTX-860M.", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "23% Modified MFCC Decreasing + Naive Bayes[10] \u2013 89%", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "ACNN [9] 81.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "7% Micro- / Macro-Roughness, Hardness, 5 MFCC + Naive Bayes[10] \u2013 91%", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "Micro- / Macro-Roughness, Hardness, 5 MFCC, Friction + Naive Bayes[10] \u2013 95%", "startOffset": 66, "endOffset": 70}, {"referenceID": 31, "context": "The HapticNet is trained using the adaptive subgradient methods [34] with the spectrograms of the training set.", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "Specifically, we compare our results with several existing methods [5][10][9].", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "Specifically, we compare our results with several existing methods [5][10][9].", "startOffset": 74, "endOffset": 77}, {"referenceID": 8, "context": "The following work [10] carefully discusses variant features for representation the movement signal, and variant discriminative models are proposed for predictions.", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "[9] is the first work which uses the raw acceleration data to train the CNN and achieves competitive classification results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "VisualNet Texture images are firstly resized to be of halfsize, where the texture structure can be preserved by the 224\u00d7224 receptive field of AlexNet [13].", "startOffset": 151, "endOffset": 155}], "year": 2017, "abstractText": "When a user scratches a hand-held rigid tool across an object surface, an acceleration signal can be captured, which carries relevant information about the surface. More importantly, such a haptic signal is complementary to the visual appearance of the surface, which suggests the combination of both modalities for the recognition of the surface material. In this paper, we present a novel deep learning method dealing with the surface material classification problem based on a Fully Convolutional Network (FCN), which takes as input the aforementioned acceleration signal and a corresponding image of the surface texture. Compared to previous surface material classification solutions, which rely on a careful design of hand-crafted domain-specific features, our method automatically extracts discriminative features utilizing the advanced deep learning methodologies. Experiments performed on the TUM surface material database demonstrate that our method achieves state-of-the-art classification accuracy robustly and efficiently.", "creator": "LaTeX with hyperref package"}}}