{"id": "1512.06757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2015", "title": "GraphConnect: A Regularization Framework for Neural Networks", "abstract": "Deep neural networks have proved very successful in domains where large training sets are available, but when the number of training samples is small, their performance suffers from overfitting. Prior methods of reducing overfitting such as weight decay, Dropout and DropConnect are data-independent. This paper proposes a new method, GraphConnect, that is data-dependent, and is motivated by the observation that data of interest lie close to a manifold. The new method encourages the relationships between the learned decisions to resemble a graph representing the manifold structure. Essentially GraphConnect is designed to learn attributes that are present in data samples in contrast to weight decay, Dropout and DropConnect which are simply designed to make it more difficult to fit to random error or noise. Empirical Rademacher complexity is used to connect the generalization error of the neural network to spectral properties of the graph learned from the input data. This framework is used to show that GraphConnect is superior to weight decay. Experimental results on several benchmark datasets validate the theoretical analysis, and show that when the number of training samples is small, GraphConnect is able to significantly improve performance over weight decay.", "histories": [["v1", "Mon, 21 Dec 2015 18:42:45 GMT  (979kb,D)", "http://arxiv.org/abs/1512.06757v1", null], ["v2", "Wed, 27 Jan 2016 03:21:15 GMT  (0kb,I)", "http://arxiv.org/abs/1512.06757v2", "Theorems need more validation"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jiaji huang", "qiang qiu", "robert calderbank", "guillermo sapiro"], "accepted": false, "id": "1512.06757"}, "pdf": {"name": "1512.06757.pdf", "metadata": {"source": "CRF", "title": "GraphConnect: A Regularization Framework for Neural Networks", "authors": ["Jiaji Huang", "Qiang Qiu", "Robert Calderbank", "Guillermo Sapiro"], "emails": ["guillermo.sapiro}@duke.edu"], "sections": [{"heading": "1. Introduction", "text": "This year it is so far that it is only about half way to the next round."}, {"heading": "2. GraphConnect", "text": "We look at the L-path classification using a deep neural network. Faced with a date x, the network first learns a multidimensional trait g (x) and then applies a softmax classifier to generate a probability distribution across the L-classes. We use cross-entropy loss to measure the effectiveness of the learning machine, and then consider the deep network as a function from a date x to the corresponding loss \"(x). The average loss achieved on the training set {x1,. xN} is the empirical loss\" emp, \"given by\" emp = 1N. \"The expected loss\" is estimated by a large test set and is given by \"Ex [x). The difference\" \u2212 emp between the expected loss of the test data and the empirical loss of the data."}, {"heading": "2.1. Analysis: Regularizing a Linear Layer", "text": "The functions U, which appear in definition 1, are multidimensional inputs to scalars, i.e. to the use of Rademacher complexity, which we must elaborate with individual coordinate entries of multidimensional characteristics. (1) We look at the linear maps f (z) = v > z, where the linear weights v depend on a series of symmetrical edge weights. (1) We have now learned where symmetrical edge weights W determine the relations between the N input samples Z def = [z1). zN, which defines the function family F, is given by {v: 1N (1 \u2212 2). We have learned a graph where symmetrical edge weights W define the relations between N input samples Z def = [z1]. The quantity V, which defines the function family F, is given by {v: 1N (1 \u2212 2)."}, {"heading": "2.2. Analysis: Regularizing Multiple Layers", "text": "We expand our analysis to include the effects in the intermediate layers of the pooling and activation functions. We look at aK layer network, which is limited to a multidimensional attribute g (x), then to a single dimension to obtain a scalar g (x) level, which requires from x) = v > KsK -1 (\u00b7 s2) level (\u00b7 s2) level (\u00b7 s2) level (\u00b7 s2) level (\u00b7 s2 (\u00b7 s2) level. V1,., VK \u2212 1 are matrices that represent linear layers, and vx is a vector that represents a single coordinate in the final output. V1,., VK \u2212 1, and vK are taken from a set of V defined by Property1N."}, {"heading": "2.4. Discussion", "text": "Generalization errors differ from the empirical complexity of Rademacher by a multiplication factor determined by the Softmax classifier as a whole [18]. The method of graph regulation can be applied on a single level or on several levels. It is designed to learn attributes present in the data samples, as opposed to weight breakdown, dropout [7] and dropconnect [18], which are designed to prevent non-learning attributes (overadjustment to random errors or noise). The approach used in both Dropout and Dropconnect is to introduce randomness into the training, so that the learned network is in a way a statistical mean of an ensemble of realizations. They complement our approach to regulating output through graphics and we plan to explore a combination of these approaches in future work."}, {"heading": "3. Experiments", "text": "In Section 3.1, we use the MNIST dataset to show that the generalization error of GraphConnectOne and GraphConnect-All is significantly smaller than the weight loss, and that they achieve a higher classification accuracy, especially if the training set is small. Section 3.2 presents extensive comparisons between the proposed GraphConnect and the weight loss on CIFAR-10 and SVHN. Section 3.3 demonstrates the improved performance of GraphConnect in the face verification task."}, {"heading": "3.1. MNIST Proof of Concept", "text": "The MNIST dataset contains approximately 60,000 training images (28 \u00d7 28) and 10,000 test images. While the most advanced methods often use the entire training set, we are interested in quantifying what is possible with much smaller training sets. Table 1 describes the network architecture we use to compare chart regularization with standard weight.We start with 500 training samples (50 per class) to train two neural networks.The average image is estimated from the training set and subtracted as a pre-processing step. The first experiment uses GraphConnect-One to regulate the results of layers 3 and 5, and the second uses GraphConnectAll to regulate the production of layer 6."}, {"heading": "3.2. Comparison on CIFAR-10 and SVHN", "text": "CIFAR-10 and SVHN are benchmark RGB image datasets, each containing 10 classes that are more difficult than the MNIST benchmark because of the more significant intra-class variation (see Figure 5). We compare regularization with GraphConnect-All with regularization using weight decay on these two datasets. Table 2 specifies the network architecture (similar to [7]), all images are subtracted in a pre-processing step, and graph weights W are calculated in the same way as for the MNIST experiment. The network transforms sample images into 2048-dimensional features that are entered into a softmax classifier, and cross-entropy loss is calculated on the basis of GraphConnect-All."}, {"heading": "3.3. Face Verification on LFW", "text": "We evaluate graphics on Face Check, using the Labeled Faces in the Wild (LFW) benchmark dataset. Face Check is when presented with a pair of face images to see if the two images represent the same theme, but that's not our goal in this work, we're trying to compare the performance of graphics with that of weight de-cay. We're taking the experimental framework that's able to reproduce a deep network, but that's our goal. We're trying to compare the performance of graphics."}, {"heading": "4. Conlusion", "text": "We have proposed GraphConnect, a data-dependent framework to regulate deep neural networks, and we have compared performance to data-independent methods of regulation that are widely used. We have demonstrated that the empirical complexity of Rademacher's GraphConnect is smaller than that of weight loss, which justifies our claim that it is better to prevent overadjustments. We have presented experimental results that support our theoretical claims and show that the improvements in generalization errors are significant. Our proposed framework complements data-independent approaches that prevent overadjustment, such as Dropout and DropConnect, and future work will examine the value of combining these methods."}], "references": [{"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network", "author": ["P. Bartlett"], "venue": "IEEE Transactions on Information Theory, 44(2):525\u2013536,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research, 3:463\u2013482,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Bayesian face revisited: A joint formulation", "author": ["D. Chen", "X. Cao", "L. Wang", "F. Wen", "J. Sun"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "How tight are the vapnikchervonenkis bounds", "author": ["D. Cohn", "G. Tesauro"], "venue": "Neural Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research, 6:615\u2013637,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Max-out networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A.C. Courville", "Y. Bengio"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Empirical margin distributions and bounding the generalization error of combined classifiers", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": "Annals of Statistics, 30(1):1\u201350,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["B. Mikhail", "P. Niyogi"], "venue": "Neural computation, 15(6):1373\u20131396,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["B. Mikhail", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research, 7:2399\u20132434,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Normbased capacity control in neural networks arxiv preprint arxiv:1503.00036 (2015)", "author": ["B. Neyshabur", "R. Tomioka", "N. Srebro"], "venue": "In The 28th Conference on Learning Theory (COLT),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["P. Sermanet", "S. Chintala", "Y. LeCun"], "venue": "International Conference on Pattern Recognition (ICPR),", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning face representation by joint identification-verification", "author": ["Y. Sun", "Y. Chen", "X. Wang", "X. Tang"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 1988\u20131996,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning face representation from predicting 10,000 classes", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1891\u20131898,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M.A. Ranzato", "L. Wolf"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1701\u20131708,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "An overview of statistical learning theory", "author": ["V. Vapnik"], "venue": "IEEE Transactions on Neural Networks, 10(5):988999,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "H. Mobahi", "R. Collobert"], "venue": "Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, pages 639\u2013 655,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Robustness and generalization", "author": ["H. Xu", "S. Mannor"], "venue": "Machine learning, 86(3):391\u2013423,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "The best known bounds on the generalization error arise from the VapnikChervonenkis (VC) dimension [17], which captures the inherent complexity of a family of classifiers.", "startOffset": 99, "endOffset": 103}, {"referenceID": 3, "context": "However it is distribution agnostic, leading to a loose and pessimistic upper bound [4].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "In the last decade, distribution dependent measures of complexity have been developed, such as the Rademacher complexity [2], which can lead to tighter bounds on the generalization error.", "startOffset": 121, "endOffset": 124}, {"referenceID": 0, "context": "Rademacher complexity has been used to show that the generalization error of a neural network depends more on the size of the weights than on the size of the network [1].", "startOffset": 166, "endOffset": 169}, {"referenceID": 11, "context": "A unified theoretical treatment of norm-based control of deep neural networks is given in [12], and this theory is used to provide insight into max-out networks in [6].", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "A unified theoretical treatment of norm-based control of deep neural networks is given in [12], and this theory is used to provide insight into max-out networks in [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 6, "context": "Dropout is a new form of regularization, where for each training example, forward propagation involves randomly deleting half the activations in each layer [7].", "startOffset": 156, "endOffset": 159}, {"referenceID": 17, "context": "Dropconnect is a recent generalization of Dropout, where a randomly selected subset of weights within the network is set to zero [18].", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "Adding a Dropconnect layer to a neural network can reduce the Rademacher complexity by a factor of the dropconnect rate [18].", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "It is motivated by the empirical observation that data of interest typically lie close to a manifold, an assumption that has previously assisted machine learning tasks such as nonlinear embedding [10], semi-supervised labeling [11], and multitask classification [5].", "startOffset": 196, "endOffset": 200}, {"referenceID": 10, "context": "It is motivated by the empirical observation that data of interest typically lie close to a manifold, an assumption that has previously assisted machine learning tasks such as nonlinear embedding [10], semi-supervised labeling [11], and multitask classification [5].", "startOffset": 227, "endOffset": 231}, {"referenceID": 4, "context": "It is motivated by the empirical observation that data of interest typically lie close to a manifold, an assumption that has previously assisted machine learning tasks such as nonlinear embedding [10], semi-supervised labeling [11], and multitask classification [5].", "startOffset": 262, "endOffset": 265}, {"referenceID": 18, "context": "Previous experimental studies [19], applying graph regularization to the semisupervised embedding problem, did not explain how the approach might control capacity.", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "In particular, it is not clear from [19] whether graph-based regularization has better generalization ability than standard approaches such as weight decay.", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "When training samples are scarce, statistical learning theory predicts overfitting to the training data [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "We recall from [8] that the generalization error is (almost surely) bounded by the empirical Rademacher complexity [2], of the loss function.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "We recall from [8] that the generalization error is (almost surely) bounded by the empirical Rademacher complexity [2], of the loss function.", "startOffset": 115, "endOffset": 118}, {"referenceID": 17, "context": "Dropconnect, [18], is a recent method for regularizing large fully connected layers within neural networks.", "startOffset": 13, "endOffset": 17}, {"referenceID": 17, "context": "the Rademacher complexity of the linear feature extraction component by a multiplicative factor that is determined by the classifier [18].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "Hence we follow [18] and focus on the Rademacher complexity of the linear feature extraction.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Theorem 2 provides an upper bound on complexity that is not very sensitive to the number of layers in the network, in contrast to weight decay where complexity is exponential in the number of layers [20].", "startOffset": 199, "endOffset": 203}, {"referenceID": 17, "context": "Generalization error differs from empirical Rademacher complexity by a multiplicative factor that is determined by the overall softmax classifier [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 6, "context": "It is designed to learn attributes that are present in data samples, in contrast to weight decay, Dropout [7], and Dropconnect [18] which are designed to prevent learning non-attributes (overfitting to random error or noise).", "startOffset": 106, "endOffset": 109}, {"referenceID": 17, "context": "It is designed to learn attributes that are present in data samples, in contrast to weight decay, Dropout [7], and Dropconnect [18] which are designed to prevent learning non-attributes (overfitting to random error or noise).", "startOffset": 127, "endOffset": 131}, {"referenceID": 6, "context": "Table 2 specifies the network architecture (similar to [7]), all images are mean-subtracted in a preprocessing step, and the graph weights W used in GraphConnect-All are computed in the same fashion as for the MNIST experiment.", "startOffset": 55, "endOffset": 58}, {"referenceID": 12, "context": "More sophisticated preprocessing methods such as contrast normalization [13] and ZCA whitening [9] will reduce intra-class variation, and we would expect them to improve performance further.", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "More sophisticated preprocessing methods such as contrast normalization [13] and ZCA whitening [9] will reduce intra-class variation, and we would expect them to improve performance further.", "startOffset": 95, "endOffset": 98}, {"referenceID": 13, "context": "Impressive verification accuracies are possible when deep neural networks are able to train on extremely large labeled training sets [14, 16].", "startOffset": 133, "endOffset": 141}, {"referenceID": 15, "context": "Impressive verification accuracies are possible when deep neural networks are able to train on extremely large labeled training sets [14, 16].", "startOffset": 133, "endOffset": 141}, {"referenceID": 2, "context": "We adopt the experimental framework used in [3], and train a deep network on the WDRef dataset, where each face is described using a high dimensional LBP feature (available at 2) that is reduced to a 5,000-dimensional feature using PCA.", "startOffset": 44, "endOffset": 47}, {"referenceID": 13, "context": "The WDRef dataset is significantly smaller than the proprietary datasets in [14, 15, 16].", "startOffset": 76, "endOffset": 88}, {"referenceID": 14, "context": "The WDRef dataset is significantly smaller than the proprietary datasets in [14, 15, 16].", "startOffset": 76, "endOffset": 88}, {"referenceID": 15, "context": "The WDRef dataset is significantly smaller than the proprietary datasets in [14, 15, 16].", "startOffset": 76, "endOffset": 88}, {"referenceID": 15, "context": "For example, [16] uses 4.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "[14] and [15] use 202,599 labeled faces from 10,177 individuals, while WDRef contains 2,995 subjects with only about 30 samples per subject, clearly a much more challenging task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[14] and [15] use 202,599 labeled faces from 10,177 individuals, while WDRef contains 2,995 subjects with only about 30 samples per subject, clearly a much more challenging task.", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "Our focus is the expressiveness of the learned feature, so we do not employ advanced verification methods such as those used in [3] (those will make the study of", "startOffset": 128, "endOffset": 131}], "year": 2015, "abstractText": "Deep neural networks have proved very successful in domains where large training sets are available, but when the number of training samples is small, their performance suffers from overfitting. Prior methods of reducing overfitting such as weight decay, Dropout and DropConnect are data-independent. This paper proposes a new method, GraphConnect, that is data-dependent, and is motivated by the observation that data of interest lie close to a manifold. The new method encourages the relationships between the learned decisions to resemble a graph representing the manifold structure. Essentially GraphConnect is designed to learn attributes that are present in data samples in contrast to weight decay, Dropout and DropConnect which are simply designed to make it more difficult to fit to random error or noise. Empirical Rademacher complexity is used to connect the generalization error of the neural network to spectral properties of the graph learned from the input data. This framework is used to show that GraphConnect is superior to weight decay. Experimental results on several benchmark datasets validate the theoretical analysis, and show that when the number of training samples is small, GraphConnect is able to significantly improve performance over weight decay.", "creator": "LaTeX with hyperref package"}}}