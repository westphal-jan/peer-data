{"id": "1512.06900", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2015", "title": "Predicting the Co-Evolution of Event and Knowledge Graphs", "abstract": "Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Knowledge graphs are typically treated as static: A knowledge graph grows more links when more facts become available but the ground truth values associated with links is considered time invariant. In this paper we address the issue of knowledge graphs where triple states depend on time. We assume that changes in the knowledge graph always arrive in form of events, in the sense that the events are the gateway to the knowledge graph. We train an event prediction model which uses both knowledge graph background information and information on recent events. By predicting future events, we also predict likely changes in the knowledge graph and thus obtain a model for the evolution of the knowledge graph as well. Our experiments demonstrate that our approach performs well in a clinical application, a recommendation engine and a sensor network application.", "histories": [["v1", "Mon, 21 Dec 2015 22:49:43 GMT  (80kb,D)", "http://arxiv.org/abs/1512.06900v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["crist\\'obal esteban", "volker tresp", "yinchong yang", "stephan baier", "denis krompa{\\ss}"], "accepted": false, "id": "1512.06900"}, "pdf": {"name": "1512.06900.pdf", "metadata": {"source": "CRF", "title": "Predicting the Co-Evolution of Event and Knowledge Graphs", "authors": ["Crist\u00f3bal Esteban", "Volker Tresp", "Yinchong Yang", "Stephan Baier", "Denis Krompa\u00df"], "emails": ["Cristobal.EstebanVolker.TrespDenis.Krompass@siemens.com", "Stephan.BaierYinchong.Yang@campus.lmu.de"], "sections": [{"heading": null, "text": "Keywords: knowledge diagram, representation learning, latent variable models, link prediction"}, {"heading": "1 Introduction", "text": "This year, it has come to the point that it has never been as far as this year."}, {"heading": "2 Related Work", "text": "There is a wide range of papers on the application of data mining and machine learning to CGs. Data Mining tries to find interesting patterns of CGs [5,27,24]. Some approaches to machine learning try to extract near deterministic dependencies and ontological constructs [19,13,16]. The paper focuses here on statistical machine learning in CGs, where representational learning has proven to be very successful. There is considerable preparatory work on applying tensor models to temporal data, e.g. EEG data, and surveys that can be found in [14] and [20]. This work typically does not focus on prediction, but attempts to understand essential underlying temporal processes by analyzing the derived latent representations. Some models consider a temporal parameter drift. Examples are the BPTF [36] and [11]. Our model has a more expressive dynamics by looking explicitly at the graphs or graphs."}, {"heading": "3 The Knowledge Graph Model", "text": "With the advent of the Semantic Web [7], Linked Open Data [6], Knowledge Graphs (KGs) = high KGs (32,1,8,30], three-dimensional representations of knowledge have gained popularity. Here, we consider a slight extension of the subject predicate object triple form by adding the value (es, ep, eo; value) where value can be a function of s, p, o and the truth value of the triple or three-dimensional representation. Thus (Jack, likes, Mary; True) states that Jack likes, and (Jack, hasBloodTest, cholesterol; 160) would indicate a certain blood cholesterol level for Jack."}, {"heading": "4 The Event Model", "text": "Without loss of universality, we assume that changes in the Kg always occur in the form of events, in the sense that the events are the gateway to the Kg. For a given time step, events are described by a typically very sparse event triple graph that contains facts that change some of the triple values in the Kg, e.g. from true to false and vice versa. Kg triple graphs that do not appear in the event graph are assumed unchanged. Events can, for example, perform a cholesterol measurement, the event cholesterol measurement that specifies the value or sequence of cholesterol-lowering medicine that determines that a particular drug is prescribed, followed by dosage information.At each step, events form triple graphs that form a sparse triple graph and determine which facts become available. The event valensor is a quadruple tensor or the order of cholesterol endings that determines a medicine, keo, keo, keo, kineo, kineo, kineo, kineo, kineo, kineo, kineo."}, {"heading": "5 The Prediction Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Predicting Events", "text": "Note that both the KG tensor and the event tensor can only model information that has been observed to date, but it would not be easy to derive predictions of future events that would be of interest, e.g. to support decision-making. The key idea of the paper is that events are predicted with both latent representations of the KG and latent representations describing recently observed events. In the prediction model, we estimate future entries in the event tensor Z. The general form of the prediction is p, o, t = f predict (args) or phenomena, p, o = f predict p, o (args), where the first version uses a single function and the second uses a different function for each (p, o) -pair.3 Here, we proceed from the sets of latent representations from the KG tensor and the event tensor. An example of a prediction model is the prediction, we predict, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o - object."}, {"heading": "5.2 Predicting Changes in the KG", "text": "In our model, any change in the status of the CG is communicated through events. Thus, any change in the CG occurs first in the event tenor and predictions of events also imply predictions in the CG. Events that change the status of the CG are transferred to the CG and the latent representations of the CG, i.e. aes, aep, aeo, are regularly reestimated (Figure 1)."}, {"heading": "5.3 More Cost Functions", "text": "There is a cost function for each tensor model and prediction model (see Appendix). In our experiments, we obtained the best results by using the cost function of the task we are trying to solve. Therefore, in the most relevant prediction task, we used the cost function in Equation 2. On the other hand, we achieved faster convergence of the prediction model by initializing latent representations based on the KG model. 3 The different functions can be realized by multiple output of a neural network."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Modeling Clinical Data", "text": "In fact, we are able to go in search of a solution that enables us, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position to survive."}, {"heading": "6.2 Recommendation Engines", "text": "We used data from the MovieLens project with 943 users and 1682 Movies.4 In the KG Tensor we looked at the triples (User, rates, Movie; Rating).For the Event Tensor we consider the quadruples (User, watches, Movie, Time; Watched) and (User, rates, Movie, Time; Rating).We define our training data as 78176 events in the first 24 calendar weeks and the test data as 2664 events in the last 7 weeks. Note that in both datasets there are only 738 users, as the remaining 205 users have only seen and rated their movies in the test."}, {"heading": "6.3 Sensor Networks", "text": "In our third experiment, we wanted to explore whether our approach could also be applied to data from the sensor networks, which we consider to be air pressure, but the main difference is that the event is now a sensor sensor equipped with sub-symbolic measurements on all sensors. \u2212 The main research topics for wind power systems are the precise wind forecasts that play an important role in the planning and design of wind farms. \u2212 The complex interactions between the major geometric parameters such as surface conditions, temperature, wind speed and wind direction are considered to be a very challenging task. In our analysis, we used data from the Automated Surface Observation System (ASOS) units that are operated and controlled in the United States. We downloaded the data from the Iowa Environmental Mesonet (IEM) 6. The data consists of 18 weather stations (the entities) distributed throughout the central United States that provide measurements."}, {"heading": "7 Conclusions and Extensions", "text": "We have experimentally demonstrated that models that use latent representation perform well in these high-dimensional and very sparse dynamic areas in a clinical application, a referral engine, and a sensor network.The clinical application will be further investigated in a funded project [35,12]. In future work, we plan to test our approach in general streaming frameworks, which often include a context model, an event model, and a sensor model that fits well into our framework.In [34] we will explore links between the presented approach and cognitive memory functionality. In general, we assume a unique representation for one entity, for example, we assume that aes is the same in the predictive model and the semantic model. Sometimes it makes sense to loosen this assumption and assume only some form of coupling. [15,3,2] There is extensive discussion about the transfer of latent representation."}, {"heading": "Appendix: Cost Functions", "text": "We consider cost functions for the KG tensor, the event tensor and the predictive model. Tilde notation X indicates subsets corresponding to the facts known in the training. If only positive facts with value = True are known, as is often the case in KGs, negative facts can be generated, e.g. by using local assumptions of the closed world [21]. We use negative log probability costs. For a Bernoulli probability, we use \u2212 logP (x | \u03b8) = log [1 + exp {(1 \u2212 2x) \u03b8}] (cross-entropy) and for a Gaussian probability \u2212 logP (x \u2212 \u03b8) = const + 12\u04452 (x \u2212 \u03b8) 2. We use regulation as in Equation 2.We describe the cost function in relation to the latent representations A and the M mappings. W stands for the parameters in the functional mapping."}], "references": [{"title": "DBpedia: A Nucleus for a Web of Open Data. In The Semantic Web, Lecture Notes in Computer Science", "author": ["S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Yoshua Bengio"], "venue": "Unsupervised and Transfer Learning Challenges in Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pierre Vincent"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Towards semantic web mining. In ISWC", "author": ["Bettina Berendt", "Andreas Hotho", "Gerd Stumme"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Linked Data ", "author": ["Tim Berners-Lee"], "venue": "Design Issues,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In SIGMOD,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Learning Structured Embeddings of Knowledge Bases", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion", "author": ["Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": "In SIGKDD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Temporal link prediction using matrix and tensor factorizations", "author": ["Daniel M. Dunlavy", "Tamara G. Kolda", "Evrim Acar"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Predicting sequences of clinical events by using a personalized temporal latent embedding model", "author": ["Crist\u00f3bal Esteban", "Danilo Schmidt", "Denis Krompa\u00df", "Volker Tresp"], "venue": "In IEEE ICHI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Dl-foil concept learning in description logics", "author": ["Nicola Fanizzi", "Claudia dAmato", "Floriana Esposito"], "venue": "In Inductive Logic Programming. Springer,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Tensor Decompositions and Applications", "author": ["Tamara G. Kolda", "Brett W. Bader"], "venue": "SIAM Review,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Dl-learner: learning concepts in description logics", "author": ["Jens Lehmann"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Graphs over time: densification laws, shrinking diameters and possible explanations", "author": ["Jure Leskovec", "Jon M. Kleinberg", "Christos Faloutsos"], "venue": "In SIGKDD,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "The resource description framework (RDF) as a modern structure for medical data", "author": ["Gabriela Lindemann", "Danilo Schmidt", "Thomas Schrader", "Dietmar Keune"], "venue": "International Journal of Medical, Health, Biomedical and Pharmaceutical Engineering,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Ontology learning for the semantic web", "author": ["Alexander Maedche", "Steffen Staab"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Applications of tensor (multiway array) factorizations and decompositions in data mining", "author": ["Morten M\u00f8rup"], "venue": "Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "A review of relational machine learning for knowledge graphs", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": "Proc. of the IEEE,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Prediction and ranking algorithms for event-based network data", "author": ["Joshua O\u2019Madadhain", "Jon Hutchins", "Padhraic Smyth"], "venue": "SIGKDD Explorations,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Data mining with background knowledge from the web", "author": ["Heiko Paulheim", "Petar Ristoski", "Evgeny Mitichkin", "Christian Bizer"], "venue": "RapidMiner World,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Factorization Machines", "author": ["Steffen Rendle"], "venue": "Proceedings of the 10th IEEE International Conference on Data Mining. IEEE Computer Society,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Factorizing personalized markov chains for next-basket recommendation", "author": ["Steffen Rendle", "Christoph Freudenthaler", "Lars Schmidt-Thieme"], "venue": "In WWW,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Mining the semantic web", "author": ["Achim Rettinger", "Uta L\u00f6sch", "Volker Tresp", "Claudia d\u2019Amato", "Nicola Fanizzi"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Modeling dynamic behavior in large evolving graphs", "author": ["Ryan A. Rossi", "Brian Gallagher", "Jennifer Neville", "Keith Henderson"], "venue": "In WSDM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Tbase2 a web-based electronic patient", "author": ["K. Schr\u00f6ter", "G. Lindemann", "L. Fritsche"], "venue": "record. Fundamenta Informaticae,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Introducing the Knowledge Graph: things, not strings", "author": ["Amit Singhal"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Yago: A Core of Semantic Knowledge", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum"], "venue": "In WWW,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Graphscope: parameter-free mining of large time-evolving graphs", "author": ["Jimeng Sun", "Christos Faloutsos", "Spiros Papadimitriou", "Philip S. Yu"], "venue": "In SIGKDD,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Learning with memory embeddings", "author": ["Volker Tresp", "Crist\u00f3bal Esteban", "Yinchong Yang", "Stephan Baier", "Denis Krompa\u00df"], "venue": "arXiv preprint arXiv:1511.07972,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Towards a new science of a clinical data intelligence", "author": ["Volker Tresp", "Sonja Zillner", "Maria J. Costa", "Yi Huang", "Alexander Cavallaro"], "venue": "In NIPS Workshop on Machine Learning for Clinical Data Analysis and Healthcare,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Temporal collaborative filtering with bayesian probabilistic tensor factorization", "author": ["Liang Xiong", "Xi Chen", "Tzu-Kuo Huang", "Jeff G. Schneider", "Jaime G. Carbonell"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}], "referenceMentions": [{"referenceID": 19, "context": ", RESCAL [22], Translational Embeddings Models [9], Neural Tensor Models [31] and the multiway neural networks as used in [10].", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": ", RESCAL [22], Translational Embeddings Models [9], Neural Tensor Models [31] and the multiway neural networks as used in [10].", "startOffset": 47, "endOffset": 50}, {"referenceID": 28, "context": ", RESCAL [22], Translational Embeddings Models [9], Neural Tensor Models [31] and the multiway neural networks as used in [10].", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": ", RESCAL [22], Translational Embeddings Models [9], Neural Tensor Models [31] and the multiway neural networks as used in [10].", "startOffset": 122, "endOffset": 126}, {"referenceID": 29, "context": "Most popular KGs like Yago [32], DBpedia [1] Freebase [8] and the Google Knowledge Graph [30] have means to store temporal information.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "Most popular KGs like Yago [32], DBpedia [1] Freebase [8] and the Google Knowledge Graph [30] have means to store temporal information.", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "Most popular KGs like Yago [32], DBpedia [1] Freebase [8] and the Google Knowledge Graph [30] have means to store temporal information.", "startOffset": 54, "endOffset": 57}, {"referenceID": 27, "context": "Most popular KGs like Yago [32], DBpedia [1] Freebase [8] and the Google Knowledge Graph [30] have means to store temporal information.", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "Data mining attempts to find interesting KG patterns [5,27,24].", "startOffset": 53, "endOffset": 62}, {"referenceID": 24, "context": "Data mining attempts to find interesting KG patterns [5,27,24].", "startOffset": 53, "endOffset": 62}, {"referenceID": 21, "context": "Data mining attempts to find interesting KG patterns [5,27,24].", "startOffset": 53, "endOffset": 62}, {"referenceID": 16, "context": "Some machine learning approaches attempt to extract close-to deterministic dependencies and ontological constructs [19,13,16].", "startOffset": 115, "endOffset": 125}, {"referenceID": 11, "context": "Some machine learning approaches attempt to extract close-to deterministic dependencies and ontological constructs [19,13,16].", "startOffset": 115, "endOffset": 125}, {"referenceID": 13, "context": "Some machine learning approaches attempt to extract close-to deterministic dependencies and ontological constructs [19,13,16].", "startOffset": 115, "endOffset": 125}, {"referenceID": 12, "context": ", EEG data, and overviews can be found in [14] and [20].", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": ", EEG data, and overviews can be found in [14] and [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 33, "context": "Examples are the BPTF [36], and [11].", "startOffset": 22, "endOffset": 26}, {"referenceID": 9, "context": "Examples are the BPTF [36], and [11].", "startOffset": 32, "endOffset": 36}, {"referenceID": 23, "context": "Markov properties in tensor models were considered in [26,25].", "startOffset": 54, "endOffset": 61}, {"referenceID": 22, "context": "Markov properties in tensor models were considered in [26,25].", "startOffset": 54, "endOffset": 61}, {"referenceID": 3, "context": "Our approach can also be related to the neural probabilistic language model [4], which coined the term representation learning.", "startOffset": 76, "endOffset": 79}, {"referenceID": 14, "context": "There is considerable recent work on dynamic graphs [17,23,33,28] with a strong focus on the Web graph and social graphs.", "startOffset": 52, "endOffset": 65}, {"referenceID": 20, "context": "There is considerable recent work on dynamic graphs [17,23,33,28] with a strong focus on the Web graph and social graphs.", "startOffset": 52, "endOffset": 65}, {"referenceID": 30, "context": "There is considerable recent work on dynamic graphs [17,23,33,28] with a strong focus on the Web graph and social graphs.", "startOffset": 52, "endOffset": 65}, {"referenceID": 25, "context": "There is considerable recent work on dynamic graphs [17,23,33,28] with a strong focus on the Web graph and social graphs.", "startOffset": 52, "endOffset": 65}, {"referenceID": 5, "context": "With the advent of the Semantic Web [7], Linked Open Data [6], Knowledge Graphs (KGs) [32,1,8,30], triple-oriented knowledge representations have gained in popularity.", "startOffset": 58, "endOffset": 61}, {"referenceID": 29, "context": "With the advent of the Semantic Web [7], Linked Open Data [6], Knowledge Graphs (KGs) [32,1,8,30], triple-oriented knowledge representations have gained in popularity.", "startOffset": 86, "endOffset": 97}, {"referenceID": 0, "context": "With the advent of the Semantic Web [7], Linked Open Data [6], Knowledge Graphs (KGs) [32,1,8,30], triple-oriented knowledge representations have gained in popularity.", "startOffset": 86, "endOffset": 97}, {"referenceID": 6, "context": "With the advent of the Semantic Web [7], Linked Open Data [6], Knowledge Graphs (KGs) [32,1,8,30], triple-oriented knowledge representations have gained in popularity.", "startOffset": 86, "endOffset": 97}, {"referenceID": 27, "context": "With the advent of the Semantic Web [7], Linked Open Data [6], Knowledge Graphs (KGs) [32,1,8,30], triple-oriented knowledge representations have gained in popularity.", "startOffset": 86, "endOffset": 97}, {"referenceID": 19, "context": "For example, the RESCAL model [22] is", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "In the multiway neural network model [10] one uses \u03b8 s,p,o = NN(aes ,aep ,aeo)", "startOffset": 37, "endOffset": 41}, {"referenceID": 18, "context": "For a recent review, please consult [21].", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "[18,29].", "startOffset": 0, "endOffset": 7}, {"referenceID": 26, "context": "[18,29].", "startOffset": 0, "endOffset": 7}, {"referenceID": 22, "context": "89 on this data set [25].", "startOffset": 20, "endOffset": 24}, {"referenceID": 32, "context": "The clinical application is explored further in a funded project [35,12].", "startOffset": 65, "endOffset": 72}, {"referenceID": 10, "context": "The clinical application is explored further in a funded project [35,12].", "startOffset": 65, "endOffset": 72}, {"referenceID": 31, "context": "In [34] we are exploring links between the presented approach and cognitive memory functions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "[15,3,2] contain extensive discussions on the transfer of latent representations.", "startOffset": 0, "endOffset": 8}, {"referenceID": 1, "context": "[15,3,2] contain extensive discussions on the transfer of latent representations.", "startOffset": 0, "endOffset": 8}], "year": 2015, "abstractText": "Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Knowledge graphs are typically treated as static: A knowledge graph grows more links when more facts become available but the ground truth values associated with links is considered time invariant. In this paper we address the issue of knowledge graphs where triple states depend on time. We assume that changes in the knowledge graph always arrive in form of events, in the sense that the events are the gateway to the knowledge graph. We train an event prediction model which uses both knowledge graph background information and information on recent events. By predicting future events, we also predict likely changes in the knowledge graph and thus obtain a model for the evolution of the knowledge graph as well. Our experiments demonstrate that our approach performs well in a clinical application, a recommendation engine and a sensor network application.", "creator": "LaTeX with hyperref package"}}}