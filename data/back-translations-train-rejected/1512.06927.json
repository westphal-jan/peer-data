{"id": "1512.06927", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2015", "title": "A C++ library for Multimodal Deep Learning", "abstract": "This is the document of Multimodal Deep Learning Library, MDL, which is written in C++. It explains principles and implementations with details of Restricted Boltzmann Machine, Deep Neural Network, Deep Belief Network, Denoising Autoencoder, Deep Boltzmann Machine, Deep Canonical Correlation Analysis, and modal prediction model. MDL uses OpenCV 3.0.0, which is the only dependency of this library. Most of its implementation has been tested in Mac OS. It also provides interface for reading various data set such as MNIST, CIFAR, XRMB, and AVLetters. To read mat file, Matlab must be installed because it uses Matlab/c++ interface provided by Matlab. There are multiple model options provided. Different gradient descent methods, loss function, annealing methods, and activation functions are given. These options are easy to extend given the structure of MDL. So MDL could be used as a frame for testings in deep learning.", "histories": [["v1", "Tue, 22 Dec 2015 01:27:23 GMT  (1305kb,D)", "http://arxiv.org/abs/1512.06927v1", "30 pages"], ["v2", "Mon, 28 Dec 2015 20:00:20 GMT  (1304kb,D)", "http://arxiv.org/abs/1512.06927v2", "29 pages"], ["v3", "Tue, 29 Dec 2015 13:39:52 GMT  (1304kb,D)", "http://arxiv.org/abs/1512.06927v3", "29 pages"], ["v4", "Tue, 12 Apr 2016 17:34:29 GMT  (1258kb,D)", "http://arxiv.org/abs/1512.06927v4", "27 pages"]], "COMMENTS": "30 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jian jin"], "accepted": false, "id": "1512.06927"}, "pdf": {"name": "1512.06927.pdf", "metadata": {"source": "CRF", "title": "Multimodal Deep Learning Library", "authors": ["Jian Jin"], "emails": [], "sections": [{"heading": null, "text": "Multimodal Deep Learning LibraryJian JinFaculty of Computer Science, Johns Hopkins University"}, {"heading": "1 Multimodal Deep Learning Library", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Introduction", "text": "This is the document of the Multimodal Deep Learning Library, MDL, written in C + +. It explains principles and implementations with details on Restricted Boltzmann Machine, Deep Neural Network, Deep Belief Network, Denoising Autoencoder, Deep Boltzmann Machine, Deep Canonical Correlation Analysis and Modal Prediction Model.MDL uses OpenCV 3.0.0, which is the only dependency of this library. Most of its implementations have been tested in Mac OS. It also provides interfaces for reading various data sets such as MNIST, CIFAR, XRMB and AVLetters. To read matt files, Matlab needs to be installed because it uses the interface provided by Matlab / c + +. Multiple model options are available. Different gradient descend methods, loss functions, annealing methods and activation functions are given. These options are easy to expand due to the structure of MDL."}, {"heading": "1.2 Content Description", "text": "Section 2 goes through common networks. Section 3 to Section 7 describes the Limited Boltzmann Machine, Deep Neural Network, Deep Belief Network, Denoising Autoencoder, Deep Boltzmann Machine or Deep Canonical Correlation Analysis. Section 8 introduces multimodal learning models. Section 9 explains the structure of the library. Section 10 presents performance. Section 3 to 7 explains each section principles and implementations of each model. Section 8 introduces two models to multimodal learning."}, {"heading": "2 Network Survey", "text": "This section contains a brief description of the network models mentioned in this document and a summary of the deep learning models of MDL."}, {"heading": "2.1 Neural Network", "text": "The neural network is a directed graph consisting of several layers of neurons, also referred to as units. In general, there is no connection between units of the same layer and there are only connections between adjacent layers. The first layer is the input layer and is referred to as the visible layer. Above the visible layer, there are several hidden layers {h1, h2,..., hn}. And the output of the last hidden layer forms the output layer o.In hidden layers, neurons in layer hi receive the input from the previous layer, hi \u2212 1 or v, and the output Xiv: 151 2.06 927v 1 [cs.L G] 22 Dec 201 5of hi the input layer to the next layer, hi + 1 or o. The value transmitted between layers is called activation or emission. Activation is calculated as: a (k) i = f ((((nk \u2212 1) j = 1) ij ij) ik (k) (i) is the function of (z)."}, {"heading": "2.2 Markov Random Field", "text": "A Markov Random Field, also called the Markov network, is an undirected graphical model in which each node is independent of the other nodes because all nodes are connected to it. It describes the distribution of variables in the graph. The Markov Random Field uses energy to describe the distribution over the graph P (u) = 1Z e \u2212 E (u), (2) where Z is a partition function defined by Z = \u2211 u e \u2212 E (u), (3) E is the energy specified by the model, and u is the state set of all nodes such as thatu = {v1, v2,..., vn} (4), where vi is the state of the node i."}, {"heading": "2.3 Belief Network", "text": "The faith network, also known as the Bayesian network, is a directed acyclic graph for probabilistic reasoning. It represents the conditional dependencies of the model by associating each node X with a conditional probability P (X | Pa (X), where Pa (X) denotes the parents of X. Here are two of its conditional independence properties: 1. Each node is conditionally independent of its non-descendants due to its parents. 2. Each node is conditionally independent of all other nodes due to its Markov ceiling, which consists of parents, children, and children. The conclusion of the faith network is to calculate the posterior probability distribution P (H | V) = P (H, V), H P (H, V) (5), where H is the set of perverse variables that form hidden units, and V the set of evidence variables that form visible units."}, {"heading": "2.4 Deep Learning Models", "text": "Below are the models implemented in MDL.The Restricted Boltzmann Machine. The Restricted Boltzmann Machine is a kind of Markov Random Field and is trained in an unattended manner. It is the building block for other models and could be used for classification by adding a classifier on top of it.The Deep Neural Network is a multi-layer neural network. Each layer is initialized by preferring a Restricted Boltzmann Machine. Then fine tuning would refine the parameters of the model. The Deep Belief Network is a hybrid of the Restricted Boltzmann Machine and the Sigmoid Belief Network. It is a generative model and is not a feeding neural network or multi-layer perceptron, even if its formation is the Deep Neural Network.The Denoising Autoencoder is a kind of neural corneural network that has a symmetrical structure. It could reconstruct the input data, and it could be reconstructed if superpowers could be trained correctly."}, {"heading": "3 Restricted Boltzmann Machine", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Logic of Restricted Boltzmann Machine", "text": "Each unit x is a stochastic binary unit like this (x), a Restricted Boltzmann Machine defines the distribution over the visible layer v, 1 \u2212 p (7), where the hidden layer p is defined by the model. Figure 3.1 shows an RBM with four hidden units and six visible units. As a Markov Random Field, a Restricted Boltzmann Machine defines the distribution over the visible layer v and the hidden layer h asP (v, h) = 1Z e \u2212 E (8), where Z is a partition function defined by Z = V, h e \u2212 E (v, h). (9) Its energy E (v, h) is defined by E (v, h)."}, {"heading": "3.2 Training of Restricted Boltzmann Machine", "text": "In the training of Boltzmann Machine, the weights and the distribution differences of the hidden and visible layers are updated according to the direction of travel. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "3.3 Tricks in Restricted Boltzmann Machine Training", "text": "In RBM training, a mask is created and placed on the hidden layer. Let's say, for example, that the hidden states are areh = {h1, h2,..., hn} (31) and the drop-out rate. Then, a mask m is generated, where a mask m = {1, ui > r0, ui \u2264 r (32), where ui is a sample from the uniform distribution, and i {1, 2,..., n}. Emission of the hidden layer would be beh = h = h m (33), where the elemental multiplication of two vectors. h, instead of h, is used to calculate visible states.Learning rate Annealing There are several methods to adjust the learning rate to the slope. If the learning rate is trivial, updates can result."}, {"heading": "3.4 Classifier of Restricted Boltzmann Machine", "text": "A classifier based on RBM could be constructed by forming a classification layer with q = q activation function on the hidden layer. Softmax activation function takes a vector a = {a1, a2,..., aq} as input and returns a vector c = {c1, c2,..., cq} with the same dimension, specificallyci = eai \u00b2 p = 1 e ak. (41) Here, a one-of-K scheme is used to represent the class distribution. If there are K classes in the training data, then the designation of a sample in class i is expressed as a vector of length K with only the ith element p as 1, the others as 0. If the training data has 5 classes, a sample in fourth grade will be called {0, 0, 1, 0}.For a training with K classes, there should be neurons in the classification layer."}, {"heading": "3.5 Implementation of RBM", "text": "Here is a selected list of methods of the class rbm: I. void dropout (double i) -Set dropout rate as input i.II. void doubleTrain (dataInBatch & trainingSet, int numEpoch, int inLayer, ActivationType at = sigmoid t, LossType lt = MSE, GDType gd = SGD, int numGibbs = 1) -Train an RBM layer in Deep Boltzmann Machine, int inLayer, ActivationType at = sigmoid t section.III. void singleTrainBinary (dataInBatch & trainingSet numb = 1) -Train an RBM layer in Deep Boltzmann Machine, int inLayer, ActivationType at = sigmoid t t, LossType at = sigmoid t section.III."}, {"heading": "3.6 Summary", "text": "RBM is the basis for several multi-layer models. Crucially, this component is correctly implemented and fully understood, and the classifier can be trained simultaneously with the hidden layer. Separating these two layers makes it easier to verify implementation problems."}, {"heading": "4 Deep Neural Network", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Construction of Deep Neural Network", "text": "A Deep Neural Network (DNN) [8] is a neural network with several hidden layers. In neural networks, the initialization of weights could greatly influence training outcomes. Pre-training, in which several Boltzmann Limited Machines are trained to initialize parameters of each DNN layer, provides a weight initialization that saves training time. Back propagation is a time-consuming process. Pre-training could significantly reduce the time required by back propagation. Figure 4.1 shows a DNN with two hidden layers. The following shows the construction of a deep neural network for classification: I. Adjust the architecture of the model, in particular the size of the visible layer and the hidden layers, n0, n1, n2,..., nN. n0 is the input dimension and input form forming a visible layer. nN corresponds to the number of classes in the training data.II. Pretrain hidden layers: for 1, n1, n1, n1, n2, n0 is the input dimension and input form forming a visible layer."}, {"heading": "4.2 Fine Tuning of Deep Neural Network", "text": "As mentioned in the above section, the fine adjustment uses backpropagation, a common learning algorithm used in neural networks. In contrast to the single-stage backpropagation used in the training classification of RBM, this is a thorough one that goes through each layer. The backprojection algorithm is: I. Run a pass through all layers of the network, calculate the total input of each layer {z (1),..., z (N)} and activations {a (1),..., a (N)} of each layer. a (i) is the line vector that activates the layer i.II. For the last layer, calculate \u03b4 (N) i as\u03b4 (N) i = 4 x L \u2202 z (N) i (52), where L is the classification error. Acquire a line vector. (N).III. For l = N \u2212 1,... \u2212 1, computing power (l) (W) (T) (l) (l)."}, {"heading": "4.3 Implementation of Deep Neural Network", "text": "The implementation of Deep Neural Network is in the header file dnn.hpp. It uses the class RBMlayer to store architectural information. Here is a selected list of methods of class dnn: I. void addLayer (RBMlayer & l) Add a layer to the current model. This object of class RBMlayer should store information about layer size, weight, bias, etc. It could also be changed after it was added to the model. II. void setLayer (std: vector < size t > rbmSize) object of class dnn could automatically initialize random weights and distortions of each layer by entering a vector of layer size. III. void train (dataInBatch & trafictionData, size t rbmEpoch, LossType l = MSE, ActivationType a = sigmoid t, Batmoid t type) This method trains all layers without classifiers."}, {"heading": "4.4 Summary", "text": "When building the Deep Neural Network, several RBMs are stacked during the training and fine-tuned afterwards. As the Deep Neural Network is a directed graph and each layer receives input from the previous adjacent layer, no additional conclusions can be drawn about the formation of this model."}, {"heading": "5 Deep Belief Network", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Logic of Deep Belief Network", "text": "A Deep Belief Network (DBN) is a hybrid of a Restricted Boltzmann Machine and a Sigmoid Belief Network. A Deep Belief Network (DBN) maximizes the probability of P (x) of the input x. Figure 5.1 shows a DBN.For a Deep Belief Network with N hidden layers, the distribution over the visible layer (input data) and hidden layers isP (v, h1,..., hN) = P (v | h1) \u00d7 (N \u2212 hk + 1)))) \u00d7 P (hN \u2212 1, hN (hN \u2212 1, hN).P To prove this, the distribution is expressed with chain rule: P (v, h1, hN) = P (v | h1, h1, h1, h1, h1, h1, h1, hK = P, P h1, P hep, P h1, P, P h1, P, P h1, P, P h1, P, P h1, P, P h1, P, P h1, P, P h1, P, P h1, P, P h1, P, P h1, P, P h1, P, P h1, P, P h1, P, P h1, P, P h1, P, P h1, P, P, P 1, P h1, P h1, P, P h1, P, P, P 1, P h1, P, P 1, P, P h1, P h1, P, P 1, P h1, P, P, P 1, P h1, P, P 1, P, P 1, P, P h1, P h1, P, P, P 1, P, P 1, P h1, P, P h1, P, P, P, P, P 1, P 1, P h1, P, P 1, P, P, P, P h1, P, P h1, P, P, P 1, P, P hN, hN, P 1, h1, P, 1, P, P, and P 1, P, P, h1)."}, {"heading": "5.2 Training of Deep Belief Network", "text": "In training, the Deep Belief Network should maximize the probability of training data (Q = Q). Furthermore, the concavity of the logarithm function (Q = Q = 65 | | P is the lower limit of the logarithm probability of training data that x could be found: logP (x) = log (h) = log (h) Q (h) Q (h) Q (h) Q (h) P (h) P (h) P (h) p (h) P (h) p (h) P (h) Q (h) Q (h) h | x | x) = x (h) logarithm) logP (x, h) \u2212 P (h) logQ (h)."}, {"heading": "5.3 Classification of Deep Belief Network", "text": "This method reminds us that RBM is able to predict missing values. In a trained deep arbitrary network, any approximation Q (hk + 1 | hk) could be calculated on the basis of states hk and model parameters. In a deep arbitrary network with classifier, the topmost RBM takes labels and hidden layer hN \u2212 1 to calculate states of the last hidden layer hN, as shown in Figure 5.2. For better prediction performance, dropout is used in training the topmost RBM. Suppose l is the amount of units representing the forecast distribution. In classification, we fill l with zeros and calculate the approximation Q (hN | l, hN \u2212 1). Then we use the states hN, which are scanned by Q (hN | l, hN \u2212 1) to calculate the prediction distribution by P (hl) \u2212 N."}, {"heading": "5.4 Implementation of Deep Belief Network", "text": "The implementation of Deep Belief Network is in the header file dbn.hpp. It uses the class RBMlayer to store architectural information. Here is a selected list of methods of class dnn: I. void addLayer (RBMlayer & l) Add a layer to the current model. This object of class RBMlayer should store information about layer size, weight, bias, etc. It could also be changed after adding a layer size vector to the model. II. void setLayer (std:: vector < size t > rbmSize) object of class dnn could automatically initialize random weights and distortions of each layer by entering a layer size vector. III. Void train (dataInBatch & trainingData, size t rbmEpoch, LossType l = MSE, ActivationType l = MSE, Activationa = sigmoid, Bataid of the layer size. QT, Bataid Infint DataType DataT DataDataT, DataSSE DataSE, SSE, ActivationType l = MSE, ActivationType ID Qtivationa = sigmoid, BataID QT, BataT DataT DataID, DataT Infint DataT."}, {"heading": "5.5 Summary", "text": "It is easy to confuse the deep-belief network with the deep-neural network. Both stack pre-formed RBMs in the training process. However, because these two models have different structures, their classification processes differ. In the deep-neural network, forward propagation indicates the predictive distribution, while in the deep-belief network, the predictive distribution is calculated by another projection from the last hidden layer."}, {"heading": "6 Denoising Autoencoder", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Construction of Autoencoder", "text": "Autoencoder (AE) [14] is a kind of neural network that forms a directed graph. Its symmetry states that for an autoencoder with (N + 1) layers (including the visible layer and output layer), the dimension of each layer is limited by ni = nN \u2212 i for 0 \u2264 i \u2264 N (70), where ni is the dimension of the output layer and n0 is the input dimension. Since the output layer and the visible layer are in the same dimension, autoencoders are expected to be able to reconstruct the input data. Therefore, the formation of autoencoders is unattended learning, as input data can be used as labels for fine-tuning and reconstruction errors can be used to access the model. Autoencoders could also be used to construct classifiers by adding a classification layer on top."}, {"heading": "6.2 Fine tuning of Autoencoder", "text": "The fine-tuning of autoencoders uses back propagation, i.e.: I. Perform a forward propagation through all layers that calculate the layer input {z (1),..., z (N)} and activations {a (1),..., a (N)}. A (i) is a line vector that represents the activation of layer i.II. e. Calculate for l = N \u2212 1,... \u2212 p \u2212 p, (l) i \u2212 p (N) (W \u2212 l) z (N) i (71), where L is the reconstruction error. This step acquires a line vector (N).III. Calculate for l = N \u2212 1,..., 1 \u2212 p \u2212 p, (l + 1) (W \u2212 l) z) T) \u2022 g \u2032 (z (l))) (72), where g is the activation function and g \u2032 i is the construction. IV. Calculate the gradients in each layer."}, {"heading": "6.3 Denoising Autoencoder", "text": "The denoising autoencoder reconstructs the input from its corrupt version. Therefore, it could predict missing values and it is quite easy to observe its performance if the input is image data. By putting a denoise mask on the input, the autoencoders could be transformed into the denoising autoencoder. This is how this transformation can be performed: First, a denoise rate r is selected and the mask is constructed as follows: mi = {1 if Ui > r 0, other1 \u2264 i \u2264 nN (79), where Ui is the ith sample from the even distribution, nN is the size of the last layer, which is also the dimension of the input data. Second, the corrupt input data aa (c) = a (0) \u00b7 m = nN \u2211 i = 1 a (0) i mi (80). Finally, a (c) is used as training data to calculate the reconstruction and the corrupt input data is still used as fine data in Fa (0)."}, {"heading": "6.4 Implementation of Denoising Autoencoder", "text": "The implementation of DAE is in the header file autoencoder.hpp.I. void addLayer (RBMlayer & l) Add a layer to current DAE. This object of the class RBMlayer should store information about layer size, weight, bias, etc. It could also after adding it to DAE.II. void setLayer (std: vector < size t > rbmSize) object of the class AutoEncoder could automatically initialize random weights and distortions of each layer by entering a vector of the layer size. III. void train (dataInBatch & trainingData, size t > rbmEpoch, LossType l = MSE, ActivationType a = sigmoid t) This method trains all layers without classifiers. The structure of this AutoEncoder object should be initialized before calling this method. IV. void reconstruct (dataInBatch & TestingData) Give the data reconstruction rate."}, {"heading": "6.5 Summary", "text": "The construction of the Denoising Autoencoder requires a pre-training of half of its layers, the other half is determined by its symmetrical structure. Fine-tuning is crucial for the Denoising Autoencoder, as it uses intact data to modify the model trained with corrupt data. Denoising Autoencoder performance is easy to assess because you could observe the reconstructed images."}, {"heading": "7 Deep Boltzmann Machine", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Logic of Deep Boltzmann Machine", "text": "A Deep Boltzmann Machine (DBM) [26] is a Markov random field consisting of several layers. Connections exist only between adjacent layers. Intuitively, it could include feedback when calculating bottom-up approximations from top to bottom. Figure 7.1 shows a BM.The energy function of a Deep Boltzmann Machine with N hidden layers is E (v, h (1),.. h (N))) = \u2212 vTW (1) h (1) \u2212 (h (1) TW (2) h (2) \u2212... \u2212 (h (N \u2212 1)) TW (N) h (81), where W (i) is the weight from the previous layer to the ith hidden layer. A Deep Boltzmann Machine maximizes the probability of input data. The course of its protocol probability results in the amount logP (v) x W (i) = < (i) (1) (lt; < < T (T) (< T)."}, {"heading": "7.2 Pretraining of Deep Boltzmann Machine", "text": "Since the Deep Boltzmann Machine is an undirected model, the last hidden layer receives input from the previous adjacent layer, and the other hidden layers receive input from both directions. Thus, when training Restricted Boltzmann Machines, the weights and distortions must be adjusted for better approximations. I. The pre-training process is as follows: I. Adjust the architecture of the model, especially the size of each layer, n0, n1, n2,..., nN. n0 is the dimension of the training data. II. Pretrain is the first hidden layer: Train on RBM, where the weight from the visible layer v to the hidden layer h1 is 2W1 and the weight is from h1 to v WT1."}, {"heading": "7.3 Mean Field Inference", "text": "Mean Field Inference Initialize M samples {v = 0,1, h, 0,1},..., {v-0, M, h-0, M} with the pre-trained model. Each sample consists of states of the visible layer and all hidden layers. For t = 0 to T (number of repetitions) do 1. Variation derivative: for each data sample vn, n = 1 to D run a bottom-up pass with 1j = circuit (n0 = 12W 1ijvi), \u03bd2k = \u03c3 (n1 = 12W 2ij = 2W 2jk\u03bd 1 j), \u00b7 \u00b7 N \u2212 1p = \u043c (nN \u2212 2 \u2212 kq = 0.1 \u00b7 Wjk = 1p = 1\u00b5m = 1p = 1p = 1p = 1p = 2p = 1p = 1p = 1p \u00b2)."}, {"heading": "2. Stochastic Approximation:", "text": "For each sample m = 1 to M doRunning one step Gibbs sampling. Get (v \u0442t + 1, m, h, h, m) from (v, m, h, t, m) end for3. Parameter update: W 1t + 1 = W 1 t + \u03b1t (1 D, D n = 1 vn (\u00b5 1 n) T \u2212 1M \u2211 M m = 1 v, t + 1, m (h, 1 t + 1, m) T) W 2t + 1 = W 2 t + \u03b1t (1 D, D n = 1 \u00b5 1 n (\u00b5 2 n) T \u2212 1M, M m = 1 h, m (h, 2 t + 1, m) T) \u00b7 \u00b7 WnNt + 1 = W nN t + \u03b1t (1 D, D n = 1 \u00b5 nN \u2212 1 n (\u00b5nNn) T \u2212 1M, m = 1 h, nN \u2212 1 t + 1, m (h, n, n) + 1, t + 1, n (n)."}, {"heading": "7.4 Implementation of Deep Boltzmann Machine", "text": "The implementation of DBM is in the header file dbm.hpp.I. void addLayer (RBMlayer & l) Add a layer to current DBM. This object of class RBMlayer should store information about layer size, weight, preload, etc. It could also, after adding it to DBM.II. void setLayer (std: vector < size t > rbmSize) object of class DBM could automatically initialize random weights and distortions of each layer by entering a vector of layer sizes. III. void train (dataInBatch & Data, dataInBatch & label, size t rbmEpoch, LossType l = MSE, ActivationType a = sigmoid t) This method classifies DBM as classified.IV. void fineTuning (dataInBatch & data, dataInBatch & label) Fine tuning uses cease field label inference.indataV."}, {"heading": "7.5 Summary", "text": "In the DBM, each layer receives input from all adjacent layers. Their formation is more complicated than with other models."}, {"heading": "8 Multimodal Learning Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Deep Canonical Correlation Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1.1 Canonical Correlation Analysis", "text": "Canonical Correlation Analysis [10] is a method for finding the relationship between two sets of variables. Specifically, this means that for two sets of variables X and Y, in which each column consists of samples of a variable, there are two projectsU = aTX, (83) V = bTY. (84) The correlation between U and V iscorr (U, V) = Cov (U, V) \u221a var (V) = 1. (87) Centering the data will not change the result, but could facilitate the calculation: x = X \u2212 IXITXX / nX = eigeny x, (88) y = Y \u2212 IY \u2212 nY. (89), where IX is an nX \u2212 vector of nons, the number of Y is the number of Y = 93. And the number of Y is the number of X = 93. (1 x) x."}, {"heading": "8.1.2 Kernel Canonical Correlation Analysis", "text": "The Kernel Canonical Correlation Analysis [16, 19] projects the data into a higher-dimensional attribute space before linear projections are performed using Figure \u03c6: x \u2192 \u03c6 (x). (94) The calculation uses Kernel K (x, y), which is defined as K (x, y) = < \u03c6 (x), \u03c6 (y) > (95), where < a, b > is the internal product of a and b. The solution suggests that projection a is the uppermost property vectors of the matrix (K (X, X) + rXIXI T X) \u2212 1K (Y, Y) (K (Y, Y) + rY IY I T Y) \u2212 1K (X, X) \u2212 97, where rX and rY are regulated terms. And b gets byb = 1\u043c (K (Y, Y) + rY IY IT Y) \u2212 1K (X, X) (97), where the corresponding property is present."}, {"heading": "8.1.3 Deep Correlation Analysis", "text": "In Deep Correlation Analysis (DCCA) [2,18,33], a layer of KCCA is placed over two separately trained Deep Neural Networks, each of which learns the data of a model. Figure 8.1 shows an example of DCCA. With KCCA at the top, the model could learn the correlation between the data of two models. Deep Canonically Correlated Autoencoders (DCCAE) [32] is an extension of DCCA. Comparing with DCCA, solutions with a higher correlation sum could be found."}, {"heading": "8.2 Modal Prediction and Transformation", "text": "It is possible to make a low-dimensional representation by building a Bimodal Deep Belief Network [25] as in Figure 8.2 (a), in which blue nodes represent data from one modal node and green nodes represent data from another modal node. In this process, the detection weights used in the bottom-up calculation and the generative weights used in the top-down calculation could be learned. If the model is trained with bound weights, half of the memory space could be saved since the transposition of the weight matrix. Figure 8.3 shows such a model by first building two DBMs, each of which is trained on a model. [30] Another option is to build a multimodal Markov Random Field learning model by combining two Deep Boltzmann machines. Figure 8.3 shows such a model by first building two DBMs, each of which is trained on a model."}, {"heading": "9 Library Structure", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Data Reading", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1.1 MNIST", "text": "MNIST [22] is a selected set of samples from the NIST dataset. It consists of a training dataset, a training dataset, a test dataset and a test dataset. The training set consists of 60,000 samples and the test dataset consists of 10,000 samples. Each dataset consists of a 28 x 28 gray image containing a handwritten integer between 0 and 9. It has 10 classes, so that the label is between 0 (inclusive) and 9 (inclusive). The data is stored in big endian format. The contents of the data should be read as unsigned characters. The header file readMNIST.hpp provides functions for reading this dataset. I. cv:: Mat imread mnist image (const char * path) Read data of MNIST. Each line is a sample number indicating this class. II. cv:: Mat imread mnist label (const char * path) Read labels of MNIST."}, {"heading": "9.1.2 CIFAR", "text": "The CIFAR-10 dataset [17] consists of 60000 32 x 32 color images in 10 classes with 6000 images per class. There are 5 batches of training images and 1 batch of test images, each containing 10,000 images. In the CIFAR-10 dataset, each sample consists of a number indicating its class and image pixel values. Place five training data sets and one CIFAR-10 test data set in the same folder called \"data.\" The following function in the header file readCIFAR.hpp reads them to four OpenCV matrices: void imread cifar (Mat & trainingData, Mat & trainingLabel, Mat & testingData, Mat & testingLabel) Each set of OpenCV matrices read consists of the label and data from a sample."}, {"heading": "9.1.3 XRMB", "text": "In the Wisconsin X-ray Microbeam Database (XRMB) [31, 35] the dimension of the MFCC data is 273 and the dimension of the XRMB data 112. There are label files folded 0 that provide the phone labels. Data is saved in double precision format.The following function in the header file readXRMB.hpp reads it: cv:: Mat imread XRMB data (const char * path, int inputDim) indexLabel imread XRMB label (const char * path) When reading MFCC files inputDim = 273. When reading XRMB files inputDim = 112. The label is between 0 (inclusive) and 39 (inclusive).The data is stored sample by sample."}, {"heading": "9.1.4 AvLetters", "text": "AvLetters [24] is the dataset that records audio and video data of different persons uttering letters. The dimension of the audio data is 26 and the dimension of the video data is 60 x 80. The data is stored in the precise big endian format. Each file is the data of a person uttering a certain letter. For example, the file A1 Anya.mfcc contains the audio data of the person named Anya, which pronounces the letter \"A.\" The following function in the header file readAvLetters.hpp reads audio data: cv:: Mat imread avletters mfcc (const char * path) The output is an OpenCV matrix, each line of which contains data from an example. The header file readMat.hpp contains the function file readMat.hpp contains the file functioncv:: Mat matRead (const char * fileName, const chvear Matrix MATsaint * file containing the name MATsaint * file)."}, {"heading": "9.1.5 Data Processing", "text": "Header file processData.hpp stores functions that process data data.data oneOfK (indexLabel l, int labelDim) Transfer index label to one-of-k expression.dataInBatch corruptImage (dataInBatch input, double denoiseRate) Enter corrupted data in batches.std:: Vector < dataInBatch > dataProcess (dataCollection & reading, int numBatch) Build data batches.dataCollection shuffleByRow (dataCollection & m) Shuffle the datacv:: Mat denoiseMask (size t rowSize, size t colSize, double rate) Create a mask to corrupt data"}, {"heading": "9.2 Computation and Utilities", "text": "activation.hpp includes several activation functions, such as sigmoid, tanh, relu, leaky relu, softmax. Each activation function is coupled with a function that calculates its derivatives to simplify the calculation in reverse propagation. cca.hpp includes functions that calculate CCA, and KCCA.gd.hpp includes functions for adaptive gradient descendence and stochastic gradient descendence, and a function to glow the learning rate in which three types of annealing methods are supplied.inference.hpp includes the implementation of medium field references. hpp includes several kernel functions used by KCCA.loss.hpp. MSE, absolute loss, cross entropy, and binary loss are provided together with the functions used to calculate their derivative. matrix.hpp includes some OpenCripp data manipulation through a matrix."}, {"heading": "9.3 Models and Running", "text": "Table 1 shows the header files and files that contain the most important functions for testing each model. In addition, modalPrediction.cpp includes the implementation of a modal prediction model."}, {"heading": "10 Performance", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10.1 Restricted Boltzmann Machine", "text": "The main function for executing Restricted Boltzmann Machine is located in runRBM.cpp. It uses RBM to classify MNIST data sets. On MNIST, you use 60,000 samples for training and 10,000 samples for testing. With a hidden layer of size 500. The classification error rate is 0.00707 (accuracy 92.93%). Several deep learning libraries provide similar results."}, {"heading": "10.2 Deep Neural Network", "text": "The main function for running Deep Neural Network is in runDNN.cpp. Normally, it takes a large number of epochs in backpropagation in training, but this number is determined by pretraining. For testing, a DNN with hidden layers of size 500, 300, 200 or respectively is constructed. On MNIST data sets, pretraining alone yields the error rate of 0.093 (accuracy 90.7%).In backpropagation, each epoch goes through the complete training data for updating once. An epoch in backpropagation yields a classification error rate of 0.0858 (accuracy 91.42%).Ten epochs in backpropagation yields a classification error rate of 0.0288 (accuracy 97.12%).In fine tuning, the learning rate is 0.01. The result is comparable to the performance when running DNN on MNIST with MEDAL."}, {"heading": "10.3 Denoising Autoencoder", "text": "The main function for running Denoisinig autoencoders is located in runDAE.cp. For testing, a denoising autoencoder was constructed with hidden layers of size 500, 300, 500. Figure 10.1 shows the reconstruction of the damaged test set by the autoencoder using a model that was trained through training, with the top six lines being reconstructions and the bottom six lines being undamaged input. Figure 10.2 shows, for comparison, the reconstruction of the undamaged test set by the auto encoder. Fine tuning in the denoising autoencoder results in greater performance improvement than in the autoencoder. Each epoch goes through the complete training data once to update. On MNIST, perform 10 epochs in fine tuning, the reconstruction error calculated by MSE in the denoising autoencoder is \"an error of denoising autoencoder.\""}, {"heading": "10.4 Deep Belief Network", "text": "The main function for operating the Deep Belief Network is runDBN.cpp. For testing, a Deep Belief Network is built with hidden layers of sizes 500, 300. The classification error rate for MNIST without fine tuning is 0.0883 (accuracy 91.17%). Up-down fine tuning has reduced the classification error rate to 0.0695 (accuracy 93.05%). [6] indicates that the best performance of DBN on MNIST has reached the error rate of 1.25%. There are several parameters in the training process that could affect the result, such as learning rate and tricks on the slope descent. This could affect the result."}, {"heading": "10.5 Deep Boltzmann Machine", "text": "The main function for the operation of the Deep Boltzmann Machine is located in runDBM.cpp. For testing, a Deep Boltzmann Machine is constructed with hidden layers of size 500, 500. The error rate on MNIST without fine tuning is 0.0937 (accuracy 90.63%). The mean field reference improves the accuracy to 93.47%. Again, several parameters could affect the result. [5] suggests that the accuracy on MNIST could reach a test error of 0.95%. The source code uses optimization of the conjugate gradient, which is not implemented in this library. This could cause the difference."}, {"heading": "10.6 Deep Canonical Correlation Analysis", "text": "The main function to execute DCCA is in runDCCA.cpp. The test data set is CIFAR-10. Each image is divided into two views, the left and the right half, and then two networks are built that are trained on two views. Each view of the data has a dimension of 512. Each network returns the output of dimension 20. The DCCA could quickly find the solution where the sum of the top 20 correlations is 10,8756. Without a network below the CCA layer, the solution could not be calculated in a reasonable time. A test on AvLetters was also performed. Unfortunately, the correlation given is close to 0. This suggests that there is a very weekly linear dependency between the data of two models, so it should be a different form of the existing dependency."}, {"heading": "10.7 Modal Prediction", "text": "Modal prediction is implemented in modal prediction. It trains modality using audio and video data from pronouncing letters from \"A\" to \"G\" and then uses audio data from the letters \"H\" and \"I\" to predict video data from \"H\" and \"I.\" The reconstruction error is 10.46%."}], "references": [{"title": "Using articulatory measurements to learn better acoustic features", "author": ["Galen Andrew", "Raman Arora", "Sujeeth Bharadwaj", "Jeff Bilmes", "Mark Hasegawa-Johnson", "Karen Livescu"], "venue": "In Speech Production in Automatic Speech Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Deep canonical correlation analysis", "author": ["Galen Andrew", "Raman Arora", "Jeff Bilmes", "Karen Livescu"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Stochastic optimization for pca and pls", "author": ["Raman Arora", "Andrew Cotter", "Karen Livescu", "Nathan Srebro"], "venue": "In Allerton Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Stochastic optimization of pca with capped msg", "author": ["Raman Arora", "Andy Cotter", "Nati Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Kernel cca for multi-view learning of acoustic features using articulatory measurements", "author": ["Raman Arora", "Karen Livescu"], "venue": "In MLSLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Multi-view cca-based acoustic features for phonetic recognition across speakers and domains", "author": ["Raman Arora", "Karen Livescu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Multi-view learning with supervision for transformed bottleneck features", "author": ["Raman Arora", "Karen Livescu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Self-organizing neural network that discovers surfaces in random-dot stereograms", "author": ["Suzanna Becker", "Geoffrey E Hinton"], "venue": "Nature, 355(6356):161\u2013163,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "Multiview acoustic feature learning using articulatory measurements", "author": ["Sujeeth Bharadwaj", "Raman Arora", "Karen Livescu", "Mark Hasegawa-Johnson"], "venue": "In Intl. Workshop on Stat. Machine Learning for Speech Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["David R Hardoon", "Sandor Szedmak", "John Shawe-Taylor"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "A practical guide to training restricted boltzmann machines. Momentum", "author": ["Geoffrey Hinton"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "The \u201dwake-sleep\u201d algorithm for unsupervised neural networks", "author": ["Geoffrey E Hinton", "Peter Dayan", "Brendan J Frey", "Radford M Neal"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Relations between two sets of variates", "author": ["Harold Hotelling"], "venue": "Biometrika, pages 321\u2013377,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1936}, {"title": "Nonlinear canonical correlation analysis by neural networks", "author": ["William W Hsieh"], "venue": "Neural Networks,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Learning multiple layers of features from tiny", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "A neural implementation of canonical correlation analysis", "author": ["Pei Ling Lai", "Colin Fyfe"], "venue": "Neural Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["Pei Ling Lai", "Colin Fyfe"], "venue": "International Journal of Neural Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["Hugo Larochelle", "Yoshua Bengio"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Representational power of restricted boltzmann machines and deep belief networks", "author": ["Nicolas Le Roux", "Yoshua Bengio"], "venue": "Neural computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Extraction of visual features for lipreading", "author": ["Iain Matthews", "Timothy F Cootes", "J Andrew Bangham", "Stephen Cox", "Richard Harvey"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Efficient learning of deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Hugo Larochelle"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["Ruslan Salakhutdinov", "Andriy Mnih", "Geoffrey Hinton"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Mean field theory for sigmoid belief networks", "author": ["Lawrence K Saul", "Tommi Jaakkola", "Michael I Jordan"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1996}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan R Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Reconstruction of articulatory measurements with smoothed low-rank matrix completion", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu"], "venue": "In Spoken Language Technology Workshop (SLT), 2014 IEEE,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "On deep multi-view representation learning", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff A Bilmes"], "venue": "In Proceedings of ICASSP,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Stochastic optimization for deep cca via nonlinear orthogonal iterations", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Nathan Srebro"], "venue": "Proceedings of the 53rd Annual Allerton Conference on Communication, Control and Computing (ALLERTON),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "X-ray microbeam speech production database", "author": ["John Westbury", "Paul Milenkovic", "Gary Weismer", "Raymond Kent"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1990}], "referenceMentions": [{"referenceID": 27, "context": "The Sigmoid Belief Network [29] is a type of the Belief Network such that P (Xi = 1|Pa(Xi)) = \u03c3( \u2211", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "A Restricted Boltzmann Machine (RBM) [20, 21, 28] is a Markov Random Field consisting of one hidden layer and one visible layer.", "startOffset": 37, "endOffset": 49}, {"referenceID": 20, "context": "A Restricted Boltzmann Machine (RBM) [20, 21, 28] is a Markov Random Field consisting of one hidden layer and one visible layer.", "startOffset": 37, "endOffset": 49}, {"referenceID": 26, "context": "A Restricted Boltzmann Machine (RBM) [20, 21, 28] is a Markov Random Field consisting of one hidden layer and one visible layer.", "startOffset": 37, "endOffset": 49}, {"referenceID": 10, "context": "3 Tricks in Restricted Boltzmann Machine Training Dropout Dropout [11] is a method to prevent neural networks from overfitting by randomly blocking emissions from certain neurons.", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "A Deep Neural Network (DNN) [8] is a neural network with multiple hidden layers.", "startOffset": 28, "endOffset": 31}, {"referenceID": 12, "context": "After pretraining is done, use Up-Down algorithm [13] as fine tuning, which is a combination of the training of RBM, and an algorithm called Wake-Sleep algorithm [12] which is for learning of the Sigmoid Belief Network.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "After pretraining is done, use Up-Down algorithm [13] as fine tuning, which is a combination of the training of RBM, and an algorithm called Wake-Sleep algorithm [12] which is for learning of the Sigmoid Belief Network.", "startOffset": 162, "endOffset": 166}, {"referenceID": 13, "context": "Autoencoder(AE) [14] is a type of neural network forming a directed graph.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "1 Logic of Deep Boltzmann Machine A Deep Boltzmann Machine(DBM) [26] is a Markov Random Field consisting of multiple layers.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "3 Mean Field Inference The mean field inference [27] of the Deep Boltzmann Machine involves iterative updates of the approximations Q.", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "1 Canonical Correlation Analysis Canonical Correlation Analysis [10] is a method to find the relationship between two sets of variables.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "(90) The solution [15] to this problem suggests that a and b are respectively the eigenvectors of the following matrices Cov(x, x)\u22121Cov(x, y)Cov(y, y)\u22121Cov(y, x), (91) Cov(y, y)\u22121Cov(y, x)Cov(x, x)\u22121Cov(x, y).", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 82, "endOffset": 93}, {"referenceID": 4, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 82, "endOffset": 93}, {"referenceID": 5, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 82, "endOffset": 93}, {"referenceID": 6, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 82, "endOffset": 93}, {"referenceID": 8, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 82, "endOffset": 93}, {"referenceID": 2, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 185, "endOffset": 195}, {"referenceID": 3, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 185, "endOffset": 195}, {"referenceID": 32, "context": "CCA based techniques have been applied to XRMB data for acoustic feature learning [1, 5\u20137, 9] and various stochastic approximation algorithms have been proposed for multi-view learning [3, 4, 34].", "startOffset": 185, "endOffset": 195}, {"referenceID": 15, "context": "2 Kernel Canonical Correlation Analysis Kernel Canonical Correlation Analysis [16, 19] projects the data into a higher-dimensional feature space before linear projections using the mapping \u03c6 : x\u2192 \u03c6(x).", "startOffset": 78, "endOffset": 86}, {"referenceID": 18, "context": "2 Kernel Canonical Correlation Analysis Kernel Canonical Correlation Analysis [16, 19] projects the data into a higher-dimensional feature space before linear projections using the mapping \u03c6 : x\u2192 \u03c6(x).", "startOffset": 78, "endOffset": 86}, {"referenceID": 1, "context": "3 Deep Correlation Analysis In Deep Correlation Analysis (DCCA) [2,18,33], A layer of KCCA is added on top of two separately trained Deep Neural Networks, of which each learns the data of one modal.", "startOffset": 64, "endOffset": 73}, {"referenceID": 17, "context": "3 Deep Correlation Analysis In Deep Correlation Analysis (DCCA) [2,18,33], A layer of KCCA is added on top of two separately trained Deep Neural Networks, of which each learns the data of one modal.", "startOffset": 64, "endOffset": 73}, {"referenceID": 31, "context": "3 Deep Correlation Analysis In Deep Correlation Analysis (DCCA) [2,18,33], A layer of KCCA is added on top of two separately trained Deep Neural Networks, of which each learns the data of one modal.", "startOffset": 64, "endOffset": 73}, {"referenceID": 30, "context": "Deep Canonically Correlated Autoencoders (DCCAE) [32] is an extension of DCCA.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "2 Modal Prediction and Transformation It is possible to make lower-dimension representation of two modals by building a Bimodal Deep Belief Network [25] as in Figure 8.", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": "Another option is to build a Markov Random Field multimodal learning model [30] by combining two Deep Boltzmann Machines.", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "1 MNIST MNIST [22] is a selected set of samples from NIST data set.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "2 CIFAR The CIFAR-10 data set [17] consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "3 XRMB In Wisconsin X-ray Microbeam Database (XRMB) [31, 35], the dimension of the MFCC data is 273 and the dimension of the XRMB data is 112.", "startOffset": 52, "endOffset": 60}, {"referenceID": 33, "context": "3 XRMB In Wisconsin X-ray Microbeam Database (XRMB) [31, 35], the dimension of the MFCC data is 273 and the dimension of the XRMB data is 112.", "startOffset": 52, "endOffset": 60}, {"referenceID": 22, "context": "4 AvLetters AvLetters [24] is the data set recording audio data and video data of different people uttering letters.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "[6] indicates the best performance of DBN on MNIST could achieve the error rate of 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] indicates that the accuracy on MNIST could achieve test error of 0.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "The Neural Network is a directed graph consists of multiple layers of neurons, which is also referred to as units. In general there is no connection between units of the same layer and there are only connections between adjacent layers. The first layer is the input and is referred to as visible layer v. Above the visible layer there are multiple hidden layers {h1, h2, ..., hn}. And the output of the last hidden layer forms the output layer o.", "creator": "LaTeX with hyperref package"}}}