{"id": "1512.07046", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2015", "title": "News Across Languages - Cross-Lingual Document Similarity and Event Tracking", "abstract": "In today's world, we follow news which is distributed globally. Significant events are reported by different sources and in different languages. In this work, we address the problem of tracking of events in a large multilingual stream. Within a recently developed system Event Registry we examine two aspects of this problem: how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event. Taking a multilingual stream and clusters of articles from each language, we compare different cross-lingual document similarity measures based on Wikipedia. This allows us to compute the similarity of any two articles regardless of language. Building on previous work, we show there are methods which scale well and can compute a meaningful similarity between articles from languages with little or no direct overlap in the training data. Using this capability, we then propose an approach to link clusters of articles across languages which represent the same event. We provide an extensive evaluation of the system as a whole, as well as an evaluation of the quality and robustness of the similarity measure and the linking algorithm.", "histories": [["v1", "Tue, 22 Dec 2015 12:11:32 GMT  (1347kb,D)", "http://arxiv.org/abs/1512.07046v1", "Accepted for publication in Journal of Artificial Intelligence Research, Special Track on Cross-language Algorithms and Applications"]], "COMMENTS": "Accepted for publication in Journal of Artificial Intelligence Research, Special Track on Cross-language Algorithms and Applications", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["jan rupnik", "rej muhic", "gregor leban", "primoz skraba", "blaz fortuna", "marko grobelnik"], "accepted": false, "id": "1512.07046"}, "pdf": {"name": "1512.07046.pdf", "metadata": {"source": "CRF", "title": "News Across Languages - Cross-Lingual Document Similarity and Event Tracking", "authors": ["Jan Rupnik", "Andrej Muhi\u010d", "Gregor Leban", "Primo\u017e \u0160kraba", "Bla\u017e Fortuna", "Marko Grobelnik"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Pipeline", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2.1 System Requirements", "text": "Our approach consists of two steps: Track events separately in each language (based on speech recognition and an online clustering approach) and then connect them to each other; the pipeline must be able to process millions of items per day and perform billions of similarity calculations per day; both steps rely heavily on similarity calculations, which means that this must be highly scalable; therefore, we focus on implementations that run on a single shared storage machine, as opposed to clusters of machines; this simplifies the implementation and maintenance of the system; in summary, the following features are desirable: \u2022 Training - Training (the creation of cross-language models) should be scalable to many languages and robust in terms of the quality of training resources; the system should be able to take advantage of comparable companies (as opposed to parallel, translation-based companies), with a lack of data. \u2022 Operational efficiency - The similarity calculation should be fast - the system must be capable of documenting billions of similarities within a day."}, {"heading": "3 Related work", "text": "In this section, we will describe previous work described in the literature. Since there are two characteristic tasks that we will address in this paper (calculating linguistic document similarity and linking linguistic clusters), we have divided the associated work into two corresponding parts."}, {"heading": "3.1 Cross-lingual document similarity", "text": "The most obvious way to compare documents written in different languages is to use machine translation and perform monolingual similarity, see [23, 25] for several variations of translation-based approaches. One can use free tools such as Moses [14] or translation services such as Google Translate (https: / / translate.google.com /), which are typically used. There are two problems with such approaches: they solve a more difficult problem than needs to be solved, and they are less robust to training resources - large sets of translated sentences are typically required. Therefore, Training Moses for languages with scarce linguistic resources is problematic. The problem with using online services such as Google Translate is that the APIs are limited and not free. Operational efficiency and cost requirements make translation-based approaches less suitable for our system."}, {"heading": "3.2 Cross-lingual cluster linking", "text": "Although there are a number of services that aggregate messages by identifying clusters of similar articles, there are virtually no services that provide linking of clusters across different languages. Google News and Yahoo! News are able to identify clusters of articles about the same event, but do not offer linking of clusters across languages. The only service we have found is the European Media Monitor (EMM) [27, 35]. EMM bundles articles in 60 languages and then tries to determine which clusters of articles in different languages describe the same event. To achieve a cluster linkage, EMM uses three different language-independent vector representations for each cluster. The first vector contains the weighted list of references to countries mentioned in the articles, while the second vector contains the weighted list of persons and organizations mentioned. The last vector contains the weighted list of Eurovoc molecule subject descriptors, which are automatically related to each other, and environmental criteria, such as air quality control."}, {"heading": "4 Cross-lingual Document Similarity", "text": "Document similarity is an important component of text mining and natural language processing techniques. Many techniques use similarity as a black box, e.g. as a core in Support Vector Machines. Comparing documents (or other types of text snippets) in a monolingual environment is a well-researched problem in the field of information gathering [33]. First, we formally present the problem, followed by a description of our approach."}, {"heading": "4.1 Problem definition", "text": "We will first describe how documents are presented as vectors and how documents are compared in a monolingual environment. (We will then define a way to measure multilingual similarity, which is natural for the models we are looking at.) The standard vector space model [33] represents documents as vectors where each term corresponds to a word or phrase in a fixed vocabulary. Formally, document d is represented by a vector x Rn, where n corresponds to the size of the vocabulary, and vector elements xk correspond to the number of times in which the term k occurred in the document, also referred to as Term Frequency or TFk (d). We also used a vector weight scheme that adapts to the fact that some words occur more frequently in general. A term weight should correspond to the meaning of the term for the given corpus. The common weight scheme is called Term Frequency Inquency Frequency Inverse Frequency (DF)."}, {"heading": "4.2 Cross-Lingual Models", "text": "In this section, we will describe several approaches to the problem of calculating the multilingual similarities introduced in Section 4.1. We will present four approaches: a simple approach based on clustering k means in Section 4.4, a standard approach based on the decomposition of singular values in Section 4.5, a related approach called Canonical Correlation Analysis (CCA) in Section 4.6, and finally a new method that represents an extension of CCA to more than two languages in Section 4.7. CCA can be used to find correlated patterns for a language pair, while the advanced method optimizes a sum of square correlations (SSCOR) between several language pairs introduced in [12]. SSCOR is difficult to solve in our environment (hundreds of thousands of features, hundreds of thousands of examples). To address this, we propose a method that consists of two ingredients. The first one is based on the observation that certain data sets (such as Wikipedia) can be optimized on a second level."}, {"heading": "4.3 Notation", "text": "In fact, it is the case that most of us are able to move into another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live."}, {"heading": "4.5 Cross-Lingual Latent Semantic Indexing", "text": "The second approach we are looking at is Cross-Lingual Latent Semantic Indexing (CL-LSI) [4], which is a variant of LSI [3] for more than one language. It is very similar to the k mean, where we first link the corpus matrices, calculate a decomposition that, in the case of CL-LSI, is an abbreviated Singular ValueDecomposition (SVD), decouple the slit space matrix and use the blocks to calculate linear maps to a common vector space in which the standard cosine similarity is used to compare documents. It is based on the calculation of an abbreviated singular value, which is the decomposition of the concatenated corpus matrix X-T. See Figure 5 for decomposition. The presentation of documents in Topic \"Utilities\" is the same way as in the case of k averages."}, {"heading": "4.6 Canonical Correlation Analysis", "text": "We now present a statistical technique for analyzing data from two sources, the extension of which will be presented in the next section. Canonical Correlation Analysis (CCA) [11] is a dimensionality reduction technology similar to Principal Component Analysis (PCA) [22], with the additional assumption that the data consists of feature vectors derived from two sources (two views) that share some information. Examples include: bilingual document collection [5] and collection of images and captions [9]. Instead of looking for linear combinations of features that maximize variance (PCA), we are looking for a linear combination of feature vectors from the first view and a linear combination of feature vectors from the second view that are maximally corrected.The columns of Xi are interpreted as observation vectors from an underlying distribution Xi and i."}, {"heading": "4.7 Hub language based CCA Extension", "text": "The first problem relates to missing alignment data: if a number of languages are large, the dataset of documents covering all languages may be small (or even blank); even if only two languages are taken into account, the set of aligned documents may be small (an extreme example is given by the Piedmontese and Hindi-speaking Wikipedias, where no interlingual links are available), in which case none of the methods presented so far are applicable; the second challenge is that the data is high-dimensional (many languages with hundreds of thousands of characteristics per language) and the number of multilingual documents can be large (over a million in Wikipedia); CCA's optimization problem is not trivial to solve the covariance matrices themselves: the covariance matrices are prohibitively large to fit into the memory (even storing 100,000 elements requires 80GB of memory)."}, {"heading": "5 Cross-lingual Event Linking", "text": "The main application by which we test the cross-lingual similarity is the cross-lingual linkage of events. In online media streams - especially news articles - there is often a duplication of reports, different viewpoints or opinions, which all revolve around a single event. The same events are covered in many articles, and the question we ask ourselves is how to find all articles in different languages describing a single event. In this essay, we look at the problem of the coincidence of events from different languages. We do not address the problem of event detection and instead base our evaluation on an online system for detecting world events, Event Registry. Events are represented by article clusters, and thus our problem is ultimately reduced to finding suitable mappings between clusters and articles in different languages."}, {"heading": "5.1 Problem definition", "text": "The problem of cross-language event linkage is to synchronize monolingual clusters of news articles describing the same event in different languages. For example, we want a cluster of Spanish news articles and a cluster of English news articles describing the same earthquake. Specifically, the articles corresponding to each cluster c \"are written in one language,\" with \"a,\" \"L,\" \"2,\" \"m.\" For each language, \"we get a set of monolingual clusters C.\" More specifically, the articles corresponding to each cluster c \"C\" are written in the language. \"Given a language pair of\" a, \"b\" L \"and\" a \"6 =\" b, \"we would like to identify all cluster pairs (ci, cj) so that ci and cj describe the same event. The assignment of clusters is a generalized matching of clusters that is a matching of clusters (we cannot assume per cluster that there is at least one full coverage per event, there is only one cluster per event per language)."}, {"heading": "5.2 Algorithm", "text": "In order to identify clusters that are similar to the clusters, we have developed a two-step algorithm that we cite as an example. \"We,\" it says, \"must first identify a small group of clusters and then find these clusters.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" \"We.\" \"\" \"\" \"We.\". \"\" \"\" We. \".\" \"\" \"\" We. \"\". \"\" \"\" We. \"\" \".\" \"\" We. \".\" \"\" \"We.\". \"\" \"\" We. \".\" \"\" \"We.\". \"\" \"\" We. \".\" \"\" \"We.\".. \"\" \"\" \"We.\". \".\" \"\" \"We.\". \".\". \"\" We. \".\". \".\" \"\" \"We...\" \"\" \".\" We... \"\". \"\" \"\" We..... \"\" \"\" \"\" We....... \"\" \"\" \"\" \"We.........\" \"\" \"\" \"\" We............ \"\" \"\" \"\" \"We...........\" \"\" \"\" \"\" \"We...........\" \"\" \"\" \"\" \"We.................\" \"\" \"\" \"\" \"\" \"We.....................\" \"\" \"\" \"\" \"\" \"\" We................ \"\" \"\" \"\" \"\" \"\" \"\" \"\".. \"\" \"\" \"\" \"We.......................................\" \"\" \"\" \"\""}, {"heading": "6 Evaluation", "text": "We will describe the most important data set for building cross-language models, which is based on Wikipedia, and then present two sets of experiments: the first set states that the hub-based approach can handle language pairs for which little or no training data is available; the second set compares the most important approaches we have presented to the task of partner search and event linkage; and finally, we will examine how different choices of traits affect event linkage."}, {"heading": "6.1 Wikipedia Comparable Corpus", "text": "To investigate the empirical performance of low-rank approximations, we will test the algorithms on a large-scale, real multilingual dataset that we have extracted from Wikipedia using interlingual links for alignment, resulting in a large number of poorly comparable documents in more than 200 languages. Wikipedia is a large source of multilingual data that is particularly important for languages for which no translation tools, multilingual dictionaries such as Eurovoc [29], or highly aligned multilingual corporations such as Europarl [13] are available. Documents in various languages are linked to so-called \"inter-language\" links that can be found on the left-hand side of the Wikipedia page. Wikipedia is constantly growing. There are currently 12 Wikipedias with more than 1 million articles, 52 with more than 100k articles, 129 with more than 10k articles, and 236 with more than 1 000 articles."}, {"heading": "6.2 Experiments With Missing Alignment Data", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times. \""}, {"heading": "6.3 Evaluation Of Cross-Lingual Event Linking", "text": "This year is the highest in the history of the country."}, {"heading": "6.4 Remarks on the scalability of the implementation", "text": "One of the main advantages of our approach is that it is highly scalable. It is fast, very robust to the quality of training data, easily expandable, easy to implement and has relatively low hardware requirements. The similarity pipeline is the most computationally intensive part and currently runs on a machine with two Intel Xeon E5-2667 v2, 3.30GHz processors with 256GB of RAM. This is enough to make similarities in terms of a large number of languages if necessary. It currently uses Wikipedia as an open knowledge base and experiments that show that the similarity dramatically reduces the search space when linking clusters together. Currently, we calculate similarities in 24 languages we work with: eng, spa, deu, ita, fra, swe, nld, jpn, ara, fin, ron, kor, hrv, tmost."}, {"heading": "7 Discussion and Future Work", "text": "In this paper, we have presented a lingual system for linking events in different languages. Building on an existing system, the event register, we present and evaluate several approaches for calculating a lingual similarity function. We also present an approach for linking events and evaluating the effectiveness of different characteristics. The final pipeline is scalable in terms of both the number of articles and the number of languages, with events being precisely linked to each other. In the task of partner search, we observe that the refinement of LSI-based projections with the CCA hub leads to improved retrieval precision, but the methods for linking events are comparable. Further research showed that the CCA-based approach achieved higher precision on smaller clusters. The interpretation is that linking functions for large clusters are highly aggregated, which means that the lower precision per document in linking LSI is lost in the Wikipedia linking area, which is a possible reason for us to compensate the other languages."}, {"heading": "7.1 Future Work", "text": "Currently, the system is loosely coupled - the language component is structured independently of the rest of the system, especially the connection component. It is possible that better embedding can be achieved through methods that jointly optimize a classification task and embedding. Another interesting point is the evaluation of the system for languages with scarce linguistic resources, where semantic annotations may not be available. To this end, the labeled dataset of linked clusters should first be expanded. Analysis of the partner search revealed that even when language pairs do not overlap, the hub CCA restores some signals. To further improve the performance of the classifier for cluster linkage, additional features should also be extracted from articles and clusters and checked to see if they can increase the accuracy of the classification. As the amount of linguistic resources varies considerably from language to language, it would also make sense to build a separate classifier for each language. Intuitively, this performance should be improved, as the weighting of each language could be adapted to the language."}], "references": [{"title": "A high-performance multithreaded approach for clustering a stream of documents", "author": ["Janez Brank", "Gregor Leban", "Marko Grobelnik"], "venue": "In Proceedings of the 17th International Multiconference Information Society", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Generalization of canonical correlation analysis to three or more sets of variables", "author": ["J.D. Carroll"], "venue": "Proceedings of the American Psychological Association,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1968}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "T.K. Landauer", "G.W. Furnas", "R.A. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "Automatic cross-language retrieval using latent semantic indexing. In AAAI spring symposium on cross-language text and speech retrieval", "author": ["S.T. Dumais", "T.A. Letsche", "M.L. Littman", "T.K Landauer"], "venue": "American Association for Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Kernel methods in bioengineering, communications and image processing, chapter A Kernel Canonical Correlation Analysis For Learning The Semantics Of Text, pages 263\u2013282", "author": ["B. Fortuna", "N. Cristianini", "J. Shawe-Taylor"], "venue": "Idea Group Publishing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Nonlinear Multivariate Analysis", "author": ["Albert Gifi"], "venue": "Wiley Series in Probability and Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.G. Martinsson", "J.A. Tropp"], "venue": "Society for Industrial and Applied Mathematics Review,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Using image stimuli to drive fmri analysis", "author": ["David R Hardoon", "Janaina Mourao-Miranda", "Michael Brammer", "John Shawe-Taylor"], "venue": "In Neural Information Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Clustering algorithms", "author": ["John Hartigan"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1975}, {"title": "The most predictable criterion", "author": ["Harold Hotelling"], "venue": "Journal of educational Psychology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1935}, {"title": "Canonical analysis of several sets of variables", "author": ["J.R. Kettenring"], "venue": "Biometrika, 58:433\u201345,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1971}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In The Tenth Machine Translation Summit,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst. Moses"], "venue": "In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Cross-lingual detection of world events from news articles", "author": ["Gregor Leban", "Blaz Fortuna", "Janez Brank", "Marko Grobelnik"], "venue": "In Proceedings of the 13th International Semantic Web Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Event registry: Learning about world events from news", "author": ["Gregor Leban", "Blaz Fortuna", "Janez Brank", "Marko Grobelnik"], "venue": "In Proceedings of the Companion Publication of the 23rd International Conference on World Wide Web Companion, WWW Companion", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Gdelt: Global data on events, location, and tone, 1979\u20132012", "author": ["Kalev Leetaru", "Philip A Schrodt"], "venue": "In International Studies Association (ISA) Annual Convention,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Distance between sets", "author": ["Michael Levandowsky", "David Winter"], "venue": "Nature, 234(5323):34\u201335,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1971}, {"title": "Learning to link with wikipedia", "author": ["David Milne", "Ian H. Witten"], "venue": "In Proceedings of the 17th ACM Conference on Information and Knowledge Management,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Polylingual topic models", "author": ["David Mimno", "Hanna M. Wallach", "Jason Naradowsky", "David A. Smith", "Andrew McCallum"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Cross-lingual document similarity. In Information Technology Interfaces (ITI)", "author": ["Andrej Muhi\u010d", "Jan Rupnik", "Primo\u017e \u0160kraba"], "venue": "Proceedings of the ITI 2012 34th International Conference on,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "Philosophical Magazine,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1901}, {"title": "Multilingual Information Retrieval", "author": ["Carol Peters", "Martin Braschler"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Translingual document representations from discriminative projections", "author": ["John C Platt", "Kristina Toutanova", "Wen-tau Yih"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Cross-language plagiarism detection", "author": ["Martin Potthast", "Alberto Barr\u00f3n-Cede\u00f1o", "Benno Stein", "Paolo Rosso"], "venue": "Language Resources and Evaluation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "A wikipedia-based multilingual retrieval model. In Advances in Information Retrieval", "author": ["Martin Potthast", "Benno Stein", "Maik Anderka"], "venue": "European Conference on Information Retrieval Research (ECIR),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Story tracking: linking similar news over time and across languages", "author": ["Bruno Pouliquen", "Ralf Steinberger", "Olivier Deguernel"], "venue": "In Proceedings of the Workshop on Multi-source Multilingual Information Extraction and Summarization,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Automatic annotation of multilingual text collections with a conceptual thesaurus", "author": ["Bruno Pouliquen", "Ralf Steinberger", "Camelia Ignat"], "venue": "arXiv preprint cs/0609059,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "Promoting government controlled vocabularies for the semantic web: the eurovoc thesaurus and the cpv product classification system", "author": ["Jose Ma\u0155\u0131a \u00c1lvarez Rod\u0155\u0131guez", "Emilio Rubiera Azcona", "Luis Polo Paredes"], "venue": "Semantic Interoperability in the European Digital Library,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Low-rank approximations for large, multi-lingual data. Low Rank Approximation and Sparse Representation, Neural Information Processing Systems", "author": ["Jan Rupnik", "Andrej Muhic", "Primoz Skraba"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Spanning spaces: Learning cross-lingual similarities. Beyond Mahalanobis: Supervised Large-Scale Learning of Similarity, Neural Information Processing Systems", "author": ["Jan Rupnik", "Andrej Muhic", "Primoz Skraba"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Multilingual document retrieval through hub languages", "author": ["Jan Rupnik", "Andrej Muhic", "Primoz Skraba"], "venue": "In Proceedings of the 15th Multiconference on Information Society 2012 (IS-2012),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Gerard Salton", "Christopher Buckley"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1988}, {"title": "Kernel Methods for Pattern Analysis", "author": ["John Shawe-Taylor", "Nello Cristianini"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Newsexplorer: Multilingual news analysis with cross-lingual linking", "author": ["Ralf Steinberger", "Bruno Pouliquen", "Camelia Ignat"], "venue": "Information Technology Interfaces,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2005}, {"title": "The internals of an aggregated web news feed", "author": ["Mitja Trampu\u0161", "Bla\u017e Novak"], "venue": "In Proceedings of 15th Multiconference on Information Society 2012 (IS-2012),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "The trec-8 question answering track report", "author": ["Ellen M Voorhees"], "venue": "In Proceedings of the 8th Text Retrieval Conference (TREC-8),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1999}, {"title": "A novel two-step method for cross language representation learning", "author": ["Min Xiao", "Yuhong Guo"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Cross-lingual latent topic extraction", "author": ["Duo Zhang", "Qiaozhu Mei", "ChengXiang Zhai"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Semantic annotation, analysis and comparison: A multilingual and cross-lingual text analytics toolkit", "author": ["Lei Zhang", "Achim Rettinger"], "venue": "In Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "X-lisa: Cross-lingual semantic annotation", "author": ["Lei Zhang", "Achim Rettinger"], "venue": "Proceedings of the Very Large Data Bases (VLDB) Endowment,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "Within a recently developed system Event Registry [16, 15] we examine two aspects of this problem: how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event.", "startOffset": 50, "endOffset": 58}, {"referenceID": 13, "context": "Within a recently developed system Event Registry [16, 15] we examine two aspects of this problem: how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event.", "startOffset": 50, "endOffset": 58}, {"referenceID": 14, "context": "Within each monolingual stream, an online clustering approach is employed, where tracked clusters correspond to our definition of events - this is based on the Event Registry system [16, 15].", "startOffset": 182, "endOffset": 190}, {"referenceID": 13, "context": "Within each monolingual stream, an online clustering approach is employed, where tracked clusters correspond to our definition of events - this is based on the Event Registry system [16, 15].", "startOffset": 182, "endOffset": 190}, {"referenceID": 28, "context": "The first ingredient of our approach to link clusters across languages represents a continuation of previous work [30, 32, 31, 21] where we explored representations of documents which were valid over multiple languages.", "startOffset": 114, "endOffset": 130}, {"referenceID": 30, "context": "The first ingredient of our approach to link clusters across languages represents a continuation of previous work [30, 32, 31, 21] where we explored representations of documents which were valid over multiple languages.", "startOffset": 114, "endOffset": 130}, {"referenceID": 29, "context": "The first ingredient of our approach to link clusters across languages represents a continuation of previous work [30, 32, 31, 21] where we explored representations of documents which were valid over multiple languages.", "startOffset": 114, "endOffset": 130}, {"referenceID": 19, "context": "The first ingredient of our approach to link clusters across languages represents a continuation of previous work [30, 32, 31, 21] where we explored representations of documents which were valid over multiple languages.", "startOffset": 114, "endOffset": 130}, {"referenceID": 14, "context": "We base our techniques of cross-lingual event linking on an online system for detection of world events, called Event Registry [16, 15].", "startOffset": 127, "endOffset": 135}, {"referenceID": 13, "context": "We base our techniques of cross-lingual event linking on an online system for detection of world events, called Event Registry [16, 15].", "startOffset": 127, "endOffset": 135}, {"referenceID": 34, "context": "The collection of the news articles is performed using the Newsfeed service [36].", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "The details are reported in [19] and [40].", "startOffset": 28, "endOffset": 32}, {"referenceID": 38, "context": "The details are reported in [19] and [40].", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "Details of the clustering approach are described in [1].", "startOffset": 52, "endOffset": 55}, {"referenceID": 21, "context": "The most obvious way to compare documents written in different languages is to use machine translation and perform monolingual similarity, see [23, 25] for several variations of translation based approaches.", "startOffset": 143, "endOffset": 151}, {"referenceID": 23, "context": "The most obvious way to compare documents written in different languages is to use machine translation and perform monolingual similarity, see [23, 25] for several variations of translation based approaches.", "startOffset": 143, "endOffset": 151}, {"referenceID": 12, "context": "One can use free tools such as Moses [14] or translation services, such as Google Translate (https://translate.", "startOffset": 37, "endOffset": 41}, {"referenceID": 23, "context": "Closely related are works Cross-Lingual Vector Space Model (CL-VSM) [25] and the approach presented in [27] which both compare documents by using dictionaries, which in both cases are EuroVoc dictionaries [29].", "startOffset": 68, "endOffset": 72}, {"referenceID": 25, "context": "Closely related are works Cross-Lingual Vector Space Model (CL-VSM) [25] and the approach presented in [27] which both compare documents by using dictionaries, which in both cases are EuroVoc dictionaries [29].", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "Closely related are works Cross-Lingual Vector Space Model (CL-VSM) [25] and the approach presented in [27] which both compare documents by using dictionaries, which in both cases are EuroVoc dictionaries [29].", "startOffset": 205, "endOffset": 209}, {"referenceID": 22, "context": "The models include: Joint Probabilistic Latent Semantic Analysis (JPLSA) [24], Coupled Probabilistic LSA (CPLSA) [24], Probabilistic Cross-Lingual LSA (PCLLSA) [39] and Polylingual Topic Models (PLTM) [20] which is a Bayesian version of PCLLSA.", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "The models include: Joint Probabilistic Latent Semantic Analysis (JPLSA) [24], Coupled Probabilistic LSA (CPLSA) [24], Probabilistic Cross-Lingual LSA (PCLLSA) [39] and Polylingual Topic Models (PLTM) [20] which is a Bayesian version of PCLLSA.", "startOffset": 113, "endOffset": 117}, {"referenceID": 37, "context": "The models include: Joint Probabilistic Latent Semantic Analysis (JPLSA) [24], Coupled Probabilistic LSA (CPLSA) [24], Probabilistic Cross-Lingual LSA (PCLLSA) [39] and Polylingual Topic Models (PLTM) [20] which is a Bayesian version of PCLLSA.", "startOffset": 160, "endOffset": 164}, {"referenceID": 18, "context": "The models include: Joint Probabilistic Latent Semantic Analysis (JPLSA) [24], Coupled Probabilistic LSA (CPLSA) [24], Probabilistic Cross-Lingual LSA (PCLLSA) [39] and Polylingual Topic Models (PLTM) [20] which is a Bayesian version of PCLLSA.", "startOffset": 201, "endOffset": 205}, {"referenceID": 36, "context": "The models include: Non-negative matrix factorization based [38], Cross-Lingual Latent Semantic Indexing CLLSI [4, 23], Canonical Correlation Analysis (CCA) [11], Oriented Principal Component Analysis (OPCA) [24].", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "The models include: Non-negative matrix factorization based [38], Cross-Lingual Latent Semantic Indexing CLLSI [4, 23], Canonical Correlation Analysis (CCA) [11], Oriented Principal Component Analysis (OPCA) [24].", "startOffset": 111, "endOffset": 118}, {"referenceID": 21, "context": "The models include: Non-negative matrix factorization based [38], Cross-Lingual Latent Semantic Indexing CLLSI [4, 23], Canonical Correlation Analysis (CCA) [11], Oriented Principal Component Analysis (OPCA) [24].", "startOffset": 111, "endOffset": 118}, {"referenceID": 9, "context": "The models include: Non-negative matrix factorization based [38], Cross-Lingual Latent Semantic Indexing CLLSI [4, 23], Canonical Correlation Analysis (CCA) [11], Oriented Principal Component Analysis (OPCA) [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 22, "context": "The models include: Non-negative matrix factorization based [38], Cross-Lingual Latent Semantic Indexing CLLSI [4, 23], Canonical Correlation Analysis (CCA) [11], Oriented Principal Component Analysis (OPCA) [24].", "startOffset": 208, "endOffset": 212}, {"referenceID": 36, "context": "For our setting, the method in [38] has a prohibitively high computational cost when building models (it uses dense matrices whose dimensions are a product of the training set size and the vocabulary size).", "startOffset": 31, "endOffset": 35}, {"referenceID": 24, "context": "Analysis (CL-ESA) [26], which uses Wikipedia (as do we in the current work) to compare documents.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "We find that the scalability argument does not apply in our case: based on advances in numerical linear algebra we can solve large CL-LSI problems (millions of documents as opposed to the 10,000 document limit reported in [26]).", "startOffset": 222, "endOffset": 226}, {"referenceID": 23, "context": "For example, the Cross-Language Character n-Gram Model (CL-CNG) [25] represents documents as bags of character n-grams.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "Another approach is to use language dependent keyword lists based on cognate words [27].", "startOffset": 83, "endOffset": 87}, {"referenceID": 25, "context": "The only service that we found, which provides cross-lingual cluster linking, is the European Media Monitor (EMM) [27, 35].", "startOffset": 114, "endOffset": 122}, {"referenceID": 33, "context": "The only service that we found, which provides cross-lingual cluster linking, is the European Media Monitor (EMM) [27, 35].", "startOffset": 114, "endOffset": 122}, {"referenceID": 26, "context": "These descriptors are topics, such as air transport, EC agreement, competition and pollution control into which articles are automatically categorized [28].", "startOffset": 151, "endOffset": 155}, {"referenceID": 15, "context": "A system, which is significantly different but worth mentioning, is the GDELT project [17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "Comparison of documents (or other types of text snippets) in a monolingual setting is a well-studied problem in the field of information retrieval [33].", "startOffset": 147, "endOffset": 151}, {"referenceID": 31, "context": "The standard vector space model [33] represents documents as vectors, where each term corresponds to a word or a phrase in a fixed vocabulary.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "CCA can be used to find correlated patterns for a pair of languages, whereas the extended method optimizes a Sum of Squared Correlations (SSCOR) between several language pairs, which was introduced in [12].", "startOffset": 201, "endOffset": 205}, {"referenceID": 8, "context": "We then run the k-means algorithm [10] and obtain a centroid matrix C \u2208 RN\u00d7k, where the k columns represent centroid vectors.", "startOffset": 34, "endOffset": 38}, {"referenceID": 3, "context": "The second approach we consider is Cross-Lingual Latent Semantic Indexing (CL-LSI) [4] which is a variant of LSI [3] for more than one language.", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "The second approach we consider is Cross-Lingual Latent Semantic Indexing (CL-LSI) [4] which is a variant of LSI [3] for more than one language.", "startOffset": 113, "endOffset": 116}, {"referenceID": 6, "context": "Instead, we use a randomized version of the SVD [8] that can be viewed as a block Lanczos method.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "Canonical Correlation Analysis (CCA) [11] is a dimensionality reduction technique similar to Principal Component Analysis (PCA) [22], with the additional assumption that the data consists of feature vectors that arose from two sources (two views) that share some information.", "startOffset": 37, "endOffset": 41}, {"referenceID": 20, "context": "Canonical Correlation Analysis (CCA) [11] is a dimensionality reduction technique similar to Principal Component Analysis (PCA) [22], with the additional assumption that the data consists of feature vectors that arose from two sources (two views) that share some information.", "startOffset": 128, "endOffset": 132}, {"referenceID": 4, "context": "Examples include: bilingual document collection [5] and collection of images and captions [9].", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "Examples include: bilingual document collection [5] and collection of images and captions [9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "If the matrices are not invertible, one can use a regularization technique by replacing Ci,i with (1\u2212 \u03ba)Ci,i + \u03baI, where \u03ba \u2208 [0, 1] is the regularization coefficient and I is the identity matrix.", "startOffset": 125, "endOffset": 131}, {"referenceID": 10, "context": "The extension we consider is based on a generalization of CCA to more than two views, introduced in [12], namely the Sum of Squared Correlations SSCOR, which we will state formally later in this section.", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "The approach is based on the sum of squares of correlations formulation by Kettenring [12], where we consider only correlations between pairs (Y1, Yi), i > 1 due to the hub language problem characteristic.", "startOffset": 86, "endOffset": 90}, {"referenceID": 1, "context": "The technique is related to Generalization of Canonical Correlation Analysis (GCCA) by Carroll [2], where an unknown group configuration variable is defined and the objective is to maximize the sum of squared correlations between the group variable and the others.", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "The complexity of our method is O(k), where k is the reduced dimension from the LSI preprocessing step, whereas solving the GCCA method scales as O(s), where s is the number of samples (see [6]).", "startOffset": 190, "endOffset": 193}, {"referenceID": 32, "context": "The classification algorithm that we used to train a model was a linear Support Vector Machine (SVM) method [34].", "startOffset": 108, "endOffset": 112}, {"referenceID": 39, "context": "Articles that are imported into Event Registry are annotated by disambiguating mentioned entities and keywords to the corresponding Wikipedia pages [41].", "startOffset": 148, "endOffset": 152}, {"referenceID": 16, "context": "\u2022 entityJaccardSim is Jaccard similarity coefficient [18] between sets of entities from clusters ci and cj .", "startOffset": 53, "endOffset": 57}, {"referenceID": 27, "context": "Wikipedia is a large source of multilingual data that is especially important for the languages for which no translation tools, multilingual dictionaries as Eurovoc [29], or strongly aligned multilingual corpora as Europarl [13] are available.", "startOffset": 165, "endOffset": 169}, {"referenceID": 11, "context": "Wikipedia is a large source of multilingual data that is especially important for the languages for which no translation tools, multilingual dictionaries as Eurovoc [29], or strongly aligned multilingual corpora as Europarl [13] are available.", "startOffset": 224, "endOffset": 228}, {"referenceID": 31, "context": "We represent the documents as normalized TFIDF [33] weighted vectors.", "startOffset": 47, "endOffset": 51}, {"referenceID": 35, "context": "We computed the Average (over language pairs) Mean Reciprocal Rank (AMRR) [37] performance of the different approaches on the Wikipedia data by holding out 15, 000 aligned test documents and using 300, 000 aligned documents as the training set.", "startOffset": 74, "endOffset": 78}], "year": 2015, "abstractText": "In today\u2019s world, we follow news which is distributed globally. Significant events are reported by different sources and in different languages. In this work, we address the problem of tracking of events in a large multilingual stream. Within a recently developed system Event Registry [16, 15] we examine two aspects of this problem: how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event. Taking a multilingual stream and clusters of articles from each language, we compare different cross-lingual document similarity measures based on Wikipedia. This allows us to compute the similarity of any two articles regardless of language. Building on previous work, we show there are methods which scale well and can compute a meaningful similarity between articles from languages with little or no direct overlap in the training data. Using this capability, we then propose an approach to link clusters of articles across languages which represent the same event. We provide an extensive evaluation of the system as a whole, as well as an evaluation of the quality and robustness of the similarity measure and the linking algorithm.", "creator": "LaTeX with hyperref package"}}}