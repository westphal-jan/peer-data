{"id": "1512.07048", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2015", "title": "Beauty and Brains: Detecting Anomalous Pattern Co-Occurrences", "abstract": "Our world is filled with both beautiful and brainy people, but how often does a Nobel Prize winner also wins a beauty pageant? Let us assume that someone who is both very beautiful and very smart is more rare than what we would expect from the combination of the number of beautiful and brainy people. Of course there will still always be some individuals that defy this stereotype; these beautiful brainy people are exactly the class of anomaly we focus on in this paper. They do not posses rare qualities, but it is the unexpected combination of factors that makes them stand out.", "histories": [["v1", "Tue, 22 Dec 2015 12:15:12 GMT  (397kb,D)", "https://arxiv.org/abs/1512.07048v1", null], ["v2", "Wed, 10 Feb 2016 15:55:56 GMT  (404kb,D)", "http://arxiv.org/abs/1512.07048v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["roel bertens", "jilles vreeken", "arno siebes"], "accepted": false, "id": "1512.07048"}, "pdf": {"name": "1512.07048.pdf", "metadata": {"source": "CRF", "title": "Beauty and Brains: Detecting Anomalous Pattern Co-Occurrences", "authors": ["Roel Bertens", "Jilles Vreeken", "Arno Siebes"], "emails": ["R.Bertens@uu.nl", "jilles@mpi-inf.mpg.de", "A.P.J.M.Siebes@uu.nl"], "sections": [{"heading": null, "text": "In this paper, we define the class of anomalies described above and propose a method for quickly identifying them in transaction data. By adopting a pattern-based approach, our method easily explains why a transaction is anomalous, and the effectiveness of our method is thoroughly verified by a wide range of experiments on both real and synthetic data."}, {"heading": "1. INTRODUCTION", "text": "In this context, it should be noted that this is not a purely formal matter, but a purely formal matter, which is a purely formal matter."}, {"heading": "2. NOTATION", "text": "In this section, we provide the notation used throughout the essay. We will look at transaction records D that contain | D | Transactions. Each transaction t contains a subset of the size | t | of items from the alphabet \u0435.Category data consists of | A | Attributes, where each attribute Ai \u0192A has a domain \u0435i and can also be considered transaction data by placing each attribute value pair on a different item.For categorical data, each transaction will have the same length as each attribute should be specified. All logarithms should belong to base 2, and according to convention 0 log 0 = 0. Furthermore, we will use P (\u00b7) to mark a probability density function."}, {"heading": "3. ANOMALIES IN TRANSACTION DATA", "text": "What is an anomaly? Anomalies are also referred to as anomalies, discordants, deviations or outliers in the data mining and statistical literature [1]. As we look at transaction data, we use the following definition. DEFINITION 3.1. A transaction is anomalous if it deviates from what we expect from the overall data set. In light of this definition, an anomaly can manifest itself in different ways, resulting in multiple classes of transaction data anomalies. In this section, we will recall 2 classes of anomalies, define 1 new class, and show how we can identify them all by formalizing appropriate anomaly values. We would like to emphasize once again that the values for different classes of anomalies complement each other."}, {"heading": "3.1 Class 0: Unexpected Transaction Lengths", "text": "A class 0 anomaly is a transaction with a significantly different transaction length. We propose an anomaly score, which represents the number of bits required to describe the transaction length taking into account all transaction lengths in the data. The intuition behind the 0 subscript for this score is that we do not need to consider any patterns at all to identify these anomalies. As a result, we will not be able to use this score to identify any interesting co-events. Moreover, since it is a fairly trivial score, we will not be able to evaluate it any further."}, {"heading": "3.2 Class 1: Unexpected Transactions", "text": "If a transaction contains no or only a few common patterns that occur in (almost) all other transactions, it can be considered an anomaly.DEFINITION 3.3. A Class 1 anomaly is a transaction that contains very little regularity. The state of the art in detecting transaction anomalies focuses on what we call Class 1 anomalies. For example, OC3 [22] evaluates transactions using a descriptive pattern set S. Transactions that contain few of these patterns but predominantly singletons get a higher Score1 score. This is because such a transaction cannot be well explained by the pattern set that is descriptive of the data. We generalize this idea by defining a score based on the probability of a transaction."}, {"heading": "3.3 Class 2: Unexpected Co-occurrences", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "4. HOW TO USE OUR SCORES", "text": "In the process of exploratory data mining, it must be borne in mind that all three classes of anomalies we identify provide different insights. In practice, we should instantiate all three and examine the above-mentioned anomalies for each class. In this context, of course, our focus is on Class 2 anomalies. In order to determine which of the best-rated transactions of the BNB should be examined, and to verify the significance of the values, we propose the following two methods based on the Bootstrap. Remember that Bootstrap methods consider the data given as a sample and generate a number of pseudo-samples from it; for each pseudo-sample, we calculate the statistics of interest and use the distribution of this statistic on pseudo-samples to draw conclusions about the distribution of the original sample statistics [6]."}, {"heading": "4.1 Significance test", "text": "For a synthetic dataset, it is easy to test the significance of anomalies, as we can generate data with and without anomalies for which the resulting values must be significantly different. Unfortunately, for real data, this is not the case, as we do not know which and how many (negative) patterns comprise the data. Nevertheless, to give a measure of significance, we use the following bootstrap approach: We sample transactions from our original dataset (with substitutes) to retrieve an equally large new dataset. We repeat this a thousand times and store the highest anomaly score for each dataset. We then repeat this process, but first remove the transaction with the highest BNB score from the sample set. That is, the best-placed anomaly is definitely not present in the second-type bootstrap samples and may or may not be present in the bootstrap samples of the first type. The larger the difference between the distributions, the more significant the transaction, and the one placed with the highest score."}, {"heading": "4.2 Which transactions to investigate", "text": "Selecting the right parameter value is never easy in exploratory data mining. Since the BNB score produces a ranking of all transactions where higher values indicate a higher probability of being abnormal, no parameters are required. To determine which transactions are to be examined on the basis of this ranking, we use Cantelli's inequality to identify transactions that are significantly different from the norm. THEOREM 4.1. Cantelli's inequality [9]. Let X be a random variable with expectation \u00b5X and standard deviation \u03c3X. The positive class then includes anomalous values for each k-R +, P (X \u2212 \u00b5X \u2265 k\u03c3X) \u2264 11 + k2. Smets and Vreeken [22] suggested a sound method for determining thresholds to distinguish between \"normal\" and anomalous transactions. The positive class includes anomalous values for \"normal\" transactions and based on the distribution of these transactions, we can choose a threshold for each of them by selecting an actual threshold for a threshold."}, {"heading": "5. RELATED WORK", "text": "In this paper, we examine anomalies in relation to binary transaction data. Since anomalies refer to many different ways, mostly with different definitions, we refer to [15] and [1] intact overviews in this area of research. Generally, most anomalies are due to distances. Here, we focus on discrete data, nominal attributes that are usually not available. Of the methods applied to transaction data, these are smets and veeks. They suggest identifying the transactions as such that are well described by the model of absence."}, {"heading": "6. EXPERIMENTS", "text": "In this section, we evaluate the strength of the BNB score to identify Class 2 anomalies. First, we show how we generated the synthetic data needed for some of the experiments. Second, we show a baseline comparison where we show that the size of the input quantity is of great importance. Next, we show the performance of BNB on synthetic data and show its statistical power. Finally, we show some beautiful results of Class 2 anomalies on a variety of real data sets. We implemented our algorithms in C + + and generated our synthetic data with Python. Our code, both for calculating anomalies and generating synthetic data, is available for research purposes.1 All experiments were conducted on a 2.6 GHz system with 64 GB of memory."}, {"heading": "6.1 Generating Synthetic Data", "text": "To generate synthetic transaction data sets, we first select the number of transactions and the size of the alphabet. We also create a series of random patterns P, and for each pattern in that set, we select random support in the range [5-10%] and a random size of 3-6 items. In addition, we similarly create two more patterns with 20% support, which we call the anomaly generators. Then, we build our data set by first adding the two anomaly generators where we make sure they occur only once together in the same transaction; this is the anomaly. Then, for each transaction, we do the following: With the probability corresponding to their support, each pattern from P is added to the transaction as long as it does not disturb the anomaly. In addition, each singleton from the transaction is added with a probability of 10%. To generate categorical data, we first add the categorical number of the transactions and not the specific data."}, {"heading": "6.2 Baseline Comparison", "text": "In the following sections we will show that the BNB provides very reliable values, but first we will show its efficiency. To emphasize the need to use small patterns as input for our BNB score, we compare the use of SLIM pattern sets with a minimum support of 1 against the use of all closed frequent patterns with a minimum support of 5%. We generated random transaction data with | D | = 5000, | | = 50 and we covered all | S | | 35 patterns. Both approaches always rank the anomaly highest to track runtimes 1http: / / eda.mmci.uni-saarland.de / bnb / and the size of the input set S, for which we have to consider all possible combinations. Both approaches always rank the anomaly highest, so we can continue to focus on the runtime and number of patterns that have been taken into account."}, {"heading": "6.3 Performance on Synthetic Transaction Data", "text": "The aim of this experiment is twofold: firstly, we show that our method is capable of identifying Class 2 anomalies in transaction data; secondly, we justify the definition of the different classes of anomalies because we show that Class 2 anomalies are not identified by the state-of-the-art Class 1 anomaly detector, namely OC3; and we reiterate that, as a result, the two values should not be further compared as they complement each other; we have generated random data sets, as described in Section 6.1, with different settings; and the results in Table 1 show that BNB always rates the anomaly highest and that OC3 does not identify it."}, {"heading": "6.4 Performance on Synthetic Categorical Data", "text": "Knowing that the BNB correctly identifies Class 2 anomalies for transaction data, we compared them here with the state of the art for categorical data, namely COMPREX [3]. Again, we only compare these methods to show that Class 2 anomalies are different from Class 1 anomalies and that these two methods should therefore be used in a complementary manner. We generated random data sets with different settings as described in Section 6.1. The results in Table 2 show that the BNB always ranks the anomalous transaction first and COMPREX is unable to identify them (which gives it a much lower ranking)."}, {"heading": "6.5 Statistical Power", "text": "To this end, we perform statistical tests using synthetic data. To this end, the null hypothesis is that the data does not contain Class 2 anomalies. To determine the cutoff for testing the null hypothesis, we first create 100 transaction records without the common occurrence between the two anomaly generators, and then include a further 100 records with this co-occurrence. For all records, we select | D | = 5000, | = 25 items and | P | = 100. Next, we report the highest BNB score for all 100 records without anomaly. Then, we set the cutoff according to the significance level \u03b1 = 0.05. The power of the BNB score is the percentage of the highest records in the data sets that are omomomomal."}, {"heading": "6.6 Real World Data", "text": "The following 3 anomalies are persons with a similar situation, but with the reverse patterns. This is the sexual relationship of the persons that contains these persons. In fact, the sexual relationship is an anomaly that is not identified by the state of the art, but by an anomalous detection of the male sex organs 4 and can give a lot of insight that we have conducted several experiments on real data sets from different areas. We used the data of adults, zoo and bike sharing data from the UCI repository, 2 along with the mammals and more and is used to predict whether someone's income exceeds $50K per year. We calculated a ranking based on the BNB score and found some interesting anomalies. The ranked transaction contains the very unexpected co-meaning of someone for whom the attribute sex is female nor for whom the relationship status of the husband has. The following 3 anomalies are persons with a similar situation, but vice versa."}, {"heading": "7. DISCUSSION", "text": "The experiments show that although the state of the art in detecting anomalies is not able to identify the newly defined Class 2 anomalies, we can identify them using our new BNB score. We have shown that a naive approach with closed, frequent items as input quantity quickly becomes impossible when the number of patterns in the data is significantly higher than \"normal\" transactions. Furthermore, we have shown, both in transactions and in categorical synthetic data, that BNB always places the planted anomaly in a fraction of the time. From our real-world dataset experiments, we find that Class 2 anomalies do exist and can provide useful insights."}, {"heading": "8. CONCLUSION", "text": "In this work, we introduced a new class of anomalies, which we call unexpected coincidences of patterns. We demonstrated that the anomalies identified by the state of the art in detecting anomalies belong to a different class and that these methods are unable to identify unexpected coincidences of patterns. We introduced the BNB score, which intuitively evaluates a transaction based on its unexpected coincidence of patterns. Using BNB, we skillfully identify all the anomalies contained in synthetic data and find interesting explanations for anomalous transactions in real data. In addition to detecting interesting behavior, BNB also allows detection of errors in data that previous methods could not, making them very suitable for data purification purposes."}, {"heading": "Acknowledgments", "text": "Roel Bertens and Arno Siebes are supported by the Dutch program COMMIT. Jilles Vreeken is supported by the Cluster of Excellence \"Multimodal Computing and Interaction\" within the Excellence Initiative of the Federal Government."}, {"heading": "9. REFERENCES", "text": "[1] C. C. Aggarwal, Editor. Outlier Analysis. Springer, 2013. [2] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. I.Verkamo. Fast discovery of association rules. In Advances in Knowledge Discovery and Data Mining, pp. 307-328. AAAI / MIT Press, 1996. [3] L. Akoglu, H. Tong, J. Vreeken, and C. Faloutsos. COMPREX: Compression based anomaly detection, p. 1999. In Proceedings of the 21st ACM Conference on Information and Knowledge Management (CIKM)."}], "references": [{"title": "Fast discovery of association rules. In Advances in Knowledge Discovery and Data Mining, pages 307\u2013328", "author": ["R. Agrawal", "H. Mannila", "R. Srikant", "H. Toivonen", "A.I. Verkamo"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "COMPREX: Compression based anomaly detection", "author": ["L. Akoglu", "H. Tong", "J. Vreeken", "C. Faloutsos"], "venue": "In Proceedings of the 21st ACM Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Direct local pattern sampling by efficient two-step random procedures", "author": ["M. Boley", "C. Lucchese", "D. Paurat", "T. G\u00e4rtner"], "venue": "In Proceedings of the 17th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Non-derivable itemset mining", "author": ["T. Calders", "B. Goethals"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Bootstrap-based improvements for inference with clustered errors", "author": ["A.C. Cameron", "J.B. Gelbach", "D.L. Miller"], "venue": "The Review of Economics and Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Maximum entropy models and subjective interestingness: an application to tiles in binary databases", "author": ["T. De Bie"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Tiling databases", "author": ["F. Geerts", "B. Goethals", "T. Mielik\u00e4inen"], "venue": "In Proceedings of Discovery Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Probability and Random Processes", "author": ["G. Grimmett", "D. Stirzaker"], "venue": "Oxford university press,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "The Minimum Description Length Principle", "author": ["P. Gr\u00fcnwald"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Musk: Uniform sampling of k maximal patterns", "author": ["M.A. Hasan", "M. Zaki"], "venue": "In Proceedings of the 9th SIAM International Conference on Data Mining (SDM),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Fp-outlier: Frequent pattern based outlier detection", "author": ["Z. He", "X. Xu", "J.Z. Huang", "S. Deng"], "venue": "Computer Science and Information Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Rule evaluation measures: A unifying view", "author": ["N. Lavra\u010d", "P. Flach", "B. Zupan"], "venue": "In International Conference on Inductive Logic Programming (ILP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Summarizing data succinctly with the most informative itemsets", "author": ["M. Mampaey", "J. Vreeken", "N. Tatti"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Novelty detection: a review", "author": ["M. Markou", "S. Singh"], "venue": "Signal processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "The Atlas of European Mammals", "author": ["A. Mitchell-Jones", "G. Amori", "W. Bogdanowicz", "B. Krystufek", "P.H. Reijnders", "F. Spitzenberger", "M. Stubbe", "J. Thissen", "V. Vohralik", "J. Zima"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Societas europaea mammalogica", "author": ["T. Mitchell-Jones"], "venue": "http://www.european-mammals.org,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Outlier detection for transaction databases using association rules", "author": ["K. Narita", "H. Kitagawa"], "venue": "In Web-Age Information Management,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Discovering frequent closed itemsets for association rules", "author": ["N. Pasquier", "Y. Bastide", "R. Taouil", "L. Lakhal"], "venue": "In Proceedings of the 7th International Conference on Database Theory (ICDT),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Discovery, analysis, and presentation of strong rules", "author": ["G. Piatetsky-Shapiro"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1991}, {"title": "Item sets that compress", "author": ["A. Siebes", "J. Vreeken", "M. van Leeuwen"], "venue": "In Proceedings of the 6th SIAM International Conference on Data Mining (SDM),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "The odd one out: Identifying and characterising anomalies", "author": ["K. Smets", "J. Vreeken"], "venue": "In Proceedings of the 11th SIAM International Conference on Data Mining (SDM),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "SLIM: Directly mining descriptive patterns", "author": ["K. Smets", "J. Vreeken"], "venue": "In Proceedings of the 12th SIAM International Conference on Data Mining (SDM),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Conditional anomaly detection", "author": ["X. Song", "M. Wu", "C. Jermaine", "S. Ranka"], "venue": "TKDE, 19(5):631\u2013645,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "KRIMP: Mining itemsets that compress", "author": ["J. Vreeken", "M. van Leeuwen", "A. Siebes"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Summarizing itemset patterns using probabilistic models", "author": ["C. Wang", "S. Parthasarathy"], "venue": "In Proceedings of the 12th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}], "referenceMentions": [{"referenceID": 20, "context": "processed model [22, 3].", "startOffset": 16, "endOffset": 23}, {"referenceID": 1, "context": "processed model [22, 3].", "startOffset": 16, "endOffset": 23}, {"referenceID": 10, "context": "Another example is to score transactions based on the number of frequent patterns that reside in it [12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "Yet another method scores transactions based on items missing from a transaction which were expected given the set of mined association rules [18].", "startOffset": 142, "endOffset": 146}, {"referenceID": 20, "context": "Moreover, someone drinking both Coke and Pepsi also does not seem surprising as it can be compressed well using the methods of [22, 3], it contains multiple frequent patterns [12] and there is nothing missing [18].", "startOffset": 127, "endOffset": 134}, {"referenceID": 1, "context": "Moreover, someone drinking both Coke and Pepsi also does not seem surprising as it can be compressed well using the methods of [22, 3], it contains multiple frequent patterns [12] and there is nothing missing [18].", "startOffset": 127, "endOffset": 134}, {"referenceID": 10, "context": "Moreover, someone drinking both Coke and Pepsi also does not seem surprising as it can be compressed well using the methods of [22, 3], it contains multiple frequent patterns [12] and there is nothing missing [18].", "startOffset": 175, "endOffset": 179}, {"referenceID": 16, "context": "Moreover, someone drinking both Coke and Pepsi also does not seem surprising as it can be compressed well using the methods of [22, 3], it contains multiple frequent patterns [12] and there is nothing missing [18].", "startOffset": 209, "endOffset": 213}, {"referenceID": 11, "context": "For this example, the score we introduce is minus the log of the lift of the association rule Pepsi\u2192 Coke and, except the log, has been introduced before in [13] as the novelty of an association rule.", "startOffset": 157, "endOffset": 161}, {"referenceID": 20, "context": "class, for which the most work has been done [22, 3], describes unexpected transactions given a model of the data.", "startOffset": 45, "endOffset": 52}, {"referenceID": 1, "context": "class, for which the most work has been done [22, 3], describes unexpected transactions given a model of the data.", "startOffset": 45, "endOffset": 52}, {"referenceID": 20, "context": "For example, OC [22] scores transactions using a descriptive pattern set S.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "Our score is related to the concept of lift [20] used in the context", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "For example, OC [22] and COMPREX [3] will not give a class 2 anomaly a higher score as both individual patterns are frequent and will add little to the anomaly score.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "For example, OC [22] and COMPREX [3] will not give a class 2 anomaly a higher score as both individual patterns are frequent and will add little to the anomaly score.", "startOffset": 33, "endOffset": 36}, {"referenceID": 10, "context": "[12] and the method from Narita et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] have no means to give higher scores to class 2 anomalies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "We could use condensed representations such as closed [19] or non-derivable [5] frequent patterns to remove as much redundancy as possible, however these sets will still be too large.", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "We could use condensed representations such as closed [19] or non-derivable [5] frequent patterns to remove as much redundancy as possible, however these sets will still be too large.", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "By sampling [11] patterns we can attain small sets of patterns, however, the choice of the size of the sample determines which anomalies one will (likely) find.", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "for the required pattern set, we choose to use KRIMP [25] or SLIM [23] to automatically find small descriptive pattern sets that describe the data well without containing noise or redundancy.", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "for the required pattern set, we choose to use KRIMP [25] or SLIM [23] to automatically find small descriptive pattern sets that describe the data well without containing noise or redundancy.", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "Recall that bootstrap methods consider the given data as a sample, and generate a number of pseudo-samples from it; for each pseudosample calculate the statistic of interest, and use the distribution of this statistic across pseudo-samples to infer the distribution of the original sample statistic [6].", "startOffset": 299, "endOffset": 302}, {"referenceID": 7, "context": "Cantelli\u2019s inequality [9].", "startOffset": 22, "endOffset": 25}, {"referenceID": 20, "context": "Smets and Vreeken [22] proposed a well-founded way to determine threshold values to distinguish between \u2018normal\u2019 and anomalous transactions.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "As anomalies are referred to in many different ways, mostly with slightly different definitions, we refer to [15] and [1] for indepth overviews on this field of research.", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "Of the methods that are applicable on transaction data, that of Smets and Vreeken [22] is perhaps the most relevant.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "[3] proposed COMPREX which takes a similar approach in that they also rank transactions based on their encoded length.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "sets are those of Wang and Parthasarathy [26] and Mampaey et al.", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] rank transactions based on the number of frequent patterns they contain given only the top-k frequent patterns, and Narita et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] rank transactions based on confidence of association rules but need a minimum confidence level as parameter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In the Introduction we already mentioned the relation between our score and novelty as introduced in [13].", "startOffset": 101, "endOffset": 105}, {"referenceID": 22, "context": "Our notion of anomaly is also related to the conditional anomalies introduced in [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "In general, we can use the result of standard frequent pattern mining [2, 19] although this incurs a high computational cost.", "startOffset": 70, "endOffset": 77}, {"referenceID": 17, "context": "In general, we can use the result of standard frequent pattern mining [2, 19] although this incurs a high computational cost.", "startOffset": 70, "endOffset": 77}, {"referenceID": 9, "context": "Instead, we can resort to pattern sampling techniques [11, 4], yet then we have to choose the number of patterns to be sampled.", "startOffset": 54, "endOffset": 61}, {"referenceID": 2, "context": "Instead, we can resort to pattern sampling techniques [11, 4], yet then we have to choose the number of patterns to be sampled.", "startOffset": 54, "endOffset": 61}, {"referenceID": 19, "context": "[21] proposed to mine such pattern sets by the Minimum Description Length principle [10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[21] proposed to mine such pattern sets by the Minimum Description Length principle [10].", "startOffset": 84, "endOffset": 88}, {"referenceID": 23, "context": "KRIMP [25] and SLIM [23] are two deterministic algorithms that heuristically optimise this score.", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "KRIMP [25] and SLIM [23] are two deterministic algorithms that heuristically optimise this score.", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "Other pattern set mining techniques, especially those that mine patterns characteristic for the data such as [14, 8, 26], are also meaningful choices to be used with BNB.", "startOffset": 109, "endOffset": 120}, {"referenceID": 6, "context": "Other pattern set mining techniques, especially those that mine patterns characteristic for the data such as [14, 8, 26], are also meaningful choices to be used with BNB.", "startOffset": 109, "endOffset": 120}, {"referenceID": 24, "context": "Other pattern set mining techniques, especially those that mine patterns characteristic for the data such as [14, 8, 26], are also meaningful choices to be used with BNB.", "startOffset": 109, "endOffset": 120}, {"referenceID": 21, "context": "we compare the use of SLIM [23] pattern sets with a minimum support of 1 against the use of all closed frequent patterns with a minimum support at 5%.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "Secondly, we justify the definition of the different classes of anomalies as we show that the class 2 anomalies are not identified by the state-of-the-art class 1 anomaly detector, which is OC [22].", "startOffset": 193, "endOffset": 197}, {"referenceID": 1, "context": "Knowing that BNB correctly identifies class 2 anomalies for transaction data, here we compared it to the state-of-the-art on categorical data, which is COMPREX [3].", "startOffset": 160, "endOffset": 163}, {"referenceID": 15, "context": "from the UCI repository, together with the Mammals [17] en ICDM Abstracts [7] datasets.", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "from the UCI repository, together with the Mammals [17] en ICDM Abstracts [7] datasets.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "The data is available upon request from the author of [7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 14, "context": "The full dataset [16] is available upon request from the Societas Europea Mammalogica, http://www.", "startOffset": 17, "endOffset": 21}], "year": 2016, "abstractText": "Our world is filled with both beautiful and brainy people, but how often does a Nobel Prize winner also wins a beauty pageant? Let us assume that someone who is both very beautiful and very smart is more rare than what we would expect from the combination of the number of beautiful and brainy people. Of course there will still always be some individuals that defy this stereotype; these beautiful brainy people are exactly the class of anomaly we focus on in this paper. They do not posses intrinsically rare qualities, it is the unexpected combination of factors that makes them stand out. In this paper we define the above described class of anomaly and propose a method to quickly identify them in transaction data. Further, as we take a pattern set based approach, our method readily explains why a transaction is anomalous. The effectiveness of our method is thoroughly verified with a wide range of experiments on both real world and synthetic data.", "creator": "LaTeX with hyperref package"}}}