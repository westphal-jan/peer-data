{"id": "1512.07074", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2015", "title": "Move from Perturbed scheme to exponential weighting average", "abstract": "In an online decision problem, one makes decisions often with a pool of decision sequence called experts but without knowledge of the future. After each step, one pays a cost based on the decision and observed rate. One reasonal goal would be to perform as well as the best expert in the pool. The modern and well-known way to attain this goal is the algorithm of exponential weighting. However, recently, another algorithm called follow the perturbed leader is developed and achieved about the same performance. In our work, we first show the properties shared in common by the two algorithms which explain the similarities on the performance. Next we will show that for a specific perturbation, the two algorithms are identical. Finally, we show with some examples that follow-the-leader style algorithms extend naturally to a large class of structured online problems for which the exponential algorithms are inefficient.", "histories": [["v1", "Tue, 22 Dec 2015 13:18:17 GMT  (571kb)", "http://arxiv.org/abs/1512.07074v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chunyang xiao"], "accepted": false, "id": "1512.07074"}, "pdf": {"name": "1512.07074.pdf", "metadata": {"source": "CRF", "title": "Move from Perturbed scheme to exponential weighting average", "authors": ["Chunyang Xiao"], "emails": [], "sections": [{"heading": null, "text": "This year, the time has come for us to be able to try to find a solution, to find a solution that is capable, that is able to find a solution, that is able to find a solution."}], "references": [{"title": "Adaptive Routing with Endto-End feedback : Distributed Learning and Geometric Approaches", "author": ["Baruch Awerbuch", "Robert D Kleinberg"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "On-line algorithms in machine learning", "author": ["Avrim L.Blum"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Perturb-and-MAP random fields: Using discrete optimization to learn and sample from energy models", "author": ["George Papandreou", "Alan L. Yuille"], "venue": "International Conference on Computer Vision,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Predicting nearly as well as the best pruning of a planar decision graph", "author": ["Eiji Takimoto", "Manfred K. Warmuth"], "venue": "Theoretical Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "For example, in shortest paths problems, cj are linear functions [1].", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "A survey of these results could be found in [3].", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "However, in recent years, another scheme called follow perturbed leading expert has been discovered and achieved similar performance [2].", "startOffset": 133, "endOffset": 136}, {"referenceID": 2, "context": "First let\u2019s remark that the weighted average is a generalization of the randomized weighted majority algorithm that appeared in the survey of Blum [3].", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "This idea leads us to the following perturbed leading expert algorithm presented in the paper of Adam Kalai [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "Observe that what we need here to choose the minimum is only a minimization oracle [1] as we do not update weight and thus modify the structure of the problem.", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "The following theorem is presented in [2] for the case when the cost functions are all linear.", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "These follow-the-leader style algorithms extend naturally to a large class of structured online problems for which the exponential weighted average algorithms are inefficient[2].", "startOffset": 174, "endOffset": 177}, {"referenceID": 4, "context": "Some clever schemes have been developed[5], but now let us consider just the algorithm Follow the perturbed leading expert:", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "As no weight update is used, what we need, as mentioned in[1], is just an optimization oracle that we have here Bellman algorithm, for example.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "And this algorithm achieves similar bounds as in [5].", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "The article [4] shows that with the addition of perturbation, the probability inference could be carried on easily.", "startOffset": 12, "endOffset": 15}], "year": 2015, "abstractText": "In an online decision problem, one makes decisions often with a pool of decisions\u2019 sequence called experts but without knowledge of the future. After each step, one pays a cost based on the decision and observed rate. One reasonal goal would be to perform as well as the best expert in the pool. The modern and well-known way to attain this goal is the algorithm of exponential weighting. However, recently, another algorithm called follow the perturbed leader is developed and achieved about the same performance. In our work, we first show the properties shared in common by the two algorithms which explain the similarities on the performance. Next we will show that for a specific perturbation, the two algorithms are identical. Finally, we show with some examples that follow-the-leader style algorithms extend naturally to a large class of structured online problems for which the exponential algorithms are inefficient. 1 Online problem setting In an online decision problem, one makes decisions often with a pool of decisions\u2019 sequence called experts but without knowledge of the future. After each step, one pays a cost based on the decision and observed state. As there is no prior knowledge on the accuracy of experts in the pool, one reasonable goal for this general problem would be to perform as well as the best expert in the pool after a number of steps. More precisely, we consider the following mathematical problem: \u2022 A set S of experts is given. \u2022 The algorithm interacts with an adversary in a series of T steps. \u2022 In each step j, the algorithm picks an expert xj \u2208 S , and the adversary selects a cost function cj: S \u2192 R. The adversary could be adaptive, in that cj may depend on {xi : i < j}. \u2022 The algorithm incurs cost , and receives as feedback the value of cj(xj). \u2022 Minimize the algorithm\u2019s regret which is defined as difference in expected cost between the algorithm\u2019s sequence of choices and that of best fixed expert in S:", "creator": "Microsoft\u00ae Word 2013"}}}