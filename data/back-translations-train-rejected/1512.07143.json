{"id": "1512.07143", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2015", "title": "SR-Clustering: Semantic Regularized Clustering for Egocentric Photo Streams Segmentation", "abstract": "While wearable cameras are becoming increasingly popular, locating relevant information in large unstructured collections of egocentric images is still a tedious and time consuming processes. This paper addresses the problem of organizing egocentric photo streams acquired by a wearable camera into semantically meaningful segments. First, contextual and semantic information is extracted for each image by employing a Convolutional Neural Networks approach. Later, by integrating language processing, a vocabulary of concepts is defined in a semantic space. Finally, by exploiting the temporal coherence in photo streams, images which share contextual and semantic attributes are grouped together. The resulting temporal segmentation is particularly suited for further analysis, ranging from activity and event recognition to semantic indexing and summarization. Experiments over egocentric sets of nearly 17,000 images, show that the proposed approach outperforms state-of-the-art methods.", "histories": [["v1", "Tue, 22 Dec 2015 16:13:54 GMT  (5361kb,D)", "https://arxiv.org/abs/1512.07143v1", "23 pages, 10 figures, 2 tables. Submitted to Pattern Recognition"], ["v2", "Mon, 17 Oct 2016 09:40:11 GMT  (6040kb,D)", "http://arxiv.org/abs/1512.07143v2", "23 pages, 10 figures, 2 tables. In Press in Computer Vision and Image Understanding Journal"]], "COMMENTS": "23 pages, 10 figures, 2 tables. Submitted to Pattern Recognition", "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["mariella dimiccoli", "marc bola\\~nos", "estefania talavera", "maedeh aghaei", "stavri g nikolov", "petia radeva"], "accepted": false, "id": "1512.07143"}, "pdf": {"name": "1512.07143.pdf", "metadata": {"source": "CRF", "title": "SR-Clustering: Semantic Regularized Clustering for Egocentric Photo Streams Segmentation", "authors": ["Mariella Dimiccolia", "Marc Bola\u00f1osa", "Estefania Talavera", "Maedeh Aghaei", "Stavri G. Nikolov", "Petia Radevaa"], "emails": ["mariella.dimiccoli@cvc.uab.es", "marc.bolanos@ub.edu", "etalavera@ub.edu", "maghaeigavari@ub.edu", "stavri.nikolov@imagga.com", "petia.ivanova@ub.edu"], "sections": [{"heading": null, "text": "As portable cameras become increasingly popular, localizing relevant information in large, unstructured collections of egocentric images is still a tedious and time-consuming process. This paper addresses the problem of organizing egocentric photo streams, which are divided into semantically significant segments by a portable camera, thus taking an important step toward the goal of automatically annotating these photos for surfing and retrieval. In the proposed method, contextual and semantic information for each image are first extracted using an approach of contextual and semantic neural networks. Later, a vocabulary of concepts in a semantic space is defined by relying on linguistic information. Finally, by exploiting the temporal coherence of concepts, images that share contextual and semantic attributes are summarized in photo streams."}, {"heading": "1. Introduction", "text": "In recent years, the number of people living in the United States has decreased significantly, both in the United States and in other countries. In the United States, the number of people living in the United States has decreased significantly in recent years. In the United States, the number of people living in the United States has doubled. In the United States, the number of people living in the United States has increased by more than half in the last ten years. In the United States, the number of people living in the United States has increased by more than a factor of one. In the United States, the number of people living in the United States has increased by more than twice as much as in the United States."}, {"heading": "2. SR-Clustering for Temporal Photo Stream Segmentation", "text": "A visual overview of the proposed method is given in Fig. 2. The input is a one-day photostream from which contextual and semantic characteristics are extracted. Initial clustering is done by AC and ADWIN. Later, GC is used to search for a trade-off between the approaches AC (represented by the bottom colored circles) and ADWIN (represented by the top colored circles).The binary term GC implies smoothness and similarity of consecutive images in relation to CNN image characteristics. The output of the proposed method is the segmented photostream. In this section, we present the semantic and contextual characteristics of SR clustering and provide a detailed description of the segmentation approach."}, {"heading": "2.1. Features", "text": "We assume that two consecutive images belong to the same segment if they can be described by similar image characteristics. If we refer to the characteristics of an image, we usually look at low-level image characteristics (e.g. color, texture, etc.) or a global representation of the environment (e.g. CNN characteristics), but the objects or concepts that semantically represent an event are also of great importance for the segmentation of the photostream. Below, we explain the characteristics that semantically describe the egocentric images."}, {"heading": "2.1.1. Semantic Features", "text": "In fact, most people are able to move to another world, in which they are able, in which they are able to move, and in which they are able to move."}, {"heading": "2.1.2. Contextual Features", "text": "In addition to the semantic characteristics, we represent images with a feature vector extracted from a pre-trained CNN. The CNN model that we use to calculate the image representation is AlexNet, which is detailed in [20]. The features are calculated by removing the last layer that corresponds to the 1http: / / www.imagga.com / solutions / auto-tagging.htmlclassifier from the network. We used the Deep Learning Framework Caffe [17] to run CNN. Due to the fact that the weights were trained on the ImageNet database [10], which consists of images containing individual objects, we expect that the features extracted from images with multiple objects are representative of the environment. It is noteworthy that we did not use the weights obtained with a pre-trained CNN for the scenes from the Places 205 database [34], which reduces the visual field of view as the entire field of view of the camera is very narrow, which means that instead the camera is characterized by 33."}, {"heading": "2.2. Temporal Segmentation", "text": "SR clustering for temporal segmentation is based on the fusion of semantic and contextual characteristics with the R clustering method described in [30]."}, {"heading": "2.2.1. Agglomerative Clustering", "text": "After concatenating semantic and contextual characteristics, the hierarchical agglomerative clustering (AC) procedure is applied using a bottom-up clustering procedure. In each iteration, the method brings together the most similar cluster pair based on the distances between the image characteristics and updates the similarity matrix of the elements, to the point where all possible consistent combinations are exhausted. The cutoff global parameter defines the consistency of the merged clusters. We use the cosine similarity between samples, which is suitable for high-dimensional positive spaces [31]. The drawback of this method is that it tends to over-segment the photo streams."}, {"heading": "2.2.2. ADWIN", "text": "To compensate for the over-segmentation caused by AC, we proposed to model the egocentric sequence as a multidimensional data stream and to detect changes in mean distribution through an adaptive learning method called ADWIN [4], which also provides a strict statistical guarantee of performance in terms of false positive rate. The method, which is based on the inequality of the farm thing [15], recursively checks whether the difference between the averages of two temporally adjacent (partial) windows of the data, say W1 and W2, is greater than a threshold. The threshold value takes into account if both partial windows are large enough and distinct enough for a k \u2212 dimensional signal [13], calculated as: cut = k 1 / p \u221a 12 m ln4k\u043c \u00b2, with p indicating the p \u2212 standard, while the threshold value (0, 1) is a user-defined confidence parameter, and m the harmonic value between the lengths of 12 m ln4k\u043c \u00b2, where p indicates the p \u2212 standard, while the threshold value (0, 1) is guaranteed to be a user-defined one confidence parameter, and m means that the mean value between certain W1 and WIN is relative to other words is higher than the WIN."}, {"heading": "2.2.3. Graph-Cuts regularization", "text": "We use graph cuts (GC) as a framework to integrate both of the aforementioned approaches, AC and ADWIN, to find a compromise between them, which naturally leads to a time-consistent result. GC is an energy minimization technique that works by finding the minimum of an energy function that normally consists of two terms: the unary term U, also called data date, which describes the relationship of the variables to a possible class, and the binary term V, also called pair or regulation, which describes the relationship between two adjacent samples (near-temporal images) according to their characteristic similarity. The binary term smoothes the boundaries between similar terms, while the unary term holds the cluster affiliation to each term frame according to its probability. In our problem, we define the unary term as the sum of 2 parts (Uac (fi) and Uadw (fi)."}, {"heading": "3. Experiments and Validation", "text": "In this section, we will discuss the data sets and statistical evaluation measurements used to validate the proposed model and compare it with the most advanced methods. In summary, we will apply the following method of validation: 1. Three different sets of data collected by 3 different portable cameras will be used for validation; 2. The F-Measure will be used as a statistical measure to compare the performance of different methods; 3. Two consistency measures will be applied to compare different manual segmentations; 4. Comparative results of SR clustering with 3 state-of-the-art techniques will be assumed; 5. The reliability of the final proposal will be demonstrated by validating the various components of SR clustering."}, {"heading": "3.1. Data", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "3.2. Experimental setup", "text": "We then measure the performance of our method by using the F measurement (FM), which is defined as: FM = 2 RPR + P, where P is the precision defined as (P = TPTP + FP), and R is the callback defined as (R = TPTP + FN). TP, FP, and FN are the number of actually positive, false positive, and false negatives of the detected segment boundaries of the photo stream. We define FM, where we look at the images that the model recognizes as boundaries of an event and that are defined by the annotator close to the boundary images of the GT (given a tolerance of 5 images on both sides).The FPs are the images that are detected as events but not defined in the GT, and the FNs represent the lost boundaries by the model specified in the GT. Lower FM values represent a false boundary detection, while higher values indicate a good segmentation."}, {"heading": "3.3. Experimental results", "text": "The first two columns correspond to the data used in this book; the fourth column corresponds to the results we have around the world; the third column indicates that the first column is a comparison; the third column indicates that the second column is a comparison; the third column indicates that the third column is a comparison; the third column indicates that the third column is a comparison; the third column indicates that the first column is a comparison; and the third column indicates that the first column is a comparison."}, {"heading": "3.4. Discussion", "text": "The experimental results detailed in Section 3.3 have shown the advantages of using semantic features for the temporal segmentation of egocentric photostreams. [14] Despite the general agreement on the inability of low-level features to provide understanding of the semantic structure in complex events [14], and the need for semantic indexing and browsing systems, the use of high-level features in the context of egocentric temporal segmentation and summary is very limited, mainly due to the difficulty of dealing with the enormous variability of object appearance and lighting conditions in egocentric images. In the work of Doherty et al. [11] and Lee and Grauman, temporal segmentation still distinguishes features at a low level. In addition to the difficulty of reliably recognizing objects, the temporal segmentation of egocentric photos and images in egocentric images is due to the fact that the time segmentation of images in egocentric images is very limited."}, {"heading": "4. Conclusions and future work", "text": "This paper proposed an uncontrolled approach to the temporal segmentation of egocentric photostreams, capable of dividing the life of a day into segments with semantic attributes, thereby creating a basis for semantic indexing and event recognition. First, the proposed approach recognizes concepts for each image separately by applying a CNN approach, and later groups the recognized concepts into a semantic space, defining the vocabulary of the concepts of a day. Semantic features are combined with global image features that capture more general contextual information to increase their discriminatory power. By leveraging these semantic features, a GC technique is used to integrate a statistical boundary generated by the concept drift method ADWIN and the AC, two methods with complementary properties for temporal segmentation."}, {"heading": "Acknowledgments", "text": "This work was partly founded by TIN2012-38187-C03-01, SGR 1219 and was awarded to the research project 20141510 to Maite Garolera (from Fundacio \"Marato\" TV3). Donors had no role in the study design, data collection and analysis, the decision to publish or prepare the manuscript. M. Dimiccoli is supported by a Beatriu de Pino \"s scholarship (Marie-Curie COFUND Action). P. Radeva is partially supported by an ICREA Academia scholarship from 2014."}], "references": [{"title": "Towards social interaction detection in egocentric photo-streams", "author": ["M. Aghaei", "M. Dimiccoli", "P. Radeva"], "venue": "In Eighth International Conference on Machine Vision, pages 987514\u2013987514. International Society for Optics and Photonics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Multi-face tracking by extended bag-of-tracklets in egocentric videos. Computer Vision and Image Understanding", "author": ["M. Aghaei", "M. Dimiccoli", "P. Radeva"], "venue": "Special Issue on Assistive Computer Vision and Robotics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "With whom do I interact? detecting social interactions in egocentric photo-streams", "author": ["M. Aghaei", "M. Dimiccoli", "P. Radeva"], "venue": "In Proceedings of the International Conference on Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Learning from time-changing data with adaptive windowing", "author": ["A. Bifet", "R. Gavalda"], "venue": "In Proceedings of SIAM International Conference on Data Mining,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Towards storytelling from visual lifelogging: An overview", "author": ["M. Bola\u00f1os", "M. Dimiccoli", "P. Radeva"], "venue": "To appear on IEEE Transactions on Human-Machine Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Video segmentation of life-logging videos", "author": ["M. Bola\u00f1os", "M. Garolera", "P. Radeva"], "venue": "In Articulated Motion and Deformable Objects,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Visual summary of egocentric photostreams by representative keyframes", "author": ["M. Bola\u00f1os", "E. Talavera R. Mestre", "X. Gir\u00f3 i Nieto", "P. Radeva"], "venue": "arXiv preprint arXiv:1505.01130,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Predicting daily activities from egocentric images using deep learning", "author": ["D. Castro", "S. Hickson", "V. Bettadapura", "E. Thomaz", "G. Abowd", "H. Christensen", "I. Essa"], "venue": "In proceedings of the 2015 ACM International symposium on Wearable Computers,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Automatically segmenting lifelog data into events", "author": ["A.R. Doherty", "A.F. Smeaton"], "venue": "In Proceedings of the 2008 Ninth International Workshop on Image Analysis for Multimedia Interactive Services,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Wearable cameras in health: the state of the art and future possibilities", "author": ["Aiden R. Doherty", "Steve E. Hodges", "Abby C. King"], "venue": "In American journal of preventive medicine,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Intestinal event segmentation for endoluminal video analysis", "author": ["M. Drozdzal", "J. Vitria", "S. Segui", "C. Malagelada", "F. Azpiroz", "P. Radeva"], "venue": "In Proceedings of International Conference on Image Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Recommendations for recognizing video events by concept vocabularies", "author": ["A. Habibian", "C. Snoek"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1963}, {"title": "Lsda: Large scale detection through adaptation", "author": ["J. Hoffman", "S. Sergio", "E.S. Tzeng", "R. Hu", "R. Girshick J. Donahue", "T. Darrell", "K. Saenko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": "http://caffe.berkeleyvision.org/,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Structural epitome: a way to summarize one\u2019s visual experience", "author": ["N. Jojic", "A. Perina", "V. Murino"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "An ethical framework for automated, wearable cameras in health behavior research", "author": ["P. Kelly", "S. Marshall", "H. Badland", "J. Kerr", "M. Oliver", "A. Doherty", "C. Foster"], "venue": "American journal of preventive medicine,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Predicting important objects for egocentric video summarization", "author": ["YJ. Lee", "K. Grauman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Daily life event segmentation for lifestyle evaluation based on multi-sensor data recorded by a wearable device", "author": ["Z. Li", "Z. Wei", "W. Jia", "M. Sun"], "venue": "In Proceedings of Engineering in Medicine and Biology Society,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Structuring continuous video recording of everyday life using time-constrained clustering", "author": ["W.-H. Lin", "A. Hauptmann"], "venue": "Proceedings of SPIE, Multimedia Content Analysis, Management, and Retrieval,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Story-driven summarization for egocentric video", "author": ["Z. Lu", "K. Grauman"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "In Proceedings of 8th International Conference on Computer Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1995}, {"title": "On estimation of a probability density function and mode. The annals of mathematical statistics, pages", "author": ["E. Parzen"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1962}, {"title": "Temporal segmentation of egocentric videos", "author": ["Y. Poleg", "C. Arora", "S. Peleg"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Compact CNN for indexing egocentric videos", "author": ["Y. Poleg", "A. Ephrat", "S. Peleg", "C. Arora"], "venue": "CoRR, abs/1504.07469,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "R-clustering for egocentric video segmentation", "author": ["E. Talavera", "M. Dimiccoli", "M. Bolanos", "M. Aghaei", "P. Radeva"], "venue": "In Iberian Conference on Pattern Recognition and Image Analysis,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Introduction to Data Mining, (First Edition)", "author": ["P.N. Tan", "M. Steinbach", "V. Kumar"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "Visual ethics: Ethical issues in visual research", "author": ["R. Wiles", "J. Prosser", "A. Bagnoli", "A. Clark", "K. Davies", "S. Holland", "E. Renold"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Seeing the big picture: Deep embedding with contextual evidences", "author": ["L. Zheng", "Sh. Wang", "F. He", "Q. Tian"], "venue": "CoRR, abs/1406.0132,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Among the advances in wearable technology during the last few years, wearable cameras specifically have gained more popularity [5].", "startOffset": 127, "endOffset": 130}, {"referenceID": 11, "context": "In addition, these images can be used as an important tool for prevention or hindrance of cognitive and functional decline in elderly people [12].", "startOffset": 141, "endOffset": 145}, {"referenceID": 8, "context": "State-of-the-art methods for temporal segmentation can be broadly classified into works with focus on what-the-camera-wearer-sees [9, 11, 30] and on whatthe-camera-wearer-does [28, 29].", "startOffset": 130, "endOffset": 141}, {"referenceID": 10, "context": "State-of-the-art methods for temporal segmentation can be broadly classified into works with focus on what-the-camera-wearer-sees [9, 11, 30] and on whatthe-camera-wearer-does [28, 29].", "startOffset": 130, "endOffset": 141}, {"referenceID": 29, "context": "State-of-the-art methods for temporal segmentation can be broadly classified into works with focus on what-the-camera-wearer-sees [9, 11, 30] and on whatthe-camera-wearer-does [28, 29].", "startOffset": 130, "endOffset": 141}, {"referenceID": 27, "context": "State-of-the-art methods for temporal segmentation can be broadly classified into works with focus on what-the-camera-wearer-sees [9, 11, 30] and on whatthe-camera-wearer-does [28, 29].", "startOffset": 176, "endOffset": 184}, {"referenceID": 28, "context": "State-of-the-art methods for temporal segmentation can be broadly classified into works with focus on what-the-camera-wearer-sees [9, 11, 30] and on whatthe-camera-wearer-does [28, 29].", "startOffset": 176, "endOffset": 184}, {"referenceID": 10, "context": "Early works on egocentric temporal segmentation [11, 23] focused on what the camera wearer sees (e.", "startOffset": 48, "endOffset": 56}, {"referenceID": 22, "context": "Early works on egocentric temporal segmentation [11, 23] focused on what the camera wearer sees (e.", "startOffset": 48, "endOffset": 56}, {"referenceID": 6, "context": "More recently, the works in [7] and [30] have used Convolutional Neural Network (CNN) features extracted by using the AlexNet model [20] trained on ImageNet as a fixed feature extractor for image representation.", "startOffset": 28, "endOffset": 31}, {"referenceID": 29, "context": "More recently, the works in [7] and [30] have used Convolutional Neural Network (CNN) features extracted by using the AlexNet model [20] trained on ImageNet as a fixed feature extractor for image representation.", "startOffset": 36, "endOffset": 40}, {"referenceID": 19, "context": "More recently, the works in [7] and [30] have used Convolutional Neural Network (CNN) features extracted by using the AlexNet model [20] trained on ImageNet as a fixed feature extractor for image representation.", "startOffset": 132, "endOffset": 136}, {"referenceID": 8, "context": "[9] used CNN features together with metadata and color histogram [9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] used CNN features together with metadata and color histogram [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 23, "context": "Most of these methods use as image representation ego-motion [24, 6, 28, 29], which is closely related to the user motion-based activity but cannot be reliably estimated in photo streams.", "startOffset": 61, "endOffset": 76}, {"referenceID": 5, "context": "Most of these methods use as image representation ego-motion [24, 6, 28, 29], which is closely related to the user motion-based activity but cannot be reliably estimated in photo streams.", "startOffset": 61, "endOffset": 76}, {"referenceID": 27, "context": "Most of these methods use as image representation ego-motion [24, 6, 28, 29], which is closely related to the user motion-based activity but cannot be reliably estimated in photo streams.", "startOffset": 61, "endOffset": 76}, {"referenceID": 28, "context": "Most of these methods use as image representation ego-motion [24, 6, 28, 29], which is closely related to the user motion-based activity but cannot be reliably estimated in photo streams.", "startOffset": 61, "endOffset": 76}, {"referenceID": 8, "context": "[9], Doherty et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] and Tavalera et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30], all other state-of-the-art methods have been designed for and tested on videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "In our previous work [30], we proposed an unsupervised method, called R-Clustering, aiming to segment photo streams from the what-the-camera-wearer-see perspective.", "startOffset": 21, "endOffset": 25}, {"referenceID": 3, "context": "The proposed methods relies on the combination of Agglomerative Clustering (AC), that usually has a high recall, but leads to temporal over-segmentation, with a statistically founded change detector, called ADWIN [4], which despite its high precision, usually leads to temporal under-segmentation.", "startOffset": 213, "endOffset": 216}, {"referenceID": 7, "context": "Both approaches are integrated in a Graph-Cut (GC) [8] framework to obtain a trade-off between AC and ADWIN, which have complementary properties.", "startOffset": 51, "endOffset": 54}, {"referenceID": 29, "context": "to our previous work published in [30], we introduce the following contributions:", "startOffset": 34, "endOffset": 38}, {"referenceID": 25, "context": "To manage the semantic redundancy, we will rely on WordNet [26], which is a lexical database that groups English words into sets of synonyms, providing additionally short definitions and word relations.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "The final confidence values are normalized so that they are in the interval [0, 1].", "startOffset": 76, "endOffset": 82}, {"referenceID": 26, "context": "To this end, we apply a Parzen Window Density Estimation method [27] to the matrix obtained by concatenating the semantic feature vectors along the sequence to obtain a smoothed and temporally coherent set of confidence values.", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "The CNN model that we use for computing the images representation is the AlexNet, which is detailed in [20].", "startOffset": 103, "endOffset": 107}, {"referenceID": 16, "context": "We used the deep learning framework Caffe [17] in order to run the CNN.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "Due to the fact that the weights have been trained on the ImageNet database [10], which is made of images containing single objects, we expect that the features extracted from images containing multiple objects will be representative of the environment.", "startOffset": 76, "endOffset": 80}, {"referenceID": 33, "context": "It is worth to remark that we did not use the weights obtained using a pre-trained CNN on the scenes from Places 205 database [34], since the Narrative camera\u2019s field of view is narrow, which means that mostly its field-of-view is very restricted to characterize the whole scene.", "startOffset": 126, "endOffset": 130}, {"referenceID": 29, "context": "As detailed in [30], to reduce the large variation distribution of the CNN features, which results problematic when computing distances between vectors, we used a signed root normalization to produce more uniformly distributed data [33].", "startOffset": 15, "endOffset": 19}, {"referenceID": 32, "context": "As detailed in [30], to reduce the large variation distribution of the CNN features, which results problematic when computing distances between vectors, we used a signed root normalization to produce more uniformly distributed data [33].", "startOffset": 232, "endOffset": 236}, {"referenceID": 29, "context": "The SR-clustering for temporal segmentation is based on fusing the semantic and contextual features with the R-Clustering method described in [30].", "startOffset": 142, "endOffset": 146}, {"referenceID": 30, "context": "We use the Cosine Similarity between samples, which is suited for high-dimensional positive spaces [31].", "startOffset": 99, "endOffset": 103}, {"referenceID": 3, "context": "To compensate the over-segmentation produced by AC, we proposed to model the egocentric sequence as a multi-dimensional data stream and to detect changes in the mean distribution through an adaptive learning method called ADWIN [4], which provides a rigorous statistical guarantee of performance in terms of false positive rate.", "startOffset": 228, "endOffset": 231}, {"referenceID": 14, "context": "The method, based on the Hoeffding\u2019s inequality [15], tests recursively if the difference between the averages of two temporally adjacent (sub)windows of the data, say W1 and W2, is larger than a threshold.", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "The value of the threshold takes into account if both sub-windows are large enough and distinct enough for a k\u2212dimensional signal [13], computed as:", "startOffset": 130, "endOffset": 134}, {"referenceID": 0, "context": ", n} are the set of contextual f c and semantic image features f for the i-th image, Ni is a set of temporal neighbors centered at i, and \u03c91 and \u03c92 (\u03c91, \u03c92 \u2208 [0, 1]) are the unary and the binary weighting terms, respectively.", "startOffset": 158, "endOffset": 164}, {"referenceID": 29, "context": "The EDUB-Seg dataset is an extension of the dataset used in our previous work [30], that we call EDUB-Seg (Set1) to distinguish it from the", "startOffset": 78, "endOffset": 82}, {"referenceID": 31, "context": "The camera wearers, as well as all the researchers involved on this work, were required to sign an informed written consent containing set of moral principles [32, 19].", "startOffset": 159, "endOffset": 167}, {"referenceID": 18, "context": "The camera wearers, as well as all the researchers involved on this work, were required to sign an informed written consent containing set of moral principles [32, 19].", "startOffset": 159, "endOffset": 167}, {"referenceID": 17, "context": "AIHS subset: is a subset of the daily images from the database called All I Have Seen (AIHS) [18], recorded by the SenseCam camera that takes a picture every 20 seconds.", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "Huji EgoSeg: due to the lack of other publicly available LTR datasets for event segmentation, we also test our temporal segmentation method to the ones provided in the dataset Huji EgoSeg [28].", "startOffset": 188, "endOffset": 192}, {"referenceID": 21, "context": "Following [22], we measured the performances of our method by using the F-Measure (FM) defined as follows:", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "The problem of the subjectivity when defining the ground truth was previously addressed in the context of image segmentation [25].", "startOffset": 125, "endOffset": 129}, {"referenceID": 24, "context": "In [25], the authors proposed two measures to compare different segmentations of the same image.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "The GCE and the LCE measures produce output values in the range [0, 1] where 0 means no error.", "startOffset": 64, "endOffset": 70}, {"referenceID": 29, "context": "The first two columns correspond to the datasets used in [30]: AIHS-subset and EDUB-Seg (Set1).", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "AIHS [18] EDUB-Seg Set1 EDUB-Seg Set2 EDUB-Seg", "startOffset": 5, "endOffset": 9}, {"referenceID": 5, "context": "Motion [6] 0.", "startOffset": 7, "endOffset": 10}, {"referenceID": 20, "context": "AC-Color [21] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "R-Clustering [30] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "We also evaluated the proposal of Lee and Grauman [21] (best with t = 25), where they apply an Agglomerative Clustering segmentation using LAB color histograms.", "startOffset": 50, "endOffset": 54}, {"referenceID": 29, "context": "The last row of the first section of the table shows the results obtained by our previously published method [30], where we were able to outperform the state-of-the-art of egocentric segmentation using contextual CNN features both on AIHS-subset and on EDUB-Seg Set1.", "startOffset": 109, "endOffset": 113}, {"referenceID": 8, "context": "[9], although the authors do not provide their trained model for applying this comparison.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "In the SR-ClusteringLSDA case, we used a simpler semantic features description, formed by using the weakly supervised concept extraction method proposed in [16], namely LSDA.", "startOffset": 156, "endOffset": 160}, {"referenceID": 27, "context": "Huji EgoSeg [28] LTR ADW-ImaggaD 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 27, "context": "Table 3: Average FM score on each of the tested methods using our proposal of semantic features on the dataset presented in [28].", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "segments obtained by the method of Lee and Grauman [21] is about 4 times bigger than the number of segments we obtained for the SenseCam dataset and about 2 times bigger than for the Narrative datasets.", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "Indeed, we achieve an higher FM score with respect to the method of Lee and Grauman [21], since it produces a considerable over-segmentation.", "startOffset": 84, "endOffset": 88}, {"referenceID": 13, "context": "Despite the common agreement about the inability of low-level features in providing understanding of the semantic structure present in complex events [14], and the need of semantic indexing and browsing systems, the use of high level features in the context of egocentric temporal segmentation and summarization has been very limited.", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "[11] and Lee and Grauman [21], temporal segmentation is still based on low level features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[11] and Lee and Grauman [21], temporal segmentation is still based on low level features.", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "[9] relies on the visual appearance of single images to predict the activity class of an image and on meta-data such as the day of the week and hour of the day to regularize over time.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2, 1, 3] employed the temporal segmentation method in [30] to extract and select segments with trackable people to be processed.", "startOffset": 0, "endOffset": 9}, {"referenceID": 0, "context": "[2, 1, 3] employed the temporal segmentation method in [30] to extract and select segments with trackable people to be processed.", "startOffset": 0, "endOffset": 9}, {"referenceID": 2, "context": "[2, 1, 3] employed the temporal segmentation method in [30] to extract and select segments with trackable people to be processed.", "startOffset": 0, "endOffset": 9}, {"referenceID": 29, "context": "[2, 1, 3] employed the temporal segmentation method in [30] to extract and select segments with trackable people to be processed.", "startOffset": 55, "endOffset": 59}], "year": 2016, "abstractText": "While wearable cameras are becoming increasingly popular, locating relevant information in large unstructured collections of egocentric images is still a tedious and time consuming process. This paper addresses the problem of organizing egocentric photo streams acquired by a wearable camera into semantically meaningful segments, hence making an important step towards the goal of automatically annotating these photos for browsing and retrieval. In the proposed method, first, contextual and semantic information is extracted for each image by employing a Convolutional Neural Networks approach. Later, a vocabulary of concepts is defined in a semantic space by relying on linguistic information. Finally, by exploiting the temporal coherence of concepts in photo streams, images which share contextual and semantic attributes are grouped together. The resulting temporal segmentation is particularly suited for further analysis, ranging from event recognition to semantic indexing and summarization. Experimental results over egocentric set of nearly 31,000 images, show the prominence of the proposed approach over state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}