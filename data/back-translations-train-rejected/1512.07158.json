{"id": "1512.07158", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2015", "title": "Feature Selection for Classification under Anonymity Constraint", "abstract": "Over the last decade, proliferation of various online platforms and their increasing adoption by billions of users have heightened the privacy risk of a user enormously. In fact, security researchers have shown that sparse microdata containing information about online activities of a user although anonymous, can still be used to disclose the identity of the user by cross-referencing the data with other data sources. To preserve the privacy of a user, in existing works several methods (k-anonymity, l-diversity, differential privacy) are proposed that ensure a dataset which is meant to share or publish bears small identity disclosure risk. However, the majority of these methods modify the data in isolation, without considering their utility in subsequent knowledge discovery tasks, which makes these datasets less informative. In this work, we consider labeled data that are generally used for classification, and propose two methods for feature selection considering two goals: first, on the reduced feature set the data has small disclosure risk, and second, the utility of the data is preserved for performing a classification task. Experimental results on various real-world datasets show that the method is effective and useful in practice.", "histories": [["v1", "Tue, 22 Dec 2015 17:06:01 GMT  (47kb)", "http://arxiv.org/abs/1512.07158v1", null], ["v2", "Sat, 13 Feb 2016 03:05:36 GMT  (54kb)", "http://arxiv.org/abs/1512.07158v2", null], ["v3", "Fri, 19 Feb 2016 02:01:57 GMT  (54kb)", "http://arxiv.org/abs/1512.07158v3", null], ["v4", "Thu, 17 Mar 2016 02:30:33 GMT  (55kb)", "http://arxiv.org/abs/1512.07158v4", null], ["v5", "Thu, 1 Dec 2016 01:05:59 GMT  (35kb)", "http://arxiv.org/abs/1512.07158v5", null], ["v6", "Tue, 31 Jan 2017 15:47:47 GMT  (202kb)", "http://arxiv.org/abs/1512.07158v6", "Transactions on Data Privacy 2017"], ["v7", "Mon, 6 Feb 2017 01:14:37 GMT  (201kb)", "http://arxiv.org/abs/1512.07158v7", "Transactions on Data Privacy 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["baichuan zhang", "noman mohammed", "vachik dave", "mohammad al hasan"], "accepted": false, "id": "1512.07158"}, "pdf": {"name": "1512.07158.pdf", "metadata": {"source": "CRF", "title": "Feature Selection for Classification under Anonymity Constraint", "authors": ["Baichuan Zhang", "Mohammad Al Hasan"], "emails": ["bz3@umail.iu.edu", "vsdave@umail.iu.edu", "alhasan@cs.iupui.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.07 158v 1 [cs.L G] 22 Dec 2"}, {"heading": "1. INTRODUCTION", "text": "Over the last decade, with the proliferation of various online platforms, such as web search, eCommerce, social networking, micro-messaging, streaming entertainment and cloud storage, the digital footprint of today's Internet user has evolved at an unprecedented rate. At the same time, however, the availability of sophisticated computer systems and advanced machine learning algorithms has enabled platform owners to use terabytes of digital footprints to build various predictive analytics and personalization products. Search engines and social networking platforms use search terms personalized for the delivery of sponsored ads that are personalized to a user's information needs; e-commerce platforms use the visitor's search history to bolster their merchandising history; digital or hard copy of all or part of that work is granted for classroom use, without charge, provided that copies are made for profit or commercial benefit, and that those first copies are carried on the page."}, {"heading": "1.1 Our Contributions", "text": "In this thesis we consider the task of selecting functions under the protection of privacy 3. Given a classification dataset with binary characteristics, our proposed solution finds a subset of characteristics, so that after projecting each instance onto these subsets each unit in the dataset fulfills a privacy restriction, called k-anonymous by Selection (AS), which we propose in this thesis. AS is a customized version of k-anonymity that strikes the right balance between disclosure risk and dataset utility, and it is particularly suitable for high-dimensional binary data. We also propose two algorithms: Maximum and Greedy. The first is a maximum itemset-mining method, and the second is a greedy incremental approach that realizes both the custom AS utility and attributes that we propose are especially intended for high-dimensional sparse micro data, where the characteristics are binary. The nature of such data differs from an existing dataset that is considered to be used in many existing datasets."}, {"heading": "2. PRIVACY BASICS", "text": "Given a data set (not necessarily binary) D, where each row corresponds to a person, and each column contains non-public information about that person; examples include illness, medication, sexual orientation, etc. In the context of online behavior, search terms, or a person's purchase history can be such information. Privacy protection methods make it difficult for an attacker to de-anonymize the identity of a person in the dataset. For de-anonymization, an attacker usually uses a set of attributes that act almost like a key and it clearly identifies some individuals in the datasets. These attributes are referred to as quasi-identification. Anonymity is a well-known privacy metric 2.1 (k-anonymity). A dataset D meets the k-anonymity-anonymity that exist at least k-entity e-entity."}, {"heading": "3. PROBLEM STATEMENT", "text": "In this section we will provide a formal definition of the problem: D (E, I) as a binary relationship between a set of entities (I) and a set of characteristics (I)."}, {"heading": "4. METHODS", "text": "In this section, we describe two algorithms, Maximum and Greedy, which we propose for the task of selecting features under privacy restrictions: Maximum is a method for selecting features based on maximum individual parts, and Greedy is a greedy method with filters based on privacy restrictions."}, {"heading": "4.1 Maximal Itemset based Approach", "text": "A key observation with respect to k anonymity by selection (AS) of a dataset is that this criterion fulfills the downward lock property under feature selection. (4) The following problem applies: Lemma 4.1. Say D (E, I) is a binary dataset and X-I and Y-I are two feature subgroups. (4) If X-Y, then there is at least one feature for which S-X (e) exists. Suppose X-Y and AS-X (D) are two feature subgroups. (4) Then, from the definition of AS, there is at least one feature e for which S-X (e) exists. < AS (5) Suppose that AX and AY are the totality of entities that are anonymous by selecting entity e in entity e (D) and Y (D)."}, {"heading": "4.1.3 Maximal Itemset based Method (Pseudo-code)", "text": "The pseudo-code of the whole process for Maximum is Givenin algorithms 1. Maximum takes the integer number k and the number of maximum patterns r as input and returns the final attribute set S, which fulfills the k anonymity by selection. line 1 uses the package used in [25] to generate all the maximum attribute sets, which fulfill the k anonymity by selection for the given k value. Line 2 groups the maximum attribute sets according to their size and selects the uppermost r-maximum attribute sets with the largest size and builds up the candidate attributes. Then, the algorithm calculates the attribute criteria HamDist of each attribute set in the candidate attributes and returns the best attribute set, which has the maximum value for this criterion. The complexity of the above algorithm depends mainly on the complexity of the maximum selection step of the attribute set in the candidate attributes and returns the best attribute set, which has the maximum value for this criterion the largest attribute. However, the complexity of the aforementioned algorithm depends on the complexity of the maximum selection step of the HamDist of each attribute set in the candidate attributes, and the best attribute set of the best attribute set, which has the maximum value for this criterion the maximum attribute size of the largest attribute."}, {"heading": "4.2 Greedy with Modular and Sub-Modular Objective Functions", "text": "A potential limitation of Maximum is that this method can be slow on dense data sets. Therefore, we propose a second method called Greedy, which is much faster because it greedily adds a new feature to an existing feasible feature set. Greedy can use various separation functions for greedy criteria that distinguish between positive and negative instances. In this work, we use HamDist (see Definition 4.2) and DistCnt (see Definition 4.3). Therefore, Greedy solves the problem (1) by replacing f with one of the two functions. Due to the monotonous property of these functions, Greedy ensures that when we add additional features, the objective functional value of (1) monotonous increases; the process stops as soon as there are no more features available to supplement the existing feature set, while at the same time ensuring the desired AS value of the projected data sets."}, {"heading": "If the above condition is satisfied with equality, the function is called modular.", "text": "Similarly, we can w (T), for the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics of the characteristics."}, {"heading": "5. EXPERIMENTS AND RESULTS", "text": "In order to evaluate the result of our proposed methods, we perform various experiments. Our main objective in the algorithm 3 Greedy algorithm for DistCntInput: D (E, I), k Output: S 1: T = \u2205 2: Repeat 3: S = T 4: \u2206 Hmax = 0,0 5: for u-I\\ S do 6: Calculation \u0445 H = DistCnt (S-U) \u2212 DistCnt (S) 7: if \u2206 H > \u2206 Hmax then 8: \u0445 Hmax = \u0445 H 9: um = u 10: end if 11: end for 12: T = S-U {um} 13: to AS (T (D) \u2265 k 14: Return experiments are intended to confirm how the performance of the task to maintain privacy varies when we change the value of AS - user-defined privacy threshold. We also compare the performance of our proposed usage methods with other existing anonymization methods. We use four real data sets for our experiments - the number of data sets that consist of two classes of data sets."}, {"heading": "5.1 Privacy-preserving Classification Tasks", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they are able to live, in which they, in which they, in which they are"}, {"heading": "5.2 Experimental Setting", "text": "For our experiments, we vary the K value of the proposed anonymity by selection (AS) metrically and perform Maximum and different variants of Greedy regardless of the structure of projected classification data sets for which the AS value is at least k. We use the names HamDist and DistCnt for the two variants of Greedy (algorithm 2 and 3), the Hamming Distance and Distinguish greedily count criteria, respectively, as we have already mentioned, k-anonymity-based method imposes strong limitations that severely affect the usefulness of the data sets. To show that we use k-anonymity as our privacy criteria for different variants of Greedy. We call these competing methods k-anonymity HamD is a strong limitation that greatly affects the usefulness of the data collection. We also use three other methods for comparing our proposed methods."}, {"heading": "5.3 Name Entity Disambiguation", "text": "In Table 4 we report on the value of the anonymized name unit ambiguity task with different privacy methods (in rows) for different k values (in columns). For better comparison, our proposed methods, competing methods and nonprivate methods are grouped by the horizontal lines: while our proposed methods are in the top group, the competing methods are in the bottom group, and non-private methods are. We also report on the performance of the AUC with full characteristics set (last line), since no privacy restriction is imposed on this value regardless of k. As we see, for most of the methods that k increase the number of selected characteristics, the cell values that were reversed."}, {"heading": "5.4 Adult Data", "text": "The performance of the different methods for the adult dataset is shown in Table 5, where the rows and columns are organized identically to the previous table. Adult datasets are low dimensional and dense (27.9% values are not zero). Achieving privacy on such a dataset is comparatively easier, so the existing anonymization methods work quite well on it. As we can see, RF performs the best performance of all methods. RF's good performance is based on the very small percentage of mirrors, ranging from 0.60% to 1.50% for various k values. Basically, RF can achieve k anonymity on this dataset with a very small number of mirrors, which helps it maintain the classification utility of the dataset. For the same reason, k-anonymity HamDist and k-anonymity DistCnt methods are also able to obtain many dimensions (8 of 19 for k = 5) of this dataset, relatively good performance and good."}, {"heading": "5.5 Spam Email Filtering", "text": "In Table 6, we compare the AUC value of various methods of spam email filtering. These are very high-dimensional data with 24604 characteristics. As we can see, our proposed methods, especially DistCnt and HamDist, perform better than the competing methods. For example, the classification AUC of RF for k = 5 is 0.87 with a flip rate of 1.30%, with less than 0.045% of DistCnt attributes receiving an AUC value of 0.95, which is equivalent to the AUC value when using the full set of characteristics. Again, k anonymity methods perform worse because they select fewer characteristics due to greater limitation of this privacy metric. For example, HamDist chooses 11 characteristics for k = 5, but k-Anonymity HamDist chooses only 4 characteristics. For this reason, classification results that use k anonymity as a constraint on all settings are worse than those that use our proposed AS data protection metric."}, {"heading": "5.6 German Data", "text": "As we can see, our proposed methods, in particular HamDist, perform better than the competing approaches. For example, RF under k = 8 receives 0.69 AUC with 6 selected characteristics. However, RF performs worse in terms of AUC. For example, RF reaches 0.58 AUC under k = 8. The poor performance of RF is due to the high percentage of flips, which varies from 11% to 16% at different k values. The significant flip rate deteriorates the data service program and results in poor classification performance. In addition, the AUC performance of RF falls from 0.65 to 0.58 when we increase the k values; in contrast, the AUC results of our proposed methods remain stable as we increase the k value. Of all the competing methods, CM has the worst AUC because they have the worst value for different k values - similar to a random predictor! Methods that use k anonymity as a privacy indicator are also worse than our proposed methods because they may choose less than our suggested ones."}, {"heading": "6. RELATED WORK", "text": "In terms of the privacy model, several privacy metrics have been widely used to quantify the privacy risk of published data instances, such as k-anonymity [23], t-closeness [17], diversity [19], and differential privacy [7]. Existing work on privacy preservation data mining solves a specific data mining problem in the face of privacy limitations across data instances, such as classification [11, 29], regression [11], clustering [18,27], and frequent pattern mining [8]. In fact, however, the majority of the above work [11, 26-29] considers distributed privacy, where the data set is divided among multiple participants who possess different portions of data, and the goal is to dismantle common insights on the global data without compromising the privacy of the local parts. A few other work [4, 5] consider minimal privacy by ensuring that the output of a minimum data requirements minimum minimum analysis task is not disclosed."}, {"heading": "7. CONCLUSION", "text": "We define a new anonymity quantity called k anonymity by selection, which is particularly suitable for high-dimensional micro data. We also propose two methods for selecting characteristics together with two classification tools. These metrics fulfil submodular properties, thus enabling effective greedy algorithms. In the experiment section, we show that both proposed methods select good quality characteristics on a variety of data sets in order to maintain the classification tool, but fulfil the user-defined anonymity limitation."}, {"heading": "8. REFERENCES", "text": "[1] C. C. Aggarwal. On k-Anonymit\u00e4t and the curse of dimensionality. VLDB, 2005. [2] E. Amaldi and V. Kann. On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems. Theor. Comput. Sci., 1998. [3] R. J. Bayardo and R. Agrawal. Data privacy through optimal k-anonymization. ICDE '05, 2005. [5] R. Bhaskar, S. Laxman, A. Smith, and A. Thakurta. Discovering frequent patterns in sensitive data. KDD' 10, pp. 503-512, 2010. [5] L. Bonomi and L. Xiong. Mining frequent patterns with differential privacy. Proc. VLDB Endow., 2013. [6] J.-W. Byun, A. Kamra, E. Bertino, and N. Li. Efficient k-Anonymization using clustering techniques."}], "references": [{"title": "On k-anonymity and the curse of dimensionality", "author": ["C.C. Aggarwal"], "venue": "VLDB", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems", "author": ["E. Amaldi", "V. Kann"], "venue": "Theor. Comput. Sci.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Data privacy through optimal k-anonymization", "author": ["R.J. Bayardo", "R. Agrawal"], "venue": "ICDE\u201905", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Discovering frequent patterns in sensitive data", "author": ["R. Bhaskar", "S. Laxman", "A. Smith", "A. Thakurta"], "venue": "KDD\u201910, pages 503\u2013512", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Mining frequent patterns with differential privacy", "author": ["L. Bonomi", "L. Xiong"], "venue": "Proc. VLDB Endow.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient k-anonymization using clustering techniques", "author": ["J.-W. Byun", "A. Kamra", "E. Bertino", "N. Li"], "venue": "DASFAA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "The algorithmic foundations of differential privacy", "author": ["C. Dwork", "A. Roth"], "venue": "Foundation and Trends in Theoretical Computer Science", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Privacy preserving mining of association rules", "author": ["A. Evfimievski", "R. Srikant", "R. Agrawal", "J. Gehrke"], "venue": "KDD\u201902, pages 217\u2013228", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Monitoring web browsing behavior with differential privacy", "author": ["L. Fan", "L. Bonomi", "L. Xiong", "V. Sunderam"], "venue": "WWW. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "A practical framework for privacy-preserving data analytics", "author": ["L. Fan", "H. Jin"], "venue": "WWW", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Privacy-preserving data sharing in high dimensional regression and classification settings", "author": ["S.E. Fienberg", "J. Jin"], "venue": "Journal of Privacy and Confidentiality, 4(1):10", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "J. Mach. Learn. Res.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Transforming data to satisfy privacy constraints", "author": ["V.S. Iyengar"], "venue": "KDD", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Injecting utility into anonymized datasets", "author": ["D. Kifer", "J. Gehrke"], "venue": "SIGMOD", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "No free lunch in data privacy", "author": ["D. Kifer", "A. Machanavajjhala"], "venue": "SIGMOD\u201911, pages 193\u2013204", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Incognito: Efficient full-domain k-anonymity", "author": ["K. LeFevre", "D.J. DeWitt", "R. Ramakrishnan"], "venue": "SIGMOD\u201905, pages 49\u201360", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "t-closeness: Privacy beyond k-anonymity and l-diversity", "author": ["N. Li", "T. Li"], "venue": "ICDE\u201907", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "C", "author": ["X. Lin", "C. Clifton", "M. Zhu", "E.M. Modeling", "X. Lin"], "venue": "Clifton, and M. Zhu. Privacy-preserving clustering with distributed em mixture modeling. Knowledge and Information Systems, pages 68\u201381", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "L-diversity: Privacy beyond k-anonymity", "author": ["A. Machanavajjhala", "D. Kifer", "J. Gehrke", "M. Venkitasubramaniam"], "venue": "ACM TKDD", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "On the complexity of optimal k-anonymity", "author": ["A. Meyerson", "R. Williams"], "venue": "ACM PODS", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Submodular set functions", "author": ["G.C. Michele Conforti"], "venue": "matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem. Discrete Applied Mathematics", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1984}, {"title": "Robust de-anonymization of large sparse datasets", "author": ["A. Narayanan", "V. Shmatikov"], "venue": "IEEE Symposium on Security and Privacy", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Protecting respondents\u2019 identities in microdata release", "author": ["P. Samarati"], "venue": "IEEE TKDE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "K-anonymity: A model for protecting privacy", "author": ["L. Sweeney"], "venue": "Int. J. Uncertain. Fuzziness Knowl.-Based Syst.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "LCM ver", "author": ["T. Uno", "M. Kiyomi", "H. Arimura"], "venue": "2: Efficient mining algorithms for frequent/closed/maximal itemsets. FIMI-ICDM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Privacy preserving association rule mining in vertically partitioned data", "author": ["J. Vaidya", "C. Clifton"], "venue": "KDD\u201902, pages 639\u2013644", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "Privacy-preserving k-means clustering over vertically partitioned data", "author": ["J. Vaidya", "C. Clifton"], "venue": "KDD", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "Privacy-preserving decision trees over vertically partitioned data", "author": ["J. Vaidya", "C. Clifton", "M. Kantarcioglu", "A.S. Patterson"], "venue": "ACM TKDD", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Privacy-preserving naive bayes classification", "author": ["J. Vaidya", "M. Kantarc", "C. Clifton"], "venue": "The VLDB Journal,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Data Mining and Analysis: Fundamental Concepts and Algorithms", "author": ["M.J. Zaki", "J. Wagner Meira"], "venue": "Cambridge University Press", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": "[22] have identified a Netflix subscriber from his anonymous movie rating by using Internet Movie Database (IMBD) as the source of background information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": ", all the points are far from each other [30].", "startOffset": 41, "endOffset": 45}, {"referenceID": 22, "context": "given data instance is k-anonymity [23] which requires that for any data instance, there are at least k \u2212 1 distinct data instances that share the same feature vector.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "is not a viable solution even for a k value of 2 [1].", "startOffset": 49, "endOffset": 52}, {"referenceID": 22, "context": "One can view this as column suppression instead of more commonly used row suppression for achieving k-anonymity [23].", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "it is well-known that privacy is always at odd with the utility of a knowledge-based system [14, 15], so finding the right balance is a difficult task.", "startOffset": 92, "endOffset": 100}, {"referenceID": 14, "context": "it is well-known that privacy is always at odd with the utility of a knowledge-based system [14, 15], so finding the right balance is a difficult task.", "startOffset": 92, "endOffset": 100}, {"referenceID": 11, "context": "Besides, feature selection itself, without considering the privacy constraint, is an NP-Hard problem [12].", "startOffset": 101, "endOffset": 105}, {"referenceID": 21, "context": "Nevertheless, as shown in [22], binary attributes are sufficient for an attacker to de-anonymize", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "For classification data, Iyengar [13] has proposed a metric called CM (Classification Metric) which is defined as below.", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "2 (Classification Metric [13]).", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "Feature selection [12] for a classification task is to select a subset of highly predictive variables so that classification accuracy possibly improves which happens due to the fact that contradictory or noisy attributes are generally ignored during the feature selection step.", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "Since the general task of feature selection even for the case of linearly separable positive and negative examples is NPHard [2], the problem 1 is also NP-Hard, because by setting k = 0, we can reduce the feature selection problem to this problem.", "startOffset": 125, "endOffset": 128}, {"referenceID": 29, "context": "Hence, Apriori [30] like algorithm for itemset mining can be used for effectively enumerating all the feature subsets of D which satisfies the required k-anonymity by selection constraint.", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "Maximal uses the LCM-Miner package provided in [25] which, at present, is the fastest method for finding maximal frequent itemsets.", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "Line 1 uses the package used in [25] to generate all the maximal feature sets that satisfy k-anonymity by selection for the given k value.", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "For modular function f(S) = f(S) [21].", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "The Adult benchmark dataset is based on census data and has been widely used in earlier works on k-anonymization [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "We call these competing methods Non-Private Greedy, CM Greedy [13], and RF [6].", "startOffset": 62, "endOffset": 66}, {"referenceID": 5, "context": "We call these competing methods Non-Private Greedy, CM Greedy [13], and RF [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 5, "context": "RF is a Randomization Flipping based kanonymization technique presented in [6], which randomly flips the feature value such that each instance in the dataset satisfies the k-anonymity privacy constraint.", "startOffset": 75, "endOffset": 78}, {"referenceID": 12, "context": "CM greedy represents another greedy based method which uses Classification Metric utility criterion proposed in [13] (See definition 2.", "startOffset": 112, "endOffset": 116}, {"referenceID": 12, "context": "Iyenger [13] uses a genetic algorithm for the classification task, but for a fair comparison we use CM criterion in the Greedy algorithm and with the selected features we use identical setup for classification.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "For practical k-anonymization, k value between 5 and 10 is suggested in earlier works [24]; we use three different k values, which are 5, 8 and 11.", "startOffset": 86, "endOffset": 90}, {"referenceID": 12, "context": "CM Greedy [13] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "68 (2) RF [6] 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "CM Greedy [13] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "75 (5) RF [6] 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "CM Greedy [13] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "86 (3) RF [6] 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "CM Greedy [13] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "57 (2) RF [6] 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 22, "context": "In terms of privacy model, several privacy metrics have been widely used in order to quantify the privacy risk of published data instances, such as k-anonymity [23], t-closeness [17], ldiversity [19], and differential privacy [7].", "startOffset": 160, "endOffset": 164}, {"referenceID": 16, "context": "In terms of privacy model, several privacy metrics have been widely used in order to quantify the privacy risk of published data instances, such as k-anonymity [23], t-closeness [17], ldiversity [19], and differential privacy [7].", "startOffset": 178, "endOffset": 182}, {"referenceID": 18, "context": "In terms of privacy model, several privacy metrics have been widely used in order to quantify the privacy risk of published data instances, such as k-anonymity [23], t-closeness [17], ldiversity [19], and differential privacy [7].", "startOffset": 195, "endOffset": 199}, {"referenceID": 6, "context": "In terms of privacy model, several privacy metrics have been widely used in order to quantify the privacy risk of published data instances, such as k-anonymity [23], t-closeness [17], ldiversity [19], and differential privacy [7].", "startOffset": 226, "endOffset": 229}, {"referenceID": 10, "context": "Existing works on privacy preserving data mining solve a specific data mining problem given a privacy constraint over the data instances, such as classification [11, 29], regression [11], clustering [18, 27] and frequent pattern mining [8].", "startOffset": 161, "endOffset": 169}, {"referenceID": 28, "context": "Existing works on privacy preserving data mining solve a specific data mining problem given a privacy constraint over the data instances, such as classification [11, 29], regression [11], clustering [18, 27] and frequent pattern mining [8].", "startOffset": 161, "endOffset": 169}, {"referenceID": 10, "context": "Existing works on privacy preserving data mining solve a specific data mining problem given a privacy constraint over the data instances, such as classification [11, 29], regression [11], clustering [18, 27] and frequent pattern mining [8].", "startOffset": 182, "endOffset": 186}, {"referenceID": 17, "context": "Existing works on privacy preserving data mining solve a specific data mining problem given a privacy constraint over the data instances, such as classification [11, 29], regression [11], clustering [18, 27] and frequent pattern mining [8].", "startOffset": 199, "endOffset": 207}, {"referenceID": 26, "context": "Existing works on privacy preserving data mining solve a specific data mining problem given a privacy constraint over the data instances, such as classification [11, 29], regression [11], clustering [18, 27] and frequent pattern mining [8].", "startOffset": 199, "endOffset": 207}, {"referenceID": 7, "context": "Existing works on privacy preserving data mining solve a specific data mining problem given a privacy constraint over the data instances, such as classification [11, 29], regression [11], clustering [18, 27] and frequent pattern mining [8].", "startOffset": 236, "endOffset": 239}, {"referenceID": 10, "context": "of the above works [11, 26\u201329] consider distributed privacy where the dataset is partitioned among multiple participants owning different portions of the data, and the goal is to mine shared insights over the global data without compromising the privacy of the local portions.", "startOffset": 19, "endOffset": 30}, {"referenceID": 25, "context": "of the above works [11, 26\u201329] consider distributed privacy where the dataset is partitioned among multiple participants owning different portions of the data, and the goal is to mine shared insights over the global data without compromising the privacy of the local portions.", "startOffset": 19, "endOffset": 30}, {"referenceID": 26, "context": "of the above works [11, 26\u201329] consider distributed privacy where the dataset is partitioned among multiple participants owning different portions of the data, and the goal is to mine shared insights over the global data without compromising the privacy of the local portions.", "startOffset": 19, "endOffset": 30}, {"referenceID": 27, "context": "of the above works [11, 26\u201329] consider distributed privacy where the dataset is partitioned among multiple participants owning different portions of the data, and the goal is to mine shared insights over the global data without compromising the privacy of the local portions.", "startOffset": 19, "endOffset": 30}, {"referenceID": 28, "context": "of the above works [11, 26\u201329] consider distributed privacy where the dataset is partitioned among multiple participants owning different portions of the data, and the goal is to mine shared insights over the global data without compromising the privacy of the local portions.", "startOffset": 19, "endOffset": 30}, {"referenceID": 3, "context": "A few other works [4, 5] consider output privacy by ensuring that the output of a data mining task does not reveal sensitive information.", "startOffset": 18, "endOffset": 24}, {"referenceID": 4, "context": "A few other works [4, 5] consider output privacy by ensuring that the output of a data mining task does not reveal sensitive information.", "startOffset": 18, "endOffset": 24}, {"referenceID": 9, "context": "Fan [10] proposed a practical data analytics framework that generates differentially private aggregates which can be used to perform data mining and recommendation tasks.", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "[9] adopted differential", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Samarati [23] has proposed formal methods of k-anonymity using suppression and generalization techniques.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "[20] proved that two definition of k-optimality is NPhard: first, to find the minimum number of cell value that needs to be suppressed; second, to find the minimum number of attributes that needs to be suppressed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "anonymization [3, 16, 20, 23].", "startOffset": 14, "endOffset": 29}, {"referenceID": 15, "context": "anonymization [3, 16, 20, 23].", "startOffset": 14, "endOffset": 29}, {"referenceID": 19, "context": "anonymization [3, 16, 20, 23].", "startOffset": 14, "endOffset": 29}, {"referenceID": 22, "context": "anonymization [3, 16, 20, 23].", "startOffset": 14, "endOffset": 29}, {"referenceID": 22, "context": "Samarati [23] proposes one of the earliest metric for privacy, which is called generalization height.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "Average size of anonymous groups [19] and discernibility [3] are other privacy metrics.", "startOffset": 33, "endOffset": 37}, {"referenceID": 2, "context": "Average size of anonymous groups [19] and discernibility [3] are other privacy metrics.", "startOffset": 57, "endOffset": 60}, {"referenceID": 13, "context": "[14] propose methods that inject utility in the form of data distribution information into k-anonymous and l-diverse tables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "gar [13] proposes a utility metric called CM which is explicitly designed for a classification dataset.", "startOffset": 4, "endOffset": 8}], "year": 2017, "abstractText": "Over the last decade, proliferation of various online platforms and their increasing adoption by billions of users have heightened the privacy risk of a user enormously. In fact, security researchers have shown that sparse microdata containing information about online activities of a user although anonymous, can still be used to disclose the identity of the user by cross-referencing the data with other data sources. To preserve the privacy of a user, in existing works several methods (k-anonymity, l-diversity, differential privacy) are proposed that ensure a dataset which is meant to share or publish bears small identity disclosure risk. However, the majority of these methods modify the data in isolation, without considering their utility in subsequent knowledge discovery tasks, which makes these datasets less informative. In this work, we consider labeled data that are generally used for classification, and propose two methods for feature selection considering two goals: first, on the reduced feature set the data has small disclosure risk, and second, the utility of the data is preserved for performing a classification task. Experimental results on various real-world datasets show that the method is effective and useful in practice.", "creator": "LaTeX with hyperref package"}}}