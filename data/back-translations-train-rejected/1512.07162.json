{"id": "1512.07162", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2015", "title": "Heuristic algorithms for finding distribution reducts in probabilistic rough set model", "abstract": "Attribute reduction is one of the most important topics in rough set theory. Heuristic attribute reduction algorithms have been presented to solve the attribute reduction problem. It is generally known that fitness functions play a key role in developing heuristic attribute reduction algorithms. The monotonicity of fitness functions can guarantee the validity of heuristic attribute reduction algorithms. In probabilistic rough set model, distribution reducts can ensure the decision rules derived from the reducts are compatible with those derived from the original decision table. However, there are few studies on developing heuristic attribute reduction algorithms for finding distribution reducts. This is partly due to the fact that there are no monotonic fitness functions that are used to design heuristic attribute reduction algorithms in probabilistic rough set model. The main objective of this paper is to develop heuristic attribute reduction algorithms for finding distribution reducts in probabilistic rough set model. For one thing, two monotonic fitness functions are constructed, from which equivalence definitions of distribution reducts can be obtained. For another, two modified monotonic fitness functions are proposed to evaluate the significance of attributes more effectively. On this basis, two heuristic attribute reduction algorithms for finding distribution reducts are developed based on addition-deletion method and deletion method. In particular, the monotonicity of fitness functions guarantees the rationality of the proposed heuristic attribute reduction algorithms. Results of experimental analysis are included to quantify the effectiveness of the proposed fitness functions and distribution reducts.", "histories": [["v1", "Tue, 22 Dec 2015 17:17:45 GMT  (1316kb)", "http://arxiv.org/abs/1512.07162v1", "44 pages, 24 figures"]], "COMMENTS": "44 pages, 24 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["xi'ao ma", "guoyin wang", "hong yu"], "accepted": false, "id": "1512.07162"}, "pdf": {"name": "1512.07162.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Xi\u2019ao Ma", "Guoyin Wang", "Hong Yu"], "emails": ["maxiao73559@163.com", "wanggy@ieee.org", "hongyu.cqupt@gmail.com"], "sections": [{"heading": null, "text": "It is well known that fitness functions play a key role in the development of heuristic attribute reduction algorithms. Monotonicity of fitness functions can guarantee the validity of heuristic attribute reduction algorithms. In the probabilistic rough-set model, distribution reductions can ensure that the decision rules derived from the reductions are compatible with those derived from the original decision table. However, there are few studies on the development of heuristic attribute reduction algorithms for determining distribution reductions. This is partly due to the fact that there are no monotonic fitness functions used to design heuristic attribute reduction algorithms in the probable coarse set model."}, {"heading": "1. Introduction", "text": "The second category introduced by Pawlak focuses on the study of attribute reduction algorithms. For the definition of attribute reduction methods is a valid mathematical theory that effectively deals with imprecise, vague and uncertain information, and it has developed into an area of active research covering many fields, such as machine learning, data mining, knowledge gathering, intelligent data analysis [5, 10, 11, 24, 40, 44]. In rough system theory, attribute reduction is a popular task to select the essential attributes of a given information system that maintain or improve a certain classification property as the entire set of available attributes. Therefore, an attribute reduction can be set as a minimal attribute that can preserve or improve certain criteria. Attribute reduction of attributes is often helpful in order to reduce computorial costs, save disk space and prevent excessive adaptation of attributes from being divided into two main studies of attributes."}, {"heading": "2. Preliminary knowledge", "text": "In this section, we will recall the basic concepts associated with attribute reduction to rough set theory. [25, 52] Q = Q (F). The definition of distribution reductionsAn information system is a four-headed system (U, A, V, F) in which U is a finite, nonempty set of objects designated as the universe, A is a nonempty finite set of attributes, V = A, A, Va, where Va is a nonempty set of values of attributes. For Brevity, IS = (U, V, f) a decision can be written as IS = (U, A). For each subset of attributes R, an indictability relationship IND (R) to U, an indictability relationship is defined as: IND (R)."}, {"heading": "3. Heuristic algorithms to find distribution reducts in probabilistic rough set model", "text": "The verification of the common condition and the evaluation of the significance of attributes are two key steps in the proposed fitness reduction algorithms based on the addition deletion strategy and the deletion strategy. Furthermore, the monotonicity of the fitness functions is very important for the validity of the monotonic attribute reduction algorithms to verify the jointly sufficient condition and the evaluation of the significance of attributes. In order to obtain the distribution reduction algorithms with heuristic attributes, we construct two monotonic fitness functions in this section first. Then we give the appropriate definition of the distribution based on the monotonic fitness functions that are constructed. Afterwards, we propose two significance dimensions of the attributes by constructing the granularity of the partitions through the monotonic fitness functions. In addition, the core and core utation algorithms for the distribution reductions are also presented."}, {"heading": "4. Experimental results", "text": "In this section, a number of experiments were conducted to demonstrate that our proposed methods are effective and applicable. Ten real-world benchmark datasets were selected for experimental evaluation; all datasets were derived from the UCI Repository of Machine Learning Databases \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 Significance of these datasets was widely used in the literature; \u2212 Because the datasets may contain missing values or continuous attributes of the selected UCI datasets, they are treated in advance of attribute reduction; \u2212 Missing values were filled with averages for continuous attributes and mode values for nominal attributes; \u2212 Continuous attributes were discredited using the same frequency discretirement method; and all pre-processing methods were implemented using WEKA filters [9].4.1. Mononicity was checked in this section to verify the effectiveness of the proposed functions."}, {"heading": "1 5.5 \u00b1 4.5 4.6 \u00b1 4.4 3.7 \u00b1 4.1 10.0 \u00b1 0.0 10.0 \u00b1 0.0 10.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 11.9 \u00b1 5.7 11.9 \u00b1 5.7 11.9 \u00b1 5.6 16.0 \u00b1 0.0 16.0 \u00b1 0.0 16.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 9.5 \u00b1 6.1 9.4 \u00b1 6.1 8.6 \u00b1 6.5 15.0 \u00b1 0.0 15.0 \u00b1 0.0 15.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 6.2 \u00b1 5.3 6.6 \u00b1 5.6 2.1 \u00b1 3.3 12.0 \u00b1 0.0 12.0 \u00b1 0.0 12.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 3.8 \u00b1 2.9 3.4 \u00b1 2.4 3.0 \u00b1 2.0 7.0 \u00b1 0.0 6.0 \u00b1 0.0 6.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 6.0 \u00b1 5.0 5.9 \u00b1 4.9 4.0 \u00b1 4.6 11.0 \u00b1 0.0 11.0 \u00b1 0.0 11.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 8.6 \u00b1 6.4 9.4 \u00b1 5.1 7.6 \u00b1 5.3 15.0 \u00b1 0.0 14.0 \u00b1 0.0 14.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 12.3 \u00b1 13.6 15.0 \u00b1 14.0 9.6 \u00b1 12.7 29.0 \u00b1 0.0 29.0 \u00b1 0.0 29.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 14.5 \u00b1 9.5 14.2 \u00b1 9.3 3.2 \u00b1 6.6 23.0 \u00b1 0.0 23.0 \u00b1 0.0 23.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 5.5 \u00b1 4.5 5.5 \u00b1 4.5 4.9 \u00b1 4.8 10.0 \u00b1 0.0 10.0 \u00b1 0.0 10.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 5.1 \u00b1 4.1 5.5 \u00b1 4.5 5.5 \u00b1 4.5 9.0 \u00b1 0.0 9.0 \u00b1 0.0 10.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 12.0 \u00b1 5.6 12.1 \u00b1 5.5 12.5 \u00b1 5.2 16.0 \u00b1 0.0 16.0 \u00b1 0.0 16.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 9.6 \u00b1 6.1 9.4 \u00b1 6.1 9.8 \u00b1 5.6 15.0 \u00b1 0.0 15.0 \u00b1 0.0 15.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 6.2 \u00b1 5.3 6.2 \u00b1 5.3 6.3 \u00b1 5.3 12.0 \u00b1 0.0 12.0 \u00b1 0.0 12.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 2.9 \u00b1 2.1 2.9 \u00b1 2.1 3.2 \u00b1 1.9 6.0 \u00b1 0.0 6.0 \u00b1 0.0 6.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 6.0 \u00b1 5.0 6.0 \u00b1 5.0 5.9 \u00b1 4.9 11.0 \u00b1 0.0 11.0 \u00b1 0.0 11.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 8.7 \u00b1 5.6 8.7 \u00b1 6.2 8.2 \u00b1 5.2 14.0 \u00b1 0.0 14.0 \u00b1 0.0 14.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 15.0 \u00b1 14.0 15.0 \u00b1 14.0 15.0 \u00b1 14.0 29.0 \u00b1 0.0 29.0 \u00b1 0.0 29.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 14.5 \u00b1 9.5 14.1 \u00b1 9.3 12.6 \u00b1 9.7 23.0 \u00b1 0.0 23.0 \u00b1 0.0 23.0 \u00b1 0.0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 5.5 \u00b1 4.5 5.5 \u00b1 4.5 6.0 \u00b1 5.1 10.0 \u00b1 0.0 10.0 \u00b1 0.0 10.0 \u00b1 0.0", "text": "From the experimental results, we can conclude that the distribution reductions in the probable coarse set model are relatively the better choice, since the selected attributes have the higher classification accuracy. 4.4. Comparison of distribution reductions with rank-based attribute reduction methods In this section, we compared the classification accuracy of the distribution reductions with rank-based attribute reduction methods (RBAR). RBAR is a classic attribute reduction algorithm. In RBAR, the attributes are classified worldwide based on the significance of individual attributes and the best attributes of the first k are selected. In our experiments, the significance of individual attributes was evaluated using the fitness functions G\u03b7 \u2212 CE, G\u03b7 \u2212 KG and G\u03b7 \u2212 CG, and the different values of k were compared based on the subset size of the corresponding (\u03b1, \u03b2) low distribution reductions, which are achieved by the DRCG size, LDRG and LDRG size models."}, {"heading": "1 0.7745 \u00b1 0.0000 0.7147 \u00b1 0.0000 0.7147 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 0.4504 \u00b1 0.0000 0.4504 \u00b1 0.0000 0.4504 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 0.8943 \u00b1 0.0000 0.8943 \u00b1 0.0000 0.8943 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 0.9307 \u00b1 0.0000 0.8416 \u00b1 0.0000 0.8416 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 0.8652 \u00b1 0.0000 0.8652 \u00b1 0.0000 0.8652 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 0.8260 \u00b1 0.0000 0.8312 \u00b1 0.0000 0.8212 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 0.8767 \u00b1 0.0000 0.8767 \u00b1 0.0000 0.8767 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 0.9178 \u00b1 0.0000 0.9178 \u00b1 0.0000 0.9223 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 0.7430 \u00b1 0.0000 0.7420 \u00b1 0.0000 0.7420 \u00b1 0.0000", "text": "Classification accuracy was achieved by applying the same experimental settings and methods used in subsection 4.3.Tables 10-13. Experimental results from RBAR on ten sets of data using BayesNet and SMO, where RBARCE, RBARKG and RBARCG represent the attribute reductions obtained by using the addition deletion method, LDRCE and LDRCG respectively show the meaning of individual attributes better than RBARCE and RBARCG. Comparing with Tables 4-7, it is easy to see that for the (\u03b1, \u03b2) low distribution reductions achieved by the addition deletion method, LDRCE and LDRCG achieve the same distribution results based on the standard distribution method."}, {"heading": "1 0.7391 \u00b1 0.0000 0.7147 \u00b1 0.0000 0.7147 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 0.6040 \u00b1 0.0000 0.6040 \u00b1 0.0000 0.6040 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 0.9563 \u00b1 0.0000 0.9563 \u00b1 0.0000 0.9563 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 0.9307 \u00b1 0.0000 0.8416 \u00b1 0.0000 0.8416 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 0.8551 \u00b1 0.0000 0.8551 \u00b1 0.0000 0.8551 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 0.9216 \u00b1 0.0000 0.9203 \u00b1 0.0000 0.9048 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 0.9528 \u00b1 0.0000 0.9528 \u00b1 0.0000 0.9528 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 0.9236 \u00b1 0.0000 0.9236 \u00b1 0.0000 0.9229 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 0.7490 \u00b1 0.0000 0.7520 \u00b1 0.0000 0.7520 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 0.7663 \u00b1 0.0000 0.7092 \u00b1 0.0000 0.7147 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 0.4504 \u00b1 0.0000 0.4504 \u00b1 0.0000 0.4504 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 0.8943 \u00b1 0.0000 0.8943 \u00b1 0.0000 0.8943 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 0.8416 \u00b1 0.0000 0.8416 \u00b1 0.0000 0.8416 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 0.8652 \u00b1 0.0000 0.8652 \u00b1 0.0000 0.8652 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 0.8312 \u00b1 0.0000 0.8312 \u00b1 0.0000 0.8087 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 0.8767 \u00b1 0.0000 0.8767 \u00b1 0.0000 0.8767 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 0.9178 \u00b1 0.0000 0.9178 \u00b1 0.0000 0.9223 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 0.7430 \u00b1 0.0000 0.7420 \u00b1 0.0000 0.7420 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5. Conclusion", "text": "The addition-deletion method and the deletion method based on attribute reduction algorithms are two representative heuristic attribute reduction algorithms in coarse set theory. Fitness functions play a crucial role in the design of heuristic attribute reduction algorithms. The monotonicity of fitness functions is very important to guarantee the validity of heuristic attribute reduction algorithms. This paper aims to develop heuristic attribute reduction algorithms to find distribution reductions in probable coarse set models. First, we proposed two monotonous fitness functions \u03b7 (\u03b1, \u03b2) R and \u00b5 (\u03b1, \u03b2) R. The equivalence definitions of the (\u03b1, \u03b2) low and upper distribution reductions are each based on \u03b7 (\u03b1, \u03b2) R and \u00b5 (\u03b1, \u03b2) R. The equivalence definitions of two distribution reductions can be used to design the stopheuristic attributes."}, {"heading": "1 0.7527 \u00b1 0.0000 0.6902 \u00b1 0.0000 0.7147 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 0.6040 \u00b1 0.0000 0.6040 \u00b1 0.0000 0.6040 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 0.9563 \u00b1 0.0000 0.9563 \u00b1 0.0000 0.9563 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 0.8416 \u00b1 0.0000 0.8416 \u00b1 0.0000 0.8416 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 0.8551 \u00b1 0.0000 0.8551 \u00b1 0.0000 0.8551 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 0.9203 \u00b1 0.0000 0.9203 \u00b1 0.0000 0.9078 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 0.9528 \u00b1 0.0000 0.9528 \u00b1 0.0000 0.9528 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 0.9236 \u00b1 0.0000 0.9236 \u00b1 0.0000 0.9229 \u00b1 0.0000", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 0.7490 \u00b1 0.0000 0.7520 \u00b1 0.0000 0.7520 \u00b1 0.0000", "text": "Reduction algorithms. Additionally, we introduced two modified fitness functions G\u03b7 (\u03b1, \u03b2) R and G\u00b5 (\u03b1, \u03b2) R. On this basis, we developed two heuristic attribute reduction algorithms to find distribution reductions based on the addition-deletion method and the deletion method. Finally, experiments will be carried out on several real data sets to test the effectiveness of the proposed fitness functions and distribution reductions. Experimental results show that G\u03b7 (\u03b1, \u03b2) R and G\u00b5 (\u03b1, \u03b2) R represent a more effective alternative to \u03b7 (\u03b1, \u03b2) R and \u00b5 (\u03b1, \u03b2) R to evaluate the importance of attributes. Experimental results also indicate that distribution reductions can achieve better performance in the probability model."}, {"heading": "Acknowledgements", "text": "This work is supported by the National Natural Science Foundation of China (grant no. 61502419, 61272060, 61379114) and the Key Natural Science Foundation of Chongqing (no. CSTC2013jjB40003)."}], "references": [{"title": "Information entropy and granulation coentropy of partitions and coverings: A summary", "author": ["D. Bianucci", "G. Cattaneo"], "venue": "Transactions on Rough Sets X, 5656:15\u201366", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Entropy and co-entropy of partitions and coverings with applications to roughness theory", "author": ["G. Cattaneo", "D. Ciucci", "D. Bianucci"], "venue": "Granular Computing: At the Junction of Rough Sets and Fuzzy Sets, 224:55\u201377", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "A rough set approach to feature selection based on ant colony optimization", "author": ["Y.M. Chen", "D.Q. Miao", "R.Z. Wang"], "venue": "Pattern Recognitiion Letters, 31:226\u2013233", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "An uncertainty measure for incomplete decision tables and its applications", "author": ["J.H. Dai", "W.T. Wang", "Q. Xu"], "venue": "IEEE Transactions on Cybernetics, 43(4):1277\u20131289", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Approximations and uncertainty measures in incomplete information systems", "author": ["J.H. Dai", "Q. Xu"], "venue": "Information Sciences, 198:62\u201380", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature selection in decision systems based on conditional knowledge granularity", "author": ["T.Q. Deng", "C.D. Yang", "Q.H. Hu"], "venue": "International Journal of Computational intelligence systems, 4(4):655\u2013671", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Decision-theoretic rough set: A multicost strategy", "author": ["H.L. Dou", "X.B. Yang", "X.N. Song", "H.L. Yu", "W.Z. Wu", "J.Y. Yang"], "venue": "Knowledge-Based Systems, 000:1\u201313", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "sigkdd explorations. 11(1):10\u201318", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "A rough set approach for selecting clustering attribute", "author": ["T. Herawan", "M.M. Deris", "J.H. Abawajy"], "venue": "Knowledge-Based Systems, 23(3):220\u2013231", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Criteria for choosing a rough set model", "author": ["J.P. Herbert", "J.T. Yao"], "venue": "Computers and Mathematics with applications, 57(6):908\u2013918", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Game-theoretic rough sets", "author": ["J.P. Herbert", "J.T. Yao"], "venue": "Fundamenta Informaticae, 108(3):267\u2013286", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Several approaches to attribute reduction in variable precision rough set model", "author": ["M. Inuiguch"], "venue": "Modeling Decisions for Artificial Intelligence, pages 215\u2013 226", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Minimum cost attribute reduction in decision-theoretic rough set models", "author": ["X.Y. Jia", "W.H. Liao", "Z.M. Tang", "L. Shang"], "venue": "Information Sciences, 219(4):151C167", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Generalized attribute reduct in rough set theory", "author": ["X.Y. Jia", "L. Shang", "B. Zhou", "Y.Y. Yao"], "venue": "Knowledge-Based Systems, 51(4):453\u2013471", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Tang", "author": ["X.Y. Jia"], "venue": "W. H. Z. M. Liao, and L. Shang. On an optimization representation of decision-theoretic rough set model. International Journal Of Approximate Reasoning, 55(1):156\u2013166", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A relative decision entropy-based feature selection approach", "author": ["F. Jiang", "Y.F. Sui", "L. Zhou"], "venue": "Pattern Recognition, 48:2151\u20132163", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparative study of alternative types of knowledge reduction in inconsistent systems", "author": ["M. Kryszkiewicz"], "venue": "International Journal of Intelligence Systems, 16(1):105\u2013120", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Certain", "author": ["M. Kryszkiewicz"], "venue": "generalized decision, and membership distribution reducts versus functional dependencies in incomplete systems. In Proceeding of RSEISP2007, LNAI 4585, pages 162\u2013174", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Constructing importance measure of attributes in covering decision table", "author": ["F.C. Li", "Z. Zhang", "C.X. Jin"], "venue": "Knowledge-Based Systems, 76:228\u2013 239", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Non-monotonic attribute reduction in decision-theoretic rough sets", "author": ["H.X. Li", "X.Z. Zhou", "J.B. Zhao", "D. Liu"], "venue": "Fundamenta Informaticae, 126(4):415\u2013432", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "The information entropy", "author": ["J.Y. Liang", "Z.Z. Shi"], "venue": "rough entropy and knowledge granulation in rough set theory. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systemss, 12(1):37\u201346", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "A new measure of uncertainty based on knowledge granulation for rough sets", "author": ["J.Y. Liang", "J.H. Wang", "Y.H. Qian"], "venue": "Information Sciences, 179(4):458\u2013470", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Unifying variable precision and classical rough sets: Granular approach", "author": ["T.Y. Lin", "Y.R. Syau"], "venue": "Rough Sets and Intelligent Systems-Professor Zdzis law Pawlak in Memoriam, pages 365\u2013373. Springer", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "A multiple-category classification approach with decision-theoretic rough sets", "author": ["D. Liu", "T.R. Li", "H.X. Li"], "venue": "Fundamenta Informaticae, 115(2):173\u2013 188", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic model criteria with decisiontheoretic rough sets", "author": ["D. Liu", "T.R. Li", "D. Ruan"], "venue": "Information Sciences, 181(17):173\u2013178", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "A set covering based approach to find the reduct of variable precision rough set", "author": ["J.N.K. Liu", "Y.X. Hu", "Y.L. He"], "venue": "Information Sciences, 275(17):83\u2013100", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic rough set over two universes and rough entropy", "author": ["W.M. Ma", "B.Z. Sun"], "venue": "International Journal of Approximate Reasoning, 53(4):608\u2013 619", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Decision region distribution preservation reduction in decision-theoretic rough set model", "author": ["X.A. Ma", "G.Y. Wang", "H. Yu"], "venue": "Information Sciences, 278:614\u2013640", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Approaches to knowledge reduction based on variable precision rough set model", "author": ["J.S. Mi", "W.Z. Wu", "W.X. Zhang"], "venue": "Information Sciences, 159(3):255\u2013272", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "A heuristic algorithm for reduction of knowledge", "author": ["D.Q. Miao", "G.R. Hu"], "venue": "Chinese Journal of Computer Research & Development, 36(6):681\u2013 684", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1999}, {"title": "Relative reducts in consistent and inconsistent decision tables of the Pawlak rough set model", "author": ["D.Q. Miao", "Y. Zhao", "H.X. Li", "F.F. Xu"], "venue": "Information Sciences, 179(24):4140\u20134150", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Test-cost-sensitive attribute reduction", "author": ["F. Min", "H.P. He", "Y.H. Qian", "W. Zhu"], "venue": "Information Sciences, 181(22):4928\u20134942", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Attribute reduction of data with error ranges and test costs", "author": ["F. Min", "W. Zhu"], "venue": "Information Sciences, 211:48\u201367", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploring the boundary region of tolerance rough sets for feature selection", "author": ["N.M. Parthalain", "Q. Shen"], "venue": "Pattern Recognition, 42(5):655\u2013667", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "A distance measure approach to exploring the rough set boundary region for attribute reduction", "author": ["N.M. Parthalain", "Q. Shen", "R. Jensen"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 22(3):305\u2013317", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Rough sets", "author": ["Z. Pawlak"], "venue": "International Journal of Computer and Information Science, 11(5):341\u2013356", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1982}, {"title": "Rough sets: theoretical aspects of reasoning about data", "author": ["Z. Pawlak"], "venue": "Kluwer Academic Publishers, Boston", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1991}, {"title": "Rough sets: probabilistic versus deterministic approach", "author": ["Z. Pawlak", "S.K.M. Wong", "W. Ziarko"], "venue": "International Journal of Man-Machine Studies, 29(1):81\u201395", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1988}, {"title": "Granular Computing: Analysis and Design of Intelligent Systems", "author": ["W. Pedrycz"], "venue": "CRC Press/Francis Taylor, Boca Raton", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Combination entropoy and combination granulation in rough set theory", "author": ["Y.H. Qian", "J.Y. Liang"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 16(2):179\u2013193", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Positive approximation: An accelerator for attribute reduction in rough set theory", "author": ["Y.H. Qian", "J.Y. Liang", "W. Pedrycz", "C.Y. Dang"], "venue": "Artificial Intelligence, 174(9):597\u2013618", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Attribute reduction in decision-theoretic rough set models using genetic algorithm", "author": ["S. Chebrolu", "G. Sanjeevi S"], "venue": "In Proceeding of 2th International Conference on the Swarm, Evolutionary, and Memetic Computing", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "Rough sets", "author": ["Q. Shen", "R. Jensen"], "venue": "their extensions and applications. International Journal of Automation and Computing, 4(3):217\u2013228", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "An incremental approach to attribute reduction from dynamic incomplete decision systems in rough set theory", "author": ["W.H. Shu andW.B. Qian"], "venue": "Data & Knowledge Engineering,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "The investigation of the bayesian rough set model", "author": ["D. Slezak", "W. Ziarko"], "venue": "International Journal Approximate Reasoning, 40(1):81\u201389", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient attribute reduction from the viewpoint of discernibility", "author": ["S.H. Teng", "M. Lu", "A.F. Yang", "J. Zhang", "Y.J. Nian", "M. He"], "venue": "Information Sciences, 326:297\u2013314", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Dimensionality reduction based on rough set theory: A review", "author": ["K. Thangavela", "A. Pethalakshmi"], "venue": "Applied Soft Computing, 9(1):1\u201312", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Granular variable precision fuzzy rough sets with general fuzzy relations", "author": ["C.Y. Wang", "B.Q. Hu"], "venue": "Fuzzy Sets and Systems, 275:39\u201357", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Research of reduct features in the variable precision rough set model", "author": ["J.Y. Wang", "J. Zhou"], "venue": "Neurocomputing, 72(10):2643\u20132648", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "Novel algorithms of attribute reduction with variable precision rough set model", "author": ["Y.Y. Yang", "D.G. Chen", "Z. Dong"], "venue": "Neurocomputing, 139:336\u2013344", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "A measurement theory view on the granularity of partitions", "author": ["Y.Y. Yao", "L.Z. Zhao"], "venue": "Information Sciences, 213:1\u201313", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "Attribute reduction in decision-theoretic rough set model", "author": ["Y.Y. Yao", "Y. Zhao"], "venue": "Information Sciences, 178(17):3356\u20133373", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2008}, {"title": "Discernibility matrix simplification for constructing attribute reducts", "author": ["Y.Y. Yao", "Y. Zhao"], "venue": "Information Sciences, 179(7):867\u2013882", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "On reduct construction algorithms", "author": ["Y.Y. Yao", "Y. Zhao", "J. Wang"], "venue": "Transactions on Computational Science II, volume 5150, pages 100\u2013117", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2008}, {"title": "A novel and better fitness evaluation for rough set based minimum attribute reduction problem", "author": ["D.Y. Ye", "Z.J. Chen", "S.L. Ma"], "venue": "Information Sciences, 222:413\u2013423", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2013}, {"title": "Approaches to knowledge reduuctions in inconsistent systems", "author": ["W.W. Zhang", "J.S. Mi", "W.Z. Wu"], "venue": "International Journal of Intelligent Systems, 18(9):989\u20131000", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2003}, {"title": "Region-based quantitative and hierarchical attribute reduction in the two-category decision theoretic rough set model", "author": ["X.Y. Zhang", "D.Q. Miao"], "venue": "Knowledge-Based Systems, 71:146\u2013161", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "A general definition of an attribute reduct", "author": ["Y. Zhao", "F. Luo", "S.K.M. Wong", "Y.Y. Yao"], "venue": "Proceeding of RSKT2007, volume 4481, pages 101\u2013108", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2007}, {"title": "A note on attribute reduction in the decision-theoretic rough set model", "author": ["Y. Zhao", "S.K.M. Wong", "Y.Y. Yao"], "venue": "Transactions on Rough Sets XIII, 6499:61\u201370", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2011}, {"title": "and Qi", "author": ["K. Zheng", "J. Hu", "Z.F. Zhan", "J. Ma"], "venue": "J. Jin. An enhancement for heuristic attribute reduction algorithm in rough set. 41(15):6748\u20136754", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2014}, {"title": "Variable precision rough set model", "author": ["W. Ziarko"], "venue": "Journal of Computer and System Sciences, 46(1):39\u201359", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1993}], "referenceMentions": [{"referenceID": 35, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 307, "endOffset": 330}, {"referenceID": 8, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 307, "endOffset": 330}, {"referenceID": 9, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 307, "endOffset": 330}, {"referenceID": 22, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 307, "endOffset": 330}, {"referenceID": 38, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 307, "endOffset": 330}, {"referenceID": 42, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 307, "endOffset": 330}, {"referenceID": 46, "context": "Attribute reduction is often helpful to reduce the computational cost, save storage space, improve learning performance and prevent over-fitting [48].", "startOffset": 145, "endOffset": 149}, {"referenceID": 13, "context": "For definition of attribute reduct, one mainly emphasizes on selecting what kinds of properties of a given information system to keep unchanged or to improve [15, 33, 34].", "startOffset": 158, "endOffset": 170}, {"referenceID": 31, "context": "For definition of attribute reduct, one mainly emphasizes on selecting what kinds of properties of a given information system to keep unchanged or to improve [15, 33, 34].", "startOffset": 158, "endOffset": 170}, {"referenceID": 32, "context": "For definition of attribute reduct, one mainly emphasizes on selecting what kinds of properties of a given information system to keep unchanged or to improve [15, 33, 34].", "startOffset": 158, "endOffset": 170}, {"referenceID": 36, "context": "For example, Pawlak [38] defined a relative reduct that keeps the quality of classification or classification positive region unchanged.", "startOffset": 20, "endOffset": 24}, {"referenceID": 29, "context": "[31] constructed the mutual information based reducts to extract relevant attribute sets that preserve the mutual information of a given decision table.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Kryszkiewicz [18, 19] investigated and compared the relationships among possible reduct, approximate reduct, \u03bc-reduct, \u03bc-decision reduct and generalized decision reduct.", "startOffset": 13, "endOffset": 21}, {"referenceID": 17, "context": "Kryszkiewicz [18, 19] investigated and compared the relationships among possible reduct, approximate reduct, \u03bc-reduct, \u03bc-decision reduct and generalized decision reduct.", "startOffset": 13, "endOffset": 21}, {"referenceID": 55, "context": "[58] introduced the maximum distribution reduct, which preserves all maximum decision rules in a decision table.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[7] proposed the notions of conditional knowledge granularity to reflect the relationship between conditional attributes and decision attribute, and defined an attribute reduct based on conditional knowledge granularity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[17] proposed a new model of relative decision entropy by combining roughness with the degree of dependency, and used it as the reduction criterion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "For attribute reduction algorithm, one mainly focuses on designing the effective reduct construction methods for finding a specific type of reduct [45, 47, 62].", "startOffset": 147, "endOffset": 159}, {"referenceID": 45, "context": "For attribute reduction algorithm, one mainly focuses on designing the effective reduct construction methods for finding a specific type of reduct [45, 47, 62].", "startOffset": 147, "endOffset": 159}, {"referenceID": 59, "context": "For attribute reduction algorithm, one mainly focuses on designing the effective reduct construction methods for finding a specific type of reduct [45, 47, 62].", "startOffset": 147, "endOffset": 159}, {"referenceID": 30, "context": "[32] discussed the structures of discernibility matrices for three different reducts, including region preservation reduct, decision preservation reduct and relationship preservation reduct.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "Yao and Zhao [55] introduced a reduct construction method based on discernibility matrix simplification.", "startOffset": 13, "endOffset": 17}, {"referenceID": 40, "context": "[42] proposed the concept of positive approximation for accelerating heuristic attribute reduction algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35, 36] proposed a distance measure-based attribute reduction algorithm by considering the proximity of objects in the boundary region to those in the lower approximation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 34, "context": "[35, 36] proposed a distance measure-based attribute reduction algorithm by considering the proximity of objects in the boundary region to those in the lower approximation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[4] proposed a novel attribute reduction algorithm based on ant colony optimization for finding", "startOffset": 0, "endOffset": 3}, {"referenceID": 54, "context": "[57] introduced a novel fitness function for designing the particle swarm optimization-based and genetic-based attribute reduction algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Relative to the classical rough set model, probabilistic rough set model allows certain acceptable levels of errors by using the threshold parameters [8, 26, 28, 49].", "startOffset": 150, "endOffset": 165}, {"referenceID": 24, "context": "Relative to the classical rough set model, probabilistic rough set model allows certain acceptable levels of errors by using the threshold parameters [8, 26, 28, 49].", "startOffset": 150, "endOffset": 165}, {"referenceID": 26, "context": "Relative to the classical rough set model, probabilistic rough set model allows certain acceptable levels of errors by using the threshold parameters [8, 26, 28, 49].", "startOffset": 150, "endOffset": 165}, {"referenceID": 47, "context": "Relative to the classical rough set model, probabilistic rough set model allows certain acceptable levels of errors by using the threshold parameters [8, 26, 28, 49].", "startOffset": 150, "endOffset": 165}, {"referenceID": 37, "context": "5 probabilistic rough set model [39], decision-theoretic rough set model [52], variable precision rough set model [63], Bayesian rough set model [46] and game rough set model [12].", "startOffset": 32, "endOffset": 36}, {"referenceID": 60, "context": "5 probabilistic rough set model [39], decision-theoretic rough set model [52], variable precision rough set model [63], Bayesian rough set model [46] and game rough set model [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 44, "context": "5 probabilistic rough set model [39], decision-theoretic rough set model [52], variable precision rough set model [63], Bayesian rough set model [46] and game rough set model [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 10, "context": "5 probabilistic rough set model [39], decision-theoretic rough set model [52], variable precision rough set model [63], Bayesian rough set model [46] and game rough set model [12].", "startOffset": 175, "endOffset": 179}, {"referenceID": 11, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 12, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 19, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 28, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 48, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 51, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 56, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 58, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 60, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 11, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 118, "endOffset": 134}, {"referenceID": 28, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 118, "endOffset": 134}, {"referenceID": 49, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 118, "endOffset": 134}, {"referenceID": 58, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 118, "endOffset": 134}, {"referenceID": 14, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 177, "endOffset": 189}, {"referenceID": 25, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 177, "endOffset": 189}, {"referenceID": 41, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 177, "endOffset": 189}, {"referenceID": 28, "context": "in [30].", "startOffset": 3, "endOffset": 7}, {"referenceID": 53, "context": "Generally speaking, the most common heuristic attribute reduction algorithms are the addition-deletion method and the deletion method [56].", "startOffset": 134, "endOffset": 138}, {"referenceID": 23, "context": "[25, 52].", "startOffset": 0, "endOffset": 8}, {"referenceID": 4, "context": "Given an information system IS = (U,A), P,Q \u2286 A, one can define a partial relation \u227a on 2 as follows [6]: P\u227aQ \u21d4 \u2200x \u2208 U, [x]P \u2286 [x]Q.", "startOffset": 101, "endOffset": 104}, {"referenceID": 20, "context": "[22] Given a decision table DT = (U,C\u222aD), R \u2286 C and U/D = {Y1, Y2, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Given a decision table DT = (U,C \u222a D), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1, R \u2286 C and U/D = {Y1, Y2, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "An attribute reduct is defined as a subset of attributes that are jointly sufficient and individually necessary for preserving or improving a particular property of the given information system [54].", "startOffset": 194, "endOffset": 198}, {"referenceID": 57, "context": "[60] Given an information system IS = (U,A), R \u2286 A and consider a certain property P, which can be represented by an evaluation function e : 2 \u2192 (L,\u227a), of IS.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "There are a huge amount of known properties, such as a description of an object relation [37], partitions of an information system [60], a classification of a set of concepts [38], where the classification of a set of concepts is the most common property in rough set theory.", "startOffset": 89, "endOffset": 93}, {"referenceID": 57, "context": "There are a huge amount of known properties, such as a description of an object relation [37], partitions of an information system [60], a classification of a set of concepts [38], where the classification of a set of concepts is the most common property in rough set theory.", "startOffset": 131, "endOffset": 135}, {"referenceID": 36, "context": "There are a huge amount of known properties, such as a description of an object relation [37], partitions of an information system [60], a classification of a set of concepts [38], where the classification of a set of concepts is the most common property in rough set theory.", "startOffset": 175, "endOffset": 179}, {"referenceID": 28, "context": "However, the decision rules derived from the reduct preserving probabilistic positive region maybe in conflict with those derived from the original decision table because of non-monotonicity of probabilistic positive region with respect to the set inclusion of attribute sets [30].", "startOffset": 276, "endOffset": 280}, {"referenceID": 28, "context": "[30] presented the concepts of distribution reducts based on variable precision rough set model which is a typical probabilistic rough set model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] Given a decision table DT = (U,C \u222a D), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1 and R \u2286 C, we have", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "If the fitness function is not monotonic regarding the set inclusion of attribute sets, individually necessary condition must consider all subsets of a reduct R to make sure it is a minimal set [14, 61].", "startOffset": 194, "endOffset": 202}, {"referenceID": 58, "context": "If the fitness function is not monotonic regarding the set inclusion of attribute sets, individually necessary condition must consider all subsets of a reduct R to make sure it is a minimal set [14, 61].", "startOffset": 194, "endOffset": 202}, {"referenceID": 53, "context": "[56] have summarized three groups of heuristic attribute reduction algorithms based on the addition-deletion method, the deletion method and the addition method, where the addition-deletion method based algorithm and the deletion method based algorithm are two most widely used heuristic attribute reduction algorithms by the rough set community.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "It is worth pointing out that the monotonicity of the fitness function in Definition 5 with respect to the set inclusion of attribute sets is very important for the completeness of Algorithm 1 and Algorithm 2 [29].", "startOffset": 209, "endOffset": 213}, {"referenceID": 15, "context": "Moreover, the fitness functions for evaluating the significance of attributes should also satisfy the monotonicity with respect to the set inclusion of attribute sets and provide enough precision to sort the attributes more effectively [17, 20].", "startOffset": 236, "endOffset": 244}, {"referenceID": 18, "context": "Moreover, the fitness functions for evaluating the significance of attributes should also satisfy the monotonicity with respect to the set inclusion of attribute sets and provide enough precision to sort the attributes more effectively [17, 20].", "startOffset": 236, "endOffset": 244}, {"referenceID": 50, "context": "Currently, Yao and Zhao [53] provided a unified framework for measures of granularity of partitions by considering the expected granularity of blocks in a partition.", "startOffset": 24, "endOffset": 28}, {"referenceID": 50, "context": "[53] Suppose U is finite and nonempty universe.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[53] Suppose \u03c0 = {X1, X2, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[2, 3, 22] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, .", "startOffset": 0, "endOffset": 10}, {"referenceID": 1, "context": "[2, 3, 22] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, .", "startOffset": 0, "endOffset": 10}, {"referenceID": 20, "context": "[2, 3, 22] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, .", "startOffset": 0, "endOffset": 10}, {"referenceID": 20, "context": "[22, 23] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, .", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[22, 23] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, .", "startOffset": 0, "endOffset": 8}, {"referenceID": 39, "context": "[41] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "All preprocessing methods were implemented by using WEKA filters [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 58, "context": "the (\u03b1, \u03b2) low distribution reduct with three traditional definitions of attribute reduct, including the qualitative positive region-preserved reduct (QLPRP) [61], the quantitative positive region-preserved reduct (QNPRP) [61] and the positive region extension reduct (EXDPR) [21], in probabilistic rough set model.", "startOffset": 158, "endOffset": 162}, {"referenceID": 58, "context": "the (\u03b1, \u03b2) low distribution reduct with three traditional definitions of attribute reduct, including the qualitative positive region-preserved reduct (QLPRP) [61], the quantitative positive region-preserved reduct (QNPRP) [61] and the positive region extension reduct (EXDPR) [21], in probabilistic rough set model.", "startOffset": 222, "endOffset": 226}, {"referenceID": 19, "context": "the (\u03b1, \u03b2) low distribution reduct with three traditional definitions of attribute reduct, including the qualitative positive region-preserved reduct (QLPRP) [61], the quantitative positive region-preserved reduct (QNPRP) [61] and the positive region extension reduct (EXDPR) [21], in probabilistic rough set model.", "startOffset": 276, "endOffset": 280}, {"referenceID": 7, "context": "All approaches were implemented based on the WEKA data mining software package [9], where the classifiers were implemented with default settings.", "startOffset": 79, "endOffset": 82}], "year": 2015, "abstractText": "Attribute reduction is one of the most important topics in rough set theory. Heuristic attribute reduction algorithms have been presented to solve the attribute reduction problem. It is generally known that fitness functions play a key role in developing heuristic attribute reduction algorithms. The monotonicity of fitness functions can guarantee the validity of heuristic attribute reduction algorithms. In probabilistic rough set model, distribution reducts can ensure the decision rules derived from the reducts are compatible with those derived from the original decision table. However, there are few studies on developing heuristic attribute reduction algorithms for finding distribution reducts. This is partly due to the fact that there are no monotonic fitness functions that are used to design heuristic attribute reduction algorithms in probabilistic rough set model. The main objective of this paper is to develop heuristic attribute reduction algorithms for finding distribution reducts in probabilistic rough set model. For one thing, two monotonic fitness functions are constructed, from which equivalence definitions of distribution reducts can be obtained. For another, two modified monotonic fitness functions are proposed to evaluate the significance of attributes more effectively. On this basis, two heuristic attribute reduction algorithms for finding distribution reducts are developed based on addition-deletion method and deletion method. In particular, the monotonicity of fitness functions guarantees the rationality of the proposed heuristic attribute reduction algorithms. Results of experimental analysis are included to quantify the effectiveness of the proposed fitness functions and distribution reducts.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}