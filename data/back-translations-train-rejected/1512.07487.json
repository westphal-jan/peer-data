{"id": "1512.07487", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Dec-2015", "title": "Selecting the top-quality item through crowd scoring", "abstract": "We investigate crowdsourcing algorithms for finding the top-quality item within a large collection of objects with unknown intrinsic quality values. This is an important problem with many relevant applications, for example in networked recommendation systems. The core of the algorithms is that objects are distributed to crowd workers, who return a noisy evaluation. All received evaluations are then combined, to identify the top-quality object. We first present a simple probabilistic model for the system under investigation. Then, we devise and study a class of efficient adaptive algorithms to assign in an effective way objects to workers. We compare the performance of several algorithms, which correspond to different choices of the design parameters/metrics. We finally compare our approach based on scoring object qualities against traditional proposals based on comparisons and tournaments.", "histories": [["v1", "Wed, 23 Dec 2015 14:23:15 GMT  (98kb)", "http://arxiv.org/abs/1512.07487v1", null], ["v2", "Mon, 2 Oct 2017 08:50:48 GMT  (102kb)", "http://arxiv.org/abs/1512.07487v2", "To be published, ACM TOMPECS 2017"]], "reviews": [], "SUBJECTS": "cs.HC cs.AI", "authors": ["alessandro nordio", "alberto tarable", "emilio leonardi", "marco ajmone marsan"], "accepted": false, "id": "1512.07487"}, "pdf": {"name": "1512.07487.pdf", "metadata": {"source": "CRF", "title": "Selecting the top-quality item through crowd scoring", "authors": ["Alessandro Nordio", "Alberto Tarable", "Emilio Leonardi", "Marco Ajmone Marsan"], "emails": [], "sections": [{"heading": null, "text": "In fact, the key elements of a crowdsourcing system are in hand: the availability of a large pool of individuals or machines (called crowdsourcing workers) who can contribute to solving the problem by performing a task; ii) an algorithm for dividing the problem into tasks; iiii) an algorithm for selecting the workforce and distributing the tasks to the selected workforce; iv) an algorithm for combining the answers to the problem; v) a requester (a.k.a. an algorithm that uses three algorithms to structure the problem."}, {"heading": "II. SYSTEM ASSUMPTIONS", "text": "We are looking at a number of N objects, each of which is endowed with an intrinsic quality, the evaluation of which requires human skills. Let's let x = [x1,.., xN] be the vector of all quality values that are instances of the i.i.d. random variable q = [q1,.., qN] with common pdf fq about R. Our goal is to use a crowdsourcing approach to identify the \"best\" object, that is, the object with the highest quality value designated by xi, wherei \u0445 = argmax i xi. Mass is composed of statistically identical workers whose evaluation of object quality is susceptible to error. We currently assume that workers provide absolute unquantified estimates (scores) of the intrinsic quality of individual objects, and we model, as additive Gaussian noise, the error that is made in such an evaluation process. Specifically, when the i-object is sent to evaluation, let's say, mass factors for time."}, {"heading": "III. ALGORITHMIC APPROACH", "text": "We examine a class of adaptive algorithms in which objects are sent through multiple rounds for evaluation. In each round, each object receives a certain number of ratings from crowd workers (possibly zero, for some objects). Then, based on all the answers collected, the algorithms will make decisions about the ability to request additional ratings for a subset of objects in a further round. If no additional ratings are performed, the algorithms end and a winner is identified. Formally, the number of ratings received by the i-th object in round and M () is identified as the total number of ratings received by the object i through (and including) round. We define one () ij = [a), j = 1., M () i as a vector1 of the random variables that represent the responses about the object i through round, and A (1),."}, {"heading": "IV. DESIGN PARAMETERS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Preliminary considerations", "text": "In the round of the algorithm, we can calculate an a-posteriori distribution f () i (x | y () i) for the quality Qi, where y () i represents a realization of the random vector a () i (provisionally assumed to be unquantified). Thanks to Bayes \"rule, such a distribution can be written as follows: f () i (x | y () i) = x M () i (j = 1exp (\u2212 (yij \u2212 x) 22\u03c32) fq (x) (1), where it is such a distribution."}, {"heading": "B. Possible performance parameters", "text": "In order to correctly select the metrics of the crowdsourcing algorithm, it is important to identify the performance parameters (which we may want to optimize) and, for the sake of simplicity of notation, ignore the round index below. Consider a response error Y = [y1,.., yN] and define the corresponding estimate i \u00b2 (Y) of i \u00b2, hereinafter simply referred to as i \u00b2. \u2022 A first possible performance parameter (to minimize) is the distortion D (Y) = E (qi) = E (qi) 2, which is too complex in terms of the current a-posterior distribution of q = Y. Unfortunately, the calculation of the distortion parameter Y is generally too complex, even for moderate values of Y. \u2022 A generalization of the previous measurement is the sequence-k distortion D (k) (Y) of x x x."}, {"heading": "C. Fitness indices", "text": "As in the case of performance parameters, different choices for fitness indices are possible. \u2022 Exact Maximum Probability: With this choice we identify the fitness index of objects with their estimated probability of being the most valuable object: \u03c6i = \u03c0i (Y) \u2022 Approximate Maximum Probability: In this case: \u03c6i = \u03b5 i (Y) \u2022 Exact Maximum Probability with Elimination: As indicated in the previous section, the participant group C, originally set to {1,..., N}, can be shortened along rounds. We have considered a strategy in which at each round those objects whose \u03c0i is less than a threshold are eliminated, E. \u2022 Approximate Maximum Probability with Elimination: Analogous.. We can consider a strategy in which all i objects fall below the respective threshold."}, {"heading": "D. Allocation function", "text": "As explained in previous sections, the assignment function A (\u00b7) determines the number of further evaluations required by each object in round. In addition, A (\u00b7) is a non-decreasing function of the fitness index (object 1). For the sake of simplicity, we are particularly interested in the case that A (\u00b7) delivers values in {0, 1} N. This means that the number of workers assigned to each object in a round is either 0 or 1. In such a case, in round, the B () high-quality objects receive an additional worker, while all other objects do not receive additional workers.In this work, two possible choices are considered, depending on whether the total valuation budget is either fixed or not. \u2022 Unlimited budget: If there is no maximum number of requested evaluations, A only depends on the fitness index, in the following way: m () i = {1, visible () i > visible, visible ()."}, {"heading": "E. Termination rules", "text": "Due to the choice of the fitness index and the assignment function A, the termination rule of the algorithm may be different. \u2022 Maximum budget achieved: If a maximum of Mmax ratings is allowed, reaching this maximum budget will result in a stop of rounds. \u2022 Singleton candidate set: For algorithms that eliminate objects if their fitness is lower than \u03c0th, E, the natural condition for termination is that the candidate set contains only a single object, i.e. | C () | = 1. \u2022 Accuracy: If only a single object exceeds the accuracy threshold \u03c0th, A, while all other objects do not, which means that there is already a strong candidate winner, the algorithm can end rounds.If applicable, the termination rule can be the combination of all three rules above, i.e. the algorithm can be terminated whenever one of the three rules comes true."}, {"heading": "V. SCORING VERSUS DIRECT COMPARISONS", "text": "In this section, we want to show that whenever workers are able to provide (approximate) quantitative estimates of object quality, algorithms that evaluate these estimates are generally more effective than algorithms that rely only on direct comparisons between subranges of objects. We start by looking at a toy case in which only two objects are given, with qualities x1 and x2 = x1 + 0, and we compare two algorithms that require the same amount of human effort. The first algorithm draws on results of direct comparisons between the objects of the crowd workers, while the second quantitative estimate of object qualities provided by the same (or other) crowd workers. Note that an algorithm uses the results of direct comparisons between objects and uses a fixed budget of W workers for each comparison, necessarily works as follows."}, {"heading": "VI. ANSWER QUANTIZATION", "text": "Until now, we have assumed that workers are not quantified (i.e., infinite precision), noisy evaluations of object qualities. This assumption is impractical in many scenarios, where instead the workers \"evaluations must belong to a finite alphabet, i.e., they are quantified. In this section, we discuss how quantization can be effectively implemented to approximate the performance of the proposed unquantified algorithm. Given that L, a specific quantization rule is set by a (L + 1) dimensional vector of thresholds Z = [z1, \u00b7 zl, zL + 1] with z1 = < z3 < zL < zL + 1 = square."}, {"heading": "VII. RESULTS", "text": "In this section, we compare the performance of several algorithms achieved by different decisions for the fitness index, the allocation function, the termination rule, and the quantifier."}, {"heading": "A. Unquantized answers, unbounded budget", "text": "In particular, we focus on algorithms with unquantified answers and an unlimited budget, defining: \u2022 the \"Greedy-Keep-Exact\" (GKE) algorithm, which takes into account the exact maximum probability as a fitness index, the unlimited budget as an allocation function and the precision termination rule; \u2022 the \"Greedy-Remove-Approximate\" (GKA) algorithm, which takes into account the approximate maximum probability as a fitness index, the unlimited budget as an allocation function and the precision termination rule; \u2022 the \"Greedy-Approximate\" (GRA) algorithm, which uses the approximate maximum probability as a fitness index, the unlimited budget as an allocation rule.To reduce the space of the parameters, we always have fixed parameters."}, {"heading": "B. Unquantized answers, bounded budget", "text": "We are now moving to scenarios in which the budget of the algorithms is based on different values of the normalized budget K = max. We limit our analysis to: \u2022 the \"bounded-greedy-keep-approximate\" (bGKA), taking into account the approximate maximum probability as a fitness index, the limited budget as an allocation function, the maximum budget, or the accuracy of the termination rule; \u2022 the \"bounded-greedy-remove-approximate\" (bGA), which achieves the approximate maximum probability with eliminations as an allocation function, the maximum budget, or singleton contestable list as a termination rule.As a reference, we also report on the performance of the bundled version of the GA algorithm (bGA), which in turn provides an obvious upper limit to performance."}, {"heading": "C. Quantization effects", "text": "Finally, we consider the effect of quantization on system performance =. In Fig. 9, we test the effects of different quantizers = 32 accuracy with respect to pe versus M / N, in the case where N = 256 objects are evenly distributed in the interval [\u2212 1, 1]. In the figure, the GKA algorithm is applied with an unlimited budget for all curves. Furthermore, the standard deviation of the value estimation error of the worker is quantified and used as a benchmark. We remember that for evenly distributed objects \u2206 = 2 / (N \u2212 1) is the smallest distance between the quality values. The continuous line without markers represents the performance of the GKA algorithm without quantization and is used as a benchmark. We observe that despite the high number of levels, the quantization significantly worsens the uniformity."}, {"heading": "VIII. DESIGN CONSIDERATIONS", "text": "In this section, we deal with two important questions: We evaluate the computational complexity of our algorithms, which turns out to be the most powerful algorithm. As for the computational complexity, for the sake of brevity, we limit ourselves to bGKA errors, which turn out to be the most powerful algorithm. (This, in turn, requires O (1) operations per object, since the approximate maximum probability can be calculated by using (4). (iii) i (this requires O (1) operations per object). (ii) must calculate a fitness index (). (This, in turn, requires O (1) operations per object, since the approximate maximum probability can be calculated. (4)). (iii) i) must compare the object () i with the fourth in order to decide whether to assign an additional worker i (again O (1) operations. (again O (1) operations are required."}, {"heading": "IX. CONCLUDING REMARKS", "text": "In contrast to previous work, our study assumes that unquantified values are returned by the assessors and highlights the potential benefits of such an approach. Subsequently, we have shown how quantified schemes whose performance is very close to their ideal unquantified counterparts can be properly designed, provided that workers \"responses are assigned an appropriate number of quantization levels. We plan to generalize the approach proposed in this paper to the case of workers with different skills and to the problem of finding the k-quality elements within a large collection of objects using crowdsourcing algorithms."}], "references": [{"title": "A Survey of Crowdsourcing Systems", "author": ["M.-C. Yuen", "I. King", "K.-S. Leung"], "venue": "IEEE PASSAT-SOCIALCOM, Boston (USA), Oct. 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Max algorithms in crowdsourcing environments", "author": ["P. Venetis", "H. Garcia-Molina", "K. Huang", "N. Polyzotis"], "venue": "Intern. Conf. on World Wide Web (WWW \u201912), New York (USA), 989\u2013998, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamic Max Algorithms in Crowdsourcing Environments", "author": ["P. Venetis", "H. Garcia-Molina"], "venue": "Tech. Rep., Stanford InfoLab.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 0}, {"title": "So who won?: dynamic max discovery with the crowd", "author": ["S. Guo", "A. Parameswaran", "H. Garcia-Molina"], "venue": "2012 ACM Intern. Conf. on Management of Data, New York (USA), 385\u2013396, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "tDP: An Optimal- Latency Budget Allocation Strategy for Crowdsourced MAXIMUM Operations", "author": ["V. Verroios", "P. Lofgren", "H. Garcia-Molina"], "venue": "2015 ACM Intern. Conf. on Management of Data, New York (USA), 1047\u20131062, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Using the crowd for top-k and group-by queries", "author": ["S.B. Davidson", "S. Khanna", "T. Milo", "S. Roy"], "venue": "ICDT \u201913, New York (USA), 225\u2013236, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Computing with noisy information", "author": ["U. Feige", "P. Raghavan", "D. Peleg", "E. Upfal"], "venue": "SIAM J. Comput., 23(5): 1001\u20131018, 1994.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1994}, {"title": "Hybrid Strategies for Finding the Max with the Crowd", "author": ["A.R. Khan", "H. Garcia-Molina"], "venue": "Tech. Rep., Stanford InfoLab.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 0}, {"title": "A law of comparative judgment", "author": ["L.L. Thurstone"], "venue": "Psychological Review, vol. 34, pp. 273\u2013286, 1927.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1927}, {"title": "Least Squares Quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Transactions on Information Theory, vol. 28, pp. 129-137, No. 2, March 1982. 10", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1982}], "referenceMentions": [{"referenceID": 0, "context": "Crowdsourcing is a term often adopted to identify distributed systems that can be used for the solution of a wide range of complex problems by integrating a large number of human and/or computer efforts [1].", "startOffset": 203, "endOffset": 206}, {"referenceID": 1, "context": ", cost, accuracy, latency) [2]\u2013[7].", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": ", cost, accuracy, latency) [2]\u2013[7].", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "A similar path was followed in the recent (still unpublished) work by Khan and Garcia Molina [8], which studies algorithms to find the maximum element in a group of objects, and discusses approaches based on comparisons, on ratings, as well as on a mix of the two possibilities.", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "Indeed, [8] assumes that workers\u2019 answers are coarsely quantized over few levels (typically three or five), and this makes objects with similar quality indistinguishable, so that direct comparisons and tournaments become necessary to break ties.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "Another significant difference with respect to [8] is in the scope of the works.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "The paper [8], instead, focuses on non-adaptive algorithms distributing resources to objects according to a fixed, pre-established scheme.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "Observe that our model differs from [2]\u2013[7] because we assume that workers can provide absolute estimates (scores) of the intrinsic quality of objects, while [2]\u2013[7] assume workers to be only able to perform noisy comparisons between groups of objects.", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "Observe that our model differs from [2]\u2013[7] because we assume that workers can provide absolute estimates (scores) of the intrinsic quality of objects, while [2]\u2013[7] assume workers to be only able to perform noisy comparisons between groups of objects.", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "Observe that our model differs from [2]\u2013[7] because we assume that workers can provide absolute estimates (scores) of the intrinsic quality of objects, while [2]\u2013[7] assume workers to be only able to perform noisy comparisons between groups of objects.", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "Observe that our model differs from [2]\u2013[7] because we assume that workers can provide absolute estimates (scores) of the intrinsic quality of objects, while [2]\u2013[7] assume workers to be only able to perform noisy comparisons between groups of objects.", "startOffset": 162, "endOffset": 165}, {"referenceID": 8, "context": "We also wish to remark that the Gaussian model of worker error is in good agreement with Thurstone\u2019s law of comparative judgment [9], according to which comparisons are based on latent quality estimations, whose distribution is Gaussian around the true quality value.", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "In the context of crowdsourcing, the same model has recently been employed also in [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 9, "context": "The mean square error E[(a(q)\u2212a)2] represents a natural candidate for such distortion index, also because the seminal work by Lloyd [10] provides an efficient iterative algorithm for the design of a quantizer that minimizes the mean square error.", "startOffset": 132, "endOffset": 136}, {"referenceID": 0, "context": "aging with respect to f (II) a = fq[1] \u2217 fn, fq[1] being the a-priori distribution of the largest quality value.", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "aging with respect to f (II) a = fq[1] \u2217 fn, fq[1] being the a-priori distribution of the largest quality value.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "Therefore, our model perfectly matches the assumptions of [8].", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": "We remark that our results seem somehow in contrast with findings in [8], where it has been shown that tournament algorithms provide the best performance, for cases in which users are only able to compare objects pairs.", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "As an example, the solid line with filled square markers refers to the case when L = 32, and the quantizer is designed according to the criteria in [10], over", "startOffset": 148, "endOffset": 152}], "year": 2015, "abstractText": "We investigate crowdsourcing algorithms for finding the top-quality item within a large collection of objects with unknown intrinsic quality values. This is an important problem with many relevant applications, for example in networked recommendation systems. The core of the algorithms is that objects are distributed to crowd workers, who return a noisy evaluation. All received evaluations are then combined, to identify the top-quality object. We first present a simple probabilistic model for the system under investigation. Then, we devise and study a class of efficient adaptive algorithms to assign in an effective way objects to workers. We compare the performance of several algorithms, which correspond to different choices of the design parameters/metrics. We finally compare our approach based on scoring object qualities against traditional proposals based on comparisons and tournaments.", "creator": "gnuplot 4.6 patchlevel 4"}}}