{"id": "1512.07716", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2015", "title": "Fast Parallel SVM using Data Augmentation", "abstract": "As one of the most popular classifiers, linear SVMs still have challenges in dealing with very large-scale problems, even though linear or sub-linear algorithms have been developed recently on single machines. Parallel computing methods have been developed for learning large-scale SVMs. However, existing methods rely on solving local sub-optimization problems. In this paper, we develop a novel parallel algorithm for learning large-scale linear SVM. Our approach is based on a data augmentation equivalent formulation, which casts the problem of learning SVM as a Bayesian inference problem, for which we can develop very efficient parallel sampling methods. We provide empirical results for this parallel sampling SVM, and provide extensions for SVR, non-linear kernels, and provide a parallel implementation of the Crammer and Singer model. This approach is very promising in its own right, and further is a very useful technique to parallelize a broader family of general maximum-margin models.", "histories": [["v1", "Thu, 24 Dec 2015 04:56:28 GMT  (288kb,D)", "http://arxiv.org/abs/1512.07716v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hugh perkins", "minjie xu", "jun zhu", "bo zhang"], "accepted": false, "id": "1512.07716"}, "pdf": {"name": "1512.07716.pdf", "metadata": {"source": "CRF", "title": "Fast Parallel SVM using Data Augmentation", "authors": ["Hugh Perkins", "Minjie Xu", "Jun Zhu", "Bo Zhang"], "emails": ["hughperkins@gmail.com,", "chokkyvista06@gmail.com,", "dcszj@mail.tsinghua.edu.cn,", "dcszb@mail.tsinghua.edu.cn"], "sections": [{"heading": "1. INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2. SVM AS BAYESIAN INFERENCE", "text": "In this section, we present the basic theories on which our extensions and distributed algorithms are based."}, {"heading": "2.1 SVM: the Basics", "text": "Let D = {(xd, yd)} Dd = 1 be the training data, where yd = {1, \u2212 1}. The aim of the SVMs is to introduce a linear discriminant function (x; w, \u03bd) = w > x + \u03bd.To simplify the spelling, we absorb the offset parameter \u03bd in w by introducing an additional characteristic dimension with a fixed value. To find the optimal w, the canonical learning problem of SVMs with tolerance for training errors is formulated as a compulsive optimization problem w, Ed 1 2 \u03bb w \u00b2 22 + 2 \u0445 d \u0445ds.t.: Ed, {ydw > xd \u2265 1 \u2212 did \u2265 0, Note that the constant factor 2 in the training error term can be included as a compulsive optimization problem w, but we leave it for the simplicity of the deduction later."}, {"heading": "2.2 SVM: the MAP estimate", "text": "Problem (1) can also be considered as a MAP estimate of a probability model in which the rear distribution (w | D) = q0 (w) q (y \u2212 w, X), where q0 (w) = N (0, \u03bb \u2212 1I) and q (y \u2212 w, X) = q (yd | w, xd) withq (yd | w, xd) = exp (\u2212 2 max (0, 1 \u2212 ydw > xd))). (2) Note that we certify the rear side only because of the simplicity of the subsequent denotation in q0 and q, and they usually differ from the real previous and probability induced from the probability model (even up to a constant factor)."}, {"heading": "2.3 MCMC Sampling for SVM", "text": "Based on this extended representation, we are able to design MCMC methods for p (w, \u03b3 | D), of which the optimal SVM solution that maximizes p (w | D) is relatively more likely to obtain a sample. Specifically, we use Gibbs samples and have the following conditional distributions [13] p (w | \u03b3, D) = N (\u00b5, \u03a3) (4) p (\u03b3 \u2212 1d | w, yd, xd) = IG (| 1 \u2212 ydw > xd | \u2212 1, 1), (5), where \u03a3 = (\u03bbI + \u2211 d 1 \u03b3d xdx > d) \u2212 1, \u00b5 = \u0445d yd (1 + 1 \u03b3d) xd) (6) and IG is the inverse Gaussian distribution."}, {"heading": "2.4 EM algorithm for SVM", "text": "The EM algorithm is useful when it comes to directly maximizing the trailing p (w | D), but it is easy to switch between the following two steps, which converge to a local maximum of the trailing p (w). E-step: Q (m) (w) = \u0442 log p (w, \u03b3 | D) p (\u03b3 | D, w (m))) d\u03b3 (7) M-step: w (m + 1) = argmax w Q (m) (w) (8) It can be proven that the above algorithm monotonously increases the true rear interest distribution p (w | D) after each iteration, just like traditional EM does. To save space, we summarize the results as the following E-step (update): \u03b3 (m) d = | 1 \u2212 ydw (m).m-step: w (m + 1).m + 1 (update).m (update).m (update).m (.m) (update).m (update).m (update) is considered as the optimal step, but not the better one in the rule (p)."}, {"heading": "3. EXTENSIONS", "text": "In this section we extend the above idea to SVR, non-linear kernel SVMs and the Crammer and Singer multi-class SVM."}, {"heading": "3.1 Learning Nonlinear Kernel SVMs", "text": "According to the representer theorem, the solution to the problem (1) has the formw = > q = > q = > q = (11), which is a linear combination of X. We can, of course, extend it to the nonlinear case by using a function that can represent the nonlinear SVM by solving min w1 2 \u00b2 w \u00b2 22 + 2 \u00b2 d max (0, 1 \u2212 ydw > h (xd)), (12), the solution of which correspondingly is w = \u2211 d \u03b1dydh (xd) = Hdiag (y) \u03b1, (13) where H = [h (x1) h (x2) h (xD) \u00b7 \u00b7 \u2212 h (xD). If we replace Eq. (13) in (12), we obtain the double problem definition in 1 2 \u00b2 \u00b2 diag (y) Kdiag (y) veag (y) veag (y) \u00b2 d max (0, 1 \u2212 ydp (xD)."}, {"heading": "3.2 Support Vector Regression", "text": "For the regression in which the response variable y \u2212 \u2212 is real-evaluated, the support vector regression problem (SVR) is defined as minimizing a regulated -insensitive loss [16] min w1 2 0 0 0 0 0 0 0 0 0, | yd \u2212 w > xd | \u2212), (20), and the extension is done by the following lemmaLemma 3. Double-scale mixing for -insensitive lossexp (\u2212 2 max (0, | yd \u2212 w > xd | \u2212))) = 0 1 0 1 0 1 0 0 0, 2 1 0 0 0, 2 xd \u2212 d = 1, (\u2212 2 max (0, | yd \u2212 w > xd | \u2212) = 1 0 0 0, 2 xd \u2212 d (> yd) = 1 0, 2 xd \u2212 d = 1 0, (> yd) = 0, 2 x \u2212 d \u2212 d = 1, d = 1 \u2212 d \u2212 d, d = 1 \u2212 d \u2212 d \u2212 d, d = 1 \u2212 d \u2212 d \u2212 d \u2212 d, d = 1 \u2212 d \u2212 d \u2212 d, d = 1 \u2212 d \u2212 d \u2212 d, d \u2212 d = 1 \u2212 d \u2212 d \u2212 d \u2212 d, d \u2212 d = 1 \u2212 d \u2212 d \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 d, d \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 d \u2212 d \u2212 1 \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 d \u2212 d \u2212 1 \u2212 d \u2212 d \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 1 \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 d \u2212 d \u2212 d = 1 \u2212 d \u2212 d \u2212 d = \u2212 d \u2212 d = 1."}, {"heading": "3.3 Learning Multi-class SVM", "text": "There are several strategies for performing a multi-class classification with SVM (max.). Here, we consider the approach proposed by Crammer and Singer (2001), where the generalized differentiation function is bef (y, x; w) = w > y (w) = w > y (w), where wy is the subvector corresponding to the class name y. And the generalized risk mitigation problem becomes bef (y, x; w) = max y (y) \u2212 fd (y; w))), (30), where d (y) is the cost of predicting y for the true name yd and f (y) fd (y) = f (yd, xd) \u2212 f (y; w) \u2212 f (y; w) is the margin, and both d (y), and both d (y) d (y; w) is the cost of predicting y for the true name yd and f (d)."}, {"heading": "4. PARALLEL SVM", "text": "For an easier explanation, we focus on the classic linear binary SVMs. And exactly the same techniques apply to all extensions we present in Section 3, as well as to their EM algorithms. Two key characteristics of the scanning process that facilitate a parallel calculation are summarized as follows: 1. The scale variables are mutually independent of each other, so the scanning step can be easily paralleled with several cores and several machines. 2. The training data (xd, yd) contribute to the global variables \u00b5 and \u03a3 by a simple sum operator (equation (6). Thus, a typical architecture of map reduction is directly applicable, as shown in Figure 1."}, {"heading": "4.1 The Basic Procedure", "text": "Let P be the total number of processes and let Dp = {(xpd, y p d)} Dp d = 1 be the data associated with the process p. Then, each process performs the following calculations. Draw scale parameters: Each p draws according to the distribution in eq. (5).2. Calculate local statistics: Each p calculates the following local statistics: \u00b5p = Dp \u2211 d = 1 (1 + 1 \u03b3dp) ypdx p d, \u043fp = Dp \u2211 d = 1 \u03b3dp xpdx p > d. (40) Since \u041ap is symmetrical, it is sufficient to calculate only the upper or lower triangle and then submit to the master. After the process p completes its local calculation, it passes the local statistics \u00b5p and \u0441p to the master process, which collects the results and permits the following aggregation operations. Then calculate \u2212 1 = QP +."}, {"heading": "4.2 Notation", "text": "We refer to parallel sampling SVM as PEMSVM. PEMSVM has the following options: \u2022 linear (\"LIN\") vs. kernelized (\"KRN\") \u2022 EM (\"EM\") vs. MCMC (\"MC\") \u2022 binary classification (\"CLS\") versus multiclass classification (\"MLT\") versus support regression (\"SVR\") These three options are orthogonal, so we can write a number of options such as \"LIN-EM-CLS.\" N is the number of training instances, K the number of attributes, M the number of classes and P the number of processes."}, {"heading": "4.3 Iteration time", "text": "We found that all formulations in P are highly scalable; the LIN formulation is highly scalable in N, but finds data sets with high K challenging; in contrast, the KRN formulation in K matrices is highly scalable; indeed, the iteration time is independent of K, but the iteration time is cubic with N, which is a significant challenge. To make the KRN formulation truly effective, it might be useful to either reduce the number of features or use an approximation."}, {"heading": "5. EXPERIMENTS", "text": "We compare parallel SVM implementations with state-of-the-art linear solutions."}, {"heading": "5.1 Summary of results", "text": "We show that LIN- * -CLS is faster than state-of-the-art linear solvers when N is large relative to K2, and that parallel hardware is available. We show that LIN- * -MLT is highly scalable when N is large relative to K2, and that parallel hardware is available. We show that the algorithms for KRN- * -CLS and LIN- * SVR can provide accuracy comparable to existing solutions. We evaluate the performance of a GPU solver for LIN-EMCLS. We show that the learning speed of our formulations can be accelerated by using a GPU."}, {"heading": "5.2 Test conditions", "text": "The tests were conducted on a cluster of 12-core nodes, the cores were 2.6 GHz and each node had 24 GB of memory."}, {"heading": "5.3 Datasets", "text": "Table 3 shows the data sets used. We have created subsets for some experiments. A K = K0 subset means that we only include features where k < = K0. Likewise, an N = N0 subset means that only the first N0 data points from the original training data set were included."}, {"heading": "5.4 Solvers", "text": "Table 4 shows the solvents used in addition to PEMSVM."}, {"heading": "5.5 Termination conditions", "text": "PEMSVM calculates the value of the objective function for each iteration. The algorithm ends when the iterative change falls to 0.001 \u0445 N or below, which we considered a reasonable stop condition for many datasets."}, {"heading": "5.6 I/O", "text": "By splitting the problem into independent sub-problems, not only can the calculations be parallelized across multiple cores, but the I / O load of loading the data file into memory can similarly be parallelized across cores and computing nodes. This alone can lead to speed gains compared to single-thread algorithms. Moreover, even for large data sets like DNA, it is possible to keep the data set fully in memory across multiple computing nodes."}, {"heading": "5.7 Implementation Details", "text": "5.7.1 MPI implementation MPI was used with C / C + + to parallelise the implementation across multiple CPU cores, the cores may be located on a single node or multiple nodes. Each MPI process was assigned a partition of the data set, which was read from the data file itself and coordinated with a master process.The MPI implementation was assigned with a sparse representation for xd.5.7.2 GPU implementation OpenCL was used with C / C + + to parallelise the calculation of \u2211 d 1 \u03b3d xdx T d across multiple GPU cores. Data was partitioned and each partition was loaded into the local memory of a computer unit. Results written into global memory were then reduced with a second GPU kernel. For multiple GPUs, the data set was first partitioned, then each partition was managed by a single GPU."}, {"heading": "5.8 MPI solver for linear classification", "text": "In Table 5, our LIN-EM-CLS implementation is compared with other solvers for the DNA dataset. For a 2.5 million line subset, our solver was the fastest when 48 cores were available. Other solvers tested were not available to exploit the additional cores. Pegasos exceeded the available memory (24GB + 30GB swap) and was killed. SDB crashed for unknown reasons. At the full 5 million line subset, our solver is one of the two that made it. The other solvers exhausted the available memory and were killed. StreamSVM uses available memory well by using a blocking method, but as a result, since it uses only two threads and runs on a single network node, it is very slower. In total, our solver was the fastest with 2.5 million lines to achieve the same accuracy as other solvers when 480 cores were available. It was the only one solver to complete a thousand times faster on the three million lines within the 24-hour period."}, {"heading": "5.9 Scalability of LIN-CLS", "text": "Figure 2 shows the scalability with the number of cores of LIN-EM-CLS using the DNA dataset. The speed is linear with the number of cores, up to 480 cores, on this dataset. Figure 3 shows the scalability with N. In this diagram, all solvers were operated with a single thread, including LIN-CLS and PSVM. We can see that LIN-CLS is linear in N and scales much better with N than PSVM. PSVM is a dual solver and scales well with K, but less well with N. Liblinear and Pegasos also scale linearly with N. Note that LINEM-CLS is slower than Liblinear and Pegasos in a single thread scenario, but taking advantage of additional cores, LIN-EM-CLS can be faster than both Liblinear and Pegasos. Figure 4 shows the effect of K on the training time, with each individual solver measured with a high SVK read."}, {"heading": "5.10 SVR", "text": "Table 6 shows the performance of LIN-EM-SVR compared to liblinear for the annual regression dataset. Data was normalized for mean and variance before testing. Epsilon was set to 0.3.LIN-EM-SVR to achieve a similar accuracy."}, {"heading": "5.11 KRN", "text": "Table 7 shows the results for KRN-EM-CLS. Our accuracy is similar to this training set liblinear. The core formulation allows the use of non-linear nuclei, and the training time is independent of the K.A limitation of the KRN formulation that the training time in N is cubic."}, {"heading": "5.12 Performance on Crammer and Singer models", "text": "Our implementation of Crammer and Singer is parallelizable across multiple cores. On a cluster today, training times are comparable to liblinear and much faster than SVM Multiclass. In the future, the number of available cores is likely to increase, possibly exponentially, and our implementation could become increasingly advantageous compared to the liblinear and SVM multiclases with only one thread. For the full mnist8m dataset, only our solver and liblinear were able to complete the training. SVMMulticlass consumed all available memory (24GB + 30GB swap) and was killed. Increasing the number of cores from 48 to 480 for our implementation yielded 7.6 times the speed, demonstrating the scalability of this algorithm."}, {"heading": "5.13 Convergence", "text": "Figure 5 shows the convergence of the objective function, for both MC and EM, for the DNA dataset, for LIN- * -CLS. Here, the EM objective function converges within 40-60 iterations, and this is what we have seen in practice in other datasets. For MC, we have two options: \u2022 use the best single sample \u2022 average multiple samples Given that this is a high-dimensional space, it is unlikely that taking individual samples comes close to the optimal solution, so we measure the average across multiple samples. Normally, one would want to select a small focal period of 10-20 iterations. This is in contrast to EM, where we use a single sample for each iteration to measure test accuracy. Thus, the objective function for MC converges more slowly in these diagrams than for EM.In these diagrams, we have not used a focal period for MC-20 iterations."}, {"heading": "5.14 Parallelization using GPU", "text": "For LIN, the execution time is asymptotic O (NK2).Table 9 shows the results for evaluating \u03a3 for simulated xd- and \u03b3d vectors. The use of 512 GPU cores was 23 times faster than a single core. The use of 2048 GPU cores was about 50 times faster. The CPU core came from an Intel i7-3930K 3.2GHz CPU, and the GPU cores came from nVidia GTX590 GPUs (one GPU contains 512 cores).Table 10 shows the performance of LIN-EM-CLS using a GPU implementation on the Alpha CPU data.We can see that the use of a single CPU core, liblinear is almost three times faster than the implementation of LIN-EM-CLS. This GPU implementation using a GPU implementation on the Alpha CPU core is an advantage that the CPU data storage speed is limited for the PLS."}, {"heading": "6. CONCLUSIONS", "text": "Our implementation of a parallel linear SVM solver is capable of processing very large data sets, and scales in our experiments to at least several hundred cores. We have provided an extension to nonlinear cores, an implementation to support vector regression, and a parallel solver for the Crammer and Singer models. It is useful in itself and represents a useful addition to our arsenal as it enables fast and precise solutions for compound maximum margin models."}, {"heading": "7. REFERENCES", "text": "[1] C.-C. Chang and C.-J. Lin. LIBSVM: A library forsupport vector machines. ACM Transactions on Intelligent Systems and Technology, 2: 27: 1-27: 27, 2011. [2] E. Y. Chang, K. Zhu, H. Wang, and H. Bai. PSVM: Parallelizing support vector machines on distributed computers. In NIPS, 2007. [3] K.-W. Chang and D. Roth. Selective block minimization for faster convergence of limited memory large-scale linear models. In Proceedings of the 17thACM SIGKDD international conference on Knowledge discovery and data mining, pp. 699-707. ACM, 2011. [4] R. Collobert, S. Bengio, and Y. Bengio. A parallel mixture of svms for very large-scale problems. Neural Computation, 14 (5): 1105-1114. [5] R.-E. Fan, K.-W. Chang, Chang J. R. J. Chang, and J. R. J. sie.7h."}], "references": [{"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "PSVM: Parallelizing support vector machines on distributed computers", "author": ["E.Y. Chang", "K. Zhu", "H. Wang", "H. Bai"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Selective block minimization for faster convergence of limited memory large-scale linear models", "author": ["K.-W. Chang", "D. Roth"], "venue": "In Proceedings of the 17th  ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "A parallel mixture of svms for very large scale problems", "author": ["R. Collobert", "S. Bengio", "Y. Bengio"], "venue": "Neural Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Parallel support vector machines: The cascade svm", "author": ["H.P. Graf", "E. Cosatto", "L. Bottou", "I. Durdanovic", "V. Vapnik"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "MIT press,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Cutting-plane training of structural SVMs", "author": ["T. Joachims", "T. Finley", "C. Yu"], "venue": "Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Linear support vector machines via dual cached loops", "author": ["S. Matsushima", "S. Vishwanathan", "A.J. Smola"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Distributed algorithms for topic models", "author": ["D. Newman", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "JMLR, 10:1801\u20131828,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "author": ["J. Platt"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Data augmentation for support vector machines", "author": ["N.G. Polson", "S.L. Scott"], "venue": "Bayesian Analysis,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "An architecture for parallel topic models", "author": ["A. Smola", "S. Narayanamurthy"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "A tutorial on support vector regression", "author": ["A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "Statistics and Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Nonparametric max-margin matrix factorization for. collaborative prediction", "author": ["M. Xu", "J. Zhu", "B. Zhang"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Gibbs max-margin topic models with fast sampling algorithms", "author": ["J. Zhu", "N. Chen", "H. Perkins", "B. Zhang"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "Traditional decomposition methods, like SVMLight [7], LibSVM [1] and SMO [12], have cubic time complexity.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "Traditional decomposition methods, like SVMLight [7], LibSVM [1] and SMO [12], have cubic time complexity.", "startOffset": 61, "endOffset": 64}, {"referenceID": 11, "context": "Traditional decomposition methods, like SVMLight [7], LibSVM [1] and SMO [12], have cubic time complexity.", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "Representative works include the parallel SVM (PSVM) [2], which performs approximate matrix factorization to reduce memory use and then uses the interior point method to solve the quadratic optimization problem on multiple machines in parallel.", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "The parallel mixture method [4] and the cascade SVM [6] decompose the entire learning problem into multiple smaller QP problems and solve them in parallel.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "The parallel mixture method [4] and the cascade SVM [6] decompose the entire learning problem into multiple smaller QP problems and solve them in parallel.", "startOffset": 52, "endOffset": 55}, {"referenceID": 12, "context": "Our algorithm is built on the recent work [13], which shows that the learning problems of SVM can be equivalently formulated as hierarchical Bayesian model, with additional scale variables.", "startOffset": 42, "endOffset": 46}, {"referenceID": 14, "context": "Our work is also inspired by the recent developments on distributed Monte Carlo methods for improving the scalability of probabilistic latent topic models [15, 11].", "startOffset": 155, "endOffset": 163}, {"referenceID": 10, "context": "Our work is also inspired by the recent developments on distributed Monte Carlo methods for improving the scalability of probabilistic latent topic models [15, 11].", "startOffset": 155, "endOffset": 163}, {"referenceID": 17, "context": "In addition, it is a useful addition to our armory, because it can be used to solve composite models, such as MedLDA [18], without needing to make the mean-field assumption.", "startOffset": 117, "endOffset": 121}, {"referenceID": 16, "context": "There are many models, such as [17] for example, that may be able to benefit from fast and accurate parallelization using the parallel sampling or parallel EM SVM formulation.", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "Specifically, Polson and Scott [13] show that the pseudo-likelihood can be represented as a scale mixture of Gaussians, namely", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "Specifically, we use Gibbs sampling and have the following conditional distributions [13]", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "2 Support Vector Regression For regression, where the response variable y are realvalued, the support vector regression (SVR) problem is defined as minimizing a regularized -insensitive loss [16]", "startOffset": 191, "endOffset": 195}, {"referenceID": 4, "context": "Name Description LL-Dual[5] Liblinear dual coordinate descent L2-regularization L2-loss biased LL-Primal[5] Liblinear primal coordinate descent L2-regularization L2-loss biased LL-CS[5] Liblinear Crammer and Singer PSVM[2] PSVM, with rank ratio set to 1/ \u221a N SVMPerf[8] SVMPerf, with defaults SVMMult[9] SVMMulticlass, with defaults Pegasos[14] Pegasos, with defaults SDB[3] Selective Block Minimization, biased StreamSVM[10] StreamSVM, with defaults", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "Name Description LL-Dual[5] Liblinear dual coordinate descent L2-regularization L2-loss biased LL-Primal[5] Liblinear primal coordinate descent L2-regularization L2-loss biased LL-CS[5] Liblinear Crammer and Singer PSVM[2] PSVM, with rank ratio set to 1/ \u221a N SVMPerf[8] SVMPerf, with defaults SVMMult[9] SVMMulticlass, with defaults Pegasos[14] Pegasos, with defaults SDB[3] Selective Block Minimization, biased StreamSVM[10] StreamSVM, with defaults", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "Name Description LL-Dual[5] Liblinear dual coordinate descent L2-regularization L2-loss biased LL-Primal[5] Liblinear primal coordinate descent L2-regularization L2-loss biased LL-CS[5] Liblinear Crammer and Singer PSVM[2] PSVM, with rank ratio set to 1/ \u221a N SVMPerf[8] SVMPerf, with defaults SVMMult[9] SVMMulticlass, with defaults Pegasos[14] Pegasos, with defaults SDB[3] Selective Block Minimization, biased StreamSVM[10] StreamSVM, with defaults", "startOffset": 182, "endOffset": 185}, {"referenceID": 1, "context": "Name Description LL-Dual[5] Liblinear dual coordinate descent L2-regularization L2-loss biased LL-Primal[5] Liblinear primal coordinate descent L2-regularization L2-loss biased LL-CS[5] Liblinear Crammer and Singer PSVM[2] PSVM, with rank ratio set to 1/ \u221a N SVMPerf[8] SVMPerf, with defaults SVMMult[9] SVMMulticlass, with defaults Pegasos[14] Pegasos, with defaults SDB[3] Selective Block Minimization, biased StreamSVM[10] StreamSVM, with defaults", "startOffset": 219, "endOffset": 222}, {"referenceID": 7, "context": "Name Description LL-Dual[5] Liblinear dual coordinate descent L2-regularization L2-loss biased LL-Primal[5] Liblinear primal coordinate descent L2-regularization L2-loss biased LL-CS[5] Liblinear Crammer and Singer PSVM[2] PSVM, with rank ratio set to 1/ \u221a N SVMPerf[8] SVMPerf, with defaults SVMMult[9] SVMMulticlass, with defaults Pegasos[14] Pegasos, with defaults SDB[3] Selective Block Minimization, biased StreamSVM[10] StreamSVM, with defaults", "startOffset": 266, "endOffset": 269}, {"referenceID": 8, "context": "Name Description LL-Dual[5] Liblinear dual coordinate descent L2-regularization L2-loss biased LL-Primal[5] Liblinear primal coordinate descent L2-regularization L2-loss biased LL-CS[5] Liblinear Crammer and Singer PSVM[2] PSVM, with rank ratio set to 1/ \u221a N SVMPerf[8] SVMPerf, with defaults SVMMult[9] SVMMulticlass, with defaults Pegasos[14] Pegasos, with defaults SDB[3] Selective Block Minimization, biased StreamSVM[10] StreamSVM, with defaults", "startOffset": 300, "endOffset": 303}, {"referenceID": 13, "context": "Name Description LL-Dual[5] Liblinear dual coordinate descent L2-regularization L2-loss biased LL-Primal[5] Liblinear primal coordinate descent L2-regularization L2-loss biased LL-CS[5] Liblinear Crammer and Singer PSVM[2] PSVM, with rank ratio set to 1/ \u221a N SVMPerf[8] SVMPerf, with defaults SVMMult[9] SVMMulticlass, with defaults Pegasos[14] Pegasos, with defaults SDB[3] Selective Block Minimization, biased StreamSVM[10] StreamSVM, with defaults", "startOffset": 340, "endOffset": 344}, {"referenceID": 2, "context": "Name Description LL-Dual[5] Liblinear dual coordinate descent L2-regularization L2-loss biased LL-Primal[5] Liblinear primal coordinate descent L2-regularization L2-loss biased LL-CS[5] Liblinear Crammer and Singer PSVM[2] PSVM, with rank ratio set to 1/ \u221a N SVMPerf[8] SVMPerf, with defaults SVMMult[9] SVMMulticlass, with defaults Pegasos[14] Pegasos, with defaults SDB[3] Selective Block Minimization, biased StreamSVM[10] StreamSVM, with defaults", "startOffset": 371, "endOffset": 374}, {"referenceID": 9, "context": "Name Description LL-Dual[5] Liblinear dual coordinate descent L2-regularization L2-loss biased LL-Primal[5] Liblinear primal coordinate descent L2-regularization L2-loss biased LL-CS[5] Liblinear Crammer and Singer PSVM[2] PSVM, with rank ratio set to 1/ \u221a N SVMPerf[8] SVMPerf, with defaults SVMMult[9] SVMMulticlass, with defaults Pegasos[14] Pegasos, with defaults SDB[3] Selective Block Minimization, biased StreamSVM[10] StreamSVM, with defaults", "startOffset": 421, "endOffset": 425}, {"referenceID": 13, "context": "% Pegasos[14] 1 - Crash SDB[3] 1 1 Crash StreamSVM[10] 2 4e-5 6138s 90.", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "% Pegasos[14] 1 - Crash SDB[3] 1 1 Crash StreamSVM[10] 2 4e-5 6138s 90.", "startOffset": 27, "endOffset": 30}, {"referenceID": 9, "context": "% Pegasos[14] 1 - Crash SDB[3] 1 1 Crash StreamSVM[10] 2 4e-5 6138s 90.", "startOffset": 50, "endOffset": 54}, {"referenceID": 7, "context": "48 SVMPerf[8] 1 2 641.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "42 LL-Primal[5] 1 4e-6 159.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "31 LL-Dual[5] 1 4e-6 126.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "% LL-Dual[5] 1 4e-6 Crash LL-Primal[5] 1 4e-6 Crash SVMPerf[8] 1 2 Crash StreamSVM[10] 2 4e-5 > 30h StreamSVM[10] 2 1e-5 44h 90.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "% LL-Dual[5] 1 4e-6 Crash LL-Primal[5] 1 4e-6 Crash SVMPerf[8] 1 2 Crash StreamSVM[10] 2 4e-5 > 30h StreamSVM[10] 2 1e-5 44h 90.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "% LL-Dual[5] 1 4e-6 Crash LL-Primal[5] 1 4e-6 Crash SVMPerf[8] 1 2 Crash StreamSVM[10] 2 4e-5 > 30h StreamSVM[10] 2 1e-5 44h 90.", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "% LL-Dual[5] 1 4e-6 Crash LL-Primal[5] 1 4e-6 Crash SVMPerf[8] 1 2 Crash StreamSVM[10] 2 4e-5 > 30h StreamSVM[10] 2 1e-5 44h 90.", "startOffset": 82, "endOffset": 86}, {"referenceID": 9, "context": "% LL-Dual[5] 1 4e-6 Crash LL-Primal[5] 1 4e-6 Crash SVMPerf[8] 1 2 Crash StreamSVM[10] 2 4e-5 > 30h StreamSVM[10] 2 1e-5 44h 90.", "startOffset": 109, "endOffset": 113}, {"referenceID": 4, "context": "Solver Cores C Train RMS error LL-Primal[5] 1 1 15.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "88 LL-Dual[5] 1 1 114.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "% LL-Dual[5] 1 1000 7.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "2 LL-Primal[5] 1 1000 1.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "% N=200,000 training subset: LL-CS[5] 1 0.", "startOffset": 34, "endOffset": 37}, {"referenceID": 8, "context": "9 SVMMult[9] 1 800000 518.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "8 Full N=4,000,000 training set: SVMMult[9] 1 80000 Crash LL-CS[5] 1 0.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "8 Full N=4,000,000 training set: SVMMult[9] 1 80000 Crash LL-CS[5] 1 0.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "% load % LL-Dual[5] 1 CPU core 44.", "startOffset": 16, "endOffset": 19}], "year": 2015, "abstractText": "As one of the most popular classifiers, linear SVMs still have challenges in dealing with very large-scale problems, even though linear or sub-linear algorithms have been developed recently on single machines. Parallel computing methods have been developed for learning large-scale SVMs. However, existing methods rely on solving local sub-optimization problems. In this paper, we develop a novel parallel algorithm for learning large-scale linear SVM. Our approach is based on a data augmentation equivalent formulation, which casts the problem of learning SVM as a Bayesian inference problem, for which we can develop very efficient parallel sampling methods. We provide empirical results for this parallel sampling SVM, and provide extensions for SVR, nonlinear kernels, and provide a parallel implementation of the Crammer and Singer model. This approach is very promising in its own right, and further is a very useful technique to parallelize a broader family of general maximum-margin models.", "creator": "TeX"}}}