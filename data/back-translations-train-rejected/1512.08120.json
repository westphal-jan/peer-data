{"id": "1512.08120", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Dec-2015", "title": "Regularized Orthogonal Tensor Decompositions for Multi-Relational Learning", "abstract": "Multi-relational learning has received lots of attention from researchers in various research communities. Most existing methods either suffer from superlinear per-iteration cost, or are sensitive to the given ranks. To address both issues, we propose a scalable core tensor trace norm Regularized Orthogonal Iteration Decomposition (ROID) method for full or incomplete tensor analytics, which can be generalized as a graph Laplacian regularized version by using auxiliary information or a sparse higher-order orthogonal iteration (SHOOI) version. We first induce the equivalence relation of the Schatten p-norm (0&lt;p&lt;\\infty) of a low multi-linear rank tensor and its core tensor. Then we achieve a much smaller matrix trace norm minimization problem. Finally, we develop two efficient augmented Lagrange multiplier algorithms to solve our problems with convergence guarantees. Extensive experiments using both real and synthetic datasets, even though with only a few observations, verified both the efficiency and effectiveness of our methods.", "histories": [["v1", "Sat, 26 Dec 2015 15:26:05 GMT  (768kb)", "https://arxiv.org/abs/1512.08120v1", "18 pages, 10 figures"], ["v2", "Sat, 16 Jan 2016 15:32:15 GMT  (700kb)", "http://arxiv.org/abs/1512.08120v2", "18 pages, 10 figures"]], "COMMENTS": "18 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["fanhua shang", "james cheng", "hong cheng"], "accepted": false, "id": "1512.08120"}, "pdf": {"name": "1512.08120.pdf", "metadata": {"source": "CRF", "title": "Regularized Orthogonal Tensor Decompositions for Multi-Relational Learning", "authors": ["Fanhua Shang", "Hong Cheng"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 151 2.08 120v 2 [cs.L G] 16 Jan 2016 JOURNAL OF LATEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 1 Regularized Orthogonal Tensor Decompositions for Multi-Relational Learning Fanhua Shang, Member, IEEE, James Cheng, and Hong ChengAbstract - Multi-Relational Learning has received a lot of attention from researchers in various research communities. Most existing methods either suffer from superlinear costs per iteration or are sensitive to the given rankings. To address both problems, we propose a scalable core tensor that uses the Standard Regulated Orthogonal Iteration Decomposition (ROID) for complete or incomplete tensor tensor analysis, which uses the Diagram Laplacian regulated Data Trace Standard."}, {"heading": "1 INTRODUCTION", "text": "In fact, in the USA, in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in"}, {"heading": "2 NOTATIONS AND PROBLEM FORMULATIONS", "text": "A third-order tensor is designated by a calligraphic letter, e.g. X-RI1 \u00b7 I2 \u00b7 I3, and its entries are designated as xi1i2i3, where the third-order tensors are x-i3, xi1: i3, and xi1i2:. The fibers of mode-n-deployment, also called matricization, of a third-order tensor are x-i2i3, xi1: i3, and xi1i2:, respectively, the mode-n fibers are arranged as columns of the resulting matrix X (n) so that the tensor X-RI1 \u00b7 I2 \u00b7 I3 becomes a row index, and all other two modes as column indices."}, {"heading": "2.1 Tensor Trace Norm", "text": "With an exact analogy to the definition of the matrix rank, the rank of a tensor X is defined as the smallest number of rank-one tensors that generate X as their sum. However, there is no easy way to determine the rank of a tensor X. In fact, the problem is NP-hard [6], [30]. Fortunately, the multilinear rank of a third-order tensor (also Tucker rank in [27], [31]) of a tensor X is easy to calculate and consists of the ranks of all mode-n unfoldings. Definition 1. The multilinear rank of a third-order tensor X is the tuple of the order of mode-n unfoldings, multilinear rank = [rank (X (1)), rank (X (2), rank (X (3), rank (ltn))."}, {"heading": "2.2 Weighted Tensor Decompositions", "text": "In fact, the fact is that most of us will be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be."}, {"heading": "2.3 Problem Formulations", "text": "For multirelational predictions, the problem of sparse tensor trace standard minimization is formulated as follows: min X3 \u2211 n = 1\u03b1n \u0445 X (n), s.t., X\u0442 = T\u044b (3), where \u03b1n's are pre-specified weights, and \u0439 is the set of indexes of observed entries. Liu et al. [22] proposed three efficient algorithms (e.g. the HaLRTC algorithm) to solve them (3). Furthermore, there are some similar convex tensor completion algorithms in [18], [23], [24]. Tomioka and Suzuki [25] proposed a latent trace standard minimization model, min Xn1\u03bbN n = 1 Xn, (n), optimization and completion algorithms in [18], [23], [24]."}, {"heading": "3 CORE TENSOR TRACE NORM REGULARIZED TENSOR DECOMPOSITION", "text": "To address the poor scalability of existing multilinear tensor recovery algorithms, we introduce two scalable tensor trace standards (or together with Laplacian diagrams) that regulate orthogonal decomposition models, and then achieve three minor matrix trace standard minimization problems. Then, in Section 4, we will develop some efficient algorithms to solve the problems."}, {"heading": "3.1 Core Tensor Trace Norm Minimization Models", "text": "Suppose X-RI1 \u00b7 I2 \u00b7 I3 is a multi-relational tensor with multi-linear rankings (r1, r2, r3), X-R-R (3), X-R (3), W-R (3), W-R (3), W-R (3), W-R (3), W-R (3), W-R (3), W-R (3), W-R (3), W-R (3), W-R (2), W-R (3), W-R (3), W-R (3), W-R (3), W-R (3), W-R (3), R (3), R-R, W-R, R (3), W-W (3), W (3), R (3), R (3), R (R), W-R, W-R (3), W-R (3)."}, {"heading": "3.2 Sparse HOOI Model", "text": "If \u03bb \u2192 \u221e, the model (6) degenerates to the following sparse tensor HOOI problem (SHOOI), min G, U, V, W, Z1 2 \u0445 W * (Z \u2212 T) \u0445 2F, s.t., Z = G \u00b7 1U \u00b7 2V \u00b7 3W, U TU = Id1, V TV = Id2, W TW = Id3. (8) In a way, the SHOOI model (8) is a special case of our ROID method (see supplementary materials for detailed discussions). If all entries of T are observed, the SHOOI model (8) in [33] becomes a traditional HOOI problem."}, {"heading": "3.3 Graph Regularized Model", "text": "Inspired by the work in [40], [41], [42], we also use the auxiliary information given as link affinity matrices in a diagram-controlled ROID (GROID) model: min G, U, V, W, X1, 2, X \u2212 G \u00b7 1U x 2V x 3W, 2 F + \u00b52 [Tr (UTL1U) + Tr (V TL2V) + Tr (W TL3W)], s.t., XB = TN, U TU = Id1, V TV = Id2, W TW = Id3 (9), where \u00b5 0 is a regulating constant, Tr (\u00b7) denotes the matrix track, Ln the laplazian matrix, i.e. Ln = Dn \u2212 Wn, Wn the weight matrix for the object being placed or other relationships, and Dn the diagonal matrix whose entries are ij (ij)."}, {"heading": "4 OPTIMIZATION ALGORITHMS", "text": "In this section, we propose an efficient method of extended lagrange multipliers (ALM) to solve our ROID problem (6) and then extend the proposed algorithm to solve (7) - (9). As a variant of the standard ALM, the alternative direction of multipliers (ADMM) method has recently attracted a lot of attention due to the huge demand for machine learning applications [43], [44]. Similar to (3), the proposed problem (6) is difficult to solve due to interdependent tensor tracking, so we will first present three much smaller auxiliary variables."}, {"heading": "4.4 Extension for GROID", "text": "Algorithm m m 1 can be extended to solve our GROID problem (9), where the main difference k = k is that the sub-problem with respect to U, V, W and G is formulated as follows: min G, U, V, W3, N = 1k2, G (n) \u2212 K (n) \u2212 K (n) \u2212 G (n) \u2212 K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n), K (n, K, n, K (n), K (n, K, K (n), K (n), K (n), K (n, K (n), K (n), K (n), K (n, K (n), K (n), K (n), K (n, K (n), K (n), K (n, K (n), K (n), K (n), K (n, K (n), K (n, K (n), K (n), K (n, K (n), K (n), K (n, K (n), K (n), K (n, K (n), K (n), K (n, K (n), K (n, K (n), K (n, K (n), K (n, K (n, n), K (n), K (n, K (n), K (n, K (n), K (n, K (n), K (n, K (n), K (n), K (n, K (n), K (n, K (n), K (n), K (n, K (n), K (n, K (n, K (n), K (n), K (n, n, K (n), K (n),"}, {"heading": "5 ALGORITHM ANALYSIS", "text": "In this section we provide convergence analysis and complexity analysis for our algorithms."}, {"heading": "5.1 Convergence Analysis", "text": "With the low multilinear rank tensor decomposition in (6), the problem (10) is k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k \u00b7 k (1) and therefore we can only consider local convergence [43]. As in [47], [48] below, we show a necessary condition for local convergence. Lemma 1. If the sequences {V} (n = 1, 2, 3) {V \u00b2 n {V} {V, k, V \u00b2 k, W \u00b2 n \u2212 K, then Z \u2212 K + 1 \u2212 Z \u2212 Z k \u2192 0, and {Z k} is bounded.Proof. First, we prove that the Lagrangian function of (10) is limited and fulfilled."}, {"heading": "5.2 Complexity Analysis", "text": "The time complexity of some multiplication operators in (18), (19) and (22) is O ((((2d1 + d2 + d3); the time complexity of some multiplication operators in (18), (19) and (22) is O ((((2d1 + d2 + d3)); the total time complexity of both ROID and GROID is O ((((2d1 + d2 + d3)); our algorithms are essentially the Gauss rope-like schemes of ADMM, and the update strategy of the Jacobi version as in [29] is easy to implement and well suited for parallel computation."}, {"heading": "6 EXPERIMENTAL RESULTS", "text": "In this section, we evaluate the effectiveness and efficiency of our ROID method for completing low multilinear tensors for synthetic data and multirelational learning for real data such as a network dataset and three popular multirelational datasets. Except for large-scale multirelational predictions, all other experiments were conducted on an Intel (R) Core (TM) i5-4570 (3.20 GHz) PC with Windows 7 and 8GB of RAM."}, {"heading": "6.1 Results on Synthetic Data", "text": "Following [22], we created low order multilinear tensors T-RI1-I2-I3, which we used as basic truth data; the generated tensor data follow the Tucker model, i.e. T = C-1U1-2U2-3U3, where C-Rr-r-r is the core tensor, whose entries are generated as independent and identically distributed (i.e.) numbers from a uniform distribution in [0, 1]; and the entries of Un-RIn-r are random samples from a uniform distribution in the range [-0.5, 0.5]. With this construction, the multilinear rank of third order tensors T almost certainly corresponds to (r, r, r, r)."}, {"heading": "6.1.1 Algorithm Settings", "text": "We compare our ROID method with the following state of the art: 1) WTucker1 [17]: In implementing WTucker, we set R1 = R2 = R3 = 1.25r for solving the weighted tucker (WTucker) decomposition problem (2). 2) WCP2 [8]: We set the tensor R2 = R3 = 1.25r for solving the weighted CP (WCP) decomposition problem (1), 1. http: / / www.lair.irb.hr / ikopriva / marko-filipovi.html 2. http: / / www.sandia.gov / \u0445 tgkolda / TensorToolbox / and the maximum number of iterations, maxiter = 100, for WCP and WTucker, both are solved by non-linear conjugation gradient methods."}, {"heading": "6.1.2 Numerical Results on Sparse Tensors", "text": "To evaluate the robustness of our ROID method in terms of multilinear ranking parameter changes, we first conduct some experiments on synthetic tensors of size 100 x 100 x 200 x 200 x 200 x 200, and illustrate the RSE results of all these tensor decomposition methods with a 10% sampling ratio, where the number of specified ranking increases, the RSE rate of all of these tensor methods (except WCP) is gradually increasing, especially for SHOOI. More precisely, SHOOI provides extremely precise solutions to exact multilinear ranking or manufacturing problems."}, {"heading": "6.1.3 Numerical Results on Full Tensors", "text": "In order to further evaluate the performance of our full range reduction method, we compare our ROID method with the Low Multilinear Rank Approximation (LMLRA) method [10], [53] and HOOI [33], [53] on Noisy Tensors, i.e. T = C \u00d7 1U1 \u00d7 2U2 \u00d7 3U3 + nf \u0445 E, where nf denotes the noise factor and E denotes the standard Gaussian random noise. Figure 5 (a) illustrates the RSE results of LMLRA, HOOI and ROID on 200 \u00d7 200 noise sensors with different noise factors. We can find that ROID is more robust and stable against noise than the other methods. In addition, we also report on the runtime of tenors of different sizes in Figure 5 (b), from which we can see that our ROID method ran more than ten times faster than the other methods."}, {"heading": "6.2 Results on Network Data", "text": "In this part we examine our ROID and graphics regulated weighted (called GROID) methods on real network records = = 49decomposition = like the YouTube records set5 [54]. YouTube is currently the most popular video sharing site, which allows users to interact with each other in various forms such as contacts, subscriptions, sharing favorite videos, etc. Overall, this record contains 848,003 users, with 15,088 users who share all information types, and includes 5-dimensional interactions: contact network, co-subscription network, co-subscription network and favorite network. Additional information about the data can be found in [54]. We run these experiments on a machine with 6-core Intel Xeon 2.4GHz CPU and 64GB memory. We address the multirelationship problem as a tensor completion problem."}, {"heading": "6.3 Results on Multi-Relational Data", "text": "Finally, we examine how well our ROID method affects the three popular multirelational datasets previously used by Kemp et al. [55] for left prediction, including the Kinship, Nations and UMLS datasets. Kinship datasets consist of kinship relationships (such as \"father\" or \"wife\" relationships) between members of the ALYawarra tribe in Central Australia [56]. The dataset contains 104 tribal members and 26 types of kinship relationships (binary), which include a three-order tensor of size 104 \u00d7 104 \u00d7 26. The dataset of nations consists of international relations between different countries of the world [57]. The dataset contains 14 countries and 56 types of (binary) relationships (such as \"Treaties\" or \"Military Alliance\"), and is a three-order tensor of size 104 \u00d7 104 \u00d7 26. The dataset of nations consists of international relations between different countries of the world [57]."}, {"heading": "6.4 Running Time and Robustness Analysis", "text": "In addition, we present the comparison of the runtime of related methods on three multirelational data sets. In [1] it was shown that WCP and RESCAL are much faster than MRC and IRM. Therefore, we report only on the runtime of WCP, RESCAL, HaLRTC and our ROID method on these three data sets with different ranks, as in Table 3. It is clear that our ROID method is much faster than WCP and RESCAL and is better suited for large-scale multirelational data. RESCAL and WCP tend to scale worse in terms of ranks than our ROID method. In other words, with the increase in the given tensor ranks, the runtime of RESCAL and WCP increases dramatically, while the runtime of our ROID method changes only slightly. We also evaluate the robustness of our ROID method against its parameters: the given tensor ranks and the regulation parameters for these three parameters, we find the most robust in our FID, whereby we find the ROID 1 and the FID in our FID."}, {"heading": "7 CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we proposed a scalable ROID method and its graphically regulated version for complete or incomplete tensor analysis, such as multirelational learning. First, we induced the equivalence relationship of the shadow p standard (0 < p < \u221e) of a low multilinear rank tensor and its core tensor. Then, we presented a novel orthogonal tensor decomposition model with core tensor trace normalization. In addition, we theoretically analyzed the local convergence of our algorithms. The convincing experimental results for real problems confirmed both the efficiency and effectiveness of our methods, especially from only a few observations. < < < our ROID method can be extended to various tensors of higher order or recovery problems."}, {"heading": "APPENDIX A PROOF OF THEOREM 1:", "text": "Before we provide the proof for Theorem 1, we will first introduce some properties of matrices and tensors in the following definition (A and B): (A) two matrices of size m \u00b7 n and p \u00b7 q, respectively. (B) The Kronecker product of both matrices A and B is an mp \u00b7 nq matrix, which is owned by: A B = [aijB] mp \u00b7 nq 1. Let A p \u00b7 p, C Rp \u00b7 q, and B Rn \u00b7 q, then ACBT Sp = C column-ormal, i.e., ATA = Ip and BTB = Iq.Proof. Let's get the SVD of C = U.V T, then ACBT = (AU)."}, {"heading": "APPENDIX B PROOF OF THEOREM 2:", "text": "The Evidence: The Optimization Problem (14) in respect of Gh (G) = 3 \u2211 n = 1\u03c1k2 The Optimization Problem (14) in respect of Gh (G) = 3 \u2211 n = 1\u03c1k2 The Optimization Problem (14) The Optimization Problem (G) The Optimization Problem (G) The Optimization Problem (G) The Optimization Problem (G) = 1\u03c1k2 The Optimization Problem (G) The Optimization Problem (G) The Optimization Problem (G) The Optimization Problem (G) The Optimization Problem (G) The Optimization Problem (G) The Optimization Problem (G) The Optimization Problem (G) The Optimization Problem (G) The Optimization Problem (G)"}, {"heading": "APPENDIX C PROOF OF THEOREM 3:", "text": "The proof: Letf (G, U, V, W) = 11 + > G (n) \u2212 Gk + 1n + Y kn / 0 (V) T \u00b7 3 (W) T \u00b7 3 (W) T \u2212 3 (G) T \u2212 G \u00b7 1U \u00b7 2V \u00b7 3W (G + 1n \u2212 Y k / 0 k).Then the closed solution of (37) with respect to G is obtained by (15), and it can be rewritten asG = 11 + 3k A + 0 (38) and according to the definitions of tensors A and B: we have < X k, G \u00b7 1U \u00b7 2V \u00b7 3W > kF \u2212 2k (A + 0)."}, {"heading": "APPENDIX D PROOF OF THEOREM 4:", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}], "references": [{"title": "A three-way model for collective learning on multi-relational data,", "author": ["M. Nickel", "V. Tresp", "H. Kriegel"], "venue": "in Proc. 28th Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Statistical predicate invention,", "author": ["S. Kok", "P. Domingos"], "venue": "Proc. 24th Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Relational learning via collective matrix factorization,", "author": ["A. Singh", "G. Gordon"], "venue": "Proc. 14th ACM Int. Conf. Know. Disc. Data Min. (KDD),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "A latent factor model for highly multi-relational data,", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "G. Obozinski"], "venue": "in Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning)", "author": ["L. Getoor", "B. Taskar"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Tensor decompositions and applications,", "author": ["T. Kolda", "B. Bader"], "venue": "SIAM Review,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "MultiVis: Content-based social network exploration through multiway visual analysis,", "author": ["J. Sun", "S. Papadimitriou", "C. Lin", "N. Cao", "S. Liu", "andW. Qian"], "venue": "in Proc. SIAM Int. Conf. Data Min. (SDM),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion,", "author": ["Y. Xu", "W. Yin"], "venue": "SIAM J. Imaging Sci.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Some mathematical notes on three-mode factor", "author": ["L. Tucker"], "venue": "analysis,\u201d Psychometrika,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1966}, {"title": "Foundations of the PARAFAC procedure: Models and conditions for an explanatory multi-modal factor analysis,", "author": ["R. Harshman"], "venue": "UCLA Working Papers in Phonetics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1970}, {"title": "PARAFAC: parallel factor analysis,", "author": ["R. Harshman", "M. Lundy"], "venue": "Comput. Stat. Data An.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Generalized coupled tensor factorization,", "author": ["Y. Yilmaz", "A. Cemgil", "U. Simsekli"], "venue": "in Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Unsupervised multiway data analysis: A literature survey,", "author": ["E. Acar", "B. Yener"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Factor matrix trace norm minimization for low-rank tensor completion,", "author": ["Y. Liu", "F. Shang", "H. Cheng", "J. Cheng", "H. Tong"], "venue": "in Proc. SIAM Int. Conf. Data Min. (SDM),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Temporal analysis of semantic graphs using ASALSAN,", "author": ["B. Bader", "R. Harshman", "T. Kolda"], "venue": "in Proc. 7th IEEE Int. Conf. Data Min. (ICDM),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Tucker factorization with missing data with application to low-n-rank tensor completion,\u201dMultidim", "author": ["M. Filipovic", "A. Jukic"], "venue": "Syst. Sign. Process.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Tensor completion and lown-rank tensor recovery via convex optimization,", "author": ["S. Gandy", "B. Recht", "I. Yamada"], "venue": "Inverse Probl.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Generalized higher-order orthogonal iteration for tensor decomposition and completion,", "author": ["Y. Liu", "F. Shang", "W. Fan", "J. Cheng", "H. Cheng"], "venue": "in Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Tensor completion for estimating missing values in visual data,", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "in Proc. of IEEE Int. Conf. Comput. Vis. (ICCV),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "A rank minimization heuristic with application to minimum order system approximation,", "author": ["M. Fazel", "H. Hindi", "S.P. Boyd"], "venue": "in Proc. IEEE Amer. Control Conf.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Tensor completion for estimating missing values in visual data,", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Learning with tensors: A framework based on covex optimization and spectral regularization,", "author": ["M. Signoretto", "Q. Dinh", "L. Lathauwer", "J. Suykens"], "venue": "Mach. Learn., vol. 94,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Convex tensor decomposition via structured Schatten norm regularization,", "author": ["R. Tomioka", "T. Suzuki"], "venue": "in Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Provable models for robust low-rank tensor completion,", "author": ["B. Huang", "C. Mu", "D. Goldfarb", "J. Wright"], "venue": "Pac. J. Optim.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Square deal: Lower bounds and improved relaxations for tensor recovery,", "author": ["C. Mu", "B. Huang", "J. Wright", "D. Goldfarb"], "venue": "in Proc. 31st Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Generalized higher order orthogonal iteration for tensor learning and decomposition,", "author": ["Y. Liu", "F. Shang", "W. Fan", "J. Cheng", "H. Cheng"], "venue": "IEEE Trans. Neural Netw. Learn. Syst.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Generalized higher-order tensor decomposition via parallel ADMM,", "author": ["F. Shang", "Y. Liu", "J. Cheng"], "venue": "in Proc. 28th AAAI Conf. Artif. Intell. (AAAI),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Most tensor problems are NP hard,", "author": ["C. Hillar", "L. Lim"], "venue": "J. ACM,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Robust low-rank tesnor recovery: Models and algorithms,", "author": ["D. Goldfarb", "Z. Qin"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "A multilinear singular value decomposition,", "author": ["L. Lathauwer", "B. Moor", "J. Vandewalle"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2000}, {"title": "Simultaneous tensor decomposition and completion using factor priors,", "author": ["Y. Chen", "C. Hsu", "H. Liao"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "A new convex relaxation for tensor completion,", "author": ["B. Romera-Paredes", "M. Pontil"], "venue": "in Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Rank estimation in missing data matrix problems,", "author": ["C. Juli\u00e0", "A.D. Sappa", "F. Lumbreras", "J. Serrat", "A. L\u00f3pez"], "venue": "J. Math. Imaging Vis.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "The power of convex relaxation: Nearoptimal matrix completion,", "author": ["E. Cand\u00e8s", "T. Tao"], "venue": "IEEE Trans. Inf. Theory, vol. 56,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Tensor facotiration using auxiliary information,", "author": ["A. Narita", "K. Hayashi", "R. Tomioka", "H. Kashima"], "venue": "Data Min. Knowl. Disc.,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "Collaborative filtering: weighted nonnegative matrix factorization incorporating user and item graphs,", "author": ["Q. Gu", "J. Zhou", "C. Ding"], "venue": "in Proc. SIAM Int. Conf. Data Min. (SDM),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "Graph dual regularization nonnegative matrix factorization for co-clustering,", "author": ["F. Shang", "L. Jiao", "F. Wang"], "venue": "Pattern Recogn.,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers,", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "Stochastic alternating direction method of multipliers,", "author": ["H. Ouyang", "N. He", "L. Tran", "A. Gray"], "venue": "in Proc. 30th Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "A singular value thresholding algorithm for matrix completion,", "author": ["J. Cai", "E. Cand\u00e8s", "Z. Shen"], "venue": "SIAM J. Optim., vol. 20,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2010}, {"title": "Matrix procrustes problems,", "author": ["H. Nick"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1995}, {"title": "An alternating direction algorithm for matrix completion with nonnegative factors,", "author": ["Y. Xu", "W. Yin", "Z. Wen", "Y. Zhang"], "venue": "Front. Math. Chin.,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2012}, {"title": "Efficient kernel learning from side information using ADMM,", "author": ["E. Hu", "J. Kwok"], "venue": "in Proc. 23rd Int. Joint Conf. Artif. Intell. (IJCAI),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Nonlinear Programming", "author": ["D. Bertsekas"], "venue": "Athena Scientific, Belmont: The 2nd edition,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1999}, {"title": "Structured low-rank matrix factorization: Optimality, algorithm, and applications to image processing,", "author": ["B. Haeffele", "E. Young", "R. Vidal"], "venue": "in Proc. 31st Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Statistical performance of convex tensor decomposition,", "author": ["R. Tomioka", "T. Suzuki", "K. Hayashi", "H. Kashima"], "venue": "in Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2011}, {"title": "A scaled conjugate gradient algorithm for fast supervised learning,", "author": ["M.F. M\u00f8ller"], "venue": "Neural Netw.,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1993}, {"title": "Uncovering groups via heterogeneous interaction analysis,", "author": ["L. Tang", "X. Wang", "H. Liu"], "venue": "in Proc. 9th IEEE Int. Conf. Data Min. (ICDM),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2009}, {"title": "Learning systems of concepts with an infinite relational model,", "author": ["C. Kemp", "J. Tenenbaum", "T. Griffiths", "T. Yamada", "N. Ueda"], "venue": "in Proc. 21st AAAI Conf. Artif. Intell. (AAAI),", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2006}, {"title": "The Detection of Patterns in Alyawarra Nonverbal Behavior", "author": ["W. Denham"], "venue": "University of Washington: PhD thesis,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1973}, {"title": "Dimensionality of nations project: Attributes of nations and behavior of nation dyads,", "author": ["R. Rummel"], "venue": "ICPSR Data File,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1999}, {"title": "An upper level ontology for the biomedical domain,", "author": ["A. McGray"], "venue": "Comp. Funct. Genom.,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "R ELATIONAL learning is becoming increasingly important because of the high value hidden in relational data and also of its many applications in various domains such as social networks, the semantic web, bioinformatics, and the linked data cloud [1].", "startOffset": 246, "endOffset": 249}, {"referenceID": 1, "context": "A class of relational learning methods focus mostly on the problem of modeling a single relation type, such as relational learning from latent attributes [2], [3], which models relations between objects as resulting from intrinsic latent attributes of these objects.", "startOffset": 154, "endOffset": 157}, {"referenceID": 2, "context": "A class of relational learning methods focus mostly on the problem of modeling a single relation type, such as relational learning from latent attributes [2], [3], which models relations between objects as resulting from intrinsic latent attributes of these objects.", "startOffset": 159, "endOffset": 162}, {"referenceID": 3, "context": "For example, in social networks [4], relationships between individuals may be personal, familial, or professional.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "This type of relational data learning is often referred to as multi-relational learning (MRL), which needs to model large-scale sparse relational databases efficiently [5].", "startOffset": 168, "endOffset": 171}, {"referenceID": 5, "context": "In recent years, tensors have become ubiquitous such as multi-channel images and videos, and become popular due to the ability to discover complex and interesting latent structures and correlations of data [6], [7], [8], [9].", "startOffset": 206, "endOffset": 209}, {"referenceID": 6, "context": "In recent years, tensors have become ubiquitous such as multi-channel images and videos, and become popular due to the ability to discover complex and interesting latent structures and correlations of data [6], [7], [8], [9].", "startOffset": 211, "endOffset": 214}, {"referenceID": 7, "context": "In recent years, tensors have become ubiquitous such as multi-channel images and videos, and become popular due to the ability to discover complex and interesting latent structures and correlations of data [6], [7], [8], [9].", "startOffset": 221, "endOffset": 224}, {"referenceID": 8, "context": "Tensor decomposition [10], [11], [12], [13] is a popular tool for multi-relational prediction problems [14], [15].", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "Tensor decomposition [10], [11], [12], [13] is a popular tool for multi-relational prediction problems [14], [15].", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "Tensor decomposition [10], [11], [12], [13] is a popular tool for multi-relational prediction problems [14], [15].", "startOffset": 33, "endOffset": 37}, {"referenceID": 11, "context": "Tensor decomposition [10], [11], [12], [13] is a popular tool for multi-relational prediction problems [14], [15].", "startOffset": 39, "endOffset": 43}, {"referenceID": 12, "context": "Tensor decomposition [10], [11], [12], [13] is a popular tool for multi-relational prediction problems [14], [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": "Tensor decomposition [10], [11], [12], [13] is a popular tool for multi-relational prediction problems [14], [15].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "[16] proposed a three-way component decomposition model for analyzing intrinsically asymmetric relationships.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] incorporated collective learning into the tensor factorization, which is designed to account for the inherent structure of relational data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Two of the most popular tensor factorizations are the Tucker decomposition [10] and the CANDECOMP/PARAFAC (CP) decomposition [11].", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "Two of the most popular tensor factorizations are the Tucker decomposition [10] and the CANDECOMP/PARAFAC (CP) decomposition [11].", "startOffset": 125, "endOffset": 129}, {"referenceID": 15, "context": "To address incomplete tensor estimation, two weighted alternating leastsquares methods [8], [17] were proposed.", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "However, these methods require the ability to reliably estimate the rank of the involved tensor [18], [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "However, these methods require the ability to reliably estimate the rank of the involved tensor [18], [19].", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "[20] first extended the trace norm (also known as the nuclear norm [21] or the Schatten 1-norm [19]) regularization for partially observed lowmultilinear rank tensor recovery.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] first extended the trace norm (also known as the nuclear norm [21] or the Schatten 1-norm [19]) regularization for partially observed lowmultilinear rank tensor recovery.", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "[20] first extended the trace norm (also known as the nuclear norm [21] or the Schatten 1-norm [19]) regularization for partially observed lowmultilinear rank tensor recovery.", "startOffset": 95, "endOffset": 99}, {"referenceID": 20, "context": "\u2019s subsequent paper [22], they proposed three efficient algorithms to solve the lowmulti-linear rank tensor completion problem.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "can also be found in [18], [23], [24], [25].", "startOffset": 21, "endOffset": 25}, {"referenceID": 21, "context": "can also be found in [18], [23], [24], [25].", "startOffset": 27, "endOffset": 31}, {"referenceID": 22, "context": "can also be found in [18], [23], [24], [25].", "startOffset": 39, "endOffset": 43}, {"referenceID": 22, "context": "In addition, there are some theoretical developments that guarantee the reconstruction of a low rank tensor from partial measurements by solving trace norm minimization under some reasonable conditions [25], [26], [27], [28].", "startOffset": 202, "endOffset": 206}, {"referenceID": 23, "context": "In addition, there are some theoretical developments that guarantee the reconstruction of a low rank tensor from partial measurements by solving trace norm minimization under some reasonable conditions [25], [26], [27], [28].", "startOffset": 208, "endOffset": 212}, {"referenceID": 24, "context": "In addition, there are some theoretical developments that guarantee the reconstruction of a low rank tensor from partial measurements by solving trace norm minimization under some reasonable conditions [25], [26], [27], [28].", "startOffset": 214, "endOffset": 218}, {"referenceID": 25, "context": "In addition, there are some theoretical developments that guarantee the reconstruction of a low rank tensor from partial measurements by solving trace norm minimization under some reasonable conditions [25], [26], [27], [28].", "startOffset": 220, "endOffset": 224}, {"referenceID": 17, "context": "Therefore, existing algorithms suffer from high computational cost, making them impractical for realworld applications [19], [29].", "startOffset": 119, "endOffset": 123}, {"referenceID": 26, "context": "Therefore, existing algorithms suffer from high computational cost, making them impractical for realworld applications [19], [29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 5, "context": "In fact, the problem is NP-hard [6], [30].", "startOffset": 32, "endOffset": 35}, {"referenceID": 27, "context": "In fact, the problem is NP-hard [6], [30].", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "Fortunately, the multi-linear rank (also called the Tucker rank in [27], [31]) of a tensor X is easy to compute, and consists of the ranks of all mode-n unfoldings.", "startOffset": 67, "endOffset": 71}, {"referenceID": 28, "context": "Fortunately, the multi-linear rank (also called the Tucker rank in [27], [31]) of a tensor X is easy to compute, and consists of the ranks of all mode-n unfoldings.", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "In [17], the weighted Tucker decomposition (WTucker) model is formulated as follows:", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "If the factor matrices of the Tucker decomposition are constrained orthogonal, the classical decomposition methods are referred to as the higher-order singular value decomposition (HOSVD) [32] or higher-order orthogonal iteration (HOOI) [33], where the latter leads to the estimation of best rank-(R1, R2, R3) approximations while the truncation of HOSVD may achieve a good rank-(R1, R2, R3) approximation but in general not the best possible one [33].", "startOffset": 188, "endOffset": 192}, {"referenceID": 30, "context": "In addition, several extensions of both tensor decomposition models are developed for tensor estimation problems, such as [34], [35], [36].", "startOffset": 128, "endOffset": 132}, {"referenceID": 16, "context": "However, for all those methods, a suitable rank value needs to be given, and it has been shown that both WTucker andWCP models are usually sensitive to the given ranks due to their least-squares formulations [18], [19], and they have poor performance when the data have a high rank [22].", "startOffset": 208, "endOffset": 212}, {"referenceID": 17, "context": "However, for all those methods, a suitable rank value needs to be given, and it has been shown that both WTucker andWCP models are usually sensitive to the given ranks due to their least-squares formulations [18], [19], and they have poor performance when the data have a high rank [22].", "startOffset": 214, "endOffset": 218}, {"referenceID": 20, "context": "However, for all those methods, a suitable rank value needs to be given, and it has been shown that both WTucker andWCP models are usually sensitive to the given ranks due to their least-squares formulations [18], [19], and they have poor performance when the data have a high rank [22].", "startOffset": 282, "endOffset": 286}, {"referenceID": 20, "context": "[22] proposed three efficient algorithms (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In addition, there are some similar convex tensor completion algorithms in [18], [23], [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "In addition, there are some similar convex tensor completion algorithms in [18], [23], [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "Tomioka and Suzuki [25] proposed a latent trace norm minimization model,", "startOffset": 19, "endOffset": 23}, {"referenceID": 24, "context": "More recently, it has been shown that the tensor trace norm minimization models mentioned above can be substantially suboptimal [27], [37].", "startOffset": 128, "endOffset": 132}, {"referenceID": 31, "context": "More recently, it has been shown that the tensor trace norm minimization models mentioned above can be substantially suboptimal [27], [37].", "startOffset": 134, "endOffset": 138}, {"referenceID": 24, "context": "However, if the order of the involved tensor is no more than three, the models (3) and (4) often perform better than the more balanced (square) matrix model in [27].", "startOffset": 160, "endOffset": 164}, {"referenceID": 32, "context": "For rn (n = 1, 2, 3), we recommend a matrix rank estimation approach recently developed in [38] to compute some good values (r 1, r \u2032 2, r \u2032 3) for the multi-linear rank of the involved tensor.", "startOffset": 91, "endOffset": 95}, {"referenceID": 19, "context": ", the Schatten 1-norm) is the tightest convex surrogate to the rank function [21], [39], we mainly consider the trace norm case in this paper.", "startOffset": 77, "endOffset": 81}, {"referenceID": 33, "context": ", the Schatten 1-norm) is the tightest convex surrogate to the rank function [21], [39], we mainly consider the trace norm case in this paper.", "startOffset": 83, "endOffset": 87}, {"referenceID": 26, "context": "When all entries of T are observed, the model (6) degenerates to the following core tensor trace norm regularized tensor decomposition problem [29]:", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "Besides, the core tensor trace norm term promotes low multi-linear rank tensor decompositions, and enhances the robustness of the multi-linear rank selection, while those traditional tensor decomposition methods are usually sensitive to the given multi-linear rank [22], [29].", "startOffset": 265, "endOffset": 269}, {"referenceID": 26, "context": "Besides, the core tensor trace norm term promotes low multi-linear rank tensor decompositions, and enhances the robustness of the multi-linear rank selection, while those traditional tensor decomposition methods are usually sensitive to the given multi-linear rank [22], [29].", "startOffset": 271, "endOffset": 275}, {"referenceID": 34, "context": "Inspired by the work in [40], [41], [42], we also exploit the auxiliary information given as link-affinity matrices in a graph regularized ROID (GROID) model:", "startOffset": 24, "endOffset": 28}, {"referenceID": 35, "context": "Inspired by the work in [40], [41], [42], we also exploit the auxiliary information given as link-affinity matrices in a graph regularized ROID (GROID) model:", "startOffset": 30, "endOffset": 34}, {"referenceID": 36, "context": "Inspired by the work in [40], [41], [42], we also exploit the auxiliary information given as link-affinity matrices in a graph regularized ROID (GROID) model:", "startOffset": 36, "endOffset": 40}, {"referenceID": 37, "context": "As a variant of the standard ALM, the alternating direction method of multipliers (ADMM) has received much attention recently due to the tremendous demand from large-scale machine learning applications [43], [44].", "startOffset": 202, "endOffset": 206}, {"referenceID": 38, "context": "As a variant of the standard ALM, the alternating direction method of multipliers (ADMM) has received much attention recently due to the tremendous demand from large-scale machine learning applications [43], [44].", "startOffset": 208, "endOffset": 212}, {"referenceID": 39, "context": "For solving (12), we give the shrinkage operator [45] below.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "Hence, our algorithm has a much lower complexity than those as in [18], [22], [23], [24], [25].", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "Hence, our algorithm has a much lower complexity than those as in [18], [22], [23], [24], [25].", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "Hence, our algorithm has a much lower complexity than those as in [18], [22], [23], [24], [25].", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "Hence, our algorithm has a much lower complexity than those as in [18], [22], [23], [24], [25].", "startOffset": 90, "endOffset": 94}, {"referenceID": 40, "context": "This is actually the wellknown orthogonal procrustes problem [46].", "startOffset": 61, "endOffset": 65}, {"referenceID": 37, "context": "To monitor convergence of Algorithm 1, the adaptively adjusting strategy of the penalty parameter \u03c1 in [43] is introduced.", "startOffset": 103, "endOffset": 107}, {"referenceID": 37, "context": "Following [43], an efficient strategy is to let \u03c1 = \u03c1 (the initialization in Algorithm 1) and update \u03c1 iteratively by:", "startOffset": 10, "endOffset": 14}, {"referenceID": 40, "context": "Following [46], the solution of (29) is given by U = ORT(F1(U ) + F2(U ) +Q+ \u03c4U).", "startOffset": 10, "endOffset": 14}, {"referenceID": 37, "context": "1 Convergence Analysis With the low multi-linear rank tensor decomposition in (6), the problem (10) is non-convex and so we can only consider local convergence [43].", "startOffset": 160, "endOffset": 164}, {"referenceID": 41, "context": "As in [47], [48], we show below a necessary condition for local convergence.", "startOffset": 6, "endOffset": 10}, {"referenceID": 42, "context": "As in [47], [48], we show below a necessary condition for local convergence.", "startOffset": 12, "endOffset": 16}, {"referenceID": 43, "context": "We first give the following lemma [49].", "startOffset": 34, "endOffset": 38}, {"referenceID": 26, "context": "Our algorithms are essentially the Gauss-Seidel-type schemes of ADMM, and the update strategy of the Jacobi version as in [29] can be easily implemented, and well suited for parallel computing.", "startOffset": 122, "endOffset": 126}, {"referenceID": 20, "context": "1 Results on Synthetic Data Following [22], we generated low multi-linear rank thirdorder tensors T \u2208R123 , which we used as the ground truth data.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": ") numbers from a uniform distribution in [0, 1], and the entries of Un \u2208 R In\u00d7r are random samples drawn from a uniform distribution in the range [-0.", "startOffset": 41, "endOffset": 47}, {"referenceID": 15, "context": "1 Algorithm Settings We compare our ROID method with the following state-ofthe-art tensor estimation algorithms: 1) WTucker [17]: In the implementation of WTucker, we set R1 = R2 = R3 = \u230a1.", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "3) HaLRTC [22]: The value of the weights \u03b1n is set to be 1/3, n = 1, 2, 3 for solving (3) by using the highly accurate LRTC (HaLRTC) algorithm.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "4) Latent [25]: The regularization parameter \u03bb is set to 10 for solving the latent trace norm minimization (Latent) problem (4).", "startOffset": 10, "endOffset": 14}, {"referenceID": 44, "context": "The main reason is that WTucker and our ROID method are all multiple structured methods similar to the matrix case [50], and need only O(d+ dNI) observations to exactly recover an Nth-order low multilinear rank tensor X with high probability, while O(rI) observations are required for recovering the true tensor by both convex tensor trace norm minimization methods, HaLRTC and Latent, as stated in [27], [28], [51].", "startOffset": 115, "endOffset": 119}, {"referenceID": 24, "context": "The main reason is that WTucker and our ROID method are all multiple structured methods similar to the matrix case [50], and need only O(d+ dNI) observations to exactly recover an Nth-order low multilinear rank tensor X with high probability, while O(rI) observations are required for recovering the true tensor by both convex tensor trace norm minimization methods, HaLRTC and Latent, as stated in [27], [28], [51].", "startOffset": 399, "endOffset": 403}, {"referenceID": 25, "context": "The main reason is that WTucker and our ROID method are all multiple structured methods similar to the matrix case [50], and need only O(d+ dNI) observations to exactly recover an Nth-order low multilinear rank tensor X with high probability, while O(rI) observations are required for recovering the true tensor by both convex tensor trace norm minimization methods, HaLRTC and Latent, as stated in [27], [28], [51].", "startOffset": 405, "endOffset": 409}, {"referenceID": 45, "context": "The main reason is that WTucker and our ROID method are all multiple structured methods similar to the matrix case [50], and need only O(d+ dNI) observations to exactly recover an Nth-order low multilinear rank tensor X with high probability, while O(rI) observations are required for recovering the true tensor by both convex tensor trace norm minimization methods, HaLRTC and Latent, as stated in [27], [28], [51].", "startOffset": 411, "endOffset": 415}, {"referenceID": 22, "context": "As suggested in [25], setting \u03bb \u2192 \u221e gives more accurate solutions for the noiseless problem.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "Algorithms Complexity WCP [8] O(8dI3) WTucker [17] O(8dI3) HaLRTC [22], Latent [25] O(3I4) ROID and SHOOI O(4dI3)", "startOffset": 46, "endOffset": 50}, {"referenceID": 20, "context": "Algorithms Complexity WCP [8] O(8dI3) WTucker [17] O(8dI3) HaLRTC [22], Latent [25] O(3I4) ROID and SHOOI O(4dI3)", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "Algorithms Complexity WCP [8] O(8dI3) WTucker [17] O(8dI3) HaLRTC [22], Latent [25] O(3I4) ROID and SHOOI O(4dI3)", "startOffset": 79, "endOffset": 83}, {"referenceID": 46, "context": "From Table 4, we can see that although WTucker and WCP have the computational complexity similar to our ROIDmethod, they are much slower in practice than ROID due to their PolakRibiere nonlinear conjugate gradient algorithms with a timeconsuming line search scheme [52].", "startOffset": 265, "endOffset": 269}, {"referenceID": 8, "context": "To further evaluate the performances of our method for full tensor decomposition, we compare our ROID method with the low multi-linear rank approximation (LMLRA) method [10], [53] and HOOI [33], [53] on noisy tensors, i.", "startOffset": 169, "endOffset": 173}, {"referenceID": 47, "context": "2 Results on Network Data In this part, we examine our ROID and graph regularized (called GROID) methods on real-world network data sets, such as the YouTube data set [54].", "startOffset": 167, "endOffset": 171}, {"referenceID": 47, "context": "Additional information about the data can be found in [54].", "startOffset": 54, "endOffset": 58}, {"referenceID": 34, "context": "For the graph regularized weighted CP (GWCP) decomposition [40], graph regularized weighted Tucker (GWTucker) decomposition [40], and our ROID and GROID methods, we set the tensor rank R = 45 and the multi-linear rank d1 = d2 = 40 and d3 = 5, and the regularization parameter \u03bb = 100.", "startOffset": 59, "endOffset": 63}, {"referenceID": 34, "context": "For the graph regularized weighted CP (GWCP) decomposition [40], graph regularized weighted Tucker (GWTucker) decomposition [40], and our ROID and GROID methods, we set the tensor rank R = 45 and the multi-linear rank d1 = d2 = 40 and d3 = 5, and the regularization parameter \u03bb = 100.", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "For HaLRTC [22] and our ROID and GROID methods, the weights \u03b1n are set to \u03b11=\u03b12=0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 48, "context": "[55] for link prediction, including the Kinship, Nations and UMLS data sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "The Kinship data set consists of kinship relationships (such as \u201cfather\u201d or \u201cwife\u201d relations) among the members of the ALyawarra tribe in Central Australia [56].", "startOffset": 156, "endOffset": 160}, {"referenceID": 50, "context": "The Nations data set consists of international relations among different countries in the world [57].", "startOffset": 96, "endOffset": 100}, {"referenceID": 51, "context": "[58].", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "mehtod, IRM [55], the hidden variable discovery method, MRC [2] and RESCAL [1] and HaLRTC [22] on these three data sets.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "mehtod, IRM [55], the hidden variable discovery method, MRC [2] and RESCAL [1] and HaLRTC [22] on these three data sets.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "mehtod, IRM [55], the hidden variable discovery method, MRC [2] and RESCAL [1] and HaLRTC [22] on these three data sets.", "startOffset": 75, "endOffset": 78}, {"referenceID": 20, "context": "mehtod, IRM [55], the hidden variable discovery method, MRC [2] and RESCAL [1] and HaLRTC [22] on these three data sets.", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "Then, we use the area under the precision-recall curve (AUC) as the evaluation metric to test the relation prediction performance as in [1], [4].", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "Then, we use the area under the precision-recall curve (AUC) as the evaluation metric to test the relation prediction performance as in [1], [4].", "startOffset": 141, "endOffset": 144}, {"referenceID": 0, "context": "9, from which we can see that similar results as in [1], [55] are obtained.", "startOffset": 52, "endOffset": 55}, {"referenceID": 48, "context": "9, from which we can see that similar results as in [1], [55] are obtained.", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "In [1], it has been shown that WCP and RESCAL are much faster than MRC as well as IRM.", "startOffset": 3, "endOffset": 6}, {"referenceID": 28, "context": "Moreover, our ROID method can be extended to various higher-order tensor recovery and completion problems, such as higher-order robust principal component analysis (RPCA) [31] and robust tensor completion.", "startOffset": 171, "endOffset": 175}], "year": 2016, "abstractText": "Multi-relational learning has received lots of attention from researchers in various research communities. Most existing methods either suffer from superlinear per-iteration cost, or are sensitive to the given ranks. To address both issues, we propose a scalable core tensor trace norm Regularized Orthogonal Iteration Decomposition (ROID) method for full or incomplete tensor analytics, which can be generalized as a graph Laplacian regularized version by using auxiliary information or a sparse higher-order orthogonal iteration (SHOOI) version. We first induce the equivalence relation of the Schatten p-norm (0<p<\u221e) of a low multi-linear rank tensor and its core tensor. Then we achieve a much smaller matrix trace norm minimization problem. Finally, we develop two efficient augmented Lagrange multiplier algorithms to solve our problems with convergence guarantees. Extensive experiments using both real and synthetic datasets, even though with only a few observations, verified both the efficiency and effectiveness of our methods.", "creator": "LaTeX with hyperref package"}}}