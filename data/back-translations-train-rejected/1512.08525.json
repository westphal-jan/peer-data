{"id": "1512.08525", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Dec-2015", "title": "Mining Massive Hierarchical Data Using a Scalable Probabilistic Graphical Model", "abstract": "Probabilistic Graphical Models (PGM) are very useful in the fields of machine learning and data mining. The crucial limitation of those models,however, is the scalability. The Bayesian Network, which is one of the most common PGMs used in machine learning and data mining, demonstrates this limitation when the training data consists of random variables, each of them has a large set of possible values. In the big data era, one would expect new extensions to the existing PGMs to handle the massive amount of data produced these days by computers, sensors and other electronic devices. With hierarchical data - data that is arranged in a treelike structure with several levels - one would expect to see hundreds of thousands or millions of values distributed over even just a small number of levels. When modeling this kind of hierarchical data across large data sets, Bayesian Networks become infeasible for representing the probability distributions. In this paper we introduce an extension to Bayesian Networks to handle massive sets of hierarchical data in a reasonable amount of time and space. The proposed model achieves perfect precision of 1.0 and high recall of 0.93 when it is used as multi-label classifier for the annotation of mass spectrometry data. On another data set of 1.5 billion search logs provided by CareerBuilder.com the model was able to predict latent semantic relationships between search keywords with accuracy up to 0.80.", "histories": [["v1", "Mon, 28 Dec 2015 21:02:20 GMT  (1220kb,D)", "http://arxiv.org/abs/1512.08525v1", "To be submitted to Big Data Journal. arXiv admin note: substantial text overlap witharXiv:1407.5656"]], "COMMENTS": "To be submitted to Big Data Journal. arXiv admin note: substantial text overlap witharXiv:1407.5656", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["khalifeh aljadda", "mohammed korayem", "camilo ortiz", "trey grainger", "john a miller", "khaled rasheed", "krys j kochut", "william s york", "rene ranzinger", "melody porterfield"], "accepted": false, "id": "1512.08525"}, "pdf": {"name": "1512.08525.pdf", "metadata": {"source": "META", "title": "Mining Massive Hierarchical Data Using a Scalable Probabilistic Graphical Model", "authors": ["Khalifeh AlJadda", "Mohammed Korayem", "Camilo Ortiz", "Trey Grainger", "John A. Miller", "Khaled Rasheed", "Krys J. Kochut", "William S. York", "Rene Ranzinger", "Melody Porterfield"], "emails": ["aljadda@uga.edu"], "sections": [{"heading": null, "text": "This year, it has reached the point where it will be able to take the lead in creating a wide range of opportunities to travel the world."}, {"heading": "Background", "text": "Graphic models can be divided into two main categories: (1) directed graphical models (the focus of this work), often referred to as Bayesian networks or belief networks, and (2) undirected graphical models, often referred to as Markov random fields, Markov networks, Boltzmann machines, or loglinear models [12]. Probabilistic graphical models (PGMs) consist of both graph structure and parameters. The graph structure represents a series of conditionally independent relationships for the probability model, while the parameters consist of common probability distributions [1]. Probabilistic graphical models are often considered more convenient than numerical representations for two main reasons: 1 To encode a common probability distribution for P (X1,..., Xn) for n propositional random variables for the mathematical representation of PXS models, we need a table of 2nantum entries. 2 Inadequacy in quantum systems in relation to the concept of the independence of Y systems are used as a problem of the mathematical representation of 2nantum entries."}, {"heading": "Bayesian Network", "text": "A Bayesian network is a concise representation of a large probability distribution that must be handled using traditional techniques such as tables and equations [15]. The graph of a Bayesian network is a directed acyclic graph (DAG) [2]. A Bayesian network consists of two components: a DAG that represents the structure (as shown in Figure 1), and a set of conditional probability tables (CPTs). Each node in a Bayesian network must have a CPT that quantifies the relationship between the variable represented by that node and its parents in the network. Completeness and consistency are guaranteed in a Bayesian network, as there is only one probability distribution that meets the Bayesian network constraints [15]. The constraints that guarantee a unique probability distribution are the numerical constraints represented by that node and its parents in the network, and the constraints that are themselves represented by the structure of independence."}, {"heading": "Related Work", "text": "Our research is closely related to Bayesian network classifiers. In this section, we examine various forms of Bayesian network classifiers to understand how the PGMHD expands BN in a different way from the existing models. We cover the following BN classifiers: 1 Naive Bayes Classifier (NB); 2 Selective Naive Bayes (SNB); 3 Tree Augmented Naive Bayes (TAN); 4 Hidden Naive Bayes (HNB); and how we applied PGMHD to other data mining problems, such as the latent semantic discovery of related search terms in search logs and semantically ambiguous keywords by analyzing users \"search logs."}, {"heading": "Naive Bayes (NB)", "text": "Naive Bayes is the simplest and most common form of BN classifiers. This classifier is based on the assumption that all characteristics are independent of the class. Figure 2 shows an example of NB. An NB classifier is defined as: P (c | x) VP (c) n-j = 1P (xj | c) Where x = (x1,.., xn) P (c) is the previous probability of class c and P (xj | c) is the conditional probability of the attribute / variable xj. The value of c, which maximizes the right side, is chosen. The performance of a Naive Bayes classifier depends on the quality of the predictor characteristics, so that performance is improved if the predictor characteristics are relevant and not redundant."}, {"heading": "Selective Naive Bayes (SNB)", "text": "In order to improve the performance of the BN classifier by selecting the predictive features that are relevant and non-redundant, Selective Naive Bayes (SNB) [16] is proposed as a feature subset selection problem. Let's define xF as the projection of x on a selected feature subset F-1, 2,..., n.}. The classification equation becomes C A1 A2 A3 Figure 3 Tree Augmented Naive BayesP (c | x) \u0445P (c | xF) = P (c)."}, {"heading": "Tree Augmented Naive Bayes (TAN)", "text": "This form of Bayesian network classifier extends the NB by allowing each attribute to have at most one attribute parent in addition to its class parent. This extension tends to represent the fact that in some cases there is dependency or influence between attributes in such a way that the value of a attribute xj depends on a value of attribute y. TAN classifier is defined as P (c | x) = P (c) n-j = 1P (xj | pxj, c), where pxj is the attribute parent of xj. Figure 3 shows an example of TAN."}, {"heading": "Hidden Naive Bayes (HNB)", "text": "HNB (Figure 4) is another extension of NB. In this extension, each attribute Ai is given a hidden Ahpi to integrate the influences of all other attributes. The definition of the HNB classifier is as follows: P (c | x) = P (c) n-j = 1P (xj | hpj, c), where P (xj | xhpj, c) = n-j = i = 1, i6 = jwji \u043a P (xj | xi, c)"}, {"heading": "Probabilistic Graphical model for Massive Hierarchical Data Problems (PGMHD)", "text": "In this section we will describe PGMHD. We will discuss the structure of the model, its learning algorithm and how it elongates BN.C A1 A2 A3 Ahp1 Ahp2 Ahp3 Figure 4 Hidden Naive Bayes."}, {"heading": "Model Structure", "text": "Consider a multi-level directed diagram G = (V, A), in which V > V \u00b7 V denotes the set of nodes and arcs v = in each case in such a way that: 1 V is divided into m levels L0,..., Lm \u2212 1 so that V = m \u2212 1i = 0 Li, andLi Lj = \u2205 for i 6 = j. 2 The arcs in A connect only one level to the next, i.e. if one level A then one level Li \u2212 1 \u00b7 Li for some i = 1,...., m \u2212 3. An arc a = (vi \u2212 1, vi), Li \u2212 1 \u00b7 Li represents the dependence on vi with one level vi \u2212 1, i = 1,.., m \u2212 1. In addition, we let pa: P (V) be the function that maps each node to its parents, i.e. pa (v) nodes model Lxt = {w, v)."}, {"heading": "Probabilistic-based Classification", "text": "Considering a result at level i (1,.., m \u2212 1), namely v-Li, we calculate the classification value Cli (w | v) from v to the parent result w-Li \u2212 1 by calculating the conditional probability P (Xi \u2212 1 = w-Xi = v) as follows: = f (w | v) In (v) = (f (w, v) Out (w))) \u00b7 (Out (w) t) (In (v) t) = forP (Xi = v-Xi \u2212 1 = w) \u00b7 P (Xi \u2212 1 = w) P (Xi = v) = P (Xi \u2212 1 = w-Xi = v), where In (v): = In (v): = u-pa (v) f (u, v), v-V, and Out (w): =."}, {"heading": "Probabilistic-based Similarity scoring", "text": "Fix a plane i,.., m \u2212 1}, and let X, Y, L0 \u00b7 \u00b7 \u00b7 Lm \u2212 1 be identically distributed random variables as in (1). We define the probability-based similarity value CO (co-occurrence) between two results xi, yi, and Li by calculating the conditional common probability CO (xi, yi): = P (Xi = xi, Yi = yi | Xi \u2212 1% pa (xi), pa (yi), pa (yi), pa (yi), pa (yi), Yi \u2212 1% pa (xi)) asCO (xi, yi) = 1% pa (xi), pa (yi) pi (w, xi), pa (yi), pa (yi), pa (yi), pa (yi) pi (w, yi), pa (yi) pi (v, yi), pi (w, yi), where pi (w, v) = P (Xi \u2212 1 = v) for each (w, v) Li \u2212 1% of the probability Li (yi), Li (yi), Li (yi), pa (yi), pa (w, xi), pa (yi), pa (yi), pa (yi), pa (yi), pa (yi), pa (yi), pa (yi), pa (yi), pa (yi), pa (yi), pa (yi), pa (yi), pa (yi), pa (yi), pa (yi), pa (yi) pi) pi (yi) pi (w, pi (w, v, v, v, v, v, v, v (v), v (v, v, v, v), v (v, v, v (v), v (v, v), v (v, v, v, v, v, v), v (), v (), v (), v (v (), v, v (), v, v (), v (), v (), v (), v"}, {"heading": "Progressive Learning", "text": "Progressive learning is a learning method that allows a model to learn gradually over a longer period of time. Training data does not need to be given to the model at once. Instead, the model can learn from all available data and gradually integrate the new knowledge. This learning method is very attractive in the big data age for the following reasons: 1 Model training does not require the processing of all data in advance 2 It is easy to learn from new data without having to re-integrate the previous data into learning. 3 The training unit can be distributed rather than conducted in a long-term session. 4 It supports recursive learning, which allows the results of the model to be used as new training data, provided they are considered accurate by the user."}, {"heading": "PGMHD an extension to NB", "text": "PGMHD expands NB in different directions to improve its scalability and ability to deal with massive hierarchical data as follows: 1 It allows multi-level classification. 2 It allows multi-level representation of predictive characteristics. 3 It allows lazy classification. The first dimension of the expansion of PGMHD is multi-level classification. Our model allows more than one class to be located at the root level of the classifier, with each instance being able to be classified to more than one class. The second dimension of this expansion is the multi-level classification, which allows the classifier to represent the predictive characteristics in m levels rather than in the normal NB. This extension allows hierarchical modeling to obtain the structure of the data, which, as our experiments have shown, is important for improving the quality of the classification. The last dimension of this extension is the lazy classification against the eager NB. PGMHD is considered lazier as the calculation of the new instance is performed throughout the process."}, {"heading": "Experiments and Results", "text": "This year, it is less than a year since such a process took place."}, {"heading": "Semantically Related Keywords in Search Logs", "text": "Semantic similarity is a metric defined by documents or terms in which the distance between them reflects the similarity of their meaning [30], and it is widely used in Natural Language Processing (NLP) and Information Retrieval (IR). Generally, there are two major techniques used to calculate semantic similarity: one is calculating a knowledge-based approach [32], and 50 20 2040 50 5010 5 15 F1 F2F4 F6F7 F8 F1015Figure 9 PGMHD, which represents MS annotations. Root nodes are the tips at the MS1 level, while levels 2 and 3 nodes are the glycical fragments that indicate the tips at the MS2 and MS3 levels."}, {"heading": "Motivation", "text": "We want to create a language-independent algorithm for modeling semantic relationships between search terms that delivers results in a format that people can understand. It is important that the searcher can be supported by an advanced query without creating a black box system in which that person is unable to understand and customize the query extension. CareerBuilder [1] operates job boards [1] http: / / www.careerbuilder.com / in many countries and receives tens of millions of searches every day. Given the vast amount of search data in our protocols, we want to uncover the latent semantic relationships between search terms and phrases for different region-specific websites using a novel technique that avoids the use of natural language processing (NLP). We want to avoid NLP to make it possible to apply the same technique to different websites that support many languages without changing the algorithms or the libraries per language."}, {"heading": "Probabilistic Semantic Similarity Scoring using PGMHD", "text": "We applied the proposed PGMHD model to determine the semantically related search terms by measuring the likely semantic similarity between these search terms. Considering the search logs for all users and the classifications of users, as shown in Table 1, PGMHD can represent this type of data by placing the user classes as root nodes and the search terms for all users in the second level as child nodes. Afterwards, an edge is formed that connects each search term with the class of the user who was looking for it. The frequency of each search term (how many users are looking for it) is stored in the node of that term, while the frequency of a specific search term sought by users of a particular class (how many users from that class were looking for the given term) is stored in the edge between the class and the term. The frequency of the root node is the frequency of the frequency of the frequencies of the children connecting these nodes to the edges 15."}, {"heading": "Distributed PGMHD", "text": "To process 1.6 billion search logs (each search log contains one or more intricate keywords used by a user to search for jobs on careerbuilder.com) provided by Careerbuilder in a reasonable amount of time, we designed a distributed PGMHD with Hadoop HDFS [37], Hadoop Map / Reduce [38] and Hive [39]. The design of the distributed PGMHD is shown in Figure 16. Basically, we use Hive to store the intermediate data while we build and train PGMHD. Once it is trained, we can then execute our requests to obtain an ordered list of semantically related keywords for a particular term (s).0.0.1 Experiment Setup and Results The experiment, which performs latent semantic discoveries using PGMHD, was executed on a hadoop cluster of 69 data nodes, each of which has a 2.6 GHz AMD Processor with 12 to 8 kernel size from 32 GB to 32 GB."}, {"heading": "Discovering Semantic Ambiguity of a Keyword", "text": "The semantic ambiguity of a keyword can be defined as the probability of seeing different meanings of the same keyword in different contexts [40, 41]. Techniques mentioned in the literature focus on the use of ontologies and dictionaries such as WordPress as described in [40, 41] These solutions are not applicable if the keywords are from an area such as job search. In the job search area, the keywords used are typically job titles, skills, company names, etc., which are not normal English keywords. For example, Java can mean a programming language, just like coffee, but an English dictionary would not provide these two meanings. PGMHD is successfully applied to detect the semantic ambiguity of a keyword. Approximately 1.6 billion search logs used in this experiment are search terms extracted from these 1.6 billion protocols to train PGMHD, which were then used to publish the normalized PMI for each term."}, {"heading": "Conclusions and Future Work", "text": "The most important questions and answers to this topic are: \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"What is this?,\" \"\" What is this?, \"\" \"What is this?,\" \"What is this?,\" \"What is this?,\" what is this?, \"what is this?,\" what is this?, \"what is this?,\" what is this?, \"what is this?,\" what is this?, \"what is this?,\" what is this?, \"what is this?,\" what is this?, \"what is this?,\" what is this?, \"what is this?,\" what is this?, \"what is this?,\" what is this?, \"what is that?,\" what is this?, \"what is this?,\" what is this?, \"what is this?,\" what is this?, \"what is that?,\" what is this?, \"what is this?,\" what is this?, \"what is this?,\" what is this?, \"what is\" what is this?, \"what is this?,\" what is this?, \"what is this?,\" what is \"what is\" what is this?, \"what is\" what is this?, \"what is this?,\" what is this?, \"what is this?,\" what is \"what is\" what is \"what is\" what is \"what is this,\" what is \"what is\" what is \"what is this,\" what is \"what is\" what is \"what is this,\" what is \"what is\" what is \"what is\" \"what is\" what is \"what is\" \"what is\" what is this, \"what is\" \"what is\" what is \"what is\" what is \"what is\" \"\" what is \""}], "references": [{"title": "Belief networks, hidden markov models, and markov random fields: A unifying view", "author": ["P. Smyth"], "venue": "Pattern recognition letters 18(11), 1261\u20131268", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "An overview of bayesian inference and graphical models", "author": ["T. Hamelryck"], "venue": "Bayesian Methods in Structural Bioinformatics, pp. 3\u201348. Springer, ???", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimizing hidden markov models using genetic algorithms and artificial immune systems", "author": ["M. Korayem", "A. Badr", "I. Farag"], "venue": "Computing and Information Systems 11(2), 44", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Rna sequence analysis using covariance models", "author": ["S.R. Eddy", "R. Durbin"], "venue": "Nucleic acids research 22(11), 2079\u20132088", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Protein homology detection by hmm\u2013hmm comparison", "author": ["J. S\u00f6ding"], "venue": "Bioinformatics 21(7), 951\u2013960", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "A bayesian mixture model for part-of-speech induction using multiple features", "author": ["C. Christodoulopoulos", "S. Goldwater", "M. Steedman"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 638\u2013647", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust part-of-speech tagging using a hidden markov model", "author": ["J. Kupiec"], "venue": "Computer Speech & Language 6(3), 225\u2013242", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Hierarchical bayesian networks: an approach to classification and learning for structured data", "author": ["E. Gyftodimos", "P.A. Flach"], "venue": "Methods and Applications of Artificial Intelligence, pp. 291\u2013300. Springer, ???", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "The hierarchical hidden markov model: Analysis and applications", "author": ["S. Fine", "Y. Singer", "N. Tishby"], "venue": "Machine learning 32(1), 41\u201362", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Modular ontology design using canonical building blocks in the biochemistry domain", "author": ["C.J. Thomas", "A.P. Sheth", "W.S. York"], "venue": "Frontiers in Artificial Intelligence and Applications 150, 115", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Glycoworkbench: a tool for the computer-assisted annotation of mass spectra of glycans\u00e2\u0102\u0103", "author": ["A. Ceroni", "K. Maass", "H. Geyer", "R. Geyer", "A. Dell", "S.M. Haslam"], "venue": "Journal of proteome research 7(4), 1650\u20131659", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Graphical models", "author": ["Jordan", "M.I"], "venue": "Statistical Science 19(1), 140\u2013155", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Markov and Bayes Networks: a Comparison of Two Graphical Representations of Probabilistic Knowledge", "author": ["J. Pearl"], "venue": "Computer Science Department, University of California, ???", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning bayesian network structure from massive datasets: the \u00absparse candidate \u00abalgorithm", "author": ["N. Friedman", "I. Nachman", "D. Pe\u00e9r"], "venue": "Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, pp. 206\u2013215", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "Bayesian networks", "author": ["A. Darwiche"], "venue": "Communications of the ACM 53(12), 80\u201390", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Discrete bayesian network classifiers: A survey", "author": ["C. Bielza", "P. Larra\u00f1aga"], "venue": "ACM Computing Surveys (CSUR) 47(1), 5", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to bioinformatics for glycomics research", "author": ["K.F. Aoki-Kinoshita"], "venue": "PLoS computational biology 4(5), 1000075", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Glycomics: an integrated systems approach to structure-function relationships of glycans", "author": ["R. Raman", "S. Raguram", "G. Venkataraman", "J.C. Paulson", "R. Sasisekharan"], "venue": "Nature Methods 2(11), 817\u2013824", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Challenges in computational analysis of mass spectrometry data for proteomics", "author": ["B. Ma"], "venue": "Journal of Computer Science and Technology 25(1), 107\u2013123", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "An empirical study of the naive bayes classifier", "author": ["I. Rish"], "venue": "IJCAI 2001 Workshop on Empirical Methods in Artificial Intelligence, vol. 3, pp. 41\u201346", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning 20(3), 273\u2013297", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Simplifying decision trees", "author": ["J.R. Quinlan"], "venue": "International journal of man-machine studies 27(3), 221\u2013234", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1987}, {"title": "An introduction to kernel and nearest-neighbor nonparametric regression", "author": ["N.S. Altman"], "venue": "The American Statistician 46(3), 175\u2013185", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1992}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. Werbos"], "venue": "Neural Networks 1(4), 339\u2013356", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1988}, {"title": "A theory of networks for approximation and learning", "author": ["T. Poggio", "F. Girosi"], "venue": "Technical report, DTIC Document", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1989}, {"title": "Bayesian netwcrks: A model cf \u00e2\u0102\u0178self-activated memory for evidential reasoning", "author": ["J. Pearl"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1985}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD explorations newsletter 11(1), 10\u201318", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Mulan: A java library for multi-label learning", "author": ["G. Tsoumakas", "E. Spyromitros-Xioufis", "J. Vilcek", "I. Vlahavas"], "venue": "The Journal of Machine Learning Research 12, 2411\u20132414", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Scaling up the accuracy of bayesian network classifiers by m-estimate", "author": ["L. Jiang", "D. Wang", "Z. Cai"], "venue": "Advanced Intelligent Computing Theories and Applications. With Aspects of Artificial Intelligence, pp. 475\u2013484. Springer, ???", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Semantic measures for the comparison of units of language, concepts or entities from text and knowledge base analysis", "author": ["S. Harispe", "S. Ranwez", "S. Janaqi", "J. Montmain"], "venue": "arXiv preprint arXiv:1310.1285", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Corpus-based and knowledge-based measures of text semantic similarity", "author": ["R. Mihalcea", "C. Corley", "C. Strapparava"], "venue": "AAAI, vol. 6, pp. 775\u2013780", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Semantic distance in wordnet: An experimental, application-oriented evaluation of five measures", "author": ["A. Budanitsky", "G. Hirst"], "venue": "Workshop on WordNet and Other Lexical Resources, vol. 2", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2001}, {"title": "Normalized (pointwise) mutual information in collocation extraction", "author": ["G. Bouma"], "venue": "Proceedings of the Biennial GSCL Conference, pp. 31\u201340", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Latent semantic analysis", "author": ["S.T. Dumais"], "venue": "Annual review of information science and technology 38(1), 188\u2013230", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "Mining the web for synonyms: PMI-IR versus lsa on toefl", "author": ["P.D. Turney"], "venue": "Proceedings of the 12th European Conference on Machine Learning. EMCL \u201901, pp. 491\u2013502. Springer, London, UK, UK", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2001}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "The hadoop distributed file system", "author": ["K. Shvachko", "H. Kuang", "S. Radia", "R. Chansler"], "venue": "Mass Storage Systems and Technologies (MSST), 2010 IEEE 26th Symposium On, pp. 1\u201310", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM 51(1), 107\u2013113", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Hive: a warehousing solution over a map-reduce framework", "author": ["A. Thusoo", "J.S. Sarma", "N. Jain", "Z. Shao", "P. Chakka", "S. Anthony", "H. Liu", "P. Wyckoff", "R. Murthy"], "venue": "Proceedings of the VLDB Endowment 2(2), 1626\u20131629", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Solving problem of ambiguity terms using ontology", "author": ["H. Jayadianti", "L.E. Nugroho", "C.S. Pinto", "P.I. Santosa", "W. Widayat"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Solving semantic ambiguity to improve semantic web based ontology matching", "author": ["J. Gracia", "V. Lopez", "M. d\u2019Aquin", "M. Sabou", "E. Motta", "E. Mena"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Introduction Probabilistic graphical models (PGM) consist of a structural model and a set of conditional probabilities [1, 2].", "startOffset": 119, "endOffset": 125}, {"referenceID": 1, "context": "Introduction Probabilistic graphical models (PGM) consist of a structural model and a set of conditional probabilities [1, 2].", "startOffset": 119, "endOffset": 125}, {"referenceID": 2, "context": "They are widely used in machine learning and data mining techniques, like classification, speech recognition [3], bioinformatics [4, 5], Natural Language Processing (NLP) [6, 7], etc.", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "They are widely used in machine learning and data mining techniques, like classification, speech recognition [3], bioinformatics [4, 5], Natural Language Processing (NLP) [6, 7], etc.", "startOffset": 129, "endOffset": 135}, {"referenceID": 4, "context": "They are widely used in machine learning and data mining techniques, like classification, speech recognition [3], bioinformatics [4, 5], Natural Language Processing (NLP) [6, 7], etc.", "startOffset": 129, "endOffset": 135}, {"referenceID": 5, "context": "They are widely used in machine learning and data mining techniques, like classification, speech recognition [3], bioinformatics [4, 5], Natural Language Processing (NLP) [6, 7], etc.", "startOffset": 171, "endOffset": 177}, {"referenceID": 6, "context": "They are widely used in machine learning and data mining techniques, like classification, speech recognition [3], bioinformatics [4, 5], Natural Language Processing (NLP) [6, 7], etc.", "startOffset": 171, "endOffset": 177}, {"referenceID": 7, "context": "One extension is offered by the hierarchical probabilistic graphical models (HPGM) which aim to extend the PGM to work with more structured domains [8, 9].", "startOffset": 148, "endOffset": 154}, {"referenceID": 8, "context": "One extension is offered by the hierarchical probabilistic graphical models (HPGM) which aim to extend the PGM to work with more structured domains [8, 9].", "startOffset": 148, "endOffset": 154}, {"referenceID": 9, "context": "Hence, the first phase of building Bayesian Network to find the optimal structure is not applicable For example, consider the glycan ontology \"GlycO\" [10] which describes 1300 glycan structures (see section ) whose theoretical tandem mass spectra (MS) can be predicted by GlycoWorkbench [11].", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "Hence, the first phase of building Bayesian Network to find the optimal structure is not applicable For example, consider the glycan ontology \"GlycO\" [10] which describes 1300 glycan structures (see section ) whose theoretical tandem mass spectra (MS) can be predicted by GlycoWorkbench [11].", "startOffset": 287, "endOffset": 291}, {"referenceID": 11, "context": "Background Graphical models can be classified into two major categories: (1) directed graphical models (the focus of this paper), which are often referred to as Bayesian Networks, or belief networks, and (2) undirected graphical models which are often referred to as Markov Random Fields, Markov networks, Boltzmann machines, or log-linear models [12].", "startOffset": 347, "endOffset": 351}, {"referenceID": 0, "context": "The graph structure represents a set of conditionally independent relations for the probability model, while the parameters consist of the joint probability distributions [1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 12, "context": "Probabilistic graphical models are often considered to be more convenient than numerical representations for two main reasons [13]: 1 To encode a joint probability distribution for P(X1,.", "startOffset": 126, "endOffset": 130}, {"referenceID": 2, "context": "For example, Hidden Markov Models (HMM) are considered a crucial component for most of the speech recognition systems [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "In bioinformatics, probabilistic graphical models are used in RNA sequence analysis [4].", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "In natural language processing (NLP), HMM and Bayesian models are used for part of speech (POS) tagging [6].", "startOffset": 104, "endOffset": 107}, {"referenceID": 13, "context": "In general, finding a network that maximizes the Bayesian score which maximizes the posterior probability and Minimum Description Length (MDL) score which gives preference to a simple BN over a complex one, is an NP-hard problem [14].", "startOffset": 229, "endOffset": 233}, {"referenceID": 14, "context": "A Bayesian Network is a concise representation of a large probability distribution to be handled using traditional techniques such as tables and equations [15].", "startOffset": 155, "endOffset": 159}, {"referenceID": 1, "context": "The graph of a Bayesian Network is a directed acyclic graph (DAG) [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 14, "context": "ity distribution that satisfies the Bayesian Network constraints [15].", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "Bayesian Networks are widely used for modeling causality in a formal way, for decision-making under uncertainty, and for many other applications [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 15, "context": "In order to improve the performance of BN classifier by selecting the predictive features that are relevant and not redundant, Selective Naive Bayes (SNB) [16] is proposed as a feature subset selection problem.", "startOffset": 155, "endOffset": 159}, {"referenceID": 16, "context": "Experiments and Results Glycans (Figure 6) are the third major class of biological macro-molecules besides nucleic acids and proteins [17].", "startOffset": 134, "endOffset": 138}, {"referenceID": 16, "context": "acterize and study glycans, as defined in [17] or an integrated systems approach to study structure-function relationships of glycans as defined in [18].", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": "acterize and study glycans, as defined in [17] or an integrated systems approach to study structure-function relationships of glycans as defined in [18].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "Mass spectrometry (MS) is an analytical technique used to identify the composition of a sample [19].", "startOffset": 95, "endOffset": 99}, {"referenceID": 19, "context": "We trained PGMHD and compared it against leading classifiers including Naive Bayes [20], SVM [21], Decision Tree [22], K-NN [23], Neural Network [24], Radial Basis Function network (RBF Network) [25] and Bayesian Network [26] from Weka [27].", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "We trained PGMHD and compared it against leading classifiers including Naive Bayes [20], SVM [21], Decision Tree [22], K-NN [23], Neural Network [24], Radial Basis Function network (RBF Network) [25] and Bayesian Network [26] from Weka [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "We trained PGMHD and compared it against leading classifiers including Naive Bayes [20], SVM [21], Decision Tree [22], K-NN [23], Neural Network [24], Radial Basis Function network (RBF Network) [25] and Bayesian Network [26] from Weka [27].", "startOffset": 113, "endOffset": 117}, {"referenceID": 22, "context": "We trained PGMHD and compared it against leading classifiers including Naive Bayes [20], SVM [21], Decision Tree [22], K-NN [23], Neural Network [24], Radial Basis Function network (RBF Network) [25] and Bayesian Network [26] from Weka [27].", "startOffset": 124, "endOffset": 128}, {"referenceID": 23, "context": "We trained PGMHD and compared it against leading classifiers including Naive Bayes [20], SVM [21], Decision Tree [22], K-NN [23], Neural Network [24], Radial Basis Function network (RBF Network) [25] and Bayesian Network [26] from Weka [27].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": "We trained PGMHD and compared it against leading classifiers including Naive Bayes [20], SVM [21], Decision Tree [22], K-NN [23], Neural Network [24], Radial Basis Function network (RBF Network) [25] and Bayesian Network [26] from Weka [27].", "startOffset": 195, "endOffset": 199}, {"referenceID": 25, "context": "We trained PGMHD and compared it against leading classifiers including Naive Bayes [20], SVM [21], Decision Tree [22], K-NN [23], Neural Network [24], Radial Basis Function network (RBF Network) [25] and Bayesian Network [26] from Weka [27].", "startOffset": 221, "endOffset": 225}, {"referenceID": 26, "context": "We trained PGMHD and compared it against leading classifiers including Naive Bayes [20], SVM [21], Decision Tree [22], K-NN [23], Neural Network [24], Radial Basis Function network (RBF Network) [25] and Bayesian Network [26] from Weka [27].", "startOffset": 236, "endOffset": 240}, {"referenceID": 27, "context": "We also used Mulan [28], which is a Java library that extends Weka classifiers to handle multi-label classification problems.", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": "Also, we applied the m-estimate as a probability estimation technique To help PGMHD overcome the common problem for any Bayesian model which is the zero-frequency problem [29].", "startOffset": 171, "endOffset": 175}, {"referenceID": 29, "context": "Semantically Related Keywords in Search Logs Semantic similarity is a metric that is defined over documents or terms in which the distance between them reflects the likeness of their meaning [30], and it is widely used in Natural Language Processing (NLP) and Information Retrieval (IR) [31].", "startOffset": 191, "endOffset": 195}, {"referenceID": 30, "context": "Semantically Related Keywords in Search Logs Semantic similarity is a metric that is defined over documents or terms in which the distance between them reflects the likeness of their meaning [30], and it is widely used in Natural Language Processing (NLP) and Information Retrieval (IR) [31].", "startOffset": 287, "endOffset": 291}, {"referenceID": 31, "context": "Generally, there are two major techniques used to compute semantic similarity: one is computed using a semantic network (Knowledge-based approach) [32], and", "startOffset": 147, "endOffset": 151}, {"referenceID": 30, "context": "the other is based on computing the relatedness of terms within a large corpus of text (corpus-based approach) [31].", "startOffset": 111, "endOffset": 115}, {"referenceID": 32, "context": "The major techniques classified as corpusbased approaches are Pointwise Mutual Information (PMI) [33] and Latent Semantic Analysis (LSA) [34], though PMI outperforms LSA on mining the web for synonyms [35].", "startOffset": 97, "endOffset": 101}, {"referenceID": 33, "context": "The major techniques classified as corpusbased approaches are Pointwise Mutual Information (PMI) [33] and Latent Semantic Analysis (LSA) [34], though PMI outperforms LSA on mining the web for synonyms [35].", "startOffset": 137, "endOffset": 141}, {"referenceID": 34, "context": "The major techniques classified as corpusbased approaches are Pointwise Mutual Information (PMI) [33] and Latent Semantic Analysis (LSA) [34], though PMI outperforms LSA on mining the web for synonyms [35].", "startOffset": 201, "endOffset": 205}, {"referenceID": 35, "context": "A group of Google researchers proposed two efficient models which can discover semantic word similarities [36].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "CareerBuilder[1] operates job boards", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "[1]http://www.", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "com) provided by Careerbuilder in reasonable time, we designed a distributed PGMHD using Hadoop HDFS [37], Hadoop Map/Reduce [38] and Hive [39].", "startOffset": 101, "endOffset": 105}, {"referenceID": 37, "context": "com) provided by Careerbuilder in reasonable time, we designed a distributed PGMHD using Hadoop HDFS [37], Hadoop Map/Reduce [38] and Hive [39].", "startOffset": 125, "endOffset": 129}, {"referenceID": 38, "context": "com) provided by Careerbuilder in reasonable time, we designed a distributed PGMHD using Hadoop HDFS [37], Hadoop Map/Reduce [38] and Hive [39].", "startOffset": 139, "endOffset": 143}, {"referenceID": 39, "context": "Discovering Semantic Ambiguity of a Keyword The semantic ambiguity of a keyword can be defined as the likelihood of seeing different meanings of the same keyword in different contexts [40, 41].", "startOffset": 184, "endOffset": 192}, {"referenceID": 40, "context": "Discovering Semantic Ambiguity of a Keyword The semantic ambiguity of a keyword can be defined as the likelihood of seeing different meanings of the same keyword in different contexts [40, 41].", "startOffset": 184, "endOffset": 192}, {"referenceID": 39, "context": "The techniques mentioned in the literature focuses on utilization of ontologies and dictionaries like Wordnet as described in [40, 41].", "startOffset": 126, "endOffset": 134}, {"referenceID": 40, "context": "The techniques mentioned in the literature focuses on utilization of ontologies and dictionaries like Wordnet as described in [40, 41].", "startOffset": 126, "endOffset": 134}], "year": 2015, "abstractText": "*Correspondence: aljadda@uga.edu 1Department of Computer Science, University of Georgia, Athens,GA, USA Full list of author information is available at the end of the article \u2020Equal contributor Abstract Probabilistic Graphical Models (PGM) are very useful in the fields of machine learning and data mining. The crucial limitation of those models,however, is the scalability. The Bayesian Network, which is one of the most common PGMs used in machine learning and data mining, demonstrates this limitation when the training data consists of random variables, each of them has a large set of possible values. In the big data era, one would expect new extensions to the existing PGMs to handle the massive amount of data produced these days by computers, sensors and other electronic devices. With hierarchical data data that is arranged in a treelike structure with several levels one would expect to see hundreds of thousands or millions of values distributed over even just a small number of levels. When modeling this kind of hierarchical data across large data sets, Bayesian Networks become infeasible for representing the probability distributions. In this paper we introduce an extension to Bayesian Networks to handle massive sets of hierarchical data in a reasonable amount of time and space. The proposed model achieves perfect precision of 1.0 and high recall of 0.93 when it is used as multi-label classifier for the annotation of mass spectrometry data. On another data set of 1.5 billion search logs provided by CareerBuilder.com the model was able to predict latent semantic relationships between search keywords with accuracy up to 0.80.", "creator": "LaTeX with hyperref package"}}}