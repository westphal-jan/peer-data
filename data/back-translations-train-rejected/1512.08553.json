{"id": "1512.08553", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Dec-2015", "title": "Conditional probability generation methods for high reliability effects-based decision making", "abstract": "Decision making is often based on Bayesian networks. The building blocks for Bayesian networks are its conditional probability tables (CPTs). These tables are obtained by parameter estimation methods, or they are elicited from subject matter experts (SME). Some of these knowledge representations are insufficient approximations. Using knowledge fusion of cause and effect observations lead to better predictive decisions. We propose three new methods to generate CPTs, which even work when only soft evidence is provided. The first two are novel ways of mapping conditional expectations to the probability space. The third is a column extraction method, which obtains CPTs from nonlinear functions such as the multinomial logistic regression. Case studies on military effects and burnt forest desertification have demonstrated that so derived CPTs have highly reliable predictive power, including superiority over the CPTs obtained from SMEs. In this context, new quality measures for determining the goodness of a CPT and for comparing CPTs with each other have been introduced. The predictive power and enhanced reliability of decision making based on the novel CPT generation methods presented in this paper have been confirmed and validated within the context of the case studies.", "histories": [["v1", "Mon, 28 Dec 2015 23:08:30 GMT  (44kb)", "http://arxiv.org/abs/1512.08553v1", "18 pages, 3 figures"]], "COMMENTS": "18 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["wolfgang garn", "panos louvieris"], "accepted": false, "id": "1512.08553"}, "pdf": {"name": "1512.08553.pdf", "metadata": {"source": "CRF", "title": "Conditional probability generation methods for high reliability effects-based decision making", "authors": ["Wolfgang Garna", "Panos Louvierisb"], "emails": ["w.garn@surrey.ac.uk", "panos.louvieris@brunel.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.08 553v 1 [cs.A I] 2 8D ec2 015Decision-making is often based on Bayesian networks, and the building blocks for Bayesian networks are their conditional probability tables (CPTs), which are obtained by parameter estimation methods or by subject matter experts (SME), some of which are insufficient approximations, and the third is a column extraction method that obtains CPTs from non-linear functions such as multinomic logistic regression. Case studies of military effects and burnt forest deserts have shown that such derived CPTs have highly reliable predictive power, including the superiority of CPTs over the CPTs systems derived from this context, and the reliability of SME methods."}, {"heading": "1. Introduction", "text": "The first stage is the derivation of a representative graph (1988), Mengshoel et al. (2006) and many other basic work in this field are often confused by the inability of the decision-making model to capture the required diversity of the situation in a frugal manner. Novel techniques addressing this skill gap are presented in this paper along with their accompanying case studies, which are considered application and evaluation methods.The second stage is the application and evaluation method used in practice.The first stage is the derivation of a representative graph (1988), Mengshoel et al al al. (2006) and many others have established themselves in the literature as a useful tool for arguing under uncertainty. Normally, there are two stages involved in the creation of a Bayesian network. The first stage is the derivation of a representative graph (topology). Pearl (1988), Mengshoel et al al al."}, {"heading": "2. Bayesian Network", "text": "In this section, we will present Bayesian network topics that are essential for creating conditional probability tables, and demonstrate the equivalence between two specific Bayesian network structures."}, {"heading": "2.1. The Basics", "text": "The fundamental difference of a Bayesian network from other network structures is the use of random variables (conditional probabilities) as \"weights\" for the nodes. The conditional probability P (Z | X) of the event Z that occurs is first defined by P (Z = z | X = x): = P (Z = z, X = x) / P (X = x). We will refer this to a simple Bayesian network with two nodes: Effect (child) Z and Cause (parent) X as shown in Figure 1 (a). We will introduce a vector matrix notation for the probabilities that are necessary because the probability / statistical literature tends to mix individual values and matrix notation. The causation probabilities are P (X) (which is a n \u00b7 1 vector) and the probabilities of effect are P (Z)."}, {"heading": "2.2. Cause Computation", "text": "In view of the effects and conditional probabilities, it is possible to determine the causes based on the Bayes rules: P (Z = z | X = x) P (X = x) = P (X = x | Z = z) P (Z = z). (4) Bayes rule gives P (X = x | Z = z) = P (X = x, Z = z) / P (Z = z)), which reverses the effect calculation and leads to a conditional probability matrix. Let's incorporate this into the matrix formulation: x = x, z \u00f7 (emz) \u21d4 P (X) = P (X, Z) \u00f7 (emP (Z) \u2032)), (5) where the vector consists of m-one and \u00f7 is the elementary division operation. Thus, we have a method for determining the cause, which determines probabilities based on the effects. This allows us to derive the causes based on the effects. Note that z = Cx (CC \u2032 probability \u2212 C = space cannot be used, because x = space."}, {"heading": "2.3. Converging Network", "text": "Definition 2.1. A Bayesian network B is a simple directed acyclic graph = > meshes consisting of a sequence of nodes N with an associated sequence of weights W and a sequence of arcs. Weights in a discrete Bayesian network are conditional probability tables or hybrid Bayesian meshes. Note that a probability vector can only be interpreted as a special case of a conditional probability table and is simply referred to as BN.Definition 2.2. Let B1 be a Bayesian network (BN) illustrated in Figure 1 (b) and defined by: N: = < Z, X2, X2, XK, XK >, W: = < P (Z | X1, XK)."}, {"heading": "2.4. Combine Operator", "text": "Important in the above proof was the use of algorithm 1, which combines several independent probability vectors into a single one; we define this as the combination operator: [0, 1] n1 \u00b7 \u00b7 \u00b7 \u00b7 [0, 1] nk \u2192 [0, 1] n1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 nk. Note: Algorithm 1 Multiple parents to a single parent. (Operator) Required: x1,..., xn.... parallel causes (column vectors) Ensure: x.... Parallel cause nodes 1: x: = x1 assigns first cause 2: for k = 2 to n (for all remaining parallel causes) do 3: Y: = (xx \u2032 k) \"Calculate external product and Transpose4: x: = Y: Transform the matrix into a single column vector by stringing together all columns 5: End formats that algorithm 1 has a repeated application of the external product and a special transformation is possible on any specific case."}, {"heading": "2.5. Evidence", "text": "Evidence can be given as a result of probability ratios. For example, it is assumed that the odds are 7 to 5, which is converted to the probability vector (.58.42).This type of evidence is referred to as soft proof. It is also common in the BN literature to express evidence by stating that a node has assumed a certain state, which we will call hard proof. This is equivalent to a 100% probability in a state. We will call such a node an evidence node. A converging network in which all parents have hard evidence for selecting exactly one column from the CPT Proposition 2.2. The probabilities of action are identical to a unique column of the CPT if all parents are evidence nodes (i.e. each parent has exactly one 100% state) in a convergent subnet. This proposition is significant for the relationship between multinomial logistic regression and CPTs, which will be discussed later. Of course, a combination of multiple assay nodes and a combination of assay nodes is also possible."}, {"heading": "2.6. Joint Probability using the Combine Operator", "text": "In general, the common probability of a convergent network, as shown in Figure 1 (b), is calculated by: P (X1, X2,..., Xn, Z) = P (Z | X1,... Xn) n-k = 1P (Xk). (8) This must be done for each state configuration. Of course, we can use the combination operator (\"vectorizing\" external product) which gives us the matrix representation: Hz, x1, x2,..., xn = z | x1x2... (e n k = 1 xk,) (9), where e is the vector needed for point-by-point multiplication with the combination product of all probability vectors."}, {"heading": "2.7. Summary", "text": "In this section we have demonstrated the computational identity of converging and simple BN. We have proposed a new probability combination operator that transforms the probabilities of multiple parents into a single probability vector. This new operator can be easily integrated into the matrix notation. In addition, we have established an important new evidence set that will be used to extract CPTs from linear and nonlinear functions."}, {"heading": "3. CPT Generation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Introduction", "text": "At the beginning of the paper, techniques for creating CPTs were outlined and their shortcomings discussed. In this section, the MLE and EM methods are re-examined and explanations given for their errors, which are more commonly placed in the area of parameter learning methods (see Koller and Friedman (2009) for details).In this section, new methods that can generate CPTs are presented based on the assumption that causes and effects can be observed. Even if they cannot be observed, it is still better and easier for a skilled person to provide the causes and effects and apply the techniques presented here. Measurements of the quality of these models (Section 4) will show that little information is required to produce CPTs with the new methods."}, {"heading": "3.2. Maximum Likelihood Estimation", "text": "A classic approach to obtaining CPTs is to use the Maximum Likelihood Estimation (MLE): \u03b8 = arg max \u03b8 L (M\u03b8 | D), (10) where \u03b8 are the parameters, M\u03b8 the models, D the given data and L (M\u03b8 | D) = \u0435d \u0442 D P (d | M). The data for a CPT are generally assumed to indicate the cause-effect state (as hard evidence). Therefore, relative frequencies can be derived, for example: P (zi | x j) = N (zi, x j) N (x j) N (x j). (11) N (x j) represents the number of occurrences of case x j in the data, and N (zi, x j) the simultaneous occurrence of state zi and x j. Some problems can be observed. (1) No occurrences of case x j are recorded; (2) N (zi, j) is zero (i.e. impossibility of state zi and x j)."}, {"heading": "3.3. Expectation Maximization", "text": "The MLE identified problems arising from incomplete data, which can be overcome with the help of the expectation maximization algorithm. The goal is to find parameters to which the probability of the model is maximized. First, the model and its parameters are to be adopted \u03b8t, t = 0, followed by the step \"Expectation.\" Here, the responsibilities, which are the expected numbers for the CPT, are calculated: E (N (zi, x j) | D) = \u2211 d-DP (zi, x j | D)). (12) The maximization step determines a new estimate for successt, t + 1: \u03b8t = E (N (zi, x j) | D) \u2022 mj = 1 E (N (z j, x j | D)))) (13) The expectation and maximization step continues until the algorithm approaches, i.e. the age is measured."}, {"heading": "3.4. Conditional Mean Basis", "text": "We will show how to estimate a CPT base from these observations, which is the basis for the probability limit method and the probability potential increase method introduced in the following sections. (This is the basis for the probability limit method and the probability potential increase method introduced in the following sections to derive the real CPT, i.e. a CPT base is a matrix that can still violate the probability limit values. (The conditional mean function E (z-x) is also called regression from z to x. (.) Kolmogorov's axioms have to follow. General discussions about multiple regressions can be found in Hastie et al (2001) and Greene (2000)."}, {"heading": "3.5. Probability boundary limitation method", "text": "If the value that is supposed to represent the probability is outside the range, it is set to its narrowest limit, that is, if xi < 0 then xi is set to zero and xi > 1 assigns it the value of one, i.e. xi 10: = min {max {xi, 0}, 1} and i. (21) After that, the column vector x = (xi 10) is set to x = x-x-i x-i. An exception that could occur is that all approximate \"probabilities\" are less than zero, in which case one could make the approximation by shifting. A rough solution, if all values are zero or one, is obtained by assigning x a uniform probability."}, {"heading": "3.6. Probability potential surge method", "text": "The x-i values of a column are transformed into the positive range by adding the negative of the lowest x-i value. For example, x = (\u2212.2 \u2212.3.4) \"then the lowest value is m = \u2212.3 and the increase of the potential results in x = (.1.0.7),\" which must be normalized by \u03b1 \u2212 1: = x-e3 = 1 + 0 +.7 =.8. Following is the formal expression of the probability potential surge method: m < 0 \u03b1 (x-en min x-en min x-x), (22) where \u03b1 is the normalization coefficient (\u03b1 \u2212 1 = 1 x-i = x-en).nal to XB. This means: XB-E (XB-en min x) E = 0 (XB-en min x), (Z \u2212 XB-i = x en).nal to XB."}, {"heading": "3.7. Probability base vector extraction method", "text": "In this section we will show a new method to extract a CPT from a logistic regression = b = 1. However, the method can be applied to any function working on [0, 1].The multinominal distribution is given by: P (Z = zk) = eyk (1 + p = 1 e yk) \u2212 1, p (Z = zn) = (1 + p = 1 e yk), (23) where yk = 1 bk = 1 bk jx j = 1 e yk) \u2212 1, x \u00b2, x \u00b2. The logistic model is a special case of the generalized linear model, explained in McCullagh and Nelder (1989).The effect described by its probabilities P (Z) (dependent variable) is calculated as cause probabilities (independent variables).The probability is l (b): = 1 P = 1 P (Z = x), where b = x."}, {"heading": "3.8. Summary and Concluding Remarks", "text": "Two of the newly introduced methods of the CPT generation were based on the modification of the conditional mean by potential surge and boundary limitation heuristics. The third - the column extraction method - demonstrated how to turn a non-linear function (logistic regression) into a CPT. Here, our proposal 2.2 played an important role in extracting an approximate CPT. Case studies in later sections will be used to evaluate the quality of these models. In the following section, new standards will be introduced that we believe are simple and intuitive and provide the opportunity to compare the quality of CPT approximation models."}, {"heading": "4. Goodness of Models", "text": "There are many classical evaluation methods of quality for regression (e.g. R2) and logistic regression (e.g. R statistics). Often the Kullback-Leibler (KL) -Divergence measurement is found in literature (e.g. Zhou et al. (2014); Zagorecki and Druzdzel (2013) for CPT comparisons, which was originally developed to measure relative entropy in information. It measures the Divergenz of C by using [i] xi [j] j ln ci j [j] c] j [j] j [j] j [j] j [j] j [j] j [j] j [j] j [j] j [j] j [j] j [j] j [j] j c] j [j c] j j j j j j j j c) j (j c) j j j j j c) j j j j (j c) j j j j j c) j j j j j c [j] c) j j j j j c [j c] j c) j j j j j j c [j c] c) j j j j j j c [j c] c) j j j j c [j c) j j j c [j c] c) j c [j c [j c] j c) j j j c [j c] c) j j c [j c) j c [j c) j c [j c] c) j j c [j c) j j c [j c) j c [j c] c) j j c [j c) j c [j c) j c [c] c) j c [c) j c) j j c [c) j c c [c) j c [c) j c [c) j j c [c) j c [c) j c [c) j j j c [c) j c [c [c] c) j c) j j j j c [c) j c [c [c] c c) j j j c) j j j j j c [c) j j j c [c) j c c c [c) j j j j j c [c) j c [c [c) c) c c"}, {"heading": "4.1. Effect comparison", "text": "There are four measurements we are interested in: diagnostic error (d), total average displacement error (s \u00b2), mean state error (s \u00b2) and absolute effectiveness observational error (s \u00b2) (s \u00b2). A good representation of these errors can be found in Figure 3. In the previous sections we have shown methods to determine an approximate CPT number, the causes X (k \u00b2 n matrix) and effects Z (k \u00b2 m matrix) training data. The quality of CPT is evaluated by using test data Xt and Zt. Of course, we assume that the test data is meaningful. The above four measurements are the result of comparing Zt and Z \u00b2 s: = XtC with each other. The absolute effectiveness observation error compares each observation with the corresponding approximation. Let us be precise about the observation of the effects on us."}, {"heading": "4.2. CPT comparison", "text": "We assume that one CPT C = (ci j) = P (Z | X) is \"correct\" and the other an approximation C = (c). It is tempting to introduce a relative error magnitude, but it can easily be shown that these are unsuitable for probability measurements. In the previous section we introduced the observation error of the absolute effect, which is based on the observation that probabilities are \"shifted\" between states. We apply this principle to the CPT to get the definition 4.6. CPT shift erroneous: = 1 nn \u00b2 j = 1 (1 m \u00b2 i = 1 m \u00b2 i = 1 m \u00b2 j = 12nn \u00b2 j = 1m \u00b2 i = 1 \u00b0 C = 1 \u00b0 C = 0 \u00b0 C = 0 0 0 0."}, {"heading": "5. Case Study (1) - Burnt forest desertification risk", "text": "A Bayesian network for determining the risk of burnt forest deserts was introduced in Stassopoulou et al. (1998). The main purpose of this paper was to determine the similarity between Bayesian and neural networks. In addition to discussing this relationship, a case study on forest desertification was presented. Our goal in this section is to use the given Bayesian network and the corresponding test data. From this data, we generate several CPTs that apply the Limitation of Probability method, the Probability Potential Leap method, and the Probability Base Vector Extraction method. Quality is tested for the predictive power of CPTs and their total average displacement error."}, {"heading": "5.1. Bayesian Network and Data", "text": "The regeneration of the Mediterranean forests usually takes place within two to five years. The potential of this regeneration depends on the depth of the soil, soil aspects and animals in the area. On the other hand, there is an erosion risk, which is influenced by the slope, the rock type and again the depth of the soil. In Figure 2 we show the complete GIS-Bayesian network. We focus on the lower part of this network, which is described by the following nodes: erosion risk E, regeneration potential R and risk of desertification D. E and R each have three states, while D has five states. Thus, the resulting CPT of D is a matrix of 5 x 9. The paper Stassopoulou et al. (1998) contains two data tables, which we use to calculate the conditional probability table. For the convenience of the reader, we have reproduced the tables and added them to the electronic companion. The first table describes the training dates (39) and the second table (11) for each of the first four years, each of which contains the first data."}, {"heading": "5.2. Boundary limitation and potential surge regressions", "text": "First, we combine the CPT observations (X: = E R, see algorithm 1). Next, we compute the CPT base according to B \u0445: = b \u0445 i, j: = (((X \u2032 X) \u2212 1X \u2032 D) \u2032. Finally, we transform the CPT base into a CPT error that guarantees that each element is a probability and that the column sum adds up to one. We use boundary boundaries and potential shock methods to achieve compliance with these limitations. Heuristics works according to columns 1,2, 7 and 8 and results in a maximum change of 13.4% for b \u0445 2.2. The boundary limitation method results in a CPT that has a lower average displacement error than the potential shock method on the test data. This CPT error is shown in Table 1."}, {"heading": "5.3. Logistic Regression", "text": "The multinomial logistic regression results in an average displacement error of 2.1% between approximate and observed effect test data. This was derived from the unique dataset. In a first step, we combined the nodes E and R and determined the logistic regression parameters. This is achieved by applying a 9 \u00d7 9 canonical matrix E as input to Equation System 27. This focus is on generating a CPT C shown in Table 2. We use this CPT to determine the probabilities of effectiveness for the test data. This is achieved by applying a 9 \u00d7 9 canonical matrix E as input to Equation System 27. This results in the CPT C shown in Table 2. We use this CPT to determine the probabilities of effectiveness for the test data of Z = C (Et Rt Rt). This must be compared with the effects of test data from Zt."}, {"heading": "6. Case Study (2) - Military Effects", "text": "The case study presented here contains military effects that are critical success factors (CSF) for a vignette. Typical military effects are locating, destroying, securing, and much more (see Louvieris et al. (2007) for details).These effects are evaluated using a Bayesian network."}, {"heading": "6.1. Bayesian Network and Data", "text": "The effect models introduced depend on the status of the situation and the relative morality. In this paper, we will not focus on the challenges of obtaining evidence for these measurements or deriving this Bayesian network - information to this effect can be found in Louvieris et al. (2005). We will focus on the convergence of Bayesian networks. The effect of the CSF node E has two parents: Situation S and relative morality M. Each node has three states. The aim is to derive the CPT for E. Whenever they noticed changes, the consensus designation yielded a table of 83 rows and 9 columns (see table in Electronic Companion). Additionally, they had to provide probability vectors for S, M and E for the effects observed in the vignettes, whenever they noticed changes. The consensus designation yielded a table of 83 rows and 9 columns (see table in Electronic Companion). Deleting identical entries yielded clear sample points for the effects observed in the vignettes. The consensus designation yielded a table of 83 rows and 9 columns (see table in Electronic Companion). In addition, deleting identical entries yielded probability for S, M and E for the effects observed in the vignettes, whenever they noticed changes. The consensus designation yielded a table of 83 rows and 9 columns (see table in Electronic Companion Electronic Companion)."}, {"heading": "6.2. Probability boundary limitation and potential surge method", "text": "The application of the probability limit method results in an absolute column sum error of 88.4% and an average displacement error of 29.5% in 38 out of 54 cases (70.4%). Applying the CPT regression to the observations, the correct state of action was chosen in 38 out of 54 cases (70.4%). Here, \"correctly\" is based on the assumption that the CPT provided by the SME is error-free. Assuming that the effect observations form the basis for the comparison, we obtain that the correct state of action is selected 50 times with B (i.e. diagnostic quality of 92.6%), while the use of the CPT method results in 40 correct state selections (i.e. diagnostic strength of 74.0%). For the command and control agents, the correct choice of the state is important so that the agents can take the appropriate action. The above results are encouraging because in a similar scenario, these case-level decisions (most SPT-correct decisions) will be made by these agents in most cases."}, {"heading": "7. Conclusion", "text": "A review of existing CPT generation methods revealed the need to apply multivariate data analysis techniques to Bayesian Networks rather than noisy-functional methods. Therefore, the main contributions of this paper are new CPT generation methods, especially when the input from parent nodes is observed as soft evidence. The first two methods - boundary limitation and potential impact method - allow us to propose tools such as a combine harvester and the necessary \"column extraction\" method by applying them to multinomial logistic regression. The introduction of Bayesian Networks using matrix notation enabled us to propose tools such as column extraction, which combines probability vectors of a converging Bayesian network into a single probability vector, enabling efficient compression of probabilities of effects and helping to generate CPTs."}, {"heading": "Acknowledgement", "text": "The work reported in this paper was sponsored by the UK research programme MOD Data and Information Fusion Defence Technology Centres (DIF DTC). \"Burnt Forest Paper\" Stassopoulou et al. (1998) proved to be a useful resource. We would like to thank Maria Petrou for her feedback."}], "references": [{"title": "Understanding Regression Assumptions", "author": ["W.D. Berry"], "venue": "Sage Publications,", "citeRegEx": "Berry,? \\Q1993\\E", "shortCiteRegEx": "Berry", "year": 1993}, {"title": "Multiple Regression in Practice", "author": ["W.D. Berry", "S. Feldman"], "venue": null, "citeRegEx": "Berry and Feldman,? \\Q1985\\E", "shortCiteRegEx": "Berry and Feldman", "year": 1985}, {"title": "A myopic approach to ordering nodes for parameter elicitation in bayesian belief networks. Knowledge and Data Engineering", "author": ["D. Bhattacharjya", "L. Deleris", "B. Ray"], "venue": "IEEE Transactions on 26,", "citeRegEx": "Bhattacharjya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bhattacharjya et al\\.", "year": 2014}, {"title": "A new dynamic rule activation method for extended belief rule-based systems. Knowledge and Data Engineering", "author": ["A. Calzada", "J. Liu", "H. Wang", "A. Kashyap"], "venue": "IEEE Transactions on", "citeRegEx": "Calzada et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Calzada et al\\.", "year": 2015}, {"title": "A Bayesian Method for the Induction of Probabilistic Networks from Data", "author": ["G.F. Cooper", "T. Dietterich"], "venue": "in: Machine Learning,", "citeRegEx": "Cooper and Dietterich,? \\Q1992\\E", "shortCiteRegEx": "Cooper and Dietterich", "year": 1992}, {"title": "Parameter adjustement in Bayes networks. The generalized noisy OR\u2013gate", "author": ["F.J. D\u0131\u0301ez"], "venue": "in: Proceedings of the 9th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "D\u0131\u0301ez,? \\Q1993\\E", "shortCiteRegEx": "D\u0131\u0301ez", "year": 1993}, {"title": "Discovering Statistics Using SPSS (Introducing Statistical Methods series)", "author": ["A. Field"], "venue": "Third edition ed.,", "citeRegEx": "Field,? \\Q2009\\E", "shortCiteRegEx": "Field", "year": 2009}, {"title": "1961a. A CAUSAL CALCULUS (I)", "author": ["I.J. Good"], "venue": "Br J Philos Sci XI,", "citeRegEx": "Good,? \\Q1961\\E", "shortCiteRegEx": "Good", "year": 1961}, {"title": "1961b. A CAUSAL CALCULUS (II)", "author": ["I.J. Good"], "venue": "Br J Philos Sci XII,", "citeRegEx": "Good,? \\Q1961\\E", "shortCiteRegEx": "Good", "year": 1961}, {"title": "Multivariate Data Analysis (5th Edition)", "author": ["J.F. Hair", "R.L. Tatham", "R.E. Anderson", "W. Black"], "venue": null, "citeRegEx": "Hair et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hair et al\\.", "year": 1998}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "A Programming Language", "author": ["K.E. Iverson"], "venue": null, "citeRegEx": "Iverson,? \\Q1962\\E", "shortCiteRegEx": "Iverson", "year": 1962}, {"title": "Bayesian Networks and Decision Graphs (Information Science and Statistics)", "author": ["F.V. Jensen"], "venue": null, "citeRegEx": "Jensen,? \\Q2001\\E", "shortCiteRegEx": "Jensen", "year": 2001}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Recursive noisy OR - a rule for estimating complex probabilistic interactions", "author": ["J.F. Lemmer", "D.E. Gossink"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B", "citeRegEx": "Lemmer and Gossink,? \\Q2004\\E", "shortCiteRegEx": "Lemmer and Gossink", "year": 2004}, {"title": "Exploiting Structure in Weighted Model Counting Approaches to Probabilistic Inference", "author": ["W. Li", "P. Poupart", "P. van Beek"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Bayesian network modeling of the consensus between experts: An application to neuron classification", "author": ["P.L. L\u00f3pez-Cruz", "P. Larra\u00f1aga", "J. DeFelipe", "C. Bielza"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "L\u00f3pez.Cruz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "L\u00f3pez.Cruz et al\\.", "year": 2014}, {"title": "Decision Support System using Dynamic Parsimonious Information Fusion Architectures", "author": ["P. Louvieris", "W. Garn", "G. White", "N. Mashanovich", "C. Papathanassiou"], "venue": "DIF DTC,", "citeRegEx": "Louvieris et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Louvieris et al\\.", "year": 2007}, {"title": "Agent-Based Parsimonious Decision Support Paradigm Employing Bayesian Belief", "author": ["P. Louvieris", "A. Gregoriades", "N. Mashanovich", "G. White", "R. O\u2019Keefe", "J. Levine", "S. Henderson"], "venue": null, "citeRegEx": "Louvieris et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Louvieris et al\\.", "year": 2005}, {"title": "Generalized linear models (Second edition)", "author": ["P. McCullagh", "J.A. Nelder"], "venue": null, "citeRegEx": "McCullagh and Nelder,? \\Q1989\\E", "shortCiteRegEx": "McCullagh and Nelder", "year": 1989}, {"title": "Controlled generation of hard and easy Bayesian networks: Impact on maximal clique size in tree clustering", "author": ["O.J. Mengshoel", "D.C. Wilkins", "D. Roth"], "venue": "Artificial Intelligence 170,", "citeRegEx": "Mengshoel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mengshoel et al\\.", "year": 2006}, {"title": "Fusion, propagation, and structuring in belief networks", "author": ["J. Pearl"], "venue": "Artificial Intelligence 29,", "citeRegEx": "Pearl,? \\Q1986\\E", "shortCiteRegEx": "Pearl", "year": 1986}, {"title": "Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Bayesian networks with a logistic regression model for the conditional probabilities", "author": ["F. Rijmen"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "Rijmen,? \\Q2008\\E", "shortCiteRegEx": "Rijmen", "year": 2008}, {"title": "Artificial Intelligence: A Modern Approach (3rd Edition). 3 ed", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig", "year": 2010}, {"title": "Application of a Bayesian Network in a GIS Based Decision Making System", "author": ["A. Stassopoulou", "M. Petrou", "J. Kittler"], "venue": "International Journal of Geographical Information Science", "citeRegEx": "Stassopoulou et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Stassopoulou et al\\.", "year": 1998}, {"title": "Exploiting causal functional relationships in Bayesian network modelling for personalised healthcare", "author": ["M. Velikova", "J.T. van Scheltinga", "P.J. Lucas", "M. Spaanderman"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "Velikova et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Velikova et al\\.", "year": 2014}, {"title": "Probabilistic inference with noisy-threshold models based on a CP tensor decomposition", "author": ["J. Vomlel", "P. Tichavsk\u00fd"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "Vomlel and Tichavsk\u00fd,? \\Q2014\\E", "shortCiteRegEx": "Vomlel and Tichavsk\u00fd", "year": 2014}, {"title": "Modeling causal reinforcement and undermining for efficient cpt elicitation", "author": ["Y. Xiang", "N. Jia"], "venue": "Knowledge and Data Engineering, IEEE Transactions", "citeRegEx": "Xiang and Jia,? \\Q2007\\E", "shortCiteRegEx": "Xiang and Jia", "year": 2007}, {"title": "Knowledge engineering for bayesian networks: How common are noisy-max distributions in practice? Systems, Man, and Cybernetics: Systems", "author": ["A. Zagorecki", "M. Druzdzel"], "venue": "IEEE Transactions on", "citeRegEx": "Zagorecki and Druzdzel,? \\Q2013\\E", "shortCiteRegEx": "Zagorecki and Druzdzel", "year": 2013}, {"title": "Bayesian network approach to multinomial parameter learning using data and expert judgments", "author": ["Y. Zhou", "N. Fenton", "M. Neil"], "venue": "International Journal of Approximate Reasoning 55,", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "Pearl (1988), Mengshoel et al.", "startOffset": 0, "endOffset": 13}, {"referenceID": 20, "context": "Pearl (1988), Mengshoel et al. (2006) and many more have provided fundamental work in this field.", "startOffset": 14, "endOffset": 38}, {"referenceID": 16, "context": "Pearl (1986) introduced and discussed the noisy-or which can be traced back to Good (1961a) and Good (1961b).", "startOffset": 0, "endOffset": 13}, {"referenceID": 6, "context": "Pearl (1986) introduced and discussed the noisy-or which can be traced back to Good (1961a) and Good (1961b).", "startOffset": 79, "endOffset": 92}, {"referenceID": 6, "context": "Pearl (1986) introduced and discussed the noisy-or which can be traced back to Good (1961a) and Good (1961b). These approaches only work for binary variables.", "startOffset": 79, "endOffset": 109}, {"referenceID": 5, "context": "D\u0131\u0301ez (1993) showed a generalized noisy-or gate for multivalued variables.", "startOffset": 0, "endOffset": 13}, {"referenceID": 5, "context": "D\u0131\u0301ez (1993) showed a generalized noisy-or gate for multivalued variables. Zagorecki and Druzdzel (2013) examined three test instances and found that noisy-max gates were a good fit for half of the given CPTs.", "startOffset": 0, "endOffset": 105}, {"referenceID": 5, "context": "D\u0131\u0301ez (1993) showed a generalized noisy-or gate for multivalued variables. Zagorecki and Druzdzel (2013) examined three test instances and found that noisy-max gates were a good fit for half of the given CPTs. Another generalization of the noisy-or is the recursive noisy-or by Lemmer and Gossink (2004). Vomlel and Tichavsk\u00fd (2014) introduced noisy-thresholds in combination with a novel tensor representation of CPTs.", "startOffset": 0, "endOffset": 304}, {"referenceID": 5, "context": "D\u0131\u0301ez (1993) showed a generalized noisy-or gate for multivalued variables. Zagorecki and Druzdzel (2013) examined three test instances and found that noisy-max gates were a good fit for half of the given CPTs. Another generalization of the noisy-or is the recursive noisy-or by Lemmer and Gossink (2004). Vomlel and Tichavsk\u00fd (2014) introduced noisy-thresholds in combination with a novel tensor representation of CPTs.", "startOffset": 0, "endOffset": 333}, {"referenceID": 5, "context": "D\u0131\u0301ez (1993) showed a generalized noisy-or gate for multivalued variables. Zagorecki and Druzdzel (2013) examined three test instances and found that noisy-max gates were a good fit for half of the given CPTs. Another generalization of the noisy-or is the recursive noisy-or by Lemmer and Gossink (2004). Vomlel and Tichavsk\u00fd (2014) introduced noisy-thresholds in combination with a novel tensor representation of CPTs. These and other noisy-methods belong to class of noisy functional dependency methods. These operate on a parsimonious set of input data (in general directly proportional to the number of states) and require additional constraints (e.g. order, independence). Techniques employed for CPT generation in order to support decision makers in our problem space must be flexible enough to accommodate a greater degree of variety than that imposed by the Noisy-Max method which imposes a sequence order constraint on the CPT output Li et al. (2011). Such a restriction is prone to produce erroneous CPTs.", "startOffset": 0, "endOffset": 960}, {"referenceID": 10, "context": "Jensen (2001) introduce several batch learning methods.", "startOffset": 0, "endOffset": 14}, {"referenceID": 3, "context": "Cooper and Dietterich (1992) present the construction of Bayesian Networks from databases.", "startOffset": 0, "endOffset": 29}, {"referenceID": 3, "context": "Cooper and Dietterich (1992) present the construction of Bayesian Networks from databases. They showed ways to derive the conditional expectancy by means of frequency observations. In the subsequent analysis we will extend this conditional expectancy approach in a natural manner by the usage of regression based CPTs. Popular parameter estimation techniques include the maximum likelihood estimation (MLE), Bayesian estimation and Expectation-Maximization (EM). We will discuss MLE and EM in section 3.2 and 3.3 respectively. These methods can be seen as having an objective subject to constraints. Zhou et al. (2014) describe a constrained optimization approach, which reveals similar conceptual ideas to the ones used in this paper but realizing it with different methods.", "startOffset": 0, "endOffset": 619}, {"referenceID": 2, "context": "The process of eliciting the information with minimum effort has been addressed by Bhattacharjya et al. (2014) by determining the order in which the CPT should be queried from a single expert.", "startOffset": 83, "endOffset": 111}, {"referenceID": 2, "context": "The process of eliciting the information with minimum effort has been addressed by Bhattacharjya et al. (2014) by determining the order in which the CPT should be queried from a single expert. Xiang and Jia (2007) investigated ways of finding CPTs by proposing a causal tree model.", "startOffset": 83, "endOffset": 214}, {"referenceID": 2, "context": "The process of eliciting the information with minimum effort has been addressed by Bhattacharjya et al. (2014) by determining the order in which the CPT should be queried from a single expert. Xiang and Jia (2007) investigated ways of finding CPTs by proposing a causal tree model. Finding consensus between experts is a challenge as we observed in our military case study. This issue has also been investigated in L\u00f3pez-Cruz et al. (2014), who developed Bayesian network to facilitate this task.", "startOffset": 83, "endOffset": 440}, {"referenceID": 26, "context": "In general the technique is especially useful when causes and effects have been measured, leading to a wide range of applications such as healthcare Velikova et al. (2014).", "startOffset": 149, "endOffset": 172}, {"referenceID": 21, "context": "In Pearl (1988) the notation Mz|x is used for the CPT and called conditional probability matrix or link matrix.", "startOffset": 3, "endOffset": 16}, {"referenceID": 27, "context": "Here, we would like to draw the readers attention to Vomlel and Tichavsk\u00fd (2014) representation of CPTs as tensors.", "startOffset": 53, "endOffset": 81}, {"referenceID": 24, "context": "The literature Russell and Norvig (2010) distinguish between three types of Bayesian Networks: discrete, continuous and hybrid Bayesian Networks.", "startOffset": 15, "endOffset": 41}, {"referenceID": 21, "context": "A representation of multiple nodes as a single node is known as a cluster node Pearl (1988).", "startOffset": 79, "endOffset": 92}, {"referenceID": 13, "context": "These techniques are more commonly placed in the area of parameter learning methods (see Koller and Friedman (2009) for details).", "startOffset": 89, "endOffset": 116}, {"referenceID": 9, "context": "General discussions of multiple regressions can be found in Hastie et al. (2001) and Greene (2000).", "startOffset": 60, "endOffset": 81}, {"referenceID": 9, "context": "General discussions of multiple regressions can be found in Hastie et al. (2001) and Greene (2000). The application of the conditional mean to approximate the conditional probability table was used in Cooper and Dietterich (1992) in relation to Bayesian Networks.", "startOffset": 60, "endOffset": 99}, {"referenceID": 4, "context": "The application of the conditional mean to approximate the conditional probability table was used in Cooper and Dietterich (1992) in relation to Bayesian Networks.", "startOffset": 101, "endOffset": 130}, {"referenceID": 19, "context": "The logistic model is a special case of the generalised linear model, explained in McCullagh and Nelder (1989). The effect described by its probabilities P(Z) (dependent variable) is computed given cause probabilities (independent variables).", "startOffset": 83, "endOffset": 111}, {"referenceID": 8, "context": "So far this has been \u201cclassic\u201d multivariate data analysis which is explained in Hair et al. (1998), Hastie et al.", "startOffset": 80, "endOffset": 99}, {"referenceID": 8, "context": "So far this has been \u201cclassic\u201d multivariate data analysis which is explained in Hair et al. (1998), Hastie et al. (2001) and Field (2009).", "startOffset": 80, "endOffset": 121}, {"referenceID": 6, "context": "(2001) and Field (2009).", "startOffset": 11, "endOffset": 24}, {"referenceID": 23, "context": "Rijmen (2008) has discussed this method in more detail.", "startOffset": 0, "endOffset": 14}, {"referenceID": 29, "context": "Zhou et al. (2014); Zagorecki and Druzdzel (2013)) for CPT comparisons, which was originally developed to measure the relative entropy in information.", "startOffset": 0, "endOffset": 19}, {"referenceID": 29, "context": "(2014); Zagorecki and Druzdzel (2013)) for CPT comparisons, which was originally developed to measure the relative entropy in information.", "startOffset": 8, "endOffset": 38}, {"referenceID": 11, "context": "4Iverson (1962) introduced these brackets for true-", "startOffset": 1, "endOffset": 16}, {"referenceID": 25, "context": "A Bayesian network to determine the risk of burnt forest desertification was introduced in Stassopoulou et al. (1998). The main purpose of this paper was to obtain the correspondence between Bayesian and Neural Networks.", "startOffset": 91, "endOffset": 118}, {"referenceID": 25, "context": "The paper Stassopoulou et al. (1998) details two tables of data, which we will use as input to compute the conditional probability table.", "startOffset": 10, "endOffset": 37}, {"referenceID": 17, "context": "Typical military effects are find, destroy, secure and many more (see Louvieris et al. (2007) for details).", "startOffset": 70, "endOffset": 94}, {"referenceID": 17, "context": "In this paper we will not discuss the challenges of obtaining evidence for these measures nor the derivation of this Bayesian Network - respective information can be found in Louvieris et al. (2005). We will focus our discussion on converging Bayesian Networks.", "startOffset": 175, "endOffset": 199}, {"referenceID": 3, "context": "As a third avenue it should be investigated of whether the converging Bayesian Networks are superior to the Extended Belief Rule-Based systems Calzada et al. (2015).", "startOffset": 143, "endOffset": 165}, {"referenceID": 25, "context": "The \u201cburnt forest paper\u201d Stassopoulou et al. (1998) proofed to be a useful resource.", "startOffset": 25, "endOffset": 52}], "year": 2015, "abstractText": "Decision making is often based on Bayesian networks. The building blocks for Bayesian networks are its conditional probability tables (CPTs). These tables are obtained by parameter estimation methods, or they are elicited from subject matter experts (SME). Some of these knowledge representations are insufficient approximations. Using knowledge fusion of cause and effect observations lead to better predictive decisions. We propose three new methods to generate CPTs, which even work when only soft evidence is provided. The first two are novel ways of mapping conditional expectations to the probability space. The third is a column extraction method, which obtains CPTs from nonlinear functions such as the multinomial logistic regression. Case studies on military effects and burnt forest desertification have demonstrated that so derived CPTs have highly reliable predictive power, including superiority over the CPTs obtained from SMEs. In this context, new quality measures for determining the goodness of a CPT and for comparing CPTs with each other have been introduced. The predictive power and enhanced reliability of decision making based on the novel CPT generation methods presented in this paper have been confirmed and validated within the context of the case studies.", "creator": "LaTeX with hyperref package"}}}