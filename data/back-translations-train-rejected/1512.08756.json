{"id": "1512.08756", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2015", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems", "abstract": "Recurrent neural networks (RNNs) have proven to be powerful models in problems involving sequential data. Recently, RNNs have been augmented with \"attention\" mechanisms which allow the network to focus on different parts of an input sequence when computing their output. We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that it can solve some long-term memory problems (specifically, those where temporal order doesn't matter). In fact, we show empirically that our model can solve these problems for sequence lengths which are both longer and more widely varying than the best results attained with RNNs.", "histories": [["v1", "Tue, 29 Dec 2015 19:03:43 GMT  (35kb,D)", "https://arxiv.org/abs/1512.08756v1", null], ["v2", "Thu, 31 Dec 2015 17:52:46 GMT  (35kb,D)", "http://arxiv.org/abs/1512.08756v2", null], ["v3", "Fri, 8 Jan 2016 01:52:06 GMT  (36kb,D)", "http://arxiv.org/abs/1512.08756v3", null], ["v4", "Mon, 28 Mar 2016 02:41:21 GMT  (102kb,D)", "http://arxiv.org/abs/1512.08756v4", null], ["v5", "Tue, 20 Sep 2016 05:02:22 GMT  (37kb,D)", "http://arxiv.org/abs/1512.08756v5", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["colin raffel", "daniel p w ellis"], "accepted": false, "id": "1512.08756"}, "pdf": {"name": "1512.08756.pdf", "metadata": {"source": "CRF", "title": "FEED-FORWARD NETWORKS WITH ATTENTION CAN SOLVE SOME LONG-TERM MEMORY PROBLEMS", "authors": ["Colin Raffel", "Daniel P. W. Ellis"], "emails": ["craffel@gmail.com", "dpwe@ee.columbia.edu"], "sections": [{"heading": "1 MODELS FOR SEQUENTIAL DATA", "text": "Many machine learning problems can best be formulated on the basis of sequential data, and suitable models for these tasks must be able to capture temporal dependencies in sequences that can potentially be of arbitrary length. One such class of models is relapsing neural networks (RNNs), which can be considered a learnable function f, the output of which ht = f (xt, ht \u2212 1) at the time t of the input text and the previous state of the model ht \u2212 1. The formation of RNNs with back propagation through time (Werbos, 1990) is hampered by the problem of disappearance and exploitation of gradients (Pascanu et al., 2012; Hochreiter & Schmidhuber, 1997; Bengio et al., 1994), and consequently, RNNNNs are typically only applied in practice to tasks where sequential dependencies can extend over a maximum of hundreds of time steps. Very long sequences can also render the training inefficient, and NNNNNNNNs cannot be fully evaluated."}, {"heading": "1.1 ATTENTION", "text": "Attention mechanisms allow a more direct dependence between the state of the model at different times. According to the definition of (Bahdanau et al., 2014), attention-based models calculate a \"context vector ct\" as a weighted mean of the state sequence h byct = T \u2211 j = 1 \u03b1tjhjwhere T is the total number of time steps in the input sequence and \u03b1tj is a weight t calculated for each time step for each state hj. These context vectors are then used to calculate a new state sequence s, with st ranging from st \u2212 1, ct and the output of the model to t \u2212 1. The weights \u03b1tj are then calculated byetj = a (st \u2212 1, hj)."}, {"heading": "1.2 FEED-FORWARD ATTENTION", "text": "A simple simplification of the attention mechanism described above, which would make it possible to generate a single vector c from an entire sequence, could be formulated as follows: et = a (ht), \u03b1t = exp (et) \u2211 Tk = 1 exp (ek), c = T \u2211 t = 1 \u03b1tht (1) As before, a is a learnable function, but it now only depends on ht. In this formulation, attention can be regarded as the creation of a standard recursive network, which c of the input sequence by calculating an adaptive weighted average of the state sequence h. A scheme of this form of attention is shown in Figure 1. S\u00f8nderby et al. (2015) compared the effectiveness of a standard recursive network with a recursive network, which is extended by this simplified version of attention to the task of protein sequence analysis. One consequence of using an attention mechanism is the ability to integrate information over time."}, {"heading": "2 TOY LONG-TERM MEMORY PROBLEMS", "text": "A common method of measuring the long-term memory capability of a given model is to test it against the synthetic problems originally proposed by Hochreiter & Schmidhuber (1997).In this paper, we focus on the problems of \"addition\" and \"multiplication\"; for reasons of space, we refer the reader to their specification (Hochreiter & Schmidhuber, 1997) or (Sutskever et al., 2013).As suggested by Hochreiter & Schmidhuber (1997), we define accuracy as the ratio of sequences for which the absolute error between predicted value and target value was less than.04.Applying our predictive model to these tasks is somewhat clumsy because they are commuting and therefore easier to solve with a model that ignores the temporal order. However, as we argue further in Section 2.4, we believe that these tasks represent a useful demonstration of our model's ability to refer to arbitrary locations in the input sequence when calculating its results."}, {"heading": "2.1 MODEL DETAILS", "text": "For all the experiments, we used the following model: First, the state ht was calculated from the input in each time step xt of ht = LReLU (Wxhxt + bxh), where Wxh RD \u00b7 2, bxh RD, and LReLU (x) = max (x,.01x) is the \"leaky rectifier,\" as proposed by Maas et al. (2013). We found that this nonlinearity improved early convergence, so we used it in all our models. We tested models where the context vector cis was then calculated either as in Equation (1), with a (ht) = tanh (Whcht + bhc), where Whc R1 \u00d7 D, bhc R, or simply as an unweighted mean of h as in Equation (2), we calculated a context vector cis then either as in Equation (1), with a (ht) = tanh (Whc + bhc), where whh is unweighted or whh (R)."}, {"heading": "2.2 FIXED-LENGTH EXPERIMENT", "text": "Traditionally, the sequence lengths tested in each task differ evenly between [T0, 1.1T0] and the different values of T0. As T0 increases, the model must be able to handle longer-term dependencies, and the greatest value of T0 achieved by RNs with different educational, regulatory and model structures varied from several hundred (Martens & Sutskever, 2011; Sutskever et al., 2013; Le et al., 2015; Krueger & Memisevic, 2015; Arjovsky et al., 2015) to several thousand (Hochreiter & Schmidhuber, 1997; Jaeger, 2012). Therefore, we tested our proposed sequence forward attention models for T0, 50, 500, 1000, 5000, 10000}. The required number of epochs or accuracy after 100 epochs for each task, sequence length, and time integration method (adaptively weighted attention or unweighted mean)."}, {"heading": "2.3 VARIABLE-LENGTH EXPERIMENT", "text": "Since the range of sequence lengths [T0, 1.1T0] is small compared to the range of T0 values we evaluated, we continued to test whether it was possible to train a single model that could handle sequences with very different lengths. To our knowledge, such a variant of these tasks has not been studied so far. We trained models of the same architecture that were used in the previous experiment on minibatches of sequences whose lengths were randomly chosen between 50 and 10,000 time steps. Using the attention mechanism of the equation (1), our model achieved an accuracy of 99.9% in addition and 99.4% in the multiplication task after training for 100 epochs with sustained test sets of 1000 sequences, suggesting that a single feed network with attention can handle both short and very long sequences, with a marginal decrease in accuracy. With an unweighted average over time, we could only achieve accuracy of 75.5% and 7.5% of the length respectively."}, {"heading": "2.4 DISCUSSION", "text": "One clear limitation of our proposed model is that it will fail in any task that depends on temporal order, because the calculation of an average over time rejects job information. For example, in the two-symbol temporal order task (Hochreiter & Schmidhuber, 1997), where a sequence must be classified in relation to whether two symbols X and Y appear in order X, X; Y; X, Y; or Y, X, can perfectly distinguish between cases X, Y, and Y, but X cannot distinguish between cases X, Y, and Y, X. Nevertheless, the temporal order for some tasks that include sequential data is much less important than the ability to handle very long sequences. For example, in Joachims \"groundbreaking paper on the categorization of text documents (Joachims, 1998), he postulates that\" word stems function well as units of representation, and that their meaning is more subordinated in a number of tasks."}, {"heading": "3 ACKNOWLEDGEMENTS", "text": "We thank Sander Dieleman, Bart van Merrie \u00bc nboer, S\u00f8ren Kaae S\u00f8nderby, Brian McFee and our anonymous critics for the discussion and feedback. 1https: / / github.com / craffel / ff-attention / tree / master / toy _ problems"}], "references": [{"title": "Unitary evolution recurrent neural networks", "author": ["Martin Arjovsky", "Amar Shah", "Yoshua Bengio"], "venue": null, "citeRegEx": "Arjovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Endto-end attention-based large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals"], "venue": null, "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Introduction to neural machine translation with GPUs", "author": ["Kyunghyun Cho"], "venue": "(part 3)", "citeRegEx": "Cho.,? \\Q2015\\E", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Long short-term memory in echo state networks: Details of a simulation study", "author": ["Herbert Jaeger"], "venue": "Technical Report 27, Jacobs University,", "citeRegEx": "Jaeger.,? \\Q2012\\E", "shortCiteRegEx": "Jaeger.", "year": 2012}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "Joachims.,? \\Q1998\\E", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Regularizing RNNs by stabilizing activations", "author": ["David Krueger", "Roland Memisevic"], "venue": null, "citeRegEx": "Krueger and Memisevic.,? \\Q2015\\E", "shortCiteRegEx": "Krueger and Memisevic.", "year": 2015}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V. Le", "Navdeep Jaitly", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Molding CNNs for text: non-linear, non-consecutive convolutions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng"], "venue": "In ICML Workshop on Deep Learning for Audio, Speech, and Language Processing,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Martens and Sutskever.,? \\Q2011\\E", "shortCiteRegEx": "Martens and Sutskever.", "year": 2011}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Pruning subsequence search with attention-based embedding", "author": ["Colin Raffel", "Daniel P.W. Ellis"], "venue": "In Proceedings of the 41st IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Raffel and Ellis.,? \\Q2016\\E", "shortCiteRegEx": "Raffel and Ellis.", "year": 2016}, {"title": "Convolutional lstm networks for subcellular localization of proteins", "author": ["S\u00f8ren Kaae S\u00f8nderby", "Casper Kaae S\u00f8nderby", "Henrik Nielsen", "Ole Winther"], "venue": null, "citeRegEx": "S\u00f8nderby et al\\.,? \\Q1919\\E", "shortCiteRegEx": "S\u00f8nderby et al\\.", "year": 1919}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J. Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "Training of RNNs with backpropagation through time (Werbos, 1990) is hindered by the vanishing and exploding gradient problem (Pascanu et al.", "startOffset": 51, "endOffset": 65}, {"referenceID": 17, "context": "Training of RNNs with backpropagation through time (Werbos, 1990) is hindered by the vanishing and exploding gradient problem (Pascanu et al., 2012; Hochreiter & Schmidhuber, 1997; Bengio et al., 1994), and as a result RNNs are in practice typically only applied in tasks where sequential dependencies span at most hundreds of time steps.", "startOffset": 126, "endOffset": 201}, {"referenceID": 4, "context": "Training of RNNs with backpropagation through time (Werbos, 1990) is hindered by the vanishing and exploding gradient problem (Pascanu et al., 2012; Hochreiter & Schmidhuber, 1997; Bengio et al., 1994), and as a result RNNs are in practice typically only applied in tasks where sequential dependencies span at most hundreds of time steps.", "startOffset": 126, "endOffset": 201}, {"referenceID": 1, "context": "Following the definition from (Bahdanau et al., 2014), given a model which produces a hidden state ht at each time step, attention-based models compute a \u201ccontext\u201d vector ct as the weighted mean of the state sequence h by", "startOffset": 30, "endOffset": 53}, {"referenceID": 1, "context": "Attention-based RNNs have proven effective in a variety of sequence transduction tasks, including machine translation (Bahdanau et al., 2014), image captioning (Xu et al.", "startOffset": 118, "endOffset": 141}, {"referenceID": 22, "context": ", 2014), image captioning (Xu et al., 2015), and speech recognition (Chan et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 6, "context": ", 2015), and speech recognition (Chan et al., 2015; Bahdanau et al., 2015).", "startOffset": 32, "endOffset": 74}, {"referenceID": 2, "context": ", 2015), and speech recognition (Chan et al., 2015; Bahdanau et al., 2015).", "startOffset": 32, "endOffset": 74}, {"referenceID": 7, "context": "(Cho, 2015) Figure 1).", "startOffset": 0, "endOffset": 11}, {"referenceID": 19, "context": "S\u00f8nderby et al. (2015) compared the effectiveness of a standard recurrent network to a recurrent network augmented with this simplified version of attention on the task of protein sequence analysis.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "This form of integration has been used to collapse the temporal dimension of audio (Dieleman, 2014) and text document (Lei et al., 2015) sequences.", "startOffset": 118, "endOffset": 136}, {"referenceID": 20, "context": "In this paper, we will focus on the \u201caddition\u201d and \u201cmultiplication\u201d problems; due to space constraints, we refer the reader to (Hochreiter & Schmidhuber, 1997) or (Sutskever et al., 2013) for their specification.", "startOffset": 163, "endOffset": 187}, {"referenceID": 20, "context": "In this paper, we will focus on the \u201caddition\u201d and \u201cmultiplication\u201d problems; due to space constraints, we refer the reader to (Hochreiter & Schmidhuber, 1997) or (Sutskever et al., 2013) for their specification. As proposed by Hochreiter & Schmidhuber (1997), we define accuracy as the proportion of sequences for which the absolute error between predicted value and the target value was less than .", "startOffset": 164, "endOffset": 260}, {"referenceID": 3, "context": ", 2015), which is built on top of Theano (Bastien et al., 2012; Bergstra et al., 2010).", "startOffset": 41, "endOffset": 86}, {"referenceID": 5, "context": ", 2015), which is built on top of Theano (Bastien et al., 2012; Bergstra et al., 2010).", "startOffset": 41, "endOffset": 86}, {"referenceID": 13, "context": "01x) is the \u201cleaky rectifier\u201d nonlinearity, as proposed by Maas et al. (2013). We found that this nonlinearity improved early convergence so we used it in all of our models.", "startOffset": 59, "endOffset": 78}, {"referenceID": 13, "context": "01x) is the \u201cleaky rectifier\u201d nonlinearity, as proposed by Maas et al. (2013). We found that this nonlinearity improved early convergence so we used it in all of our models. We tested models where the context vector cwas then computed either as in Equation (1), with a(ht) = tanh(Whcht+ bhc) where Whc \u2208 R1\u00d7D, bhc \u2208 R, or simply as the unweighted mean of h as in Equation (2). We then computed an intermediate vector s = LReLU(Wcsc+ bcs) where Wcs \u2208 RD\u00d7D, b \u2208 R from which the output was computed as y = LReLU(Wsys+ bsy) where Wsy \u2208 R1\u00d7D, bsy \u2208 R. For all experiments, we set D = 100. We used the squared error of the output y against the target value for each sequence as an objective. Parameters were optimized using \u201cadam\u201d, a recently proposed stochastic optimization technique (Kingma & Ba, 2014), with the optimization hyperparameters \u03b21 and \u03b22 set to the values suggested by Kingma & Ba (2014) (.", "startOffset": 59, "endOffset": 900}, {"referenceID": 20, "context": "The largest value of T0 attained using RNNs with different training, regularization, and model structures has varied from a few hundred (Martens & Sutskever, 2011; Sutskever et al., 2013; Le et al., 2015; Krueger & Memisevic, 2015; Arjovsky et al., 2015) to a few thousand (Hochreiter & Schmidhuber, 1997; Jaeger, 2012).", "startOffset": 136, "endOffset": 254}, {"referenceID": 13, "context": "The largest value of T0 attained using RNNs with different training, regularization, and model structures has varied from a few hundred (Martens & Sutskever, 2011; Sutskever et al., 2013; Le et al., 2015; Krueger & Memisevic, 2015; Arjovsky et al., 2015) to a few thousand (Hochreiter & Schmidhuber, 1997; Jaeger, 2012).", "startOffset": 136, "endOffset": 254}, {"referenceID": 0, "context": "The largest value of T0 attained using RNNs with different training, regularization, and model structures has varied from a few hundred (Martens & Sutskever, 2011; Sutskever et al., 2013; Le et al., 2015; Krueger & Memisevic, 2015; Arjovsky et al., 2015) to a few thousand (Hochreiter & Schmidhuber, 1997; Jaeger, 2012).", "startOffset": 136, "endOffset": 254}, {"referenceID": 9, "context": ", 2015) to a few thousand (Hochreiter & Schmidhuber, 1997; Jaeger, 2012).", "startOffset": 26, "endOffset": 72}, {"referenceID": 10, "context": "For example, in Joachims\u2019 seminal paper on text document categorization (Joachims, 1998), he posits that \u201cword stems work well as representation units and that their ordering in a document is of minor importance for many tasks\u201d.", "startOffset": 72, "endOffset": 88}, {"referenceID": 14, "context": "In fact, the current state-of-the-art system for document classification still uses order-agnostic sequence integration (Lei et al., 2015).", "startOffset": 120, "endOffset": 138}, {"referenceID": 1, "context": "They also provide an alternate argument for the claim made by Bahdanau et al. (2014) that attention helps models handle very long and widely variable-length sequences.", "startOffset": 62, "endOffset": 85}], "year": 2016, "abstractText": "We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic \u201caddition\u201d and \u201cmultiplication\u201d long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks. 1 MODELS FOR SEQUENTIAL DATA Many problems in machine learning are best formulated using sequential data and appropriate models for these tasks must be able to capture temporal dependencies in sequences, potentially of arbitrary length. One such class of models are recurrent neural networks (RNNs), which can be considered a learnable function f whose output ht = f(xt, ht\u22121) at time t depends on input xt and the model\u2019s previous state ht\u22121. Training of RNNs with backpropagation through time (Werbos, 1990) is hindered by the vanishing and exploding gradient problem (Pascanu et al., 2012; Hochreiter & Schmidhuber, 1997; Bengio et al., 1994), and as a result RNNs are in practice typically only applied in tasks where sequential dependencies span at most hundreds of time steps. Very long sequences can also make training computationally inefficient due to the fact that RNNs must be evaluated sequentially and cannot be fully parallelized.", "creator": "LaTeX with hyperref package"}}}