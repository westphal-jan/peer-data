{"id": "1512.08903", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2015", "title": "Online Keyword Spotting with a Character-Level Recurrent Neural Network", "abstract": "In this paper, we propose a context-aware keyword spotting model employing a character-level recurrent neural network (RNN) for spoken term detection in continuous speech. The RNN is end-to-end trained with connectionist temporal classification (CTC) to generate the probabilities of character and word-boundary labels. There is no need for the phonetic transcription, senone modeling, or system dictionary in training and testing. Also, keywords can easily be added and modified by editing the text based keyword list without retraining the RNN. Moreover, the unidirectional RNN processes an infinitely long input audio streams without pre-segmentation and keywords are detected with low-latency before the utterance is finished. Experimental results show that the proposed keyword spotter significantly outperforms the deep neural network (DNN) and hidden Markov model (HMM) based keyword-filler model even with less computations.", "histories": [["v1", "Wed, 30 Dec 2015 10:32:12 GMT  (405kb)", "http://arxiv.org/abs/1512.08903v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["kyuyeon hwang", "minjae lee", "wonyong sung"], "accepted": false, "id": "1512.08903"}, "pdf": {"name": "1512.08903.pdf", "metadata": {"source": "CRF", "title": "Online Keyword Spotting with a Character-Level Recurrent Neural Network", "authors": ["Kyuyeon Hwang"], "emails": ["kyuyeon.hwang@gmail.com;", "mjlee@dsp.snu.ac.kr;", "wysung@snu.ac.kr)."], "sections": [{"heading": null, "text": "This year, it is so far that it will only take a few days to achieve an outcome in which everyone else will participate."}, {"heading": "II. END-TO-END KEYWORD SPOTTING MODEL", "text": "The proposed keyword spotter consists of a front-end RNN and a back-end decoder. For each frame, the RNN converts an audio input function into probabilities of characters. The decoder then calculates the probabilities of the keywords based on the most recent probabilities at character level."}, {"heading": "A. Character-Level Unidirectional RNN", "text": "This year, it has reached the point where there is only one person who is able to establish himself in the region."}, {"heading": "B. Decoder Models", "text": "The results of the study show that the number of people recorded who are able to identify themselves is not the number of people recorded, but the number of people recorded who are able to identify themselves. The number of people recorded who are able to identify themselves is about the same as the number of people recorded who are able to identify themselves. The number of people recorded who are able to identify themselves is as small as possible, the number of people recorded who are able to identify themselves is as small as the number of people recorded, the number of people recorded who are able to identify themselves is as small as possible."}, {"heading": "III. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Baseline DNN-HMM Keyword-Filler Model", "text": "As a basic model, the performance of the DNN-HMM Keyword Spotter is also determined, replacing the Gaussian mixing model (GMM) of the keyword filler model in [1] with a DNN acoustic model [8] The DNN has 4 layers of 1,024 logistic sigmoid units each and a Softmax layer of 2,717 units, corresponding to 2,717 bound triphon states. Input of the DNN-HMM Keyword Spotter takes place in 17 consecutive 39-dimensional Gibson Frequency Cepstral Coefficients (MFCC) with vectors (13 Cepstral coefficients plus their delta and double delta values), which are extracted every 10 ms with 25 ms hamming window. In our DNN-HMM experiments, it is observed that the MFCC function performs better than the log filter bank characteristics. The basic model is trained on the same 167 hours proposed for the model."}, {"heading": "B. Experimental Setup", "text": "The experiments are conducted using the 42-minute audio stream generated by summarizing all 330 expressions in the WSJ Nov '92 20K evaluation set. In the DNN-HMM Keyword Spotter with multiple keywords, the threshold of each keyword is given in proportion to the duration of the keyword pronounced. In the proposed CTC Keyword Spotter, on the other hand, the threshold is proportional to the number of characters in the keyword. Models are evaluated using the following keyword sets: \u2022 Set A: percent, hundred thousand, millions, people, president, average, foreigners, international, nuclear \u2022 Set B: that these, they, but, and, or, with, off, notSet A consists of polysyllabic words. Set B, on the other hand, is a set of monosyllabic words that are extremely difficult to recognize without understanding the underlying linguistic structures."}, {"heading": "C. Evaluation", "text": "1) The proposed keyword spotter employs unidirectional RNNs for lowlatency online spoken term detection. When unidirectional networks are formed with CTC, the network learns the output delay required to use a sufficient amount of information from the input. Fig. 3 shows the input waveform and the posterior probabilities of the keyword-only decoders. In this example, the keywords are detected within 200 ms after they are fully spoken, which is comparable to the human response time to speech stimuli, which are also reported as approximately 200 ms. [18] Comparison of the decoding models: The precision recovery plots for Set A and Set B are shown in Figure. 4, with various decoding models in which the network size is set at 3x768 (i.e., 3 LSTM layers, each 768 cells)."}, {"heading": "IV. CONCLUDING REMARKS", "text": "Due to the weak language model embedded in the front-end RNN, the proposed keyword spotter shows remarkable performance improvements over the base model DNN-HMM keyword spotter, even with significantly fewer calculations. In addition, the back-end of the proposed keyword spotter is much simpler than that of the DNN-HMM model. Therefore, the proposed CTC keyword spotter is suitable for applications with a small footprint and low latency with continuous voice input."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was partially supported by the Brain Korea 21 Plus Project and the National Research Foundation of Korea (NRF) grants funded by the Korean Government (MSIP) (No. 2015R1A2A1A10056051).10"}], "references": [{"title": "A hidden Markov model based keyword recognition system", "author": ["R.C. Rose", "D.B. Paul"], "venue": "Acoustics, Speech, and Signal Processing, 1990. ICASSP-90., 1990 International Conference on. IEEE, 1990, pp. 129\u2013132.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Online sequence training of recurrent neural networks with connectionist temporal classification", "author": ["K. Hwang", "W. Sung"], "venue": "arXiv preprint arXiv:1511.06841, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "A statistical model-based voice activity detection", "author": ["J. Sohn", "N.S. Kim", "W. Sung"], "venue": "Signal Processing Letters, IEEE, vol. 6, no. 1, pp. 1\u20133, 1999.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "An application of recurrent neural networks to discriminative keyword spotting", "author": ["S. Fern\u00e1ndez", "A. Graves", "J. Schmidhuber"], "venue": "Artificial Neural Networks\u2013ICANN 2007. Springer, 2007, pp. 220\u2013229.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Bidirectional LSTM networks for context-sensitive keyword detection in a cognitive virtual agent framework", "author": ["M. W\u00f6llmer", "F. Eyben", "A. Graves", "B. Schuller", "G. Rigoll"], "venue": "Cognitive Computation, vol. 2, no. 3, pp. 180\u2013190, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Small-footprint keyword spotting using deep neural networks", "author": ["G. Chen", "C. Parada", "G. Heigold"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 4087\u20134091.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "The design for the Wall Street Journal-based CSR corpus", "author": ["D.B. Paul", "J.M. Baker"], "venue": "Proceedings of the workshop on Speech and Natural Language. Association for Computational Linguistics, 1992, pp. 357\u2013362.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural computation, vol. 12, no. 10, pp. 2451\u20132471, 2000.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["F.A. Gers", "N.N. Schraudolph", "J. Schmidhuber"], "venue": "The Journal of Machine Learning Research, vol. 3, pp. 115\u2013143, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["J.S. Bridle"], "venue": "Neurocomputing. Springer, 1990, pp. 227\u2013236.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1764\u20131772.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech 2: End-to-end speech recognition in English and Mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos", "E. Elsen", "J. Engel", "L. Fan", "C. Fougner", "T. Han", "A. Hannun", "B. Jun", "P. LeGresley", "L. Lin", "S. Narang", "A. Ng", "S. Ozair", "R. Prenger", "J. Raiman", "S. Satheesh", "D. Seetapun", "S. Sengupta", "Y. Wang", "Z. Wang", "C. Wang", "B. Xiao", "D. Yogatama", "J. Zhan", "Z. Zhu"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Single stream parallelization of generalized LSTM-like RNNs on a GPU", "author": ["K. Hwang", "W. Sung"], "venue": "arXiv preprint arXiv:1503.02852, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "author": ["R.J. Williams", "J. Peng"], "venue": "Neural Computation, vol. 2, no. 4, pp. 490\u2013501, 1990.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "Simple reaction-times to speech and non-speech stimuli", "author": ["D. Fry"], "venue": "Cortex, vol. 11, no. 4, pp. 355\u2013360, 1975.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1975}], "referenceMentions": [{"referenceID": 0, "context": "The traditional hidden Markov model (HMM) based keyword spotter [1] does not include a language model to limit the complexity of the decoding stage.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "The RNN is end-to-end trained with connectionist temporal classification (CTC) [2] to directly transcribe the input speech to a sequence of character labels and a wordboundary label.", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "The proposed keyword spotter has several advantages over the previous approaches as follows: \u2022 The front-end RNN is unidirectional and trained by online CTC [3] to process infinitely long audio streams with low latency.", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "Therefore, there is no need for pre-segmenting the utterance with an additional voice activity detector [4].", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "On the other hand, in the previous approaches with bidirectional RNNs [5], [6], keywords are detected after listening each (pre-segmented) utterance to the end.", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "On the other hand, in the previous approaches with bidirectional RNNs [5], [6], keywords are detected after listening each (pre-segmented) utterance to the end.", "startOffset": 75, "endOffset": 78}, {"referenceID": 0, "context": "Unlike the previous phoneme based approaches [1], [6], there is no need for phoneme or senone modeling in both learning and testing.", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "Unlike the previous phoneme based approaches [1], [6], there is no need for phoneme or senone modeling in both learning and testing.", "startOffset": 50, "endOffset": 53}, {"referenceID": 0, "context": "\u2022 New keywords can be added without retraining the RNN by modifying the back-end that is simpler than that of the HMM keyword-filler model [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "This is not possible with the previous end-to-end approaches [5], [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "This is not possible with the previous end-to-end approaches [5], [7].", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "Experimental results show that the proposed CTC keyword spotter greatly outperforms the deep neural network (DNN) [8] and HMM [1] hybrid keyword-filler model on the Wall Street Journal (WSJ) [9] Nov\u201992 20K evaluation set especially when the length of the keyword is short.", "startOffset": 114, "endOffset": 117}, {"referenceID": 0, "context": "Experimental results show that the proposed CTC keyword spotter greatly outperforms the deep neural network (DNN) [8] and HMM [1] hybrid keyword-filler model on the Wall Street Journal (WSJ) [9] Nov\u201992 20K evaluation set especially when the length of the keyword is short.", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "Experimental results show that the proposed CTC keyword spotter greatly outperforms the deep neural network (DNN) [8] and HMM [1] hybrid keyword-filler model on the Wall Street Journal (WSJ) [9] Nov\u201992 20K evaluation set especially when the length of the keyword is short.", "startOffset": 191, "endOffset": 194}, {"referenceID": 9, "context": "The front-end RNN is a deep unidirectional long short-term memory (LSTM) [10] network with forget gates [11] and peephole connections [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "The front-end RNN is a deep unidirectional long short-term memory (LSTM) [10] network with forget gates [11] and peephole connections [12].", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "The front-end RNN is a deep unidirectional long short-term memory (LSTM) [10] network with forget gates [11] and peephole connections [12].", "startOffset": 134, "endOffset": 138}, {"referenceID": 12, "context": "Specifically, the RNN consists of three LSTM layers and a softmax output layer [13], and sequence-to-sequence trained with connectionist temporal classification (CTC) [2].", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "Specifically, the RNN consists of three LSTM layers and a softmax output layer [13], and sequence-to-sequence trained with connectionist temporal classification (CTC) [2].", "startOffset": 167, "endOffset": 170}, {"referenceID": 13, "context": "The network is similar to the one employed for the endto-end speech recognition [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "However, in our case, the network is unidirectional and trained with online CTC [3] on very long speech streams, instead of bidirectional LSTM networks with sequence-wise standard CTC training.", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "CTC [2] is one of the most successful sequence-to-sequence learning algorithms for RNNs.", "startOffset": 4, "endOffset": 7}, {"referenceID": 13, "context": "Recently, CTC based end-to-end trained RNNs have been reported to show impressive performance in automatic speech recognition [14], [15], which are comparable to state-of-the-art DNN-HMM models.", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "Recently, CTC based end-to-end trained RNNs have been reported to show impressive performance in automatic speech recognition [14], [15], which are comparable to state-of-the-art DNN-HMM models.", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "For the proposed keyword spotter, we employ the online CTC algorithm [3] to train continuously running unidirectional RNNs, which can process an infinitely long input speech without pre-segmentation.", "startOffset": 69, "endOffset": 72}, {"referenceID": 15, "context": "This enables efficient RNN training with multiple parallel training streams on a GPU with synchronized forward and backward steps [16] using the online backpropagation through time algorithm [17].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "This enables efficient RNN training with multiple parallel training streams on a GPU with synchronized forward and backward steps [16] using the online backpropagation through time algorithm [17].", "startOffset": 191, "endOffset": 195}, {"referenceID": 8, "context": "All speaker independent training utterances in WSJ corpus [9] except verbalized punctuation versions are used for training.", "startOffset": 58, "endOffset": 61}, {"referenceID": 0, "context": "filler model [1].", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "As a baseline, the performance of the DNN-HMM keyword spotter is also obtained, where the Gaussian mixture model (GMM) of the keyword-filler model in [1] is replaced by a DNN acoustic model [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "As a baseline, the performance of the DNN-HMM keyword spotter is also obtained, where the Gaussian mixture model (GMM) of the keyword-filler model in [1] is replaced by a DNN acoustic model [8].", "startOffset": 190, "endOffset": 193}, {"referenceID": 17, "context": "This is comparable to the human reaction time to speech stimuli, which is also roughly 200 ms as reported in [18].", "startOffset": 109, "endOffset": 113}], "year": 2015, "abstractText": "In this paper, we propose a context-aware keyword spotting model employing a character-level recurrent neural network (RNN) for spoken term detection in continuous speech. The RNN is end-toend trained with connectionist temporal classification (CTC) to generate the probabilities of character and word-boundary labels. There is no need for the phonetic transcription, senone modeling, or system dictionary in training and testing. Also, keywords can easily be added and modified by editing the text based keyword list without retraining the RNN. Moreover, the unidirectional RNN processes an infinitely long input audio streams without pre-segmentation and keywords are detected with low-latency before the utterance is finished. Experimental results show that the proposed keyword spotter significantly outperforms the deep neural network (DNN) and hidden Markov model (HMM) based keyword-filler model even with less computations.", "creator": "LaTeX with hyperref package"}}}