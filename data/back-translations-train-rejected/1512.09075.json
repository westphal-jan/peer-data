{"id": "1512.09075", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2015", "title": "A Notation for Markov Decision Processes", "abstract": "This paper specifies a notation for Markov decision processes.", "histories": [["v1", "Wed, 30 Dec 2015 19:34:01 GMT  (5kb,D)", "https://arxiv.org/abs/1512.09075v1", null], ["v2", "Thu, 8 Sep 2016 14:30:43 GMT  (7kb,D)", "http://arxiv.org/abs/1512.09075v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["philip s thomas", "billy okal"], "accepted": false, "id": "1512.09075"}, "pdf": {"name": "1512.09075.pdf", "metadata": {"source": "CRF", "title": "A Notation for Markov Decision Processes", "authors": ["Philip S. Thomas", "Billy Okal"], "emails": [], "sections": [{"heading": null, "text": "A notation for Markov decision-making Philip S. Thomas1 and Billy Okal21Carnegie Mellon University, 2Albert-Ludwigs-Universita \u00bc t Freiburg"}, {"heading": "1 Introduction", "text": "Many reinforcement learning (RL) research papers contain paragraphs that define Markov decision-making processes (MDPs). These paragraphs occupy space that could otherwise be used to present more useful content. In this paper, we set a notation for MDPs that can be used by other papers. Explaining the use of this notation with a single sentence can replace several paragraphs of notation specifications in other papers. It is important that the notation we define is a common basis that appears in many RL papers, and is not intended to be a complete notation for an entire paper.We refer to our notation as Markov Decision Process Notation, Version 1 or MDPNv1. It can be invoked in research papers with the sentence: \"We use the notation standard MDPNv1\" This sentence states that the notation specified in this document is Mmpision Process Notation Notation, Version 1 or MDPN1."}, {"heading": "2 Discrete and Continuous Random Variables", "text": "In general, the state, action and reward at the time t can be discrete or continuous. 3. Random variables or even a mixture of both. A discrete random variable, X, which records values in a sentence, X, has a random mass function (PMF), f: X \u2192 [0, 1], so that f (x) = Pr (X = x) for any x-arbitrary random variables (and random variables that are a mixture of discrete and continuous) are not characterized by a PMF. Although theoretical probability provides a uniform notation for discussing arbitrary random variables, their use in the affirmation of learning literature is not common, and so it can dilute and shrink the message of a paper. We therefore introduce an abuse of notation in MDPNv1: notoriously we treat the state, action and reward as if they are discrete variable, even if they are not expendable."}], "references": [{"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "It assumes that the reader is already familiar with the basic concepts of RL, as covered by Sutton and Barto (1998). Also, we try to minimize the number of assumptions that we make.", "startOffset": 92, "endOffset": 116}], "year": 2016, "abstractText": "Many reinforcement learning (RL) research papers contain paragraphs that define Markov decision processes (MDPs). These paragraphs take up space that could otherwise be used to present more useful content. In this paper we specify a notation for MDPs that can be used by other papers. Declaring the use this notation using a single sentence can replace several paragraphs of notational specifications in other papers. Importantly, the notation that we define is a common foundation that appears in many RL papers, and is not meant to be a complete notation for an entire paper. We refer to our notation as the Markov Decision Process Notation, version 1 or MDPNv1. It can be invoked in research papers with the sentence:", "creator": "LaTeX with hyperref package"}}}