{"id": "1512.09254", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2015", "title": "Evolving Non-linear Stacking Ensembles for Prediction of Go Player Attributes", "abstract": "The paper presents an application of non-linear stacking ensembles for prediction of Go player attributes. An evolutionary algorithm is used to form a diverse ensemble of base learners, which are then aggregated by a stacking ensemble. This methodology allows for an efficient prediction of different attributes of Go players from sets of their games. These attributes can be fairly general, in this work, we used the strength and style of the players.", "histories": [["v1", "Thu, 31 Dec 2015 10:37:04 GMT  (96kb)", "http://arxiv.org/abs/1512.09254v1", "Published in 2015 IEEE Symposium Series on Computational Intelligence"]], "COMMENTS": "Published in 2015 IEEE Symposium Series on Computational Intelligence", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["josef moud\\v{r}\\'ik", "roman neruda"], "accepted": false, "id": "1512.09254"}, "pdf": {"name": "1512.09254.pdf", "metadata": {"source": "CRF", "title": "Evolving Non-linear Stacking Ensembles for Prediction of Go Player Attributes", "authors": ["Josef Moud\u0159\u0131\u0301k", "Roman Neruda"], "emails": ["j.moudrik@gmail.com", "roman@cs.cas.cz"], "sections": [{"heading": null, "text": "ar Xiv: 151 2.09 254v 1 [cs.A I] 3 1D ecI. INTRODUCTIONThe field of computer Go is primarily focused on the problem of developing a program to play the game by finding the best move from a particular board position [1]. We focus on analyzing existing game records with the aim of helping people to play and understand the game better. In our previous work [2] we presented a way to extract information that is rich in characteristics from the games of Go players. This paper presents machine learning methods that we have developed to make full use of these extracted characteristics. We have used stack ensembles with non-linear learners of the second level. Both the members of the ensemble and the learners of the second level are selected by a genetic algorithm. The resulting model performs better than any single base learner (see Section III) in their own way, and also better than the best handcrafted ensemble."}, {"heading": "II. RELATED WORK", "text": "Ensemble approaches to machine learning have been in the attention of researchers for more than two decades, and various approaches have appeared over time. Some of them train a model based on differently collected data [4], as in the case of dredging [5] and the associated random forest algorithm [6]. Schemes for selective combinations of such models seem to be well understood [7]. For example, these ideas have recently been reintroduced in neural networks in the form of a dropout model [8]. Another approach to combining different models is [9], in which a (presumably weak) model is iteratively trained to specialize in hard spacing. Stacking [10], on the other hand, uses a two-layer approach, in which the second-level model learns to correct errors made by learners at the first level. For classification, various methods for forming traits from the first level of prediction have been proposed ([11], [12]), multi-layer linear regression at the second level."}, {"heading": "III. LEARNERS", "text": "Suppose we have a set of dataTr = {(x1, y1),.., (xN, yN)}, and we want to find a function r that is able to predict the value yi of xi with reasonable accuracy and generalize this dependence to invisible pairs. Machine learning methods presented here are considered as learners. For certain data Tr, the learner should output a regression function that performs the regression of the dependent variable, since he has learned from the data. Of course, some regression functions work better than others. This is mainly because each learner has different (innate) assumptions about the form of the function he is using."}, {"heading": "A. Base Learners", "text": "1) Mean Regression: is a very simple method that we use as a reference for comparing the performance of other learners. It simply displays the mean of the y's in the training set, and thus it is independent of the input method. The network consists of simple computational units organized in a layered topology, as described in a monograph [16]. We have used a simple neural network with 20 hidden units in a hidden layer. Neurons have default activation function and the network is trained using the RPROP algorithm [17] for most 100 iterations (or until the error is less than 0.001). In both datasets the domain of the respective target variables (strength, style) is specified."}, {"heading": "B. Ensemble Learners", "text": "The idea is to train a particular basic researcher on the basis of repetitions, and the basic researcher is trained on each of these levels. Regression simply uses the results from the resulting models as a basis. He discusses that this method is particularly useful for learners who are unstable - small disturbances in the data have a big impact on the resulting model. Aggregation of the bootstrapped models essentially results in the robustness of such models. Examples of learners are useful for learners who are unstable, and the small disturbances in the data have a big impact on learners."}, {"heading": "C. Evolving Stacking Ensembles", "text": "We have discussed that ensemble learning could be beneficial in terms of performance. For stacking it is desirable toAlgorithm 1: Stacking for Regression input: an ordered set of 1st Level learners ensemble, alevel 2 learners l2, training data tr, number of folds folds folds output: The Level 1 learners have opted for the Level 2 learners ensemble, alevel 2 learners l2, training data tr, number of folds folds folds output: The Level 1 learners have opted for the Level 2 learners. * / 1 L2Tr-2 learner indicators are used, 2 foreach (tr, ts)) in CrossValidation (tr, folds) do / * The Level 1 learners have onsplit tr. \"* / 3 L1 (ensemble0), (tr-2). The ensembles (tr-2)). 4 foreach (x) in ts.\" we do / * responsibilities of level-1 unseen x \"and the real answer y.\""}, {"heading": "IV. EVALUATING LEARNERS", "text": "In order to compare the performance of different regression functions (learners), we need a reliable measurement. The goal is to estimate the performance of a particular regression function based on real invisible data. We can estimate this performance by dividing the data into parts that are only used for training (Tr) and testing (Ts)."}, {"heading": "A. Cross-Validation", "text": "Cross-validation is a standard statistical method for estimating parameters. The idea is to divide the data into k-disjunct subsets (so-called folds) and then iteratively assemble the training and test sets and measure errors. In each of the k-th iterations, the k-th fold is selected as the test data, and all remaining k-1 folds form the training data. The folds are randomly divided so that the folds are approximately the same size (in cases where the number of samples | D | is not divisible by k, some folds are slightly smaller than others)."}, {"heading": "B. Error Analysis", "text": "A commonly used measure of performance is the mean square error (MSE), which estimates the variance of the error distribution. We use its square root (RMSE), which represents an estimate of the standard deviation of the predictions, RMSE = \u221a \u221a \u221a 1 | Ts | \u2211 (ev, y) \u0445 Ts (predict (ev) \u2212 y) 2, Algorithm 2: Genetic algorithm for determining the optimal stack semis input: Size of the population S, Size of the elite E, Probabilities of the PmM and Pmv mutation, Maximum number of steps Max output: The best individual.1 Pop \u2190 RandomPopulation (S); / * The best single person so far. * / 2 The best single person."}, {"heading": "V. EXPERIMENTS AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Strength", "text": "One of the two major domains we have tested is the prediction of player strength. In the game of Go, strength (amateur) is measured by kyu (student) and dan (master) ranks. kyu ranks drop from about 20 kyu (absolute beginner) to 1 kyu server (fairly strong player), the sample consists of over 100 000 records of games. The records are detailed by the strength of the player and the preprocessing. [2] It is enough to note that the data collection of the Kiseido Go server [23], the sample consists of over 100 records."}, {"heading": "B. Style", "text": "Apart from estimating the strength, we also chose the framework that was presented in this work to test the prediction of the game styles of professional players. The game style has various aspects, some of them are vaguely defined (for details see [24]). In addition, none of them has a clear definition in a mathematical sense. In order to grasp these terms at least approximately, we came up with four axes, the ends of which correspond to the opposing principles in traditional Go knowledge. Next, we used a questionnaire (which was submitted to domain experts) to find the style values for a number of well-known professional players from the 20th century. Style 1 10 Territoriality Moyo Territoriality Orthodoxity Classical Classics of the opposing aggressicism Classical Way we used a questionnaire (which was submitted to domain experts) to determine the style values for a number of well-known professional players from the 20th century. Style 1 10 Territoriality Territoriality of the Orthodoxity Classical Classicism of the opposing aggressicism ensemble includes the opposite classicism classicism of the classicism of the classicism of the classicism of the classicism."}, {"heading": "VI. DISCUSSION", "text": "The results in both areas show that the development of batch ensembles does not trivially improve on the performance of the best hand tunes ensemble, about 1.5% for the case of strength and 4% for the style domains (averaged across different styles) The resulting ensembles also show a lot of diversity. Clearly, the biggest disadvantage of the methods described is the time required - even for such relatively small sets of data - in rows of hours per iteration, the main cause of which is the cross-validation carried out at different levels, multiplying the complexity of the time. On the outer level, it is executed to get better error estimates. Cross-validation is also used in the inner loop during the training, and moreover in some of the base learners. In this extreme case, the complexity of the training of the base model is multiplied by the number of torture3."}, {"heading": "VII. CONCLUSION", "text": "In both areas, the algorithm outperforms other methods, with the disadvantage that it takes a relatively long time; solutions to this problem are proposed. VIII. IMPLEMENTATION The code used in this thesis is released online as part of the GoStyle project [27]. Most of the source code is implemented in the Python programming language [28]. Machine learning models were implemented and evaluated using the Orange Datamining Suite [29] and the Fast Artificial Neural Network library FANN [30]."}, {"heading": "Acknowledgment", "text": "This research was partially supported by project No. 15-18108S of the Czech Science Foundation. J. Moudr \u0432\u0430\u0432\u0430\u0441\u043a\u0430k was supported by project No. 364015 of Charles University and by project No. 260 224.1http: / / gostyle.j2m.cz."}], "references": [{"title": "Achieving master level play in 9x9 computer go", "author": ["S. Gelly", "D. Silver"], "venue": "AAAI\u201908: Proceedings of the 23rd national conference on Artificial intelligence. AAAI Press, 2008, pp. 1537\u20131540.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Evaluating go game records for prediction of player attributes", "author": ["J. Moud\u0159\u0131\u0301k", "P. Baudi\u0161", "R. Neruda"], "venue": "IEEE Computational Intelligence in Games 2015. IEEE, 2015, pp. 162\u2013168, in Print.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Meta-learning methods for analyzing go playing trends", "author": ["J. Moud\u0159\u0131\u0301k"], "venue": "Master\u2019s thesis, Charles University, Faculty of Mathematics and Physics, Prague, Czech Republic, 2013. [Online]. Available: http://www.j2m.cz/\u223cjm/master thesis.pdf", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural network ensembles", "author": ["L.K. Hansen", "P. Salamon"], "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence, no. 10, pp. 993\u20131001, 1990.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Mach. Learn., vol. 24, no. 2, pp. 123\u2013140, Aug. 1996. [Online]. Available: http://dx.doi.org/10.1023/A:1018054314350", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Random forests", "author": ["\u2014\u2014"], "venue": "Machine Learning, vol. 45, no. 1, pp. 5\u201332, Oct. 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "On combining classifiers", "author": ["J. Kittler", "M. Hatef", "R.P. Duin", "J. Matas"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 20, no. 3, pp. 226\u2013239, 1998.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R. Schapire"], "venue": "Computational Learning Theory, ser. Lecture Notes in Computer Science, vol. 904. Springer Berlin Heidelberg, 1995, pp. 23\u201337. [Online]. Available: http://dx.doi.org/10.1007/3-540-59119-2 166", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural Networks, vol. 5, pp. 241\u2013259, 1992. [Online]. Available: http://www.machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1992}, {"title": "Issues in stacked generalization", "author": ["K.M. Ting", "I.H. Witten"], "venue": "1999.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "An extensible meta-learning approach for scalable and accurate inductive learning", "author": ["P.K.-W. Chan"], "venue": "Ph.D. dissertation, Columbia University, 1996.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1996}, {"title": "Stacked regressions", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 24, pp. 49\u201364, 1996. [Online]. Available: http://dx.doi.org/10.1007/BF00117832", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1996}, {"title": "What is your playing style? [Online]. Available: http://style.baduk.com", "author": ["A. Dinerchtein"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Neural Networks: A Comprehensive Foundation (2nd Edition), 2nd ed", "author": ["S. Haykin"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm", "author": ["M. Riedmiller", "H. Braun"], "venue": "IEEE International Conference on Neural Networks, 1993, pp. 586\u2013591.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "Nearest neighbor pattern classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Transactions on Information Theory, vol. 13, no. 1, pp. 21\u201327, 1967.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1967}, {"title": "Overview and recent advances in partial least squares", "author": ["R. Rosipal", "N. Krmer"], "venue": "in Subspace, Latent Structure and Feature Selection Techniques, Lecture Notes in Computer Science. Springer, 2006, pp. 34\u201351. [Online]. Available: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.85.7735", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Classification and regression trees", "author": ["L. Breiman", "O.R.A.J.H. Friedman", "C.J. Stone"], "venue": "Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1984}, {"title": "A genetic algorithm tutorial", "author": ["D. Whitley"], "venue": "Statistics and computing, vol. 4, no. 2, pp. 65\u201385, 1994.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection.", "author": ["R. Kohavi"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1995}, {"title": "KGS archives \u2014 kiseido go", "author": ["W. Shubert"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "winter 2011) Games of Go on Disk \u2014 GoGoD Encyclopaedia and Database, Go players", "author": ["J. Fairbairn"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "winter 2011) Games of Go on Disk \u2014 GoGoD Encyclopaedia and Database", "author": ["T.M. Hall", "J. Fairbairn"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "On move pattern trends in a large go games corpus", "author": ["P. Baudi\u0161", "J. Moud\u0159\u0131\u0301k"], "venue": "Arxiv, CoRR, October 2012. [Online]. Available: http://arxiv.org/abs/1209.5251", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "GoStyle \u2014 Determine playing style in the game of Go", "author": ["J. Moud\u0159\u0131\u0301k", "P. Baudi\u0161"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Orange: Data mining toolbox in python", "author": ["J. Dem\u0161ar"], "venue": "Journal of Machine Learning Research, vol. 14, pp. 2349\u20132353, 2013. [Online]. Available: http://jmlr.org/papers/v14/demsar13a.html", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Implementation of a fast artificial neural network library (fann)", "author": ["S. Nissen"], "venue": "Department of Computer Science University of Copenhagen (DIKU), Tech. Rep., 2003, http://fann.sf.net.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "The field of computer Go is primarily focused on the problem of creating a program to play the game by finding the best move from a given board position [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "In our previous work [2], we have presented a way to extract information rich features from sets of Go players\u2019 games.", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "All these player attributes (various axes of style, and strength) can be mapped on a subset of real numbers [3].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "Some of them train one model on differently sampled data [4], as is the case of bagging [5] and the related random forests algorithm [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "Some of them train one model on differently sampled data [4], as is the case of bagging [5] and the related random forests algorithm [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "Some of them train one model on differently sampled data [4], as is the case of bagging [5] and the related random forests algorithm [6].", "startOffset": 133, "endOffset": 136}, {"referenceID": 6, "context": "The schemes for combining such models in a voting-like manner seems to be well understood [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "For example, in neural networks, these ideas have also recently been re-introduced in the form of a dropout [8].", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "Another approach to combine different models is boosting [9], where a (presumably weak) model is iteratively trained to specialize on hard instances.", "startOffset": 57, "endOffset": 60}, {"referenceID": 9, "context": "Stacking [10] on the other hand, uses a two-layered approach, where model on the second level learns to correct for mistakes that first level learners make.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "For classification, various ways of forming the features from the first level prediction have been proposed ([11], [12]), multi-response linear regression has been found to work well for second level learner.", "startOffset": 109, "endOffset": 113}, {"referenceID": 11, "context": "For classification, various ways of forming the features from the first level prediction have been proposed ([11], [12]), multi-response linear regression has been found to work well for second level learner.", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "For regression task such as ours, simple linear second level models have been proposed by Breiman [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "Prediction of Go player attributes has until recently been limited to pre-defined questionnaires and simple methods [14], [15].", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "Universal approach to the problem has been introduced by our previous work [2] and [3].", "startOffset": 75, "endOffset": 78}, {"referenceID": 2, "context": "Universal approach to the problem has been introduced by our previous work [2] and [3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 14, "context": "in a monograph [16].", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "The neurons have standard sigmoidal activation function and the network is trained using the RPROP algorithm [17] for at most 100 iterations (or until the error is smaller than 0.", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "3) k-Nearest Neighbor Regression: is another commonly used machine learning algorithm [18].", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "For a good overview, see the work [19].", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "5) Random Forests: utilize an ensemble of tree learners to predict the dependent value [6], [20].", "startOffset": 87, "endOffset": 90}, {"referenceID": 18, "context": "5) Random Forests: utilize an ensemble of tree learners to predict the dependent value [6], [20].", "startOffset": 92, "endOffset": 96}, {"referenceID": 5, "context": "See the Breiman\u2019s paper [6] for details.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "1) Bagging: is a simple ensemble method introduced in [5].", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "Paper [5] discusses, that this procedure is especially useful for learners bl which are unstable\u2014small perturbations in the data have big impact on the resulting model.", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "The original idea was pioneered by [10] and extended by [12], [13].", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "The original idea was pioneered by [10] and extended by [12], [13].", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "The original idea was pioneered by [10] and extended by [12], [13].", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "Genetic algorithms are an universal optimization tool, see [21] for a good tutorial.", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "Refer to [22] for details.", "startOffset": 9, "endOffset": 13}, {"referenceID": 21, "context": "Dataset: We have collected a large sample of games from the public archives of the Kiseido Go server [23], the sample consists of over 100 000 records of games.", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "divided by player\u2019s strength and preprocessed as is detailed in [2].", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "Here, it is enough to note that the dataset consists of 120 independent pairs (x, y) for each of the 26 ranks, where x is the feature vector described in [2] (dimension of x was 1040), and y is the target variable, which is one number describing the rank (1-26).", "startOffset": 154, "endOffset": 157}, {"referenceID": 22, "context": "Playing style has different aspects, some of them are vaguely defined (for details, see [24]).", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": "Dataset: The collection of games in this dataset comes from the Games of Go on Disk (GoGoD) database by [25].", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "[26] The experts (Alexander Dinerchtein 3-pro, Motoki Noguchi 7-dan, Vladim\u0131\u0301r Dan\u011bk 5-dan and V\u0131\u0301t Brunner 4-dan) were asked to value the players on four scales, each ranging from 1 to 10.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "For each of 24 professional players, we obtained 12 pairs (x, y), where x is the feature vector obtained from the games as described in [2] (dimension of x was 640), and y is one of the 4 styles\u2014basically, we view the problem as 4 different regression problems which share the same feature vectors.", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "The parameters of the models are omitted for brevity, see [3] for more.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "The prediction of player attributes as demonstrated in this work has been (together with the feature extraction presented in [2]) combined in an online web application1, which evaluates games submitted by players and predicts their playing strength and style.", "startOffset": 125, "endOffset": 128}, {"referenceID": 13, "context": "The first one is the tool of [14]\u2014the user answers 15 questions and based on the answers he gets one of predefined recommendations.", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "We believe that our approach is more precise, both because it takes into account many different aspects of the games [2], and because the methods presented in this paper are able to use the information well.", "startOffset": 117, "endOffset": 120}, {"referenceID": 25, "context": "The code used in this work is released online as a part of GoStyle project [27].", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "The machine learning models were implemented and evaluated using the Orange Datamining suite [29] and the Fast Artificial Neural Network library FANN [30].", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "The machine learning models were implemented and evaluated using the Orange Datamining suite [29] and the Fast Artificial Neural Network library FANN [30].", "startOffset": 150, "endOffset": 154}], "year": 2013, "abstractText": "The paper presents an application of non-linear stacking ensembles for prediction of Go player attributes. An evolutionary algorithm is used to form a diverse ensemble of base learners, which are then aggregated by a stacking ensemble. This methodology allows for an efficient prediction of different attributes of Go players from sets of their games. These attributes can be fairly general, in this work, we used the strength and style of the players.", "creator": "gnuplot 4.6 patchlevel 1"}}}