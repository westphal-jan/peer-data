{"id": "1601.00025", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2015", "title": "Write a Classifier: Predicting Visual Classifiers from Unstructured Text", "abstract": "People typically learn through exposure to visual concepts associated with linguistic descriptions. For instance, teaching visual object categories to children is often accompanied by descriptions in text or speech. In a machine learning context, these observations motivates us to ask whether this learning process could be computationally modeled to learn visual classifiers. More specifically, the main question of this work is how to utilize purely textual description of visual classes with no training images, to learn explicit visual classifiers for them. We propose and investigate two baseline formulations, based on regression and domain transfer that predict a linear classifier. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the linear classifier parameters for new classes. We also propose a generic kernelized models where a kernel classifier, in the form defined by the representer theorem, is predicted. The kernelized models allow defining any two RKHS kernel functions in the visual space and text space, respectively, and could be useful for other applications. We finally propose a kernel function between unstructured text descriptions that builds on distributional semantics, which shows an advantage in our setting and could be useful for other applications. We applied all the studied models to predict visual classifiers for two fine-grained categorization datasets, and the results indicate successful predictions of our final model against several baselines that we designed.", "histories": [["v1", "Thu, 31 Dec 2015 22:23:34 GMT  (1811kb,D)", "https://arxiv.org/abs/1601.00025v1", "Journal submission. arXiv admin note: text overlap witharXiv:1506.08529"], ["v2", "Wed, 28 Dec 2016 02:13:59 GMT  (2280kb,D)", "http://arxiv.org/abs/1601.00025v2", "(TPAMI) Transactions on Pattern Analysis and Machine Intelligence 2017"]], "COMMENTS": "Journal submission. arXiv admin note: text overlap witharXiv:1506.08529", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["mohamed elhoseiny", "ahmed elgammal", "babak saleh"], "accepted": false, "id": "1601.00025"}, "pdf": {"name": "1601.00025.pdf", "metadata": {"source": "CRF", "title": "Write a Classifier: Predicting Visual Classifiers from Unstructured Text", "authors": ["Mohamed Elhoseiny", "Babak Saleh"], "emails": [], "sections": [{"heading": null, "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "II. RELATED WORK", "text": "In recent years, the number of people who are able to stay in the US has multiplied, not only in the US, but also in other parts of the world. In recent years, the number of people who are able to stay in the US has increased significantly, both in the US and in Europe."}, {"heading": "III. PROBLEM DEFINITION", "text": "The information in our problem comes from two different domains: the visual domain and the textual domain, each designated by V and T. Similar to traditional visual learning problems, we receive training data in the form V = {(xi, li)} N, where xi an4image and li {1 \u00b7 \u00b7 \u00b7 Nsc} are the class names. We refer to the number of classes available in the training as Nsc, where sc indicates \"seen classes.\" As is usually done in the visual classification, we can learn Nsc binary one-on-all classifiers, one for each of these classes. Our goal is to be able to predict a classification for a new category based only on the classes learned and a textual description (s) of that category. To achieve this, the learning process must also include textual descriptions of the classes seen (as shown in Figure 2). Depending on the domain, we may find a few or few problem descriptions in that category."}, {"heading": "A. Linear Classifier", "text": "Consider a typical linear classifier in the attribute space in formfj (x) = c T j \u00b7 xwhere x (bold) is the visual attribute vector of an image x (not bold), supplemented with 1, and cj \u0441 Rdv is the linear classifier of class j. When a test image is created, its class is determined by T = arg max j fj (x) (1), where tj is the attribute of the text description tj (not bold). Let us describe the linearly extracted textual attribute with T = {tj \u0440Rdt} j = 1 \u00b7 \u00b7 Nsc, where tj is the attribute of the text description tj (not bold)."}, {"heading": "B. Kernel Classifier", "text": "For kernel classifiers, we assume that each of the domains is equipped with a kernel function that corresponds to a reproducing kernel q description (RKHS).Let's label the kernel for V with k (\u00b7 \u00b7 \u00b7 \u00b7) and the kernel for T with g (\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Description of the reproducing kernel Q class (RKHS).After the generalized representer theorem [14], we define a minimizer of a regulated empirical risk function using an RKHS text that could be represented as a linear combination of cores evaluated on the training set. Adopting the representer theorem to the classification risk function, we define a kernel classifier of a visual class j as a successor class j as a test image, xi is the ith image in the training data V, k (x) = [k (x1), \u00b7, k (k), k (k), k (k), k (k), k (k), x, T, x (x)."}, {"heading": "IV. RELATION TO REGRESSION AND KNOWLEDGE TRANSFER MODELS", "text": "We present two possible frameworks for this problem and discuss possible limitations for them. In this background section, we focus on predicting linear classifiers for simplicity, which motivates the linear classification formulations examined in Section V."}, {"heading": "A. Regression Models", "text": "A simple way to solve this problem is to present it as a regression problem where the goal is to use the textual data and the learned classifiers, i.e., a function c (\u00b7): Rdt \u2192 Rdv. The question is which regression model would be suitable for this problem? and would the problem lead to reasonable results in this way? A typical regression model, such as the ridge regression [43] or the Gaussian process (GP) regression [44], learns the regressor for each dimension of the output domain (the parameters of a linear classifier) separately, i.e. a set of functions ci (\u00b7): Rdt \u2192 R will not capture the correlation between the visual classification classes."}, {"heading": "B. Knowledge Transfer Models", "text": "An alternative formulation is to place the problem as a domain adaptation from the textual to the visual domain. In the context of computer vision, the domain adaptation work focuses on the transfer of categories learned from a source domain, with a given distribution of images into a target domain with a different distribution, e.g. images or videos from different sources [45], [46], [47], [48]. What we need is an approach that learns the correlation between the textual domain characteristics and the visual domain characteristics and uses this correlation to predict new visual classifiers in light of textual characteristics. In particular, [47] introduces an approach to the transformation of learning domains over the domain. In this work, a regulated asymmetric transformation between points in two domains is learned. The approach has been applied to transfer learned categories between different data distributions, both in the visual domain and in the visual domain."}, {"heading": "V. FORMULATIONS FOR PREDICTING A LINEAR", "text": "The proposed formulations in this section are aimed at predicting a linear hyperplane parameter c of a one-against-all classifier for a new invisible class. We begin by defining the learning components used by the formulations described in this section: Classifiers: A set of linear one-against-all classifiers is learned, one for each class seen. Probabilistic regressor: Given {(tj, cj)}, a regressor is learned that can be used to make a prior estimate for preg (c | t) (details in Sec V-A). Domain Transfer: Faced with T and V, a domain transfer function encoded in the W matrix is learned that captures the correlation between the textual and visual ranges (details in Sec V-A)."}, {"heading": "A. Probabilistic Regressor", "text": "There are various regressors that can be used, but we need a regressor that provides a probable estimate (c). (t) For the reasons explained in Sec III, we also need a structural predictive approach that is able to predict all dimensions of the classifiers together. (For these reasons, we use the Twin Gaussian Process (TGP). (i.e.) The relationships between the two input factors and structured outputs using Gaussian Process Priors are minimized by minimizing the Kullback-Leibler divergence between the outputs (i.e. classifiers in our case) and observations (i.e.) The estimated regressor output (c) outputs (t) in2The notations follow from Subsection III-A6 Pure textual description / on an unknown object class: Correla / on between text description / on and images: Learned on object categories during training:"}, {"heading": "B. Constrained Probabilistic Regressor", "text": "We also examined formulations that use regression to predict an initial hyperplane, as described in Section V-A, which is then optimized to put all the data seen into one page, i.e.: (t) = c (t) = argmin c, \u0441i [cTc + \u03b1\u043a (c, c (t)) + C N = 1 \u0441i] s.t.: \u2212 cTxi [i] = 0, i = 1, \u00b7 \u00b7 \u00b7, N, which is a similarity function between hyperplanes, e.g. a point product used in this paper, \u03b1 is its constant weight, and C is the weight on the soft constraints of existing images as negative examples (inspired by linear SVM formulation). We call this class of methods limited GPR / TGP, since c (t) is originally predicted by GPR or TGP.3notice, in order to address the output of regressorting problematics while we use the final output c."}, {"heading": "C. Domain Transfer (DT)", "text": "To learn the domain transfer function W, we have adjusted the approach in [47] as follows: Let T be the textual feature data matrix and X be the visual feature data matrix, where each feature vector is supplemented with a 1. Note that changing the feature vectors with a 1 in our formulation is essential, since we need tTW as a classifier. We have to solve the following optimization problem in W r (W) + \u03bb i ci (TWX T) (6), where ci's loss functions are above the constraints and r (\u00b7) is a matrix regulator. It was shown in [47] on the condition on the regulator that the optimal W is in the form of W * = TK \u2212 12 T L * K * K \u2212 12 X T, where KT = TTT, KX = XXT (\u00b7) is a matrix regulator."}, {"heading": "D. Constrained-DT", "text": "We also examined formulations with restricted DT that learn a transfer matrix W and force tTjW to be close to the classifiers learned on the basis of the seen data, {cj}, i.e. min W r (W) + \u03bb1 \u2211 i ci (TWX T) + \u03bb2 \u2211 j-cj \u2212 tTjW-2A classifier can then be obtained by \u03a6 (t) = c (t) = tT * W.7."}, {"heading": "E. Constrained Regression and Domain Transfer for classifier prediction", "text": "Figure 3 illustrates our final framework, which combines regression (formulation A (use of TGP) and domain transfer (formulation C) with additional constraints. \u2212 This formulation combines the three learning components described at the beginning of this section. \u2212 Each of these components contains partial knowledge of the problem. The question is how to combine this knowledge to predict a new classification function. \u2212 The new classification component must be compatible with the classes seen. \u2212 The new classifier must place all seen instances on one side of the hyperlevel and must be consistent with the learned domain transfer function. \u2212 This results in the following limited optimization function. \u2212 The new classifier must be compatible with the classes seen. \u2212 The new classifier must place all seen instances on one side of the hyperlevel and must be compatible with the learned domain transfer function. \u2212 The new classifier must be compatible with the following limited optimization function."}, {"heading": "VI. FORMULATIONS FOR PREDICTING A KERNEL", "text": "CLASSIFIER FORM (\u03a6 (t \u043a) = \u03b2 (t \u043a)) The prediction of \u03a6 (t \u043a) = \u03b2 (t \u043a) (para. III-B) is divided into training processes (domain transfer) and prediction phases, detailed as the following 4http: / / www-01.ibm.com / software / integration / optimization / cplex-optimizer"}, {"heading": "A. Kernelized Domain Transfer", "text": "During the training we first learn Bsc (x) > l, j = 1, j = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1, b = 1"}, {"heading": "B. Kernel Classifier Prediction", "text": "We examine two ways to derive the final kernel classification prediction. (1) Direct \u03b2 \u03b2 \u03b2-domain transfer prediction, referred to as \"DT kernel,\" (2) One-class SVM adapted DT prediction, referred to as \"SVM-DT kernel.\" Direct Domain Transfer (DT) prediction: By building a classifier of an unseen class, the source code can be calculated directly from our trained Domain Transfer Model. (t-vision-projects / write kernel classifier.Direct Domain Transfer (DT) prediction: By building a classifier of an unseen class, the source code can be calculated directly from our trained Domain Transfer Model."}, {"heading": "VII. DISTRIBUTIONAL SEMANTIC (DS) KERNEL FOR TEXT DESCRIPTIONS", "text": "We propose a distribution semantic kernel g (\u00b7, \u00b7) = gDS (\u00b7, \u00b7) to define the similarity between two text descriptions in the T domain. While this kernel is applicable to kernel classifier predictors presented in Sec VI, it could be used for other applications. We start with distributional semantic models in [41], [52] to represent the semantic manifold Ms, and a function vec (\u00b7) that paints a word to a K \u00b7 1 vector in Ms. The main assumption behind this class of distributional semantic models is that similar words share a similar context."}, {"heading": "VIII. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Datasets and Features", "text": "In fact, it is the case that most people who work for the rights of women and men work for the rights of men and women. (...) It is not as if they work for the rights of women. (...) It is as if they work for the rights of men. (...) It is as if they work for the rights of women. (...) It is as if they work for the rights of women. (...) It is as if they work for the rights of men. (...) It is as if they work for the rights of women. (...) It is as if they work for the rights of women. \"(...) It is as if they work for the rights of women. (...) It is as if they work for the rights of men. (...) It is as if they work for themselves. (...) It is as if they work for themselves. (...) It is as if they work for the rights of women."}, {"heading": "B. Experimental Results for Linear Classifier Prediction", "text": "In fact, most of us are able to put ourselves in a different world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they are able to put themselves into another world, in which they are able, in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in"}, {"heading": "C. Experimental Results for Kernel Classifier Prediction", "text": "We have two additional metrics that we all compare with other categories. \"\" We, \"it says in the message,\" have two additional metrics that we care about. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" \"We.\" \"\" We. \"\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"\". \"\" \"We.\" \"\" \"We.\" \"\" \"We.\". \"\" \"\" \"We.\". \"\" \"\" \"\" \"We.\". \"\" \"\" \".\" \"\" \"\" \"\" We. \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\" \"\" \".\" \"\" \"\". \".\". \".\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \".\" \"\" \".\" \"\" \".\" \"\" \"\" \".\" \"\" \"\". \"\". \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\". \"\" \"\" \"\". \"\". \"\" \"\" \".\" \"\" \".\". \"\" \"\". \"\" \".\". \"\" \"\". \".\" \"\". \"\" \"\" \".\" \".\". \"\" \"\" \".\" \"\". \"\". \"\". \".\". \"\" \"\". \"\" \".\". \"\". \"\" \"\". \".\". \"\" \".\". \".\" \"\". \"\". \"\" \"\". \"\" \"\". \".\" \"\" \".\" \"\" \".\" \"\". \"\" \".\" \".\" \"\" \".\" \"\" \".\" \".\". \"\" \"\". \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \".\" \".\" \"\" \"\". \"\" \"\". \"\". \"\" \"\". \"\". \"\". \"\". \"\" \"\" \"\" \".\" \".\" \"\" \"\". \".\". \"\""}, {"heading": "D. Multiple Representation Experiment and Distributional Semantic(DS) Kernel", "text": "In fact, we are able to move to another world, where we can move to another world, \"he said.\" We are very proud that we are able to change the world, \"he said.\" We have to be able to change the world, \"he said.\" We have to be able to change the world, \"he said."}, {"heading": "E. Experiments using deep image-sentence similarity and more recent Zero-shot learning methods", "text": "We used a state-of-the-art model [58] for image-sentence similarity by dividing each text document into sentences and treating it as a positive sentence for all images in the corresponding class. We then measured the similarities between an image and a class by calculating their similarity to all sentences in that class. Images were encoded using VGGNet [69], and the sentences were encoded using an RNN with GRU activation [33]. The MAU data set for these experiments yielded a MAU of 3.3%, which is better than the linear classifier in Table VII. However, our core method (Eq 15) still performs 2.03% better on deep features (i.e. 5.35% MAU)."}, {"heading": "F. SC-Seen Split on CUB 2011 [70]", "text": "We report on the zero-shot performance of SC lakes (super category lakes) splitting, detailed in subsection A. We applied both our linear and kernel methods and compared them with recently published results in our setting [57], [58], [71], [56]. We performed all experiments in the previous sections (the best zero-shot performance CUB dataset on SC-unseen split is 5.35% on SC-unseen split, designed in [13]. It is not difficult to see that the performance of our methods (both linear and kernel) is significantly better on SC-unseen split than on SC-unseen split, designed in [13]."}, {"heading": "IX. CONCLUSION", "text": "We investigated and experimented with different formulations of the problem within the fine-grained categorization context. First, we proposed a new formulation that captures information between the visual and textual domains by incorporating the knowledge transfer from textual characteristics to visual characteristics, which indirectly leads to the prediction of a linear visual classifier described by the text. We also proposed a new zero-shot learning method to predict kernel classifiers of invisible categories based on information from a privilege space. We formulated the problem as a domain transfer function from the text description into the visual classification space, with cores supported in both domains. We proposed a classified SVM adaptation of our domain transfer function to improve prediction. We validated the performance of our model through several experiments. We demonstrated the value of a kernelized version by using cores that were multiplied to achieve better results through learning KL."}], "references": [{"title": "Learning to share visual appearance for multiclass object detection", "author": ["R. Salakhutdinov", "A. Torralba", "J.B. Tenenbaum"], "venue": "CVPR, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "The caltech-ucsd birds-200-2011 dataset", "author": ["C. Wah", "S. Branson", "P. Welinder", "P. Perona", "S. Belongie"], "venue": "2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Avibase", "author": ["D. Lepage"], "venue": "http://avibase.bsc-eoc.org/, 2016, [Online; accessed 19-July-2016].", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "All About Birds16", "author": ["S. Belongie", "P. Perona", "G. Van Horn", "S.B."], "venue": "info.allaboutbirds.org/nabirds, 2016, [Online; accessed 31-July-2016].", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "What does classifying more than 10,000 image categories tell us?", "author": ["J. Deng", "A.C. Berg", "K. Li", "L. Fei-Fei"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "A bayesian approach to unsupervised one-shot learning of object categories", "author": ["L. Fe-Fei", "R. Fergus", "P. Perona"], "venue": "CVPR. IEEE, 2003, pp. 1134\u20131141.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Cross-generalization: Learning novel classes from a single example by feature replacement", "author": ["E. Bart", "S. Ullman"], "venue": "CVPR, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning to detect unseen object classes by betweenclass attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "In CVPR, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Automated flower classification over large number of classes", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "ICVGIP, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning models for object recognition from natural language descriptions.", "author": ["J. Wang", "K. Markert", "M. Everingham"], "venue": "in BMVC,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Dog breed classification using part localization", "author": ["J. Liu", "A. Kanazawa", "D. Jacobs", "P. Belhumeur"], "venue": "ECCV, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Finegrained visual classification of aircraft", "author": ["S. Maji", "E. Rahtu", "J. Kannala", "M. Blaschko", "A. Vedaldi"], "venue": "arXiv preprint arXiv:1306.5151, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Write a classifier: Zero shot learning using purely text descriptions", "author": ["M. Elhoseiny", "B. Saleh", "A. Elgammal"], "venue": "ICCV, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A generalized representer theorem", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": "COLT, 2001.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Caltech-UCSD Birds 200", "author": ["P. Welinder", "S. Branson", "T. Mita", "C. Wah", "F. Schroff", "S. Belongie", "P. Perona"], "venue": "California Institute of Technology, Tech. Rep., 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning from one example through shared densities on transforms", "author": ["E.G. Miller", "N.E. Matsakis", "P.A. Viola"], "venue": "CVPR, 2000.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Object classification from a single example utilizing class relevance metrics", "author": ["M. Fink"], "venue": "NIPS, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Zeroshot learning with semantic output codes", "author": ["M. Palatucci", "D. Pomerleau", "G.E. Hinton", "T.M. Mitchell"], "venue": "NIPS, 2009.  14", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D.A. Forsyth"], "venue": "CVPR, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Combining language sources and robust semantic relatedness for attribute-based knowledge transfer", "author": ["M. Rohrbach", "M. Stark", "G. Szarvas", "B. Schiele"], "venue": "Parts and Attributes Workshop at ECCV, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Interactively building a discriminative vocabulary of nameable attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "CVPR, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning hypergraph-regularized attribute predictors", "author": ["S. Huang", "M. Elhoseiny", "A. Elgammal", "D. Yang"], "venue": "CVPR, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning and using taxonomies for fast visual categorization", "author": ["G. Griffin", "P. Perona"], "venue": "CVPR. IEEE, 2008, pp. 1\u20138.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Semantic label sharing for learning with many categories", "author": ["R. Fergus", "H. Bernal", "Y. Weiss", "A. Torralba"], "venue": "ECCV, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Wordnet: A lexical database for english", "author": ["G.A. Miller"], "venue": "COMMUNI- CATIONS OF THE ACM, 1995.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR, 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["A. Torralba", "R. Fergus", "W.T. Freeman"], "venue": "PAMI, 2008.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Clustering art", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth"], "venue": "CVPR, 2001.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "ECCV, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y. Yang", "C.L. Teo", "H. Daum\u00e9 III", "Y. Aloimonos"], "venue": "EMNLP, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Generating natural-language video descriptions using text-mined knowledge", "author": ["N. Krishnamoorthy", "G. Malkarnenkar", "R. Mooney", "K. Saenko", "U. Lowell", "S. Guadarrama"], "venue": "NAACL HLT 2013, p. 10, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "ICML, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. Yuille"], "venue": "ICLR, 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "NIPS, 2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Devise: A deep visual-semantic embedding model.", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "M. Ranzato", "T. Mikolov"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Zero shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "H. Sridhar", "O. Bastani", "C.D. Manning", "A.Y. Ng"], "venue": "NIPS, 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, 2013.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Zeroshot event detection by multimodal distributional semantic embedding of videos", "author": ["M. Elhoseiny", "J. Liu", "H. Cheng", "H. Sawhney", "A. Elgammal"], "venue": "AAAI, 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Ridge Regression: Biased Estimation for Nonorthogonal Problems", "author": ["A.E. Hoerl", "R.W. Kennard"], "venue": "Technometrics, 1970.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1970}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2005}, {"title": "Cross-domain video concept detection using adaptive svms", "author": ["J. Yang", "R. Yan", "A.G. Hauptmann"], "venue": "MULTIMEDIA, 2007.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "ECCV, 2010.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "CVPR, 2011.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Visual event recognition in videos by learning from web data", "author": ["L. Duan", "D. Xu", "I.W.-H. Tsang", "J. Luo"], "venue": "TPAMI, 2012.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Twin gaussian processes for structured prediction", "author": ["L. Bo", "C. Sminchisescu"], "venue": "IJCV, 2010.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallel Optimization: Theory, Algorithms, and Applications", "author": ["Y. Censor", "S. Zenios"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1997}, {"title": "Some properties of the gaussian kernel for one class learning", "author": ["P.F. Evangelista", "M.J. Embrechts", "B.K. Szymanski"], "venue": "ICANN, 2007.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR, 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "IPM, 1988.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1988}, {"title": "Clsi: A flexible approximation scheme from clustered term-document matrices", "author": ["D. Zeimpekis", "E. Gallopoulos"], "venue": "SDM, 2005.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient object category recognition using classemes", "author": ["L. Torresani", "M. Szummer", "A. Fitzgibbon"], "venue": "ECCV, 2010.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluation of output embeddings for fine-grained image classification", "author": ["Z. Akata", "S. Reed", "D. Walter", "H. Lee", "B. Schiele"], "venue": "CVPR, 2015.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Less is more: zeroshot learning from online textual documents with noise suppression", "author": ["R. Qiao", "L. Liu", "C. Shen", "A. v. d. Hengel"], "venue": "CVPR, 2016.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Order-embeddings of images and language", "author": ["I. Vendrov", "R. Kiros", "S. Fidler", "R. Urtasun"], "venue": "ICLR, 2016.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["M. Norouzi", "T. Mikolov", "S. Bengio", "Y. Singer", "J. Shlens", "A. Frome", "G.S. Corrado", "J. Dean"], "venue": "ICLR, 2014.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple kernel learning algorithms", "author": ["M. Gonen", "E. Alpaydin"], "venue": "JMLR, 2011.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernel descriptors for visual recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "NIPS, 2010.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-label multiple kernel learning by stochastic approximation: Application to visual object recognition.", "author": ["S.S. Bucak", "R. Jin", "A.K. Jain"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2010}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "ACM Multimedia, 2014.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems (NIPS), 2012.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2012}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "ICML, 2014.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2014}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["C. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "TPAMI, vol. 36, no. 3, pp. 453\u2013465, March 2014.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2014}, {"title": "Video event recognition using concept attributes", "author": ["J. Liu", "Q. Yu", "O. Javed", "S. Ali", "A. Tamrakar", "A. Divakaran", "H. Cheng", "H. Sawhney"], "venue": "WACV, 2013.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "CVPR, 2011.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR, 2015.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2015}, {"title": "Label-embedding for attribute-based classification", "author": ["Z. Akata", "F. Perronnin", "Z. Harchaoui", "C. Schmid"], "venue": "CVPR, 2013.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "This is reflected in the number of images per category available for training in most object categorization datasets, which, as pointed out in [1], shows a Zipf distribution.", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": ", CUBirds 200 dataset [2]).", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": ", [3], [4]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": ", [3], [4]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": "[5], [6], [7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5], [6], [7].", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "[5], [6], [7].", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Diverse applications of fine-grained categories range from classification of natural species [2], [9], [10], [11] to retrieval of different types of commercial products [12].", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "Diverse applications of fine-grained categories range from classification of natural species [2], [9], [10], [11] to retrieval of different types of commercial products [12].", "startOffset": 98, "endOffset": 101}, {"referenceID": 9, "context": "Diverse applications of fine-grained categories range from classification of natural species [2], [9], [10], [11] to retrieval of different types of commercial products [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "Diverse applications of fine-grained categories range from classification of natural species [2], [9], [10], [11] to retrieval of different types of commercial products [12].", "startOffset": 109, "endOffset": 113}, {"referenceID": 11, "context": "Diverse applications of fine-grained categories range from classification of natural species [2], [9], [10], [11] to retrieval of different types of commercial products [12].", "startOffset": 169, "endOffset": 173}, {"referenceID": 12, "context": "The contribution of the paper is on exploring this new problem, which to the best of our knowledge, is firstly explored in the computer vision community in an earlier version of this work [13].", "startOffset": 188, "endOffset": 192}, {"referenceID": 12, "context": "While a part of this work was published in [13], we extend the work here to study more formulations to solve the problem in Sec.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "In addition, we propose a kernel method to explicitly predict a kernel classifier in the form defined in the representer theorem [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "Section VI presents a kernelized version of our approach where \u03a6(\u00b7) predicts a kernel classifier in the form defined by the representer theorem [14].", "startOffset": 144, "endOffset": 148}, {"referenceID": 8, "context": "Section VIII presents our experiments on Flower Dataset [9] and Caltech-UCSD dataset [15] for both the linear and the kernel classifier predictions.", "startOffset": 56, "endOffset": 59}, {"referenceID": 14, "context": "Section VIII presents our experiments on Flower Dataset [9] and Caltech-UCSD dataset [15] for both the linear and the kernel classifier predictions.", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "Zero/Few-Shot Learning: Motivated by the practical need to learn visual classifiers of rare categories, researchers have explored approaches for learning from a single image (oneshot learning [16], [6], [17], [7]) or even from no images (zero-shot learning).", "startOffset": 192, "endOffset": 196}, {"referenceID": 5, "context": "Zero/Few-Shot Learning: Motivated by the practical need to learn visual classifiers of rare categories, researchers have explored approaches for learning from a single image (oneshot learning [16], [6], [17], [7]) or even from no images (zero-shot learning).", "startOffset": 198, "endOffset": 201}, {"referenceID": 16, "context": "Zero/Few-Shot Learning: Motivated by the practical need to learn visual classifiers of rare categories, researchers have explored approaches for learning from a single image (oneshot learning [16], [6], [17], [7]) or even from no images (zero-shot learning).", "startOffset": 203, "endOffset": 207}, {"referenceID": 6, "context": "Zero/Few-Shot Learning: Motivated by the practical need to learn visual classifiers of rare categories, researchers have explored approaches for learning from a single image (oneshot learning [16], [6], [17], [7]) or even from no images (zero-shot learning).", "startOffset": 209, "endOffset": 212}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Recognition can then proceed on the basis of these learned attributes [8], [19].", "startOffset": 70, "endOffset": 73}, {"referenceID": 18, "context": "Recognition can then proceed on the basis of these learned attributes [8], [19].", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "Typically attributes [8], [19] are manually defined by humans to describe shape, color, surface material, e.", "startOffset": 21, "endOffset": 24}, {"referenceID": 18, "context": "Typically attributes [8], [19] are manually defined by humans to describe shape, color, surface material, e.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "[20] investigated extracting useful attributes from large text corpora.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In [21], an approach was introduced for interactively defining a vocabulary of attributes that are both human understandable and visually discriminative.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "[22] relaxed the attribute independence assumption by modeling correlation between attributes to achieve better zero shot performance, as opposed to prior models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": ", [8], [19]), in our work we do not use any explicit attributes.", "startOffset": 2, "endOffset": 5}, {"referenceID": 18, "context": ", [8], [19]), in our work we do not use any explicit attributes.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "[23]; or exporting semantic knowledge at the level of category similarities and hierarchies, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24], [1].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[24], [1].", "startOffset": 6, "endOffset": 9}, {"referenceID": 4, "context": "For example in [5], it was shown that there is a strong correlation between semantic similarity between classes, based on WordNet, and confusion between classes.", "startOffset": 15, "endOffset": 18}, {"referenceID": 24, "context": "Linguistic semantics in terms of nouns from WordNet [25] have been used in collecting large-scale image datasets such as ImageNet[26] and Tiny Images [27].", "startOffset": 52, "endOffset": 56}, {"referenceID": 25, "context": "Linguistic semantics in terms of nouns from WordNet [25] have been used in collecting large-scale image datasets such as ImageNet[26] and Tiny Images [27].", "startOffset": 129, "endOffset": 133}, {"referenceID": 26, "context": "Linguistic semantics in terms of nouns from WordNet [25] have been used in collecting large-scale image datasets such as ImageNet[26] and Tiny Images [27].", "startOffset": 150, "endOffset": 154}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[28], which showed that learning a joint distribution of words and visual elements facilitates clustering the images in a semantic way, generating illustrative images from a caption, and generating annotations for novel images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29], [30], [31], [32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[29], [30], [31], [32].", "startOffset": 6, "endOffset": 10}, {"referenceID": 30, "context": "[29], [30], [31], [32].", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "[29], [30], [31], [32].", "startOffset": 18, "endOffset": 22}, {"referenceID": 32, "context": ", [33]), impressive works has been recently proposed for image captioning (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 33, "context": ", [34], [35], [36], [37]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 34, "context": ", [34], [35], [36], [37]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 35, "context": ", [34], [35], [36], [37]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": ", [34], [35], [36], [37]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 37, "context": "[38].", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "In [39], [40], word embedding language models (e.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "In [39], [40], word embedding language models (e.", "startOffset": 9, "endOffset": 13}, {"referenceID": 40, "context": "[41]) were adopted to represent class names as vectors, which require training using a big text-corpus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "In [42], a similar yet multimodal approach was adopted for Multimedia Event Detection in videos instead of object classification.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "According to the generalized representer theorem [14], a minimizer of a regularized empirical risk function over an RKHS could be represented as a linear combination of kernels,", "startOffset": 49, "endOffset": 53}, {"referenceID": 42, "context": "A typical regression model, such as ridge regression [43] or Gaussian Process (GP) Regression [44], learns the regressor to each dimension of the output domain (the parameters of a linear classifier) separately, i.", "startOffset": 53, "endOffset": 57}, {"referenceID": 43, "context": "A typical regression model, such as ridge regression [43] or Gaussian Process (GP) Regression [44], learns the regressor to each dimension of the output domain (the parameters of a linear classifier) separately, i.", "startOffset": 94, "endOffset": 98}, {"referenceID": 44, "context": ", images or videos from different sources [45], [46], [47], [48].", "startOffset": 42, "endOffset": 46}, {"referenceID": 45, "context": ", images or videos from different sources [45], [46], [47], [48].", "startOffset": 48, "endOffset": 52}, {"referenceID": 46, "context": ", images or videos from different sources [45], [46], [47], [48].", "startOffset": 54, "endOffset": 58}, {"referenceID": 47, "context": ", images or videos from different sources [45], [46], [47], [48].", "startOffset": 60, "endOffset": 64}, {"referenceID": 46, "context": "In particular, in [47] an approach for learning cross domain transformation was introduced.", "startOffset": 18, "endOffset": 22}, {"referenceID": 46, "context": "A particular attractive characteristic of [47], over other domain adaptation models, is that the source and target domains do not have to share the same feature spaces or the same dimensionality.", "startOffset": 42, "endOffset": 46}, {"referenceID": 46, "context": "While a totally different setting is studied in [47], it inspired us to formulate the zero-shot learning problem as a domain transfer problem.", "startOffset": 48, "endOffset": 52}, {"referenceID": 48, "context": "For these reasons, we use the Twin Gaussian Process (TGP) [49].", "startOffset": 58, "endOffset": 62}, {"referenceID": 48, "context": "TGP is given by the solution of the following non-linear optimization problem [49] 3.", "startOffset": 78, "endOffset": 82}, {"referenceID": 48, "context": "This optimization problem can be solved using a second order, BFGS quasi-Newton optimizer with cubic polynomial line search for optimal step size selection [49].", "startOffset": 156, "endOffset": 160}, {"referenceID": 46, "context": "To learn the domain transfer function W we adapted the approach in [47] as follows.", "startOffset": 67, "endOffset": 71}, {"referenceID": 46, "context": "It was shown in [47], under condition on the regularizer, that the optimal W is in the form of W\u2217 =", "startOffset": 16, "endOffset": 20}, {"referenceID": 49, "context": "Another approach that can be used to minimize L(\u03a8) is through alternating projection using Bregman algorithm [50], where \u03a8 is updated by a single constraint every iteration.", "startOffset": 109, "endOffset": 113}, {"referenceID": 50, "context": "The Lagrangian dual of the one-class SVM [51] can be written as", "startOffset": 41, "endOffset": 45}, {"referenceID": 40, "context": "We start by distributional semantic models in [41], [52] to represent the semantic manifold Ms, and a function vec(\u00b7) that maps a word to a K\u00d71 vector inMs.", "startOffset": 46, "endOffset": 50}, {"referenceID": 51, "context": "We start by distributional semantic models in [41], [52] to represent the semantic manifold Ms, and a function vec(\u00b7) that maps a word to a K\u00d71 vector inMs.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Datasets: We evaluated our methods using two large datasets, widely used for fine-grained categorization: CU200 Birds [15] dataset (200 classes - 6033 images) and the Oxford Flower102 [9] dataset (102 classes - 8189 images).", "startOffset": 118, "endOffset": 122}, {"referenceID": 8, "context": "Datasets: We evaluated our methods using two large datasets, widely used for fine-grained categorization: CU200 Birds [15] dataset (200 classes - 6033 images) and the Oxford Flower102 [9] dataset (102 classes - 8189 images).", "startOffset": 184, "endOffset": 187}, {"referenceID": 52, "context": "We used the normalized frequency of a term in the given textual description [53].", "startOffset": 76, "endOffset": 80}, {"referenceID": 52, "context": "The inverse document frequency is a measure of whether the term is common; in this work we used the standard logarithmic idf [53].", "startOffset": 125, "endOffset": 129}, {"referenceID": 53, "context": "The second phase is a dimensionality reduction step, in which Clustered Latent Semantic Indexing (CLSI) algorithm [54] is used.", "startOffset": 114, "endOffset": 118}, {"referenceID": 54, "context": "Visual features Extraction: We used the Classemes features [55] as the visual feature for our experiments, where they provide an intermediate semantic representation of the input image.", "startOffset": 59, "endOffset": 63}, {"referenceID": 54, "context": "Classemes features are output of a set of classifiers corresponding to a set of C category labels, which are drawn from an appropriate term list defined in [55], and not related to our textual features.", "startOffset": 156, "endOffset": 160}, {"referenceID": 54, "context": ") is extracted, a subset of feature dimensions was selected [55], and a one-versus-all classifier \u03c6c is trained for each category.", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "This is Zeroshot setting split for both CUB and Flower Datasets (first defined in our work [13]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 55, "context": "Super Category Seen (SC-Seen) Split a (150-50) Split on CUB 2011 dataset [56]: We also evaluate our work on another zero-shot learning split for CUB 2011 dataset, which is used in some recent works (e.", "startOffset": 73, "endOffset": 77}, {"referenceID": 55, "context": "g, [56], [57]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 56, "context": "g, [56], [57]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 55, "context": "We investigated the difference between this training/testing split and found that most of the unseen/test classes in split defined in [56] are actually seen in some-perspective.", "startOffset": 134, "endOffset": 138}, {"referenceID": 43, "context": "Namely we used: 1) A Gaussian Process Regressor (GPR) [44], 2) Twin Gaussian Process (TGP) [49] as a structured regression method, 3) Domain Transfer (DT) [47].", "startOffset": 54, "endOffset": 58}, {"referenceID": 48, "context": "Namely we used: 1) A Gaussian Process Regressor (GPR) [44], 2) Twin Gaussian Process (TGP) [49] as a structured regression method, 3) Domain Transfer (DT) [47].", "startOffset": 91, "endOffset": 95}, {"referenceID": 46, "context": "Namely we used: 1) A Gaussian Process Regressor (GPR) [44], 2) Twin Gaussian Process (TGP) [49] as a structured regression method, 3) Domain Transfer (DT) [47].", "startOffset": 155, "endOffset": 159}, {"referenceID": 59, "context": "We conducted an experiment where the final kernel on the visual domain is produced by Multiple Kernel Learning [60].", "startOffset": 111, "endOffset": 115}, {"referenceID": 60, "context": "We specifically used four types of kernels introduced by [61] as follows: Gradient Match Kernels that captures image variation based on predefined kernels on image", "startOffset": 57, "endOffset": 61}, {"referenceID": 61, "context": "We learn these weights by applying Bucak\u2019s Multiple Kernel Learning algorithm [62].", "startOffset": 78, "endOffset": 82}, {"referenceID": 62, "context": "We used caffe [63] implementation of [64].", "startOffset": 14, "endOffset": 18}, {"referenceID": 63, "context": "We used caffe [63] implementation of [64].", "startOffset": 37, "endOffset": 41}, {"referenceID": 64, "context": "We found this consistent with the results of [65] over different CNN layers.", "startOffset": 45, "endOffset": 49}, {"referenceID": 40, "context": "In this DS experiment, we used the distributional semantic model by [41] trained on GoogleNews corpus (100 billion words) resulting in a vocabulary of size 3 million words, and word vectors of K = 300 dimensions.", "startOffset": 68, "endOffset": 72}, {"referenceID": 58, "context": "We also applied the zero shot learning approach in [59] which has the lowest performance in our settings; see Table VII.", "startOffset": 51, "endOffset": 55}, {"referenceID": 65, "context": "In particular, we applied the DAP attribute-based model [66], [8] to the Birds dataset, which is widely adopted in many applications (e.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "In particular, we applied the DAP attribute-based model [66], [8] to the Birds dataset, which is widely adopted in many applications (e.", "startOffset": 62, "endOffset": 65}, {"referenceID": 66, "context": ", [67], [68]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 67, "context": ", [67], [68]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 65, "context": "8% multiclass accuracy using DAP approach [66].", "startOffset": 42, "endOffset": 46}, {"referenceID": 65, "context": "Most importantly, we achieved these results without learning any attribute classifiers, as in [66].", "startOffset": 94, "endOffset": 98}, {"referenceID": 57, "context": "20 % Order Embedding [58] 3.", "startOffset": 21, "endOffset": 25}, {"referenceID": 58, "context": "65 % [59] 2.", "startOffset": 5, "endOffset": 9}, {"referenceID": 7, "context": "7 % Lampert DAP [8] 68.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "03 % Lampert DAP [8] 4.", "startOffset": 17, "endOffset": 20}, {"referenceID": 57, "context": "We used a state of the art model [58] for image-sentence similarity by breaking down each text document into sentences and considering it as a positive sentence for all images in the corresponding class.", "startOffset": 33, "endOffset": 37}, {"referenceID": 68, "context": "Images were encoded using VGGNet [69] and sentences were encoded by an RNN with GRU activations [33].", "startOffset": 33, "endOffset": 37}, {"referenceID": 32, "context": "Images were encoded using VGGNet [69] and sentences were encoded by an RNN with GRU activations [33].", "startOffset": 96, "endOffset": 100}, {"referenceID": 69, "context": "SC-Seen Split on CUB 2011 [70]", "startOffset": 26, "endOffset": 30}, {"referenceID": 56, "context": "We applied both our linear and kernel method and compare against recently published results in our setting [57], [58], [71], [56].", "startOffset": 107, "endOffset": 111}, {"referenceID": 57, "context": "We applied both our linear and kernel method and compare against recently published results in our setting [57], [58], [71], [56].", "startOffset": 113, "endOffset": 117}, {"referenceID": 55, "context": "We applied both our linear and kernel method and compare against recently published results in our setting [57], [58], [71], [56].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "35% on SC-Unseen split designed in [13].", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "It is not hard to see that the performance of our methods (both linear and kernel) on SC-Seen split is significantly better than SC-Unseen split designed in [13].", "startOffset": 157, "endOffset": 161}, {"referenceID": 56, "context": "02 % Less is more [57] CVPR 2016 29.", "startOffset": 18, "endOffset": 22}, {"referenceID": 57, "context": "0 % Order Embedding [58] ICLR 2016 17.", "startOffset": 20, "endOffset": 24}, {"referenceID": 55, "context": "[56] CVPR 2015 with Word2vec 23.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[56] CVPR 2015 with GloVE 28.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "People typically learn through exposure to visual concepts associated with linguistic descriptions. For instance, teaching visual object categories to children is often accompanied by descriptions in text or speech. In a machine learning context, these observations motivates us to ask whether this learning process could be computationally modeled to learn visual classifiers. More specifically, the main question of this work is how to utilize purely textual description of visual classes with no training images, to learn explicit visual classifiers for them. We propose and investigate two baseline formulations, based on regression and domain transfer, that predict a linear classifier. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the parameters of a linear classifier. We also propose a generic kernelized models where a kernel classifier is predicted in the form defined by the representer theorem. The kernelized models allow defining and utilizing any two RKHS kernel functions in the visual space and text space, respectively. We finally propose a kernel function between unstructured text descriptions that builds on distributional semantics, which shows an advantage in our setting and could be useful for other applications. We applied all the studied models to predict visual classifiers on two fine-grained and challenging categorization datasets (CU Birds and Flower Datasets), and the results indicate successful predictions of our final model over several baselines that we designed.", "creator": "LaTeX with hyperref package"}}}