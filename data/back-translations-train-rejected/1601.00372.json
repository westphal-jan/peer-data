{"id": "1601.00372", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jan-2016", "title": "Mutual Information and Diverse Decoding Improve Neural Machine Translation", "abstract": "Sequence-to-sequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., $p(y|x)$, an objective that ignores other potentially useful sources of information. We introduce an alternative objective function for neural MT that maximizes the mutual information between the source and target sentences, modeling the bi-directional dependency of sources and targets. We implement the model with a simple re-ranking method, and also introduce a decoding algorithm that increases diversity in the N-best list produced by the first pass. Applied to the WMT German/English and French/English tasks, both mechanisms offer a consistent performance boost on both standard LSTM and attention-based neural MT architectures. The result is the best published performance for a single (non-ensemble) neural MT system, as well as the potential application of our diverse decoding algorithm to other NLP re-ranking tasks.", "histories": [["v1", "Mon, 4 Jan 2016 03:04:05 GMT  (106kb,D)", "https://arxiv.org/abs/1601.00372v1", null], ["v2", "Tue, 22 Mar 2016 21:15:30 GMT  (109kb,D)", "http://arxiv.org/abs/1601.00372v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["jiwei li", "dan jurafsky"], "accepted": false, "id": "1601.00372"}, "pdf": {"name": "1601.00372.pdf", "metadata": {"source": "CRF", "title": "Mutual Information and Diverse Decoding Improve Neural Machine Translation", "authors": ["Jiwei Li"], "emails": ["jiweil@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": null, "text": "Sequence-to-sequence models for neural translation learn semantic and syntactic relationships between sentence pairs by optimizing the probability of the target with respect to the source, i.e. p (y | x), a target that ignores other potentially useful information sources. We introduce an alternative objective function for neural MT that maximizes the mutual information between source and target sentence and models the bidirectional dependence on sources and targets. We implement the model using a simple re-ranking method and also implement a decryption algorithm that increases the diversity in the N-ranking generated by the first pass. Applied to the WMT tasks in German / English and French / English, the proposed models offer a consistent increase in performance for both standard LSTM and attention-based neural MT architectures."}, {"heading": "1 Introduction", "text": "The sequence-to-sequence machine translation models (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their ability to learn semantic and syntactical relationships between sequence pairs that capture contextual dependencies in a more continuous manner than phrase-based SMT approaches. SEQ2SEQ models require minimal domain knowledge, can be trained end-to-end, have much smaller memory tables required for sequence-based SMT, and achieve state-of-the-art performance in large tasks such as English to French (Luong et al., 2015b) and English to German (Luong et al.)."}, {"heading": "2 Related Work", "text": "This work refers to three previous lines of research: SEQ2SEQ models, mutual information modeling, and promoting translation diversity. SEQ2SEQ models SEQ2SEQ models show source sequences of vector space representations, from which a target sequence is then generated. They perform well in a variety of NLP generation tasks, including contextual response generation (Vinyals and Le, 2015; Serban et al., 2015). A neural machine translation system uses distributed representations to model conditions. As shown in (Li et al., 2015) logs p (x) p (y) p (y) \u2212 x."}, {"heading": "3 Background: Neural Machine Translation", "text": "Neural machine translation models assign the source x = {x1, x2,... xNx} to a continuous vector representation from which the target output y = {y1, y2,..., yNy} is to be generated."}, {"heading": "3.1 LSTM Models", "text": "A long-term short-term memory model (Hochreiter and Schmidhuber, 1997) links each time step with an input gate, a memory gate, and an output gate, each designated as it, ft, and ot. Let's designate the vector for the current word wt, ht the vector calculated by the LSTM model at a time t, by combining et and ht \u2212 1., the cell state vector at a time t, and the sigmoid function. Vector representation ht for each time step t is given by: it = \u03c3 (Wi \u00b7 [ht \u2212 1, et]) (4) ft = \u03c3 (Wf \u00b7 [ht \u2212 1, et]) (5) ot = \u03c3 (Wo \u00b7 [ht \u2212 1, et]) (6) lt = tanh (Wl \u00b7 [ht \u2212 1, et]))) (7) ct = ft \u00b7 ct \u2212 1 + it \u00b7 lt (8) hst \u00b7 tanh (ct) (9), where \u2212 gaW \u2212 W, \u2212 Wkf \u2212 Wkl \u2212 Where \u2212 A"}, {"heading": "3.2 Attention Models", "text": "Attention models adopt a look-back strategy that combines the current decoding stage with input-time steps to show which parts of the input are most responsible for the current decoding state (Xuet al., 2015; Luong et al., 2015b; Bahdanau et al., 2014). Let H = {h, h, h, h, 2,..., h, Nx) be the collection of hidden vectors output from LSTMs during encoding. Each element in H contains information about the input sequences, focused on the parts surrounding each specific brand. Let ht \u2212 1 be the LSTM outputs for decoding at time t \u2212 1. Attention models link the current-step decoding information, i.e., ht with each of the representations at decoding step h, using a weight variable at. at can be the LSTM outputs for the decoding at."}, {"heading": "3.3 Unknown Word Replacements", "text": "One of the main problems in neural MT models is the arithmetic complexity of the softmax function for predicting target words, which requires a summation of all the tokens in the vocabulary. Neural models tend to maintain a shortlist of 50,00-80,000 most common words and use an unknown (UNK) token to represent all the rare tokens, which significantly affects BLEU values. Recent work has suggested addressing this problem: (Luong et al., 2015b) use a post-processing strategy based on aligners from IBM models, while (Jean et al., 2014) approximates Softmax functions by selecting a small subset of target vocabularies. In this essay, we use a strategy similar to that of Jean et al. (2014) to avoid dependence on external IBM model word processing. From the attention models, we obtain a pair of words from the training data."}, {"heading": "4 Mutual Information via Reranking", "text": "As discussed in Li et al. (2015), a direct decoding of (2) is not feasible because the second part, p (x | y), requires a complete generation of the target before it can be calculated. Therefore, we use an approximation approach: 1. Train p (y | x) and p (x | y) separately using vanilla SEQ2SEQ models or attention models. 2. Generate N leaderboards from p (y | x).3. Replace the N leaderboard with linear addition of p (x | y)."}, {"heading": "4.1 Standard Beam Search for N-best lists", "text": "N leaderboards are created using a bar search decoder whose bar size is set to K = 1.5, from p (y | x) models. As shown in Figure 1, in time step t \u2212 1, when decoding, we record N hypotheses based on score S (Yt \u2212 1 | x) = log p (y1, y2,..., yt \u2212 1 | x). As we move to time step t, we extend each of the K hypotheses (referred to as Y kt \u2212 1 = {yk1, yk2,..., ykt \u2212 1}, k [1, K]) by selecting the top K list of translations designated as yk, k \u2212 t, k \u2012 [1, K], which leads to the construction of K \u00d7 K new hypotheses: [Y kt \u2212 1, y \u2212 1, k \u2032 t], k \u2012 theses."}, {"heading": "4.2 Generating a Diverse N-best List", "text": "Unfortunately, the N-best lists issued by the standard bar search are a poor substitute for the entire search space (Finkel et al., 2006; Huang, 2008). The beam search algorithm can only find a small proportion of candidates in the search space and most of the generated translations in N-best list2For example, for the development set of the English-German WMT14 task, each input has an average of our reranking process, it is important to find a way to generate a-best list.We propose to change the way S (Y kt \u2212 1, y \u2032 t | x) is used in an attempt to promote diversity, as shown in Figure 1."}, {"heading": "4.3 Reranking", "text": "The generated N leaderboard is then recalculated by linear combination of log p (y | x) with log p (x | y). The score of the specified source for each generated translation can be calculated immediately from the previously trained p (x | y). Unlike log p (y | x), we also consider log p (y), which indicates the average language model probability calculated from monolingual data. It is worth 3Decoding for neural MT models that use large lot sizes can be expensive, resulting from Softmax word prediction functionality. The proposed model supports batch decoding using GPU, significantly speeds up the decoding process than other diversity promotion models that are tailored to phrase systems. Nothing that requires the integration of log p (y | x) and log p (y) into the reranking function of the Softmax word, is not new in 2003, and has been used for a long time in the modelling of an additional channel length (MT) in the standard MT and MT modelling of the latest progress (MT)."}, {"heading": "5 Experiments", "text": "Our models are trained on the WMT '14 training dataset, which contains 4.5 million pairs for the English and German-English translations, and 12 million pairs for the English-French translation. For the English-German translation, we limit our vocabulary to the 50 K-most common words for both languages. For the English-French translation, we use the 200 K-most frequent words for the source language and 80 K-words for the target language. Words not included in the vocabulary list are noted as universal unknowns. For the English-German and English-German translation, we use the newest 2013 (3000 sentence pairs) as development set and translation performance will be updated in BLEU (Papineni et al., 2002) to newest 2014 (2737) sentences. For the English-German translation, we link newest-2012 and news-test-2013 to form a development set (6,003 pairs in total) and rate the models on news-2014."}, {"heading": "5.2 Training p(y) from Monolingual Data", "text": "We use a subset of the original data set containing approximately 50-60 million records. Subsequently (Gulcehre et al., 2015; Sennrich et al., 2015a) we remove records with more than 10% unknown words based on the vocabulary built from parallel records. We used protocols similar to those we trained on SEQ2SEQ models, such as gradient clipping and mini batch."}, {"heading": "5.3 English-German Results", "text": "The results for different models on WMT2014 English-German translation task are presented in Figure 1. Among all features, the reverse probability results from mutual information (i.e., p (x | y) leads to the most significant performance increases, + 1.4 and + 1.1 for standard SEQ2SEQ models with no and unknown word exchange, + 0.9 for attention models7. In line with (Gul-5Tesla K40m, 1 Kepler GK110B, 6http: / / www.statmt.org / wmt13 / translation-task.html 7Target length has proven to be one of the most important features in the phrase MT due to the significant sensitivity of the BLEU score."}, {"heading": "5.4 French-English Results", "text": "Results from the 14 Franco-English datasets of WMTs are presented in Table 2, together with results reprinted from Sutskever et al. (2014; Luong et al. (2015b). Once again, we observe that the use of mutual information performs better than the corresponding standard neural models. Compared to the English-German dataset, the English-French translation task shows a larger gap between our new model and vanilla models, which do not take information transmission into account; our models each provide up to + 3.2, + 2.6, + 2.7 boost in BLEU compared to standard neural models without and with unknown word exchange and attention models."}, {"heading": "6 Discussion", "text": "In this paper, we present a new goal for neural MT based on the interdependence between source and target sentences, inspired by recent work in the field of neural conversation generation (Li et al., 2015). We are building an approximate implementation of our model by means of reranking, and in order to make reranking more efficient, we are introducing a new decryption method that promotes diversity in the first N list. In English \u2192 French and English \u2192 German translation tasks, we show that the neural machine translation models carried by the proposed method work better than corresponding standard models, and that both the mutual information goal and the diversity-enhancing decryption methods contribute to performance enhancement. The new models have the advantages of easy implementation with interchangeable sources and objectives and a general solution that can be integrated into all neural generation models with minor adjustments."}], "references": [{"title": "Neural machine translation by jointly", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Maximum mutual information estimation of hidden Markov model parameters for speech recognition", "author": ["LR Bahl", "Peter F Brown", "Peter V De Souza", "Robert L Mercer."], "venue": "proc. icassp, volume 86, pages 49\u201352.", "citeRegEx": "Bahl et al\\.,? 1986", "shortCiteRegEx": "Bahl et al\\.", "year": 1986}, {"title": "Diverse m-best solutions in Markov random fields", "author": ["Dhruv Batra", "Payman Yadollahpour", "Abner GuzmanRivera", "Gregory Shakhnarovich."], "venue": "Computer Vision\u2013ECCV 2012, pages 1\u201316. Springer.", "citeRegEx": "Batra et al\\.,? 2012", "shortCiteRegEx": "Batra et al\\.", "year": 2012}, {"title": "Positive diversity tuning for machine translation system combination", "author": ["Daniel Cer", "Christopher D Manning", "Daniel Jurafsky."], "venue": "Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 320\u2013328.", "citeRegEx": "Cer et al\\.,? 2013", "shortCiteRegEx": "Cer et al\\.", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Traitbased hypothesis selection for machine translation", "author": ["Jacob Devlin", "Spyros Matsoukas."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Devlin and Matsoukas.,? 2012", "shortCiteRegEx": "Devlin and Matsoukas.", "year": 2012}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M Schwartz", "John Makhoul."], "venue": "ACL (1), pages 1370\u20131380. Citeseer.", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines", "author": ["Jenny Rose Finkel", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Finkel et al\\.,? 2006", "shortCiteRegEx": "Finkel et al\\.", "year": 2006}, {"title": "A systematic exploration of diversity in machine translation", "author": ["Kevin Gimpel", "Dhruv Batra", "Chris Dyer", "Gregory Shakhnarovich", "Virginia Tech."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, October.", "citeRegEx": "Gimpel et al\\.,? 2013", "shortCiteRegEx": "Gimpel et al\\.", "year": 2013}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1503.03535.", "citeRegEx": "Gulcehre et al\\.,? 2015", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Forest reranking: Discriminative parsing with non-local features", "author": ["Liang Huang."], "venue": "ACL, pages 586\u2013 594.", "citeRegEx": "Huang.,? 2008", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.2007.", "citeRegEx": "Jean et al\\.,? 2014", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP, pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Minimum Bayes-risk decoding for statistical machine translation", "author": ["Shankar Kumar", "William Byrne."], "venue": "Technical report, DTIC Document.", "citeRegEx": "Kumar and Byrne.,? 2004", "shortCiteRegEx": "Kumar and Byrne.", "year": 2004}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1510.03055.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of ACL.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Lattice-based minimum error rate training for statistical machine translation", "author": ["Wolfgang Macherey", "Franz Josef Och", "Ignacio Thayer", "Jakob Uszkoreit."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Macherey et al\\.,? 2008", "shortCiteRegEx": "Macherey et al\\.", "year": 2008}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu."], "venue": "arXiv preprint arXiv:1503.01838.", "citeRegEx": "Meng et al\\.,? 2015", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Proceedings of ACL 2002, pages 295\u2013302.", "citeRegEx": "Och and Ney.,? 2002", "shortCiteRegEx": "Och and Ney.", "year": 2002}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160\u2013167. Association for Computational Linguistics.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1511.06709.", "citeRegEx": "Sennrich et al\\.,? 2015a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1507.04808.", "citeRegEx": "Serban et al\\.,? 2015a", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "A survey of available corpora for building data-driven dialogue systems", "author": ["Iulian Vlad Serban", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1512.05742.", "citeRegEx": "Serban et al\\.,? 2015b", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "String-to-dependency statistical machine translation", "author": ["Libin Shen", "Jinxi Xu", "Ralph Weischedel."], "venue": "Computational Linguistics, 36(4):649\u2013671.", "citeRegEx": "Shen et al\\.,? 2010", "shortCiteRegEx": "Shen et al\\.", "year": 2010}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Lattice minimum bayes-risk decoding for statistical machine translation", "author": ["Roy W Tromble", "Shankar Kumar", "Franz Och", "Wolfgang Macherey."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 620\u2013629. As-", "citeRegEx": "Tromble et al\\.,? 2008", "shortCiteRegEx": "Tromble et al\\.", "year": 2008}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "arXiv preprint arXiv:1412.7449.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Large scale discriminative training of hidden Markov models for speech recognition", "author": ["P.C. Woodland", "D. Povey."], "venue": "Computer Speech and Language, 16:25\u201347.", "citeRegEx": "Woodland and Povey.,? 2002", "shortCiteRegEx": "Woodland and Povey.", "year": 2002}, {"title": "Bagging and boosting statistical machine translation systems", "author": ["Tong Xiao", "Jingbo Zhu", "Tongran Liu."], "venue": "Artificial Intelligence, 195:496\u2013527.", "citeRegEx": "Xiao et al\\.,? 2013", "shortCiteRegEx": "Xiao et al\\.", "year": 2013}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1502.03044.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 29, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 0, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 4, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 13, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 23, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 24, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 9, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 17, "context": "SEQ2SEQ models require minimal domain knowledge, can be trained end-to-end, have a much smaller memory footprint than the large phrase tables needed for phrase-based SMT, and achieve state-of-the-art performance in large-scale tasks like English to French (Luong et al., 2015b) and English to German (Luong et al.", "startOffset": 256, "endOffset": 277}, {"referenceID": 16, "context": ", 2015b) and English to German (Luong et al., 2015a; Jean et al., 2014) translation.", "startOffset": 31, "endOffset": 71}, {"referenceID": 12, "context": ", 2015b) and English to German (Luong et al., 2015a; Jean et al., 2014) translation.", "startOffset": 31, "endOffset": 71}, {"referenceID": 20, "context": ", p(y|x), they ignore p(x|y), the dependency from the target to the source, which has long been an important feature in phrase-based translation (Och and Ney, 2002; Shen et al., 2010).", "startOffset": 145, "endOffset": 183}, {"referenceID": 27, "context": ", p(y|x), they ignore p(x|y), the dependency from the target to the source, which has long been an important feature in phrase-based translation (Och and Ney, 2002; Shen et al., 2010).", "startOffset": 145, "endOffset": 183}, {"referenceID": 15, "context": "As Li et al. (2015) recently showed in the context of conversational response generation, the MMI based objective function is equivalent to linearly combining p(x|y) and p(y|x).", "startOffset": 3, "endOffset": 20}, {"referenceID": 15, "context": "(2) But as also discussed in Li et al. (2015), direct decoding from (2) is infeasible because computing p(x|y) cannot be done until the target has been ar X iv :1 60 1.", "startOffset": 29, "endOffset": 46}, {"referenceID": 31, "context": "They yield good performance in a variety of NLP generation tasks including conversational response generation (Vinyals and Le, 2015; Serban et al., 2015a; Li et al., 2015), and parsing (Vinyals et al.", "startOffset": 110, "endOffset": 171}, {"referenceID": 25, "context": "They yield good performance in a variety of NLP generation tasks including conversational response generation (Vinyals and Le, 2015; Serban et al., 2015a; Li et al., 2015), and parsing (Vinyals et al.", "startOffset": 110, "endOffset": 171}, {"referenceID": 15, "context": "They yield good performance in a variety of NLP generation tasks including conversational response generation (Vinyals and Le, 2015; Serban et al., 2015a; Li et al., 2015), and parsing (Vinyals et al.", "startOffset": 110, "endOffset": 171}, {"referenceID": 15, "context": "As demonstrated in (Li et al., 2015)", "startOffset": 19, "endOffset": 36}, {"referenceID": 19, "context": "Similar convolutional networks are used in (Meng et al., 2015) for encoding.", "startOffset": 43, "endOffset": 62}, {"referenceID": 11, "context": "Kalchbrenner and Blunsom (2013) used an encoding model akin to convolutional networks for encoding and standard hidden unit recurrent nets for decoding.", "startOffset": 0, "endOffset": 32}, {"referenceID": 11, "context": "Kalchbrenner and Blunsom (2013) used an encoding model akin to convolutional networks for encoding and standard hidden unit recurrent nets for decoding. Similar convolutional networks are used in (Meng et al., 2015) for encoding. Sutskever et al. (2014; Luong et al. (2015a) employed a stacking LSTM model for both encoding and decoding.", "startOffset": 0, "endOffset": 275}, {"referenceID": 0, "context": "Bahdanau et al. (2014), Jean et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Bahdanau et al. (2014), Jean et al. (2014) adopted bi-directional recurrent nets for the encoder.", "startOffset": 0, "endOffset": 43}, {"referenceID": 1, "context": "Maximum Mutual Information Maximum Mutual Information (MMI) was introduced in speech recognition (Bahl et al., 1986) as a way of measuring the mutual dependence between inputs (acoustic feature vectors) and outputs (words) and improving discriminative training (Woodland and Povey, 2002).", "startOffset": 97, "endOffset": 116}, {"referenceID": 33, "context": ", 1986) as a way of measuring the mutual dependence between inputs (acoustic feature vectors) and outputs (words) and improving discriminative training (Woodland and Povey, 2002).", "startOffset": 152, "endOffset": 178}, {"referenceID": 28, "context": ", I don\u2019t know) regardless of the inputs (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2015b).", "startOffset": 41, "endOffset": 107}, {"referenceID": 31, "context": ", I don\u2019t know) regardless of the inputs (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2015b).", "startOffset": 41, "endOffset": 107}, {"referenceID": 26, "context": ", I don\u2019t know) regardless of the inputs (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2015b).", "startOffset": 41, "endOffset": 107}, {"referenceID": 1, "context": "Maximum Mutual Information Maximum Mutual Information (MMI) was introduced in speech recognition (Bahl et al., 1986) as a way of measuring the mutual dependence between inputs (acoustic feature vectors) and outputs (words) and improving discriminative training (Woodland and Povey, 2002). Li et al. (2015) show that MMI could solve an important problem in SEQ2SEQ conversational response generation.", "startOffset": 98, "endOffset": 306}, {"referenceID": 1, "context": "Maximum Mutual Information Maximum Mutual Information (MMI) was introduced in speech recognition (Bahl et al., 1986) as a way of measuring the mutual dependence between inputs (acoustic feature vectors) and outputs (words) and improving discriminative training (Woodland and Povey, 2002). Li et al. (2015) show that MMI could solve an important problem in SEQ2SEQ conversational response generation. Prior SEQ2SEQ models tended to generate highly generic, dull responses (e.g., I don\u2019t know) regardless of the inputs (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2015b). Li et al. (2015) show that modeling the mutual dependency between messages and response promotes the diversity of response outputs.", "startOffset": 98, "endOffset": 602}, {"referenceID": 20, "context": "In that sense, our work is designed to incorporate into SEQ2SEQ models features that have proved useful in phrasebased MT, like the reverse translation probability or sentence length (Och and Ney, 2002; Shen et al., 2010; Devlin et al., 2014).", "startOffset": 183, "endOffset": 242}, {"referenceID": 27, "context": "In that sense, our work is designed to incorporate into SEQ2SEQ models features that have proved useful in phrasebased MT, like the reverse translation probability or sentence length (Och and Ney, 2002; Shen et al., 2010; Devlin et al., 2014).", "startOffset": 183, "endOffset": 242}, {"referenceID": 6, "context": "In that sense, our work is designed to incorporate into SEQ2SEQ models features that have proved useful in phrasebased MT, like the reverse translation probability or sentence length (Och and Ney, 2002; Shen et al., 2010; Devlin et al., 2014).", "startOffset": 183, "endOffset": 242}, {"referenceID": 18, "context": "Generating Diverse Translations Various algorithms have been proposed for generated diverse translations in phrase-based MT, including compact representations like lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al.", "startOffset": 189, "endOffset": 257}, {"referenceID": 30, "context": "Generating Diverse Translations Various algorithms have been proposed for generated diverse translations in phrase-based MT, including compact representations like lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al.", "startOffset": 189, "endOffset": 257}, {"referenceID": 14, "context": "Generating Diverse Translations Various algorithms have been proposed for generated diverse translations in phrase-based MT, including compact representations like lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al.", "startOffset": 189, "endOffset": 257}, {"referenceID": 5, "context": ", 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al.", "startOffset": 65, "endOffset": 93}, {"referenceID": 34, "context": ", 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al., 2013), or multiple systems (Cer et al.", "startOffset": 112, "endOffset": 131}, {"referenceID": 3, "context": ", 2013), or multiple systems (Cer et al., 2013).", "startOffset": 29, "endOffset": 47}, {"referenceID": 2, "context": "(2013; Batra et al. (2012), produce diverse N-best lists by adding a dissimilarity function based on N-gram overlaps, distancing the current translation from already-generated ones by choosing translations that have higher scores but distinct from previous ones.", "startOffset": 7, "endOffset": 27}, {"referenceID": 10, "context": "A long-short term memory model (Hochreiter and Schmidhuber, 1997) associates each time step with an input gate, a memory gate and an output gate, denoted respectively as it, ft and ot.", "startOffset": 31, "endOffset": 65}, {"referenceID": 35, "context": "Attention models adopt a look-back strategy that links the current decoding stage with input time steps to represent which portions of the input are most responsible for the current decoding state (Xu et al., 2015; Luong et al., 2015b; Bahdanau et al., 2014).", "startOffset": 197, "endOffset": 258}, {"referenceID": 17, "context": "Attention models adopt a look-back strategy that links the current decoding stage with input time steps to represent which portions of the input are most responsible for the current decoding state (Xu et al., 2015; Luong et al., 2015b; Bahdanau et al., 2014).", "startOffset": 197, "endOffset": 258}, {"referenceID": 0, "context": "Attention models adopt a look-back strategy that links the current decoding stage with input time steps to represent which portions of the input are most responsible for the current decoding state (Xu et al., 2015; Luong et al., 2015b; Bahdanau et al., 2014).", "startOffset": 197, "endOffset": 258}, {"referenceID": 0, "context": ", 2015b; Bahdanau et al., 2014). Let H = {\u01251, \u01252, ..., \u0125Nx} be the collection of hidden vectors outputted from LSTMs during encoding. Each element in H contains information about the input sequences, focusing on the parts surrounding each specific token. Let ht\u22121 be the LSTM outputs for decoding at time t \u2212 1. Attention models link the current-step decoding information, i.e., ht with each of the representations at decoding step \u0125t\u2032 using a weight variable at. at can be constructed from different scoring functions such as the dot product between the two vectors, i.e., ht\u22121 \u00b7 \u0125t, a general model akin to tensor operation i.e., ht\u22121 \u00b7 W \u00b7 \u0125t, and the concatenation model by concatenating the two vectors i.e., UT \u00b7tanh(W \u00b7 [ht\u22121, \u0125t]). The behavior of different attention scoring functions have been extensively studied in Luong et al. (2015a). For all experiments in this paper, we adopt the general strategy where the relevance score between the current step of the decoding representation and the encoding representation is given by:", "startOffset": 9, "endOffset": 848}, {"referenceID": 16, "context": "Luong et al. (2015a) reported a significant performance boost by integrating ~ht\u22121 into the next step LSTM hidden state computation (referred to as the input-feeding model), making LSTM compositions in decoding as follows:", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "Recent work has proposed to deal with this issue: (Luong et al., 2015b) adopt a post-processing strategy based on aligner from IBM models, while (Jean et al.", "startOffset": 50, "endOffset": 71}, {"referenceID": 12, "context": ", 2015b) adopt a post-processing strategy based on aligner from IBM models, while (Jean et al., 2014) approximates softmax functions by selecting a small subset of target vocabulary.", "startOffset": 82, "endOffset": 101}, {"referenceID": 12, "context": "In this paper, we use a strategy similar to that of Jean et al. (2014), thus avoiding the reliance on external IBM model word aligner.", "startOffset": 52, "endOffset": 71}, {"referenceID": 15, "context": "As discussed in Li et al. (2015), direct decoding from (2) is infeasible since the second part, p(x|y), requires completely generating the target before it can be computed.", "startOffset": 16, "endOffset": 33}, {"referenceID": 7, "context": "Unfortunately, the N-best lists outputted from standard beam search are a poor surrogate for the entire search space (Finkel et al., 2006; Huang, 2008).", "startOffset": 117, "endOffset": 151}, {"referenceID": 11, "context": "Unfortunately, the N-best lists outputted from standard beam search are a poor surrogate for the entire search space (Finkel et al., 2006; Huang, 2008).", "startOffset": 117, "endOffset": 151}, {"referenceID": 15, "context": "We employ the diversity evaluation metrics in (Li et al., 2015) to evaluate the degree of diversity of the N-best lists: calculating the average number of distinct unigrams distinct-1 and bigrams distinct2 in the N-best list given each source sentence, scaled by the total number of tokens.", "startOffset": 46, "endOffset": 63}, {"referenceID": 9, "context": "In neural MT literature, recent progress has demonstrated the effectiveness of modeling reranking with language model (Gulcehre et al., 2015).", "startOffset": 118, "endOffset": 141}, {"referenceID": 21, "context": "We optimize \u03b7, \u03bb and \u03b3 using MERT (Och, 2003) BLEU score (Papineni et al.", "startOffset": 34, "endOffset": 45}, {"referenceID": 22, "context": "We optimize \u03b7, \u03bb and \u03b3 using MERT (Och, 2003) BLEU score (Papineni et al., 2002) on the development set.", "startOffset": 57, "endOffset": 80}, {"referenceID": 22, "context": "For the English-German and English-German translation, we use newstest2013 (3000 sentence pairs) as the development set and translation performances are reported in BLEU (Papineni et al., 2002) on newstest2014 (2737) sentences.", "startOffset": 170, "endOffset": 193}, {"referenceID": 22, "context": "For the English-German and English-German translation, we use newstest2013 (3000 sentence pairs) as the development set and translation performances are reported in BLEU (Papineni et al., 2002) on newstest2014 (2737) sentences.", "startOffset": 171, "endOffset": 217}, {"referenceID": 29, "context": "We trained p(y|x) following the standard training protocols described in (Sutskever et al., 2014).", "startOffset": 73, "endOffset": 97}, {"referenceID": 16, "context": "As in (Luong et al., 2015a).", "startOffset": 6, "endOffset": 27}, {"referenceID": 16, "context": "As in (Luong et al., 2015a). All texts are tokenized with tokenizer.perl and BLEU scores are computed with multibleu.perl ing, each of which consists of a different set of parameters. We followed the detailed protocols from Luong et al. (2015a): each LSTM layer consists of 1,000 hidden neurons, and the dimensionality of word embeddings is set to 1,000.", "startOffset": 7, "endOffset": 245}, {"referenceID": 9, "context": "Following (Gulcehre et al., 2015; Sennrich et al., 2015a), we remove sentences with more than 10% Unknown words based on the vocabulary constructed using parallel datasets.", "startOffset": 10, "endOffset": 57}, {"referenceID": 23, "context": "Following (Gulcehre et al., 2015; Sennrich et al., 2015a), we remove sentences with more than 10% Unknown words based on the vocabulary constructed using parallel datasets.", "startOffset": 10, "endOffset": 57}, {"referenceID": 12, "context": "Baselines performances are reprinted from Jean et al. (2014), Luong et al.", "startOffset": 42, "endOffset": 61}, {"referenceID": 12, "context": "The unknown token replacement technique yields significant gains, in line with observations from Jean et al. (2014; Luong et al. (2015a).", "startOffset": 97, "endOffset": 137}, {"referenceID": 12, "context": "For the models described in (Jean et al., 2014) and (Lu-", "startOffset": 28, "endOffset": 47}, {"referenceID": 12, "context": "We compare our English-German system with various others: (1) The end-to-end neural MT system from Jean et al. (2014) using a large vocabulary size.", "startOffset": 99, "endOffset": 118}, {"referenceID": 12, "context": "We compare our English-German system with various others: (1) The end-to-end neural MT system from Jean et al. (2014) using a large vocabulary size. (2) Models from Luong et al. (2015a) that combines different attention models.", "startOffset": 99, "endOffset": 186}, {"referenceID": 16, "context": "The ensemble procedure is known to result in improved performance (Luong et al., 2015a; Jean et al., 2014; Sutskever et al., 2014).", "startOffset": 66, "endOffset": 130}, {"referenceID": 12, "context": "The ensemble procedure is known to result in improved performance (Luong et al., 2015a; Jean et al., 2014; Sutskever et al., 2014).", "startOffset": 66, "endOffset": 130}, {"referenceID": 29, "context": "The ensemble procedure is known to result in improved performance (Luong et al., 2015a; Jean et al., 2014; Sutskever et al., 2014).", "startOffset": 66, "endOffset": 130}, {"referenceID": 16, "context": "Note that the reported results from the standard SEQ2SEQ models and attention models in Table 1 (those without considering mutual information) are from models identical in structure to the corresponding models described in (Luong et al., 2015a), and achieve similar performances (13.", "startOffset": 223, "endOffset": 244}, {"referenceID": 27, "context": "Google is the LSTM-based model proposed in Sutskever et al. (2014). Luong et al.", "startOffset": 43, "endOffset": 67}, {"referenceID": 16, "context": "Luong et al. (2015) is the extension of Google models with unknown token replacements.", "startOffset": 0, "endOffset": 20}, {"referenceID": 16, "context": "(2014; Luong et al. (2015b). We again observe that applying mutual information yields better performance than the corresponding standard neural MT models.", "startOffset": 7, "endOffset": 28}, {"referenceID": 15, "context": "In this paper, we introduce a new objective for neural MT based on the mutual dependency between the source and target sentences, inspired by recent work in neural conversation generation (Li et al., 2015).", "startOffset": 188, "endOffset": 205}], "year": 2016, "abstractText": "Sequence-to-sequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., p(y|x), an objective that ignores other potentially useful sources of information. We introduce an alternative objective function for neural MT that maximizes the mutual information between the source and target sentences, modeling the bi-directional dependency of sources and targets. We implement the model with a simple re-ranking method, and also introduce a decoding algorithm that increases diversity in the N-best list produced by the first pass. Applied to the WMT German/English and French/English tasks, the proposed models offers a consistent performance boost on both standard LSTM and attention-based neural MT architectures.", "creator": "TeX"}}}