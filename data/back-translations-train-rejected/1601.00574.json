{"id": "1601.00574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jan-2016", "title": "NFL Play Prediction", "abstract": "Based on NFL game data we try to predict the outcome of a play in multiple different ways. An application of this is the following: by plugging in various play options one could determine the best play for a given situation in real time. While the outcome of a play can be described in many ways we had the most promising results with a newly defined measure that we call \"progress\". We see this work as a first step to include predictive analysis into NFL playcalling.", "histories": [["v1", "Mon, 4 Jan 2016 17:30:07 GMT  (1301kb,D)", "http://arxiv.org/abs/1601.00574v1", "14 pages"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["brendan teich", "roman lutz", "valentin kassarnig"], "accepted": false, "id": "1601.00574"}, "pdf": {"name": "1601.00574.pdf", "metadata": {"source": "CRF", "title": "NFL Play Prediction", "authors": ["Brendan Teich", "Roman Lutz", "Valentin Kassarnig"], "emails": ["vkassarnig}@cs.umass.edu"], "sections": [{"heading": "1 Introduction", "text": "Especially at the professional level, coaches spend most of their time either analyzing their opponent's past games to learn about weaknesses or teaching their players how to exploit their opponent's weaknesses. Although American football is a very data-driven sport, coaches have the final say on the game, often leading to controversy over whether coaches have made good or bad decisions. A prime example is this year's Super Bowl, where the Seahawks went with a pass rather than a run and ended up being intercepted and lost. We argue that computers could help coaches or even take over the game entirely by predicting the outcome of certain games. We build the foundation for such an application by providing predictions of the outcome of a game based on the game situation and the game description. As a next step, coaches could vary the game description based on games from their own playbook and see which game has the best chance of success."}, {"heading": "2 Related work", "text": "They found that undervaluation, which can result from coaches \"propensity to\" mix \"games, is a bias toward rushed plays, and that the generalized matching equation accounts for much of the variability in game selection. Authors have also contributed to this work: 1http: / / bleacherreport.com / articles / 2350553-questionable-play-call-cost-Seahawks-Super Bowl-victoryar Xiv: 160 1.00 574v 1 [cs.L G] 4J an2 an2 for each game below reveals that they tend toward rushed play. It was also found that rushed play was preferred when less than 4 yards were played, and that passing was preferred when more than 10 yards remained. Kovash and Levitt [7] analyzed whether in American football and baseball decisions were played at 5,000 yards."}, {"heading": "3 Data set", "text": "The nflgame [2] API provides play-by-play access to data from all NFL games played over the past six years (2009-2014). From this data, we extracted a total of 12 features and filtered out irrelevant games to give us 177245 games. The following sections describe this process and features in more detail."}, {"heading": "4 Features", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Feature extraction", "text": "For each game, we get a data structure from the API. This structure contains basic information about the current situation of the game such as the music box, the field position, the opposing team, etc. It also contains a string that describes what happened on the field. Apart from the actual game, this string can describe injuries, timeouts, quarterback substitutions, penalties or other information.Since we are only interested in the actual games, we have to filter out any irrelevant information, i.e. we have to find the sentence that describes the actual game. In the case of penalties, we reject the entire string because a penalty affects the course of the game. In addition, there are games that are not of interest to us and that are also ignored, including field goals, punts, sacks, fumbles, etc. The last string that describes the current game is then analyzed to extract the characteristics. Table 1 lists and describes all characteristics we use."}, {"heading": "4.1.1 Examples", "text": "Table 6 in the appendix shows some sample descriptions and the extracted features and labels. Missed features are set to 0. Note that not all features are extracted from the description string. Some features are obtained directly from the data structure of the API."}, {"heading": "4.2 Encoding", "text": "Some of our characteristics have categorical values. For example, team and opponent take a value that represents one of the 32 teams. A simple numbering of the teams from 1 to 32 would not represent the real situation, since team # 1 is no closer to team # 2 than to team # 32. Therefore, we need a more complex encoding. That's why we have chosen a uniform encoding for our categorical characteristics: each categorical characteristic is replaced by k binary characteristics, where k is the number of possible values. For example, the \"team\" characteristic is replaced by 32 characteristics, such as team = GB, team = NE, and so on. In each example, only one of these characteristics has a value of \"1,\" while all the others are \"0.\" The encoding of all our categorical characteristics in our dataset expands the size from 12 to 77 dimensions."}, {"heading": "4.3 Exploratory data analysis", "text": "The first component already covers more than 99.7% of the variance. Figure 2 shows a projection of the entire dataset onto the two dimensions that represent the largest part of the variance. Red dots represent successful games and blue dots are failure games. The distribution of the games shows some kind of pattern or structure. Remarkable are the two red clusters near the left and right edges. However, the data is far too loud to separate the two classes. Note that PCA relies on continuously evaluated variables, while our data consists of a mixture of continuous and binary values. As discussed by Yu and Tresp [20] there are some problems in applying PCA to mixed types of data. Therefore, PCA helps us examine the data, but it is not suitable for dimensionality reduction. Another approach to reduce the dimensionality of the classification is the method of removing only two characteristics."}, {"heading": "5 Targets", "text": "Apart from the characteristics we have to extract the basic truths of each game. As we take several approaches, we also need to extract several labels. Table 2 lists and describes all the labels we apply. While predicting success is a classification task, yards and progress are real numbers that require regression methods. We examine various methods for classification and regression in Section 6. If we use the binary value we define for successful games that either reach a first down or a touch, the game will be classified as a failure. With this target definition, our data collection is fairly unbalanced, as it contains about 70% unsuccessful and only 30% successful examples. Therefore, the imbalance has to be taken into account in order to obtain the different classification methods or otherwise a classification that classifies each sample as a \"failure,\" a classification accuracy of 70% would be achieved, which is actually quite good."}, {"heading": "6 Methods", "text": "This section briefly explains all the methods we use for classification and regression."}, {"heading": "6.1 Classification Trees", "text": "Classification trees [10] are binary trees in which each internal node defines a rule for one of the input characteristics that divides the data into two subsets aimed at the best division. In order to avoid overfitting, it is useful to define a maximum depth for the tree. We used a balanced class weighting to handle the unbalanced data, that is, the weighting factors for the classes are automatically adjusted to their inverse frequencies in the input data.Figure 4 shows a simple classification tree trained on the basis of our dataset. It contains only one decision rule that divides the data set into two subsets. The only rule here is whether the next first downward motion is more than 7.5 meters away. That is, all games in which the team has to spend 7.5 meters or less on its next first downward motion are classified as \"success\" and all others as \"failures.\" This simple rule already gives a good classification accuracy of only 67.50%, with a good accuracy of only 0."}, {"heading": "6.2 Regression Trees", "text": "Figure 5 shows a simple regression tree that has been trained to predict the distance gained based on our data set. It contains only a single decision rule that divides the data set into two subsets, the only decision rule here being whether a low pass has been played. I.e., the first subset contains all games with a short pass or running game, and the second subset contains all games with a low pass. The first subset is labeled with 5.3 yards and the second subset with 11.1 yards. With this decision rule, we achieve a MAE of 5.7 yards and an RMSE of 8.4 yards.Table 9 in the appendix shows the performance of the regression tree with different depth constraints. You can see that the performance remains fairly constant, but no depth restriction causes the algorithm to exceed the training data, resulting in poor performance."}, {"heading": "6.3 Nearest Centroid", "text": "For the Nearest Centroid classification, we first calculate the centroids of the success and failure markers of the trait vectors in our training set. To then predict the classification of a given trait vector, we simply assign them to the class with the nearest centroid. This method yielded about 50% accuracy, precision, and recall after we sample the training set to have the same amount of success and failure markers, so that overall it is no better than a random assignment of the labels. This suggests that the classifications are not divided into two different clusters at some distance, but are positioned in a way that makes the distance from the centroids a bad indicator of the class."}, {"heading": "6.4 Linear Discriminant Analysis", "text": "For our linear discriminant analysis classification, we used Scikit Learn's LDAClassifer. We then compared the results between the use of singular value decomposition, least squares and eigenvalue decomposition to solve the LDA problem before classification. Both the least squares and eigenvalue decomposition used shrinkage to further improve the accuracy of our high-dimensional feature vectors. All three solutions delivered similar results with classification accuracy of approximately 66% and F1 values."}, {"heading": "6.5 Support Vector Machines", "text": "Our best result was an accuracy of 57.72% at C = 2 \u2212 5. Then we tried a Gaussian basic radial kernel function and performed a grid search at C = 211 and \u03b3 = 2 \u2212 17. This accuracy is similar to what we achieved with the LDA classifiers, but no improvement."}, {"heading": "6.6 Support Vector Regression", "text": "For regression, we looked at two different metrics: yards to go and progress, which are discussed in Section 5. Once again, we searched the grid for values of C-2k | k-2k [\u2212 5, 17] and \u03b3-2k | k-2k [\u2212 17, 4] to try to find good hyperparameters for support vector regression. Our best results were found at C = 27 and \u03b3 = 2 \u2212 17 with an average error of 5,207 yards and an average error of 8,977 yards square. We were dissatisfied in this regard, as a first descent is most often less than 10 yards away, so a error of 5 yards could be very significant. Therefore, we continued to use the progress metric, achieving an average error of 0.1351 yards and an average error square of 0.2332 yards. As a rough estimate, you can consider 0.13% of progress as about 13% of the distance to a first descent."}, {"heading": "6.7 Artificial Neural Networks", "text": "Our artificial neural network implementation uses the PyBrain library [3]. PyBrain allows the user to specify a number of parameters, including the number of hidden layers, the number of units per hidden layer, the type of units in the hidden layers, and the maximum number of epochs for training the network. It is worth noting that training the neural networks took considerably longer than any other method, which is why fewer configurations were studied. In addition, this could be a disadvantage for a scenario where the network needs to be retrained in real time between games or drives. In our experiments, it meant that retraining the networks after taking into account the unbalanced datasets for success classification was not possible. Otherwise, neural networks rated almost everything (or nothing) as an error. We present only values that were reasonable. Specifically, the networks with linear units in the hidden layers were not used well in the 12, and the other 51 were not used poorly in the hidden layers."}, {"heading": "7 Evaluation and Comparison", "text": "The full result tables are attached. Tables 3, 4 and 5 contain a compressed version of the results for success, yards and progress measurements, respectively. We report only the data of the configuration with the best results for each of the approaches. When comparing the results for success accuracy is not the main criterion. Consider the real scenario in which a team uses an application based on these methods to select the next game. We must ensure that games that lead to failure are classified as such, even if it is at the expense of classifying successful games as errors. This is represented by precision or, in other words, the percentage of true positives among all positives. High memory and accuracy, are nice, but not so important. Based on this observation support Vector Machines (SVM) with a Radial Base Function (RBF) is the best method with a precision."}, {"heading": "8 Applications", "text": "Given the fact that our methods evaluate the success of a game in a particular game situation, you can go ahead and apply all the different games that a team has learned, allowing the coach to determine which game is best suited to that particular game situation. Unfortunately, it is impossible to judge the effectiveness of this method based on our data set. We only have the result of the one game that was selected, not the results of all the games that are in the team's playbook. The only way to measure whether it actually makes sense to apply the method for a while and not to apply it for another period of time and compare the success, even then you could argue that a number of factors other than playcalling play a role, including injuries, shape, temperature, weather, etc."}, {"heading": "9 Conclusion and Future Work", "text": "In this work, we aimed to provide predictions of the outcome of a particular game in a particular game situation in the NFL. This is a novel approach that could be used to determine the best game in real games by finding the available game with the maximum success before the game is actually played. To define an appropriate measure of the success of a game, we have developed a new measurement called progress, which is both intuitive and more accurate than other metrics simultaneously. The accuracy we have achieved by using a number of different regression models is promising for our intended application, especially with more comprehensive data (formations, routes, players, etc.), more random samples, advanced regression methods to achieve greater accuracy, and possibly by considering series of games instead of individual games, the actual use in real game models is practicable. In addition, it may be useful to classify or regression methods in a team's games to be more predictable than some of the other ANVA has shown."}], "references": [{"title": "Scikit-learn: Machine learning in Python.", "author": ["Pedregosa", "Fabian", "Gal Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Pass or run: an empirical test of the matching pennies game using data from the National Football League.", "author": ["McGarrity", "Joseph P", "Brian Linnen"], "venue": "Southern Economic Journal 76,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "The Matching Relation and situation-specific bias modulation in professional football play selection.", "author": ["Stilling", "Stephanie T", "Thomas S. Critchfield"], "venue": "Journal of the Experimental Analysis of Behavior 93,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Professionals do not play minimax: evidence from Major League Baseball and the National Football League. No. w15347", "author": ["Kovash", "Kenneth", "Steven D. Levitt"], "venue": "National Bureau of Economic Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Dynamic Matching Pennies with Asymmetries: An Application to NFL Play Calling", "author": ["Mitchell", "Matthew"], "venue": "Working Paper,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "The generalized matching law in elite sport competition: Football play calling as operant choice.", "author": ["Reed", "Derek D", "Thomas S. Critchfield", "Brian K. Martens"], "venue": "Journal of Applied Behavior Analysis 39,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Classification and regression trees", "author": ["Breiman", "Leo", "Jerome Friedman", "Charles J. Stone", "Richard A. Olshen"], "venue": "CRC press,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1984}, {"title": "Beating the Defense: Using Plan Recognition to Inform Learning Agents.", "author": ["Molineaux", "Matthew", "David W. Aha", "Gita Sukthankar"], "venue": "In FLAIRS Conference", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Neural network prediction of NFL football games.", "author": ["Kahn", "Joshua"], "venue": "World Wide Web electronic publication", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Neural network quarterbacking.", "author": ["Purucker", "Michael C"], "venue": "Potentials, IEEE 15,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "A compound framework for sports results prediction: A football case study.", "author": ["Min", "Byungho", "Jinhyuck Kim", "Chongyoun Choe", "Hyeonsang Eom", "RI Bob McKay"], "venue": "Knowledge-Based Systems 21, no", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Predictions for National Football League games via linear-model methodology.", "author": ["Harville", "David"], "venue": "Journal of the American Statistical Association", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1980}, {"title": "An artificial neural network approach to college football prediction and ranking.", "author": ["Pardee", "Michael"], "venue": "University of WisconsinElectrical and Computer Engineering Department", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Improved least squares football, basketball, and soccer predictions.", "author": ["Stefani", "Raymond T"], "venue": "IEEE transactions on systems, man, and cybernetics 10,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1980}, {"title": "Foehrenbach. \u201dA Statistical Data Mining Approach to Determining the Factors that Distinguish Championship Caliber Teams in the National Football League.", "author": ["Fokoue", "Ernest", "Dan"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Heterogenous Data Fusion via a Probabilistic Latent-Variable Model.", "author": ["Yu", "Kai", "Tresp", "Volker"], "venue": "In Proceedings of 17th International Conference on Architecture of Computing Systems - Organic and Pervasive Computing (ARCS", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Fantasy Football Prediction", "author": ["Lutz", "Roman"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Stilling and Critchfield [6] used generalized matching equations to analyze the relationship between play selection and yards gained.", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "Kovash and Levitt [7] analyzed whether in American Football and Baseball decisions conform with Minimax.", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "Mitchell [8] enhanced the matching pennies model by Kovash and Levitt [7] by adding the notion of investment to playcalling.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "Mitchell [8] enhanced the matching pennies model by Kovash and Levitt [7] by adding the notion of investment to playcalling.", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "Reed, Critchfield and Martens [9] analyzed playcalling with the Generalized Matching Law, a mathematical model of operant choice.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "McGarrity and Linnen [5] developed a game theoretic model to analyze how a team changes its play calling when the starting quarterback is replaced by its backup.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "Molineaux, Aha and Sukthankar [11] used plan recognition to identify the defensive strategy and thus improve the results of their case-based Q-learning algorithm.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "Lutz [21] already identified related work some of which we refer to in the following.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "[15] used Bayesian inference and rule-based reasoning to predict the result of American Football matches.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Similarly, Harville [16] used a linear model for the same task.", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "A Neural Network approach to predicting the winner of College Football games was proposed by Pardee [17].", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "Purucker [13] and Kahn [12] applied neural networks to NFL game predictions and mostly improved on existing accuracies.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "Purucker [13] and Kahn [12] applied neural networks to NFL game predictions and mostly improved on existing accuracies.", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "Stefani [18] used an improved least squares rating system to predict the winner for nearly 9000 games from College and Pro Football and even other sports.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "Fokoue and Foehrenbach [19] have analyzed important factors for NFL teams with Data Mining.", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "As discussed by Yu and Tresp [20] there are some issues when applying PCA to mixed types of data.", "startOffset": 29, "endOffset": 33}, {"referenceID": 6, "context": "Classification Trees [10] are binary trees where each internal node defines a rule for one of the input features which divides the data into two subsets aiming for the best split.", "startOffset": 21, "endOffset": 25}, {"referenceID": 0, "context": "Finally, we would like to give credit to three open-source projects without which this work would not have been possible: scikit-learn [4], PyBrain [3] and nflgame [2].", "startOffset": 135, "endOffset": 138}], "year": 2016, "abstractText": "Based on NFL game data we try to predict the outcome of a play in multiple different ways including Decision and Classification Trees, Nearest Neighbors, Naive Bayes, Linear Discriminant Analysis, Support Vector Machines and Regression, and Artificial Neural Networks. An application of this is the following: by plugging in various play options one could determine the best play for a given situation in real time. While the outcome of a play can be described in many ways we had the most promising results with a newly defined measure that we call progress. We see this work as a first step to include predictive analysis into NFL playcalling.", "creator": "LaTeX with hyperref package"}}}