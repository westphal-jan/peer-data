{"id": "1601.00626", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jan-2016", "title": "Scalable Models for Computing Hierarchies in Information Networks", "abstract": "Information hierarchies are organizational structures that often used to organize and present large and complex information as well as provide a mechanism for effective human navigation. Fortunately, many statistical and computational models exist that automatically generate hierarchies; however, the existing approaches do not consider linkages in information {\\em networks} that are increasingly common in real-world scenarios. Current approaches also tend to present topics as an abstract probably distribution over words, etc rather than as tangible nodes from the original network. Furthermore, the statistical techniques present in many previous works are not yet capable of processing data at Web-scale. In this paper we present the Hierarchical Document Topic Model (HDTM), which uses a distributed vertex-programming process to calculate a nonparametric Bayesian generative model. Experiments on three medium size data sets and the entire Wikipedia dataset show that HDTM can infer accurate hierarchies even over large information networks.", "histories": [["v1", "Mon, 4 Jan 2016 20:05:19 GMT  (637kb,D)", "http://arxiv.org/abs/1601.00626v1", "Preprint for \"Knowledge and Information Systems\" paper, in press"]], "COMMENTS": "Preprint for \"Knowledge and Information Systems\" paper, in press", "reviews": [], "SUBJECTS": "cs.AI cs.DL cs.LG", "authors": ["baoxu shi", "tim weninger"], "accepted": false, "id": "1601.00626"}, "pdf": {"name": "1601.00626.pdf", "metadata": {"source": "META", "title": "Scalable Models for Computing Hierarchies in Information Networks", "authors": ["Baoxu Shi", "Tim Weninger"], "emails": [], "sections": [{"heading": null, "text": "In the last few years, it has been shown that the number of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance,"}, {"heading": "A. Taxonomies versus Hierarchies", "text": "In this paper we draw specific distinctions between a hierarchy and a hierarchy. A taxonomy is defined as a classification of objects into ever finer granularities, each non-leaf node being a conceptual combination of its children. A biological taxonomy is a canonical example of this definition, because a \u0445 bshi @ nd.edu \u2020 tweninge @ nd.eduar Xiv: 160 1.00 626v 1 [cs.A I] 4 Jan 201 62 classified species, say Homo sapiens (i.e. humans), can only be placed on one leaf in the taxonomy; the inner nodes, e.g. primates, mammals, animals, do not declare new species, but they are conceptual agglomerations of species. Furthermore, each species is described by its path through the taxonomy, so that Homo sapiens, generally referred to as command, mammals, and hierarchy, are not necessarily objects on the top of the hierarchy (i.e., an object above the hierarchy)."}, {"heading": "B. Hierarchies of Documents", "text": "In similar studies, \"document hierarchies\" were not actual hierarchies of documents in the literal sense. For example, Hierarchical LDA (hLDA) [7, 8], TopicBlock [3], and the Tree Structured Stick Breaking (TSSB) model [1] learn a conceptual taxonomy in which the non-leaf topics are a combination of words and do not represent a real document in the corpus; the hierarchical Pachinko allocation model (hPAM) [6] constructs a tree-like conceptual taxonomy like hLDA, where each topic may have multiple parents. In these related models, only the sheets contained the actual literary documents. Contrary to our point of view, the internal nodes of existing models contain ephemeral word-theme distributions rather than actual documents. See Figure 3 for a brief comparison of model editions. The HDTM model presented in this paper requires that internal nodes, which are examined in these earlier work, are more likely to be made of these general documents than in some emergency situations."}, {"heading": "1. Web sites as Document Hierarchies", "text": "A Site G can be viewed as a guided graphic with Web pages as vertices V and hyperlinks as directed edges E between Web pages vx \u2192 vy - without Intersite hyperlinks. In most cases, naming the Web site entrance page as root r allows a Web site to be viewed as a rooted guided graphic. Site creators and curators purposefully organize the hyperlinks between documents in a topically meaningful manner. As a result, Web documents farther from the root document typically contain more specific topics than Web documents that are graphically located close to the root documentation. For example, the Web site at the University of Notre Dame, shown in Figure 1, shows a root-linked Web document (the front page), and dozens of children Web documents. Even with a very small subset of documents and edges, the corresponding Web graphics can be quite complicated and chaotic."}, {"heading": "2. Term propagation in Document Graphs", "text": "The document graph structure is also used to enrich the document description by adding functions to improve retrieval performance. Some of the intuition behind this previous work is helpful in designing the generative model. A limitation of the Random Walker model is that it only looks at the graphical structure of the network. Clearly, the word distributions found in each document are an important factor to take into account when creating document hierarchies. Previous work by Song, et al. [9] and Qin, et al. [10] shows that a particular website can be enriched by the dissemination of information from their children. Their relevance distribution models modify the language distribution of a website to be a mixture of themselves and their children. [9] and Qin, et al. [10] show that a particular website can be enriched by the dissemination of information from their children."}, {"heading": "C. Other hierarchies", "text": "Documents from many different collections exist in hidden hierarchies. While technically a website, Wikipedia documents and categories form a unique document graph. Wikipedia categories are particularly interesting because they provide a kind of ontology within which categories have more specific sub-categories and more general parent categories. Most Wikipedia articles are represented by at least one category description, allowing the user to access relevant articles in a few clicks by leafing through the category graphics. A partial example of the Wikipedia category graph is shown in Figure 2. This illustration illustrates how a document graph can be interpreted into a document hierarchy. Specifically, the Wikipedia community has crafted a category hierarchy represented by colored circles and edges, at the top of the article graph, represented by gray squares and gray edges. Although the category graph is imperfect and incomplete, Wikipedia's browser can maintain a sense of fit and granularity for the article."}, {"heading": "D. Challenges and Contributions", "text": "The aim of this work is to construct node hierarchies from an information network in which the nodes (e.g. text) and the nodes are linked to each other."}, {"heading": "II. RELATED WORK", "text": "The first efforts in hierarchical clusters based on hierarchical hypotheses have recently been using much hierarchical explanations to observe the Chinese accumulation of Chinese models. [17] The first attempts at hierarchical clusters based on hierarchical hierarchies are likely to be made by Chinese SDA models. [17] The first attempts at hierarchical clusters based on hierarchical hypotheses are made by Chinese SDA users. [17] Each of these approaches is a complex process in which the root node is divided into a series of branches, each containing a single document in each sheet. Other hierarchical clustering algorithms include top-down processes that iteratively divide the data [19], incremental methods such as CobWeb [20], Classit [21] and other algorithms that are likely to fit for hierarchical text clustering.The processes that typically define most hierarchical clusters in an apparent environment."}, {"heading": "III. HIERARCHICAL DOCUMENT TOPIC MODEL", "text": "Unlike previous algorithms that detect latent theme taxonomies, the Hierarchical Document-Theme Model (HDTM) finds hidden hierarchies by selecting edges in the document diagram. In this section, a detailed description of the model is presented, starting with a document diagram G = {V, E} of documents V and edges E. Each document is a collection of words in which a word is an element in a vocabulary. The basic assumption of HDTM and similar models is that each document can be generated by probabilistic mixing of words from different topics. Distributions over topics are represented by z, which is a multinomic variable with an associated set of distributions over the words p (w | z, \u03b2), where \u03b2 is a dirichlet hyperparameter."}, {"heading": "A. Random Walks with Restart", "text": "The nCRP stochastic process could not be used to derive hierarchical documents because the nCRP process forces documents to the leaves in the tree. HDTM replaces nCRP with a random path with a restart (RWR) (also known as Personalized PageRank (PPR)) [41]. In contrast, the random path with teleportation (also known as PageRank) takes random paths by selecting a random starting point, and, with probability (1 \u2212 \u03b3), the walker randomly goes to a new connected place or selects a random place with a probability where it is called jump probability [42]. In HDTM, the root node is either fixed as the entrance page of a web page by another heuristic or manually generated document. Thus, for the purpose of hierarchical inference, the random walker is forced to start and restart the document at the root."}, {"heading": "B. Generating document paths", "text": "This is, in other words, the probability of landing on d is the product of the emission probabilities from each document in the path through T to d.This random walker function associates higher probabilities than those in lower hierarchy.This is, in the hierarchy, the product of the emission probabilities from each document."}, {"heading": "1. Sampling document paths", "text": "The first Gibbs sampling step consists of drawing a path from any document to the root through the graph. The sampling distribution for a path cd isp (cd | c, p, p, p, p) (cd, wd, w, w, w, w, w, p, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, p, p, w, p, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w, w"}, {"heading": "2. Sampling word levels", "text": "In view of the current state of all variables, the word pattern must first select an assignment z for the word n in document d. Sample distribution of zd, n isp (zd, n | c, z, w, p, p) (wd, n, zd, n | c, z \u2212 (d, n), w \u2212 (d, n), \u03b7) = p (wd, n | c, z, w \u2212 (d, n), \u03b7) p (zd, n | zd, \u2212 n, c, \u03b3) (7), where zd, \u2212 n = {zd, n, n, n \u2212 wd, n. The first term is a distribution by word assignments: p (wd, n | c, z, w \u2212 dp) (7), where zd, \u2212 n \u2212 zd, n \u2212 stors refer to the condition."}, {"heading": "D. Distributed HDTM", "text": "A common complaint among data scientists is that graphical models, especially non-parametric Bayesian graphical models, do not perform well in scale. Therefore, we have also implemented the HDTM inference algorithm in the scalable, distributed vertex programming paradigm. The mechanism behind Gibbs sampling and other Markov Chain Monte Carlo methods requires sequential sampling steps, and the execution of each step depends on the results of the previous step, making Gibbs samplers and the MCMC method generally difficult to parallelise. Approximate distributed LDA (AD-LDA) is an attempt to find approximate distributed solutions to the serial inference problem by splitting documents into P parts where P is the number of processors and the distribution of topics z is globally initialized. Then, for each Gibbs iteration, each processor of the 1th pct of the data set is used under the topics."}, {"heading": "1. Vertex Programming", "text": "Although MapReduce is a widely used, universally accepted parallel scheme that can easily handle scalable data, it is not automatically optimized for iterative computing tasks such as statistical inference or logistical regression [44], because MapReduce materializes all intermediate results and brings them to disk to tolerate task failures *. Mapreduce therefore has relatively high I / O costs compared to other designs that keep data in memory over iterations [45]. Apart from MapReduce, another scalable solution is to build a custom distributed system using a message delivery interface (MPI). Custom approaches are usually closer to optimal, because developers can adjust the code based on their own needs and minimize unnecessary overhead. However, the disadvantages are also significant: because MPI is a barebone communication specification that developers must use their own code for dispatching, load distribution, and node handling."}, {"heading": "2. Distributed Inference Algorithm", "text": "Algorithm 3: Path-Global Update globals: Vertices V, Hierarchy T, Restart Prob. \u03b3 Input: Receive messages containing path probabilities m Output: Messages sent to adjacent edges for node u * / if m \u2190 getmsg () then for each document in parallel * / foreach u-cd, k do sendmsg (u, [dk.n, dk.z]) / * Send local n, z to node u * / if m \u2190 getmsg () then for each u-m do dk.n \u2190 u.n dk.z \u2190 The distributed HDTM inference algorithm is similar to procedural HDTM. We do not detail the entire distributed HDTM inference algorithm in this paper; however, the source code is referenced in Section V. To adapt HDTM to the vertex programming model, changes in the sampling sequence and attention to global synchronization are executed."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "This section describes the method and results for evaluating the HDTM model. It shows a quantitative and qualitative analysis of the ability of the hierarchical document topic model to learn accurate and interpretable hierarchies of document graphs. Main evaluations examine the empirical probability of the data and a very large case study in which human judges are asked to evaluate the constructed hierarchies."}, {"heading": "A. Data", "text": "In fact, most of them are able to outdo themselves by outdoing themselves, by outdoing themselves, by outdoing themselves, by outdoing themselves, by outdoing themselves, by outdoing themselves, by outdoing themselves, by outdoing, taking advantage of, taking advantage of, and taking advantage of."}, {"heading": "B. Summary of the Sampling Algorithm", "text": "Using the traditional sequence model from Sec. III C or the high-throughput distributed sampling algorithm from Sec. III D, we essentially relax by scanning paths for each node CD except for the root and words in document zd, n. Given the state of the sampler at a given time t, i.e. c (t) 1: D and z (t) 1: D, sample each variable bound to the others, as in III B. The conditional distribution of latent variables in the HDTM model determines the given document network. After executing the Markov chain, we approach the stationary distribution sufficiently; the process of approaching the stationary distribution is referred to as the \"burn-in\" period. After burning, we collect samples at a selected interval, i.e. the sampling storage (the collected \"samples\" from the Markov chain are complete hierarchies."}, {"heading": "C. Quantitative Analysis", "text": "HDTM has some distinct qualities that make it difficult to compare apples and apples. However, since HDTM is the first model to generate document hierarchies based on diagrams, there is nothing to compare directly with. However, some of the models in the related work perform similar tasks, and so comparisons have been made when applicable. The related models typically perform quantitative evaluation by measuring the likelihood of log on held data or by performing another task such as link prediction. Protocol probability analysis looks at the quality of fit accuracy of the data received. Unfortunately, it is not possible to create a \"hold out\" dataset, as each document, especially documents on the first or second level of the document, is very important for the resulting hierarchy. Removing certain documents might even make hierarchy conclusions impossible. Instead, for the first quantitative evaluation, a comparison with the other models we compare is possible."}, {"heading": "1. Large-scale Analysis", "text": "Remember that the goal of HDTM is to generate document hierarchies from an idea or an object, because each article can have several categories. Therefore, a positive result should somehow show that HDTM has hierarchies that resemble the hierarchy that a human (or many people) would generate based on the same document graph. In this sense, we conducted a large-scale experiment on the full English Wikipedia article chart. Remember also that the Wikipedia category chart-graphic-graphic-graphic-graphic-graphic-graphic-graphic-graphic-graphic-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-category-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-nearly-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-category-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-almost-chart-chart-chart-chart-chart-chart-document-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart-chart"}, {"heading": "D. Discussion", "text": "To correctly understand the results recorded in Table III from above, we remember that the probability in the log is a measure of the fit of the observations to the configuration of the model. Initial work on LDA [2] found that the more topics there are, the more likely the probability increases. Along these lines, Chang, et al. showed that finer-grained topics appearing in models with a larger number of topics have a lower interpretative ability, although they have higher probability values [55]. Simply put, there is a negative correlation between probability values and human interpretative ability in LDA and similar topic models. Applying these teachings to our experiments, we remember that HDTM has as many topics as there are documents, and non-root document topics are mixtures of topics on the way to the root. Also, we remember that HLDA, TopicBlock and TSSB all generate a large number of latent topics. In HLDA and the number of topical document topics, there are practically a lot of definitive CRP seen in the hierarchical block / DA number."}, {"heading": "E. Qualitative Analysis", "text": "In order to measure the coherence of the inferred groupings, the word intruder task developed by Chang et al. [55] is slightly modified to create the intruder task, in which a human subject is presented with a randomly arranged set of eight document titles. The task of the human judge is to find the intruder, that is, the judge is asked to find out which document is out of place or does not belong. If the amount of documents without the intruder document all makes sense together, then the human judge should easily be able to find the intruder. For example, most people, even non-computer scientists, would choose Alan Turing as the intruder because the remaining words together are meaningful - they are all students of computer science. Because the set {systems, networks, RAM, databases, graphics, Alan Turing} represent selective groupings that identify a single intruder."}, {"heading": "1. Comparison Models", "text": "HDTM and fsLDA store documents at each node in the hierarchy. A grouping is selected by first selecting a document at random and then selecting its siblings. TopicBlock and fsLDA store documents at the sheets of taxonomy, which often comprise several documents. From these models, a grouping is selected by first selecting a document at random and then selecting the other documents in the sheet. The hierarchies constructed by the TSSB model allowed several documents to live at inner nodes. Attempts to evaluate groupings at inner nodes with more than 4 documents were unsuccessful. Nodes with 4 or more siblings were also difficult to find because the hierarchies that were created were too sparse to find practical groupings. Thus, human judges with TSSB groupings could not be found. Each document collection had different types of labels presented to the judges. The CompSci site collection was labeled by the website, the page with the title of the website and the Wikipedia page with the illustration of the page 9."}, {"heading": "2. Analyzing human judgments", "text": "The intrusive tasks described above are selected by the judges themselves. (The intrusive tasks are selected by the judges themselves.) The intrusive tasks are evaluated by the judges themselves. (The intruders are recognized by the judges.) The intruders are identified by the judges. (The intruders are identified by the judges.) The intruders are identified by the judges. (The intruders are identified by the judges.) If the intruders choose the word wmk from the model m and the task, the intruder is selected by the judges. (The intruders are identified by the judges.) The intruders are selected by the judges. (The intruders are identified by the judges.) The intruders are identified by the judges. (The intruders are identified by the judges.) The intruders are identified by the judges. (The intruders are identified by the judges.) The intruders are identified by the judges."}, {"heading": "F. Reproducibility", "text": "The HDTM source code, analysis code, and scripts that produced the results of this work can be downloaded at https: / / github.com / nddsg / HDTM / releases / tag / kais. Wikipedia, website, and bibliographic data are all publicly available and can be downloaded free of charge and replicated in our own data storage.24"}, {"heading": "V. CONCLUSIONS", "text": "The Hierarchical Document Theme Model (HDTM) is a Bayesian generative model that creates document and theme hierarchies from entrenched document diagrams; the original hypothesis was that document graphs such as websites, Wikipedia, and bibliographic networks contain a hidden hierarchy. Unlike most previous work, HDTM allows documents to live in the hierarchy on non-leaf-based nodes, requiring a random path with a method to restart path sampling. An interesting side effect of the Random Walker adaptation is that the path sampling step, Eq. 4, is much faster and easier to scale than the nCRP, as RWR only creates a sample distribution for the parents of a document, while the nCRP process creates a sample distribution across all possible paths within the taxonomy."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is sponsored by an AFOSR grant FA9550-15-1-0003, and a John Templeton Foundation grant FP053369-M. [1] R. P. Adams, Z. Ghahramani, and M. I. Jordan, in NIPS (2010) p. 19-27. [2] D. M. Lead, A. Y. Ng, and M. I. Jordan, The Journal of Machine Learning Research 3, 993 (2003). [3] Q. Ho, J. Eisenstein, and E. P. Xing, in WWW (New York, New York, USA, 2012) p. 739. [4] A. Chambers, P. Smyth, and M. Steyvers, in NIPS. T. 334-342. [5] J. Chang and D. M. Blei, Annals of Applied Statistics 4, 121 (2010). [6] D. Mimno, W. Li, and A. McCallum, in ICML (York, New York, New York, New York, New York, USA, 2007) 633-73."}], "references": [{"title": "The Journal of Machine Learning Research 3", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "993 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "and E", "author": ["Q. Ho", "J. Eisenstein"], "venue": "P. Xing, in WWW ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "and M", "author": ["A. Chambers", "P. Smyth"], "venue": "Steyvers, in NIPS ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Annals of Applied Statistics 4", "author": ["J. Chang", "D.M. Blei"], "venue": "121 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "and A", "author": ["D. Mimno", "W. Li"], "venue": "McCallum, in ICML ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "and J", "author": ["D.M. Blei", "T.L. Griffiths", "M.I. Jordan"], "venue": "B. Tenenbaum, in NIPS ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Journal of the ACM 57", "author": ["D.M. Blei", "T.L. Griffiths", "M.I. Jordan"], "venue": "7 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "J", "author": ["R. Song", "J.-R. Wen", "S. Shi", "G. Xin", "T.-Y. Liu", "T. Qin", "X. Zheng"], "venue": "Zhang, G.-R. Xue, and W.-Y. Ma, in TREC ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Z", "author": ["T. Qin", "T.-Y. Liu", "X.-D. Zhang"], "venue": "Chen, and W.-Y. Ma, in SIGIR ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "ACM TOIS 22", "author": ["C. Zhai", "J. Lafferty"], "venue": "179 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "PloS one 8", "author": ["C. Lin", "Y. Zou", "J. Qin", "X. Liu", "Y. Jiang", "C. Ke", "Q. Zou"], "venue": "e56499 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Journal of molecular biology 247", "author": ["A.G. Murzin", "S.E. Brenner", "T. Hubbard", "C. Chothia"], "venue": "536 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}, {"title": "in AISTATS", "author": ["R. Nallapati", "D.A. McFarland", "C.D. Manning"], "venue": "Vol. 15 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "and M", "author": ["T. Furukawa", "Y. Matsuo", "I. Ohmukai", "K. Uchiyama"], "venue": "Ishizuka, in ICWSM ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Information Processing & Management 24", "author": ["P. Willett"], "venue": "577 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1988}, {"title": "Data Mining and Knowledge Discovery 10", "author": ["Y. Zhao", "G. Karypis", "U. Fayyad"], "venue": "141 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Machine Learning 2", "author": ["D.H. Fisher"], "venue": "139 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1987}, {"title": "Artificial Intelligence 40", "author": ["J.H. Gennari", "P. Langley", "D. Fisher"], "venue": "11 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "and K", "author": ["Y. Petinot", "K. McKeown"], "venue": "Thadani, in ACL ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "ACL", "author": ["J. Reisinger", "M. Paca"], "venue": "620 ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "and Y", "author": ["J. Huang", "H. Sun", "J. Han", "H. Deng", "Y. Sun"], "venue": "Liu, in CIKM ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Nature 453", "author": ["A. Clauset", "C. Moore", "M.E.J. Newman"], "venue": "98 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Social Networks 5", "author": ["P.W. Holland", "K.B. Laskey", "S. Leinhardt"], "venue": "109 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1983}, {"title": "Foundations and Trends in Machine Learning Foundations and Trends in Machine Learning", "author": ["A. Goldenberg", "A.X. Zheng", "S.E. Fienberg", "E.M. Airoldi"], "venue": "2, 129 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "and C", "author": ["Q. Mei", "D. Cai", "D. Zhang"], "venue": "Zhai, in WWW ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "and W", "author": ["R.M. Nallapati", "A. Ahmed", "E.P. Xing"], "venue": "W. Cohen, in SIGKDD ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "and Y", "author": ["A. Gruber", "M. Rosen-Zvi"], "venue": "Weiss, in UAI ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "and X", "author": ["A. McCallum", "A. Corrada-Emmanuel"], "venue": "Wang, in IJCAI ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "and P", "author": ["M. Rosen-Zvi", "T.L. Griffiths", "M. Steyvers"], "venue": "Smyth, in UAI ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "and A", "author": ["D. Newman", "P. Smyth", "M. Welling"], "venue": "U. Asuncion, in Advances in neural information processing systems ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "and A", "author": ["P. Smyth", "M. Welling"], "venue": "U. Asuncion, in Advances in Neural Information Processing Systems ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "and A", "author": ["A. Ahmed", "M. Aly", "J. Gonzalez", "S. Narayanamurthy"], "venue": "J. Smola, in Proceedings of the fifth ACM international conference on Web search and data mining ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "and J", "author": ["Y. Low", "D. Bickson", "J. Gonzalez", "C. Guestrin", "A. Kyrola"], "venue": "M. Hellerstein, in Proceedings of the VLDB Endowment ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "and I", "author": ["R.S. Xin", "J.E. Gonzalez", "M.J. Franklin"], "venue": "Stoica, GRADES \u201913: First International Workshop on Graph Data Management Experiences and Systems ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "PVLDB 4", "author": ["B. Bahmani", "A. Chowdhury", "A. Goel"], "venue": "173 ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "and G", "author": ["G. Malewicz", "M.H. Austern", "A.J. Bik", "J.C. Dehnert", "I. Horn", "N. Leiser"], "venue": "Czajkowski, in Proceedings of the 2010 ACM SIGMOD International Conference on Management of data ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "in Proceedings of the 2nd USENIX conference on Hot topics in cloud computing", "author": ["M. Zaharia", "M. Chowdhury", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "Vol. 10 ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "SIGMOD Record 40", "author": ["K.-H. Lee", "Y.-J. Lee", "H. Choi", "Y.D. Chung", "B. Moon"], "venue": "11 ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Briefings in bioinformatics", "author": ["Q. Zou", "X.-B. Li", "W.-R. Jiang", "Z.-Y. Lin", "G.-L. Li", "K. Chen"], "venue": "bbs088 ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "and G", "author": ["R.R. McCune", "T. Weninger"], "venue": "Madey, ACM Computing Surveys ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "and H", "author": ["A. Mccallum", "D.M. Mimno"], "venue": "M. Wallach, in Advances in neural information processing systems ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "and S", "author": ["C.L. Giles", "K.D. Bollacker"], "venue": "Lawrence, in Proceedings of the third ACM conference on Digital libraries ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1998}, {"title": "and Z", "author": ["J. Tang", "J. Zhang", "L. Yao", "J. Li", "L. Zhang"], "venue": "Su, in SIGKDD ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2008}, {"title": "SDM", "author": ["C. Faloutsos", "D. Koutra", "J.T. Vogelstein"], "venue": "162 ", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "and D", "author": ["J. Chang", "S. Gerrish", "C. Wang", "J.L. Boyd-graber"], "venue": "M. Blei, in Advances in neural information processing systems ", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "This process becomes increasingly impractical as the number of documents grows to Web-scale, and has motivated research towards the automatic inference of taxonomies [1\u20136].", "startOffset": 166, "endOffset": 171}, {"referenceID": 1, "context": "This process becomes increasingly impractical as the number of documents grows to Web-scale, and has motivated research towards the automatic inference of taxonomies [1\u20136].", "startOffset": 166, "endOffset": 171}, {"referenceID": 2, "context": "This process becomes increasingly impractical as the number of documents grows to Web-scale, and has motivated research towards the automatic inference of taxonomies [1\u20136].", "startOffset": 166, "endOffset": 171}, {"referenceID": 3, "context": "This process becomes increasingly impractical as the number of documents grows to Web-scale, and has motivated research towards the automatic inference of taxonomies [1\u20136].", "startOffset": 166, "endOffset": 171}, {"referenceID": 4, "context": "This process becomes increasingly impractical as the number of documents grows to Web-scale, and has motivated research towards the automatic inference of taxonomies [1\u20136].", "startOffset": 166, "endOffset": 171}, {"referenceID": 5, "context": "For example, Hierarchical LDA (hLDA) [7, 8], TopicBlock [3], and the Tree Structured Stick Breaking (TSSB) model [1] learn a conceptual taxonomy in which the non-leaf topics are a combination of words and does not represent a real document in the corpus; the hierarchical Pachinko allocation model (hPAM) [6] constructs a tree-like conceptual taxonomy like hLDA, but where each topic can have multiple parents.", "startOffset": 37, "endOffset": 43}, {"referenceID": 6, "context": "For example, Hierarchical LDA (hLDA) [7, 8], TopicBlock [3], and the Tree Structured Stick Breaking (TSSB) model [1] learn a conceptual taxonomy in which the non-leaf topics are a combination of words and does not represent a real document in the corpus; the hierarchical Pachinko allocation model (hPAM) [6] constructs a tree-like conceptual taxonomy like hLDA, but where each topic can have multiple parents.", "startOffset": 37, "endOffset": 43}, {"referenceID": 1, "context": "For example, Hierarchical LDA (hLDA) [7, 8], TopicBlock [3], and the Tree Structured Stick Breaking (TSSB) model [1] learn a conceptual taxonomy in which the non-leaf topics are a combination of words and does not represent a real document in the corpus; the hierarchical Pachinko allocation model (hPAM) [6] constructs a tree-like conceptual taxonomy like hLDA, but where each topic can have multiple parents.", "startOffset": 56, "endOffset": 59}, {"referenceID": 4, "context": "For example, Hierarchical LDA (hLDA) [7, 8], TopicBlock [3], and the Tree Structured Stick Breaking (TSSB) model [1] learn a conceptual taxonomy in which the non-leaf topics are a combination of words and does not represent a real document in the corpus; the hierarchical Pachinko allocation model (hPAM) [6] constructs a tree-like conceptual taxonomy like hLDA, but where each topic can have multiple parents.", "startOffset": 305, "endOffset": 308}, {"referenceID": 7, "context": "[9] and Qin, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] show that a given Web page can be enriched by propagating information from its children.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "For illustration purposes, we apply a Dirichlet prior smoothing function [11] to smooth the term distribution where the f \u2032(w; d) from above is used in place of the usual c(w; d) from the original Dirichlet prior smoothing function yielding:", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "Apart from Web and citation graphs, bioinformatics networks, for example protein networks, can also be hierarchically organized for protein fold prediction [13].", "startOffset": 156, "endOffset": 160}, {"referenceID": 11, "context": "The nodes in such network are proteins and two proteins are connected if they have structural or evolutionary relationships [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 5, "context": "Unlike existing models, such as hLDA [7, 8], that select topic paths using the nested Chinese Restaurant Process (nCRP), HDTM performs document placement based on a stochastic process resembling random walks with restart (RWR) over the original document-graph.", "startOffset": 37, "endOffset": 43}, {"referenceID": 6, "context": "Unlike existing models, such as hLDA [7, 8], that select topic paths using the nested Chinese Restaurant Process (nCRP), HDTM performs document placement based on a stochastic process resembling random walks with restart (RWR) over the original document-graph.", "startOffset": 37, "endOffset": 43}, {"referenceID": 12, "context": "This limits the scalability of many topic diffusion algorithms [15, 16].", "startOffset": 63, "endOffset": 71}, {"referenceID": 13, "context": "This limits the scalability of many topic diffusion algorithms [15, 16].", "startOffset": 63, "endOffset": 71}, {"referenceID": 14, "context": "Initial efforts in hierarchical clustering used greedy heuristics such as single-link or complete-link agglomoration rules to infer dendrograms [17], in which the root node is split into a series of branches that terminate", "startOffset": 144, "endOffset": 148}, {"referenceID": 1, "context": ", point out that manually-curated Web hierarchies like the Open Directory Project[18] are typically flatter and contain fewer inner nodes than agglomerative clustering techniques produce [3].", "startOffset": 187, "endOffset": 190}, {"referenceID": 15, "context": "Other hierarchical clustering algorithms include top-down processes which iteratively partition the data [19], incremental methods like CobWeb [20], Classit [21], and other algorithms optimized for hierarchical text clustering.", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "Other hierarchical clustering algorithms include top-down processes which iteratively partition the data [19], incremental methods like CobWeb [20], Classit [21], and other algorithms optimized for hierarchical text clustering.", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "Other hierarchical clustering algorithms include top-down processes which iteratively partition the data [19], incremental methods like CobWeb [20], Classit [21], and other algorithms optimized for hierarchical text clustering.", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "This process has also been called the Chinese Restaurant Franchise because of this analogy [2].", "startOffset": 91, "endOffset": 94}, {"referenceID": 18, "context": "[23] hLLDA, as well as fixed structure LDA (fsLDA) by Reisinger and Pasca [24] which modify hLDA by fixing the hierarchical structure and learning hierarchical topic distributions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[23] hLLDA, as well as fixed structure LDA (fsLDA) by Reisinger and Pasca [24] which modify hLDA by fixing the hierarchical structure and learning hierarchical topic distributions.", "startOffset": 74, "endOffset": 78}, {"referenceID": 4, "context": ", higher level, topics [6].", "startOffset": 23, "endOffset": 26}, {"referenceID": 20, "context": "The SHRINK algorithm creates hierarchical clusters by identifying tightly-knit communities and by finding disparate clusters by looking for hubs and other heuristics [25].", "startOffset": 166, "endOffset": 170}, {"referenceID": 21, "context": "Clauset, et al, discover dendrograms by Monte Carlo sampling [26]; however, dendrograms poorly represent the manually curated hierarchies and taxonomies that we are pursuing.", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "Stochastic block models (SBM) are an alternative line of network clustering research that partitions nodes into communities in order to generatively infer link probabilities [27].", "startOffset": 174, "endOffset": 178}, {"referenceID": 23, "context": "Several extensions to the original SBM have since been proposed (for a survey see [28]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "Topic Modeling with Network Structure (TMN) is similar in this regard because it regularizes a statistical topic model with a harmonic regularizer based on the graph structure in the data; the result is that topic proportions of linked documents are similar to each other [29].", "startOffset": 272, "endOffset": 276}, {"referenceID": 25, "context": "Other work on generative models that combine text and links include: a probabilistic model for document connectivity [31], the Link-PLSA-LDA and Pairwise-Link-LDA methods [32], the Latent Topic Model for Hypertext (LTHM) method [33], role discovery in social networks [34], the author-topic-model [35], and others.", "startOffset": 171, "endOffset": 175}, {"referenceID": 26, "context": "Other work on generative models that combine text and links include: a probabilistic model for document connectivity [31], the Link-PLSA-LDA and Pairwise-Link-LDA methods [32], the Latent Topic Model for Hypertext (LTHM) method [33], role discovery in social networks [34], the author-topic-model [35], and others.", "startOffset": 228, "endOffset": 232}, {"referenceID": 27, "context": "Other work on generative models that combine text and links include: a probabilistic model for document connectivity [31], the Link-PLSA-LDA and Pairwise-Link-LDA methods [32], the Latent Topic Model for Hypertext (LTHM) method [33], role discovery in social networks [34], the author-topic-model [35], and others.", "startOffset": 268, "endOffset": 272}, {"referenceID": 28, "context": "Other work on generative models that combine text and links include: a probabilistic model for document connectivity [31], the Link-PLSA-LDA and Pairwise-Link-LDA methods [32], the Latent Topic Model for Hypertext (LTHM) method [33], role discovery in social networks [34], the author-topic-model [35], and others.", "startOffset": 297, "endOffset": 301}, {"referenceID": 3, "context": "The relational topic model (RTM) builds links between topics, where observed links are given a very high likelihood [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 22, "context": "The TopicBlock model combines the non-parametric hLDA and stochastic block models [27] to generate document taxonomies from text and links [3]; however, TopicBlock, like hLDA, does not permit documents to reside at non-leaf nodes of the resulting tree.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "The TopicBlock model combines the non-parametric hLDA and stochastic block models [27] to generate document taxonomies from text and links [3]; however, TopicBlock, like hLDA, does not permit documents to reside at non-leaf nodes of the resulting tree.", "startOffset": 139, "endOffset": 142}, {"referenceID": 29, "context": "Newman, et al proposed an exact distributed Gibbs sampling algorithm as well as an approximate distributed Gibbs sampling algorithm that uses local Gibbs sampling and global synchronization [36].", "startOffset": 190, "endOffset": 194}, {"referenceID": 30, "context": "Smyth, et al introduced an asynchronous distributed algorithm that was capable of learning LDA-style topics [37]; and Ahmed, et al recently released Yahoo LDA, which is a scalable approximate inference framework on large-scale streaming data [38].", "startOffset": 108, "endOffset": 112}, {"referenceID": 31, "context": "Smyth, et al introduced an asynchronous distributed algorithm that was capable of learning LDA-style topics [37]; and Ahmed, et al recently released Yahoo LDA, which is a scalable approximate inference framework on large-scale streaming data [38].", "startOffset": 242, "endOffset": 246}, {"referenceID": 32, "context": ", Pregel, GraphLab and GraphX [39, 40], can distribute sampling operations at a much finer granularity by treating each graph-node as an independent computing unit.", "startOffset": 30, "endOffset": 38}, {"referenceID": 33, "context": ", Pregel, GraphLab and GraphX [39, 40], can distribute sampling operations at a much finer granularity by treating each graph-node as an independent computing unit.", "startOffset": 30, "endOffset": 38}, {"referenceID": 34, "context": "HDTM replaces nCRP with random walk with restart (RWR) (which is also known as Personalized PageRank (PPR)) [41].", "startOffset": 108, "endOffset": 112}, {"referenceID": 1, "context": "This is in line with the intuition that flatter hierarchies are easier for human understanding than deep hierarchies [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "6 is adapted from the standard ratio of normalizing constants for the Dirichlet distribution [8].", "startOffset": 93, "endOffset": 96}, {"referenceID": 29, "context": "When all processors are finished, a global synchronization is performed and z is updated [36].", "startOffset": 89, "endOffset": 93}, {"referenceID": 35, "context": "The process of random walks over the network topology combined with the term sampling process described above is a good candidate for the vertex-programming paradigm using frameworks like Pregel [43] or GraphLab [39].", "startOffset": 195, "endOffset": 199}, {"referenceID": 32, "context": "The process of random walks over the network topology combined with the term sampling process described above is a good candidate for the vertex-programming paradigm using frameworks like Pregel [43] or GraphLab [39].", "startOffset": 212, "endOffset": 216}, {"referenceID": 36, "context": "Although MapReduce is a widely used, general purpose parallel scheme that can easily deal with scalable data, it is not optimized for iterative computational tasks such as statistical inference or logistic regression [44].", "startOffset": 217, "endOffset": 221}, {"referenceID": 37, "context": "Mapreduce, therefore, has relatively high I/O costs compared to other designs that keep data in memory across iterations [45].", "startOffset": 121, "endOffset": 125}, {"referenceID": 38, "context": "However, the drawbacks are also significant: because MPI is a barebone communication specification developers need to write their own code for job dispatching, load balancing, and dealing with node failure [46].", "startOffset": 206, "endOffset": 210}, {"referenceID": 39, "context": "to implement, can be distributed easily, and are much more computationally efficient than conventional, procedural programming when working with iterative computational tasks [47].", "startOffset": 175, "endOffset": 179}, {"referenceID": 26, "context": ", crawled 105 pages starting with the article on the NIPS conference finding 799 links [33].", "startOffset": 87, "endOffset": 91}, {"referenceID": 1, "context": "performed a larger evaluation of their TopicBlock model using 14,675 document with 152,674 links; however, they truncated each article to only the first 100 terms and limited the vocabulary to the 10,000 most popular words [3].", "startOffset": 223, "endOffset": 226}, {"referenceID": 40, "context": "We did not perform any text preprocessing procedure, including stop word removal and stemming, in any of the experiments due to empirical findings that these models are robust to the presents of stop words, etc [48].", "startOffset": 211, "endOffset": 215}, {"referenceID": 41, "context": "Due to different citation styles, vendor abbreviations, and ambiguous names, without human interference most bibliographical metadata extraction algorithms can only extract a portion of correct data [49\u201351].", "startOffset": 199, "endOffset": 206}, {"referenceID": 42, "context": "Due to different citation styles, vendor abbreviations, and ambiguous names, without human interference most bibliographical metadata extraction algorithms can only extract a portion of correct data [49\u201351].", "startOffset": 199, "endOffset": 206}, {"referenceID": 42, "context": "Here we choose to use the most complete citation data set available from by the authors of the ArnetMiner project [51], however the citation graph is not guaranteed to be complete.", "startOffset": 114, "endOffset": 118}, {"referenceID": 5, "context": "Quantitative experiments were performed on many of the aforementioned algorithms including: HLDA [7, 8], TopicBlock [3], TSSB [1], and fsLDA [24].", "startOffset": 97, "endOffset": 103}, {"referenceID": 6, "context": "Quantitative experiments were performed on many of the aforementioned algorithms including: HLDA [7, 8], TopicBlock [3], TSSB [1], and fsLDA [24].", "startOffset": 97, "endOffset": 103}, {"referenceID": 1, "context": "Quantitative experiments were performed on many of the aforementioned algorithms including: HLDA [7, 8], TopicBlock [3], TSSB [1], and fsLDA [24].", "startOffset": 116, "endOffset": 119}, {"referenceID": 19, "context": "Quantitative experiments were performed on many of the aforementioned algorithms including: HLDA [7, 8], TopicBlock [3], TSSB [1], and fsLDA [24].", "startOffset": 141, "endOffset": 145}, {"referenceID": 43, "context": "Among many options we chose DeltaCon [53] to measure the similarity between HDTM\u2019s hierarchy and the Wikipedia category hierarchy three reasons: 1) DeltaCon can calculate the similarity between two graphs with nodes that partially overlap, as observed in the messy Wikipedia category graph, 2) the underlying metric in DeltaCon is the affinity score calculated by fast belief propagation (FABP), which models the connectivity of nodes, and 3) DeltaCon can process millions of nodes in only a few hours.", "startOffset": 37, "endOffset": 41}, {"referenceID": 31, "context": "[38] and Smyth et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The original work on LDA [2] found that likelihood increases as the number of topics increases.", "startOffset": 25, "endOffset": 28}, {"referenceID": 44, "context": "demonstrated that more fine grained topics, which appear in models with a larger number of topics have a lower interpretability, despite having higher likelihood scores [55].", "startOffset": 169, "endOffset": 173}, {"referenceID": 44, "context": "To measure the coherence of the inferred groupings, the word intrusion task developed by Chang et al [55] is slightly modified to create the document intrusion task.", "startOffset": 101, "endOffset": 105}, {"referenceID": 44, "context": "As in [55], the likelihood scores do not necessarily correspond to human judgments.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "[2] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] Q.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[19] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[20] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[21] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[23] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[24] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[25] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[26] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[27] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[28] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[29] Q.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[32] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[33] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[34] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[35] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[36] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[37] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[38] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[39] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[40] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[41] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[43] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[44] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[45] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[46] Q.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[47] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[48] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[49] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[51] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[53] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[55] J.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Information hierarchies are organizational structures that often used to organize and present large and complex information as well as provide a mechanism for effective human navigation. Fortunately, many statistical and computational models exist that automatically generate hierarchies; however, the existing approaches do not consider linkages in information networks that are increasingly common in real-world scenarios. Current approaches also tend to present topics as an abstract probably distribution over words, etc rather than as tangible nodes from the original network. Furthermore, the statistical techniques present in many previous works are not yet capable of processing data at Web-scale. In this paper we present the Hierarchical Document Topic Model (HDTM), which uses a distributed vertex-programming process to calculate a nonparametric Bayesian generative model. Experiments on three medium size data sets and the entire Wikipedia dataset show that HDTM can infer accurate hierarchies even over large information networks.", "creator": "LaTeX with hyperref package"}}}