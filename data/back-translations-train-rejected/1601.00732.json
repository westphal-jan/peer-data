{"id": "1601.00732", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "Low-Rank Representation over the Manifold of Curves", "abstract": "In machine learning it is common to interpret each data point as a vector in Euclidean space. However the data may actually be functional i.e.\\ each data point is a function of some variable such as time and the function is discretely sampled. The naive treatment of functional data as traditional multivariate data can lead to poor performance since the algorithms are ignoring the correlation in the curvature of each function. In this paper we propose a method to analyse subspace structure of the functional data by using the state of the art Low-Rank Representation (LRR). Experimental evaluation on synthetic and real data reveals that this method massively outperforms conventional LRR in tasks concerning functional data.", "histories": [["v1", "Tue, 5 Jan 2016 04:21:45 GMT  (211kb,D)", "https://arxiv.org/abs/1601.00732v1", null], ["v2", "Wed, 6 Jan 2016 09:50:20 GMT  (211kb,D)", "http://arxiv.org/abs/1601.00732v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["stephen tierney", "junbin gao", "yi guo", "zhengwu zhang"], "accepted": false, "id": "1601.00732"}, "pdf": {"name": "1601.00732.pdf", "metadata": {"source": "CRF", "title": "Low-Rank Representation over the Manifold of Curves", "authors": ["Stephen Tierney", "Junbin Gao", "Yi Guo", "Zhengwu Zhang"], "emails": ["stierney@csu.edu.au;", "junbin.gao@sydney.edu.au;", "yi.guo@csiro.au;", "zhengwustat@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. LRR over the Curve Manifold", "text": "As already discussed, LRR is limited to a linear model and its current version can only be applied to vector data from a Euclidean space. Matrix Z in (1) or (2) encodes affinity / similarity between data points. However, this assumption is often unnatural and quite limited. Many of the data occurring in the real world is functional, in other words, it has a curve-like structure over a certain range. Euclidean linear models are not able to capture the nonlinear inventory embedded in each data point. For example, in thermal infrared data of geological substances, a curve can contain an important identifying feature, such as a decrease near a certain frequency. This decrease may shift or vary over time even for the same substance, because impurities occur. Under a linear vector model, this variation may cause the vector to move drastically in the surrounding euclidean space and cause poor results to shift or vary unambiguously for the same substance. Or, in other cases, this variation may cause the function to shrink or, in other cases, to be extended within a linear space."}, {"heading": "2.1. The Curve Manifold", "text": "In this essay we focus on the set of open curves, e.g. the curves do not form a loop (\u03b2). For the handling of general curves, we refer readers to [20]. The SRVF enables measurement and geometry, invariance up to the scaling, displacement and repair of the curves."}, {"heading": "2.2. The Proposed Curve LRR", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "3. Optimisation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Algorithm", "text": "(LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK) (LASK)"}, {"heading": "3.2. Complexity Analysis", "text": "To simplify the analysis, we first define some symbols used below. Let K and r specify the total number of iterations and the lowest rank of the matrix W. The size of W is N \u00d7 N. The main calculation cost of our proposed method consists of two parts, which calculate all Bi's and update W. With respect to the formula (9) to (4) and (5), the computational complexity of the log algorithm is O (T 2), where T is the number of terms in a discretized curve; therefore, the complexity of Bijk is at most O (T 2) and Bi's computational complexity is O (N2T 2). Thus, the total sum for Bi is O (N3). In each iteration of the algorithm, the singular value threshold is assumed to update the matrix W, the complexity of which is O (rN2)."}, {"heading": "3.3. Convergence Analysis", "text": "Algorithm 1 is adopted by the algorithm proposed in [11]. However, due to the terms of Bi in the objective function (11), the convergence theorem proved in [11] cannot be applied directly to this case, since the linearisation is applied to both the extended Lagrange terms and the term with Bi's. Fortunately, we can apply the revised approach presented in [30] to prove the convergence for the algorithm. Without repeating all the details, we present the convergence theorem for algorithm 1 as follows. Theorem 1 (convergence of the algorithm 1) If \u03b7W = max = Bi \u00b2 2 + N + 1, + 3, k = 1 \u03b2 \u2212 1k = + 1 \u2212 \u03b2k > C0 \u2022 i \u2022 Bi \u00b2 2\u03b7W \u2212 max {Bi \u00b2 2} \u2212 N, where C0 is the agive constant and vice versa < < < the algorithm < < the problem is generated by the algorithm < < the algorithm < in the objective function)."}, {"heading": "4. Experiments", "text": "In this section we show three groups of experiments to evaluate the newly proposed cLRR. The performance of the proposed method is compared with the same type of clustering algorithm LRR [12]. To compare the segmentation accuracy, we use the clustering accuracy (SCA) metric [6], which is defined as SCA = 1 \u2212 number of incorrectly classified points total number of points. (18) Therefore, a higher SCA% means greater clustering accuracy. The parameters used in all experiments with \u03bb for LRR were set to 1 and 0.1 for cLRR. Overall, a wide range of parameters were tested for each algorithm. Overall, we found that the segmentation accuracy of LRR does not vary so much with changes in der.4.1. Synthetic dataIn order to evaluate and confirm the effectiveness of the LRR method, we first perform experimental evaluations with synthetic data."}, {"heading": "4.2. Semi-synthetic TIR Data", "text": "We collect synthetic data from a library of pure infrared hyperspectral mineral data. For each cluster, we select a spectral sample from the library as the basis. Each curve base is then randomly shifted and stretched in a random section. This random distortion is performed 20 times to generate the curves for each cluster. See Figure 3 for an example of the data used in this experiment. In this experiment, we used three clusters. In this experiment, too, we repeated the test 50 times. The results are in Table 2 and Figure 4. The results show that LRR cannot cluster accurately data with this type of nonlinear inventory, which is often found in this type of data due to impurities in the mineral samples."}, {"heading": "4.3. Character Classification", "text": "In this experiment, a collection of handwritten English characters was used to evaluate the performance of a real dataset, which consists of position data collected from a digitization tablet at 200Hz, which is then converted into horizontal and vertical velocities. [27, 26] These 2D curves are normalized so that the mean value of each curve is close to zero. See Figure 5 for some examples of these data. Figure 6 shows the sample curves used in character classification. To evaluate performance, twenty characters were randomly selected from three character classes. Originally published data was carefully produced and processed so that the trajectories for each character are extremely similar. Far more than realistic. For example, the start time for each character was aligned with the writing speed, character size, and speed variance over time."}, {"heading": "5. Conclusion", "text": "In this thesis we extended the conventional LRR model in Euclidean space to a new LRR model for the diversity of open curves. The new LRR formulation is based on the approximation of the tangential space to the manifold, so that the classical data expressing itself can be well preserved for the diversity of the curves with relevant high accuracy. The resulting optimization problem can be solved with the help of the LADMAP technique, and convergence and complexity of the algorithms were presented. Finally, we tested the new model by experiments on synthetic, semi-synthetic and real data, and the experimental results show the outstanding performance compared to the conventional LRR. Our next work will extend the LRR model to the diversity of general closed curves."}, {"heading": "Acknowledgments", "text": "Financing information that is hidden for the review process."}], "references": [{"title": "Optimization algorithms on matrix manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": "Princeton University Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Functional subspace clustering with application to time series", "author": ["M.T. Bahadori", "D. Kale", "Y. Fan", "Y. Liu"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, pages 228\u2013237", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Pattern Recognition and Machine Learning", "author": ["C. Bishop"], "venue": "Information Science and Statistics. Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM J. on Optimization, 20(4):1956\u20131982", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust principal component analysis? Submitted for publication", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Stanford University", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse subspace clustering: Algorithm", "author": ["E. Elhamifar", "R. Vidal"], "venue": "theory, and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(11):2765\u20132781", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "editors", "author": ["F. Ferraty", "Y. Romain"], "venue": "The Oxford Handbook of Functional Data Analysis. Oxford University Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Low rank representation on Riemannian manifold of symmetrical positive definite matrices", "author": ["Y. Fu", "J. Gao", "X. Hong", "D. Tien"], "venue": "SIAM Conferences on Data Mining (SDM), pages 316\u2013324", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "A novel representation for Riemannian analysis of elastic curves in r", "author": ["S.H. Joshi", "E. Klassen", "A. Srivastava", "I. Jermyn"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20137", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning", "author": ["Z. Lin", "R. Liu", "H. Li"], "venue": "Machine Learning, 99:287\u2013325", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Linearized alternating direction method with adaptive penalty for low rank representation", "author": ["Z. Lin", "R. Liu", "Z. Su"], "venue": "Proceedings of NIPS", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["G. Liu", "Z. Lin", "S. Yan", "J. Sun", "Y. Yu", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(1):171\u2013184", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "International Encyclopedia of Statistical Science", "author": ["H.-G. M\u00fcller"], "venue": "chapter Functional data analysis, pages 554\u2013555. Springer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Proximal algorithms", "author": ["N. Parikh", "S. Boyd"], "venue": "Foundations and Trends in Optimization, 1(3):123\u2013231", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic time warping averaging of time series allows faster and more accurate classification", "author": ["F. Petitjean", "G. Forestier", "G.I. Webb", "A.E. Nicholson", "Y. Chen", "E. Keogh"], "venue": "icdm, 2014. In International Conference on Data Mining", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Addressing big data time series: Mining trillions of time series subsequences under dynamic time warping", "author": ["T. Rakthanmanon"], "venue": "ACM Transactions on Knowledge Discovery from Data, 7(3):1\u201331", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Functional Data Analysis", "author": ["J. Ramsay", "B.W. Silverman"], "venue": "Springer Series in Statistics. Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Functional analysis and partial matching in the square root velocity framework", "author": ["D. Robinson"], "venue": "PhD thesis, Florida State University", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 22:888\u2013905", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "Shape analysis of elastic curves in Euclidean spaces", "author": ["A. Srivastava", "E. Klassen", "S.H. Joshi", "I.H. Jermyn"], "venue": "IEEE Transactionson Pattern Analysis and Machine Intelligence, 33(7):1415\u20131428", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Registration of functional data using Fisher-Rao metric", "author": ["A. Srivastava", "W. Wu", "S. Kurtek", "E. Klassen", "J.S. Marron"], "venue": "varXiv:1103.3817", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Rate-invariant analysus of trajectories on Riemannian manifolds with application in visual speech recognition", "author": ["J. Su", "A. Srivastava"], "venue": "Proceedings of International Conference on Computer Vision and Pattern Recognition", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Detection", "author": ["J. Su", "A. Srivastava", "F.W. Huffer"], "venue": "classification and estimation of individual shapes in 2D and 3D point clouds. Computational Statistics & Data Analysis, 58:227\u2013 241", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Generative models for functional data using phase and amplitude separation", "author": ["J.D. Tucker", "W. Wu", "A. Srivastava"], "venue": "Computational Statistics and Data Analysis, 61:50\u201366", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Low rank representation on grassmann manifolds: An extrinsic perspective", "author": ["B. Wang", "Y. Hu", "J. Gao", "Y. Sun", "B. Yin"], "venue": "arXiv:1301.3529, 1:1\u20139", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Modelling motion primitives and their timing in biologically executed movements", "author": ["B. Williams", "M. Toussaint", "A.J. Storkey"], "venue": "Advances in neural information processing systems, pages 1609\u20131616", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Extracting motion primitives from natural handwriting data", "author": ["B.H. Williams", "M. Toussaint", "A.J. Storkey"], "venue": "Springer", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex optimization based low-rank matrix completion and recovery for photometric stereo and factor classification", "author": ["L. Wu", "A. Ganesh", "B. Shi", "Y. Matsushita", "Y. Wang", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "A nonlinear low-rank representation on Stiefel manifold", "author": ["M. Yin", "J. Gao", "Y. Guo"], "venue": "Electronics Letters, 51(10):749\u2013 751", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Graph dual regularized low-rank matrix approximation for data representation", "author": ["M. Yin", "J. Gao", "Z. Lin", "Q. Shi", "Y. Guo"], "venue": "IEEE Transactions on Image Processing, 24(12):4918\u2013 4933", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Videobased action recognition using rate-invariant analysis of covariance trajectories", "author": ["Z. Zhang", "J. Su", "E. Klassen", "H. Le", "A. Srivastava"], "venue": "arXiv:1503.06699v1, 1", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "In machine learning it is common to interpret each data point as a vector in Euclidean space [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "Analyzing functional data has been an emerging topic in statistical research [7, 13, 20, 21] and has attracted great attention from machine learning community in recent years [2, 15].", "startOffset": 77, "endOffset": 92}, {"referenceID": 12, "context": "Analyzing functional data has been an emerging topic in statistical research [7, 13, 20, 21] and has attracted great attention from machine learning community in recent years [2, 15].", "startOffset": 77, "endOffset": 92}, {"referenceID": 19, "context": "Analyzing functional data has been an emerging topic in statistical research [7, 13, 20, 21] and has attracted great attention from machine learning community in recent years [2, 15].", "startOffset": 77, "endOffset": 92}, {"referenceID": 20, "context": "Analyzing functional data has been an emerging topic in statistical research [7, 13, 20, 21] and has attracted great attention from machine learning community in recent years [2, 15].", "startOffset": 77, "endOffset": 92}, {"referenceID": 1, "context": "Analyzing functional data has been an emerging topic in statistical research [7, 13, 20, 21] and has attracted great attention from machine learning community in recent years [2, 15].", "startOffset": 175, "endOffset": 182}, {"referenceID": 14, "context": "Analyzing functional data has been an emerging topic in statistical research [7, 13, 20, 21] and has attracted great attention from machine learning community in recent years [2, 15].", "startOffset": 175, "endOffset": 182}, {"referenceID": 16, "context": "The classic functional principal component analysis (fPCA) [17] is one of such examples to discover dominant modes of variation in the data.", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": ", shifting and stretching) along the temporal axis [16, 24].", "startOffset": 51, "endOffset": 59}, {"referenceID": 23, "context": ", shifting and stretching) along the temporal axis [16, 24].", "startOffset": 51, "endOffset": 59}, {"referenceID": 22, "context": "Another important type of functional data is shape [23, 20].", "startOffset": 51, "endOffset": 59}, {"referenceID": 19, "context": "Another important type of functional data is shape [23, 20].", "startOffset": 51, "endOffset": 59}, {"referenceID": 8, "context": "A very useful shape representation is the square-root velocity function (SRVF) representation [9, 20].", "startOffset": 94, "endOffset": 101}, {"referenceID": 19, "context": "A very useful shape representation is the square-root velocity function (SRVF) representation [9, 20].", "startOffset": 94, "endOffset": 101}, {"referenceID": 11, "context": "In particular, we adapt the well known lowrank representation (LRR) framework [12] to deal with data that lie on the manifold of open curves by implementing the classical LRR in tangent spaces of the manifold [8, 25, 29].", "startOffset": 78, "endOffset": 82}, {"referenceID": 7, "context": "In particular, we adapt the well known lowrank representation (LRR) framework [12] to deal with data that lie on the manifold of open curves by implementing the classical LRR in tangent spaces of the manifold [8, 25, 29].", "startOffset": 209, "endOffset": 220}, {"referenceID": 24, "context": "In particular, we adapt the well known lowrank representation (LRR) framework [12] to deal with data that lie on the manifold of open curves by implementing the classical LRR in tangent spaces of the manifold [8, 25, 29].", "startOffset": 209, "endOffset": 220}, {"referenceID": 28, "context": "In particular, we adapt the well known lowrank representation (LRR) framework [12] to deal with data that lie on the manifold of open curves by implementing the classical LRR in tangent spaces of the manifold [8, 25, 29].", "startOffset": 209, "endOffset": 220}, {"referenceID": 11, "context": "LRR on Euclidean spaces [12] is closely related to several state-of-the-art subspace analysis approaches such as Sparse Subspace Clustering (SSC) [6], Robust PCA (RPCA) [5] and low-rank Matrix Completion (MC) [28] methods.", "startOffset": 24, "endOffset": 28}, {"referenceID": 5, "context": "LRR on Euclidean spaces [12] is closely related to several state-of-the-art subspace analysis approaches such as Sparse Subspace Clustering (SSC) [6], Robust PCA (RPCA) [5] and low-rank Matrix Completion (MC) [28] methods.", "startOffset": 146, "endOffset": 149}, {"referenceID": 4, "context": "LRR on Euclidean spaces [12] is closely related to several state-of-the-art subspace analysis approaches such as Sparse Subspace Clustering (SSC) [6], Robust PCA (RPCA) [5] and low-rank Matrix Completion (MC) [28] methods.", "startOffset": 169, "endOffset": 172}, {"referenceID": 27, "context": "LRR on Euclidean spaces [12] is closely related to several state-of-the-art subspace analysis approaches such as Sparse Subspace Clustering (SSC) [6], Robust PCA (RPCA) [5] and low-rank Matrix Completion (MC) [28] methods.", "startOffset": 209, "endOffset": 213}, {"referenceID": 18, "context": "The core of both SSC and LRR is to learn an affinity matrix for the given dataset and the learned affinity matrix will be pipelined to a spectral clustering method like nCUT [19] to obtain the final subspace labels.", "startOffset": 174, "endOffset": 178}, {"referenceID": 5, "context": "To learn the affinity matrix, SSC relies on the self expressive property [6], which is that", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "Similarly LRR [12] exploits the self expressive property but attempts to learn the global subspace structure by computing the lowest-rank representation of the set of data points.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "Given a smooth parameterized n-dimension curve \u03b2 : D = [0, 1] \u2192 R, we represent it using he square-root velocity function (SRVF) representation [9, 20], which is given by", "startOffset": 55, "endOffset": 61}, {"referenceID": 8, "context": "Given a smooth parameterized n-dimension curve \u03b2 : D = [0, 1] \u2192 R, we represent it using he square-root velocity function (SRVF) representation [9, 20], which is given by", "startOffset": 144, "endOffset": 151}, {"referenceID": 19, "context": "Given a smooth parameterized n-dimension curve \u03b2 : D = [0, 1] \u2192 R, we represent it using he square-root velocity function (SRVF) representation [9, 20], which is given by", "startOffset": 144, "endOffset": 151}, {"referenceID": 19, "context": "For handling general curves, we refer readers to [20].", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "Robinson [18] proved that if the curve \u03b2(t) is absolutely continuous, then its SRVF q(t) is square-integrable, i.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "This property has been exploited in [2] for functional data clustering under the subspace clustering framework.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "Different from the work proposed in [2], we will adopt the newly developed LRR on manifolds framework to the model of curves LRR, see [8, 25, 29].", "startOffset": 36, "endOffset": 39}, {"referenceID": 7, "context": "Different from the work proposed in [2], we will adopt the newly developed LRR on manifolds framework to the model of curves LRR, see [8, 25, 29].", "startOffset": 134, "endOffset": 145}, {"referenceID": 24, "context": "Different from the work proposed in [2], we will adopt the newly developed LRR on manifolds framework to the model of curves LRR, see [8, 25, 29].", "startOffset": 134, "endOffset": 145}, {"referenceID": 28, "context": "Different from the work proposed in [2], we will adopt the newly developed LRR on manifolds framework to the model of curves LRR, see [8, 25, 29].", "startOffset": 134, "endOffset": 145}, {"referenceID": 0, "context": "Let \u0393 be the set of all diffeomorphisms from D = [0, 1] to D = [0, 1].", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "Let \u0393 be the set of all diffeomorphisms from D = [0, 1] to D = [0, 1].", "startOffset": 63, "endOffset": 69}, {"referenceID": 0, "context": "The manifold C has some nice properties, see [1].", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": "For any two points q0 and q1 in C, a geodesic connecting them is given by \u03b1 : [0, 1]\u2192 C,", "startOffset": 78, "endOffset": 84}, {"referenceID": 0, "context": "Given any two points [q0] and [q1] in S, a tangent representative [1] in the tangent space T[q0](S) can be calculated in the following way, as suggested in [31, 22] based on (4),", "startOffset": 66, "endOffset": 69}, {"referenceID": 30, "context": "Given any two points [q0] and [q1] in S, a tangent representative [1] in the tangent space T[q0](S) can be calculated in the following way, as suggested in [31, 22] based on (4),", "startOffset": 156, "endOffset": 164}, {"referenceID": 21, "context": "Given any two points [q0] and [q1] in S, a tangent representative [1] in the tangent space T[q0](S) can be calculated in the following way, as suggested in [31, 22] based on (4),", "startOffset": 156, "endOffset": 164}, {"referenceID": 30, "context": "where q\u03031 is the representative of [q1] given by the welldefined algorithm in [31, 22] and \u03b8\u0303 = cos(\u3008q0, q\u03031\u3009).", "startOffset": 78, "endOffset": 86}, {"referenceID": 21, "context": "where q\u03031 is the representative of [q1] given by the welldefined algorithm in [31, 22] and \u03b8\u0303 = cos(\u3008q0, q\u03031\u3009).", "startOffset": 78, "endOffset": 86}, {"referenceID": 9, "context": "To solve the cLRR objective we use the Linearized Alternative Direction Method with Adaptive Penalty (LADMAP) [10, 11] .", "startOffset": 110, "endOffset": 118}, {"referenceID": 10, "context": "To solve the cLRR objective we use the Linearized Alternative Direction Method with Adaptive Penalty (LADMAP) [10, 11] .", "startOffset": 110, "endOffset": 118}, {"referenceID": 3, "context": "Problem (13) admits a closed form solution by using SVD thresholding operator [4], given by", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "where UW\u03a3WV T W is the SVD of W (k) \u2212 1 \u03b7W \u03b2k \u2202F (W ) and S\u03c4 (\u00b7) is the Singular Value Thresholding (SVT) [4, 14] operator defined by", "startOffset": 106, "endOffset": 113}, {"referenceID": 13, "context": "where UW\u03a3WV T W is the SVD of W (k) \u2212 1 \u03b7W \u03b2k \u2202F (W ) and S\u03c4 (\u00b7) is the Singular Value Thresholding (SVT) [4, 14] operator defined by", "startOffset": 106, "endOffset": 113}, {"referenceID": 18, "context": "Once the coefficient matrix W is found, a spectral clustering like nCUT [19] is applied on the affinity matrix |W|+|W| T 2 to obtain the segmentation of the data.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "In each iteration of the Algorithm, the singular value thresholding is adopted to update the low rank matrix W whose complexity is O(rN) [12].", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "Algorithm 1 is adopted from the algorithm proposed in [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "However due to the terms of B\u2019s in the objective function (11), the convergence theorem proved in [11] cannot be directly applied to this case as the linearization is implemented on both the augmented Lagrangian terms and the term involving B\u2019s.", "startOffset": 98, "endOffset": 102}, {"referenceID": 29, "context": "Fortunately we can employ the revised approach, presented in [30], to prove the convergence for the algorithm.", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "The performance of the proposed method is compared with the same type of subspace clustering algorithm LRR [12].", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "To compare segmentation accuracy we use the subspace clustering accuracy (SCA) metric [6], which is defined as", "startOffset": 86, "endOffset": 89}, {"referenceID": 26, "context": "The dataset consists of pen position data collected by a digitisation tablet at 200Hz, which is then converted to horizontal and vertical velocities [27, 26].", "startOffset": 149, "endOffset": 157}, {"referenceID": 25, "context": "The dataset consists of pen position data collected by a digitisation tablet at 200Hz, which is then converted to horizontal and vertical velocities [27, 26].", "startOffset": 149, "endOffset": 157}], "year": 2016, "abstractText": "In machine learning it is common to interpret each data point as a vector in Euclidean space. However the data may actually be functional i.e. each data point is a function of some variable such as time and the function is discretely sampled. The naive treatment of functional data as traditional multivariate data can lead to poor performance since the algorithms are ignoring the correlation in the curvature of each function. In this paper we propose a method to analyse subspace structure of the functional data by using the state of the art Low-Rank Representation (LRR). Experimental evaluation on synthetic and real data reveals that this method massively outperforms conventional LRR in tasks concerning functional data.", "creator": "LaTeX with hyperref package"}}}