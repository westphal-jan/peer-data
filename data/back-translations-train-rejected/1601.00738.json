{"id": "1601.00738", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "Resource Sharing for Multi-Tenant NoSQL Data Store in Cloud", "abstract": "Multi-tenancy hosting of users in cloud NoSQL data stores is favored by cloud providers because it enables resource sharing at low operating cost. Multi-tenancy takes several forms depending on whether the back-end file system is a local file system (LFS) or a parallel file system (PFS), and on whether tenants are independent or share data across tenants. In this thesis I focus on and propose solutions to two cases: independent data-local file system, and shared data-parallel file system.", "histories": [["v1", "Tue, 5 Jan 2016 05:15:12 GMT  (1582kb)", "http://arxiv.org/abs/1601.00738v1", "PhD dissertation, December 2015"]], "COMMENTS": "PhD dissertation, December 2015", "reviews": [], "SUBJECTS": "cs.DC cs.AI cs.DB cs.SY", "authors": ["jiaan zeng"], "accepted": false, "id": "1601.00738"}, "pdf": {"name": "1601.00738.pdf", "metadata": {"source": "CRF", "title": "RESOURCE SHARING FOR MULTI-TENANT NOSQL DATA STORE IN CLOUD", "authors": ["Jiaan Zeng", "Guangchen Ruan", "Zong Peng", "Milinda Pathirage", "Samitha Harshani Liyan"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 1.00 738v 1 [cs.D C] 5J an2 016RESOURCE SHARING FOR MULTI-TENANT NOSQL DATASTORE IN CLOUDJiaan ZengSubmitted to the Faculty of University Graduate Schoolin Partial Fulfillment of the Requirements for the Doctor of Philosophy at the Faculty of Computer Science and Informatics, Indiana University December 2015Accepted by the Graduate Faculty, Indiana University, Partial Fulfillment of the Requirements for the Doctorate Degree of Philosophy. Doctoral ObligationsBeth Plale, Ph.D. (Chairman) Martin Swany, Ph.D.Judy Qiu, Ph.D.David Crandall, Ph.D.Atul Prakash, Ph.D.4th December 2015iiiCopyright c \u00a9 2015Jiaan Zengiii I dedicate this dissertation to my family."}, {"heading": "Acknowledgements", "text": "It's about the question of whether it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way it's about a way and it's about a way it's about a way and it's about a way and it's about a way it's about a way and it's about a way and it's about a way it's about a way it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way it's about a way and it's about a way and it's about a way and it's about a way and it's about a way and it's about a way it's about a way and it's about a way and it's about a way and it's about a way and it's about a way it's about a way and it's about a way and it's"}, {"heading": "1 Introduction 1", "text": "1.1 Emerging Characteristics for Big Data Storage......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "2 Background 12", "text": "2.1 Multi-tenant model......................................................................................................................"}, {"heading": "3 Related Work 24", "text": "3.1 Storage service sharing.............................................. 253.1.1 Storage services.................................. 25 3.1.2 Resource planning................................................. 283.2 Data release.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 Multi-Tenant Fair Share in NoSQL Stores 34", "text": "4.1 Fairness experiments in Cassandra......................................................................................................................................................................................................................................"}, {"heading": "5 Workload-Aware Resource Reservation for Multi-Tenant NoSQL Stores 58", "text": "..........................................................................................................................................................."}, {"heading": "6 A Lightweight Key-Value Store for Distributed Access 97", "text": "......................................................................................................................................................"}, {"heading": "7 Conclusions and Future Work 129", "text": "- The superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordin-, superordinate, superordinate, superordinate, superordinate, superordinate, -, superordinate, superordinate, superordinate, superordinate, superordin-, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordin-, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, superordinate, super-, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, super-, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, supervisor, super-, supervisor, supervisor, supervisor, super"}, {"heading": "1.1 Emerging Characteristics for Big Data Storage", "text": "In its informal definition, big data is not just large, diverse and structured or unstructured data; the sheer volume of data is expected to grow exponentially by a factor of 300 from 130 exabytes in 2005 to 40,000 exabytes in 2020. Furthermore, data coming from different sources with different formats and schemes is increasingly dynamically generated, all of which suggests that distributed storage and flexible data models are the future. Experience has shown that traditional systems such as relative databases (RDBMS) struggle to manage large amounts of data because they are difficult and expensive."}, {"heading": "1.2 Multi-tenant NoSQL Data Store", "text": "A NoSQL data storage in a cloud environment can be viewed as a two-layer architecture: logical viewing layer and storage layer. The logical viewing layer provides tenants with a view of the storage and a set of APIs to interact with the storage, and the storage layer represents the underlying infrastructure that physically stores and delivers the data."}, {"heading": "1.2.1 Logical View Layer", "text": "The logical level of observation determines how tenants view the data and hides the complexity of the underlying infrastructure in front of them. Each tenant has their own view of the store with unshared data sets. From a tenant's perspective, their workload is driven by dedicated resources and unaware of the existence of other tenants. In fact, the cloud provider consolidates each tenant's data into as small an infrastructure as possible to maximize resource usage. Tenants \"data and requests are ultimately processed jointly in the underlying infrastructure. Performance failures become a problem because current solutions are flawed in isolation. On the other hand, tenants can share the same data sets in-store. Tenants can be represented by various components in a pipeline that process the data, such as the HathiTrust Research Center (HTRC) [49], which provides analytical access to nearly 14 million digitized data sets from HathiTrust's digital library."}, {"heading": "1.2.2 Storage Layer", "text": "Figure 1.2a shows a NoSQL data storage used across multiple nodes. Data is stored on each node in the local file system. Since a local file system on a single node can suffer from disk failures and is not scalable, NoSQL storage usually replicates data in a few nodes in the cluster. A persistent daemon service runs on each node in the cluster and communicates with other daemon services to4provide a unified view of local file systems. Different protocols exist between daemons to coordinate the behavior of daemons to support tasks such as data replication, failover, and so on. While the local file system has prevailed on the cloud platform for quite a while, the parallel file system FNOS (PFS) is in the cloud both in industry [68] and science [79]."}, {"heading": "1.2.3 Layer Mapping", "text": "One of the two logical views in Figure 1.1 can be mapped to one of the physical implementations in Figure 1.2. Normally, the NoSQL storage relies on the local file system to store data. Therefore, this dissertation examines performance disruption in the case of shared rooms using a local file system (non-shared data LFS). Ignorance of NoSQL storage about the features offered by PFS makes it inefficient to operate a NoSQL storage over PFS today. This dissertation examines the profitability of using PFS to support data exchange between tenants in the NoSQL data storage (shared data PFS)."}, {"heading": "1.3 Research Problems", "text": "NoSQL data storage hosted in the cloud has been widely used due to its scalability and high availability. However, as cloud environments encourage resource sharing, data sharing and reuse between tenants is a growing use case. However, multi-person households in such a shared environment present significant performance challenges for the use of NoSQL storage. Below are the issues addressed in this dissertation. \u2022 Preventing performance disruptions between tenants in the case of non-shared data LFS \u2022 Cost-effective multi-person access in the case of shared data PFS 6"}, {"heading": "1.3.1 Performance Interference Prevention", "text": "This dissertation focuses on providing a non-client-centric solution to enforce a fair share on the server side. However, a behavioural tenant may consume the resources of a well-behaved tenant through his workload, thereby worsening the tenant's performance. This multi-tendency interference behaviour is not desirable. The resource reservation approach in Chapter 5 can potentially be used to enforce SLA for each tenant. We leave enforcement of the SLA for future work. Real-world scenarios provide evidence of performance limitations. Although Amazon DynamoDB introduces a tenant throughput limit to prevent the store from being dominated by a few tenants, they do not guarantee throughput loads."}, {"heading": "1.3.2 Cost Effective Multi-tenant Access", "text": "Operating a distributed NoSQL store over PFS could entail overheads that transmit to the PFS the ignorance of the former over the PFS. For example, data may be unnecessarily replicated; additional network circuits may be required to access the PFS, since the daemon service delegates all requests to the backend file system; and additional overhead may also arise from data replication and failover protocols that are unnecessary in the presence of PFS. Furthermore, the store vendor can maintain resources (e.g. VM) even when no request comes, which is not cost-effective, because when the VMs are shut down or redirected, the data stored in the VMs is no longer accessible. Lately, Greenberg et al. point to the burden and inefficiency of operating persistent daemon services for key value storage in the HPC environment [40]."}, {"heading": "1.4 Contributions", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "1.5 Dissertation Outline", "text": "The rest of this dissertation is as follows: Chapter 2 presents the background information on the multi-tenancy model and the NoSQL data storage; Chapter 3 summarizes the related work; Chapter 4 examines performance interference and proposes a flow regulation mechanism aimed at system-wide fairness; Chapter 5 examines performance interference based on the results of Chapter 4 and describes a workload-aware10resource reservation mechanism for performance isolation; Chapter 6 focuses on building a lightweight key-value store called KVLight, using a parallel file system with novel data structures; Chapter 7 concludes with future work. Chapter 2BackgroundSupporting multi-tenancy is important in a storage service such as a NoSQL data storage. First, we present the background of multi-tenancy storage including its definition and various models. A cloud-hosted NoSL data storage typically runs as a mid-file application."}, {"heading": "2.1 Multi-tenancy Model", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2.2 Overview of NoSQL Data Store", "text": "To cope with the rapid growth of data, NoSQL data stores are designed as modern web-scaled databases that take into account the characteristics of a schema-free data model, simple replication support, distributed access, simple APIs, and ultimate consistency. [74] According to the CAP theorem [39] conflicts arise between different aspects of a distributed system under terms 14 of three factors: consistency, availability, and partition tolerance. The relationship between the factors is illustrated in Figure 2.2.2. The CAP theorem postulates that only two of the three factors can be achieved simultaneously. In the traditional SQL database, which emphasizes ACID properties (atomics, consistency, isolation, and permanence), partition tolerance or availability is usually sacrificed to respect consistency."}, {"heading": "2.2.1 Architecture", "text": "In general, a NoSQL storage can fall into one of two categories: single-node-oriented and multi-node-oriented data storage. Single-node NoSQL storage is designed to work in a single node environment and is usually embedded in the application as a peer-peer architecture. Unlike multi-node NoSQL storage, which has persistent daemon services, delegate access to the file system through network, single-node NoSQL storage allows applications to directly access the file system and read or write data without going through a network. Furthermore, no persistent daemon service is required. Therefore, it is easier and performs better than multi-node NoSQL storage. Examples include Berkeley DB (BDB) and Level DB [62]. Figure 2.3 shows the architecture embedded in BDB as an application."}, {"heading": "2.2.2 Data Distribution", "text": "In fact, it is the case that it will be able to take the lead."}, {"heading": "2.2.3 Data Replication and Consistency", "text": "Distributed NoSQL data storage typically replicates data to provide highly reliable and simultaneous data access. Different replication policies can be applied and result in different consistency levels. Note that Berkeley DB is not concerned with data replication and consistency because it stores data locally only. 21HBase's replication policies and consistency are by default adopted by the Hadoop Distributed File System (HDFS), which is the underlying storage for HBase. HDFS replicates a block, its basic data unit, to two more nodes in the cluster. If one of the three nodes is down, the other replication levels can serve the other replication levels. HDFS maintains strong consistency for data access. A letter to HDFS only returns when all replicates are successfully written, so each reader can get the latest value after writing."}, {"heading": "2.2.4 Resource Management", "text": "For example, CPU for serialization and de-serialization, memory for caching and buffering, hard disk for 22 reading and writing, network for transmission, and so on. Configuration about how resources are allocated plays an important role in performance in some cases. Individual NoSQL nodes function as an embedded library and manage memory and disk usage all by themselves. Berkeley DB manages the memory for read-caching and writes buffering. It also controls the way to generate the data files on the hard disk, such as how large a file is when it flushes a file, and so on. Compared with the monotonous resource management of a single NoSQL node, NoSQL memory splits management into layers. In HBase, HDFS stores data and manages the disk resource, while HBase daemon service controls the CPU."}, {"heading": "3.1 Storage Service Sharing", "text": "The Common Process Model is generally preferred to provide tenants with non-shared data [53], as it provides reasonable isolation without imposing excessive overheads as described in Section 2.1. Therefore, much of the work [24,72,84,95,104,106] on shared storage services focuses on the Common Process Model. This section first summarizes support for multiple tenants in different storage services classified as file systems, relational databases, and NoSQL storage, and then examines the literature on resource planning approaches used for performance isolation."}, {"heading": "3.1.1 Storage Services", "text": "This year is the highest in the history of the country."}, {"heading": "3.1.2 Resource Scheduling", "text": "This year, the time has come for us to find a solution that is capable, that we are able, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution. \""}, {"heading": "3.2 Data Sharing", "text": "This dissertation focuses on the Key Value Store (KVS), a special form of NoSQL data storage, in the case of shared data. KVLight, the proposed system, uses the shared table model for multi-tenant relationships, which is the most efficient way to share data among tenants in the same format as described above."}, {"heading": "3.2.1 File System and Key-Value Store", "text": "We focus on building a memory layer over a parallel file system in order to make better use of its properties. Yin et al. compare the performance between a parallel file system and a KVS in terms of throughput [111]. Ren et al. propose a file system that uses an embedded KVS to manage file system metadata [85]. KVLight is a KVS that builds on a PFS and realizes its data reliability and high scalability features."}, {"heading": "3.2.2 Key-Value Store As A Library", "text": "Individual nodes KVS (S-KVS) are usually embedded as a library in the application for high-performance access in a single node environment. Berkeley DB uses B-tree or hash to organize the key-value pairs [75], while LevelDB stores data in files on a logical tree level [62]. RocksDB [87] extends LevelDB in terms of utilizing multiple cores, multi-thread compression, and so on. Systems such as NVMKV [69], FlushStore [26] focus on building S-KVS over flash memory with appropriate data structures. KVLight extends the usability of S-KVS for simultaneous write operations in a distributed environment where traditional S-KVS is not supported. 31A distributed KVS over multiple nodes (M-KVS) is commonly used in the real world as flash memory with suitable data structures."}, {"heading": "3.2.3 Compaction Management", "text": "In fact, most of them are able to go to another world, to go to another world, to go to another world, to go to another world, to find themselves in another world, to find themselves in another world, to not find themselves, to not find themselves, to not find themselves, to not find themselves, to not find themselves, to not find themselves, to not find themselves, to not find themselves, to not find themselves, to not find themselves, to not find themselves."}, {"heading": "4.1 Fairness Experiments in Cassandra", "text": "In fact, most of them are able to go to another world, to go to another world, to go to another world, to go to another world."}, {"heading": "4.2 Request Scheduling", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "4.3 Adaptive Control Mechanisms", "text": "The adaptive control mechanisms include the approach of local weight adjustment to provide a system-wide fair share, and the division of the scanning process to avoid blocking read operations. Local weight adjustment The planning approach outlined above focuses on providing a fair share in a single node. We refer to this fairness as local fairness and system-wide fairness as global fairness. It is easy to show local fairness that leads to global fairness. In a43Algorithm 1 Request Scheduling Algorithm1: crediti are the credits in the credit account of the tenant i. 2: esti is the estimation of a resource demand for tenants i. 3: Procedure SCHEDULE 4: for each tenant I 5: esti \u2190 RequestcreditRcredicEstimation (tenant i) 6: while mi \u00d7 crediti \u2265 esti do 7: Take a request if tenant i's queue is not true, crediti, crediti: 9, crediti, crediti, creditor: 11, if it is true."}, {"heading": "4.3 suggests that 1) global fairness can be achieved without local fairness; 2) number of", "text": "The idea is to recalculate the weight that a tenant should have on a particular node, based on the ratio of his number of threads on that node to his total number of threads on all nodes (line 10 to 15).In this step, the maximum credits of a node would be changed and differ from each other, although the total credits of a tenant within the system are not changed. Finally, the algorithm spreads the new maximum credit number for each node (line 21 to 24).45Algorithm 2 Local Adjustment Algorithm1: total is the total number of credits for the entire system."}, {"heading": "4.4 Evaluation", "text": "We use computing resources from FutureGrid [36] in evaluating our system. Each node has 2 Intel (R) Xeon (R) 2.93 GHz CPUs, 25 GB of memory and an 800 GB local hard disk. Nodes are connected to InfiniBand. Using a 9-node cluster, we install a modified version of Cassandra 1.2.4 equipped with the fairness control dispatcher in each node that maintains the default Cassandra settings. On the client side, we use YCSB [20] to generate the workloads and use 5 additional nodes to simulate the clients accessing Cassandra. Each tenant stays in a separate node, so that interferences on the client side avoid having their own keyspace with a column family. We load 1,000,000 lines into the keyspace of each tenant. The row size is 1.2 KB and the replication factor is read to 3."}, {"heading": "4.4.1 Overall Performance", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "4.4.2 Effectiveness of Adaptive Control Mechanisms", "text": "Local weight adjustment We compare the read-only workload throughputs and read-write workload with local weight adjustment to those without. Two thread distributions are tested. The first is a random distribution in which each tenant randomly selects a server node to connect to, while the second is a Gaussian distribution in which each tenant selects a server node based on a predefined Gaussian distribution.54 Figure 4.6 presents the normalized throughputs. If the local weight adjustment is present, the red and blue bars have similar heights on fixed size data and variable size data. Therefore, the local weight adjustment can handle different thread distributions. If there is no local weight adjustment, the gray bar is higher than the green bar, meaning that the random distribution receives more throughputs than the tendencies in our system."}, {"heading": "4.5 Summary", "text": "This year is the highest in the history of the country."}, {"heading": "5.1 Analysis of Interference", "text": "NoSQL data storage is typically used across multiple nodes to improve availability and performance. Data is represented in rows and distributed across nodes. We motivate our approach by showing that NoSQL data storage can suffer performance disruptions when multiple tenants access it at the same time, and that even a single resource reservation can in some cases prevent disruption."}, {"heading": "5.1.1 Setup", "text": "In this chapter we will examine HBase [1], a popular NoSQL storage. HBase is an open source implementation of Google BigTable [16]. HBase abstracts the data partition and distribution to a distributed file system, i.e. HDFS [96], and runs beyond that. HBase follows the master-slave design: The HMaster on the master node is responsible for coordinating and monitoring slave node activities; the HRegionServers on the slave node process client requests directly; the HRegionServer exchanges data with HDFS in the unit of a block and implements a block cache equipped with an LRU replacement algorithm to avoid HDFS access. Thus, HBase can be regarded as a two-tiered hierarchy storage system and provides a clean separation between different resource management at different levels."}, {"heading": "5.1.2 Interference Experiments", "text": "This year it is more than ever before."}, {"heading": "5.2 Resource Reservation", "text": "Argus is based on the master-slave architecture of HBase (see Figure 5.3 for details).The master collects resource information from different slave nodes and makes smart resource reservation decisions. Client support improves cache access in the RegionServer. Inside a RegionServer, the Workload Monitor module collects the performance indicators of the workloads, and the Resource Reservation module describes the current resource reservation policy. Disk reservation is represented by the HDFS throughput reservation roughly as discussed in 5.1. The Requirements Planner is used to enforce the workload indicators, and the Resource Reservation module describes the current resource reservation."}, {"heading": "5.2.1 Block Cache Reservation", "text": "Unlike ACache [84], which replaces the default HBase cache replacement, we apply the built-in LRU cache replacement in HBase to each tenant's cache partition because the cache replacement in HBase has been improved to prioritize eviction based on the times the 68 blocks are reused. Although a strict cache reservation can provide strong isolation, it can lead to poor cache utilization if some tenants do not exhaust their reservations. We categorize such situations into two categories: Some tenants change their access behavior, for example, they switch from hotspot access to random access; some tenants slow down their request rates. We discuss the details and present the solutions in Section 5.2.3."}, {"heading": "5.2.2 Disk Reservation", "text": "For the reasons given in Section 5.1, we use HDFS throughput to approximate disk usage. We design the Request Scheduler in the RegionServer rather than the Hadoop Distributed File System (HDFS) because many of the file systems, including HDFS, are not designed for multi-mandate usage: the enforcement of multiple mandates is done through the application that builds on the file system. As explained in Chapter 3, there are two types of scheduling approaches similar to the general Processor Sharing (GPS) model [80] to ensure fair allocation: One is virtual time-based approximation and the other is quantum-based approximation. To understand what approach can work well in the context of multi-mandate usage in HBase, we examine weighted fair queues (WFQ) [80], which are a virtual scheduler, and deficit round-robin (DRR) [93], which is a quantum-based scheduler."}, {"heading": "A. Approaches for Request Scheduling", "text": "In order to reduce the cost of calculating the request time, the term virtual time is used to process requests. Each request is marked with a virtual start time and a virtual processing time. Equations 5.1 and 5.2 show how they are calculated [34.108], where Sni and F n i are the virtual start time and the virtual processing time for the tenant's nth request i or v (t) is the virtual time for the real-time request, Lni is the size of the nth request and ri the share of the tenant i.Sni = max (v (t), F n \u2212 1 i) (5.1) F ni = S n i + L n i \u00d7 ri (5.2) the virtual start time is the maximum virtual start time and the virtual processing time of the last request. Virtual processing time is based on an estimate of how long the request will take. Estimation is based on a linear relationship between the virtual request and the processing time (R)."}, {"heading": "B. Comparison", "text": "The goal of the evaluation is to evaluate the fairness and efficiency of different planning approaches in the context of throughput reservation. We have implemented WFQ and DRR as requirements planning approaches in HBase. DRR requires some adjustments and enhancements in the context of HBase. First, we have the planner, who interprets the credits as bytes read by HDFS or written to HDFS. He assumes that there is a linear function that translates the credits to the underlying resource usage, mainly to hard disk access, in HDFS. Second, we do not know how much data is read by HDFS. The planner simply uses an average window that reads the bytes needed for future requests. We also use this as a prediction in WFQ. Krebs et al. discuss other advanced prediction options [59].The evaluation environment is the same as the one in Section 5.1."}, {"heading": "5.2.3 Elastic Reservation", "text": "Neither the cache reservation nor the hard drive reservation would be efficient if some tenants did not use up their resources, since both reservations are statically applied without any elasticity. Static resource consumption leads to inefficiency, as some of them are idle and cannot be used by other tenants. There are two cases in which a tenant does not use his reservation."}, {"heading": "5.3 Reservation Planning", "text": "The elastic reservation approach mentioned in Section 5.2.3 adjusts the reservation monotonously and is not suitable if tenants have different resource requirements. Since resource usage is not independent, e.g. increasing cache allocation can reduce disk usage and vice versa, a reservation of this type requires a model that reflects the dependence between different resources. In this section, we discuss reservation planning, which is used to decide how many resources should be dynamically reserved for each tenant according to their needs."}, {"heading": "5.3.1 Problem Formalization", "text": "We demonstrate experimentally that the same reservation could result in a different throughput if used for different workloads. We perform the three workloads defined in Section 5.1. We vary the percentage of cache reservation and HDFS throughput reservation in a range from 0.125 to 1.0 at an interval of 0.125. For example, 0.5 of cache and 0.25 of HDFS throughput means that we reserve 50% of the total cache space and 25% of the total credits from HDFS. Each experiment performs only one workload in the cluster. Figure 5.4 plots the results. Figure 5.4a shows the uniform workload as disk-sensitive. Increasing HDFS throughput significantly increases its throughput, while increasing cache reservation does not work. Figure 5.4b, the regular hotspot workload requires both disk and cache."}, {"heading": "5.3.2 Solution", "text": "Solving the problem requires knowledge of the various capabilities. Instead of inferring a complicated form of searching that is difficult and error-prone, we simply use regression to interpolate the function to a certain amount of data collected as part of the interpolation; the profiler performs the interpolation and generates the performance."}, {"heading": "5.3.3 Limitations", "text": "To simplify the scenario, the current prototype assumes the following: First, we do not consider the location of the data and assume that each byte read from HDFS consumes the same amount of resources; second, reading from the local disk is faster and consumes fewer resources than from remote nodes; second, we do not assume the interference between read and write and assume that both read and write consume the same amount of resources; third, we assume that both the data and the request are evenly distributed; and last but not least, we assume that the access pattern will not change in a short time, which will allow Argus sufficient time to detect and respond. 80"}, {"heading": "5.4 Evaluation", "text": "Each node has a 2.0GHz dual-core CPU with 4GB of memory and a 40GB hard drive. Three nodes are configured as a zookeeper group, one node is configured with both HDFS master and HBase master, and the other 24 nodes are configured as HRegion server and HDFS data nodes. Blockcache size is configured to 1200 MB, and the number of RPC handler threads in HBase to 30. HDFS replication factor is set to 1 to save space. We use the YCSB benchmark [20] to fill the data and generate the workload. Once all data is pre-installed, we perform large compressions to compress the memory files. We have HBase to balance the number of regions across the nodes. YCSB clients are executed in additional nodes to simulate multiple tenants accessing the system at the same time."}, {"heading": "5.4.1 Micro Evaluation", "text": "In this section, we present micro-benchmark results of resource enforcement, as it is the promotion of resource reservation. Specifically, we perform an in-depth analysis of plate reservation to examine its impact on fairness and efficiency, and then we examine the stability of resource enforcement in complex scenarios. In addition, we evaluate the effectiveness of elasticity reservation, and finally, we evaluate the overhead caused by enforcement approaches."}, {"heading": "A. Disk Reservation", "text": "In fact, the fact is that most of them will be able to be in a position without being able to play by the rules, and that they will be able to play by the rules."}, {"heading": "B. Stability of Reservation Enforcement", "text": "Figure 5.4 in Section 5.3 already shows that resource reservation is able to reserve mixed throughput given HDFS throughput and cache size. To take this one step further, we evaluate its stability in more complex scenarios. We have 3 groups of workloads: uniform, extreme hotspots and mixed. There are 1, 2, 5 and 8 tenants who execute the workloads in uniform and extreme hotspot groups, while some tenants execute uniform workloads and other extreme hotspot workloads in the mixed group. In the cases of 1 and 2 tenants, each tenant uses 50 threads. In the cases of 5 and 8 tenants, the thread numbers are 50, 50, 100, 100, 200 and 300 and 50, 50, 100, 100, 200, 200, 300 and 300. Reservation planning is turned off in this assessment to avoid redistribution of workloads."}, {"heading": "C. Elastic Reservation", "text": "The elastic reservation approach is used to block cache and disk reservation, to dynamically adjust the reservation when some tenants reduce their throughput. Reservation is used to plan the reservation when the tenant's workload has different resource requirements. We evaluate the elastic reservation by having two tenants perform the uniform workload and one of them pushes its throughput rate. Both use 50 threads. Figure 5.7a shows the throughput as a function of time. Both tenants share the system fairly in the first 200 seconds. Between the 200th second and 400th tenants, the throughput decreases to 200."}, {"heading": "D. Overhead", "text": "Figure 5.5a shows that if the number of credits per node is 70 million, which imposes no restrictions on requirements scheduling, about 2% overhead for throughput and about 1% for latency are observed. We attribute this to the implementation of DRR and queues, and the other source of overhead is the number of credits used in the DRR algorithm. On 50 million credits in Figure 5.5a, we observe about 3% overhead for throughput and 2% for latency. To investigate the overhead costs of cache reservation, we disable disk reservation and have two tenants run the uniform, hotspot and extreme hotspot workload each with the same number of shifts."}, {"heading": "5.4.2 Macro Evaluation", "text": "This year it is more than ever before."}, {"heading": "B. Resource Reservation Planning", "text": "Argus relies on reservation planning to decide how much resource should be reserved for each tenant. We study the effectiveness of planning by re-running the interference experiments in 5.4.2, with planning off (i.e. even reserving for HDFS throughput 91 and cache occupancy), comparing the throughput to the corresponding throughput during which the planning is running. Figure 5.9 shows the normalized results. Overall, the throughput in the unified workload experiment is similar to that in the planning, but falls behind in the other two experiments. We think it is because both tenants perform uniform workloads and the planning has no need to redistribute cache and HDFS throughput. So tenants see a similar throughput with the cancelled planning. In the Hot + Uniform and ExHot + Uniform experiments, the throughput of such work 92 loads, i.e. Tenants # 2, is not absolutely necessary compared to the one-time DFS reservation experiment."}, {"heading": "C. Comparison with A-Cache", "text": "Finally, we compare Argus with A-Cache [84] in terms of fairness and efficiency. A-Cache is also designed on HBase to prevent performance interference between workloads. It focuses on avoiding cache interference. It uses the cache reuse ratio to represent cache usage for each tenant. Reuse ratio is calculated as the ratio of the number of cache blocks visited at least twice to the total number of cache blocks. It is estimated how much cache space this workload can take up, as a cache block will only be useful if it is visited at least twice. Intuition is a tenant should only have what is needed. Workloads with large reuse ratios earn more cache space, while workloads with low reuse ratios require less. We have implemented the A-Cache approach and evaluated its fairness as well as the improvement with the three workloads used in Table 5.7 showing the results."}, {"heading": "5.5 Real World Applications", "text": "To see how Argus might work for real-world applications, we refer to Big Data Text Mining from the HathiTrust Research Center (HTRC) [49, 116]. HTRC rules for collaborative text mining of the nearly 14 million digital documents (books, series, government documents) in the HathiTrust digital repository [48]. HTRC manages various types of data objects: raw text data, metadata about the books, and derived data, such as term frequency. This is managed by a single key value store. All of these data objects change slowly, making workloads widely readable against all three, with the observation consistent with the capture of realistic workloads in NoSQL stores. Access patterns differ between workloads and could therefore lead to performance interference. Reading raw texts and metadata has demonstrable effects on locality, as parts of the corpus are more interesting than other parts."}, {"heading": "5.6 Summary", "text": "We introduce Argus, a workload-conscious resource reservation framework that prevents interference by forcing the reservation of cache and disk. Furthermore, the resource reservation technique is workload-conscious. Empirical results show that Argus is able to prevent interference between tenants and adapt accordingly to dynamic workloads. Future work can go in several directions. We intend to quantify the impact of write operations on read operations and model the I / O behavior through offline sampling. We want to investigate another resource reservation, namely memory usage for write operations. Increasing the write buffer will increase write performance but impair read performance as the size of the block cache decreases. It is advantageous to specify the size of the cache and write the write buffer according to block 95different workload characteristics. In addition, we want to expand Argus \"Nocda SL-size to other solutions."}, {"heading": "A Lightweight Key-Value Store for Distributed Access", "text": "This year, it has come to the point where there is only one occasion when there is a scandal, and that is when there is a scandal."}, {"heading": "6.1 Background and Motivation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1.1 Background", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "6.1.2 KVS on Parallel File System", "text": "We are running the experiments on Lustre 2.1.6 in Data Capacitor 2 at Indiana University [25]. We are using Berkeley DB Java Edition (BDB) version 6.2.3 [8] as S-KVS and Cassandra version 2.0.14 [14] as M-KVS. Yahoo Cloud Storage Benchmark (YCSB) [20] is used to generate the workloads. For more details about the setup, refer to Section 6.5. We are using only read-only and read-only workloads. There are two setup variants for Cassandra: single node Cassandra instance (S-Cassandra) and 15 node Cassandra cluster (M-Cassandra). We are using one client and six clients to access the BDB and Cassandra instances. A client is running on a separate node Cassandra instance (S-Cassandra) to perform the workloads."}, {"heading": "6.2 The KVLight Structure", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1 System Model", "text": "In a large computing environment, such as HPC and cloud, the system can typically be organized in a two-layer architecture, consisting of application and memory layer. The application layer generates queries to the memory layer and processes the query results as the memory layer stores the data and processes queries. We design KVLight as middleware that remains between application and memory layer in Figure 6.3. The KVLight library provides basic key-value-storage APIs, including Get (key), Put (key, value) and Delete (key). KVLight storage is a list of files in Lustre. The files contain metadata, such as KVLight103 status and data, i.e. the key-value pairs. Lustre is used as the underlying storage system and communication medium between nodes."}, {"heading": "6.2.2 Design Choices", "text": "The heavy mechanisms such as data replication, failover, node coordination, etc., are shifted from KVS itself to the underlying file system, i.e. Lustre. We examine the design space in Figure 6.4. An S-KVS has low concurrent write performance because it only supports exclusive writes. A simple solution is to have multiple processes written to independent SKVSs and have all existing S-KVSs read. However, the read performance will deteriorate as the number of S-KVS grows, as reading needs to consult more BDBs to get the data. To address the read deterioration, compression can be used to merge several S-KVSs into one to reduce the number of concurrent writes in the system. So, to support concurrent writes without sacrificing too much read performance, we remotely design KVS + Light VVS in the KS-104 category. \""}, {"heading": "6.3 Design Details", "text": "KVLight uses Berkeley DB (BDB), a widely used S-KVS, to store key-value pairs. In this section, we introduce the log structure merge tree design (LSM) that allows KVLight to support concurrent writing, and the asynchronous mechanisms that hide the overheads introduced by LSM. We then describe two different compaction approaches, i.e. size-based compaction and tree-based compaction, and their parallel implementations. Finally, we present the consistency model used in KVLight."}, {"heading": "6.3.1 Concurrent Write", "text": "In fact, the majority of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "6.3.2 Compaction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "6.3.3 Consistency", "text": "Although Lustre, where KVLight is built, provides strong consistency for concurrent access, KVLight introduces data inconsistency because it needs to support concurrent writing. There are two scenarios that are related to consistency: consistency within a process and consistency between processes. To ensure consistency within a process, KVLight follows consistency between read and write according to [101]. The effect of a write operation can always be detected by following readings in the same process. This is because a write operation remains either in the write BDB or in one of the fixed BDB. Successive readings in the same application can read the value of the write operation immediately either from the write BDB or from invariant BDBs. To ensure consistency between processes, a process may not read the latest value written by another process in the process in progress, but it eventually becomes the key for allowing the pair to be visible."}, {"heading": "6.3.4 Limitations", "text": "To simplify the situation, the current KVLight prototype is limited to the simple search for key values. It does not support transactions and does not support atomic operation. Both require a central Mech-115 method that can be coordinated between applications, and it does not support advanced operations such as scan and join. The scanning process requires a global sequence of keys across different BDBs, while the join requires additional indexes. Furthermore, it assumes that the back-end PFS can accommodate read and write operations both from applications and from internal procedures, such as compression, without causing controversy."}, {"heading": "6.3.5 Applications", "text": "The above-mentioned consistency model and constraints impose constraints on the applications that can use KVLight. KVLight is not suitable for applications that involve operations across multiple processes. Examples include the strong consistency case, where one process reads from the writes of another process, the transaction case, where a process wants to transfer a series of operations as a transaction, and the atomic operation case, where a counter is maintained between applications. Despite these constraints, KVLight is suitable for a wide range of applications. KVLight can also be used in situations where final consistency can be tolerated. An example is the advertising application. Many users (applications) post advertisements. It is fine that some ads do not immediately reach the reader. KVLight can also be used in situations where write and read are separated. An example is log processing [112]. Log data is used as key value pairs to KVLight Job, which is potentially more than one that is distributed to the injected into the process."}, {"heading": "6.4 Implementation", "text": "We have implemented KVLight in Java 1.7 with Berkeley DB Java Edition 6.2.3. The KVLight library shown in Figure 6.10 consists of a write manager and a read manager. The write manager sends the key-value pairs based on the hash partition to different BDBs. It also implements an asynchronous mechanism to avoid blockages when flushing a write BDB. The read manager manages the tree structure of different BDBs and is responsible for handling reads.The compact manager is a separate process triggered by the library. It starts serial compact packages in parallel to compact BDBs. Only one compact manager may be run at the same time. Both the compact manager and the workers are passed through Ssh on additional nodes. KVLight must maintain the system status. We implement the system status, e.g. paths of the current BDB logic table, to be applied to the Metatry of the current Metadmanager in a second."}, {"heading": "6.5 Evaluation", "text": "We evaluate the performance of KVLight in a cluster environment and compare it to Cassandra version 2.0.14 and Voldemort version 1.9.17, both of which are state-of-the-art. Voldemort is a KVS with Berkeley DB Java Edition as standard backend memory and is used as a 15-node cluster. Experiments are conducted in a distributed environment, each node has 2 Intel Xeon E5-2650 V2 memory and 32 GB of memory. Both Cassandra and Voldemort are set up as 15-node clusters."}, {"heading": "6.5.1 Overall Performance", "text": "This year it is more than ever before."}, {"heading": "6.5.2 Effectiveness of Compaction", "text": "In this section we will examine the effectiveness of various compaction strategies and the acceleration of parallel compaction strategies. The compaction threshold for size-based compaction is set to 48, which is the maximum number of BDBs in tree-based compaction. We use a read-only workload and a written workload in this experiment. The compaction threshold is normalized without any compaction and shown in Figure 6,14. Compaction improves throughput by about 40% and the 65% in read-based workload."}, {"heading": "6.5.3 Real World Applications", "text": "To further evaluate the performance of KVLight, we apply two real applications. The first is a Facebook key value pairing, namely ETC [4]. We generate 25 million key value pairs based on the key size and value distributions specified in [4]. We use the power law distribution with the parameters set to approximate the key access sequence in [4]. There are three workloads on this data set: a write-load (W) that loads the data into the memory; a read-write ratio is 30: 1; a read-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write-write)."}, {"heading": "6.6 Summary", "text": "This chapter describes KVLight, a lightweight key value store in a distributed environment. KVLight uses Berkeley DB for lightweight access and adds a parallel file system for data reliability, failover and simultaneous access. The core design behind KVLight is a novel tree-based organization of data with parallel compression. Empirical results show that KVLight is capable of outperforming Cassandra and Voldemort in most workloads. There are a number of future instructions for this work. First, it is useful to dynamically adjust the tree structure (i.e. height and fan) to handle different data distributions. For key areas with many keys, it is better to divide such areas further to avoid consulting too many BDBs when reading, which leads to an increase in height or fan size."}, {"heading": "7.1 Conclusions", "text": "This year it is more than ever before."}, {"heading": "7.2 Future Directions", "text": "This year it has come to the point that it has never come as far as this year."}], "references": [{"title": "pWalrus: Towards better integration of parallel file systems into cloud storage", "author": ["Yoshihisa Abe", "Garth Gibson"], "venue": "In Workshop on Interfaces and Abstractions for Scientific Data Storage", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Compaction management in distributed key-value data stores", "author": ["Muhammad Yousuf Ahmad", "Bettina Kemme"], "venue": "In Proceedings of the VLDB Endowment,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Workload analysis of a large-scale key-value store", "author": ["Berk Atikoglu", "Yuehai Xu", "Eitan Frachtenberg", "Song Jiang", "Mike Paleczny"], "venue": "In Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "PLFS: A checkpoint filesystem for parallel applications", "author": ["John Bent", "Garth Gibson", "Gary Grider", "Ben McClelland", "Paul Nowoczynski", "James Nunez", "Milo Polte", "Meghan Wingate"], "venue": "In Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis, SC", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Apache Hadoop goes realtime at Facebook", "author": ["Dhruba Borthakur", "Jonathan Gray", "Joydeep Sen Sarma", "Kannan Muthukkaruppan", "Nicolas Spiegelberg", "Hairong Kuang", "Karthik Ranganathan", "Dmytro Molkov", "Aravind Menon", "Samuel Rash", "Rodrigo Schmidt", "Amitanand Aiyer"], "venue": "In Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Pvfs: A parallel file system for linux clusters", "author": ["Philip H. Carns", "III Walter B. Ligon", "Robert B. Ross", "Rajeev Thakur"], "venue": "Proceedings of the 4th Annual Linux Showcase and Conference,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Scalable sql and nosql data stores", "author": ["Rick Cattell"], "venue": "SIGMOD Record,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Bigtable: A distributed storage system for structured data", "author": ["Fay Chang", "Jeffrey Dean", "Sanjay Ghemawat", "Wilson C. Hsieh", "Deborah A. Wallach", "Mike Burrows", "Tushar Chandra", "Andrew Fikes", "Robert E. Gruber"], "venue": "ACM Transactions on Computer Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Comparison of the three cpu schedulers in xen", "author": ["Ludmila Cherkasova", "Diwaker Gupta", "Amin Vahdat"], "venue": "SIGMETRICS Performance Evaluation Review,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Benchmarking cloud serving systems with YCSB", "author": ["Brian F. Cooper", "Adam Silberstein", "Erwin Tam", "Raghu Ramakrishnan", "Russell Sears"], "venue": "In Proceedings of the 1st ACM Symposium on Cloud Computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "MeT: Workload aware elasticity for nosql", "author": ["Francisco Cruz"], "venue": "In Proceedings of the 8th ACM European Conference on Computer Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Relational Cloud: A Database Service for the Cloud", "author": ["Carlo Curino", "Evan Jones", "Raluca Ada Popa", "Nirmesh Malviya", "Eugene Wu", "Samuel Madden", "Hari Balakrishnan", "Nickolai Zeldovich"], "venue": "In 5th Biennial Conference on Innovative Data Systems Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "CPU sharing techniques for performance isolation in multi-tenant relational datab-as-a-service", "author": ["Sudipto Das", "Vivek R. Narasayya", "Feng Li", "Manoj Syamala"], "venue": "In Proceedings of the VLDB Endowment, PVLDB \u201913. Very Large Data Bases Endowment Inc.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "FlashStore: High throughput persistent key-value store", "author": ["Biplob Debnath", "Sudipta Sengupta", "Jin Li"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Dynamo: Amazon\u2019s highly available key-value store", "author": ["Giuseppe DeCandia"], "venue": "In Proceedings of the 21th ACM SIGOPS Symposium on Operating Systems Principles,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Dynamo: Amazon\u2019s highly available key-value store", "author": ["Giuseppe DeCandia", "Deniz Hastorun", "Madan Jampani", "Gunavardhan Kakulapati", "Avinash Lakshman", "Alex Pilchin", "Swaminathan Sivasubramanian", "Peter Vosshall", "Werner Vogels"], "venue": "In Proceedings of 21st ACM SIGOPS Symposium on Operating Systems Principles,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Analysis and simulation of a fair queueing algorithm", "author": ["A. Demers", "S. Keshav", "S. Shenker"], "venue": "SIGCOMM Computer Communication Review,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1989}, {"title": "Hyperdex: A distributed, searchable key-value store", "author": ["Robert Escriva", "Bernard Wong", "Emin G\u00fcn Sirer"], "venue": "In Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "The digital universe in 2020: Big data, bigger digital shadows, and biggest growth in the far east", "author": ["John Gantz", "David Reinsel"], "venue": "http://www.emc.com/collateral/ analyst-reports/idc-the-digital-universe-in-2020.pdf,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Dominant resource fairness: Fair allocation of multiple resource types", "author": ["Ali Ghodsi", "Matei Zaharia", "Benjamin Hindman", "Andy Konwinski", "Scott Shenker", "Ion Stoica"], "venue": "In Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Brewer\u2019s conjecture and the feasibility of consistent, available, partition-tolerant web services", "author": ["Seth Gilbert", "Nancy Lynch"], "venue": "SIGACT News,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2002}, {"title": "MDHIM: a parallel key/value framework for HPC", "author": ["Hugh Greenberg", "John Bent", "Gary Grider"], "venue": "In 7th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 15),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "PARDA: proportional allocation of resources for distributed storage access", "author": ["Ajay Gulati", "Irfan Ahmad", "Carl A. Waldspurger"], "venue": "In Proccedings of the 7th Conference on File and Storage Technologies,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "mClock: Handling throughput variability for hypervisor io scheduling", "author": ["Ajay Gulati", "Arif Merchant", "Peter J. Varman"], "venue": "In Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Cassandra: The Definitive Guide", "author": ["Eben Hewitt"], "venue": "O\u2019Reilly Media,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2010}, {"title": "HBase: The Definitive Guide", "author": ["Eben Hewitt"], "venue": "O\u2019Reilly Media,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2011}, {"title": "Aggregating vulnerability metrics in enterprise networks using attack graphs", "author": ["John Homer", "Su Zhang", "Xinming Ou", "David Schmidt", "Yanhui Du", "S. Raj Rajagopalan", "Anoop Singhal"], "venue": "Journal of Computer Security,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Ruminations on multi-tenant databases", "author": ["Dean Jacobs", "Stefan Aulbach", "Technische Universitt Mnchen"], "venue": "In BTW Proceedings,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2007}, {"title": "Bridging the tenant-provider gap in cloud services", "author": ["Virajith Jalaparti", "Hitesh Ballani", "Paolo Costa", "Thomas Karagiannis", "Ant Rowstron"], "venue": "In Proceedings of the Third ACM Symposium on Cloud Computing,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2012}, {"title": "Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the world wide web", "author": ["David Karger", "Eric Lehman", "Tom Leighton", "Matthew Levine", "Daniel Lewin", "Rina Panigrahy"], "venue": "In the 29th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1997}, {"title": "Weighted round-robin cell multiplexing in a general-purpose atm switch chip", "author": ["M. Katevenis", "S. Sidiropoulos", "C. Courcoubetis"], "venue": "IEEE Journal on Selected Areas in Communications,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2006}, {"title": "Network virtualiza- 140  tion in multi-tenant datacenters", "author": ["Teemu Koponen", "Keith Amidon", "Peter Balland", "Martin Casado", "Anupam Chanda", "Bryan Fulton", "Igor Ganichev", "Jesse Gross", "Paul Ingram", "Ethan Jackson", "Andrew Lambeth", "Romain Lenglet", "Shih-Hao Li", "Amar Padmanabhan", "Justin Pettit", "Ben Pfaff", "Rajiv Ramanathan", "Scott Shenker", "Alan Shieh", "Jeremy Stribling", "Pankaj Thakkar", "Dan Wendlandt", "Alexander Yip", "Ronghua Zhang"], "venue": "In 11th USENIX Symposium on Networked Systems Design and Implementation,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2014}, {"title": "Resource usage control in multi-tenant applications", "author": ["Rouven Krebs", "Simon Spinner", "Nadia Ahmed", "Samuel Kounev"], "venue": "In Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing. IEEE/ACM,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2014}, {"title": "Cassandra: a decentralized structured storage system", "author": ["Avinash Lakshman", "Prashant Malik"], "venue": "ACM SIGOPS Operating Systems Review,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2010}, {"title": "vFair: Latency-aware fair storage scheduling via per-io cost-based differentiation", "author": ["Hui Lu", "Brendan Saltaformaggio", "Ramana Kompella", "Dongyan Xu"], "venue": "In Proceedings of the Sixth ACM Symposium on Cloud Computing,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2015}, {"title": "An em-based ensemble learning algorithm on piecewise surface regression problem", "author": ["Juan Luo", "Alexander Brodsky", "Yuan Li"], "venue": "International Journal of Applied Mathematics and Statistics,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2012}, {"title": "Hierarchical mapreduce programming model and scheduling algorithms", "author": ["Yuan Luo", "Beth Plale"], "venue": "In Proceedings of the 2012 12th IEEE/ACM International Sympo- 141  sium on Cluster, Cloud and Grid Computing (Ccgrid", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2012}, {"title": "Hierarchical mapreduce: towards simplified cross-domain dataprocessing", "author": ["Yuan Luo", "Beth Plale", "Zhenhua Guo", "Wilfred W. Li", "Judy Qiu", "Yiming Sun"], "venue": "Concurrency and Computation: Practice and Experience,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2012}, {"title": "NVMKV: A scalable and lightweight flash aware key-value store", "author": ["Leonardo M\u00e1rmol", "Swaminathan Sundararaman", "Nisha Talagala", "Raju Rangaswami", "Sushma Devendrappa", "Bharath Ramsundar", "Sriram Ganesan"], "venue": "In Proceedings of the 6th USENIX Conference on Hot Topics in Storage and File Systems, HotStorage", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2014}, {"title": "On packet switches with infinite storage", "author": ["J. Nagle"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 1987}, {"title": "Chaudhur. SQLVM: Performance isolation in multi-tenant relational database-as-aservice", "author": ["Vivek Narasayya", "Sudipto Das", "Manoj Syamala", "Badrish Chandramouli", "Surajit"], "venue": "In 7th Biennial Conference on Innovative Data Systems Research,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2013}, {"title": "Sharing buffer pool memory in multi-tenant relational database-as-aservice", "author": ["Vivek Narasayya", "Ishai Menache", "Mohit Singh", "Feng Li", "Manoj Syamala", "Surajit Chaudhuri"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2015}, {"title": "The log-structured merge-tree (LSM-tree)", "author": ["Patrick O\u2019Neil"], "venue": "In Acta Informatica,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 1996}, {"title": "Integrating high performance file systems in a cloud computing environment", "author": ["Abhisek Pan", "John Paul Walters", "Vijay S. Pai", "Dong-In D. Kang", "Stephen P. Crago"], "venue": "In High Performance Computing, Networking, Storage and Analysis,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2012}, {"title": "A generalized processor sharing approach to flow control in integrated services networks: The single-node case", "author": ["Abhay K. Parekh", "Robert G. Gallager"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 1993}, {"title": "Solving big data challenges 143  for enterprise application performance management", "author": ["Tilmann Rabl", "Sergio G\u00f3mez-Villamor", "Mohammad Sadoghi", "Victor Munt\u00e9s- Mulero", "Hans-Arno Jacobsen", "Serge Mankovskii"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2012}, {"title": "A-Cache: Resolving cache interference for distributed storage with mixed workloads", "author": ["Bharath Ravi", "Hrishikesh Amur", "Karsten Schwan"], "venue": "IEEE International Conference on Cluster Computing,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2013}, {"title": "TABLEFS: Enhancing metadata efficiency in the local file system", "author": ["Kai Ren", "Garth Gibson"], "venue": "In Proceedings of the 2013 USENIX Conference on Annual Technical Conference, USENIX ATC\u201913,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2013}, {"title": "GPFS: A shared-disk file system for large computing clusters", "author": ["Frank Schmuck", "Roger Haskin"], "venue": "In Proceedings of the 1st USENIX Conference on File and Storage Technologies, FAST \u201902,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2002}, {"title": "Transaction costs-aware portfolio optimization via fast lowner-john ellipsoid approximation", "author": ["Weiwei Shen", "Jun Wang"], "venue": "In Proceedings of the National Conference on Artificial Intelligence, AAAI\u2019", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2015}, {"title": "Portfolio choices with orthogonal bandit learning", "author": ["Weiwei Shen", "Jun Wang", "Yu-Gang Jiang", "Hongyuan Zha"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI\u2019", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2015}, {"title": "Doubly regularized portfolio with risk minimization", "author": ["Weiwei Shen", "Jun Wang", "Shiqian Ma"], "venue": "In Proceedings of the National Conference on Artificial Intelligence, AAAI\u2019", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2014}, {"title": "Cloudscale: Elastic resource scaling for multi-tenant cloud systems", "author": ["Zhiming Shen", "Sethuraman Subbiah", "Xiaohui Gu", "John Wilkes"], "venue": "In Proceedings of the 2Nd ACM Symposium on Cloud Computing,", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2011}, {"title": "Efficient fair queueing using deficit round robin", "author": ["M. Shreedhar", "George Varghese"], "venue": "In Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication,", "citeRegEx": "93", "shortCiteRegEx": "93", "year": 1995}, {"title": "From application requests to virtual iops: Provisioned key-value storage with libra", "author": ["David Shue", "Michael J. Freedman"], "venue": "In Proceedings of the 9th ACM European Conference on Computer Systems,", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2014}, {"title": "Performance isolation and fairness for multi-tenant cloud storage", "author": ["David Shue", "Michael J. Freedman", "Anees Shaikh"], "venue": "In Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2012}, {"title": "The hadoop distributed file system", "author": ["Konstantin Shvachko", "Hairong Kuang", "Sanjay Radia", "Robert Chansler"], "venue": "In 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies,", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 2010}, {"title": "Dynamic resource allocation for database servers running on virtual storage", "author": ["Gokul Soundararajan", "Daniel Lupei", "Saeed Ghanbari", "Adrian Daniel Popescu", "Jin Chen", "Cristiana Amza"], "venue": "In Proccedings of the 7th Conference on File and Storage Technologies,", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 2009}, {"title": "Torque resource manager", "author": ["Garrick Staples"], "venue": "In Proceedings of the 2006 ACM/IEEE Conference on Supercomputing, SC \u201906", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2006}, {"title": "Serving large-scale batch computed data with project voldemort", "author": ["Roshan Sumbaly", "Jay Kreps", "Lei Gao", "Alex Feinberg", "Chinmay Soman", "Sam Shah"], "venue": "In Proceedings of the 10th USENIX Conference on File and Storage Technologies,", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 2012}, {"title": "Distributed Systems: Principles and Paradigms (2nd Edition)", "author": ["Andrew S. Tanenbaum", "Maarten van Steen"], "venue": null, "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2007}, {"title": "Argon: Performance insulation for shared storage servers", "author": ["Matthew Wachs", "Michael Abd-El-Malek", "Eno Thereska", "Gregory R. Ganger"], "venue": "In Proceedings of the 5th USENIX Conference on File and Storage Technologies,", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2007}, {"title": "Towards performance isolation in multi-tenant saas applications", "author": ["Stefan Walraven", "Tanguy Monheim", "Eddy Truyen", "Wouter Joosen"], "venue": "In Proceedings of the 7th Workshop on Middleware for Next Generation Internet Computing,", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2012}, {"title": "Cake: Enabling high-level slos on shared storage systems", "author": ["Andrew Wang", "Shivaram Venkataraman", "Sara Alspaugh", "Randy Katz", "Ion Stoica"], "venue": "In Proceedings of the 3rd ACM Symposium on Cloud Computing,", "citeRegEx": "106", "shortCiteRegEx": "106", "year": 2012}, {"title": "Understanding Lustre file system internals", "author": ["Feiyi Wang"], "venue": "Technical report, Oak Ridge National Laboratory,", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2009}, {"title": "The design of the force.com multitenant internet application development platform", "author": ["Craig D. Weissman", "Steve Bobrowski"], "venue": "In Proceedings of the 2009 ACM SIG- MOD International Conference on Management of Data,", "citeRegEx": "109", "shortCiteRegEx": "109", "year": 2009}, {"title": "Intelligent management of virtualized resources for database systems in cloud environment", "author": ["Pengcheng Xiong", "Yun Chi", "Shenghuo Zhu", "Hyun Jin Moon", "Calton Pu", "Hakan Hacigumus"], "venue": "In Proceedings of the 2011 IEEE 27th International Conference on Data Engineering,", "citeRegEx": "110", "shortCiteRegEx": "110", "year": 2011}, {"title": "Rethinking key-value store for parallel i/o optimization", "author": ["Yanlong Yin", "Antonios Kougkas", "Kun Feng", "Hassan Eslami", "Yin Lu", "Xian-He Sun", "Rajeev Thakur", "William Gropp"], "venue": "In Proceedings of the 2014 International Workshop on Data Intensive Scalable Computing Systems,", "citeRegEx": "111", "shortCiteRegEx": "111", "year": 2014}, {"title": "Data pipeline in mapreduce", "author": ["Jiaan Zeng", "B. Plale"], "venue": "IEEE 9th International Conference on eScience, eScience", "citeRegEx": "112", "shortCiteRegEx": "112", "year": 2013}, {"title": "Multi-tenant fair share in nosql data stores", "author": ["Jiaan Zeng", "Beth Plale"], "venue": "IEEE International Conference on Cluster Computing,", "citeRegEx": "113", "shortCiteRegEx": "113", "year": 2014}, {"title": "Towards building a lightweight key-value store on parallel file system", "author": ["Jiaan Zeng", "Beth Plale"], "venue": "In 2015 IEEE International Conference on Cluster Computing,", "citeRegEx": "114", "shortCiteRegEx": "114", "year": 2015}, {"title": "Workload-aware resource reservation for multi-tenant nosql", "author": ["Jiaan Zeng", "Beth Plale"], "venue": "IEEE International Conference on Cluster Computing,", "citeRegEx": "115", "shortCiteRegEx": "115", "year": 2015}, {"title": "Cloud computing data capsules for non-consumptiveuse of texts", "author": ["Jiaan Zeng", "Guangchen Ruan", "Alexander Crowell", "Atul Prakash", "Beth Plale"], "venue": "In Proceedings of the 5th ACM Workshop on Scientific Cloud Computing, ScienceCloud", "citeRegEx": "116", "shortCiteRegEx": "116", "year": 2014}, {"title": "Quantitative Risk Assessment under Multi-Context Environments", "author": ["Su Zhang"], "venue": "PhD thesis, Kansas State University,", "citeRegEx": "117", "shortCiteRegEx": "117", "year": 2014}, {"title": "Effective network vulnerability assessment through model abstraction", "author": ["Su Zhang", "Xinming Ou", "John Homer"], "venue": "In Proceedings of the 8th International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment,", "citeRegEx": "118", "shortCiteRegEx": "118", "year": 2011}, {"title": "After we knew it: Empirical study and modeling of cost-effectiveness of exploiting prevalent known vulnerabilities across 148  iaas cloud", "author": ["Su Zhang", "Xinwen Zhang", "Xinming Ou"], "venue": "In Proceedings of the 9th ACM Symposium on Information, Computer and Communications Security, ASIA CCS", "citeRegEx": "119", "shortCiteRegEx": "119", "year": 2014}, {"title": "DeltaFS: Exascale file systems scale better without dedicated servers", "author": ["Qing Zheng", "Kai Ren", "Garth Gibson", "Bradley W. Settlemyer", "Gary Grider"], "venue": "Parallel Data Storage Workshop,", "citeRegEx": "120", "shortCiteRegEx": "120", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "The sheer volume of data increase is predicted to grow exponentially by a factor of 300 from 130 exabytes in 2005 to 40,000 exabytes in 2020 [37].", "startOffset": 141, "endOffset": 145}, {"referenceID": 6, "context": "Experience has shown that traditional systems like the relational databases (RDBMS) struggle to handle big data applications as they are difficult and expensive to scale across multiple nodes and have I/O performance that does not meet application requirements [15, 83].", "startOffset": 261, "endOffset": 269}, {"referenceID": 45, "context": "Experience has shown that traditional systems like the relational databases (RDBMS) struggle to handle big data applications as they are difficult and expensive to scale across multiple nodes and have I/O performance that does not meet application requirements [15, 83].", "startOffset": 261, "endOffset": 269}, {"referenceID": 4, "context": "Because of these attracting features, NoSQL data stores have seen a great deal of uptake in both industry [11,28,100] and academia [33,49].", "startOffset": 106, "endOffset": 117}, {"referenceID": 15, "context": "Because of these attracting features, NoSQL data stores have seen a great deal of uptake in both industry [11,28,100] and academia [33,49].", "startOffset": 106, "endOffset": 117}, {"referenceID": 59, "context": "Because of these attracting features, NoSQL data stores have seen a great deal of uptake in both industry [11,28,100] and academia [33,49].", "startOffset": 106, "endOffset": 117}, {"referenceID": 17, "context": "Because of these attracting features, NoSQL data stores have seen a great deal of uptake in both industry [11,28,100] and academia [33,49].", "startOffset": 131, "endOffset": 138}, {"referenceID": 15, "context": "Typical examples include Amazon Dynamo DB [5, 28], Google Cloud Datastore [19] and Microsoft DocumentDB [30].", "startOffset": 42, "endOffset": 49}, {"referenceID": 27, "context": "Generally, there are several advantages of adopting multi-tenancy in various storage services [53].", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "While the local file system has prevailed in the cloud platform for quite some time, the parallel file system (PFS) has begun seeing usage in the cloud in both industry [68] and academia [2, 79].", "startOffset": 187, "endOffset": 194}, {"referenceID": 43, "context": "While the local file system has prevailed in the cloud platform for quite some time, the parallel file system (PFS) has begun seeing usage in the cloud in both industry [68] and academia [2, 79].", "startOffset": 187, "endOffset": 194}, {"referenceID": 55, "context": "Although Amazon DynamoDB imposes a throughput limit to tenants to prevent the store deing dominated by a few tenants [5], it does not guarantee throughput provision, nor provides fair share among tenants [31,95].", "startOffset": 204, "endOffset": 211}, {"referenceID": 21, "context": "point out the burden and inefficiency of running persistent daemon services for a key-value store in the HPC environment [40].", "startOffset": 121, "endOffset": 125}, {"referenceID": 39, "context": "Specifically, we adapt and extend the deficit round robin algorithm [71] with linear programming as the scheduler to regulate throughput.", "startOffset": 68, "endOffset": 72}, {"referenceID": 55, "context": "Such an assumption is also used by [95, 106].", "startOffset": 35, "endOffset": 44}, {"referenceID": 63, "context": "Such an assumption is also used by [95, 106].", "startOffset": 35, "endOffset": 44}, {"referenceID": 42, "context": "The proposed KVS follows the spirit of log structure merge tree [76] to organize the data in PFS to support concurrent writes in a distributed environment.", "startOffset": 64, "endOffset": 68}, {"referenceID": 31, "context": "Multi-tenancy can be found in several different places: network multiplexing [58], virtual machine management [92], file system sharing [104], job runtime framework [54], data management system [53] and", "startOffset": 77, "endOffset": 81}, {"referenceID": 52, "context": "Multi-tenancy can be found in several different places: network multiplexing [58], virtual machine management [92], file system sharing [104], job runtime framework [54], data management system [53] and", "startOffset": 110, "endOffset": 114}, {"referenceID": 61, "context": "Multi-tenancy can be found in several different places: network multiplexing [58], virtual machine management [92], file system sharing [104], job runtime framework [54], data management system [53] and", "startOffset": 136, "endOffset": 141}, {"referenceID": 28, "context": "Multi-tenancy can be found in several different places: network multiplexing [58], virtual machine management [92], file system sharing [104], job runtime framework [54], data management system [53] and", "startOffset": 165, "endOffset": 169}, {"referenceID": 27, "context": "Multi-tenancy can be found in several different places: network multiplexing [58], virtual machine management [92], file system sharing [104], job runtime framework [54], data management system [53] and", "startOffset": 194, "endOffset": 198}, {"referenceID": 27, "context": "Generally, in a data management system, the multi-tenancy can be supported in three different models: shared machine, shared table, and shared process [53].", "startOffset": 151, "endOffset": 155}, {"referenceID": 12, "context": "Most research work about interference prevention target the shared process model in regard to provide isolation across tenants because of the advantages mentioned above [24, 84,104,106,113,115].", "startOffset": 169, "endOffset": 193}, {"referenceID": 46, "context": "Most research work about interference prevention target the shared process model in regard to provide isolation across tenants because of the advantages mentioned above [24, 84,104,106,113,115].", "startOffset": 169, "endOffset": 193}, {"referenceID": 61, "context": "Most research work about interference prevention target the shared process model in regard to provide isolation across tenants because of the advantages mentioned above [24, 84,104,106,113,115].", "startOffset": 169, "endOffset": 193}, {"referenceID": 63, "context": "Most research work about interference prevention target the shared process model in regard to provide isolation across tenants because of the advantages mentioned above [24, 84,104,106,113,115].", "startOffset": 169, "endOffset": 193}, {"referenceID": 69, "context": "Most research work about interference prevention target the shared process model in regard to provide isolation across tenants because of the advantages mentioned above [24, 84,104,106,113,115].", "startOffset": 169, "endOffset": 193}, {"referenceID": 71, "context": "Most research work about interference prevention target the shared process model in regard to provide isolation across tenants because of the advantages mentioned above [24, 84,104,106,113,115].", "startOffset": 169, "endOffset": 193}, {"referenceID": 20, "context": "According to the CAP-theorem [39], conflicts arise among different aspects of a distributed system in terms", "startOffset": 29, "endOffset": 33}, {"referenceID": 20, "context": "For the traditional SQL database, which stresses the ACID properties (Atomic, Consistency, Isolation, and Durability) [39], partition-tolerance or availability is usually sacrificed to honor the consistency.", "startOffset": 118, "endOffset": 122}, {"referenceID": 20, "context": "This results in the BASE properties (Basically Available, Soft-state, Eventually consistent) [39].", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "The master-slave architecture is used in Google BigTable [16], and its open source implementations e.", "startOffset": 57, "endOffset": 61}, {"referenceID": 15, "context": "The peer-to-peer architecture used by Amazon Dynamo [28] treats each node as an equal peer.", "startOffset": 52, "endOffset": 56}, {"referenceID": 59, "context": "For example, Berkeley DB is used as the back-end store in each individual node in Voldemort [100] and Riak [86].", "startOffset": 92, "endOffset": 97}, {"referenceID": 29, "context": "In practice, consistent hashing is usually preferred because it makes adding and removing machines in the system easy [56].", "startOffset": 118, "endOffset": 122}, {"referenceID": 27, "context": "As discussed in Chapter 2, the models of multi-tenancy can be realized in three different abstractions: shared machine, shared table and shared process [53].", "startOffset": 152, "endOffset": 156}, {"referenceID": 66, "context": "[110] allow tenants to set up database instances within VMs that share a single host.", "startOffset": 0, "endOffset": 5}, {"referenceID": 65, "context": "com [109], provided as software-as-a-service (SaaS), uses the shared table model where different tenants share the same set of database tables.", "startOffset": 4, "endOffset": 9}, {"referenceID": 11, "context": "Amazon DynamoDB service [5] and Relational Cloud [23] expose themselves as platform-as-a-service (PaaS), and follow the shared process model where tenants share the same data store process.", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": "The shared process model is usually preferred to serve tenants with non-shared data [53], because it provides reasonable isolation without imposing too much overhead as discussed in Section 2.", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "Therefore, a majority of work [24,72,84,95,104,106] about storage service sharing targets the shared process model.", "startOffset": 30, "endOffset": 51}, {"referenceID": 40, "context": "Therefore, a majority of work [24,72,84,95,104,106] about storage service sharing targets the shared process model.", "startOffset": 30, "endOffset": 51}, {"referenceID": 46, "context": "Therefore, a majority of work [24,72,84,95,104,106] about storage service sharing targets the shared process model.", "startOffset": 30, "endOffset": 51}, {"referenceID": 55, "context": "Therefore, a majority of work [24,72,84,95,104,106] about storage service sharing targets the shared process model.", "startOffset": 30, "endOffset": 51}, {"referenceID": 61, "context": "Therefore, a majority of work [24,72,84,95,104,106] about storage service sharing targets the shared process model.", "startOffset": 30, "endOffset": 51}, {"referenceID": 63, "context": "Therefore, a majority of work [24,72,84,95,104,106] about storage service sharing targets the shared process model.", "startOffset": 30, "endOffset": 51}, {"referenceID": 61, "context": "use a time-quanta-based disk scheduling approach with cache space partitioning for performance insulation among applications running on a single file server [104].", "startOffset": 157, "endOffset": 162}, {"referenceID": 22, "context": "use a feedback-based approach on a black-box storage system to dynamically adjust the number of IOs issued to the storage system with observed latency as feedback [41].", "startOffset": 163, "endOffset": 167}, {"referenceID": 40, "context": "propose an abstract of resource reservation called SQLVM on resources such as CPU, I/O and memory for tenant performance isolation and focus on I/O scheduling [72].", "startOffset": 159, "endOffset": 163}, {"referenceID": 12, "context": "present a CPU scheduling approach in SQLVM to reserve CPU usage for CPU interference prevention [24].", "startOffset": 96, "endOffset": 100}, {"referenceID": 57, "context": "propose a multi-resource allocator to dynamically partition the database\u2019s cache and its storage bandwidth so as to minimize request latency for all the tenants [97].", "startOffset": 161, "endOffset": 165}, {"referenceID": 57, "context": "However, different from [97], we intend to provide fairness across tenants.", "startOffset": 24, "endOffset": 28}, {"referenceID": 62, "context": "utilize a central scheduler to dispatch requests to different back-end RDBMS in a multi-tier web application [105].", "startOffset": 109, "endOffset": 114}, {"referenceID": 55, "context": "NoSQL Data Store Pisces [95] uses partition placement, replica selection, and fair queuing to provide multi-tenant fair share in terms of throughput in Membase, a memory-based NoSQL store with hash partitioning.", "startOffset": 24, "endOffset": 28}, {"referenceID": 53, "context": "In addition, we also adapt the deficit round robin algorithm [93] for scheduling.", "startOffset": 61, "endOffset": 65}, {"referenceID": 46, "context": "A-Cache [84] divides the block", "startOffset": 8, "endOffset": 12}, {"referenceID": 53, "context": "We adapt the deficit round robin algorithm [93] because of its simplicity and effectiveness.", "startOffset": 43, "endOffset": 47}, {"referenceID": 12, "context": "Most work on resource scheduling focus on either enforcing resource reservation [24, 42, 84], or providing proportional share [17, 41, 95, 104].", "startOffset": 80, "endOffset": 92}, {"referenceID": 23, "context": "Most work on resource scheduling focus on either enforcing resource reservation [24, 42, 84], or providing proportional share [17, 41, 95, 104].", "startOffset": 80, "endOffset": 92}, {"referenceID": 46, "context": "Most work on resource scheduling focus on either enforcing resource reservation [24, 42, 84], or providing proportional share [17, 41, 95, 104].", "startOffset": 80, "endOffset": 92}, {"referenceID": 8, "context": "Most work on resource scheduling focus on either enforcing resource reservation [24, 42, 84], or providing proportional share [17, 41, 95, 104].", "startOffset": 126, "endOffset": 143}, {"referenceID": 22, "context": "Most work on resource scheduling focus on either enforcing resource reservation [24, 42, 84], or providing proportional share [17, 41, 95, 104].", "startOffset": 126, "endOffset": 143}, {"referenceID": 55, "context": "Most work on resource scheduling focus on either enforcing resource reservation [24, 42, 84], or providing proportional share [17, 41, 95, 104].", "startOffset": 126, "endOffset": 143}, {"referenceID": 61, "context": "Most work on resource scheduling focus on either enforcing resource reservation [24, 42, 84], or providing proportional share [17, 41, 95, 104].", "startOffset": 126, "endOffset": 143}, {"referenceID": 44, "context": "Scheduling Algorithms Generalized processor sharing (GPS) is an idealized scheduler and achieves perfect fairness with the assumption that tenants\u2019 traffic is fluid [80].", "startOffset": 165, "endOffset": 169}, {"referenceID": 44, "context": "However, in real world scenarios, resource schedulers can only approximate the behavior of GPS due to the discretionary nature of computer [80].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "Fair queuing scheduler (FQ) assumes request time is linear to the size of data delivered [29].", "startOffset": 89, "endOffset": 93}, {"referenceID": 44, "context": "Weighted fair queuing scheduler (WFQ) extends FQ by considering weights in the estimation of finish time [80].", "startOffset": 105, "endOffset": 109}, {"referenceID": 30, "context": "It works for fixed size tasks but struggles with variable size tasks because it requires an estimation of mean task size [57].", "startOffset": 121, "endOffset": 125}, {"referenceID": 53, "context": "Deficit round robin (DRR) is a variation of WRR in the sense that it approximates GPS without knowing the mean size of tasks [93].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "Due to its simplicity and low time complexity as shown in [24,95], we use DRR in our resource scheduling.", "startOffset": 58, "endOffset": 65}, {"referenceID": 55, "context": "Due to its simplicity and low time complexity as shown in [24,95], we use DRR in our resource scheduling.", "startOffset": 58, "endOffset": 65}, {"referenceID": 23, "context": "Reservation mClock uses reservation and limitation to mitigate I/O interference across VMs running on the same hypervisor [42].", "startOffset": 122, "endOffset": 126}, {"referenceID": 46, "context": "ACache reserves the block cache space for tenants to protect hotspot oriented workload [84].", "startOffset": 87, "endOffset": 91}, {"referenceID": 53, "context": "calculate the deficit of CPU reservation and propose a variant of deficit round robin algorithm (DRR) [93] for elastic CPU reservation [24].", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "calculate the deficit of CPU reservation and propose a variant of deficit round robin algorithm (DRR) [93] for elastic CPU reservation [24].", "startOffset": 135, "endOffset": 139}, {"referenceID": 40, "context": "SQLVM reserves IOPS (IO operations/second) for each tenant [72].", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "It employs the virtual-time-based scheduling approach in [42] and translates the IOPS to a deadline for each tenant to guide the scheduling.", "startOffset": 57, "endOffset": 61}, {"referenceID": 41, "context": "study a page replacement algorithm for fair sharing the buffer pool memory in a RDBMS [73].", "startOffset": 86, "endOffset": 90}, {"referenceID": 55, "context": "Proportional Share Pisces [95] adapts the deficit round robin algorithm [93] for throughput regulation and intends to achieve Dominant Resource Fairness (DRF) [38].", "startOffset": 26, "endOffset": 30}, {"referenceID": 53, "context": "Proportional Share Pisces [95] adapts the deficit round robin algorithm [93] for throughput regulation and intends to achieve Dominant Resource Fairness (DRF) [38].", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "Proportional Share Pisces [95] adapts the deficit round robin algorithm [93] for throughput regulation and intends to achieve Dominant Resource Fairness (DRF) [38].", "startOffset": 159, "endOffset": 163}, {"referenceID": 61, "context": "use a time-quanta-based scheduling for fair sharing the disk [104].", "startOffset": 61, "endOffset": 66}, {"referenceID": 22, "context": "use the FAST-TCP algorithm to detect congestion and provide fair share on a black box storage system [41].", "startOffset": 101, "endOffset": 105}, {"referenceID": 8, "context": "The virtualization hypervisor Xen [17] also uses the notion of credit to schedule VCPU, a virtual CPU mapped to a physical core.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "There have been efforts on integrating parallel file system (PFS) in the cloud [2,52,68,79].", "startOffset": 79, "endOffset": 91}, {"referenceID": 43, "context": "There have been efforts on integrating parallel file system (PFS) in the cloud [2,52,68,79].", "startOffset": 79, "endOffset": 91}, {"referenceID": 67, "context": "compare the performance between a parallel file system and a KVS in terms of throughput [111].", "startOffset": 88, "endOffset": 93}, {"referenceID": 47, "context": "propose a file system that utilizes an embedded KVS to manage the file system metadata [85].", "startOffset": 87, "endOffset": 91}, {"referenceID": 38, "context": "Systems like NVMKV [69], FlushStore [26] focus on building S-KVS over flash storage with suitable data structures.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "Systems like NVMKV [69], FlushStore [26] focus on building S-KVS over flash storage with suitable data structures.", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "Both Dynamo [28] and Voldemort [100] use Berkeley DB as their default back-end storage.", "startOffset": 12, "endOffset": 16}, {"referenceID": 59, "context": "Both Dynamo [28] and Voldemort [100] use Berkeley DB as their default back-end storage.", "startOffset": 31, "endOffset": 36}, {"referenceID": 21, "context": "MDHIM [40] is a recently developed KVS that also provides on-demand access without running persistent servers in a distributed environment through MPI and a S-KVS i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "PLFS is a library file system which optimizes an application\u2019s data layout for the underlying file system [7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 76, "context": "DeltaFS embeds the file system metadata server as a library in an application to remove the centralized metadata server bottleneck [120].", "startOffset": 131, "endOffset": 136}, {"referenceID": 1, "context": "A background procedure called compaction runs periodically to remedy the read deterioration caused by writes [3,45,46].", "startOffset": 109, "endOffset": 118}, {"referenceID": 24, "context": "A background procedure called compaction runs periodically to remedy the read deterioration caused by writes [3,45,46].", "startOffset": 109, "endOffset": 118}, {"referenceID": 25, "context": "A background procedure called compaction runs periodically to remedy the read deterioration caused by writes [3,45,46].", "startOffset": 109, "endOffset": 118}, {"referenceID": 7, "context": "data files whose sizes are the smallest into one single file [16].", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "propose a compaction management framework that offloads the compaction on a dedicated server to lower the impact on actual workloads and uses a cache pre-fetching scheme to avoid the performance penalty from offloading [3].", "startOffset": 219, "endOffset": 222}, {"referenceID": 55, "context": "In a cloud environment, tenants often want to treat the entire storage system as a black box that can scale on demand, while in reality their data sets are usually co-located and cause resource contentions [95].", "startOffset": 206, "endOffset": 210}, {"referenceID": 58, "context": ", Hadoop fair scheduler [35] schedules based on task slots and Moab scheduler [98]", "startOffset": 78, "endOffset": 82}, {"referenceID": 57, "context": "For traditional storage systems, fair share is usually provided at the infrastructure level and involves a single physical resource on a single machine [97], e.", "startOffset": 152, "endOffset": 156}, {"referenceID": 33, "context": "In this chapter, we propose a novel approach to provide fair share across multiple tenants for NoSQL data stores, especially for Cassandra [14,60].", "startOffset": 139, "endOffset": 146}, {"referenceID": 39, "context": "In summary, this chapter makes the following contributions: \u2022 A framework that employs a feedback control loop to monitor and schedule requests; \u2022 A scheduler based on the adaption and extension of deficit round robin algorithm [71] with linear programming; \u2022 Adaptive control approaches to provide global fairness instead of local fairness and protect reads from scans; \u2022 Experimental results show effectiveness of our system.", "startOffset": 228, "endOffset": 232}, {"referenceID": 9, "context": "We use Yahoo Cloud Storage Benchmark (YCSB) [20] to generate the workloads and simulate multiple tenant access.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "Like [22, 95, 105], we measure operation throughput i.", "startOffset": 5, "endOffset": 18}, {"referenceID": 55, "context": "Like [22, 95, 105], we measure operation throughput i.", "startOffset": 5, "endOffset": 18}, {"referenceID": 62, "context": "Like [22, 95, 105], we measure operation throughput i.", "startOffset": 5, "endOffset": 18}, {"referenceID": 58, "context": "This draws from the Moab scheduler [98] where future jobs have fewer chances to be scheduled if past jobs consume more resources.", "startOffset": 35, "endOffset": 39}, {"referenceID": 53, "context": "We use the deficit round robin (DRR) [93] algorithm to schedule requests because of its simplicity and effectiveness, and linear pro-", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "Dominant resource fairness (DRF) [38] could be used here, and this is what Pisces does [95].", "startOffset": 33, "endOffset": 37}, {"referenceID": 55, "context": "Dominant resource fairness (DRF) [38] could be used here, and this is what Pisces does [95].", "startOffset": 87, "endOffset": 91}, {"referenceID": 63, "context": "Instead of exploring how to suspend an ongoing operation temporarily, we use a simple approach, which is used in other systems as well [55, 106].", "startOffset": 135, "endOffset": 144}, {"referenceID": 9, "context": "On the client side, we use YCSB [20] to generate the workloads and use 5 additional nodes to run the clients to simulate 5 tenants accessing Cassandra.", "startOffset": 32, "endOffset": 36}, {"referenceID": 33, "context": "This triggers the compaction procedure [60], which requires many disk I/O and influences other tenants\u2019 throughputs.", "startOffset": 39, "endOffset": 43}, {"referenceID": 7, "context": "Similar situation also happens to Bigtable like systems [16].", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "Previous research on preventing performance interference does so by simplifying the scenario, either by considering a single resource [24, 59, 84, 105] (e.", "startOffset": 134, "endOffset": 151}, {"referenceID": 32, "context": "Previous research on preventing performance interference does so by simplifying the scenario, either by considering a single resource [24, 59, 84, 105] (e.", "startOffset": 134, "endOffset": 151}, {"referenceID": 46, "context": "Previous research on preventing performance interference does so by simplifying the scenario, either by considering a single resource [24, 59, 84, 105] (e.", "startOffset": 134, "endOffset": 151}, {"referenceID": 62, "context": "Previous research on preventing performance interference does so by simplifying the scenario, either by considering a single resource [24, 59, 84, 105] (e.", "startOffset": 134, "endOffset": 151}, {"referenceID": 55, "context": "CPU, cache), or representing multiple resources consumption as a single \u201cvirtual resource\u201d consumption [95, 113].", "startOffset": 103, "endOffset": 112}, {"referenceID": 69, "context": "CPU, cache), or representing multiple resources consumption as a single \u201cvirtual resource\u201d consumption [95, 113].", "startOffset": 103, "endOffset": 112}, {"referenceID": 46, "context": "As [84,95,113] show, multi-tenant performance interference could occur in various NoSQL stores.", "startOffset": 3, "endOffset": 14}, {"referenceID": 55, "context": "As [84,95,113] show, multi-tenant performance interference could occur in various NoSQL stores.", "startOffset": 3, "endOffset": 14}, {"referenceID": 69, "context": "As [84,95,113] show, multi-tenant performance interference could occur in various NoSQL stores.", "startOffset": 3, "endOffset": 14}, {"referenceID": 7, "context": "HBase is an open source implementation of Google BigTable [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 56, "context": "HDFS [96] and runs on top of it.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "We use the Yahoo Cloud Storage Benchmark (YCSB) [20] to simulate multi-tenant access.", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "Similar to [22, 95, 105, 113], we measure the operation throughput, i.", "startOffset": 11, "endOffset": 29}, {"referenceID": 55, "context": "Similar to [22, 95, 105, 113], we measure the operation throughput, i.", "startOffset": 11, "endOffset": 29}, {"referenceID": 62, "context": "Similar to [22, 95, 105, 113], we measure the operation throughput, i.", "startOffset": 11, "endOffset": 29}, {"referenceID": 69, "context": "Similar to [22, 95, 105, 113], we measure the operation throughput, i.", "startOffset": 11, "endOffset": 29}, {"referenceID": 12, "context": "Similar to [24], to quantify the interference, we calculate the throughput violation as violationi = (baselinei \u2212 throughputi)/baselinei, where baselinei and throughputi are the baseline throughput and actual throughput of tenant i respectively.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "bytes delivered, CPU usage, and cache usage, has been used by many people [24, 84, 95, 113].", "startOffset": 74, "endOffset": 91}, {"referenceID": 46, "context": "bytes delivered, CPU usage, and cache usage, has been used by many people [24, 84, 95, 113].", "startOffset": 74, "endOffset": 91}, {"referenceID": 55, "context": "bytes delivered, CPU usage, and cache usage, has been used by many people [24, 84, 95, 113].", "startOffset": 74, "endOffset": 91}, {"referenceID": 69, "context": "bytes delivered, CPU usage, and cache usage, has been used by many people [24, 84, 95, 113].", "startOffset": 74, "endOffset": 91}, {"referenceID": 55, "context": "Similar to [95, 113], we use the bytes delivered from HBase as a \u201cvirtual resource\u201d to represent the underlying resource consumption.", "startOffset": 11, "endOffset": 20}, {"referenceID": 69, "context": "Similar to [95, 113], we use the bytes delivered from HBase as a \u201cvirtual resource\u201d to represent the underlying resource consumption.", "startOffset": 11, "endOffset": 20}, {"referenceID": 46, "context": "In the second experiment, we divide up the block cache space into half for two tenants so as to provide strong isolation in the cache space as suggested in [84].", "startOffset": 156, "endOffset": 160}, {"referenceID": 46, "context": "Unlike ACache [84] that replaces HBase\u2019s default cache replacement, we apply the built-in LRU cache replacement in HBase to the cache partition of each tenant because the cache replacement in HBase has been improved to prioritize the eviction based on the times the", "startOffset": 14, "endOffset": 18}, {"referenceID": 44, "context": "As stated in Chapter 3, there are two types of scheduling approaches that approximate the generalized processor sharing (GPS) model [80] to provide fair sharing.", "startOffset": 132, "endOffset": 136}, {"referenceID": 44, "context": "To understand which approach may work well in the context of multi-tenancy in HBase, we study weighted fair queuing (WFQ) [80], which is a virtual time based scheduler, and deficit round robin (DRR) [93], which is a quanta based scheduler.", "startOffset": 122, "endOffset": 126}, {"referenceID": 53, "context": "To understand which approach may work well in the context of multi-tenancy in HBase, we study weighted fair queuing (WFQ) [80], which is a virtual time based scheduler, and deficit round robin (DRR) [93], which is a quanta based scheduler.", "startOffset": 199, "endOffset": 203}, {"referenceID": 30, "context": "DRR is a variant of weighted round robin [57] that uses quanta (sometimes called tokens or credits) to throttle requests.", "startOffset": 41, "endOffset": 45}, {"referenceID": 32, "context": "discuss more advanced prediction options [59].", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "Similar to [24], to quantify fairness, we use the Jain index (J-index) defined in equation 5.", "startOffset": 11, "endOffset": 15}, {"referenceID": 34, "context": "[64] also measures fairness through comparing the actual throughput to the baseline throughput.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "[97] also evidences that such a variant in a single node file system setting leads to failure of fairness enforcement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "Instead of inferring to an analytic form of the function, which is difficult and error prone [97], we simply use regression to interpolate the function on some sample data collected by running the workload offline with different cache and HDFS throughput reservation percentages.", "startOffset": 93, "endOffset": 97}, {"referenceID": 42, "context": "HBase follows the LSM [76] design which periodically flushes data from MemStore, an in-memory structure, to fixed size files in HDFS and merges those files later in the compaction procedure which incurs extra I/O in the background.", "startOffset": 22, "endOffset": 26}, {"referenceID": 9, "context": "We use the YCSB benchmark [20] to populate the data and generate the workload.", "startOffset": 26, "endOffset": 30}, {"referenceID": 61, "context": "A more advanced option is to dynamically adjust the credits according to the workload characteristics suggested in [104].", "startOffset": 115, "endOffset": 120}, {"referenceID": 46, "context": "Finally, we compare Argus with A-Cache [84], a HBase based system that aims at preventing cache interference.", "startOffset": 39, "endOffset": 43}, {"referenceID": 63, "context": "[106] suggests a two-level scheduler that works both at the HBase level and HDFS level to deal with get and scan mixed workloads.", "startOffset": 0, "endOffset": 5}, {"referenceID": 54, "context": "It requires a model that can represent the I/O behavior of the system with background procedures running, which is very difficult to infer even in a single node storage system [94].", "startOffset": 176, "endOffset": 180}, {"referenceID": 54, "context": "Similar to our offline modeling approach, [94] derives a non-linear function that transforms a request to underlying I/O cost by running workload offline.", "startOffset": 42, "endOffset": 46}, {"referenceID": 54, "context": "In a word, we feel Argus can be extended to use the approaches in [94, 106] although there are challenges from aggregating additional resources in the workload model.", "startOffset": 66, "endOffset": 75}, {"referenceID": 63, "context": "In a word, we feel Argus can be extended to use the approaches in [94, 106] although there are challenges from aggregating additional resources in the workload model.", "startOffset": 66, "endOffset": 75}, {"referenceID": 46, "context": "Lastly, we compare Argus with A-Cache [84] in terms of fairness and efficiency.", "startOffset": 38, "endOffset": 42}, {"referenceID": 72, "context": "To see how Argus could work for real world applications, we draw on big data text mining of the HathiTrust Research Center (HTRC) [49, 116].", "startOffset": 130, "endOffset": 139}, {"referenceID": 2, "context": "All of these data objects are slow changing so workloads against all three are largely read-only, matching the observation in [4] capturing realistic workloads in NoSQL stores.", "startOffset": 126, "endOffset": 129}, {"referenceID": 35, "context": "topic modeling analysis, advanced machine learning for classification [65, 89, 90], while some of the others may just run for one time, e.", "startOffset": 70, "endOffset": 82}, {"referenceID": 49, "context": "topic modeling analysis, advanced machine learning for classification [65, 89, 90], while some of the others may just run for one time, e.", "startOffset": 70, "endOffset": 82}, {"referenceID": 50, "context": "topic modeling analysis, advanced machine learning for classification [65, 89, 90], while some of the others may just run for one time, e.", "startOffset": 70, "endOffset": 82}, {"referenceID": 14, "context": "Various key-value stores [27,60,100] have been developed to facilitate analysis on social media feeds, web logs, and etc.", "startOffset": 25, "endOffset": 36}, {"referenceID": 33, "context": "Various key-value stores [27,60,100] have been developed to facilitate analysis on social media feeds, web logs, and etc.", "startOffset": 25, "endOffset": 36}, {"referenceID": 59, "context": "Various key-value stores [27,60,100] have been developed to facilitate analysis on social media feeds, web logs, and etc.", "startOffset": 25, "endOffset": 36}, {"referenceID": 33, "context": "Examples include Cassandra [60], HBase [1], and etc.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "Parallel file system (PFS) has begun seeing usage in the cloud in both industry [68] and academia [2, 52, 79].", "startOffset": 98, "endOffset": 109}, {"referenceID": 43, "context": "Parallel file system (PFS) has begun seeing usage in the cloud in both industry [68] and academia [2, 52, 79].", "startOffset": 98, "endOffset": 109}, {"referenceID": 21, "context": "also point out the burden and inefficiency of running persistent KVS service in the HPC environment [40].", "startOffset": 100, "endOffset": 104}, {"referenceID": 70, "context": "The design, presented in a poster [114], is further developed in this chapter.", "startOffset": 34, "endOffset": 39}, {"referenceID": 42, "context": "It makes use of the log structure merge tree (LSM) [76] structure to support concurrent writes and uses a novel tree based compaction strategy to support concurrent reads efficiently.", "startOffset": 51, "endOffset": 55}, {"referenceID": 59, "context": "In summary, this chapter makes the following contributions: \u2022 A LSM based framework with asynchronous mechanisms to support concurrent writes and reads; \u2022 A tree based compaction equipped with parallel processing to improve read performance; \u2022 Experimental results that show KVLight has better performance than other M-KVS including Cassandra and Voldemort [100] in several different workloads including two real world applications.", "startOffset": 357, "endOffset": 362}, {"referenceID": 5, "context": "The Parallel Virtual File System (PVFS) [12] runs its server processes, i.", "startOffset": 40, "endOffset": 44}, {"referenceID": 48, "context": "The General Purpose File System (GPFS) [88], developed in IBM, bears a similar architecture with PVFS.", "startOffset": 39, "endOffset": 43}, {"referenceID": 56, "context": "Unlike the Hadoop File System [96] which stores all the metadata in a single server, GPFS distributes the metadata e.", "startOffset": 30, "endOffset": 34}, {"referenceID": 64, "context": ", Lustre [107] is the most widely used PFS nowadays.", "startOffset": 9, "endOffset": 14}, {"referenceID": 9, "context": "Yahoo Cloud Storage Benchmark (YCSB) [20] is used to generate the workloads.", "startOffset": 37, "endOffset": 41}, {"referenceID": 60, "context": "For consistency within a process, according to [101], KVLight follows the read-your-write consistency.", "startOffset": 47, "endOffset": 52}, {"referenceID": 68, "context": "An example is log processing [112].", "startOffset": 29, "endOffset": 34}, {"referenceID": 21, "context": "The access to compute nodes is granted as the job is scheduled and revoked as the job is terminated, which makes traditional MKVS like Cassandra unable to persist data as it requires a long running service on each compute node [40].", "startOffset": 227, "endOffset": 231}, {"referenceID": 59, "context": "Voldemort is a KVS using Berkeley DB Java Edition as the default backend store and used widely in LinkedIn [100].", "startOffset": 107, "endOffset": 112}, {"referenceID": 9, "context": "The client uses YCSB [20] to generate the data as well as workloads and runs on additional nodes.", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "For Cassandra, we keep its default settings, and configure the Java heap size to 4GB so as to leave most of the memory to OS as recommended in [45].", "startOffset": 143, "endOffset": 147}, {"referenceID": 2, "context": "ETC [4].", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "We generate 25 million keyvalues pairs based on the key-size and value-size distributions specified in [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "2 to approximate the key access sequence in [4].", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "These ratios are also specified in [4].", "startOffset": 35, "endOffset": 38}, {"referenceID": 67, "context": "write in the trace as a key-value pair whose key size is fixed and value size is the number of bytes written [111].", "startOffset": 109, "endOffset": 114}, {"referenceID": 69, "context": "Our evaluations [113, 115] show that writes from client requests may trigger extra writes to the internal data structures and influence reads.", "startOffset": 16, "endOffset": 26}, {"referenceID": 71, "context": "Our evaluations [113, 115] show that writes from client requests may trigger extra writes to the internal data structures and influence reads.", "startOffset": 16, "endOffset": 26}, {"referenceID": 36, "context": "It is also interesting to apply the NoSQL store equipped with multi-tenant support in the MapReduce framework [66, 67] to support multi-tenancy.", "startOffset": 110, "endOffset": 118}, {"referenceID": 37, "context": "It is also interesting to apply the NoSQL store equipped with multi-tenant support in the MapReduce framework [66, 67] to support multi-tenancy.", "startOffset": 110, "endOffset": 118}, {"referenceID": 26, "context": "Another direction for exploration is to apply the isolation mechanisms to cloud environments with security requirements [47,117\u2013119].", "startOffset": 120, "endOffset": 132}, {"referenceID": 73, "context": "Another direction for exploration is to apply the isolation mechanisms to cloud environments with security requirements [47,117\u2013119].", "startOffset": 120, "endOffset": 132}, {"referenceID": 74, "context": "Another direction for exploration is to apply the isolation mechanisms to cloud environments with security requirements [47,117\u2013119].", "startOffset": 120, "endOffset": 132}, {"referenceID": 75, "context": "Another direction for exploration is to apply the isolation mechanisms to cloud environments with security requirements [47,117\u2013119].", "startOffset": 120, "endOffset": 132}, {"referenceID": 72, "context": "It is also interesting to explore the application space for KVLight in a digital library setting [116] and a finance setting [91].", "startOffset": 97, "endOffset": 102}, {"referenceID": 51, "context": "It is also interesting to explore the application space for KVLight in a digital library setting [116] and a finance setting [91].", "startOffset": 125, "endOffset": 129}], "year": 2016, "abstractText": "vii", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}