{"id": "1601.00740", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "Brain4Cars: Car That Knows Before You Do via Sensory-Fusion Deep Learning Architecture", "abstract": "Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for unsafe road conditions and alert drivers if they perform a dangerous maneuver. However, many accidents are unavoidable because by the time drivers are alerted, it is already too late. Anticipating maneuvers beforehand can alert drivers before they perform the maneuver and also give ADAS more time to avoid or prepare for the danger.", "histories": [["v1", "Tue, 5 Jan 2016 05:25:14 GMT  (3099kb,D)", "http://arxiv.org/abs/1601.00740v1", "Journal Version (ICCV and ICRA combination with more system details)this http URL"]], "COMMENTS": "Journal Version (ICCV and ICRA combination with more system details)this http URL", "reviews": [], "SUBJECTS": "cs.RO cs.CV cs.LG", "authors": ["ashesh jain", "hema s koppula", "shane soh", "bharad raghavan", "avi singh", "ashutosh saxena"], "accepted": false, "id": "1601.00740"}, "pdf": {"name": "1601.00740.pdf", "metadata": {"source": "CRF", "title": "Brain4Cars: Car That Knows Before You Do via Sensory-Fusion Deep Learning Architecture", "authors": ["Ashesh Jain", "Hema S Koppula", "Shane Soh", "Bharad Raghavan", "Avi Singh", "Ashutosh Saxena"], "emails": ["ashesh@cs.cornell.edu,", "hema@cs.cornell.edu,", "avisingh@iitk.ac.in,", "shanesoh@cs.stanford.edu", "bharadr@cs.stanford.edu", "asaxena@cs.stanford.edu"], "sections": [{"heading": null, "text": "This year, we will be in a position to try to find a solution that is capable of finding a solution, in which we will have to try to find a solution."}, {"heading": "II. RELATED WORK", "text": "In fact, it is such that we will be able to maneuver ourselves into a situation in which we are able to maintain ourselves, in which we are able to maintain ourselves, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able, that we are"}, {"heading": "III. OVERVIEW", "text": "We first give an overview of the problem of manoeuvring anticipation and then describe our system."}, {"heading": "A. Problem Overview", "text": "Our goal is to anticipate driving maneuvers a few seconds before they occur, including anticipating lane change before the wheels touch the road markings or anticipating when the driver is driving straight or making a curve when approaching an intersection. This is a difficult problem for several reasons. First, it requires modeling context from multiple sources. Information from a single source, such as a camera that captures events outside the car, is not sufficiently abundant. Additional visual information from inside the car can also be used. For example, the driver's head movements are useful for anticipation - drivers typically check side traffic while recording cross traffic at intersections. Second, thinking about maneuvers should take into account the driving context at both the local and global levels. Local context requires modeling of events near the vehicle, such as surrounding vision, GPS and speed information. On the other hand, the overall context, such as the final destination, is influencing factors such as the final destination."}, {"heading": "B. System Overview", "text": "For manoeuvrability anticipation, our vehicle-specific sensor platform includes the following (as shown in Figure 4): 1) A driver-oriented camera in the vehicle. We mount this camera on the dashboard and use it to track the driver's head movements. 2) A driver-oriented camera is mounted on the dashboard to capture the (exterior) view in front of the car. This camera operates at 30 fps. The video from this camera allows additional reflection on manoeuvres. For example, when the vehicle is in the left lane, the only safe manoeuvres are right-hand turning or straight-turning unless the vehicle is approaching an intersection. 3) A speed logger for vehicle dynamics, because manoeuvres correlate with the speed of the vehicle, e.g. turns usually at lower speeds than lane changes. 4) A global positioning system (GPS) to locate the vehicle on the map. This allows us to detect upcoming traffic junctions, such as highways, etc."}, {"heading": "IV. PRELIMINARIES", "text": "The goal of anticipation is to predict an event several seconds before it occurs in view of the contextual information up to the present. The future event can be one of several possibilities. At training times, a series of temporal sequences of observations and events {(x1, x2,..., xT) j, yj} Nj = 1 is provided, xt being the observation at the time t, y being the representation of the event (described below) that occurs at the end of the sequence at t = T, and j being the sequence index. However, at test time, the algorithm receives an observation xt at each time step, and its goal is to predict the future event as early as possible, i.e. by observing only a partial sequence of observations {(x1,..., xt) | t < T}. This distinguishes the expectation from the activity detection [70, 37] where in the latter the complete observation sequence is available at a time, i.e. by observing only a partial sequence of observations {(x1,..., xt} (< T, xxt)."}, {"heading": "A. Recurrent Neural Networks", "text": "A standard RNN [54] accepts a temporal sequence of vectors (x1, x2,..., xT) as input and outputs a sequence of vectors (h1, h2,..., hT), also known as high-level representations, which are generated by non-linear transformation of the input sequence from t = 1 to T. ht = f (Wxt + Hht \u2212 1 + b) (1) yt = softmax (Wyht + by) (2), where f is a non-linear function applied elementally, and yt is the softmax probability of the events that the observations have seen up to xt. W, H, b, Wy are the learned parameters. Matrices are labeled with bold uppercase letters, and vectors are labeled with bold, lowercase time teeth. In a standard RNN, a frequent choice is for f tanh or sigmoid."}, {"heading": "B. Long-Short Term Memory Cells", "text": "LSTM is a network of neurons that implements a memory cell [25]. The central idea behind LSTM is that the memory cell can maintain its state over time. In combination with RNN, LSTM units allow the recursive network to remember long-term context dependencies. LSTM consists of three gates - entrance gate i, exit gate o, and forget gate f - and a memory cell. See Figure 6 for an illustration. At each step t, LSTM first computes the activations of its gates {it, ft} (3) (4) and updates its memory cell from ct \u2212 1 to ct (5), then computes the output gate activation ot (6) and finally outputs a hidden representation ht (7). The inputs in LSTM are the observations xt and the hidden representation from the previous time step \u2212 1. LSTM we are."}, {"heading": "V. NETWORK ARCHITECTURE FOR ANTICIPATION", "text": "Previous work has treated anticipation as a detection problem [34, 50, 57] and trained discriminatory classifiers (such as SVM or CRF) over the entire temporal context. However, at test times, these classifiers observe only a partial temporal context and make predictions within a filter frame. We model anticipation with a recurring architecture that unfolds over time, enabling us to form a single classifier that learns to deal with a partial temporal context of varying length. Furthermore, anticipation in robotics applications is challenging because the contextual information can come from multiple sensors with different data modalities, such as autonomous vehicles that emanate from multiple sensors [3] or robots that jointly think about perception and voice instructions. [48] In such applications, the way information from different sensors is fused is crucial for the final application."}, {"heading": "A. RNN with LSTM units for anticipation", "text": "At the time of training, we observe the complete time observation sequence and the event {(x1, x2,..., xT), y}. Our goal is to train a network that predicts the future event based on a partial time observation sequence {(x1, x2,..., xt) | t < T}. We do this by training an RNN in a sequence-to-sequence prediction mode. In view of training examples {(x1, x2,..., xT) j, yj} Nj = 1, we train an RNN with LSTM units to map the sequence of observations (x1, x2,..., xT) to the sequence of events (y1,..., yT) in such a way that yt = y, t as shown in Fig. 2."}, {"heading": "B. Fusion-RNN: Sensory fusion RNN for anticipation", "text": "We now present an instance of our RNN architecture to merge two sensory currents: {(x1,..., xT), (z1,..., zT)}. In the next section we will describe these currents to maneuver the anticipation.One obvious way to allow sensory merging in the RNN is to concatenate the currents, i.e. to use ([x1; z1],..., [xT; zT]) as input to the RNN. However, we have found that this type of simple concatenation does not work well. Instead, we learn a sensory fusion layer that combines the high-level representations of sensor data. Our proposed architecture first carries the two sensory currents {(x1,..., xT), (z1,..., zT)} independently through the individual RNNs (9) and (10).The high-level representations of both RNN {(xh1, S, xT), (xT), (xT), (xT)."}, {"heading": "C. Exponential loss-layer for anticipation.", "text": "We propose a new loss layer that encourages the architecture to anticipate errors early, while ensuring that the architecture does not fit the training data early enough if there is not enough context for anticipation. If the standard softmax loss is used, the architecture suffers a loss of \u2212 log (ykt) for the errors it makes at each step, ykt being the probability of the event k calculated by the architecture using equation. (12) We propose to modify this loss by multiplying it by an exponential term, as shown in Figure 7. Under this new scheme, the loss grows exponentially over time, as shown below. Loss = N \u00b2 j = 1 T \u00b2 t = 1 \u2212 e \u2212 e \u2212 (T) log (ykt) (13) This loss punishes the RNN exponentially more for the errors it makes, as it sees more observations. This encourages the model to correct errors as early as possible."}, {"heading": "VI. FEATURES", "text": "We do this by subdividing the general contextual information from the sensors into the following areas: (i) the context from inside the vehicle emanating from the driver's camera and therefore presented as a temporal sequence of features (z1,..., zT); and (ii) the context outside the vehicle emanating from the remaining sensors: GPS, street-facing camera and street maps. We represent the outer context with (x1,..., xT). To anticipate maneuvers, our RNN architecture (Figure 7) runs in the temporal context (x1,..., xt), (z1, zt). We present the outer context with the following five maneuvers: M = {left turn, right turn, left lane change, right lane change, straight driving}.A. Inside-vehicle voicures.The inside features capture the driver's head movements at each time t, and outputs softtest for max yvers} {left turn euvers:"}, {"heading": "B. Outside-vehicle features.", "text": "The external feature vector xt encodes information about the external environment such as road conditions, vehicle dynamics, etc. To get this information, we use the road-facing camera together with the GPS coordinates of the vehicle, its speed and the road maps. Specifically, we get two binary features from the road-facing camera that indicates if there is a lane on the left and right side of the vehicle. We also add the GPS coordinates of the vehicle with the road maps and extract a binary feature that indicates if the vehicle is within 15 meters of a road artifact, such as crossroads, curves, highways, etc. We also encode the average, maximum and minimum speeds of the vehicle during the last 5 seconds as features. This results in an xt-R6-dimensional feature vector."}, {"heading": "VII. BAYESIAN NETWORKS FOR MANEUVER ANTICIPATION", "text": "In this section, we propose alternative Bayesian networks [30] based on the Hidden Markov Model (HMM) for maneuver anticipation, which forms a strong basis for comparing our sensory-fusion deep-learning architecture. Driving maneuvers are influenced by a variety of interactions that affect the vehicle, its driver, external traffic, and occasionally global factors such as the driver's destination. These interactions affect the driver's intention, i.e. his state of mind before the maneuver, which is not directly observable. In our Bayesian network formulation, we represent the driver's intention with discrete states that are latent (or hidden). To anticipate maneuvers, we jointly model the driving context and latent states in a tractable manner. We present the driving context as a set of features described in Section VI. We will now present the motivation for the Bayesian networks, and then discuss our key model, Autegressive Input-Output-HAIM (HMO-MM)."}, {"heading": "A. Modeling driving maneuvers", "text": "The modelling of manoeuvres requires a temporal modeling of the driving context. Discriminatory methods such as the support vector machine and the relevance vector machine [65], which do not model the temporal aspect, do not work well in anticipation tasks, as we see in Section VIII. Therefore, a temporal model such as the Hidden Markov Model (HMM) is better suited for anticipating manoeuvres. An HMM models how the latent states of the driver generate both the inner driving context (zt) and the outer driving context (xt). However, a more precise model should capture how events outside the vehicle (i.e. the outer driving context) affect the mental state of the driver, which then generates the observations within the vehicle (i.e. the inner driving context). Such interactions can be modelled using an input output Autorence HMM (IOHMM)."}, {"heading": "VIII. EXPERIMENTS", "text": "In this section, we will first give an overview of our data set and then present the quantitative results. We will also demonstrate our system and algorithm on real-world driving scenarios. Our video demonstrations are available at: http: / / www.brain4cars.com."}, {"heading": "A. Driving data set", "text": "Our data set consists of natural driving videos with interior and exterior views of the car, its speed and the coordinates of the global positioning system (GPS).2 The external car video2The indoor and outdoor cameras work at 25 and 30 frames / sec to capture the view of the road in front of us. We captured this data set under completely natural conditions without interference.3 It consists of 1180 miles of driving and urban driving and covers 21,000 square miles over two states. We captured this data set of 10 drivers over a two-month period. The complete data set comprises a total of 2 million video images and includes various landscapes. Figure 11 shows some examples from our data set. We commented on the driving videos with a total of 700 events involving 274 lane changes, 131 curves and 295 randomly detected cases of straight-line driving. Each lane change or turn comment marks the start time of the maneuver, i.e. before the car touches the lane or lane."}, {"heading": "B. Baseline algorithms", "text": "We compare the following algorithms: \u2022 Chance: Uniformly randomly anticipates a maneuver (50): Support Vector Machine is a discriminativeclassifier [11]. Morris et al. [50] takes this approach to anticipating maneuvers. [50] We train the SVM to 5 seconds of the driving context by linking all the frame features together to obtain an R3840-dimensional feature vector. \u2022 Random Forest [12]: This is also a discriminatory classifier that learns many decision trees from the training data, and in the test period it measures the prediction of each decision tree. We train it on the same characteristics as SVM with 150 trees of depth each. \u2022 HMM: This is the Hidden Markov model. We train the HMM on a time sequence of feature vectors that we extract every 0.8 seconds, i.e."}, {"heading": "C. Evaluation protocol", "text": "We evaluate an algorithm based on its accuracy in predicting future manoeuvres. We expect manoeuvres every 0.8 seconds in which the algorithm processes the current context and assigns a probability to each of the four manoeuvres: {left turn, right turn, left turn, right turn} and a probability for the event of the ride straight ahead. These five probabilities add up to one. After the prediction, i.e. if the algorithm has calculated all five probabilities, the algorithm predicts a manoeuvre if its probability is above a threshold. If none of the manoeuvres exceeds the probabilities above this threshold, the algorithm does not perform a manoeuvre prediction and predicts the ride straight ahead. However, if it predicts one of the four manoeuvres, it sticks to this prediction and makes no further predictions for the next 5 seconds or until a manoeuvre occurs, then nothing happens."}, {"heading": "D. Quantitative results", "text": "It's only a matter of time before it's as far as it's ever been."}, {"heading": "IX. CONCLUSION", "text": "We proposed a novel deep-learning architecture based on recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) units for anticipation; our architecture learns to merge multiple sensory currents, and by training them in a sequence-to-sequence prediction mode, it explicitly learns to anticipate the use of only a partial temporal context; we also proposed a novel loss layer for anticipation that avoids overpassing.We published an open-source dataset of 1180 miles of natural driving; we conducted a comprehensive evaluation and showed improvements across many basic algorithms. Our sensory fusion deep learning approach gives an accuracy of 84.5% and a memory of 77.1%."}, {"heading": "APPENDIX A MODELING MANEUVERS WITH AIO-HMM", "text": "The driving context C consists of the outer driving context and the inner driving context. The outer and inner context are temporal sequences defined by the external characteristics xT1 = {x1,.., xT} and the internal characteristics zT1 = {z1,.., zT} and the internal characteristics zT1 = {zT1,.., zT}. The corresponding sequence of the latent states of the driver is hT1 = {h1,.., hT}. x and z are vectors and h is a discrete state.P (C | M) is a discrete state.P (zT1, x T 1, h | M) are temporal sequences of the states hT1 P (zT1, h)."}, {"heading": "A. Learning AIO-HMM parameters", "text": "The goal is to maximize the data because the parameters h, which represent the states of the driver, are latent. Therefore, we use the iterative EM method to learn the model parameters. In EM, instead of directly maximizing the equation (15), we maximize the lower limit in the E-step and then maximize this estimate in the M-step. These two steps repeat iteratively. In the E-step, we get the lower limit of the equation (15) by calculating the expected value of the complete data."}], "references": [{"title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "author": ["A. Andreas", "P. Lenz", "R. Urtasun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Constrained local neural fields for robust facial landmark detection in the wild", "author": ["T. Baltrusaitis", "P. Robinson", "L-P. Morency"], "venue": "ICCV Workshop", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "On the expressive power of deep architectures", "author": ["Y. Bengio", "O. Delalleau"], "venue": "Algorithmic Learning Theory, pages 18\u201336", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "An input output hmm architecture", "author": ["Y. Bengio", "O. Frasconi"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning motion patterns of people for compliant robot motion", "author": ["M. Bennewitz", "W. Burgard", "G. Cielniak", "S. Thrun"], "venue": "International Journal of Robotics Research", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Continuous driver intention recognition with hidden markov models", "author": ["H. Berndt", "J. Emmert", "K. Dietmayer"], "venue": "IEEE Intelligent Transportation Systems Conference", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Active appearance models", "author": ["T.F. Cootes", "G.J. Edwards", "C.J. Taylor"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(6)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, 20(3)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "Decision forests for classification", "author": ["A. Criminisi", "J. Shotton", "E. Konukoglu"], "venue": "regression, density estimation, manifold learning and semi-supervised learning. MSR TR, 5(6)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "H", "author": ["Y.N. Dauphin"], "venue": "de Vries, J. Chung, and Y. Bengio. Rmsprop and equilibrated adaptive learning rates for nonconvex optimization. arXiv:1502.04390", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "On-road prediction of driver\u2019s intent with multimodal sensory cues", "author": ["A. Doshi", "B. Morris", "M.M. Trivedi"], "venue": "IEEE Pervasive Computing", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Formalizing assistive teleoperation", "author": ["A. Dragan", "S. Srinivasa"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating legible motion", "author": ["A. Dragan", "S. Srinivasa"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Improved driver modeling for human-in-the-loop vehicular con-  trol", "author": ["K. Driggs-Campbell", "V. Shia", "R. Bajcsy"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical recurrent neural network for skeleton based action recognition", "author": ["Y. Du", "W. Wang", "L. Wang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Vision in and out of vehicles", "author": ["L. Fletcher", "N. Apostoloff", "L. Petersson", "A. Zelinsky"], "venue": "IEEE IS, 18(3)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Correlating driver gaze with the road scene for driver assistance systems", "author": ["L. Fletcher", "G. Loy", "N. Barnes", "A. Zelinsky"], "venue": "Robotics and Autonomous Systems, 52(1)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Will this car change the lane?turn signal recognition in the frequency domain", "author": ["B. Frohlich", "M. Enzweiler", "U. Franke"], "venue": "IEEE International Vehicle Symposium Proceedings", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepspeech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates"], "venue": "arXiv:1412.5567", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["S. El Hihi", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Facial expression analysis for predicting unsafe driving behavior", "author": ["M.E. Jabon", "J.N. Bailenson", "E. Pontikakis", "L. Takayama", "C. Nass"], "venue": "IEEE Pervasive Computing, (4)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Neuralmodels", "author": ["A. Jain"], "venue": "https://github.com/asheshjain399/ NeuralModels", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Beyond geometric path planning: Learning context-driven user preferences via sub-optimal feedback", "author": ["A. Jain", "S. Sharma", "A. Saxena"], "venue": "Proceedings of the International Symposium on Robotics Research", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Planit: A crowdsourcing approach for learning to plan paths from large scale preference feedback", "author": ["A. Jain", "D. Das", "J. Gupta", "A. Saxena"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Car that knows before you do: Anticipating maneuvers via learning temporal driving models", "author": ["A. Jain", "H.S. Koppula", "B. Raghavan", "S. Soh", "A. Saxena"], "venue": "ICCV", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning preferences for manipulation tasks from online coactive feedback", "author": ["A. Jain", "S. Sharma", "T. Joachims", "A. Saxena"], "venue": "International Journal of Robotics Research", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Forwardbackward error: Automatic detection of tracking failures", "author": ["Z. Kalal", "K. Mikolajczyk", "J. Matas"], "venue": "Proceedings of the International Conference on Pattern Recognition", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Activity forecasting", "author": ["K.M. Kitani", "B.D. Ziebart", "J.A. Bagnell", "M. Hebert"], "venue": "In Proceedings of the European Conference on Computer Vision", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Anticipating human activities using object affordances for reactive robotic response", "author": ["H. Koppula", "A. Saxena"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning spatio-temporal  structure from rgb-d videos for human activity detection and anticipation", "author": ["H. Koppula", "A. Saxena"], "venue": "Proceedings of the International Conference on Machine Learning", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Anticipating human activities using object affordances for reactive robotic response", "author": ["H. Koppula", "A. Saxena"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning human activities and object affordances from rgb-d videos", "author": ["H. Koppula", "R. Gupta", "A. Saxena"], "venue": "International Journal of Robotics Research, 32(8)", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Anticipatory planning for humanrobot teams", "author": ["H. Koppula", "A. Jain", "A. Saxena"], "venue": "ISER", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature-based prediction of trajectories for socially compliant navigation", "author": ["M. Kuderer", "H. Kretzschmar", "C. Sprunk", "W. Burgard"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "A driver behavior recognition method based on a driver model framework", "author": ["N. Kuge", "T. Yamamura", "O. Shimoyama", "A. Liu"], "venue": "Technical report, SAE Technical Paper", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning-based approach for online lane change intention prediction", "author": ["P. Kumar", "M. Perrollaz", "S. Lefevre", "C. Laugier"], "venue": "IEEE International Vehicle Symposium Proceedings", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of the International Conference on Machine Learning", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2001}, {"title": "Probabilistic analysis of dynamic scenes and collision risks assessment to improve driving safety", "author": ["C. Laugier", "I.E. Paromtchik", "M. Perrollaz", "MY. Yong", "J-D. Yoder", "C. Tay", "K. Mekhnacha", "A. Negre"], "venue": "ITS Magazine, IEEE, 3(4)", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Driver intent inference at urban intersections using the intelligent driver model", "author": ["M. Liebner", "M. Baumann", "F. Klanner", "C. Stiller"], "venue": "IEEE International Vehicle Symposium Proceedings", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "An iterative image registration technique with an application to stereo vision", "author": ["B. Lucas", "T. Kanade"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1981}, {"title": "Human-robot collaborative manipulation planning using early prediction of human motion", "author": ["J. Mainprice", "D. Berenson"], "venue": "Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Active appearance models revisited", "author": ["I. Matthews", "S. Baker"], "venue": "International Journal of Computer Vision, 60 (2)", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2004}, {"title": "Tell me dave: Context-sensitive grounding of natural language to manipulation instructions", "author": ["D.K. Misra", "J. Sung", "K. Lee", "A. Saxena"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Latent-dynamic discriminative models for continuous gesture recognition", "author": ["L. Morency", "A. Quattoni", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2007}, {"title": "Lane change  intent prediction for driver assistance: On-road design and evaluation", "author": ["B. Morris", "A. Doshi", "M. Trivedi"], "venue": "IEEE International Vehicle Symposium Proceedings", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "MIT press", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "Proceedings of the International Conference on Machine Learning", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2011}, {"title": "Graphical models for driver behavior recognition in a smartcar", "author": ["N. Oliver", "A.P. Pentland"], "venue": "IEEE International Vehicle Symposium Proceedings", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2000}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "arXiv:1211.5063", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2012}, {"title": "Look at the driver", "author": ["M. Rezaei", "R. Klette"], "venue": "look at the road: No distraction! no accident! In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "J", "author": ["T. Rueda-Domingo", "P. Lardelli-Claret"], "venue": "Luna del Castillo, J. Jimenez-Moleon, M. Garcia-Martin, and A. Bueno-Cavanillas. The influence of passengers on the risk of the driver causing a car collision in spain: Analysis of collisions from 1990 to 1999. Accident Analysis & Prevention", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2004}, {"title": "Human activity prediction: Early recognition of ongoing activities from streaming videos", "author": ["M.S. Ryoo"], "venue": "Proceedings of the International Conference on Computer Vision", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2011}, {"title": "Good features to track", "author": ["J. Shi", "C. Tomasi"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1994}, {"title": "Semiautonomous vehicular control using driver modeling", "author": ["V. Shia", "Y. Gao", "R. Vasudevan", "K.D. Campbell", "T. Lin", "F. Borrelli", "R. Bajcsy"], "venue": "IEEE Transactions on Intelligent Transportation Systems, 15(6)", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "A human aware mobile robot motion planner", "author": ["E.A. Sisbot", "L.F. Marin-Urias", "R. Alami", "T. Simeon"], "venue": "IEEE Transactions on Robotics", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2007}, {"title": "Robobarista: Object part-based transfer of manipulation trajectories from crowd-sourcing in 3d pointclouds", "author": ["J. Sung", "S.H. Jin", "A. Saxena"], "venue": "Proceedings of the International Symposium on Robotics Research", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2014}, {"title": "Looking-in and looking-out vision for urban intelligent assistance: Estimation of driver attentive state and dynamic surround for safe merging and braking", "author": ["A. Tawari", "S. Sivaraman", "M. Trivedi", "T. Shannon", "M. Tippelhofer"], "venue": "IEEE IVS", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic robotics", "author": ["S. Thrun", "W. Burgard", "D. Fox"], "venue": "MIT press", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2005}, {"title": "Sparse bayesian learning and the relevance vector machine", "author": ["M.E. Tipping"], "venue": "Journal of Machine Learning Research, 1", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2001}, {"title": "Detection and tracking of point features", "author": ["C. Tomasi", "T. Kanade"], "venue": "International Journal of Computer Vision", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1991}, {"title": "Looking-in and looking-out of a vehicle: Computer-vision-based enhanced vehicle safety", "author": ["M. Trivedi", "T. Gandhi", "J. McCall"], "venue": "IEEE Transactions on Intelligent Transportation Systems, 8(1)", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2007}, {"title": "Safe semi-autonomous control with enhanced driver modeling", "author": ["R. Vasudevan", "V. Shia", "Y. Gao", "R. Cervera-Navarro", "R. Bajcsy", "F. Borrelli"], "venue": "American Control Conference", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M.J. Jones"], "venue": "International Journal of Computer Vision, 57(2)", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2004}, {"title": "Action recognition with improved trajectories", "author": ["H. Wang", "C. Schmid"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2013}, {"title": "Hidden conditional random fields for gesture recognition", "author": ["S.B. Wang", "A. Quattoni", "L. Morency", "D. Demirdjian", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2006}, {"title": "A dynamic conditional random field model for object segmentation in image sequences", "author": ["Y. Wang", "Q. Ji"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2005}, {"title": "Probabilistic movement modeling for intention inference in human-robot interaction", "author": ["Z. Wang", "K. M\u00fclling", "M. Deisenroth", "H. Amor", "D. Vogt", "B. Sch\u00f6lkopf", "J. Peters"], "venue": "International Journal of Robotics Research", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2013}, {"title": "Supervised descent method and its applications to face alignment", "author": ["X. Xiong", "F. De la Torre"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2013}, {"title": "Supervised descent method for solving nonlinear least squares problems in computer vision", "author": ["X. Xiong", "F. De la Torre"], "venue": "arXiv preprint arXiv:1405.0601,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2014}, {"title": "A survey of recent advances in face detection", "author": ["C. Zhang", "Z. Zhang"], "venue": "Technical report, Microsoft Research", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2010}, {"title": "Planning-based prediction for pedestrians", "author": ["B.D. Ziebart", "N. Ratliff", "G. Gallagher", "C. Mertz", "K. Peterson", "J.A. Bagnell", "M. Hebert", "A.K. Dey", "S. Srinivasa"], "venue": "Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 40, "context": ", are successful in alerting drivers whenever they commit a dangerous maneuver [43].", "startOffset": 79, "endOffset": 83}, {"referenceID": 53, "context": "We therefore need mechanisms that can alert drivers before they perform a dangerous maneuver in order to avert many such accidents [56].", "startOffset": 131, "endOffset": 135}, {"referenceID": 19, "context": "Some previous works [22, 41, 50] also predict a driver\u2019s future maneuver.", "startOffset": 20, "endOffset": 32}, {"referenceID": 38, "context": "Some previous works [22, 41, 50] also predict a driver\u2019s future maneuver.", "startOffset": 20, "endOffset": 32}, {"referenceID": 47, "context": "Some previous works [22, 41, 50] also predict a driver\u2019s future maneuver.", "startOffset": 20, "endOffset": 32}, {"referenceID": 36, "context": "Anticipation of the future actions of a human is an important perception task with applications in robotics and computer vision [39, 77, 33, 34, 73].", "startOffset": 128, "endOffset": 148}, {"referenceID": 74, "context": "Anticipation of the future actions of a human is an important perception task with applications in robotics and computer vision [39, 77, 33, 34, 73].", "startOffset": 128, "endOffset": 148}, {"referenceID": 30, "context": "Anticipation of the future actions of a human is an important perception task with applications in robotics and computer vision [39, 77, 33, 34, 73].", "startOffset": 128, "endOffset": 148}, {"referenceID": 31, "context": "Anticipation of the future actions of a human is an important perception task with applications in robotics and computer vision [39, 77, 33, 34, 73].", "startOffset": 128, "endOffset": 148}, {"referenceID": 70, "context": "Anticipation of the future actions of a human is an important perception task with applications in robotics and computer vision [39, 77, 33, 34, 73].", "startOffset": 128, "endOffset": 148}, {"referenceID": 70, "context": "This differentiates anticipation from activity recognition [73], where the complete temporal context is available for prediction.", "startOffset": 59, "endOffset": 63}, {"referenceID": 30, "context": "Previous works on anticipation [33, 34, 39] usually deal with singledata modality and do not address anticipation for sensory-rich robotics applications.", "startOffset": 31, "endOffset": 43}, {"referenceID": 31, "context": "Previous works on anticipation [33, 34, 39] usually deal with singledata modality and do not address anticipation for sensory-rich robotics applications.", "startOffset": 31, "endOffset": 43}, {"referenceID": 36, "context": "Previous works on anticipation [33, 34, 39] usually deal with singledata modality and do not address anticipation for sensory-rich robotics applications.", "startOffset": 31, "endOffset": 43}, {"referenceID": 27, "context": "Additionally, they learn representations using shallow architectures [30, 33, 34, 39] that cannot handle long temporal dependencies [6].", "startOffset": 69, "endOffset": 85}, {"referenceID": 30, "context": "Additionally, they learn representations using shallow architectures [30, 33, 34, 39] that cannot handle long temporal dependencies [6].", "startOffset": 69, "endOffset": 85}, {"referenceID": 31, "context": "Additionally, they learn representations using shallow architectures [30, 33, 34, 39] that cannot handle long temporal dependencies [6].", "startOffset": 69, "endOffset": 85}, {"referenceID": 36, "context": "Additionally, they learn representations using shallow architectures [30, 33, 34, 39] that cannot handle long temporal dependencies [6].", "startOffset": 69, "endOffset": 85}, {"referenceID": 3, "context": "Additionally, they learn representations using shallow architectures [30, 33, 34, 39] that cannot handle long temporal dependencies [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 27, "context": "Images from the data set [30] for a left lane change.", "startOffset": 25, "endOffset": 29}, {"referenceID": 56, "context": "These systems warn drivers when they perform a potentially dangerous maneuver [59, 68].", "startOffset": 78, "endOffset": 86}, {"referenceID": 65, "context": "These systems warn drivers when they perform a potentially dangerous maneuver [59, 68].", "startOffset": 78, "endOffset": 86}, {"referenceID": 18, "context": "Driver monitoring for distraction and drowsiness has also been extensively researched [21, 55].", "startOffset": 86, "endOffset": 94}, {"referenceID": 52, "context": "Driver monitoring for distraction and drowsiness has also been extensively researched [21, 55].", "startOffset": 86, "endOffset": 94}, {"referenceID": 6, "context": "Vehicle trajectory has been used to predict the intent for lane change or turn maneuver [9, 22, 41, 44].", "startOffset": 88, "endOffset": 103}, {"referenceID": 19, "context": "Vehicle trajectory has been used to predict the intent for lane change or turn maneuver [9, 22, 41, 44].", "startOffset": 88, "endOffset": 103}, {"referenceID": 38, "context": "Vehicle trajectory has been used to predict the intent for lane change or turn maneuver [9, 22, 41, 44].", "startOffset": 88, "endOffset": 103}, {"referenceID": 41, "context": "Vehicle trajectory has been used to predict the intent for lane change or turn maneuver [9, 22, 41, 44].", "startOffset": 88, "endOffset": 103}, {"referenceID": 47, "context": "Previous works have addressed maneuver anticipation [1, 50, 15, 67] through sensoryfusion from multiple cameras, GPS, and vehicle dynamics.", "startOffset": 52, "endOffset": 67}, {"referenceID": 12, "context": "Previous works have addressed maneuver anticipation [1, 50, 15, 67] through sensoryfusion from multiple cameras, GPS, and vehicle dynamics.", "startOffset": 52, "endOffset": 67}, {"referenceID": 64, "context": "Previous works have addressed maneuver anticipation [1, 50, 15, 67] through sensoryfusion from multiple cameras, GPS, and vehicle dynamics.", "startOffset": 52, "endOffset": 67}, {"referenceID": 47, "context": "[50] and Trivedi et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "[67] used Relevance Vector Machine (RVM) for intent prediction and performed sensory fusion by concatenating feature vectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Our work is also related to works on driver behavior prediction with different sensors [26, 21, 20], and vehicular controllers which act on these predictions [59, 68, 18].", "startOffset": 87, "endOffset": 99}, {"referenceID": 18, "context": "Our work is also related to works on driver behavior prediction with different sensors [26, 21, 20], and vehicular controllers which act on these predictions [59, 68, 18].", "startOffset": 87, "endOffset": 99}, {"referenceID": 17, "context": "Our work is also related to works on driver behavior prediction with different sensors [26, 21, 20], and vehicular controllers which act on these predictions [59, 68, 18].", "startOffset": 87, "endOffset": 99}, {"referenceID": 56, "context": "Our work is also related to works on driver behavior prediction with different sensors [26, 21, 20], and vehicular controllers which act on these predictions [59, 68, 18].", "startOffset": 158, "endOffset": 170}, {"referenceID": 65, "context": "Our work is also related to works on driver behavior prediction with different sensors [26, 21, 20], and vehicular controllers which act on these predictions [59, 68, 18].", "startOffset": 158, "endOffset": 170}, {"referenceID": 15, "context": "Our work is also related to works on driver behavior prediction with different sensors [26, 21, 20], and vehicular controllers which act on these predictions [59, 68, 18].", "startOffset": 158, "endOffset": 170}, {"referenceID": 70, "context": "Anticipating human activities has shown to improve human-robot collaboration [73, 36, 46, 38, 16].", "startOffset": 77, "endOffset": 97}, {"referenceID": 33, "context": "Anticipating human activities has shown to improve human-robot collaboration [73, 36, 46, 38, 16].", "startOffset": 77, "endOffset": 97}, {"referenceID": 43, "context": "Anticipating human activities has shown to improve human-robot collaboration [73, 36, 46, 38, 16].", "startOffset": 77, "endOffset": 97}, {"referenceID": 35, "context": "Anticipating human activities has shown to improve human-robot collaboration [73, 36, 46, 38, 16].", "startOffset": 77, "endOffset": 97}, {"referenceID": 13, "context": "Anticipating human activities has shown to improve human-robot collaboration [73, 36, 46, 38, 16].", "startOffset": 77, "endOffset": 97}, {"referenceID": 30, "context": "Similarly, forecasting human navigation trajectories has enabled robots to plan sociable trajectories around humans [33, 8, 39, 29].", "startOffset": 116, "endOffset": 131}, {"referenceID": 5, "context": "Similarly, forecasting human navigation trajectories has enabled robots to plan sociable trajectories around humans [33, 8, 39, 29].", "startOffset": 116, "endOffset": 131}, {"referenceID": 36, "context": "Similarly, forecasting human navigation trajectories has enabled robots to plan sociable trajectories around humans [33, 8, 39, 29].", "startOffset": 116, "endOffset": 131}, {"referenceID": 26, "context": "Similarly, forecasting human navigation trajectories has enabled robots to plan sociable trajectories around humans [33, 8, 39, 29].", "startOffset": 116, "endOffset": 131}, {"referenceID": 54, "context": "Feature matching techniques have been proposed for anticipating human activities from videos [57].", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "Modeling human preferences has enabled robots to plan good trajectories [17, 60, 28, 31].", "startOffset": 72, "endOffset": 88}, {"referenceID": 57, "context": "Modeling human preferences has enabled robots to plan good trajectories [17, 60, 28, 31].", "startOffset": 72, "endOffset": 88}, {"referenceID": 25, "context": "Modeling human preferences has enabled robots to plan good trajectories [17, 60, 28, 31].", "startOffset": 72, "endOffset": 88}, {"referenceID": 28, "context": "Modeling human preferences has enabled robots to plan good trajectories [17, 60, 28, 31].", "startOffset": 72, "endOffset": 88}, {"referenceID": 3, "context": "Such interactions are absent in the previous works, and they use shallow architectures [6] that do not properly model temporal aspects of human activities.", "startOffset": 87, "endOffset": 90}, {"referenceID": 66, "context": "The vision approaches related to our work are face detection and tracking [69, 76], statistical models of face [10] and pose estimation methods for face [75].", "startOffset": 74, "endOffset": 82}, {"referenceID": 73, "context": "The vision approaches related to our work are face detection and tracking [69, 76], statistical models of face [10] and pose estimation methods for face [75].", "startOffset": 74, "endOffset": 82}, {"referenceID": 7, "context": "The vision approaches related to our work are face detection and tracking [69, 76], statistical models of face [10] and pose estimation methods for face [75].", "startOffset": 111, "endOffset": 115}, {"referenceID": 72, "context": "The vision approaches related to our work are face detection and tracking [69, 76], statistical models of face [10] and pose estimation methods for face [75].", "startOffset": 153, "endOffset": 157}, {"referenceID": 7, "context": "Active Appearance Model (AAM) [10] and its variants [47, 74] statistically model the shape and texture of the face.", "startOffset": 30, "endOffset": 34}, {"referenceID": 44, "context": "Active Appearance Model (AAM) [10] and its variants [47, 74] statistically model the shape and texture of the face.", "startOffset": 52, "endOffset": 60}, {"referenceID": 71, "context": "Active Appearance Model (AAM) [10] and its variants [47, 74] statistically model the shape and texture of the face.", "startOffset": 52, "endOffset": 60}, {"referenceID": 72, "context": "AAMs have also been used to estimate the 3D-pose of a face from a single image [75] and in design of assistive features for driver monitoring [55, 63].", "startOffset": 79, "endOffset": 83}, {"referenceID": 52, "context": "AAMs have also been used to estimate the 3D-pose of a face from a single image [75] and in design of assistive features for driver monitoring [55, 63].", "startOffset": 142, "endOffset": 150}, {"referenceID": 60, "context": "AAMs have also been used to estimate the 3D-pose of a face from a single image [75] and in design of assistive features for driver monitoring [55, 63].", "startOffset": 142, "endOffset": 150}, {"referenceID": 66, "context": "In our approach we adapt off-theshelf available face detection [69] and tracking algorithms [58] (see Section VI).", "startOffset": 63, "endOffset": 67}, {"referenceID": 55, "context": "In our approach we adapt off-theshelf available face detection [69] and tracking algorithms [58] (see Section VI).", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "We demonstrate this by using the Constrained Local Neural Field (CLNF) model [4] and tracking 68 fixed landmark points on the driver\u2019s face and estimating the 3D head-pose.", "startOffset": 77, "endOffset": 80}, {"referenceID": 32, "context": "Temporal models are commonly used to model human activities [35, 49, 71, 72].", "startOffset": 60, "endOffset": 76}, {"referenceID": 46, "context": "Temporal models are commonly used to model human activities [35, 49, 71, 72].", "startOffset": 60, "endOffset": 76}, {"referenceID": 68, "context": "Temporal models are commonly used to model human activities [35, 49, 71, 72].", "startOffset": 60, "endOffset": 76}, {"referenceID": 69, "context": "Temporal models are commonly used to model human activities [35, 49, 71, 72].", "startOffset": 60, "endOffset": 76}, {"referenceID": 39, "context": "The discriminative temporal models are mostly inspired by the Conditional Random Field (CRF) [42] which captures the temporal structure of the problem.", "startOffset": 93, "endOffset": 97}, {"referenceID": 69, "context": "[72] and Morency et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[49] propose dynamic extensions of the CRF for image segmentation and gesture recognition respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "On the other hand, generative approaches for temporal modeling include various filtering methods, such as Kalman and particle filters [64], Hidden Markov Models, and many types of Dynamic Bayesian Networks [51].", "startOffset": 134, "endOffset": 138}, {"referenceID": 48, "context": "On the other hand, generative approaches for temporal modeling include various filtering methods, such as Kalman and particle filters [64], Hidden Markov Models, and many types of Dynamic Bayesian Networks [51].", "startOffset": 206, "endOffset": 210}, {"referenceID": 6, "context": "Some previous works [9, 40, 53] used HMMs to model different aspects of the driver\u2019s behaviour.", "startOffset": 20, "endOffset": 31}, {"referenceID": 37, "context": "Some previous works [9, 40, 53] used HMMs to model different aspects of the driver\u2019s behaviour.", "startOffset": 20, "endOffset": 31}, {"referenceID": 50, "context": "Some previous works [9, 40, 53] used HMMs to model different aspects of the driver\u2019s behaviour.", "startOffset": 20, "endOffset": 31}, {"referenceID": 27, "context": "In the following sections, we will describe the Autoregressive Input-Output HMM (AIO-HMM) for maneuver anticipation [30] and will use it as a baseline to compare our deep learning approach.", "startOffset": 116, "endOffset": 120}, {"referenceID": 21, "context": "Unlike AIO-HMM our deep architecture have internal memory which allows it to handle long temporal dependencies [24].", "startOffset": 111, "endOffset": 115}, {"referenceID": 51, "context": "Two building blocks of our architecture are Recurrent Neural Networks (RNNs) [54] and Long Short-Term Memory (LSTM) units [25].", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "Two building blocks of our architecture are Recurrent Neural Networks (RNNs) [54] and Long Short-Term Memory (LSTM) units [25].", "startOffset": 122, "endOffset": 126}, {"referenceID": 59, "context": "Our work draws upon ideas from previous works on RNNs and LSTM from the language [62], speech [23], and vision [14] communities.", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "Our work draws upon ideas from previous works on RNNs and LSTM from the language [62], speech [23], and vision [14] communities.", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "Our work draws upon ideas from previous works on RNNs and LSTM from the language [62], speech [23], and vision [14] communities.", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "Our approach to the joint training of multiple RNNs is related to the recent work on hierarchical RNNs [19].", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "We consider RNNs in multi-modal setting, which is related to the recent use of RNNs in imagecaptioning [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 49, "context": "We focus on sensory-rich robotics applications, and our architecture extends previous works doing sensoryfusion with feed-forward networks [52, 61] to the fusion of temporal streams.", "startOffset": 139, "endOffset": 147}, {"referenceID": 58, "context": "We focus on sensory-rich robotics applications, and our architecture extends previous works doing sensoryfusion with feed-forward networks [52, 61] to the fusion of temporal streams.", "startOffset": 139, "endOffset": 147}, {"referenceID": 67, "context": "This differentiates anticipation from activity recognition [70, 37] where in the latter the complete observation sequence is available at test time.", "startOffset": 59, "endOffset": 67}, {"referenceID": 34, "context": "This differentiates anticipation from activity recognition [70, 37] where in the latter the complete observation sequence is available at test time.", "startOffset": 59, "endOffset": 67}, {"referenceID": 22, "context": "In this work we propose a deep RNN architecture with Long Short-Term Memory (LSTM) units [25] for anticipation.", "startOffset": 89, "endOffset": 93}, {"referenceID": 51, "context": "A standard RNN [54] takes in a temporal sequence of vectors (x1,x2, .", "startOffset": 15, "endOffset": 19}, {"referenceID": 51, "context": "RNNs with this choice of f suffer from a well-studied problem of vanishing gradients [54], and hence are poor at capturing long temporal dependencies which are essential for anticipation.", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "A common remedy to vanishing gradients is to replace tanh non-linearities by Long Short-Term Memory cells [25].", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "LSTM is a network of neurons that implements a memory cell [25].", "startOffset": 59, "endOffset": 63}, {"referenceID": 31, "context": "Previous works treat anticipation as a recognition problem [34, 50, 57] and train discriminative classifiers (such as SVM or CRF) on the complete temporal context.", "startOffset": 59, "endOffset": 71}, {"referenceID": 47, "context": "Previous works treat anticipation as a recognition problem [34, 50, 57] and train discriminative classifiers (such as SVM or CRF) on the complete temporal context.", "startOffset": 59, "endOffset": 71}, {"referenceID": 54, "context": "Previous works treat anticipation as a recognition problem [34, 50, 57] and train discriminative classifiers (such as SVM or CRF) on the complete temporal context.", "startOffset": 59, "endOffset": 71}, {"referenceID": 0, "context": "Examples include autonomous vehicles that reason from multiple sensors [3] or robots that jointly reason over perception and language instructions [48].", "startOffset": 71, "endOffset": 74}, {"referenceID": 45, "context": "Examples include autonomous vehicles that reason from multiple sensors [3] or robots that jointly reason over perception and language instructions [48].", "startOffset": 147, "endOffset": 151}, {"referenceID": 66, "context": "trained Viola-Jones face detector [69].", "startOffset": 34, "endOffset": 38}, {"referenceID": 55, "context": "From the detected face, we first extract visually discriminative (facial) points using the Shi-Tomasi corner detector [58] and then track those facial points using the Kanade-Lucas-Tomasi (KLT) tracker [45, 58, 66].", "startOffset": 118, "endOffset": 122}, {"referenceID": 42, "context": "From the detected face, we first extract visually discriminative (facial) points using the Shi-Tomasi corner detector [58] and then track those facial points using the Kanade-Lucas-Tomasi (KLT) tracker [45, 58, 66].", "startOffset": 202, "endOffset": 214}, {"referenceID": 55, "context": "From the detected face, we first extract visually discriminative (facial) points using the Shi-Tomasi corner detector [58] and then track those facial points using the Kanade-Lucas-Tomasi (KLT) tracker [45, 58, 66].", "startOffset": 202, "endOffset": 214}, {"referenceID": 63, "context": "From the detected face, we first extract visually discriminative (facial) points using the Shi-Tomasi corner detector [58] and then track those facial points using the Kanade-Lucas-Tomasi (KLT) tracker [45, 58, 66].", "startOffset": 202, "endOffset": 214}, {"referenceID": 29, "context": "To address this problem, we re-initialize the tracker with new discriminative facial points once the number of tracked points falls below a threshold [32].", "startOffset": 150, "endOffset": 154}, {"referenceID": 1, "context": "We track facial landmark points using the CLNF tracker [4] which results in more consistent 2D trajectories as compared to the KLT tracker [58] used by Jain et al.", "startOffset": 55, "endOffset": 58}, {"referenceID": 55, "context": "We track facial landmark points using the CLNF tracker [4] which results in more consistent 2D trajectories as compared to the KLT tracker [58] used by Jain et al.", "startOffset": 139, "endOffset": 143}, {"referenceID": 27, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "For example we replace the KLT tracker described above with the Constrained Local Neural Field (CLNF) model [4] and track 68 fixed landmark points on the driver\u2019s face.", "startOffset": 108, "endOffset": 111}, {"referenceID": 27, "context": "In this section we propose alternate Bayesian networks [30] based on Hidden Markov Model (HMM) for maneuver anticipation.", "startOffset": 55, "endOffset": 59}, {"referenceID": 62, "context": "Discriminative methods, such as the Support Vector Machine and the Relevance Vector Machine [65], which do not model the temporal aspect perform poorly on anticipation tasks, as we show in Section VIII.", "startOffset": 92, "endOffset": 96}, {"referenceID": 4, "context": "Such interactions can be modeled by an Input-Output HMM (IOHMM) [7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 47, "context": "\u2022 SVM [50]: Support Vector Machine is a discriminative classifier [11].", "startOffset": 6, "endOffset": 10}, {"referenceID": 8, "context": "\u2022 SVM [50]: Support Vector Machine is a discriminative classifier [11].", "startOffset": 66, "endOffset": 70}, {"referenceID": 47, "context": "[50] takes this approach for anticipating maneuvers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "\u2022 Random-Forest [12]: This is also a discriminative classifier that learns many decision trees from the training data, and at test time it averages the prediction of the individual decision trees.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "[30] modeled driving maneuvers with this Bayesian network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[30] proposed this Bayesian net-", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[50] considered binary classification problem (lane change vs driving straight) and used RVM [65].", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[50] considered binary classification problem (lane change vs driving straight) and used RVM [65].", "startOffset": 93, "endOffset": 97}, {"referenceID": 24, "context": "Our RNN and LSTM implementations are open-sourced and available at NeuralModels [27].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "Our overall architecture (F-RNN-EL and F-RNN-UL) have nearly 25,000 parameters that are learned using RMSprop [13].", "startOffset": 110, "endOffset": 114}, {"referenceID": 27, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[50] SVM 73.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[50] SVM 15.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "We use [4] to track 68 facial landmark points and estimate 3D head-pose.", "startOffset": 7, "endOffset": 10}, {"referenceID": 27, "context": "We replace the pipeline for extracting features from the driver\u2019s face [30] by a Constrained Local Neural Field (CLNF) model [4].", "startOffset": 71, "endOffset": 75}, {"referenceID": 1, "context": "We replace the pipeline for extracting features from the driver\u2019s face [30] by a Constrained Local Neural Field (CLNF) model [4].", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "20 milliseconds using Theano [5] on Nvidia K40 GPU on Ubuntu 12.", "startOffset": 29, "endOffset": 32}, {"referenceID": 48, "context": "We efficiently estimate Q(\u0398; \u0398\u0302) using the forward-backward algorithm [51].", "startOffset": 70, "endOffset": 74}, {"referenceID": 48, "context": "The inference in equation (20) simply requires a forwardpass [51] of the AIO-HMM, the complexity of which is O(T (|S| + |S||z| + |S||x|)).", "startOffset": 61, "endOffset": 65}, {"referenceID": 66, "context": "input Driving videos, GPS, Maps and Vehicle Dynamics output Probability of each maneuver Initialize the face tracker with the driver\u2019s face while driving do Track the driver\u2019s face [69] Extract features z1 and x T 1 (Sec.", "startOffset": 181, "endOffset": 185}], "year": 2016, "abstractText": "Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for unsafe road conditions and alert drivers if they perform a dangerous maneuver. However, many accidents are unavoidable because by the time drivers are alerted, it is already too late. Anticipating maneuvers beforehand can alert drivers before they perform the maneuver and also give ADAS more time to avoid or prepare for the danger. In this work we propose a vehicular sensor-rich platform and learning algorithms for maneuver anticipation. For this purpose we equip a car with cameras, Global Positioning System (GPS), and a computing device to capture the driving context from both inside and outside of the car. In order to anticipate maneuvers, we propose a sensory-fusion deep learning architecture which jointly learns to anticipate and fuse multiple sensory streams. Our architecture consists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM) units to capture long temporal dependencies. We propose a novel training procedure which allows the network to predict the future given only a partial temporal context. We introduce a diverse data set with 1180 miles of natural freeway and city driving, and show that we can anticipate maneuvers 3.5 seconds before they occur in realtime with a precision and recall of 90.5% and 87.4% respectively.", "creator": "LaTeX with hyperref package"}}}