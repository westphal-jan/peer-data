{"id": "1601.00741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "Learning Preferences for Manipulation Tasks from Online Coactive Feedback", "abstract": "We consider the problem of learning preferences over trajectories for mobile manipulators such as personal robots and assembly line robots. The preferences we learn are more intricate than simple geometric constraints on trajectories; they are rather governed by the surrounding context of various objects and human interactions in the environment. We propose a coactive online learning framework for teaching preferences in contextually rich environments. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this coactive preference feedback can be more easily elicited than demonstrations of optimal trajectories. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms.", "histories": [["v1", "Tue, 5 Jan 2016 05:47:09 GMT  (16943kb,D)", "http://arxiv.org/abs/1601.00741v1", "IJRR accepted (Learning preferences over trajectories from coactive feedback)"]], "COMMENTS": "IJRR accepted (Learning preferences over trajectories from coactive feedback)", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["ashesh jain", "shikhar sharma", "thorsten joachims", "ashutosh saxena"], "accepted": false, "id": "1601.00741"}, "pdf": {"name": "1601.00741.pdf", "metadata": {"source": "CRF", "title": "Learning Preferences for Manipulation Tasks from Online Coactive Feedback", "authors": ["Ashesh Jain", "Shikhar Sharma", "Thorsten Joachims", "Ashutosh Saxena"], "emails": ["asaxena}@cs.cornell.edu"], "sections": [{"heading": null, "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "II. RELATED WORK", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "III. COACTIVE LEARNING WITH INCREMENTAL FEEDBACK", "text": "We first give an overview of our robot learning system and then describe in detail three mechanisms of user feedback."}, {"heading": "A. Robot learning setup", "text": "We propose an online algorithm for learning trajectory preferences from sub-optimal user feedback. At each step, the robot receives an input task and outputs a trajectory that maximizes its current estimate of a score function. Then, it observes user feedback - an improved trajectory - and updates the score function to better match user preferences. This method of learning by iterative improvement is known as coactive learning. We implement the algorithm on PR2 and Baxter robots, both of which have two 7 DoF arms. In the training process, the initial trajectory proposed by the robot can be far from the desired behavior, so instead of performing trajectories directly in a human environment, users visualize them first in the OpenRAVE simulator [13] and then decide what kind of feedback they want to provide."}, {"heading": "B. Feedback mechanisms", "text": "It is the first time that a human being is able to move. (It is the second time that a human being is able to move.) It is the second time that a human being is able to move. (It is the first time that a human being is able to move.) It is the second time that a human being is able to move. (It is the second time that a human being is able to move.) It is the third time that a human being is able to move. (It is the first time that a human being is able to move.)"}, {"heading": "IV. LEARNING AND FEEDBACK MODEL", "text": "We model the learning problem in the following ways. \u2022 Step 1: The robot receives a Context > Scoring that describes the environment, objects, and any other input relevant to the problem. \u2022 The robot must find out what a good scoring function is for that context. \u2022 Formally, we assume that the user has a scoring function that reflects how much he rates each scoring function y for the context x. The higher the scoring function, the better the trajectory of the robot. \u2022 Step 1: The goal of the robot is to learn a function (x, y; w) (where w are the parameters to be learned) that resembles the actual scoring function of the user. \u2022 Step 1: The goal of the robot is to learn a function (x, y; w) (where w are the parameters to be learned)."}, {"heading": "V. LEARNING ALGORITHM", "text": "For each task, we model the scoring function of the user s * (x, y) with the following parameterized family of functions. s (x, y; w) = w \u00b7 \u03c6 (x, y) (1) w is a weight vector that needs to be learned, and \u03c6 (\u00b7) are features that describe the trajectory y for the context x. such a linear representation of scoring functions has previously been used to generate desired robot behavior [1, 44, 62]. We further split the scoring function into two parts, one referring only to the objects with which the trajectory interacts, and the other to manipulate the object and the environments (x, y; wO, wE) = sO (x, y; wO) + sE (x, y; wE) = wO \u00b7 throuO (x, y) + wE ()."}, {"heading": "A. Features Describing Object-Object Interactions", "text": "This attribute captures the interaction between objects in the vicinity with the object being manipulated. We count q q = q waypoints of the path y as y1,.., yN and objects in the vicinity as O = {o1,.., oK}. The robot manipulates the object o = q O. Some waypoints of the path would be affected by the other objects in the vicinity. For example, in Figure 5, o1 and o2, the waypoint y3 is affected by the proximity. Specifically, we associate an object ok with a waypoint if the minimum distance to the collision is less than a threshold or if ok is below o.The edge connecting yj and ok is called (yj, ok).E. Since these are the attributes [31] of the object that really matter in determining the quality of the path, we represent each object with its attributes. Specifically, we consider a vector of the M binary variables [lk, 1k, l] with each object, i.e. we can actually possess an object with any property."}, {"heading": "B. Trajectory Features", "text": "We describe the characteristics we have made our own, as if they were changing in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move, in the way we move ourselves, in the way we move ourselves, in the way we move ourselves. \""}, {"heading": "C. Computing Trajectory Rankings", "text": "However, in order to obtain the top trajectory (or a top pair RT) for a specific task with context x, we would like to maximize the current scoring function s (x, y; wO, wE).y \u043a = arg max y s (x, y; wO, wE). (4) Secondly, for a particular set {y (1),.., y (n) of discrete trajectories, we need to calculate (4). Fortunately, the latter problem is easy to solve and boils down to simply sorting the trajectories by their trajectory values (x, y (i); wO, wE).Two effective methods for solving the former problem are either discrediting the state space [3, 7, 59] or directly scanning the trajectories from continuous space [6, 12].Previously, both approaches have been investigated."}, {"heading": "D. Learning the Scoring Function", "text": "The goal is to learn the parameters wO and wE of the scoring function s (x, y; wO, wE) so that it can be used to evaluate the trajectories according to the user's preferences. 5If the RRT becomes too slow, we switch to more efficient bidirectional feedback. The cost function (or its approximation) we learn can be transferred to trajectory optimizers like CHOMP [47] or optimal planners like RRT * [26] to produce reasonably good trajectories. We adapt the Preference Perceptron algorithm [51] as detailed in Algorithm 1, and we call it the Trajectory Preference Preceptron (TPP)."}, {"heading": "VI. EXPERIMENTS AND RESULTS", "text": "We first describe our experimental setup, then present quantitative results (section VI-B) and then present robot-assisted experiments on PR2 and Baxter (section VI-D)."}, {"heading": "A. Experimental Setup", "text": "This year is the highest in the history of the country."}, {"heading": "B. Results and Discussion", "text": "We present quantitative results where we compare TPP against the baseline algorithms on our dataset of marked trajectories. How well does TPP generalize to new tasks? To study the generalization of preference feedback, we therefore evaluate the performance of TPP pre-trained (i.e., TPP algorithm under pre-trained setting) to a number of tasks that the algorithm has not yet seen. We study generalizations when (a) only the object is manipulated that is replaced by a cup or egg carton that is replaced by tomatoes, (b) only consider the surrounding environmental changes, e.g. the reordering of objects in the environment or the change of the starting location of tasks, and when both change. Figure 9 shows that tasks are averaged for all types of activities for both households and grocery shops.6 TPP pre-trained start-off with higher nDCG values than TPP-untrained in all three cases."}, {"heading": "C. Comparison with fully-supervised algorithms", "text": "In this section, we compare TPP to a fully monitored algorithm that monitors the labels of the experts during the training. In practice, the use of such labels in the large space of trajectories is not feasible, but empirically, it nevertheless provides an upper limit for generalizing new tasks. We refer to this algorithm as Oracle-svm and it learns to evaluate the trajectories using the SVM ranking [24]. Since expert labels are not available during the prediction, Oraclesvm predicts once in the test kit and does not learn from the feedback. Figure 11 compares TPP and Oracle-svm for new tasks. Without feedback on new tasks, Oracle-svm labels are better than Oracle-svm, but after a few feedback iterations, TPP improves feedback over Oracle-svm, which is not updated, as it can improve the expert labels for an average of 5."}, {"heading": "D. Robotic Experiment: User Study in learning trajectories", "text": "In fact, most of them will be able to move to another world, where they can move to another world, where they can find their way to another world."}, {"heading": "VII. CONCLUSION AND FUTURE WORK", "text": "When manipulating objects in human environments, it is important for robots to plan movements that follow users \"preferences. In this work, we took into account preferences that go beyond simple geometric constraints and take into account the context of different objects and people in the environment. We presented a coactive learning approach to teach robots these preferences through iterative improvements by non-knowledgeable users. Unlike traditional learning from demonstration approaches, our approach does not require the user to provide optimal trajectories as training data. We evaluated our approach for different situations in the home (with PR2) and at the checkout of grocery stores (with Baxter). Our experiments suggest that it is actually possible to train robots within minutes with only a few incremental feedback from non-knowledgeable users. Future research could extend coactive learning to situations with uncertainty in object positions and attributes."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This research was supported by the ARO Prize W911NF-121-0267, the Microsoft Faculty Fellowship and the NSF Career Award (to Saxena)."}, {"heading": "APPENDIX A PROOF FOR AVERAGE REGRET", "text": "(D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D (D): (D): (D): (D): (D): (D): (D (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D): (D (D): (D): (D): (D (D): (D): (D (D): (D): (D): (D): (D (D): (D): (D): (D (D): (D): (D): (D (D): (D): (D): (D: (D): (D): (D: (D): (D): (D: (D): (D): (D): (D): (D)"}, {"heading": "APPENDIX B PROOF FOR EXPECTED REGRET", "text": "We now show the limits of regret for the TPP under a weaker feedback assumption - expected feedback between both sides of the equation: Et [s (xt, y), w), w (E), w (T), w (E), w (E), w (T), w (E), w (T), w (T), w (T), w (E), w (E), w (T), w (E), w (E), w (T), w (T), w (T), w (T), w (T), w (T), w (W), w (E), w (E), w (T), w (T), w (T), w (T), w (T), w (T), w (T), w (T), w (T), w, w (T), w (T), w (T), w (T), w (T), w (T), w (T), w, w (T), w (T), w (T), w (T), w (T, w (T), w (T), w (T), w (T (T), w (T), w (T), w (T (T), w (T), w (T), w (T (T), w (T), w (T (T), w (T), w (T (T), w (T), w (T (T), w (T), w (T (T), w (T), w (T (T), w (T), w (T (T), w (T), w (T (T), w (T (T), w (T), w (T (T), w (T (T), w (T), w (T (T), w (T), w (T), w (T), w (T (T), w (T), w (T), w (T (T), w (T), w (T), w (T), w (T (T), w (T)"}], "references": [{"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["P. Abbeel", "A. Coates", "A.Y. Ng"], "venue": "International Journal of Robotics Research, 29(13)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Keyframe-based learning from demonstration", "author": ["B. Akgun", "M. Cakmak", "K. Jiang", "A.L. Thomaz"], "venue": "International Journal of Social Robotics, 4(4):343\u2013355", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "The stochastic motion roadmap: A sampling framework for planning with markov motion uncertainty", "author": ["R. Alterovitz", "T. Sim\u00e9on", "K. Goldberg"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and Autonomous Systems, 57(5):469\u2013483", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A robot path planning framework that learns from experience", "author": ["D. Berenson", "P. Abbeel", "K. Goldberg"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Lqg-mp: Optimized path planning for robots with motion uncertainty and imperfect state information", "author": ["J.V.D. Berg", "P. Abbeel", "K. Goldberg"], "venue": "In Proceedings of Robotics: Science and Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Identification and representation of homotopy classes of trajectories for search-based path planning in 3d", "author": ["S. Bhattacharya", "M. Likhachev", "V. Kumar"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "The morpha style guide for icon-based programming", "author": ["R. Bischoff", "A. Kazi", "M. Seyfarth"], "venue": "Proceedings. 11th IEEE International Workshop on RHIC.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "On learning", "author": ["S. Calinon", "F. Guenter", "A. Billard"], "venue": "representing, and generalizing a task in a humanoid robot. Sys., Man, and Cybernetics, Part B: Cybernetics, IEEE Trans. on", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Search-based planning for manipulation with motion primitives", "author": ["B.J. Cohen", "S. Chitta", "M. Likhachev"], "venue": "Proceedings of the International Conference on Robotics and Automation, pages 2902\u20132908", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Anytime rrts", "author": ["Dave D. Ferguson", "A. Stentz"], "venue": "In Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Contextual sequence prediction with application to control library optimization", "author": ["D. Dey", "T.Y. Liu", "M. Hebert", "J.A. Bagnell"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Automated Construction of Robotic Manipulation Programs", "author": ["R. Diankov"], "venue": "PhD thesis, CMU,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Generating legible motion", "author": ["A. Dragan", "S. Srinivasa"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Legibility and predictability of robot motion", "author": ["A. Dragan", "K. Lee", "S. Srinivasa"], "venue": "Human Robot Interaction", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Survivability: Measuring and ensuring path diversity", "author": ["L.H. Erickson", "S.M. LaValle"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Randomised rough-terrain robot motion planning", "author": ["A. Ettlin", "H. Bleuler"], "venue": "Proceedings of the IEEE/RSJ  Conference on Intelligent Robots and Systems", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "A", "author": ["D. Gossow"], "venue": "Leeperand D. Hershberger, and M. Ciocarlie. Interactive markers: 3-d user interfaces for ros applications [ros topics]. Robotics & Automation Magazine, IEEE, 18(4):14\u201315", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Toward optimal sampling in the space of paths", "author": ["C.J. Green", "A. Kelly"], "venue": "In Robotics Research", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Sampling-based path planning on configuration-space costmaps", "author": ["L. Jaillet", "J. Cort\u00e9s", "T. Sim\u00e9on"], "venue": "26(4)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Beyond geometric path planning: Learning context-driven user preferences via sub-optimal feedback", "author": ["A. Jain", "S. Sharma", "A. Saxena"], "venue": "Proceedings of the International Symposium on Robotics Research", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning trajectory preferences for manipulators via iterative improvement", "author": ["A. Jain", "B. Wojcik", "T. Joachims", "A. Saxena"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to place new objects in a scene", "author": ["Y. Jiang", "M. Lim", "C. Zheng", "A. Saxena"], "venue": "International Journal of Robotics Research, 31(9):1021\u20131043", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the ACM Special Interest Group on Knowledge Discovery and Data Mining", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Cutting-plane training of structural svms", "author": ["T. Joachims", "T. Finley", "C.-N.J. Yu"], "venue": "Machine Learning, 77(1): 27\u201359", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Incremental sampling-based algorithms for optimal motion planning", "author": ["S. Karaman", "E. Frazzoli"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Sampling-based algorithms for optimal motion planning", "author": ["S. Karaman", "E. Frazzoli"], "venue": "International Journal of Robotics Research, 30(7):846\u2013894", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Activity forecasting", "author": ["K.M. Kitani", "B.D. Ziebart", "J.A. Bagnell", "M. Hebert"], "venue": "In Proceedings of the European Conference on Computer Vision", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Grasping with application to an autonomous checkout robot", "author": ["E. Klingbeil", "D. Rao", "B. Carpenter", "V. Ganapathi", "A.Y. Ng", "O. Khatib"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Policy search for motor primitives in robotics", "author": ["J. Kober", "J. Peters"], "venue": "ML, 84(1)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Semantic labeling of 3d point clouds for indoor scenes", "author": ["H.S. Koppula", "A. Anand", "T. Joachims", "A. Saxena"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to predict trajectories of cooperatively navigating agents", "author": ["H. Kretzschmar", "M. Kuderer", "W. Burgard"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Randomized kinodynamic planning", "author": ["S.M. LaValle", "J.J. Kuffner"], "venue": "International Journal of Robotics Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "Deep learning for detecting robotic grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Using manipulability to  bias sampling during the construction of probabilistic roadmaps", "author": ["P. Leven", "S. Hutchinson"], "venue": "IEEE Trans. on Robotics and Automation, 19(6)", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2003}, {"title": "Continuous inverse optimal control with locally optimal examples", "author": ["S. Levine", "V. Koltun"], "venue": "Proceedings of the International Conference on Machine Learning", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Planning human-aware motions using a sampling-based costmap planner", "author": ["J. Mainprice", "E.A. Sisbot", "L. Jaillet", "J. Cort\u00e9s", "R. Alami", "T. Sim\u00e9on"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "volume 1. Cambridge University Press Cambridge", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Human-robot teaming using shared mental models", "author": ["S. Nikolaidis", "J. Shah"], "venue": "HRI, Workshop on Human- Agent-Robot Teamwork", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Human-robot cross-training: Computational formulation", "author": ["S. Nikolaidis", "J. Shah"], "venue": "modeling and evaluation of a human team training strategy. In IEEE/ACM ICHRI", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Egraphs: Bootstrapping planning with experience graphs", "author": ["M. Phillips", "B. Cohen", "S. Chitta", "M. Likhachev"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning socially optimal information systems from egoistic users", "author": ["K. Raman", "T. Joachims"], "venue": "Proceedings of the European Conference on Machine Learning", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to Search: Structured Prediction Techniques for Imitation Learning", "author": ["N. Ratliff"], "venue": "PhD thesis, CMU, RI", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Maximum margin planning", "author": ["N. Ratliff", "J.A. Bagnell", "M. Zinkevich"], "venue": "Proceedings of the International Conference on Machine Learning", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "online) subgradient methods for structured prediction", "author": ["N. Ratliff", "J.A. Bagnell", "M. Zinkevich"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["N. Ratliff", "D. Silver", "J.A. Bagnell"], "venue": "Autonomous Robots, 27(1):25\u201353", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Chomp: Gradient optimization techniques for efficient motion planning", "author": ["N. Ratliff", "M. Zucker", "J.A. Bagnell", "S. Srinivasa"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning policies for contextual submodular prediction", "author": ["S. Ross", "J. Zhou", "Y. Yue", "D. Dey", "J.A. Bagnell"], "venue": "Proceedings of the International Conference on Machine Learning", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Robotic grasping of novel objects using vision", "author": ["A. Saxena", "J. Driemeyer", "A.Y. Ng"], "venue": "International Journal of Robotics Research, 27(2):157\u2013173", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2008}, {"title": "Finding locally optimal", "author": ["J. Schulman", "J. Ho", "A. Lee", "I. Awwal", "H. Bradlow", "P. Abbeel"], "venue": "collision-free trajectories with sequential convex optimization. In Proceedings of Robotics: Science and Systems", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Online structured  prediction via coactive learning", "author": ["P. Shivaswamy", "T. Joachims"], "venue": "Proceedings of the International Conference on Machine Learning", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "Designing The User Interface: Strategies for Effective Human-Computer Interaction", "author": ["B. Shneiderman", "C. Plaisant"], "venue": "Addison-Wesley Publication", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning from demonstration for autonomous navigation in complex unstructured terrain", "author": ["D. Silver", "J.A. Bagnell", "A. Stentz"], "venue": "International Journal of Robotics Research", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2010}, {"title": "Spatial reasoning for human robot interaction", "author": ["E.A. Sisbot", "L.F. Marin", "R. Alami"], "venue": "Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2007}, {"title": "A human aware mobile robot motion planner", "author": ["E.A. Sisbot", "L.F. Marin-Urias", "R. Alami", "T. Simeon"], "venue": "IEEE Transactions on Robotics", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards interactive learning for manufacturing assistants", "author": ["A. Stopp", "S. Horstmann", "S. Kristensen", "F. Lohnert"], "venue": "Proceedings. 10th IEEE International Workshop on RHIC.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2001}, {"title": "The Open Motion Planning Library", "author": ["I.A. Sucan", "M. Moll", "L.E. Kavraki"], "venue": "IEEE Robotics & Automation Magazine, 19(4):72\u201382", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2012}, {"title": "Synthesizing object receiving motions of humanoid robots with human motion database", "author": ["K. Tamane", "M. Revfi", "T. Asfour"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient high dimensional maximum entropy modeling via symmetric partition functions", "author": ["P. Vernaza", "J.A. Bagnell"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2012}, {"title": "A bayesian approach for policy learning from trajectory preference queries", "author": ["A. Wilson", "A. Fern", "P. Tadepalli"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2012}, {"title": "Making planned paths look more human-like in humanoid robot manipulation planning", "author": ["F. Zacharias", "C. Schlette", "F. Schmidt", "C. Borst", "J. Rossmann", "G. Hirzinger"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B.D. Ziebart", "A. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "AAAI", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2008}, {"title": "and S", "author": ["M. Zucker", "N. Ratliff", "A.D. Dragan", "M. Pivtoraiko", "M. Klingensmith", "C.M. Dellin", "J.A. Bagnell"], "venue": ".S. Srinivasa. Chomp: Covariant hamiltonian optimization for motion planning. International Journal of Robotics Research, 32", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": "1Parts of this work has been published at NIPS and ISRR conferences [22, 21].", "startOffset": 68, "endOffset": 76}, {"referenceID": 20, "context": "1Parts of this work has been published at NIPS and ISRR conferences [22, 21].", "startOffset": 68, "endOffset": 76}, {"referenceID": 62, "context": "[63], Sucan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "[57], Schulman et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[50]) a priori.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "In this work we propose an algorithm for learning user preferences over trajectories through interactive feedback from the user in a coactive learning setting [51].", "startOffset": 159, "endOffset": 163}, {"referenceID": 3, "context": "Unlike in other learning settings, where a human first demonstrates optimal trajectories for a task to the robot [4], our learning model does not rely on the user\u2019s ability to demonstrate optimal trajectories a priori.", "startOffset": 113, "endOffset": 116}, {"referenceID": 44, "context": "[45]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Over the last decade many planning algorithms have been proposed, such as sampling based planners by Lavalle and Kuffner [33], and Karaman and Frazzoli [26], search based planners by Cohen et al.", "startOffset": 121, "endOffset": 125}, {"referenceID": 25, "context": "Over the last decade many planning algorithms have been proposed, such as sampling based planners by Lavalle and Kuffner [33], and Karaman and Frazzoli [26], search based planners by Cohen et al.", "startOffset": 152, "endOffset": 156}, {"referenceID": 9, "context": "[10], trajectory optimizers by Schul-", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[50], and Zucker et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[63] and many more [27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[63] and many more [27].", "startOffset": 19, "endOffset": 23}, {"referenceID": 25, "context": "Such preferences are often encoded as a cost which planners optimize [26, 50, 63].", "startOffset": 69, "endOffset": 81}, {"referenceID": 49, "context": "Such preferences are often encoded as a cost which planners optimize [26, 50, 63].", "startOffset": 69, "endOffset": 81}, {"referenceID": 62, "context": "Such preferences are often encoded as a cost which planners optimize [26, 50, 63].", "startOffset": 69, "endOffset": 81}, {"referenceID": 0, "context": "Examples of LfD includes, autonomous helicopter flights [1], ball-in-a-cup game [30], planning 2-D paths [43, 44], etc.", "startOffset": 56, "endOffset": 59}, {"referenceID": 29, "context": "Examples of LfD includes, autonomous helicopter flights [1], ball-in-a-cup game [30], planning 2-D paths [43, 44], etc.", "startOffset": 80, "endOffset": 84}, {"referenceID": 42, "context": "Examples of LfD includes, autonomous helicopter flights [1], ball-in-a-cup game [30], planning 2-D paths [43, 44], etc.", "startOffset": 105, "endOffset": 113}, {"referenceID": 43, "context": "Examples of LfD includes, autonomous helicopter flights [1], ball-in-a-cup game [30], planning 2-D paths [43, 44], etc.", "startOffset": 105, "endOffset": 113}, {"referenceID": 1, "context": "In many scenarios, especially involving high DoF manipulators, this is extremely challenging to do [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 44, "context": "[45] the robot observes optimal user feedback but performs approximate inference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[62], or locally optimal, as in Levine et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "[60] proposed a Bayesian framework for learning rewards of a", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "Our approach advances over [60] and Calinon et.", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "[9] in", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "human gestures) [8, 56] have been employed to teach assembly line robots.", "startOffset": 16, "endOffset": 23}, {"referenceID": 55, "context": "human gestures) [8, 56] have been employed to teach assembly line robots.", "startOffset": 16, "endOffset": 23}, {"referenceID": 38, "context": "[39, 40] in human-robot collaboration learns human preferences over a sequence of sub-tasks in assembly line manufacturing.", "startOffset": 0, "endOffset": 8}, {"referenceID": 39, "context": "[39, 40] in human-robot collaboration learns human preferences over a sequence of sub-tasks in assembly line manufacturing.", "startOffset": 0, "endOffset": 8}, {"referenceID": 54, "context": "[55, 54] and Mainprice et.", "startOffset": 0, "endOffset": 8}, {"referenceID": 53, "context": "[55, 54] and Mainprice et.", "startOffset": 0, "endOffset": 8}, {"referenceID": 36, "context": "[37] planned trajectories satisfying user specified preferences in form of constraints on the distance of the robot from the user, the visibility of the robot and the user\u2019s arm comfort.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] used functional gradients [47] to optimize for legibility of robot trajectories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[14] used functional gradients [47] to optimize for legibility of robot trajectories.", "startOffset": 31, "endOffset": 35}, {"referenceID": 32, "context": "Several works build upon the sampling-based planner RRT [33] to optimize various cost heuristics [17, 11, 20].", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": "Several works build upon the sampling-based planner RRT [33] to optimize various cost heuristics [17, 11, 20].", "startOffset": 97, "endOffset": 109}, {"referenceID": 10, "context": "Several works build upon the sampling-based planner RRT [33] to optimize various cost heuristics [17, 11, 20].", "startOffset": 97, "endOffset": 109}, {"referenceID": 19, "context": "Several works build upon the sampling-based planner RRT [33] to optimize various cost heuristics [17, 11, 20].", "startOffset": 97, "endOffset": 109}, {"referenceID": 25, "context": "Additive cost functions with Lipschitz continuity can be optimized using optimal planners such as RRT* [26].", "startOffset": 103, "endOffset": 107}, {"referenceID": 34, "context": "Some approaches introduce sampling bias [35] to guide the sampling based planner.", "startOffset": 40, "endOffset": 44}, {"referenceID": 46, "context": "Recent trajectory optimizers such as CHOMP [47] and TrajOpt [50] provide optimization based approaches to finding optimal trajectory.", "startOffset": 43, "endOffset": 47}, {"referenceID": 49, "context": "Recent trajectory optimizers such as CHOMP [47] and TrajOpt [50] provide optimization based approaches to finding optimal trajectory.", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "[5] and Phillips et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 40, "context": "[41] consider the problem of trajectories for high-dimensional manipulators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Other recent works consider generating human-like trajectories [15, 14, 58].", "startOffset": 63, "endOffset": 75}, {"referenceID": 13, "context": "Other recent works consider generating human-like trajectories [15, 14, 58].", "startOffset": 63, "endOffset": 75}, {"referenceID": 57, "context": "Other recent works consider generating human-like trajectories [15, 14, 58].", "startOffset": 63, "endOffset": 75}, {"referenceID": 52, "context": "[53], Kretzschmar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] and Kitani et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "first visualize them in the OpenRAVE simulator [13] and then decide the kind of feedback they would like to provide.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "This stands in contrast to learning from demonstration (LfD) methods [1, 30, 43, 44] which require (near) optimal demonstrations of the complete trajectory.", "startOffset": 69, "endOffset": 84}, {"referenceID": 29, "context": "This stands in contrast to learning from demonstration (LfD) methods [1, 30, 43, 44] which require (near) optimal demonstrations of the complete trajectory.", "startOffset": 69, "endOffset": 84}, {"referenceID": 42, "context": "This stands in contrast to learning from demonstration (LfD) methods [1, 30, 43, 44] which require (near) optimal demonstrations of the complete trajectory.", "startOffset": 69, "endOffset": 84}, {"referenceID": 43, "context": "This stands in contrast to learning from demonstration (LfD) methods [1, 30, 43, 44] which require (near) optimal demonstrations of the complete trajectory.", "startOffset": 69, "endOffset": 84}, {"referenceID": 1, "context": "Such demonstrations can be extremely challenging and nonintuitive to provide for many high DoF manipulators [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 20, "context": "Instead, we found [21, 22] that it is more intuitive for users to give incremental feedback on high DoF arms by improving upon a proposed trajectory.", "startOffset": 18, "endOffset": 26}, {"referenceID": 21, "context": "Instead, we found [21, 22] that it is more intuitive for users to give incremental feedback on high DoF arms by improving upon a proposed trajectory.", "startOffset": 18, "endOffset": 26}, {"referenceID": 12, "context": "(a) Re-ranking: We display the ranking of trajectories using OpenRAVE [13] on a touch screen device and ask the user to identify whether any of the lower-ranked trajectories is better than the top-ranked one.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "(c) Interactive: For the robots whose hardware does not permit zero-G feedback, such as PR2, we built an alternative interactive Rviz-ROS [18] interface for allowing the users to improve the trajectories by waypoint correction.", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Such linear representation of score functions have been previously used for generating desired robot behaviors [1, 44, 62].", "startOffset": 111, "endOffset": 122}, {"referenceID": 43, "context": "Such linear representation of score functions have been previously used for generating desired robot behaviors [1, 44, 62].", "startOffset": 111, "endOffset": 122}, {"referenceID": 61, "context": "Such linear representation of score functions have been previously used for generating desired robot behaviors [1, 44, 62].", "startOffset": 111, "endOffset": 122}, {"referenceID": 30, "context": "Since it is the attributes [31] of the object that really", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "laptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively.", "startOffset": 41, "endOffset": 59}, {"referenceID": 0, "context": "laptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively.", "startOffset": 41, "endOffset": 59}, {"referenceID": 0, "context": "laptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively.", "startOffset": 64, "endOffset": 82}, {"referenceID": 60, "context": "They comprise the following three types of the features: 1) Robot Arm Configurations: While a robot can reach the same operational space configuration for its wrist with different configurations of the arm, not all of them are preferred [61].", "startOffset": 237, "endOffset": 241}, {"referenceID": 13, "context": "Furthermore, humans like to anticipate robots move and to gain users\u2019 confidence, robot should produce predictable and legible robotic motions [14].", "startOffset": 143, "endOffset": 147}, {"referenceID": 30, "context": "In practice, such knowledge can be extracted using an object attribute labeling algorithms such as in [31].", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "Two effective ways of solving the former problem are either discretizing the state space [3, 7, 59] or directly sampling trajectories from the continuous space [6, 12].", "startOffset": 89, "endOffset": 99}, {"referenceID": 6, "context": "Two effective ways of solving the former problem are either discretizing the state space [3, 7, 59] or directly sampling trajectories from the continuous space [6, 12].", "startOffset": 89, "endOffset": 99}, {"referenceID": 58, "context": "Two effective ways of solving the former problem are either discretizing the state space [3, 7, 59] or directly sampling trajectories from the continuous space [6, 12].", "startOffset": 89, "endOffset": 99}, {"referenceID": 5, "context": "Two effective ways of solving the former problem are either discretizing the state space [3, 7, 59] or directly sampling trajectories from the continuous space [6, 12].", "startOffset": 160, "endOffset": 167}, {"referenceID": 11, "context": "Two effective ways of solving the former problem are either discretizing the state space [3, 7, 59] or directly sampling trajectories from the continuous space [6, 12].", "startOffset": 160, "endOffset": 167}, {"referenceID": 5, "context": "However, for high DoF manipulators the sampling based approach [6, 12] maintains tractability of the problem, hence we take this approach.", "startOffset": 63, "endOffset": 70}, {"referenceID": 11, "context": "However, for high DoF manipulators the sampling based approach [6, 12] maintains tractability of the problem, hence we take this approach.", "startOffset": 63, "endOffset": 70}, {"referenceID": 5, "context": "[6], we sample trajectories using rapidly-exploring random trees (RRT) [33].", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "[6], we sample trajectories using rapidly-exploring random trees (RRT) [33].", "startOffset": 71, "endOffset": 75}, {"referenceID": 47, "context": "[48] propose the use of sub-modularity to achieve diversity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "For more details on sampling trajectories we refer interested readers to the work by Erickson and LaValle [16], and Green and Kelly [19].", "startOffset": 106, "endOffset": 110}, {"referenceID": 18, "context": "For more details on sampling trajectories we refer interested readers to the work by Erickson and LaValle [16], and Green and Kelly [19].", "startOffset": 132, "endOffset": 136}, {"referenceID": 46, "context": "The cost function (or its approximation) we learn can be fed to trajectory optimizers like CHOMP [47] or optimal planners like RRT* [26] to produce reasonably good trajectories.", "startOffset": 97, "endOffset": 101}, {"referenceID": 25, "context": "The cost function (or its approximation) we learn can be fed to trajectory optimizers like CHOMP [47] or optimal planners like RRT* [26] to produce reasonably good trajectories.", "startOffset": 132, "endOffset": 136}, {"referenceID": 50, "context": "we adapt the Preference Perceptron algorithm [51] as detailed in Algorithm 1, and we call it the Trajectory Preference Perceptron (TPP).", "startOffset": 45, "endOffset": 49}, {"referenceID": 43, "context": "[44].", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "Despite its simplicity and even though the algorithm typically does not receive the optimal trajectory y\u2217 t = arg maxy s (xt, y) as feedback, the TPP enjoys guarantees on the regret [51].", "startOffset": 182, "endOffset": 186}, {"referenceID": 50, "context": "Using these two parameters, the proof by Shivaswamy and Joachims [51] can be adapted (for proof see Appendix A & B) to show that average regret of TPP is upper bounded by:", "startOffset": 65, "endOffset": 69}, {"referenceID": 48, "context": "Our work complements previous works on grasping items [49, 34], pick and place tasks [23], and detecting bar codes for grocery checkout [29].", "startOffset": 54, "endOffset": 62}, {"referenceID": 33, "context": "Our work complements previous works on grasping items [49, 34], pick and place tasks [23], and detecting bar codes for grocery checkout [29].", "startOffset": 54, "endOffset": 62}, {"referenceID": 22, "context": "Our work complements previous works on grasping items [49, 34], pick and place tasks [23], and detecting bar codes for grocery checkout [29].", "startOffset": 85, "endOffset": 89}, {"referenceID": 28, "context": "Our work complements previous works on grasping items [49, 34], pick and place tasks [23], and detecting bar codes for grocery checkout [29].", "startOffset": 136, "endOffset": 140}, {"referenceID": 32, "context": "\u2022 Geometric: The robot plans a path, independent of the task, using a Bi-directional RRT (BiRRT) [33] planner.", "startOffset": 97, "endOffset": 101}, {"referenceID": 43, "context": "\u2022 MMP-online: This is an online implementation of the Maximum Margin Planning (MMP) [44, 46] algorithm.", "startOffset": 84, "endOffset": 92}, {"referenceID": 45, "context": "\u2022 MMP-online: This is an online implementation of the Maximum Margin Planning (MMP) [44, 46] algorithm.", "startOffset": 84, "endOffset": 92}, {"referenceID": 43, "context": "However, directly adapting MMP [44] to our experiments poses two challenges: (i) we do not have knowledge of the optimal trajectory; and (ii) the state space of the manipulator we consider is too large, discretizing which makes intractable to train MMP.", "startOffset": 31, "endOffset": 35}, {"referenceID": 43, "context": "To ensure a fair comparison, we follow the MMP algorithm from [44, 46] and train it under similar settings as TPP.", "startOffset": 62, "endOffset": 70}, {"referenceID": 45, "context": "To ensure a fair comparison, we follow the MMP algorithm from [44, 46] and train it under similar settings as TPP.", "startOffset": 62, "endOffset": 70}, {"referenceID": 24, "context": "At every iteration MMP-online trains a structural support vector machine (SSVM) [25] using all previous feedback as training examples, and use the learned weights to predict trajectory scores in the next iteration.", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "[25]) end for", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "To quantify the quality of a ranked list of trajectories we report normalized discounted cumulative gain (nDCG) [38] \u2014 criterion popularly used in Information Retrieval for document ranking.", "startOffset": 112, "endOffset": 116}, {"referenceID": 23, "context": "We refer to this algorithm as Oracle-svm and it learns to rank trajectories using SVM-rank [24].", "startOffset": 91, "endOffset": 95}, {"referenceID": 41, "context": "In situations where this difference is significant and a system is desired for a user population, a future work might explore coactive learning for satisfying user population, similar to Raman and Joachims [42].", "startOffset": 206, "endOffset": 210}, {"referenceID": 51, "context": "better user interfaces [52] could further reduce this time.", "startOffset": 23, "endOffset": 27}, {"referenceID": 50, "context": "This proof builds upon Shivaswamy & Joachims [51].", "startOffset": 45, "endOffset": 49}], "year": 2016, "abstractText": "We consider the problem of learning preferences over trajectories for mobile manipulators such as personal robots and assembly line robots. The preferences we learn are more intricate than simple geometric constraints on trajectories; they are rather governed by the surrounding context of various objects and human interactions in the environment. We propose a coactive online learning framework for teaching preferences in contextually rich environments. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this coactive preference feedback can be more easily elicited than demonstrations of optimal trajectories. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We implement our algorithm on two high degree-of-freedom robots, PR2 and Baxter, and present three intuitive mechanisms for providing such incremental feedback. In our experimental evaluation we consider two context rich settings \u2013 household chores and grocery store checkout \u2013 and show that users are able to train the robot with just a few feedbacks (taking only a few minutes).", "creator": "LaTeX with hyperref package"}}}