{"id": "1601.01058", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2016", "title": "Wikiometrics: A Wikipedia Based Ranking System", "abstract": "We present a new concept - Wikiometrics - the derivation of metrics and indicators from Wikipedia. Wikipedia provides an accurate representation of the real world due to its size, structure, editing policy and popularity. We demonstrate an innovative mining methodology, where different elements of Wikipedia - content, structure, editorial actions and reader reviews - are used to rank items in a manner which is by no means inferior to rankings produced by experts or other methods. We test our proposed method by applying it to two real-world ranking problems: top world universities and academic journals. Our proposed ranking methods were compared to leading and widely accepted benchmarks, and were found to be extremely correlative but with the advantage of the data being publically available.", "histories": [["v1", "Wed, 6 Jan 2016 02:44:42 GMT  (903kb)", "http://arxiv.org/abs/1601.01058v1", null], ["v2", "Fri, 8 Jan 2016 05:25:27 GMT  (904kb)", "http://arxiv.org/abs/1601.01058v2", null]], "reviews": [], "SUBJECTS": "cs.DL cs.AI cs.SI", "authors": ["gilad katz", "lior rokach"], "accepted": false, "id": "1601.01058"}, "pdf": {"name": "1601.01058.pdf", "metadata": {"source": "CRF", "title": "Wikiometrics: A Wikipedia Based Ranking System", "authors": ["Gilad Katz", "Lior Rokach"], "emails": [], "sections": [{"heading": null, "text": "We present a new concept - Wikiometrics - the derivation of metrics and indicators from Wikipedia. Wikipedia, by its size, structure, editorial policy and popularity, provides an accurate representation of the real world. We demonstrate an innovative \"mining\" method, in which various elements of Wikipedia - content, structure, editorial actions and reviews - are used to rate articles in a way that is in no way inferior to rankings produced by experts or other methods. We test our proposed method by applying it to two real-world problems: top universities and scientific journals. Our proposed ranking methods have been compared with leading and widely accepted benchmarks and have proven to be extremely correlative, but with the advantage that the data is publicly available."}, {"heading": "1. Introduction", "text": "It is the process in which the relative position of the elements is determined, and this process is reflected in several areas, both scientific and non-scientific. Ranking is considered a difficult problem in many cases, as there is no absolute \"basic truth\" to which the evaluations generated can be compared. Nevertheless, several studies have been conducted that use the ranking in general and Wikipedia in particular. Wikipedia has been used in several scientific areas: in computer science, physics, sociology, etc. According to [1], an increasing number of Wikipedia-related papers seem to be generated every year. Wikipedia has several properties that make it a valuable source of information for research."}, {"heading": "2. Related Work", "text": "In this section, we will discuss four topics. In Section 2.1, we will describe existing ranking methods of universities worldwide. In Section 2.2, we will discuss the DBpedia project, which aims to extract structured content from Wikipedia. In Section 2.3, we will discuss existing methods for ranking scientific journals. Finally, in Section 2.4, we will describe the WikiProject Academic Journal, which aims to improve Wikipedia's reporting on scientific publications."}, {"heading": "2.1. International rankings of world universities", "text": "In this section, we review three such rankings, to which we will later compare our proposed ranking methods: Academic Rating of World Universities1 (ARWU), Times Higher Education World University Ratings2 (THE), and Webometrics Rating.3Academic Rating of World Universities (ARWU) ARWU was the first attempt to establish a global ranking of university rating. http: / / www.shanghairanking.com / index.html 2 http: / www.timeshighereducation.com / world ranking of world ranking of world universities (ARWU) is the first attempt to establish a global measuring tool. http: / / www.shanghairanking.com / index.html 2 http: / www.timeshighereducation.com / world-university-ranking / 3 http: / www.webometrics.info / The weights that make up the ranking are as they appeared on the official website of the rankings in May 2013."}, {"heading": "2.2. DBpedia", "text": "DBpedia is a collaborative effort to extract structured content from Wikipedia. It is probably one of the easiest ways to retrieve and exploit structured information from the online encyclopedia, and the extracted content is then made available via a database that allows complex queries (e.g., \"Which cities in the United States have a population of over 4 million people?\"). The project was launched in 2007 by teams from Freie Universit\u00e4t Berlin and the University of Leipzig in conjunction with OpenLink Software.9 DBpedia was released under a free license that allows others to reuse the code. It currently classifies nearly 4 million objects, about 2.5 million of which are part of a consistent ontology. In addition, it contains hundreds of thousands of links to external websites, nearly 200,000 links to other RDF databases (Auer et al., 2007), and over 8 million YAGO categories [4]. The ontology generated is diverse and includes people, places, creations (music albums, other valuable categories, etc.), and many other organizations."}, {"heading": "2.3. Journal Rankings", "text": "Journals serve as the main outlet for publishing the results of scientific research. Journal rankings help academic libraries choose which books and journals to buy and are often used as a measure of research quality; the ranking of a journal allows researchers to focus their work on first-class journals and improve their chances of advancement; the results reflect the cumulative opinion of a representative group of experts within a particular discipline or field. However, expert surveys have also been criticized for their subjectivity, the lack of clarity of their evaluation criteria [5], and various biases (such as favoring sources that publish more articles per year [6]). Finally, establishing a valid expert survey that includes a sufficient number of qualitative respondents is very time-consuming."}, {"heading": "2.4. The Academic Journals WikiProject", "text": "A WikiProject is a generic term for a collaborative project carried out by members of the Wikipedia community. Currently, there are over 2,250 such projects 10, with a wide variety of objectives. The WikiProject for Scientific Journals is an attempt to improve Wikipedia's coverage of scientific publications by expanding, categorizing, cleansing, and creating new articles by scanning four types of \"citation days\" in Wikipedia - {{{Citation}}, {{Cite journal}}, {{Vancite journal}}, and {{Vcite journal}}. This task is repeated by Bots.Although the ranking is informative (and - as we will prove later in this study - useful and indicative), it is by no means free of errors. WikiProject Site 11 describes several shortcomings in its rating system (we present just a few examples; the full list can be found on the website): citations that do not occur in the formats given above."}, {"heading": "3. First Case Study: University Ranking", "text": "The proposed method consists of two phases - information extraction and ranking. During the information extraction phase, we query DBpedia and Wikipedia to extract a number of entities (academic institutions and persons associated with them) and then use this information during the ranking phase to classify the extracted university entities.In order to extract all relevant entities from Wikipedia, all that was required was a simple DBpedia query for all entities of the type \"University.\" Since this query led to tens of thousands of institutions, we filtered this list and retained only entities that appeared in at least two of the three rankings presented in Section 2.1. This step was also made necessary by the fact that there are large differences in the number of universities in each ranking: ARWU ranks 500, THE ranks only 400, and the Webometry rankings over 12,000 universities. Following this decision, we were faced with the number of 9 universities in the richest rankings in the world."}, {"heading": "3.1. The Proposed Approaches", "text": "We propose three different methods for ranking real entities: a) shortcuts - the number of different Wikipedia pages that contain links to the entity's Wikipedia page; b) total page views - the number of visits to a particular page over a certain period of time; and c) relevant attributes of the Infobox - the identification of different entities associated with each of the ranked elements and the evaluation of their significance. Next, we describe these methods in detail."}, {"heading": "3.1.1. Links", "text": "Let's be the corpus of all Wikipedia entities and be a set of Wikipedia entities (i.e. pages) that we want to rate. For each ranked entity, we count the number of entities in that contain links that point to (see Equation 1). It should be noted that we count the number of entities, not the links. Therefore, even if a company contains multiple links to the ranked entity, it is only counted as a single link. This was done to prevent a small number of highly detailed entities from affecting the ranking process. Entities"}, {"heading": "3.1.2. Page Views", "text": "Let's be the corpus of all Wikipedia entities and let's be a set of Wikipedia entities we want to classify. For each ranked entity, we count the number of times it has been viewed over a certain period of time. It's important to note that the views of the redirect pages (pages that immediately transfer the user to another page) that point to the entity are also counted in the entity overcall count. The hypothesis behind this ranking method is that the more important / respected an entity is perceived, the more page views it is likely to have. To offset the effects of temporary \"peaks\" in popularity (e.g. due to news events), the page views were eagerly awaited over a period of several months."}, {"heading": "3.1.3. Infobox Attributes", "text": "We use the information from the infoboxes to obtain for each academic institution the notable persons associated with it. Since Wikipedia's editorial policy requires the person to be \"notable\" in order to have an entry, we considered this definition applicable to all entities of the \"person\" type in DBpedia. In addition, we extracted a general \"visibility indicator\" for each academic institution. The extracted characteristics are as follows: Faculty members - For each university, we count the number of persons who have at least one of these attributes in their infoboxes: Labor Institutions, Employers and Workplaces. We count the notable alumni of each institution by counting the persons with at least one of the following attributes: Alumnus, Alma mater, Education and Education. Other Affilities - This component is used to identify additional affiliations that can contribute to the reputation of an academic institution."}, {"heading": "3.2. Results", "text": "We evaluate our proposed ranking method by calculating its correlation to the leading and well-known above university rankings - ARWU, DAS and Webometrics, all from 2011. The Wikipedia version that was used was from December 2013, and the page views statistics were extracted from September-December 2013. Since each ranking method has a different number of universities, we have only left universities that have appeared in at least two of the three above rankings. In Table 3, we present the correlation of our proposed ranking approaches to each other. We also show the correlation between the three existing ranking methods. It is clear that they are highly correlated (all correlations are statistically significant with p < 0.001). In Table 3, we present the correlation of our proposed ranking approaches to each other of the three original ranking methods. We also show the correlation of each of the proposed methods, as well as that of a combined ranking consisting of all three approaches together.If the ranking method consists of several links (namely the information boxes), we will evaluate our proposed ranking method by calculating its correlation to the leading and known above university rankings - ARWU, DAS and Webometrics, all from 2011."}, {"heading": "4. Second Case Study: Journal Ranking", "text": "In this section, we show that Wikipedia can be used very effectively to rank academic journals, a task that has proved more difficult than ranking the world's leading universities, as many journals are not included in \"regular\" Wikipedia, but only in projects such as the WikiGroups described in Section 2.4. This makes two of the approaches presented in the previous case study - the use of links and page views - inapplicable. Therefore, we pursue this task with a weighted set of Infobox attributes. In order to compare our results with previously published results, we have decided to focus on the domain of artificial intelligence (AI). In the literature, several rankings of AI journals are available. Cheng et al. (1996) and Serenko (2010) used quotation-based metrics, while Serenko and Dohan (2011) reported on expert surveys in this area. Rokach (2012) used authorship-based rankings."}, {"heading": "4.1. Information Extraction", "text": "The metrics used in this case study were: 1. Quotations: Number of citations of the journal by Wikipedia 2. Quotations: Number of Wikipedia articles that quoted the journal (if the same Wikipedia article quoted the journal twice, it was counted only once) 3. Does Wikipedia page - This is a binary indicator that gets the value 1 if the journal has its own Wikipedia page. Our assumption is that top-notch journals have their own Wikipedia page (but this does not apply to all journals and is the reason for not using the linkages and page views of ranking methods). In addition, we have created a linear combination of all the above metrics. To find the weights, we have used linear regression, with the dependent (target) variable being the 5-year impact factor mentioned by Thomson Reuters. As mentioned in Section 2.4, the values generated by Wikiometry are a problem that has two serious limitations."}, {"heading": "4.2. The Ranking Phase", "text": "Our evaluation included 108 peer-reviewed AI journals indexed according to the \"Computer Science - Artificial Intelligence\" subcategory by the Thomson Reuters Web of Knowledge (WoK). These data relate to all journal publications of the Benchmark Fellowship. We used linear regression to find the optimal weights for the above parameters. After scaling the values of quote and quote to [0.1], we arrived at the final formula: 4.3848 (3) The measure used to evaluate the correlation of our proposed method to existing rankings was the Spearman ranking correlations. The results of the evaluation are presented in Table 7 (correlations marked with an asterisk < 0.05)."}, {"heading": "5. Conclusions and Future Work", "text": "In this paper, we presented two case studies demonstrating Wikipedia's ability to provide valuable information about the real world by capitalizing on the \"wisdom of the masses\" and extracting simple metrics from it. We showed that the opinions of tens of thousands of people (if not more) can be a surprisingly accurate alternative to the opinions of experts. We believe that the estimates provided by Wikiometrics could be further improved by more elaborate processing of their content. For example, journal rankings could also include references that do not use the < Ref > template and expand the evidence used for ranking. To this end, a suitable reference extraction for correctly identifying journal titles would need to be developed. University rankings could be similarly improved by taking into account the university affiliation of notable people, even if this information is not indexed in the info box, but appears as biographical text."}, {"heading": "6. References", "text": "1. Nielsen, F. A., Wikipedia Research and Tools: Review and comments. 2011. 2. Ferron, M. and P. Massa, The arab spring | wikirevolutions: Wikipedia as a lens forstudy the real-time formation of collective memories of revolutions. International Journal of Communication, 2011. 5: p. 20.3. Aguillo, I.F., J. Bar-Ilan, M. Levene, and J.L. Ortega, Comparing university rankings. Scientometrics, 2010. 85 (1): p. 243-256. 4. Suchanek, F.M., G. Kasneci, and G. Weikum, YAGO: A Large Ontology from Wikipedia and WordNet. Web Semantics: Science, Services and Agents on the World Wide Web, 2008. 6 (3): p. 203-217. 5. Holsapple, C.W., A Publication power approach for premier information systems. Journal of the American Society for Science and Technology, 185."}], "references": [{"title": "The arab spring| wikirevolutions: Wikipedia as a lens for studying the real-time formation of collective memories of revolutions", "author": ["M. Ferron", "P. Massa"], "venue": "International Journal of Communication,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "YAGO: A Large Ontology from Wikipedia and WordNet", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "A publication power approach for identifying premier information systems journals", "author": ["C.W. Holsapple"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Comparing the expert survey and citation impact journal ranking methods: Example from the field of Artificial Intelligence", "author": ["A. Serenko", "M. Dohan"], "venue": "Journal of Informetrics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "The history and meaning of the journal impact factor", "author": ["E. Garfield"], "venue": "Jama,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Measuring the value and prestige of scholarly journals", "author": ["C. Bergstrom"], "venue": "College & Research Libraries News,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Google Scholar: the democratization of citation analysis", "author": ["Harzing", "A.-W", "R. Van der Wal"], "venue": "Ethics in science and environmental politics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Should we use the mean citations per paper to summarise a journal\u2019s impact or to rank journals in the same field", "author": ["M. Calver", "J. Bradley"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Impact and relevance of LIS journals: A scientometric analysis of international and German\u2010language LIS journals\u2014Citation analysis versus reader survey", "author": ["C. Schloegl", "W.G. Stock"], "venue": "Journal of the American Society for information Science and Technology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "The development of an AI journal ranking based on the revealed preference approach", "author": ["A. Serenko"], "venue": "Journal of Informetrics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Revision of the journal list for doctoral designation", "author": ["D. Harless", "R. Reilly"], "venue": "Unpublished report,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Applying the author affiliation index to library and information science journals", "author": ["B. Cronin", "L.I. Meho"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Applying the Publication Power Approach to Artificial Intelligence Journals", "author": ["L. Rokach"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Convergence of the Nelder--Mead Simplex Method to a Nonstationary Point", "author": ["K.I. McKinnon"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}], "referenceMentions": [{"referenceID": 2, "context": "(1996) and Serenko (2010) used citation-based measures while Serenko and Dohan (2011) reported on expert surveys in the field.", "startOffset": 4, "endOffset": 26}, {"referenceID": 0, "context": "(1996) and Serenko (2010) used citation-based measures while Serenko and Dohan (2011) reported on expert surveys in the field.", "startOffset": 20, "endOffset": 86}, {"referenceID": 0, "context": "(1996) and Serenko (2010) used citation-based measures while Serenko and Dohan (2011) reported on expert surveys in the field. Rokach (2012) used author-based rankings.", "startOffset": 20, "endOffset": 141}], "year": 2016, "abstractText": "We present a new concept\u2014Wikiometrics\u2014the derivation of metrics and indicators from Wikipedia. Wikipedia provides an accurate representation of the real world due to its size, structure, editing policy and popularity. We demonstrate an innovative \u201cmining\u201d methodology, where different elements of Wikipedia \u2013 content, structure, editorial actions and reader reviews \u2013 are used to rank items in a manner which is by no means inferior to rankings produced by experts or other methods. We test our proposed method by applying it to two real-world ranking problems: top world universities and academic journals. Our proposed ranking methods were compared to leading and widely accepted benchmarks, and were found to be extremely correlative but with the advantage of the data being publically available.", "creator": "Microsoft\u00ae Word 2013"}}}