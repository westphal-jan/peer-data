{"id": "1601.01297", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2016", "title": "Angrier Birds: Bayesian reinforcement learning", "abstract": "We train a reinforcement learner to play a simplified version of the game Angry Birds. The learner is provided with a game state in a manner similar to the output that could be produced by computer vision algorithms. We improve on the efficiency of regular {\\epsilon}-greedy Q-Learning with linear function approximation through more systematic exploration in Randomized Least Squares Value Iteration (RLSVI), an algorithm that samples its policy from a posterior distribution on optimal policies. With larger state-action spaces, efficient exploration becomes increasingly important, as evidenced by the faster learning in RLSVI.", "histories": [["v1", "Wed, 6 Jan 2016 20:22:22 GMT  (503kb,D)", "https://arxiv.org/abs/1601.01297v1", "Stanford University CS221 Final Project"], ["v2", "Thu, 7 Jan 2016 01:28:34 GMT  (503kb,D)", "http://arxiv.org/abs/1601.01297v2", "Stanford University CS221 Final Project"]], "COMMENTS": "Stanford University CS221 Final Project", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["imanol arrieta ibarra", "bernardo ramos", "lars roemheld"], "accepted": false, "id": "1601.01297"}, "pdf": {"name": "1601.01297.pdf", "metadata": {"source": "CRF", "title": "Angrier Birds: Bayesian reinforcement learning", "authors": ["Imanol Arrieta Ibarra", "Bernardo Ramos", "Lars Roemheld"], "emails": [], "sections": [{"heading": null, "text": "Keywords Reinforcement Learning - Q-Learning - RLSVI - Exploration1Department of Management Science and Engineering, Stanford University, California, United StatesContentsIntroduction 1"}, {"heading": "1 Model 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Algorithms 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Q-Learning (linear function approximation) . . . . 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2 Randomized Least Squares Value Iteration . . . . 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.3 Feature extractors . . . . . . . . . . . . . . . . . . . . . . 3", "text": "Pig position values \u2022 Pig position indicator (PP) \u2022 Pig position counter (NPP) \u2022 Pig position counter (NPPO) \u2022 Pig position counter with obstacles (NPPO)"}, {"heading": "3 Results and Discussion 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Comparison of feature extractors . . . . . . . . . . . 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2 RLSVI vs. regular Q-Learning . . . . . . . . . . . . . . 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3 Learning success . . . . . . . . . . . . . . . . . . . . . . . 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Conclusions and Future Work 5", "text": "References 6Introducing Angry Birds was a hugely successful internet franchise based around an original mobile game in which players shoot birds out of a sling to hit targets for points. Angry Birds is largely a deterministic game: 1 a complex physics engine controls flight, collision and impact to appear almost realistic, so optimum gameplay could be achieved by optimizing a highly complex feature that describes flight path planning - instead, we train multiple reinforcement learning algorithms to play the game, and compare their final performance and learning speed with that of a man-made oracle. 1In our simulation, the physics engine actually seemed to produce semirandome results: the same sequence of moves was not guaranteed to produce exactly the same results. To simulate the gameplay, we have adapted an open source project to recreate Angry Birds in Python [1] using the Python model."}, {"heading": "1. Model", "text": "The state of play uncovered by our API is composed of the following information: This state representation completely describes the relevant game situation from the perspective of the player and is therefore sufficient to establish an interface between the learner and the game. We point out that all parts of the game state could be determined using computer vision algorithms, so that in principle no direct interaction with the game mechanics would be necessary. \u2022 Number of birds remaining (\"ammunition\") \u2022 Number of targets (\"pigs\") and their absolute positions on the game mapar Xiv: 160 1.01 297v 2 [cs.A I] 7 Jan 201 6 \u2022 Number of obstacles (\"bars\") and their absolute positions on the game map \u2022 The game scores and the level achieved by the player are further simplified by making the game mechanics completely spin-based: the learner always waits until all objects are motionless before starting the next action. This assumption allowed us to work exclusively at absolute positions. The sequence of possible actions (the range of motion, the angle of the pair, and the movement direction of the pair) is one (only)."}, {"heading": "2. Algorithms", "text": "Reinforcement learning is a subset of artificial intelligence that describes ways in which a learning algorithm can operate in unknown and unspecified decision-making processes that are guided only by rewards and penalties for desirable or undesirable outcomes. As a result, reinforcement learning closely mirrors human intuition about learning through conditioning and enables algorithms to master systems that would otherwise be difficult or impossible to specify. A standard algorithm in Reinforcement Learning is greedy Q-Learning. Although this algorithm is very effective, it is based on anerratic research of possible actions. We have implemented a variant that updates a belief distribution to optimal guidelines and then scans strategies from that distribution [2]. This implies more systematic research, as strategies that seem unattractive with high confidence are not reviewed again. After the introduction of the two algorithms, we discuss various iterations to find optimal feature extractors."}, {"heading": "2.1 Q-Learning (linear function approximation)", "text": "As explained in [3], Q-Learning is a modelless algorithm that approaches the function Qopt (s, a) of a state action value to derive an optimal policy for an unknown decision-making process. In order to learn optimal behavior in an unknown game, a Q-Learner must both explore the system by selecting previously invisible actions, and use the knowledge thus gained by selecting the action that promises the best results in light of previous experiences. The balance between efficient exploration and exploitation is a major challenge in reinforcement learning: The most popular method in the literature is the use of \u03b5-greedy methods: with a high probability, the algorithm would ignore its previous experiences and randomly select an action. In practice, this method tends to work well if the actions required to increase the payout are not too complicated. However, if complex action sets are taken [to obtain a higher reward, we must use this algorithm [4 to obtain a higher reward, see 5] the algorithm inclined to receive a higher reward in time]."}, {"heading": "2.2 Randomized Least Squares Value Iteration", "text": "Osband et al. propose Randomized Least Squares Value Iteration (RLSVI) (RLSVI), an algorithm that differs from our implementation of Q-Learning in two respects: 1. Instead of using random ancestry to learn from experiences in an online manner, RLSVI stores a complete history of (state, action, reward, new state) tuples to derive an exact \"optimal\" policy, given the data. 2. Given the hyperparameters about the quality of linear approximation, Bayesian is least likely to use squares to derive a distribution around the \"optimal\" policy that derives from this distribution - which replaces - greedy exploration. Specifically, RLSVI models the state's action value function as follows: Qw (st, at) = Reward (st, at) = Bayesian best, \"politics in\" 1. The end of this policy is then derived from the distribution of this learning."}, {"heading": "2.3 Feature extractors", "text": "When designing our feature extractors \u03c6 (s, a), we followed two premises: First, given that our approximation of the value function was linear, we wanted to reveal as little domain knowledge as possible - the task for the algorithm was to understand how to play Angry Birds without first having a concept of goals or obstacles. These two premises effectively restrict strategies as there is no reliable way for the algorithm to detect - for example, whether an obstacle is directly in front of or behind a target. Nevertheless, our learner developed impressive performance, as we will see later. One reason might be that the levels in our simulator were relatively simple. We repeated the following five ideas to form meaningful features. Due to the relatively complex state action space, our feature was relatively large, quickly spanning several thousand features."}, {"heading": "2.3.1 Pig Position Values", "text": "In a first experiment, we used rounded target positions as characteristic values: For each pig i in the given state S, we would have characteristic values pi = (x, y, xy) T. The characteristics were repeated for each action a and would only be non-zero for the action taken. This allows different weights for different position action combinations, which ultimately implies learning the best position action combinations. By including the interaction term xy, we had hoped to capture the relative distance of the target to the sling shot (which x and y could not produce themselves in a linear function): this would have allowed a quick generalization of the target positions. Unfortunately, if not surprising in retrospect, this approach failed very quickly and produced virtually no learning."}, {"heading": "2.3.2 Pig Position Indicator (PP)", "text": "The next approach was an indicator variable for a fine grid of pig positions: We created a separate feature for each possible pig position and recorded the measures taken again. This created an impractically large feature pace, but it worked relatively well. However, it was clearly \"overfitting\" - if a successful series of actions was found, the algorithm would reliably complete a level over and over again. However, if a target had moved only a few pixels, the algorithm would have to be completely retrained."}, {"heading": "2.3.3 Nested Pig Position Counter (NPP)", "text": "To solve the problem of mismatch, we developed a nested mechanism that would generalize to unobserved states. To achieve this, we defined 3 nested grids above the game screen, as in Figure 2. The three grids are getting smaller and smaller, and each square of each grid is a number of the targets it contains. This solved the generalization problem while retaining some of the nice features of the previous feature extractor. While the larger grids contributed to the generalization, the finer ones offered a way to remember when a level had already been observed."}, {"heading": "2.3.4 Nested Pig Positions: Shifted Counters (NPPS)", "text": "To improve this, we created a copy of each grid that was shifted diagonally by half a square. Now, each target lies in exactly two squares, which makes it possible to judge whether a target is further to the left or right within a square. NPPS was therefore a feature set of two sets of three overlapping square grids. To our surprise, NPPS performed worse than the simpler CPP. We assume that this is due to the much larger attribute space and the fact that due to computational limitations, we were only able to learn the algorithms after complete convergence."}, {"heading": "2.3.5 Nested Pig Positions Counters with Obstacles (NPPO)", "text": "Finally, we tried to address the problem of obstacles: As described, we did not want to give away game-related information by establishing a fixed relationship between goals and obstacles, so we added obstacle counters in the same way that we used them for goals, hoping that the algorithm would learn to favor areas with a high target-to-target ratio. Just as in the case of NPPO, we were surprised that adding information about obstacles was detrimental to learning success - in fact, adding obstacle information led to a complete breakdown of learning. As in the NPPPS, we suspect that NPO could work well if we were given more time to assimilate into the bloated feature space."}, {"heading": "3. Results and Discussion", "text": "The somewhat crude feature extractor NPP delivered the best results, and RSLVI actually did better than the regular \u03b5-greedy Q-Learner. RLSVI was particularly impressive for its ability to clear levels at almost the same speed as an (untrained) human player. We were only able to simulate gameplay when there was convergence due to computational constraints: the game engine we used was not intended for large simulations, and did not accelerate significantly even when graphics were disabled."}, {"heading": "3.1 Comparison of feature extractors", "text": "We compared our five different feature extractors based on the QLearning Baseline algorithm, as shown in Figure 3. It is noteworthy that PP starts with a high baseline score, but does not improve much from there: this is due to the fact that the learner masters a level very quickly, but then does not generalize to the next level. As a result, PP tends to hang and achieves relatively constant play scores in the first few levels. In contrast, NPP learns to master simple levels just as quickly, but then moves better to harder levels, resulting in higher total scores per day. NPPS shows a very similar gradient to NPP, which supports the explanation that the sheer number of features slows down the learning process. However, NPPS is clearly outperformed by NPP within our simulation timeframe."}, {"heading": "3.2 RLSVI vs. regular Q-Learning", "text": "We trained both algorithms with the same Feature Extractor (NPP) and compared a moving average score per attempt. RLSVI scored higher overall in the simulated timeframe and learned faster. A comparison of the two algorithms is shown in Figure 4.It should be noted that RLSVI required significantly more memory and calculation in our implementation than the regular Q-Learner, as it required a complete history of observed features and matrix operations on large blocks of data."}, {"heading": "3.3 Learning success", "text": "Neither a human player - our oracle for comparing scores - nor any of the algorithms managed to pass through all 11 levels occupied by the game engine we used in one attempt. However, both the regular Q-Learner and RLSVI outperformed the human player (who will not be named because he was shamefully outdone by coarse algorithms) in terms of the maximum score achieved in a single attempt and in terms of the highest level achieved. Table 1 summarizes the maximum score and the highest level achieved for different players. Comparing the number of attempts required to reach a certain level, it yields interesting insights: especially in later stages, the exploration of the RLSVI proves to be very efficient, as it passes through the levels with almost the same number of attempts as the human player requires."}, {"heading": "4. Conclusions and Future Work", "text": "RLSVI's Bayesian approach to space exploration seems to be a promising and fairly intuitive way to navigate unknown systems. Previous work has shown that Bayesian updating and sampling are highly efficient for exploration [4], which confirms our findings. RLSVI has both the basic Q-learning algorithm and, somewhat embarrassingly, our human oracle. A major obstacle to our work has been the speed of simulation, limiting the convergence of algorithms we could achieve. We would have liked to explore other possible features, especially after further calculations. Likewise, it would be interesting to train a deep neural network for the Q (s, a) function to allow for more complex interaction effects, especially between targets and obstacles. Promising results that combine Bayesian exploration with deep amplification learning have been shown in [5 of RLI] to be interesting within various variables."}, {"heading": "Acknowledgments", "text": "We would like to thank the CS221 teaching team for an inspiring quarter."}], "references": [{"title": "Generalization and exploration via randomized value functions", "author": ["Ian Osband", "Benjamin Van Roy", "Zheng Wen"], "venue": "arXiv preprint arXiv:1402.0635,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "more) efficient reinforcement learning via posterior sampling", "author": ["Ian Osband", "Dan Russo", "Benjamin Van Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Bootstrapped thompson sampling and deep exploration", "author": ["Ian Osband", "Benjamin Van Roy"], "venue": "arXiv preprint arXiv:1507.00300,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Bayesian q-learning", "author": ["Richard Dearden", "Nir Friedman", "Stuart Russell"], "venue": "In AAAI/IAAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "The elements of statistical learning, volume 1. Springer series in statistics", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Reinforcement learning-based multi-agent system for network traffic signal control", "author": ["Itamar Arel", "Cong Liu", "T Urbanik", "AG Kohls"], "venue": "Intelligent Transport Systems, IET,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "We implemented a variation which updates a belief distribution on optimal policies, and then samples policies from this distribution [2].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "As exposed in [3], Q-Learning is a model-free algorithm that approximates a state-action value function Qopt(s,a) in order to derive an optimal policy for an unknown decision process.", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "However, when complex sets of actions need to be taken in order to get a higher reward, \u03b5-greedy tends to take exponential time to find these rewards (see [2], [4], [5].", "startOffset": 155, "endOffset": 158}, {"referenceID": 2, "context": "However, when complex sets of actions need to be taken in order to get a higher reward, \u03b5-greedy tends to take exponential time to find these rewards (see [2], [4], [5].", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "However, when complex sets of actions need to be taken in order to get a higher reward, \u03b5-greedy tends to take exponential time to find these rewards (see [2], [4], [5].", "startOffset": 165, "endOffset": 168}, {"referenceID": 1, "context": "This allows the exploitation step to be an update only on the weights vector w, which we performed in a fashion similar to stochastic gradient descent (following the standard algorithm from [3]).", "startOffset": 190, "endOffset": 193}, {"referenceID": 0, "context": "propose Randomized Least Squares Value Iteration (RLSVI) [2], an algorithm that differs from our implementation of Q-Learning in two points:", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "Given a memory of (state, action, reward, new state) tuples, we can use Bayesian least squares to derive w\u0304t , the expected value of the optimal policy and \u03a3t = Cov(wt), the covariance of the optimal policy (the details of which are fairly straight-forward, and can be found in [2]).", "startOffset": 278, "endOffset": 281}, {"referenceID": 2, "context": "Previous work has shown Bayesian belief updating and sampling for exploration to be highly efficient [4], which our findings confirm.", "startOffset": 101, "endOffset": 104}, {"referenceID": 3, "context": "Promising results combining Bayesian exploration with deep reinforcement learning have been shown in [5].", "startOffset": 101, "endOffset": 104}], "year": 2016, "abstractText": "We train a reinforcement learner to play a simplified version of the game Angry Birds. The learner is provided with a game state in a manner similar to the output that could be produced by computer vision algorithms. We improve on the efficiency of regular \u03b5-greedy Q-Learning with linear function approximation through more systematic exploration in Randomized Least Squares Value Iteration (RLSVI), an algorithm that samples its policy from a posterior distribution on optimal policies. With larger state-action spaces, efficient exploration becomes increasingly important, as evidenced by the faster learning in RLSVI.", "creator": "LaTeX with hyperref package"}}}