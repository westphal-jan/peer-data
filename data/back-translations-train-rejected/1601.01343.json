{"id": "1601.01343", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2016", "title": "Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation", "abstract": "Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method specifically designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset. Our code and pre-trained vectors will be made available online.", "histories": [["v1", "Wed, 6 Jan 2016 22:19:20 GMT  (29kb)", "http://arxiv.org/abs/1601.01343v1", null], ["v2", "Sat, 19 Mar 2016 07:31:47 GMT  (30kb)", "http://arxiv.org/abs/1601.01343v2", null], ["v3", "Sun, 1 May 2016 06:39:19 GMT  (30kb)", "http://arxiv.org/abs/1601.01343v3", null], ["v4", "Fri, 10 Jun 2016 01:51:26 GMT  (30kb)", "http://arxiv.org/abs/1601.01343v4", "Accepted at CoNLL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ikuya yamada", "hiroyuki shindo", "hideaki takeda", "yoshiyasu takefuji"], "accepted": false, "id": "1601.01343"}, "pdf": {"name": "1601.01343.pdf", "metadata": {"source": "CRF", "title": "Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation", "authors": ["Ikuya Yamada", "Hiroyuki Shindo", "Hideaki Takeda", "Yoshiyasu Takefuji"], "emails": ["ikuya@ousia.jp", "shindo@is.naist.jp", "takeda@nii.ac.jp", "takefuji@sfc.keio.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 160 1,01 343v 1 [cs.C L] 6J an2 01"}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Joint Embedding of Words and Entities", "text": "In this section, we first describe the traditional Skip-gram model for embedding words, then explain our method for constructing an embedding that maps words and entities together into the same continuous d-dimensional vector space. We extend the Skip-gram model with the KB graph model and the anchor context model."}, {"heading": "2.1 Skip-gram Model for Word Similarity", "text": "Formally, given a sequence of T-words w1, w2,..., wT, the model aims to maximize the following objective function: Lw = T \u2211 t = 1 \u2211 \u2212 c \u2264 j \u2264 c, j 6 = 0log P (wt + j | wt) (1), where c is the size of the context window, wt is the target word and wt + j is the context word. The conditional probability P (wt + j | wt) is calculated with the following Softmax function: P (wt + j | wt) = exp (Vwt Uwt + j), w W exp (Vwt Uw) (2), where W is a sentence containing all the words of the vocabulary, and Vw Rd and Uw Rd are the vectors of the word w in matrices and V U, whereby the function of the vocabulary is trained accordingly."}, {"heading": "2.2 Extending the Skip-gram Model", "text": "We extend the skip gram model to learn the vector representations of entities. We extend the matrices V and U to include the vectors of entities Ve, Rd and Ue, Rd in addition to the vectors for words."}, {"heading": "2.2.1 KB Graph Model", "text": "We use an internal link structure in KB to allow the model to learn the kinship between entities pairs. Wikipedia Linkbased Measure (WLM) (Milne and Witten, 2008a) is a method for measuring kinship between entities based on their link structure. It has been used in past NED studies as the standard method for calculating the kinship of entities to model coherence. Relationship between two entities is calculated with the following function: WLM (e1, e2) = 1 \u2212 log max (Ce1 |, | Ce2 |) \u2212 log | Ce1 \u2012 Ce2 | log min (Ce1 |, | Ce2 |) (3), where E is the set of all entities in KB and Ce the set of entities associated with an entity e. Intuitively, WLM assumes that entities with similar entities are related to a space function in spite of their uniformity."}, {"heading": "2.2.2 Anchor Context Model", "text": "If we only add the KB chart model to the skipgram model, the vectors of words and entities do not interact and can be placed in different subspaces of the vector space. To solve this problem, we introduce the anchor context model to place similar words and entities in the vector space close together. The idea behind this model is to use KB anchors and their context words to train the model. As mentioned in Section 1, we use Wikipedia as KB. It contains many internal anchors that can be safely treated as unique occurrences of referential KB entities. By using these anchors, we can easily use many occurrences of entities and their associated context words directly from the KB. As in the skip gram model, we simply train the model to predict the context words of an entity that is pointed out by the target anchor. The objective function is as follows: La = ei, Q."}, {"heading": "2.3 Training", "text": "In view of the three model components mentioned above, we propose the following objective function by linearly combining the above objective functions: L = Lw + Le + La (8) The formation of the model is to maximize the function mentioned above, and the resulting matrix V is used to embed words and entities. One of the problems in the formation of our model is that the normalizers included in the Softmax functions P (wt + j | wt), P (eo | ei) and P (where they are very expensive in terms of computing because they include the summation of all words W or entities. To solve this problem, we use negative sampling (NEG et al), 2013b) to transform the original objective functions into computationally realizable ones. NEG is defined by the following objective function: Log (Vwt, Uwt, Uwt, Uwt, Uwt, Ewn, etc."}, {"heading": "3 Named Entity Disambiguation Using Embedding", "text": "In this section, we will explain our NED method based on our proposed embedding. Let's formally define the task. Given that a unit M = {m1, m2,..., mN} is mentioned in a document d with a unit E = {e1, e2,..., eK} in the KB, the task is defined as resolving mentions (e.g. \"Washington\") into their referential units (e.g. Washington D.C.) We will introduce two yardsticks that have been frequently observed in past NED studies: unit before P (e) and previous probability P (e | m). We will define unit before P (e) = | Ae, \u0445 | / | A, where A, \u0435 | are all anchors in the KB and the Ae: unit before P (e) and previous probability P (e | m)."}, {"heading": "3.1 Mention Disambiguation", "text": "In view of a document d and mention m with its reference units for candidates {e1, e2,..., ek} generated in the step of candidate generation, the task is to clearly determine mention m by selecting the most relevant unit from the candidate units. The key to improving this task is the effective modeling of the context. We propose two new methods to model the context using the proposed embedding. Furthermore, we combine these two models with several standard NED characteristics using supervised machine learning."}, {"heading": "3.1.1 Modeling Textual Context", "text": "Text context is designed based on the assumption that an entity appears more likely when the context of a given mention is similar to that of the entity. We propose a method to measure the similarity between text context and entity based on the proposed embedding by first deriving the vectors of context words and then calculating the similarity between context and entity based on cosinal similarity. To derive the vector of the context, we calculate the vectors of context words: ~ vcw = 1 | Wcm | \u0445 w-Wcm ~ vw (10), where Wcm is a set of context words from mention m and ~ vw-V is the vector representation of word w. We use all the noun words in document d as context words. Furthermore, we ignore a context word when the surface of mention m contains it. We then measure the similarity between the candidate unit and the context by using a cosmic entity derived from the context."}, {"heading": "3.1.2 Modeling Coherence", "text": "It has been shown that effective modeling of coherence in assigning units to mentions is important for NED. To solve this problem, we introduce a simple two-step approach: We first train the machine learning model using the coherence point for unique mentions 2, in addition to other attributes, and then retrain the model using the coherence point for predicted unit assignments. To estimate coherence, we first calculate the vector representation of context units and measure the similarity between the vector of context units and that of the target unit e. Note that context units are unique units in the first step and predicted units are used instead in the second step.1We used Apache OpenNLP taggers to see the context."}, {"heading": "3.1.3 Learning to Rank", "text": "In order to combine the context information described above with the standard NED features described above, we use a method of supervised machine learning to classify the candidates mentioned and the document. In particular, we use Gradient Boosted Regression Trees (GBRT) (Friedman, 2001), a state-of-the-art, point-by-point learning-to-rank algorithm that is widely used for various tasks and for the type of tasks we are using it for here (Meij et al., 2012). GBRT consists of an overall set of regression trees and predicts a relevance value given in an example. We use GBRT implementation in scikit-learn3 and logistical loss as a loss function. The most important parameters of GBRT are the number of iterations, the learning rate \u03b2 and the maximum depth of the decision trees."}, {"heading": "4 Experiments", "text": "In this section, we describe the setup and results of our experiments. In addition to the experiments on the NED task, we conducted two experiments - one with a word similarity and one with a kinship - to test the effectiveness of our method in capturing pairwise similarities between pairs of words and pairs of beings. First, we describe the details of the embedding training and then present the experimental results."}, {"heading": "4.1 Training for the Proposed Embedding", "text": "We first removed the pages for navigation, maintenance and discussion and used the remaining 4.9 million pages. We analyzed the Wikipedia pages and extracted text and anchors from each page. We continued to produce the text using the Apache OpenNLP tokenizer. We also filtered out rare words that occurred less than five times in the corpus. Thus, we obtained approximately 2 billion tokens and 73 million anchors. The total number of words and units in the embedding was about 2.1 million and 5 million, respectively. Consequently, the number of rows of matrices V and U was 7.1 million. The number of dimensions d of the embedding was set to 500. Subsequently (Mikolov et al., 2013b) we also used the learning rate \u03b1 = 0.025, which decreased linear with the Wikipedia landfill iterations."}, {"heading": "4.2 Word Similarity", "text": "To test the quality of the vector representations of words, we used three standard word similarity datasets: the WordSim 353 dataset (Finkelstein et al., 2002), the MC dataset (Miller and Charles, 1991), and the RG dataset (Rubenstein and Goodenough, 1965), which contains 353, 65, and 30 word pairs, respectively. Each word pair has a gold standard similarity value assigned by human judgment. In this case, we used the Skip gram model as a baseline value. We used our implementation to train the skipgram model. In addition, the following parameters were used to train the model: d = 500, c = 10, g = 30, and \u03b1 = 0.025."}, {"heading": "4.3 Entity Relatedness", "text": "To test the quality of entity vector representation, we conducted an experiment with an entity kinship dataset created by Ceccarelli et al. (Ceccarelli et al., 2013), which consists of training, testing, and validation sets, and we use only the test set. The test set contains 3,314 entities, each of which has 91 candi-date entities with gold-standard labels indicating whether the two entities are related. In the following (Huang et al., 2015), we determined the order of candidates based on cosmic similarity between the target unit and each of the candidates, and calculated the two standard measures: normalized discounted cumulative increase (NDCG) (Ja \ufffd rvelin and Keka \ufffd la \ufffd inen, 2002) and mean average accuracy (MAP) (Manning et al.)."}, {"heading": "4.4 Named Entity Disambiguation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4.1 Setup", "text": "We explain ourselves by the performance of our candidates for the office of Vice-President."}, {"heading": "4.4.2 Comparison with State-of-the-art Methods", "text": "We compared our method with the following recently proposed state-of-the-art methods: \u2022 Hoffart et al. (Hoffart et al., 2011) is a graphical approach that finds a dense subgraph of entities in a document to address NED. \u2022 He et al. (He et al., 2013) uses deep neural networks to derive entity representations and mention contexts and apply them to NED. \u2022 Chisholm and Hachey5 https: / / github.com / masha-p / PPRforNED 6 http: / / www.nist.gov / tac / (Chisholm and Hachey, 2015) use a Wikilinks dataset (Singh et al., 2012) to improve NED performance."}, {"heading": "4.4.3 Results", "text": "Table 3 shows the experimental results of our proposed method as well as those of the most advanced methods. Our proposed method achieved a micro accuracy of 93.1% and a macro accuracy of 92.6% on the CoNLL dataset and a micro accuracy of 85.2% on the TAC 2010 dataset. Our method significantly exceeded all other modern methods on both datasets."}, {"heading": "4.4.4 Feature Study", "text": "We conducted a feature study of our method, starting with basic features, progressively adding different features to our system and reporting on their impact on performance, and then introducing our two-step approach to achieve the final results.Table 4 shows the results. Surprisingly, we achieved results comparable to those of most modern methods on the two sets of data, using only basic characteristics. Adding string similarities further improves performance slightly. We noted significant improvements in adding textual context characteristics based on our proposed embedding. Our method outperformed other state-of-the-art methods without using coherence. Further, coherence based on unique entity mentions and our two-step approach significantly improved the performance of the CoNLL dataset. However, it did not contribute to the performance of the TAC 2010 dataset. This was due to the significant difference in the density of the entity mentions per year that contains approximately one item of data set between the TAC-2010 dataset, but only one item of the document of the L."}, {"heading": "4.4.5 Error Analysis", "text": "We observed that approximately 48.6% of the errors were caused by metonymic mentions (Ling et al., 2015) (i.e. mentions with more than one plausible comment). In particular, our NED method was often flawed when an incorrect entity was very popular and exactly matched the mention interface (e.g. \"South Africa\" refers to the South African national rugby team and not to the South African entity).This is useful because our machine learning model uses KB popularity statistics (i.e. previous probability and entity) and the string similarity between the title of the entity and the mention interface. This problem is discussed further in (Ling et al., 2015)."}, {"heading": "5 Related Work", "text": "These methods focused primarily on modeling the similarity of textual (local) contexts. Most recent methods of statecraft focus on modeling coherence between distinct entities in the same document (Cucerzan, 2007; Milne and Witten, 2008b; Hoffart et al., 2011; Ratinov et al., 2011) These approaches have also been called collective or global approaches in literature. Presentation of units for NED has been addressed in the past literature. Guo and Barbosa have used random walks on 35mm graphs to construct vector representations of entities and documents. Blanco et al al al al al al al al. (Blanco et al., 2015) proposed a method of embedding entities in the word."}, {"heading": "6 Conclusions", "text": "Our method enables us to model both textual and global contexts effectively. Equipped with these context models, our NED method significantly outperforms modern NED methods. In future work, we intend to improve our model by using relevant knowledge, such as relationships in a knowledge diagram (e.g. Freebase). We would also like to make applications of our proposed embedding outside of NED. The code of the proposed embedding method and the pre-formed vectors used in our experiments will be made publicly available prior to the conference."}], "references": [{"title": "Fast and Space-Efficient Entity Linking for Queries", "author": ["Blanco et al.2015] Roi Blanco", "Giuseppe Ottaviano", "Edgar Meij"], "venue": "In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining (WSDM),", "citeRegEx": "Blanco et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blanco et al\\.", "year": 2015}, {"title": "Learning Structured Embeddings of Knowledge Bases", "author": ["Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Proceedings of the 25th Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Using Encyclopedic Knowledge for Named Entity Disambiguation", "author": ["Bunescu", "Pasca2006] Razvan Bunescu", "Marius Pasca"], "venue": "In Proceedings of the 11th Conference of the European Chapter of the Association", "citeRegEx": "Bunescu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2006}, {"title": "Learning Relatedness Measures for Entity Linking", "author": ["Claudio Lucchese", "Salvatore Orlando", "Raffaele Perego", "Salvatore Trani"], "venue": "In Proceedings of the 22nd ACM International Conference on Information and Knowledge", "citeRegEx": "Ceccarelli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ceccarelli et al\\.", "year": 2013}, {"title": "Entity Disambiguation with Web Links. Transactions of the Association for Computational Linguistics, 3:145\u2013156", "author": ["Chisholm", "Hachey2015] Andrew Chisholm", "Ben Hachey"], "venue": null, "citeRegEx": "Chisholm et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chisholm et al\\.", "year": 2015}, {"title": "Large-Scale Named Entity Disambiguation Based on Wikipedia Data", "author": ["Silviu Cucerzan"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning", "citeRegEx": "Cucerzan.,? \\Q2007\\E", "shortCiteRegEx": "Cucerzan.", "year": 2007}, {"title": "Placing Search in Context: The Concept Revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "Finkelstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2002}, {"title": "Greedy Function Approximation: A Gradient Boosting Machine", "author": ["Jerome H. Friedman"], "venue": "The Annals of Statistics,", "citeRegEx": "Friedman.,? \\Q2001\\E", "shortCiteRegEx": "Friedman.", "year": 2001}, {"title": "Entity Linking with a Unified Semantic Representation", "author": ["Guo", "Barbosa2014] Zhaochen Guo", "Denilson Barbosa"], "venue": "In Proceedings of the 23rd International Conference on World Wide Web (WWW),", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Learning Entity Representation for Entity Disambiguation", "author": ["He et al.2013] Zhengyan He", "Shujie Liu", "Mu Li", "Ming Zhou", "Longkai Zhang", "Houfeng Wang"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "He et al\\.,? \\Q2013\\E", "shortCiteRegEx": "He et al\\.", "year": 2013}, {"title": "Robust Disambiguation of Named Entities in Text", "author": ["Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum"], "venue": null, "citeRegEx": "Hoffart et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2011}, {"title": "KORE: Keyphrase Overlap Relatedness for Entity Disambiguation", "author": ["Stephan Seufert", "Dat Ba Nguyen", "Martin Theobald", "Gerhard Weikum"], "venue": "In Proceedings of the 21st ACM International Conference on Informa-", "citeRegEx": "Hoffart et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2012}, {"title": "Entity Hierarchy Embedding", "author": ["Hu et al.2015] Zhiting Hu", "Poyao Huang", "Yuntian Deng", "Yingkai Gao", "Eric Xing"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural", "citeRegEx": "Hu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "Leveraging Deep Neural Networks and Knowledge Graphs for Entity Disambiguation", "author": ["Huang et al.2015] Hongzhao Huang", "Larry Heck", "Heng Ji"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Cumulated gain-based evaluation of IR techniques", "author": ["J\u00e4rvelin", "Kek\u00e4l\u00e4inen2002] Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "J\u00e4rvelin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin et al\\.", "year": 2002}, {"title": "Overview of the TAC 2010 Knowledge Base Population Track", "author": ["Ji et al.2010] Heng Ji", "Ralph Grishman", "Hoa Trang Dang", "Kira Griffitt", "Joe Ellis"], "venue": "In Proceeding of Text Analytics Conference (TAC)", "citeRegEx": "Ji et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2010}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "author": ["Lin et al.2015] Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "In Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Design Challenges for Entity Linking. Transactions of the Association for Computational Linguistics, 3:315\u2013328", "author": ["Ling et al.2015] Xiao Ling", "Sameer Singh", "Daniel S. Weld"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Introduction to Information Retrieval", "author": ["Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Overview of the TAC 2009 Knowledge Base Population Track", "author": ["McNamee", "Dang2009] P McNamee", "HT Dang"], "venue": "In Proceeding of Text Analysis Conference (TAC)", "citeRegEx": "McNamee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McNamee et al\\.", "year": 2009}, {"title": "Adding Semantics to Microblog Posts", "author": ["Meij et al.2012] Edgar Meij", "Wouter Weerkamp", "Maarten de Rijke"], "venue": "In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining (WSDM),", "citeRegEx": "Meij et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Meij et al\\.", "year": 2012}, {"title": "Wikify!: Linking Documents to Encyclopedic Knowledge", "author": ["Mihalcea", "Csomai2007] Rada Mihalcea", "Andras Csomai"], "venue": "In Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "Mihalcea et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2007}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Greg Corrado", "Kai Chen", "Jeffrey Dean"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual Correlates of Semantic Similarity", "author": ["Miller", "Charles1991] George A. Miller", "Walter G. Charles"], "venue": "Language and Cognitive Processes,", "citeRegEx": "Miller et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1991}, {"title": "An Effective, Low-Cost Measure of Semantic Relatedness Obtained from Wikipedia Links", "author": ["Milne", "Witten2008a] David Milne", "Ian H. Witten"], "venue": "In Proceedings of the First AAAI Workshop on Wikipedia and Artificial Intelligence (WIKIAI)", "citeRegEx": "Milne et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Milne et al\\.", "year": 2008}, {"title": "Learning to Link with Wikipedia", "author": ["Milne", "Witten2008b] David Milne", "Ian H. Witten"], "venue": "In Proceeding of the 17th ACM Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "Milne et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Milne et al\\.", "year": 2008}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Personalized Page Rank for Named Entity Disambiguation", "author": ["Yifan He", "Ralph Grishman"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Pershina et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pershina et al\\.", "year": 2015}, {"title": "Local and Global Algorithms for Disambiguation to Wikipedia", "author": ["Ratinov et al.2011] Lev Ratinov", "Dan Roth", "Doug Downey", "Mike Anderson"], "venue": "In Proceedings of the 49th Annual Meeting of the Associa-", "citeRegEx": "Ratinov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2011}, {"title": "Contextual Correlates of Synonymy", "author": ["Rubenstein", "John B. Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "Wikilinks: A Large-scale Cross-Document Coreference Corpus Labeled via Links to Wikipedia", "author": ["Singh et al.2012] Sameer Singh", "Amarnag Subramanya", "Fernando Pereira", "Andrew McCallum"], "venue": "Technical Report UM-CS-2012-015", "citeRegEx": "Singh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2012}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Modeling Mention, Context and Entity with Neural Networks for Entity Disambiguation", "author": ["Sun et al.2015] Yaming Sun", "Lei Lin", "Duyu Tang", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang"], "venue": "In Proceedings of the Twenty-Fourth International Joint Conference", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "Knowledge Graph and Text Jointly Embedding", "author": ["Wang et al.2014] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "NED has lately been extensively studied (Cucerzan, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Ratinov et al., 2011) and used as a fundamental component in numerous tasks, such as information extraction, knowledge base population (McNamee and Dang, 2009; Ji et al.", "startOffset": 40, "endOffset": 130}, {"referenceID": 29, "context": "NED has lately been extensively studied (Cucerzan, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Ratinov et al., 2011) and used as a fundamental component in numerous tasks, such as information extraction, knowledge base population (McNamee and Dang, 2009; Ji et al.", "startOffset": 40, "endOffset": 130}, {"referenceID": 15, "context": ", 2011) and used as a fundamental component in numerous tasks, such as information extraction, knowledge base population (McNamee and Dang, 2009; Ji et al., 2010), and semantic search (Blanco et al.", "startOffset": 121, "endOffset": 162}, {"referenceID": 0, "context": ", 2010), and semantic search (Blanco et al., 2015).", "startOffset": 29, "endOffset": 50}, {"referenceID": 27, "context": "Word embedding methods are also becoming increasingly popular (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014).", "startOffset": 62, "endOffset": 133}, {"referenceID": 11, "context": "Despite its simplicity, WLM yields state-of-the-art performance (Hoffart et al., 2012).", "startOffset": 64, "endOffset": 86}, {"referenceID": 7, "context": "In particular, we use Gradient Boosted Regression Trees (GBRT) (Friedman, 2001), a state-of-the-art point-wise learning-to-rank algorithm widely used for various tasks, which has been recently adopted for the sort of tasks for which we employ it here (Meij et al.", "startOffset": 63, "endOffset": 79}, {"referenceID": 20, "context": "In particular, we use Gradient Boosted Regression Trees (GBRT) (Friedman, 2001), a state-of-the-art point-wise learning-to-rank algorithm widely used for various tasks, which has been recently adopted for the sort of tasks for which we employ it here (Meij et al., 2012).", "startOffset": 251, "endOffset": 270}, {"referenceID": 20, "context": "We also use several string similarity features used in past work on NED (Meij et al., 2012).", "startOffset": 72, "endOffset": 91}, {"referenceID": 6, "context": "In order to test the quality of vector representations of words, we used three standard word similarity datasets: the WordSim-353 dataset (Finkelstein et al., 2002), the MC dataset (Miller and Charles, 1991), and the RG dataset (Rubenstein and Goodenough, 1965) that contain 353, 65, and 30 word pairs, respectively.", "startOffset": 138, "endOffset": 164}, {"referenceID": 3, "context": "(Ceccarelli et al., 2013).", "startOffset": 0, "endOffset": 25}, {"referenceID": 13, "context": "Following (Huang et al., 2015), we obtained the ranked order of the candidate entities using cosine similarity between the target entity and each of the candidate entities, and computed the two standard measures: normalized discounted cumulative gain (NDCG) (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) and mean average precision (MAP) (Manning et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 18, "context": ", 2015), we obtained the ranked order of the candidate entities using cosine similarity between the target entity and each of the candidate entities, and computed the two standard measures: normalized discounted cumulative gain (NDCG) (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) and mean average precision (MAP) (Manning et al., 2008).", "startOffset": 300, "endOffset": 322}, {"referenceID": 13, "context": "(Huang et al., 2015).", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "(Hoffart et al., 2011).", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Following (Hoffart et al., 2011), we only used 27,816 mentions with valid entries in the KB and reported the standard micro- (aggregates over all mentions) and macro- (aggregates over all documents) accuracies of the top-ranked candidate entities to assess disambiguation performance.", "startOffset": 10, "endOffset": 32}, {"referenceID": 28, "context": "(Pershina et al., 2015).", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "TAC 2010 The TAC 2010 dataset is another popular NED dataset constructed for the Text Analysis Conference (TAC)6 (Ji et al., 2010).", "startOffset": 113, "endOffset": 130}, {"referenceID": 9, "context": "Following past work (He et al., 2013; Chisholm and Hachey, 2015), we used mentions only with a valid entry in the KB, and reported the micro-accuracy score of the top-ranked candidate entities.", "startOffset": 20, "endOffset": 64}, {"referenceID": 10, "context": "(Hoffart et al., 2011) is a graphbased approach that finds a dense subgraph of entities in a document to address NED.", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "(He et al., 2013) uses deep neural networks to derive the representations of entities and mention contexts and applies them to NED.", "startOffset": 0, "endOffset": 17}, {"referenceID": 31, "context": "(Chisholm and Hachey, 2015) uses a Wikilinks dataset (Singh et al., 2012) to improve the performance of NED.", "startOffset": 53, "endOffset": 73}, {"referenceID": 17, "context": "6% errors were caused by metonymy mentions (Ling et al., 2015) (i.", "startOffset": 43, "endOffset": 62}, {"referenceID": 17, "context": "This problem is discussed further in (Ling et al., 2015).", "startOffset": 37, "endOffset": 56}, {"referenceID": 5, "context": "Most recent stateof-the-art methods focus on modeling coherence among disambiguated entities in the same document (Cucerzan, 2007; Milne and Witten, 2008b; Hoffart et al., 2011; Ratinov et al., 2011).", "startOffset": 114, "endOffset": 199}, {"referenceID": 10, "context": "Most recent stateof-the-art methods focus on modeling coherence among disambiguated entities in the same document (Cucerzan, 2007; Milne and Witten, 2008b; Hoffart et al., 2011; Ratinov et al., 2011).", "startOffset": 114, "endOffset": 199}, {"referenceID": 29, "context": "Most recent stateof-the-art methods focus on modeling coherence among disambiguated entities in the same document (Cucerzan, 2007; Milne and Witten, 2008b; Hoffart et al., 2011; Ratinov et al., 2011).", "startOffset": 114, "endOffset": 199}, {"referenceID": 0, "context": "(Blanco et al., 2015) proposed a method to map entities into the word embedding (i.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "(He et al., 2013) used deep neural networks to compute representations of entities and contexts of mentions directly from the KB.", "startOffset": 0, "endOffset": 17}, {"referenceID": 33, "context": "(Sun et al., 2015) proposed a method based on deep neural networks to model representations of mentions, contexts of mentions, and entities.", "startOffset": 0, "endOffset": 18}, {"referenceID": 13, "context": "(Huang et al., 2015) also leveraged deep neural networks to learn entity representations such that the consequent pairwise entity relatedness was more suitable than of a standard method (i.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "(Hu et al., 2015) used hierarchical information in the KB to build entity embedding and applied it to model coherence.", "startOffset": 0, "endOffset": 17}, {"referenceID": 1, "context": "Furthermore, in the context of knowledge graph embedding, another tenor of recent works has been published (Bordes et al., 2011; Socher et al., 2013; Lin et al., 2015).", "startOffset": 107, "endOffset": 167}, {"referenceID": 32, "context": "Furthermore, in the context of knowledge graph embedding, another tenor of recent works has been published (Bordes et al., 2011; Socher et al., 2013; Lin et al., 2015).", "startOffset": 107, "endOffset": 167}, {"referenceID": 16, "context": "Furthermore, in the context of knowledge graph embedding, another tenor of recent works has been published (Bordes et al., 2011; Socher et al., 2013; Lin et al., 2015).", "startOffset": 107, "endOffset": 167}, {"referenceID": 34, "context": "(Wang et al., 2014) have recently revealed that the joint modeling of the embedding of words and entities can improve performance in several tasks including the link prediction task, which is somewhat analogous to our experimental results.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method specifically designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset. Our code and pre-trained vectors will be made available online.", "creator": "LaTeX with hyperref package"}}}