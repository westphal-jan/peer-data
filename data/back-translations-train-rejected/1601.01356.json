{"id": "1601.01356", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2016", "title": "From Word Embeddings to Item Recommendation", "abstract": "Social network platforms can archive data produced by their users and re-use the data to serve the users better. One of the services that these platforms provide is the recommendation service. Recommendation systems can predict the future preferences of the users using various different techniques. One of the most popular technique for recommendation is matrix-factorization, which uses low-rank approximation of input data. Similarly word embedding methods from natural language processing literature learn low-dimensional vector space representation of input elements. Noticing the similarities among word embedding and matrix factorization techniques and based on the previous works that apply techniques from text processing for recommendation, Word2Vec's skip-gram technique is employed to make recommendations. Unlike previous works that use Word2Vec for recommendation, non-textual features are used. The aim of this work is to make recommendation on next check-in venues and a Foursquare check-in dataset is used for this purpose. The results showed that use of vector space representations of items modelled by skip-gram technique is promising for making recommendations.", "histories": [["v1", "Thu, 7 Jan 2016 00:09:37 GMT  (890kb,D)", "http://arxiv.org/abs/1601.01356v1", null], ["v2", "Sun, 6 Mar 2016 16:09:10 GMT  (890kb,D)", "http://arxiv.org/abs/1601.01356v2", null], ["v3", "Wed, 15 Jun 2016 08:07:36 GMT  (922kb,D)", "http://arxiv.org/abs/1601.01356v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR cs.SI", "authors": ["makbule gulcin ozsoy"], "accepted": false, "id": "1601.01356"}, "pdf": {"name": "1601.01356.pdf", "metadata": {"source": "CRF", "title": "From Word Embeddings to Item Recommendation", "authors": ["Makbule Gulcin Ozsoy"], "emails": ["makbule.ozsoy@ceng.metu.edu.tr"], "sections": [{"heading": null, "text": "In fact, most people who are able to move are able to move, to move and to move, to move, to move, to move, to move, to move, to move and to move, to move, to move, to move and to move."}, {"heading": "II. RELATED WORK", "text": "In fact, it is the case that most of them will be able to move to another world, in which they are able to move to another world, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live,"}, {"heading": "III. RECOMMENDATION USING MULTIPLE DATA SOURCES", "text": "Our goal is to list the top k check-in points (e.g. restaurant, cafe) that the target user will visit in the future. To this end, I have used a technique from the Word2Vec toolbox, namely the skip program. In this section, I will give brief information about the techniques in the Word2Vec toolbox and explain how the skip technique is used to check-in recommendations. Word2Vec is a group of models introduced by Mikolov et al. ([18], [17]). It contains two different techniques, namely skip programs and continuous bags of words (CBOW) that produce word embeddings, i.e. distributed word representations. The word embeddings represent the words in a low-dimensional continuous space and carry the semantic and syntactic information of words [11]. While the CBOW technique uses the words around the current word as a matrix, the matrix is the matrix."}, {"heading": "A. Modelling the input data using the skip-gram technique", "text": "The preferred method for making recommendations is the Skipgram technique: First, the use of word processing techniques for the recommendation is already reinforced in the llterature. Second, the Skip-gram technique implicitly factorises the input matrix and the matrix factoring techniques take effect in the gensimtoolbox 1. This implementation accepts a list of sentences, which in turn are a list of words. These words are used to create the internal dictionary that holds the words and their frequencies."}, {"heading": "B. Recommendation using vector representation", "text": "This report uses three different recommendation techniques that use the vector representation of items and users: Recommendation by k-next items (KNI): In our recommendation by k-next items (KNI) approach, the similarities between users and items are used. In this method, the most similar k items are found directly for the target user. To this end, cosine similarities between the related vectors1https: / radimrehurek.com / gensim / models / word2vec.htmlis are used. The collected top k items are recommended to the target user. Recommendation by N users use the most similar items: / radimrehurek.com / gensim / models / word2vec.htmlis."}, {"heading": "IV. EVALUATION", "text": "This year it is so far that it will be able to name the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the"}, {"heading": "V. CONCLUSION", "text": "In the literature, there are many different techniques for making recommendations, e.g. by applying neighborhood-based, machine learning, and matrix factorization-based methods. One of the most popular methods is matrix factorization-based approaches, which use an approximation of input data to low values. Similarly, text embedding methods from the literature for processing natural language learn low-dimensional vector space representations of input elements. In this paper, the goal is to recommend top-k locations to users based on their previous preferences. I used a dataset collected by Foursquare to make recommendations, and the results showed that the use of techniques from natural language processing is effective, and the use of combination techniques from the W2Vec method for combining sketchgrams that provide promising results for future improvements in vector technology."}], "references": [{"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["S. Arora", "Y. Li", "Y. Liang", "T. Ma", "A. Risteski"], "venue": "CoRR, vol. abs/1502.03520, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "An empirical comparison of social, collaborative filtering, and hybrid recommenders", "author": ["A. Bellog\u0131\u0301n", "I. Cantador", "F. D\u0131\u0301ez", "P. Castells", "E. Chavarriaga"], "venue": "ACM TIST, vol. 4, no. 1, p. 14, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Where you like to go next: Successive point-of-interest recommendation", "author": ["C. Cheng", "H. Yang", "M.R. Lyu", "I. King"], "venue": "IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence, Beijing, China, August 3-9, 2013, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring social-historical ties on location-based social networks", "author": ["H. Gao", "J. Tang", "H. Liu"], "venue": "Proceedings of the Sixth International Conference on Weblogs and Social Media, Dublin, Ireland, June 4-7, 2012, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "A non-iid framework for collaborative filtering with restricted boltzmann machines", "author": ["K. Georgiev", "P. Nakov"], "venue": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, 2013, pp. 1148\u20131156.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluating collaborative filtering recommender systems", "author": ["J.L. Herlocker", "J.A. Konstan", "L.G. Terveen", "J. Riedl"], "venue": "ACM Trans. Inf. Syst., vol. 22, no. 1, pp. 5\u201353, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R.M. Bell", "C. Volinsky"], "venue": "IEEE Computer, vol. 42, no. 8, pp. 30\u201337, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, 2014, pp. 1188\u20131196.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, 2014, pp. 2177\u20132185.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "TACL, vol. 3, pp. 211\u2013 225, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Word embedding revisited: A new representation learning and explicit matrix factorization perspective", "author": ["Y. Li", "L. Xu", "F. Tian", "L. Jiang", "X. Zhong", "E. Chen"], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, 2015, pp. 3650\u20133656.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Geomf: Joint geographical modeling and matrix factorization for point-ofinterest recommendation", "author": ["D. Lian", "C. Zhao", "X. Xie", "G. Sun", "E. Chen", "Y. Rui"], "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201914. New York, NY, USA: ACM, 2014, pp. 831\u2013840.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Soco: a social network aided context-aware recommender system", "author": ["X. Liu", "K. Aberer"], "venue": "22nd International World Wide Web Conference, WWW \u201913, Rio de Janeiro, Brazil, May 13-17, 2013, 2013, pp. 781\u2013802.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Sorec: Social recommendation using probabilistic matrix factorization", "author": ["H. Ma", "H. Yang", "M.R. Lyu", "I. King"], "venue": "Proceedings of the 17th ACM Conference on Information and Knowledge Management, ser. CIKM \u201908. New York, NY, USA: ACM, 2008, pp. 931\u2013940.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Recommender systems with social regularization", "author": ["H. Ma", "D. Zhou", "C. Liu", "M.R. Lyu", "I. King"], "venue": "Proceedings of the Forth International Conference on Web Search and Web Data Mining, WSDM 2011, Hong Kong, China, February 9-12, 2011, 2011, pp. 287\u2013296.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Trust-aware recommender systems", "author": ["P. Massa", "P. Avesani"], "venue": "RecSys, 2007, pp. 17\u201324.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, vol. abs/1301.3781, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., 2013, pp. 3111\u20133119.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Word embedding techniques for content-based recommender systems: An empirical evaluation.", "author": ["C. Musto", "G. Semeraro", "M. de Gemmis", "P. Lops"], "venue": "RecSys Posters, ser. CEUR Workshop Proceedings,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Multi-objective optimization based location and social network aware recommendation", "author": ["M.G. Ozsoy", "F. Polat", "R. Alhajj"], "venue": "10th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing, CollaborateCom 2014, Miami, Florida, USA, October 22-25, 2014, 2014, pp. 233\u2013242.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G.E. Hinton"], "venue": "Machine Learning, Proceedings of the Twenty-Fourth International Conference (ICML 2007), Corvallis, Oregon, USA, June 20-24, 2007, 2007, pp. 791\u2013798.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Recommending tumblr blogs to follow with inductive matrix completion.", "author": ["D. Shin", "S. Cetintas", "K.-C. Lee"], "venue": "RecSys Posters, ser. CEUR Workshop Proceedings,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Social computing: an intersection of recommender systems, trust/reputation systems, and social networks", "author": ["M. Tavakolifard", "K.C. Almeroth"], "venue": "IEEE Network, vol. 26, no. 4, pp. 53\u201358, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Collaborative deep learning for recommender systems", "author": ["H. Wang", "N. Wang", "D. Yeung"], "venue": "CoRR, vol. abs/1409.2944, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaborative location and activity recommendations with GPS history data", "author": ["V.W. Zheng", "Y. Zheng", "X. Xie", "Q. Yang"], "venue": "Proceedings of the 19th International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, USA, April 26-30, 2010, 2010, pp. 1029\u20131038.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": "Recently, matrix factorization (MF) based approaches gained more attention by researchers, as these methods can efficiently deal with large datasets by using low-rank approximation of input data [15].", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "They are used to learn linguistic regularities and semantic information from large text datasets and they are gaining more attention especially in natural language processing and text mining fields [19].", "startOffset": 198, "endOffset": 202}, {"referenceID": 17, "context": "In this work, I aimed to use Word2Vec\u2019s [18] skip-gram word embedding technique for recommending next check-in locations.", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "Efficiency of using text processing techniques in recommendation systems is already exemplified in some of the previous works in the literature ([4], [22], [19]).", "startOffset": 145, "endOffset": 148}, {"referenceID": 21, "context": "Efficiency of using text processing techniques in recommendation systems is already exemplified in some of the previous works in the literature ([4], [22], [19]).", "startOffset": 150, "endOffset": 154}, {"referenceID": 18, "context": "Efficiency of using text processing techniques in recommendation systems is already exemplified in some of the previous works in the literature ([4], [22], [19]).", "startOffset": 156, "endOffset": 160}, {"referenceID": 3, "context": "[4] is one of the state-of-the-art methods for venue recommendation on Location Based Social Networks (LBSNs) and employs a language model based method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22] aims to make recommendation to users about which blog to follow.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] employs three different word embedding techniques, one of which is Word2Vec, to make recommendation on MovieLens and DBbook datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Unlike the previous works that use Word2Vec for recommendation ([22], [19]), I used a non-textual feature, namely the past check-ins of the users.", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "Unlike the previous works that use Word2Vec for recommendation ([22], [19]), I used a non-textual feature, namely the past check-ins of the users.", "startOffset": 70, "endOffset": 74}, {"referenceID": 3, "context": "For the evaluation I used a Foursquare check-in dataset, which is already used in previous works ([4], [20]).", "startOffset": 98, "endOffset": 101}, {"referenceID": 19, "context": "For the evaluation I used a Foursquare check-in dataset, which is already used in previous works ([4], [20]).", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "Recommendation systems make recommendation of items by estimating their preferences ([16], [23]).", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "Recommendation systems make recommendation of items by estimating their preferences ([16], [23]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "These methods use low-rank approximation of input data and can handle large volume of data [15].", "startOffset": 91, "endOffset": 95}, {"referenceID": 6, "context": "In [7], it is stated that matrix factorization can represent the items and the users as vectors where high correlation between vectors leads to recommendation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 13, "context": "[14], Zheng et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27], Liu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13], Cheng et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] and [12].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[3] and [12].", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "Among these works [27] and [3] have similar purpose as ours and they make location/activity recommendations to the target users, however they do not employ Word2Vec for this purpose.", "startOffset": 18, "endOffset": 22}, {"referenceID": 2, "context": "Among these works [27] and [3] have similar purpose as ours and they make location/activity recommendations to the target users, however they do not employ Word2Vec for this purpose.", "startOffset": 27, "endOffset": 30}, {"referenceID": 18, "context": "The word embeddings learn linguistic regularities and semantic information from the input text datasets and represent the the meaning of the words by a vector representation ([19], [1]).", "startOffset": 175, "endOffset": 179}, {"referenceID": 0, "context": "The word embeddings learn linguistic regularities and semantic information from the input text datasets and represent the the meaning of the words by a vector representation ([19], [1]).", "startOffset": 181, "endOffset": 184}, {"referenceID": 0, "context": "In [1] it is stated that word embeddings can be learnt by Latent Semantic Analysis (LSA), topic models and matrix factorization techniques.", "startOffset": 3, "endOffset": 6}, {"referenceID": 17, "context": "Techniques defined in Word2Vec [18], namely skip-gram and continuous bag of words (CBOW), are commonly used in the literature to represent the word vectors.", "startOffset": 31, "endOffset": 35}, {"referenceID": 21, "context": "Some of the recommendation methods ([22], [19]) use techniques from Word2Vec to represent their text based features.", "startOffset": 36, "endOffset": 40}, {"referenceID": 18, "context": "Some of the recommendation methods ([22], [19]) use techniques from Word2Vec to represent their text based features.", "startOffset": 42, "endOffset": 46}, {"referenceID": 21, "context": "[22] aims to make recommendation to users about which Tumblr blogs to follow.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] empirically evaluates three word embedding techniques, namely Latent Semantic Indexing, Random Indexing and Word2Vec, to make recommendation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Another recommendation method that uses techniques from natural language processing is SocioHistorical method proposed in [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 20, "context": "[21], [5] and [24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[21], [5] and [24].", "startOffset": 6, "endOffset": 9}, {"referenceID": 23, "context": "[21], [5] and [24].", "startOffset": 14, "endOffset": 18}, {"referenceID": 20, "context": "In [21] uses Restricted Boltzmann Machines (RBM\u2019s) to make movie recommendations.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "[5] extends [21] by modelling both user-user and item-item correlations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[5] extends [21] by modelling both user-user and item-item correlations.", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "[24] proposes a hierarchical Bayesian model thet learns models on both content informationon items and past preferences of users.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Unlike the previous works that use Word2Vec for recommendation ([22], [19]), I used non-textual features, namely the past check-ins of the users.", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "Unlike the previous works that use Word2Vec for recommendation ([22], [19]), I used non-textual features, namely the past check-ins of the users.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "([18], [17]).", "startOffset": 1, "endOffset": 5}, {"referenceID": 16, "context": "([18], [17]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "The word embeddings represent the words in a low dimensional continuous space and carry the semantic and syntactic information of words [11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 9, "context": "[10] states that CBOW combines words from the context window and cannot be easily expressed as a factorization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "However, [9] shows that skip-gram performs a matrix factorization implicitly.", "startOffset": 9, "endOffset": 12}, {"referenceID": 17, "context": "Lastly, skip-gram s preffered to CBOW, since it performs better than or equally well to CBOW technique ([18], [10])", "startOffset": 104, "endOffset": 108}, {"referenceID": 9, "context": "Lastly, skip-gram s preffered to CBOW, since it performs better than or equally well to CBOW technique ([18], [10])", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "Inspiring from [8], I used the item lists together with the users, i.", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "The skip-gram model provides the word vectors where the words with similar meaning are located closer in the vector space [8].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "For this purpose the Checkin2011 dataset 2, previously used by [4] and [20], is used.", "startOffset": 63, "endOffset": 66}, {"referenceID": 19, "context": "For this purpose the Checkin2011 dataset 2, previously used by [4] and [20], is used.", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "However, in [20] the researchers used a subset of this dataset by using the check-ins made in January as training set, and named it as CheckinsJan.", "startOffset": 12, "endOffset": 16}, {"referenceID": 3, "context": "The evaluation results of the method presented in this report are compared to [4] and [20].", "startOffset": 78, "endOffset": 81}, {"referenceID": 19, "context": "The evaluation results of the method presented in this report are compared to [4] and [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 1, "context": "In the recommendation systems literature, some of the methods may loose coverage in order to increase the accuracy [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 5, "context": "[6] states that coverage and accuracy should be analysed together.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[20] decided that best performing values are N = 30 and k = 10 for the CheckinsJan dataset, these values are used for the experiments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Also the upper-bound of the methods based on the decided parameters are presented in [20]: The upper-bound for Precision metric is found as 0.", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "In our experiments this parameter is refferred as feature count(F ) and is set to different values in the range of [10, 100] with 10 increments.", "startOffset": 115, "endOffset": 124}, {"referenceID": 4, "context": "In our experiments this parameter is refferred as context count(C) and is set to different values in the range of [5, 20] with 5 increments.", "startOffset": 114, "endOffset": 121}, {"referenceID": 19, "context": "In our experiments this parameter is refferred as context count(C) and is set to different values in the range of [5, 20] with 5 increments.", "startOffset": 114, "endOffset": 121}, {"referenceID": 4, "context": "In our experiments it is refferred as epoch count(E) and is set to different values in the range of [5, 25] with 5 increments.", "startOffset": 100, "endOffset": 107}, {"referenceID": 19, "context": "The next two methods are from [20], which are based on multi-objective optimization technique and combines past preferences of the users with other features, such as users\u2019 hometowns, friendship and their influence on each other.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "The last two methods are from [4], which uses a language model based method from natural language processing Fig.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "[4], KNI method performs better compared to the other methods.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "Social network platforms can archive data produced by their users and re-use the data to serve the users better. One of the services that these platforms provide is the recommendation service. Recommendation systems can predict the future preferences of the users using various different techniques. One of the most popular technique for recommendation is matrix-factorization, which uses low-rank approximation of input data. Similarly word embedding methods from natural language processing literature learn low-dimensional vector space representation of input elements. Noticing the similarities among word embedding and matrix factorization techniques and based on the previous works that apply techniques from text processing for recommendation, Word2Vec\u2019s skip-gram technique is employed to make recommendations. Unlike previous works that use Word2Vec for recommendation, non-textual features are used. The aim of this work is to make recommendation on next check-in venues and a Foursquare check-in dataset is used for this purpose. The results showed that use of vector space representations of items modelled by skip-gram technique is promising for making recommendations. Keywords\u2014Recommendation systems, Location based social networks, Word embedding, Word2Vec, Skip-gram technique", "creator": "TeX"}}}