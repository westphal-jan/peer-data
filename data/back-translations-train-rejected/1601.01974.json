{"id": "1601.01974", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jan-2016", "title": "Scale-Free Online Learning", "abstract": "We design algorithms for online linear optimization that have optimal regret and at the same time do not need to know any upper or lower bounds on the norm of the loss vectors. We achieve adaptiveness to the norms of the loss vectors by scale invariance, i.e., our algorithms make exactly the same decisions if the sequence of loss vectors is multiplied by any positive constant. One of our algorithms works for any decision set, bounded or unbounded. For unbounded decisions sets, this is the first adaptive algorithm for online linear optimization with a non-vacuous regret bound.", "histories": [["v1", "Fri, 8 Jan 2016 18:47:18 GMT  (27kb)", "https://arxiv.org/abs/1601.01974v1", "arXiv admin note: text overlap witharXiv:1502.05744"], ["v2", "Wed, 14 Dec 2016 18:32:39 GMT  (27kb)", "http://arxiv.org/abs/1601.01974v2", "arXiv admin note: text overlap witharXiv:1502.05744"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1502.05744", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["francesco orabona", "d\\'avid p\\'al"], "accepted": false, "id": "1601.01974"}, "pdf": {"name": "1601.01974.pdf", "metadata": {"source": "CRF", "title": "Scale-Free Online Learning", "authors": ["Francesco Orabona", "D\u00e1vid P\u00e1l"], "emails": ["francesco@orabona.com", "dpal@yahoo-inc.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 1.01 974v 2 [cs.L G] 14 DWe design and analyze linear online optimization algorithms that exhibit optimum regret without having to know the upper or lower limit of the loss vector norm. Our algorithms are instances of the meta-algorithms Follow the Regularized Leader (FTRL) and Mirror Descent (MD). We achieve alignment with the loss vector norms through scale invariance, i.e. our algorithms make exactly the same decisions when the sequence of loss vectors is multiplied by a positive constant. The FTRL-based algorithm works for all decision sets, limited or unlimited. In the case of unlimited decision sets, this is the first adaptive algorithm for linear online optimization with a non-vacuum limit of regret. In contrast, we show lower limits on scale-free algorithms based on MD unlimited domains."}, {"heading": "1. Introduction", "text": "However, the aim of the algorithm is to have a small cumulative loss. An algorithm's performance is evaluated by the so-called regret, which is the difference between the cumulative losses of the algorithm and the (hypothetical) strategy, which is the same best point in hindsight.OLO is a fundamental problem in machine learning [2, 3, 4]. Many learning problems can be formulated directly as OLO, e.g. learning with expert advice [5, 6, 7, 8] and online combinatorial optimization [9, 10, 11]. Other problems can be reduced to OLO, e.g. online convex optimization [12], [4, chapter]."}, {"heading": "1.1. Previous results", "text": "The majority of existing algorithms for OLO are based on two generic algorithms: Follow The Regularizer Leader (FTRL) and Mirror Descent (MD). The same algorithm was analyzed in [29] of convex optimization under the name Dual Averaging and refound in [28]. The name Follow The Regularized Leader was analyzed in [24] and the analysis in [27] of convex optimization in [21] of online optimization."}, {"heading": "1.2. Overview of the Results", "text": "We design and analyze two scale-free algorithms: SOLO FTRL and AdaFTRL are based on FTRL. AdaFTRL is a generalization of AdaHedge [25] to arbitrary strong convex regularizers. SOLO FTRL can be called the \"correct\" scale-free version of the diagonal version of AdaGrad FTRL [24] generalized to arbitrary strong convex regularizers. Sale-Free MD is a generalization of FTRL [24] generalized to arbitrary strong convex regularizers. Sale-Free MD is based on MD. It is a generalization of AdaGrad MD [24] to arbitrary strong convex regularizers."}, {"heading": "2. Notation and Preliminaries", "text": "Let's say V is a finite dimensional 3 real vector space endowed with a norm. We call V its dual vector space. The bilinear map associated with (V, V) is denoted by < \u00b7, \u00b7 >: V, \u00b7 V, \u00b7 R. The dual norm of our vector space is defined by \"V.\" 3Many, but not all of our results can be extended to more general standardized vector spaces. In OLO, the immediate loss of the algorithm in turn t = 1, 2,.... The cumulative loss of the algorithm after T rounds is \"T\" = 1 < t, wt >. The immediate loss of the algorithm in round t is < t, wt >. The cumulative loss of the algorithm after T rounds is \"T.\""}, {"heading": "2.1. Convex Analysis", "text": "The Bregman conjugation of fennel easily follows the definition: When f, g, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f"}, {"heading": "2.2. Generic FTRL with Varying Regularizer", "text": "Two of our scale-free algorithms are examples of FTRL with different regulators, represented as algorithm 1. The algorithm is parameterized by a sequence {Rt} \u221e t = 1 of the functions called Rt: K \u2192 R regulator. If the regulators R1, R2,... chosen by algorithm 1 are strongly convex and low semi-continuous, the regret of the algorithm is above its limits. Lemma 1 (RegretT (u) \u2264 RT + 1 (u) + R \u0445 1 (0) + T = 1BR,. t (\u2212 Lt, \u2212 Lt \u2212 1) \u2212 Rendit (\u2212 Annex) & t + 1 (\u2212 Lt).The proof of the dilemma can be found in range A.R."}, {"heading": "2.3. Generic Mirror Descent with Varying Regularizer", "text": "The algorithm is called algorithm 2. The algorithm is parameterized by a sequence {Rt} \u221e t = 0 of convex functions Rt: K \u2192 R. Any regulator Rt can depend on past loss vectors in an arbitrary way. If Rt is not differentiable, the Bregman divergence 4 must be selected so that BRt (u, v) = Rt (u) \u2212 Rt (v) \u2212 < Rt (v), u \u2212 v > must be defined. This is done by selecting a subgradient card."}, {"heading": "2.4. Per-Coordinate Learning", "text": "An interesting class of algorithms proposed in [22] and [24] is based on so-called procoordinate learning rates. As shown in [33], any algorithm for OLO can also be used with procoordinate learning rates. In abstract terms, we assume that the decision theorem is a cartesian product K = K1 \u00b7 K2 \u00b7 \u00b7 \u00b7 \u00b7 Kd of a finite number of convex sets. For each factor Kj, j = 1, 2,..., d we can execute any OLO algorithm separately, and we point out that this can happen even if Rt is a limitation of a differentiable function defined on a superset of K. If K is limited and closed, Rt cannot be differentiable at the boundary of K. If K is a subset of an affine subspace of a dimension smaller than the dimension of V, then Rt cannot be differentiable everywhere as a free version."}, {"heading": "3. SOLO FTRL", "text": "In this section we present our first scale-free algorithm based on FTRL."}, {"heading": "4. Scale-Free Mirror Descent", "text": "In this case, we are able to find a solution with which we disagree. (...) In this case, we disagree. (...) In this case, we disagree. (...) In this case, we disagree. (...) In this case, we disagree. (...) In this case, we agree. (...) In this case, we agree. (...) In this case, we agree. (...) In this case, we disagree. (...) In this case, we agree. (...) In this case, we agree. (...) In this case, we agree. (...) In this case, we agree. (...) In this case, we agree. (...) In this case, we agree. (...) In this case, we agree. (...) In this case, we agree. (...) In this case, we agree. (...) In this case, we agree. (...) In this case, we agree. (...) In this case, we agree."}, {"heading": "4.1. Lower Bounds for Scale-Free Mirror Descent", "text": "The boundaries in Theorem 2 and Corollary 2 are empty if BR (u, v) is not limited. One might wonder if the assumption that BR (u, v) is limited is necessary to have a sublinear regret. We show the necessity of this assumption on two counter-examples. In these counter-examples, we consider strongly convex regulators R such that BR (u, v) is not limited, and we construct sequences of loss vectors. The first counter-example is given as Theorem 3 below....., that the decision turns out to be negative.,...... that the decision is so................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "5. Lower Bound", "text": "The proof presented in Appendix F is a standard probabilistic argumentation.Theorem 5 (lower limit).Let K V be any non-empty, closed convex subset and let a1, a2,.., aT be any non-negative real number. There is a sequence of vectors limited to K (possibly randomized) algorithm to K. Let T be any non-negative integer and let a1, a2,.., aT be any non-negative real number. There is a sequence of vectors limited to K 1, 2,.., namely to T in the dual vector space V, leaving a gap of 1, a1, a2,."}, {"heading": "6. Conclusions", "text": "We have studied scale-free algorithms for linear online optimization and we have shown that scale-free ownership leads to algorithms that have optimum regrets and do not need to know or assume anything about the sequence of loss vectors. Specifically, the algorithms have no upper or lower limits as to the norms of loss vectors or the number of rounds. We have developed a scale-free algorithm that is based on Follow The Regularizer Leader. Its regret with respect to any competitor u isO f (u) s (u) s (t) s (t) s (t) s (t) s) s (t) s (t) s (t) s (t) s \"s (t) s (t) s (t) s (t) s (s) s (t) s (s) s (t) s (s) s (t) s (s) s (s) s (t) s (s) s (s) s (s) s (s) s (t) s (s) s (s (s) s (s) s (s) s (s (t) s (s) s (s) c (c) s (t) c (t) s (t) s (c) s (t) s (t) s (s (s) s (s) s (s) s (s (s) s (s) s (s) s (s) s (s (s) s (s) s (s) s (s) s (s) s (s) s (s (s) s (s (s) s) s (s (s) s (s (s) s (s (s) s (s (s) c (s) c (s) c (s (s) s (s (s) c (s) s (s (s) c (s (s) s (s) s (s) s (s (s (s) c (s (s) c (s) s (s) s (s) s (s (s (s) c (s) s (s (s) c (s) s (s (s) s (s (s) s (s (s) s (s (s (s) c (s"}, {"heading": "Acknowledgments", "text": "We thank an anonymous reviewer who has suggested a simpler proof for Lemma 7."}, {"heading": "Appendix A. Proofs for Preliminaries", "text": "The detection (proof for Proposition 1). \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 set point (v) \u2212 set point (V) and compactness of K. The optimal condition for V (V) and 1-strong convexity of f (V) implies that for all u (K), S (S) \u2212 f (V) \u2212 f (V) \u2212 f (V) \u2212 f (V) \u2212 f (V) \u2212 v (V), u (R), u (R), u (V), u (V), u (T), t (T), t (T), t (L), t (T), t (L), t (T), t (T), t (T), t (T), t (T), t (T), t (T), t (T), t (T), t (T), t (T), t (T), t (T), t (T)."}, {"heading": "Appendix B. AdaFTRL", "text": "In this section we show that it is possible to derive a scale-free algorithm that differs from SOLO FTRL. We generalize the AdaHedge algorithms (25) to the OLO setting and show that it maintains its scale-free property. We call the resulting algorithm AdaFTRL. The analysis is very general and is based on the general properties of strongly convex functions rather than specific properties of the entropic regulator as in the original analysis of AdaHedge.Assume that R \u2192 R is a strongly convex lower semi-continuous function from above. We instantiate algorithms 1 with the sequence of regularizersRt (w)."}, {"heading": "Appendix C. Limits", "text": "In this section we show that the prediction of AdaFTRL is correctly defined if the regulator is < R = > P = > P = > P = < R = > P = > P = > P = > P = < R = < R (K) < R (K) < R (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P (K) # P) # P (K) # P # P (P) # P) # P (K) P) # P (P) P) # P (P)."}, {"heading": "Appendix D. Proofs for SOLO FTRL", "text": "Proof (proof of Lemma 4): We use the inequality x / \u221a x + y \u2264 2 (\u221a x + y \u2212 \u221a y), which is valid for non-negative x, y, which are not both zero. If we replace x = at and y = \u2211 t \u2212 1 i = 1 ai, we get this for each t \u2265 1, at \u221a \u2211 t = 1 ai \u2264 2 \u221a \u221a t \u0445 i = 1ai \u2212 2 \u221a \u221a \u221a s = 1ai. If we sum up the above inequality over all t = 1, 2,..., T, the right side telescope 2 \u0445 T = 1 at."}, {"heading": "Appendix E. Proofs for Scale-Free Mirror Descent", "text": "The proof (proof of Lemma 2). \u2212 vz = 1 = 1 = 1 = 1 (vz). \u2212 vz = 1 (vz). \u2212 vz = 1 (vz). \u2212 vz (vz). \u2212 vz (vz) \u2212 vz (vz). \u2212 vz (vz). \u2212 vz (vz). \u2212 vz (vz). \u2212 vz (vz). \u2212 vz (vz) \u2212 vz (vz). \u2212 vz (vz) \u2212 vz (vz). \u2212 vz (vz). \u2212 vz (vz). \u2212 vz (vz)."}, {"heading": "Appendix F. Lower Bound Proof", "text": "The proof (proof of theorem 5). Pick x, y-K in such a way that it does not get to the point where there is a reduction. Since it is compact, there are \"V\" variables, i.e. Pr [Zt = + 1] = Pr [Zt = \u2212 1] = 1 / 2. Let's be Z1, Z2,.., ZT so compact that there are \"V\" variables, i.e. Pr [Zt = + 1] = 1 / 2. Let's call \"T\" = Ztat. Quite clearly \"T\" = at. The problem will emerge if we show that (4) holds with positive probability. We show a stronger statement that inequality holds in expectation, i.e., \"E.\""}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>We design and analyze algorithms for online linear optimization that have opti-<lb>mal regret and at the same time do not need to know any upper or lower bounds<lb>on the norm of the loss vectors. Our algorithms are instances of the Follow the<lb>Regularized Leader (FTRL) and Mirror Descent (MD) meta-algorithms. We<lb>achieve adaptiveness to the norms of the loss vectors by scale invariance, i.e.,<lb>our algorithms make exactly the same decisions if the sequence of loss vectors is<lb>multiplied by any positive constant. The algorithm based on FTRL works for<lb>any decision set, bounded or unbounded. For unbounded decisions sets, this is<lb>the first adaptive algorithm for online linear optimization with a non-vacuous<lb>regret bound. In contrast, we show lower bounds on scale-free algorithms based<lb>on MD on unbounded domains.", "creator": "LaTeX with hyperref package"}}}