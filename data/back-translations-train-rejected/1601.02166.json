{"id": "1601.02166", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2016", "title": "Empirical Gaussian priors for cross-lingual transfer learning", "abstract": "Sequence model learning algorithms typically maximize log-likelihood minus the norm of the model (or minimize Hamming loss + norm). In cross-lingual part-of-speech (POS) tagging, our target language training data consists of sequences of sentences with word-by-word labels projected from translations in $k$ languages for which we have labeled data, via word alignments. Our training data is therefore very noisy, and if Rademacher complexity is high, learning algorithms are prone to overfit. Norm-based regularization assumes a constant width and zero mean prior. We instead propose to use the $k$ source language models to estimate the parameters of a Gaussian prior for learning new POS taggers. This leads to significantly better performance in multi-source transfer set-ups. We also present a drop-out version that injects (empirical) Gaussian noise during online learning. Finally, we note that using empirical Gaussian priors leads to much lower Rademacher complexity, and is superior to optimally weighted model interpolation.", "histories": [["v1", "Sat, 9 Jan 2016 23:34:05 GMT  (11kb)", "http://arxiv.org/abs/1601.02166v1", "Presented at NIPS 2015 Workshop on Transfer and Multi-Task Learning"]], "COMMENTS": "Presented at NIPS 2015 Workshop on Transfer and Multi-Task Learning", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["anders s{\\o}gaard"], "accepted": false, "id": "1601.02166"}, "pdf": {"name": "1601.02166.pdf", "metadata": {"source": "CRF", "title": "Empirical Gaussian priors for cross-lingual transfer learning", "authors": ["Anders S\u00f8gaard"], "emails": ["soegaard@hum.ku.dk"], "sections": [{"heading": null, "text": "ar Xiv: 160 1.02 166v 1 [cs.C L] 9Sequence model learning algorithms typically maximize the log probability minus the standard of the model (or minimize hamming loss + standard). In lingual Part-of-Speech (POS) tagging, our training data consists of sequences of sentences with word-for-word markings projected over word alignments by translations into k-languages for which we have labeled data. Therefore, our training data is very loud, and when the complexity of Rademacher is high, learning algorithms tend to overmatch. Norm-based regulation assumes a constant width and mean. Instead, we suggest using the k-source language models to estimate the parameters of a Gaussian predecessor for learning new POS taggers. This leads to significantly better performance in multi-source transfer set learning, which we present clearly during a raid-out, which we also present via a drone-out."}, {"heading": "1 Cross-lingual transfer learning of sequence models", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "2 Empirical Gaussian priors", "text": "We will consider empirical Gaussian regulators as Lk-regulated CRFs as L2-CRFs. We will apply empirical Gaussian regulators to the linear chains of conditional random fields (CRFs) that are only applied in practice. Lafferty et al. [2001]) and averaged structured perceptrons [Collins, 2002]. Linear-chain CRFs are therefore associated with the sequences of discrete input symbols x = x1,., xn by maximizing the conditional log probability of the described sequences LL (w, D) = averaged sequences LL (x, y > D logP (y, x) with the sequences of discrete input symbols x = x1, xn with sequences of discrete labels y = y1,.,., yn-regulated CRFs maximize LL (w, D) \u2212 w | k with typically k symbols x = 1, all sequences y = 1."}, {"heading": "2.1 Empirical Gaussian noise injection", "text": "We also introduce a drop-out variant of empirical Gaussian Priors. Our starting point is average structured perception. We implement empirical Gaussian noise injection with Gaussian characteristics < (\u00b51, \u03c31),..., (\u00b5m, \u03c3m) > for m characteristics as follows. We inject our model parameters with the means \u00b5j. For each instance we pass over, we extract a corruption vector g of random values vi from the corresponding Gaussians (1, \u03c3i). We inject noise into g by pairing multiplications of g and our characteristic representations of the input sequence with the relevant identification sequences. Note that this drop-out algorithm is parameter-free, but of course we could easily throw in a hyper parameter that controls the degree of regulation. We enter the algorithm in lt. & lt. & lt; < 1. < 1."}, {"heading": "3 Cross-lingual POS Experiments", "text": "In our multilingual transfer facility, we have agreed on 15 multilingual language models to estimate our primates."}, {"heading": "4 Observations", "text": "We make the following additional observations: (i) Following the approach in Zhu et al. [2009], we can calculate the Rademacher complexity of our models, i.e. their ability to learn noise in the labels (overfit). Sampling POS tags randomly from a uniform distribution, the random complexity is 0.083. With small sample sizes, L2-CRFs actually begin to learn patterns with the Rademacher complexity, which increases to 0.086, while both L2-PRIOR and EMPGAUSS never match better than chance. (ii) Geman et al. [1992] present a simple approach to explicitly investigate bias variance compromises during learning. They draw subsamples from l < m training point D1,. Dk and use a validation dataset of m \u2032 data to define the integrated variance of our methods."}], "references": [{"title": "If all you have is a bit of the Bible: Learning POS taggers for truly low-resource languages", "author": ["Zeljko Agic", "Dirk Hovy", "Anders S\u00f8gaard"], "venue": "In ACL,", "citeRegEx": "Agic et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agic et al\\.", "year": 2015}, {"title": "An introduction to empirical Bayes data analysis", "author": ["George Casella"], "venue": "American Statistician,", "citeRegEx": "Casella.,? \\Q1985\\E", "shortCiteRegEx": "Casella.", "year": 1985}, {"title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms", "author": ["Michael Collins"], "venue": "In EMNLP,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["Janez Demsar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Demsar.,? \\Q2006\\E", "shortCiteRegEx": "Demsar.", "year": 2006}, {"title": "Neural networks and the bias/variance dilemma", "author": ["Stuart Geman", "Elie Bienenstock", "Rene Doursat"], "venue": "Neural Computation,", "citeRegEx": "Geman et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1992}, {"title": "Conditional random fields: probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira"], "venue": "In ICML,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Wiki-ly supervised part-of-speech tagging", "author": ["Shen Li", "Jo\u00e3o Gra\u00e7a", "Ben Taskar"], "venue": "In EMNLP,", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Using universal linguistic knowledge to guide grammar induction", "author": ["Tahira Naseem", "Harr Chen", "Regina Barzilay", "Mark Johnson"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Naseem et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Naseem et al\\.", "year": 2010}, {"title": "Regularisation techniques for conditional random fields: Parameterised versus parameter-free", "author": ["Andrew Smith", "Miles Osborne"], "venue": "In IJCNLP,", "citeRegEx": "Smith and Osborne.,? \\Q2005\\E", "shortCiteRegEx": "Smith and Osborne.", "year": 2005}, {"title": "Zipfian corruptions for robust pos tagging", "author": ["Anders S\u00f8gaard"], "venue": "In Proceedings of NAACL,", "citeRegEx": "S\u00f8gaard.,? \\Q2013\\E", "shortCiteRegEx": "S\u00f8gaard.", "year": 2013}, {"title": "Inverted indexing for cross-lingual nlp", "author": ["Anders S\u00f8gaard", "\u017deljko Agi\u0107", "H\u00e9ctor Mart\u0131\u0301nez Alonso", "Barbara Plank", "Bernd Bohnet", "Anders Johannsen"], "venue": "In ACL,", "citeRegEx": "S\u00f8gaard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "S\u00f8gaard et al\\.", "year": 2015}, {"title": "Human Rademacher complexity", "author": ["Jerry Zhu", "Timothy Rogers", "Bryan Gibson"], "venue": "In NIPS,", "citeRegEx": "Zhu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "Some work on unsupervised POS tagging has assumed other resources such as tag dictionaries [Li et al., 2012], but such resources are also only available for a limited number of languages.", "startOffset": 91, "endOffset": 108}, {"referenceID": 7, "context": "Unsupervised learning with typologically informed priors [Naseem et al., 2010] is an interesting approach to unsupervised POS induction that is more applicable to low-resource languages.", "startOffset": 57, "endOffset": 78}, {"referenceID": 4, "context": "In other words, we want a model with higher integrated bias and lower integrated variance [Geman et al., 1992].", "startOffset": 90, "endOffset": 110}, {"referenceID": 2, "context": "[2001]) and averaged structured perceptrons [Collins, 2002].", "startOffset": 44, "endOffset": 59}, {"referenceID": 4, "context": "We will apply empirical Gaussian priors to linear-chain conditional random fields (CRFs; Lafferty et al. [2001]) and averaged structured perceptrons [Collins, 2002].", "startOffset": 89, "endOffset": 112}, {"referenceID": 1, "context": "In empirical Bayes [Casella, 1985], the parameters are learned from D itself.", "startOffset": 19, "endOffset": 34}, {"referenceID": 7, "context": ", S\u00f8gaard [2013]. If these parameters are assumed to be constant, the above objective becomes equivalent to L2-regularization.", "startOffset": 2, "endOffset": 17}, {"referenceID": 1, "context": "In empirical Bayes [Casella, 1985], the parameters are learned from D itself. Smith and Osborne [2005] suggest learning the parameters from a validation set.", "startOffset": 20, "endOffset": 103}, {"referenceID": 0, "context": "We use a subset of the data in [Agic et al., 2015].", "startOffset": 31, "endOffset": 50}, {"referenceID": 9, "context": "We learned these embeddings using an improvement over the technique suggested in S\u00f8gaard et al. [2015]. S\u00f8gaard et al.", "startOffset": 81, "endOffset": 103}, {"referenceID": 10, "context": "While this approach assumes fewer resources available, published results suggest that such representations are superior to previous work [S\u00f8gaard et al., 2015].", "startOffset": 137, "endOffset": 159}, {"referenceID": 3, "context": "We compute significance using Wilcoxon over datasets following Demsar [2006] and mark p < 0.", "startOffset": 63, "endOffset": 77}, {"referenceID": 10, "context": "We make the following additional observations: (i) Following the procedure in Zhu et al. [2009], we can compute the Rademacher complexity of our models, i.", "startOffset": 78, "endOffset": 96}, {"referenceID": 4, "context": "(ii) Geman et al. [1992] present a simple approach to explicitly studying bias-variance trade-offs during learning.", "startOffset": 5, "endOffset": 25}], "year": 2016, "abstractText": "Sequence model learning algorithms typically maximize log-likelihood minus the norm of the model (or minimize Hamming loss + norm). In cross-lingual part-ofspeech (POS) tagging, our target language training data consists of sequences of sentences with word-by-word labels projected from translations in k languages for which we have labeled data, via word alignments. Our training data is therefore very noisy, and if Rademacher complexity is high, learning algorithms are prone to overfit. Norm-based regularization assumes a constant width and zero mean prior. We instead propose to use the k source language models to estimate the parameters of a Gaussian prior for learning new POS taggers. This leads to significantly better performance in multi-source transfer set-ups. We also present a drop-out version that injects (empirical) Gaussian noise during online learning. Finally, we note that using empirical Gaussian priors leads to much lower Rademacher complexity, and is superior to optimally weighted model interpolation. 1 Cross-lingual transfer learning of sequence models The people of the world speak about 6,900 different languages. Open-source off-the-shelf natural language processing (NLP) toolboxes like OpenNLP1 and CoreNLP2 cover only 6\u20137 languages, and we have sufficient labeled training data for inducing models for about 20\u201330 languages. In other words, supervised sequence learning algorithms are not sufficient to induce POS models for but a small minority of the world\u2019s languages. What can we do for all the languages for which no training data is available? Unsupervised POS induction algorithms have methodological problems (in-sample evaluation, community-wide hyperparameter tuning, etc.), and performance is prohibitive of downstream applications. Some work on unsupervised POS tagging has assumed other resources such as tag dictionaries [Li et al., 2012], but such resources are also only available for a limited number of languages. In our experiments, we assume that no training data or tag dictionaries are available. Our only assumption is a bit of text translated into multiple languages, specifically, fragments of the Bible. We will use Bible data for annotation projection, as well as for learning cross-lingual word embeddings (\u00a73). Unsupervised learning with typologically informed priors [Naseem et al., 2010] is an interesting approach to unsupervised POS induction that is more applicable to low-resource languages. Our work is related to this work, but we learn informed priors rather than stipulate them and combine these priors with annotation projection (learning from noisy labels) rather than unsupervised learning. https://opennlp.apache.org/ http://nlp.stanford.edu/software/corenlp.shtml", "creator": "LaTeX with hyperref package"}}}