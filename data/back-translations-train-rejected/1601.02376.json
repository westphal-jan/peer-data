{"id": "1601.02376", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2016", "title": "Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction", "abstract": "Predicting user responses, such as click-through rate and conversion rate, are critical in many web applications including web search, personalised recommendation, and online advertising. Different from continuous raw features that we usually found in the image and audio domains, the input features in web space are always of multi-field and are mostly discrete and categorical while their dependencies are little known. Major user response prediction models have to either limit themselves to linear models or require manually building up high-order combination features. The former loses the ability of exploring feature interactions, while the latter results in a heavy computation in the large feature space. To tackle the issue, we propose two novel models using deep neural networks (DNNs) to automatically learn effective patterns from categorical feature interactions and make predictions of users' ad clicks. To get our DNNs efficiently work, we propose to leverage three feature transformation methods, i.e., factorisation machines (FMs), restricted Boltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper presents the structure of our models and their efficient training algorithms. The large-scale experiments with real-world data demonstrate that our methods work better than major state-of-the-art models.", "histories": [["v1", "Mon, 11 Jan 2016 10:04:40 GMT  (3075kb)", "http://arxiv.org/abs/1601.02376v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["weinan zhang", "tianming du", "jun wang"], "accepted": false, "id": "1601.02376"}, "pdf": {"name": "1601.02376.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Weinan Zhang", "Tianming Du", "Jun Wang"], "emails": ["w.zhang@cs.ucl.ac.uk,", "j.wang@cs.ucl.ac.uk,", "dutianming@quicloud.cn"], "sections": [{"heading": null, "text": "ar Xiv: 160 1,02 376v 1 [cs.L G] 11 Jan 20"}, {"heading": "1 Introduction", "text": "\"It's as if we've gone around the world in the region,\" he says. \"It's as if we've gone around the world in the region,\" he says. \"It's as if we've gone around the world in the region,\" he says. \"It's as if we've gone around the world in the region,\" he says."}, {"heading": "2 Related Work", "text": "Most current models use a logistic regression that ranges from the original categorical features to a one-time encryption to the features mentioned. [15] The inclusion of very large features in the categorical vectors is necessary to design the data. [15] The inclusion of these features in the categorical vectors proves necessary as it will increase the effectiveness and efficiency of the systems. [15] The inclusion in the categorical vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector"}, {"heading": "3 DNNs for CTR Estimation given Categorical Features", "text": "In this section, we will discuss the two proposed DNN architectures in detail, namely Factorisation-machine supported Neural Networks (FNN) and Sampling-based Neural Networks (SNN). Categorical input characteristics are encoded uniformly by field. For each field, e.g. city, there are several units, each representing a specific value of this field, e.g. City = London, and there is only one positive (1) unit, while all others are negative (0). The coded characteristics, which are referred to as x, are the input of many CTR estimation models [32, 26] as well as our DNN models as those shown in the lower layer of Figure 1."}, {"heading": "3.1 Factorisation-machine supported Neural Networks (FNN)", "text": "In this year it has reached the point where it will be able to put itself at the forefront, \"he said in an interview with the\" World on Sunday, \"in which it deals with the\" world, \"the\" world, \"the\" world, \"the\" world, \"the\" world, \"the\" world, \"the\" world, \"the\" world, \"the\" world, \"the\" world, \"the\" world, \"the\" world, \"the\" world, \"the\" world, \"the\" time, \"the\" world, \"the\" world, \"the\" world, \"the\" world, \"the\" the \"world,\" the \"the\" world, \"the\" the \"world,\" the \"the\" world, \"the\" the \"the\" world, \"the\" the \"world,\" the \"the\" the \"world,\" the \"the\" world, \"the\" the \"world,\" the \"the\" the \"world,\" the \"the\" the \"world,\" the \"the\" the \"world,\" the \"the\" the \"the\" the \"world,\" the \"the\" the \"the\" the \"the\" the \"world, the\" the \"the\" the \"the\" the \"the\", \"the\" the \"the\" the \"the\" the \"the\" the \"the\" world, \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"world,\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"world,\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"world, the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"world,\" the \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \""}, {"heading": "3.2 Sampling-based Neural Networks (SNN)", "text": "The structure of the second model SNN is shown in Figure 2 (a). The difference between SNN and FNN lies in the structure and the training method in the lower layer. SNN's lower layer is fully connected to the sigmoid activation function: z = sigmoid (W 0x + b0). (10) To initialize the weights of the lower layer, we have tried both a sampling-based RBM (RBM) [16] and a denoising auto-encoder (DAE) [4] in the preparation phase. To solve the arithmetic problem of the formation of large sparse one-hot encoding data, we propose a sampling-based RBM (Figure 2 (b), which is called SNN-RBM) and a sampling-based DAE in (Figure 2 (c), which is called SNN-DAE)."}, {"heading": "3.3 Regularisation", "text": "To prevent overadjustment, the widespread term L2 regularization is added to the loss function. For example, L2 regularization for FNN in Figure 1 is equivalent to [w] = | | W 0 | | 2 2 + 3 \u2211 l = 1 (| W l | | 2 2 2 + | bl | | 2 2). (11) On the other hand, dropout [35] is a technique that has become a popular and effective regularization technique for deep learning in recent years. We are also implementing this regularization and comparing it in our experiment."}, {"heading": "4 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experiment Setup", "text": "We evaluate our models based on the iPinYou dataset [27], a real-world public ad dataset with all ad information and corresponding click feedback from users. The data logs are organized by different advertisers and in a line-per-record format. In total, there are 19.50M data instances with 14.79K positive label (click). The functions for each data instance are all categorical. Feature examples in the ad log data are User Agent, partially masked IP, region, city, ad exchange, domain, URL, ad space ID, ad space visibility, ad space format, creative ID, user tags, etc. Following uniform encoding, the number of binary features is 937.67K in the entire dataset. We feed each comparison model with these binary data instances and the user click (1) and non-click (0) feedback as a label."}, {"heading": "4.2 Performance Comparison", "text": "Table 1 shows the results comparing LR, FM, FNN and SNN with RBM and PCS on 5 different advertisers and the entire dataset. We note that FM is not significantly better than LR, meaning that combination functions of 2-order may not be good enough to capture the underlying data patterns. AUC performance of the proposed FNN and SNN is better than the performance of LR and FM on all tested datasets. Based on the latent structure learned from FM, FNN learns other effective patterns between these latent characteristics and provides a consistent improvement over FM. The performance of SNN-DAE and SNN-RBM is generally consistent, i.e. the relative order of SNN results is almost identical."}, {"heading": "4.3 Hyperparameter Tuning", "text": "Due to the fact that deep neural networks contain many implementation details and a relatively large number of hyperparameters need to be adjusted, the following details show how we implement our models and tune hyperparameters in the models. We use stochastic gradient descent to learn most of our parameters for all proposed models. Regarding the selection of the number of training periods, we use an early stop [30], i.e. the training stops when the validation error increases. We try different learning rates from 1, 0.1, 0.01, 0.001, 0.001 to 0.0001, and select those selected with the optimal performance on the validation data.For negative unit sampling of SNN-RBM and SNN-DAE, we try the negative sample number m = 1, 2, and 4 per field, as described in Section 3.2, and find m = 2 the best results in most situations. For the activation functions of SNN-RBM function and SNN-DAE, we try the optimum SampO function and Sampling-O (we often depend on Sampling-E and Sampling-E)."}, {"heading": "4.4 Architecture Selection", "text": "In our models, we examine architectures with 3, 4 and 5 hidden layers by fixing all layer sizes, and find that the architecture with 3 hidden layers (i.e. 5 layers in total) is the best in terms of AUC performance. However, the range of choice of their layer sizes is exponential in the number of hidden layers. Instead of trying out all combinations of hidden layers, we use a different strategy in our experiment by tuning the different hidden layer sizes with the same number of hidden layers in all three hidden layers 5, since architecture with equally large hidden layers is empirically better than architecture with increasing width or decreasing width in [24]. For this reason, we start tuning the layer sizes with the same hidden layer sizes in all three hidden layers. 5 Apart from the fact that architecture with equally large hidden layers is better in width than architecture with increasing width [24]."}, {"heading": "4.5 Regularisation Comparison", "text": "For DNN models, we compared L2 regularization (Equation (11)) and dropout [35] to prevent complex co-adaptation of training data. The drop-out rate implemented in this experiment refers to the probability that each unit is active. [5] Some advanced Bayesian methods of hyperparameter adjustment [34] are not considered in this paper and can be studied in future work. Figure 4 (a) shows the comparative AUC performance of SNN-RBM, which is regulated by L2 standard and drop-out. It is obvious that drop-out exceeds L2 in all comparative settings. The reason why drop-out is more effective is that when feeding each training case, each hidden unit is pochastically excluded from the network, with a probability of drop-out rate, i.e. each training case can be considered a new case, with these models being considered more effective than average [5]."}, {"heading": "4.6 Analysis of Parameters", "text": "As a summary of sections 4.4 and 4.5, there are therefore two important parameters for both FNN and SNN that should be adjusted to make the model more effective: (i) the parameters of the layer size determine the architecture of the neural network and (ii) the parameters of the failure rate change the generalization capability on all datasets compared to neural networks only with L2 regulation. Figures 4 (b) and 4 (c) show how the AUC performance changes with the increase in the failure rate for both FNN and SNN. We can see that in both models there is an upward trend in performance at the beginning and then a steep decline with a continuous rate. The distinction between two models is the different sensitivity of the failure. Figure 4 (c) shows that the SNN model is more sensitive to the failure rate."}, {"heading": "5 Conclusion", "text": "In this paper, we investigated the potential of deep neural network (DNN) training to predict users \"ad click response based on categorical features with multiple fields. To solve the problem of the computational complexity of high-dimensional discrete categorical features, we proposed two DNN models: field-by-field embedding of features with supervised factoring machine training and fully networked DNN with sample-based RBM and DAE unattended pretraining. These architectures and pre-training algorithms make our DNNs very efficient to train. Comprehensive experiments on a public dataset from the real world confirm that the proposed DNN models successfully learn the underlying data patterns and provide superior CTR estimation performance than other comparative models. The proposed models are very general and could allow a wide range of future work. For example, the proposed DNN models can be improved by using dynamics for second-order handling of DNN]."}], "references": [{"title": "High-level student modeling with machine learning", "author": ["J.E. Beck", "B.P. Woolf"], "venue": "Intelligent tutoring systems. pp. 584\u2013593. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning 2(1), 1\u2013127", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H Larochelle"], "venue": "NIPS 19, 153", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Y. Bengio", "L. Yao", "G. Alain", "P. Vincent"], "venue": "NIPS. pp. 899\u2013907", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine learning 24(2), 123\u2013140", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Computational advertising", "author": ["A.Z. Broder"], "venue": "SODA. vol. 8, pp. 992\u2013992", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR 12, 2493\u20132537", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion", "author": ["L. Deng", "O. Abdel-Hamid", "D. Yu"], "venue": "ICASSP. pp. 6669\u20136673. IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey of partially connected neural networks", "author": ["D. Elizondo", "E. Fiesler"], "venue": "International journal of neural systems 8(05n06), 535\u2013558", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.A. Manzagol", "P. Vincent", "S. Bengio"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological cybernetics 36(4), 193\u2013202", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1980}, {"title": "Web-scale bayesian clickthrough rate prediction for sponsored search advertising in microsoft\u2019s bing search engine", "author": ["T. Graepel", "J.Q. Candela", "T. Borchert", "R. Herbrich"], "venue": "ICML. pp. 13\u201320", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "Mohamed", "A.r.", "G. Hinton"], "venue": "ICASSP. pp. 6645\u20136649. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Idiot\u2019s bayesnot so stupid after all? International statistical review", "author": ["D.J. Hand", "K. Yu"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Practical lessons from predicting clicks on ads at facebook", "author": ["X. He", "J. Pan", "O. Jin", "T. Xu", "B. Liu", "T. Xu", "Y. Shi", "A. Atallah", "R. Herbrich", "S Bowers"], "venue": "ADKDD. pp. 1\u20139. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G. Hinton"], "venue": "Momentum 9(1), 926", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural computation 14(8), 1771\u20131800", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science 313(5786), 504\u2013507", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "CIKM. pp. 2333\u20132338", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "3 idiots approach for display advertising challenge", "author": ["Y.C. Juan", "Y. Zhuang", "W.S. Chin"], "venue": "Internet and Network Economics, pp. 254\u2013265. Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "On combining classifiers", "author": ["J. Kittler", "M. Hatef", "R.P. Duin", "J. Matas"], "venue": "PAMI 20(3), 226\u2013239", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic latent network visualization: inferring and embedding diffusion networks", "author": ["T. Kurashima", "T. Iwata", "N. Takaya", "H. Sawada"], "venue": "KDD. pp. 1236\u20131245. ACM", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploring strategies for training deep neural networks", "author": ["H. Larochelle", "Y. Bengio", "J. Louradour", "P. Lamblin"], "venue": "JMLR 10, 1\u201340", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature 521(7553)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimating conversion rate in display advertising from past performance data", "author": ["Lee", "K.c.", "B. Orten", "A. Dasdan", "W. Li"], "venue": "KDD. pp. 768\u2013776. ACM", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "ipinyou global rtb bidding algorithm competition dataset", "author": ["H. Liao", "L. Peng", "Z. Liu", "X. Shen"], "venue": "ADKDD. pp. 1\u20136. ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Ad click prediction: a view from the trenches", "author": ["H.B. McMahan", "G. Holt", "D. Sculley", "M. Young", "D. Ebner", "J. Grady", "L. Nie", "T. Phillips", "E. Davydov", "D Golovin"], "venue": "KDD. pp. 1222\u20131230. ACM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Predicting response in mobile advertising with hierarchical importance-aware factorization machine", "author": ["R.J. Oentaryo", "E.P. Lim", "D.J.W. Low", "D. Lo", "M. Finegold"], "venue": "WSDM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic early stopping using cross validation: quantifying the criteria", "author": ["L. Prechelt"], "venue": "Neural Networks 11(4), 761\u2013767", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Factorization machines with libfm", "author": ["S. Rendle"], "venue": "ACM TIST 3(3), 57", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Predicting clicks: estimating the clickthrough rate for new ads", "author": ["M. Richardson", "E. Dominowska", "R. Ragno"], "venue": "WWW. pp. 521\u2013530. ACM", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Y. Shen", "X. He", "J. Gao", "L. Deng", "G. Mesnil"], "venue": "CIKM", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "NIPS. pp. 2951\u20132959", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR 15(1), 1929\u20131958", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "ICML. pp. 1139\u20131147", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Line: Large-scale information network embedding", "author": ["J. Tang", "M. Qu", "M. Wang", "M. Zhang", "J. Yan", "Q. Mei"], "venue": "WWW. pp. 1067\u20131077", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Using boosted trees for click-through rate prediction for sponsored search", "author": ["I. Trofimov", "A. Kornetova", "V. Topinskiy"], "venue": "WINE. p. 2. ACM", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Click-through rate estimation for rare events in online advertising", "author": ["X. Wang", "W. Li", "Y. Cui", "R. Zhang", "J. Mao"], "venue": "Online Multimedia Advertising: Techniques and Technologies pp. 1\u201312", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "ICCV. pp. 2018\u20132025. IEEE", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimal real-time bidding for display advertising", "author": ["W. Zhang", "S. Yuan", "J. Wang"], "venue": "KDD. pp. 1077\u20131086. ACM", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Mariana: Tencent deep learning platform and its applications", "author": ["Y. Zou", "X. Jin", "Y. Li", "Z. Guo", "E. Wang", "B. Xiao"], "venue": "VLDB 7(13), 1772\u20131777", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": ", the probability that the user in a certain context will click a given ad [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 31, "context": "Sponsored search, contextual advertising, and the recently emerged realtime bidding (RTB) display advertising all heavily rely on the ability of learned models to predict ad click-through rates (CTR) [32,41].", "startOffset": 200, "endOffset": 207}, {"referenceID": 40, "context": "Sponsored search, contextual advertising, and the recently emerged realtime bidding (RTB) display advertising all heavily rely on the ability of learned models to predict ad click-through rates (CTR) [32,41].", "startOffset": 200, "endOffset": 207}, {"referenceID": 31, "context": "The applied CTR estimation models today are mostly linear, ranging from logistic regression [32] and naive Bayes [14] to FTRL logistic regression [28] and Bayesian probit regression [12], all of which are based on a huge number of sparse features with one-hot encoding [1].", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "The applied CTR estimation models today are mostly linear, ranging from logistic regression [32] and naive Bayes [14] to FTRL logistic regression [28] and Bayesian probit regression [12], all of which are based on a huge number of sparse features with one-hot encoding [1].", "startOffset": 113, "endOffset": 117}, {"referenceID": 27, "context": "The applied CTR estimation models today are mostly linear, ranging from logistic regression [32] and naive Bayes [14] to FTRL logistic regression [28] and Bayesian probit regression [12], all of which are based on a huge number of sparse features with one-hot encoding [1].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "The applied CTR estimation models today are mostly linear, ranging from logistic regression [32] and naive Bayes [14] to FTRL logistic regression [28] and Bayesian probit regression [12], all of which are based on a huge number of sparse features with one-hot encoding [1].", "startOffset": 182, "endOffset": 186}, {"referenceID": 0, "context": "The applied CTR estimation models today are mostly linear, ranging from logistic regression [32] and naive Bayes [14] to FTRL logistic regression [28] and Bayesian probit regression [12], all of which are based on a huge number of sparse features with one-hot encoding [1].", "startOffset": 269, "endOffset": 272}, {"referenceID": 11, "context": "independent raw features [12].", "startOffset": 25, "endOffset": 29}, {"referenceID": 28, "context": "For example, factorisation machines (FMs) [29] map the user and item binary features into a low dimensional continuous space.", "startOffset": 42, "endOffset": 46}, {"referenceID": 37, "context": "Gradient boosting trees [38] automatically learn feature combinations while growing each decision/regression tree.", "startOffset": 24, "endOffset": 28}, {"referenceID": 19, "context": "However, these models cannot make use of all possible combinations of different features [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "Another problem of the mainstream ad CTR estimation models is that most prediction models have shallow structures and have limited expression to model the underlying patterns from complex and massive data [15].", "startOffset": 205, "endOffset": 209}, {"referenceID": 24, "context": "Deep learning [25] has become successful in computer vision [22], speech recognition [13], and natural language processing (NLP) [19,33] during recent five years.", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "Deep learning [25] has become successful in computer vision [22], speech recognition [13], and natural language processing (NLP) [19,33] during recent five years.", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "Deep learning [25] has become successful in computer vision [22], speech recognition [13], and natural language processing (NLP) [19,33] during recent five years.", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "Deep learning [25] has become successful in computer vision [22], speech recognition [13], and natural language processing (NLP) [19,33] during recent five years.", "startOffset": 129, "endOffset": 136}, {"referenceID": 32, "context": "Deep learning [25] has become successful in computer vision [22], speech recognition [13], and natural language processing (NLP) [19,33] during recent five years.", "startOffset": 129, "endOffset": 136}, {"referenceID": 17, "context": "As visual, aural, and textual signals are known to be spatially and/or temporally correlated, the newly introduced unsupervised training on deep structures [18] would be able to explore such local dependency and establish a dense representation of the feature space, making neural network models effective in learning high-order features directly from the raw feature input.", "startOffset": 156, "endOffset": 160}, {"referenceID": 30, "context": "Specifically, FNN with a supervised-learning embedding layer using factorisation machines [31] is proposed to efficiently reduce the dimension from sparse features to dense continuous features.", "startOffset": 90, "endOffset": 94}, {"referenceID": 41, "context": ", [42]), there is no detail of the models or implementation.", "startOffset": 2, "endOffset": 6}, {"referenceID": 38, "context": "Click-through rate, defined as the probability of the ad click from a specific user on a displayed ad, is essential in online advertising [39].", "startOffset": 138, "endOffset": 142}, {"referenceID": 25, "context": "The majority of current models use logistic regression based on a set of sparse binary features converted from the original categorical features via one-hot encoding [26,32].", "startOffset": 166, "endOffset": 173}, {"referenceID": 31, "context": "The majority of current models use logistic regression based on a set of sparse binary features converted from the original categorical features via one-hot encoding [26,32].", "startOffset": 166, "endOffset": 173}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "Various methods of embedding architectures have been proposed [37,23].", "startOffset": 62, "endOffset": 69}, {"referenceID": 22, "context": "Various methods of embedding architectures have been proposed [37,23].", "startOffset": 62, "endOffset": 69}, {"referenceID": 30, "context": "Factorisation machine (FM) [31], originally proposed for collaborative filtering recommendation, is regarded as one of the most successful embedding models.", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "Deep Learning [2] is a branch of artificial intelligence research that attempts to develop the techniques that will allow computers to handle complex tasks such as recognition and prediction at high performance.", "startOffset": 14, "endOffset": 17}, {"referenceID": 39, "context": "DNNs have been successfully applied in computer vision [40], speech recognition [8] and natural language processing (NLP) [7,19,33].", "startOffset": 55, "endOffset": 59}, {"referenceID": 7, "context": "DNNs have been successfully applied in computer vision [40], speech recognition [8] and natural language processing (NLP) [7,19,33].", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "DNNs have been successfully applied in computer vision [40], speech recognition [8] and natural language processing (NLP) [7,19,33].", "startOffset": 122, "endOffset": 131}, {"referenceID": 18, "context": "DNNs have been successfully applied in computer vision [40], speech recognition [8] and natural language processing (NLP) [7,19,33].", "startOffset": 122, "endOffset": 131}, {"referenceID": 32, "context": "DNNs have been successfully applied in computer vision [40], speech recognition [8] and natural language processing (NLP) [7,19,33].", "startOffset": 122, "endOffset": 131}, {"referenceID": 9, "context": "Furthermore, with the help of unsupervised pre-training, we can get good feature representation which guides the learning towards basins of attraction of minima that support better generalisation from the training data [10].", "startOffset": 219, "endOffset": 223}, {"referenceID": 17, "context": "Usually, these deep models have two stages in learning [18]: the first stage performs model initialisation via unsupervised learning (i.", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "Compared with the word-embedding techniques used in NLP [19,33], our models deal with more general multi-field categorical features without any assumed data structures such as word alignment and letter-n-gram etc.", "startOffset": 56, "endOffset": 63}, {"referenceID": 32, "context": "Compared with the word-embedding techniques used in NLP [19,33], our models deal with more general multi-field categorical features without any assumed data structures such as word alignment and letter-n-gram etc.", "startOffset": 56, "endOffset": 63}, {"referenceID": 31, "context": "The encoded features, denoted as x, are the input of many CTR estimation models [32,26] as well as our DNN models, as depicted at the bottom layer of Figure 1.", "startOffset": 80, "endOffset": 87}, {"referenceID": 25, "context": "The encoded features, denoted as x, are the input of many CTR estimation models [32,26] as well as our DNN models, as depicted at the bottom layer of Figure 1.", "startOffset": 80, "endOffset": 87}, {"referenceID": 0, "context": ", W i0[0] is initialised by wi, W i 0[1] is initialised by v 1 i , W i 0[2] is initialised by v i , etc.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": ", W i0[0] is initialised by wi, W i 0[1] is initialised by v 1 i , W i 0[2] is initialised by v i , etc.", "startOffset": 72, "endOffset": 75}, {"referenceID": 30, "context": "In this way, z vector of the first layer is initialised as shown in Figure 1 via training a factorisation machine (FM) [31]:", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "The idea using FM in the bottom layer is ignited by Convolutional Neural Networks (CNNs) [11], which exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers.", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "However, according to [21], if the observational discriminatory information is highly ambiguous (which is true in our case for ad click behaviour), the posterior weights (from DNN) will not deviate dramatically from the prior (FM).", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "Furthermore, the weights in hidden layers (except the FM layer) are initialised by layer-wise RBM pre-training [3] using contrastive divergence [17], which effectively preserves the information in input dataset as detailed in [18,16].", "startOffset": 111, "endOffset": 114}, {"referenceID": 16, "context": "Furthermore, the weights in hidden layers (except the FM layer) are initialised by layer-wise RBM pre-training [3] using contrastive divergence [17], which effectively preserves the information in input dataset as detailed in [18,16].", "startOffset": 144, "endOffset": 148}, {"referenceID": 17, "context": "Furthermore, the weights in hidden layers (except the FM layer) are initialised by layer-wise RBM pre-training [3] using contrastive divergence [17], which effectively preserves the information in input dataset as detailed in [18,16].", "startOffset": 226, "endOffset": 233}, {"referenceID": 15, "context": "Furthermore, the weights in hidden layers (except the FM layer) are initialised by layer-wise RBM pre-training [3] using contrastive divergence [17], which effectively preserves the information in input dataset as detailed in [18,16].", "startOffset": 226, "endOffset": 233}, {"referenceID": 30, "context": "The initial weights for FMs are trained by stochastic gradient descent (SGD), as detailed in [31].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "To initialise the weights of the bottom layer, we tried both restricted Boltzmann machine (RBM) [16] and denoising auto-encoder (DAE) [4] in the pretraining stage.", "startOffset": 96, "endOffset": 100}, {"referenceID": 3, "context": "To initialise the weights of the bottom layer, we tried both restricted Boltzmann machine (RBM) [16] and denoising auto-encoder (DAE) [4] in the pretraining stage.", "startOffset": 134, "endOffset": 137}, {"referenceID": 16, "context": "With the sampled units, we can train an RBM via contrastive divergence [17] and a DAE via SGD with unsupervised approaches to largely reduce the data", "startOffset": 71, "endOffset": 75}, {"referenceID": 34, "context": "On the other hand, dropout [35] is a technique which becomes a popular and effective regularisation technique for deep learning during the recent years.", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "We evaluate our models based on iPinYou dataset [27], a public realworld display ad dataset with each ad display information and corresponding user click feedback.", "startOffset": 48, "endOffset": 52}, {"referenceID": 31, "context": "LR: Logistic Regression [32] is a linear model with simple implementation and fast training speed, which is widely used in online advertising estimation.", "startOffset": 24, "endOffset": 28}, {"referenceID": 30, "context": "FM: Factorisation Machine [31] is a non-linear model able to estimate feature interactions even in problems with huge sparsity.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "The AUC [12] metric is a widely used measure for evaluating the CTR performance.", "startOffset": 8, "endOffset": 12}, {"referenceID": 29, "context": "Regarding selecting the number of training epochs, we use early stopping [30], i.", "startOffset": 73, "endOffset": 77}, {"referenceID": 23, "context": "Instead of trying all combinations of hidden units, in our experiment we use another strategy by starting tuning the different hidden layer sizes with the same number of hidden units in all three hidden layers since the architecture with equal-size hidden layers is empirically better than the architecture with increasing width or decreasing width in [24].", "startOffset": 352, "endOffset": 356}, {"referenceID": 34, "context": "(11)) and dropout [35] for preventing complex co-adaptations on the training data.", "startOffset": 18, "endOffset": 22}, {"referenceID": 33, "context": "5 Some advanced Bayesian methods for hyperparameter tuning [34] are not considered in this paper and may be investigated in the future work.", "startOffset": 59, "endOffset": 63}, {"referenceID": 4, "context": ", each training case can be regarded as a new model and these models are averaged as a special case of bagging [5], which effectively improves the generalisation ability of DNN models.", "startOffset": 111, "endOffset": 114}, {"referenceID": 35, "context": "For example, the model performance can be improved by momentum methods in that it suffices for handling the curvature problems in DNN training objectives without using complex second-order methods [36].", "startOffset": 197, "endOffset": 201}, {"referenceID": 8, "context": "In addition, the partial connection in the bottom layer could be extended to higher hidden layers as partial connectivities have many advantages such as lower complexity, higher generalisation ability and more similar to human brain [9].", "startOffset": 233, "endOffset": 236}], "year": 2016, "abstractText": "Predicting user responses, such as click-through rate and conversion rate, are critical in many web applications including web search, personalised recommendation, and online advertising. Different from continuous raw features that we usually found in the image and audio domains, the input features in web space are always of multi-field and are mostly discrete and categorical while their dependencies are little known. Major user response prediction models have to either limit themselves to linear models or require manually building up high-order combination features. The former loses the ability of exploring feature interactions, while the latter results in a heavy computation in the large feature space. To tackle the issue, we propose two novel models using deep neural networks (DNNs) to automatically learn effective patterns from categorical feature interactions and make predictions of users\u2019 ad clicks. To get our DNNs efficiently work, we propose to leverage three feature transformation methods, i.e., factorisation machines (FMs), restricted Boltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper presents the structure of our models and their efficient training algorithms. The large-scale experiments with real-world data demonstrate that our methods work better than major state-of-the-art models.", "creator": "LaTeX with hyperref package"}}}