{"id": "1601.02705", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2016", "title": "Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal Embedding", "abstract": "There is a large variety of objects and appliances in human environments, such as stoves, coffee dispensers, juice extractors, and so on. It is challenging for a roboticist to program a robot for each of these object types and for each of their instantiations. In this work, we present a novel approach to manipulation planning based on the idea that many household objects share similarly-operated object parts. We formulate the manipulation planning as a structured prediction problem and learn to transfer manipulation strategy across different objects by embedding point-cloud, natural language, and manipulation trajectory data into a shared embedding space using a deep neural network. In order to learn semantically meaningful spaces throughout our network, we introduce a method for pre-training its lower layers for multimodal feature embedding and a method for fine-tuning this embedding space using a loss-based margin. In order to collect a large number of manipulation demonstrations for different objects, we develop a new crowd-sourcing platform called Robobarista. We test our model on our dataset consisting of 116 objects and appliances with 249 parts along with 250 language instructions, for which there are 1225 crowd-sourced manipulation demonstrations. We further show that our robot with our model can even prepare a cup of a latte with appliances it has never seen before.", "histories": [["v1", "Tue, 12 Jan 2016 00:56:30 GMT  (7318kb,D)", "http://arxiv.org/abs/1601.02705v1", "Journal Version"]], "COMMENTS": "Journal Version", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["jaeyong sung", "seok hyun jin", "ian lenz", "ashutosh saxena"], "accepted": false, "id": "1601.02705"}, "pdf": {"name": "1601.02705.pdf", "metadata": {"source": "CRF", "title": "Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal Embedding", "authors": ["Jaeyong Sung", "Seok Hyun Jin", "Ian Lenz", "Ashutosh Saxena"], "emails": ["@cs.cornell.edu"], "sections": [{"heading": null, "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "II. RELATED WORK", "text": "In this case, it is a mere red herring."}, {"heading": "III. OUR APPROACH", "text": "Our goal is to develop an algorithm that will allow a robot to infer a tampering path when presented with a new object or device and its operating instructions in natural language. Intuition for our approach is that many differently shaped objects share similarly operated object parts; therefore, the tampering path of an object can be transferred to a completely different object when they share similarly operated parts. For example, the movement required to operate the handle of the espresso machine in Figure 2 is almost identical to the movement required to flush a urinal with a handle. By identifying and transferring paths from previous experiences with parts of other objects, robots can even manipulate objects they have not seen before. We formulate this problem as a structured prediction problem, as in Figure 2. Faced with a dot cloud for each part of an espresso machine and an instruction in natural language, such as \"Push the handle to add hot water,\" our problem presents an extremely complex algorithm of motion."}, {"heading": "A. Problem Formulation", "text": "\"Our goal is to learn a function f that can manipulate a certain pair of point-cloud p-P of an object part and a natural language instruction l-L to a trajectory \u03c4 T that can manipulate the object part as described by a free natural language l: f: P \u00b7 L \u2192 T (1) For example, the algorithm should output a manipulation path that correctly performs the task on the object part according to the instruction. Point-cloud representation. Each instance of a point-cloud p-P is represented as a series of n points in three-dimensional euclidean space where each point (x, y, g, b) is represented with its RGB color (r, g, b): p = {p (i)} ni = 1 = (x, y, z, g, b)."}, {"heading": "B. Data Pre-processing", "text": "We represent point cloud p of any length as an allocation grid in which each cell indicates whether a point lives in the space it represents. Since point cloud p consists only of the part of an object that is limited in size, we can represent p with two allocation grid-like structures of size 10 x 10 x 10 x 10 voxels with different scales: one with each voxel spanning a cube of 1 x 1 x 1 x 1 (cm), and the other with each cell representing 2.5 x 2.5 x 2.5 (cm). Each language lesson is presented as a fixed-size representation in which stopwords are removed. Finally, for each trajectory T we first calculate its smooth interpolated trajectory and then normalize all trajectories Ts to the same length, while \"maintaining the opening of the sequence.\""}, {"heading": "C. Direct manipulation trajectory transfer", "text": "Even if it is a mere formality, it is clear that it is a mere formality."}, {"heading": "IV. DEEP MULTIMODAL EMBEDDING", "text": "This is much more difficult than the uni-modal binary or multi-class classification / regression problems (e.g. image recognition [33]) to which deep learning has mostly been applied [6]. We could simply turn our problem into a binary classification problem, i.e. it requires evaluating the network on any combination of potential candidate manipulation and the given point cloud / language pair. \"Then we can use a multimodal feed-forward deep network to solve this problem [47]. However, this approach has several drawbacks. First, it requires evaluating the network on any combination of potential candidate manipulation and the given point cloud / language pair, which is computationally expensive. Second, this method does nothing to handle noisy labels, a significant problem when dealing with crowdsourcing data, as we are doing here. Finally, while this method is able to produce reasonable results for our problem, we show in Section VIII a problem that we can improve in principle."}, {"heading": "A. Model", "text": "We use two separate multi-layer, deep neural networks, one for \u03a6P, L (p, l) and one for \u03a6T (\u03c4). Let's take Np as the size of point-cloud input p, Nl as similar to natural speech input l, N1, p and N1, l as the number of hidden units in the first hidden layers projected by point-cloud and natural speech characteristics, and N2, pl as the number of hidden units in the combined point-cloud / language layer. With W's as network weights, which are the learned parameters of our system, and a (\u00b7) as a reflected linear unit (ReLU) activation function [78], our model for projecting point-cloud and speech characteristics onto the common embedding h3 is as follows: h1, pi = a (..."}, {"heading": "V. LEARNING JOINT POINT-CLOUD/LANGUAGE/TRAJECTORY MODEL", "text": "The main challenge of our work is to learn a model that converts three different modalities - dot clouds, natural language, and trajectories - into a single semantically meaningful space. We present a method that learns a common point-cloud / language / trajectory space in such a way that all trajectories relevant to a given task (dot-cloud / language combination) should be more similar to the projection of that task than a task-irrelevant trajectory. Among these irrelevant trajectories, some may be less relevant than others, and should therefore be pushed farther. For example, in the face of a door knob that needs to be grasped normally at the door surface, and an instruction to rotate it clockwise, a trajectory approach to the door knobs that rotates counter-clockwise should be more similar to the task than one that approaches the button from a completely wrong angle and does not execute."}, {"heading": "A. Pre-training Joint Point-cloud/Language Model", "text": "A major advantage of modern deep learning methods is the use of unattended pre-training to initialize neural network parameters to a good starting point before final controlled fine-tuning. Our lower layers h2, pl and h2 represent properties derived exclusively from the combination of point clouds and language or trajectories. Our pre-training method initializes h2, pl and h2 as semantically significant embedding spaces similar to h3 as shown later in Section VIII-C. First, we train the layers leading to these layers with spared de-noising autoencoders [71, 78]. Then, our pre-training process h2, pl is similar to our approach to fine-tuning a semantically meaningful embedding space for h3."}, {"heading": "B. Pre-training Trajectory Model", "text": "For our task of deriving manipulation curves for novel objects, it is particularly important that similar curves are mapped to similar regions in the attribute space defined by h2, so that embedding h2, \u03c4 itself makes semantic sense and these, in turn, can be mapped to similar regions in h3. Standard pre-training methods such as the sparse denoising autoencoder [71, 78] would only pre-train h2 to reconstruct individual course curves. Instead, we use pre-training similar to Sec. V-A, except now we train only for a single modality - trajectory data. As shown on the right in Fig. 4, the layer embedding h2, is duplicated instead. These duplicated embedding layers are treated as if they were two different modalities, but all their weights are shared and updated simultaneously."}, {"heading": "C. Label Noise", "text": "If our data contains a significant number of noisy trajectories, e.g. due to crowd sourcing (Sec. VII), not all trajectories should be considered equally appropriate, as in Sec. VIII.For each pair of input factors (pi, li), we have Ti = {\u03c4i, 1, \u03c4i, 2,..., \u03c4i, ni}, a set of trajectories submitted by the mass for (pi, li). First, we assume that at least half of the mass has tried to give a reasonable demonstration of the trajectories. Thus, a demonstration with the lowest average trajectory distance to the others must be a good demonstration. We use the DTW-MT function (which will be described later in Sec) to give a reasonable demonstration of the trajectories. (Thus, a demonstration with the lowest average trajectory distance to all other demonstrations must be a good demonstration."}, {"heading": "VI. LOSS FUNCTION FOR MANIPULATION TRAJECTORY", "text": "For both learning and evaluation methods, we need a function that exactly measures the distance between two tracks i = a = a = a = a = a = a = a = a = a = c = c = c = c = c = c = c c = c c c = c c c = c c = c = c = c c = c = c = c = c = c = c = c = c = c = c = c = c = c c = c = c = c c = c c = c c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = = c = c = c = = c = c = = c = = c = c = = = c = c = = c = c = = = c = c = c = = c = c = c = c = c = c = c = c = c = c = c = = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c"}, {"heading": "VII. ROBOBARISTA: CROWD-SOURCING PLATFORM", "text": "In fact, most of them will be able to orient themselves in a different direction than in the other direction, namely the direction in which they are moving."}, {"heading": "VIII. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Robobarista Dataset", "text": "To test our model, we collected a dataset of 116 point clouds of objects with 249 object parts (examples shown in Figure 6) ranging from kitchen appliances such as cookers and rice cookers to bathroom appliances such as sinks and toilets. Figure 14 shows an example of 70 such 3We have taken care not to initialize it with trajectories from other folds in order to keep a 5-fold cross validation validated in the experimental section. In addition, there are a total of 250 instructions in natural language (in 155 manuals).4 Using the crowd sourcing platform Robobarista, we have collected 1225 trajectories for these objects from 71 incompetent users on Amazon Mechanical Turk. After a user has been shown a 20-second instructional video, the user first completes a 2-minute tutorial problem. At each session, the user was asked to complete 10 mappings for these objects, each consisting of an object and a manual following it."}, {"heading": "B. Baselines", "text": "We compared our model with many basic functions: 1) Random Transfers (Chance): Trajectories are not randomly selected from the series of trajectories in the training set. 2) Object Part Classifier: To test our hypothesis that the classification of object parts as an intermediate step is no guarantee of successful transfers, we first divided an object part into an object category (e.g. \"handle,\" knob \"), then used the same attribute space to find its nearest neighbor in the same class from the training set. Then, the trajectorization is transferred to p. 3) Structured Support Vector Machine (SSVM): We used SSVM to learn a discriminatory scoring function."}, {"heading": "C. Results and Discussions", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "D. Robotic Validation", "text": "To validate the concept of part-based manipulation of trajectory transmission in a real-world environment, we tested our algorithm on our PR2 robot. To ensure real-world transfers, we tested with four objects that the algorithm had never seen before - a coffee dispenser, coffee grinder, lamp, and espresso machine. The PR2 robot has two 7DoF arms, an omni-directional base, and many sensors, including a Microsoft Kinect, stereo cameras, and a tiltable laser scanner. For these experiments, a point cloud is purchased from the head-mounted Kinect sensor, and each move is performed on the specified arm using a Cartesian end-effect stiffness controller in ROS [51]. For each object, the robot is presented with a segmented point cloud, along with a natural text manual, with each step performed manually in connection with a segmented part of the point cloud."}, {"heading": "IX. CONCLUSION AND FUTURE WORK", "text": "In this paper, we present a novel approach to predicting manipulation paths by means of partial feedback, which allows robots to successfully manipulate objects they have never seen before. We formulate this as a structured output problem and approach the problem of deriving manipulation paths for novel objects by collectively embedding point cloud, natural language and trajectory data in a common space by using a deep neural network. We introduce a method for learning a common representation of multimodal data by using a new loss-increased cost function that learns a semantically meaningful embedding of data. We also introduce a method for prefabricating the lower layers of the network, learning embedding for subsets of modalities, and demonstrate that it exceeds standard pre-training algorithms. Learning such an embedding space enables efficient inferences by comparing the embedding of a new cloud with embedding demonstrations."}, {"heading": "ACKNOWLEDGMENT", "text": "We thank Joshua Reichler for creating the first prototype of the crowdsourcing platform. We thank Ross Knepper and Emin Gu \ufffd n Sirer for useful discussions. We thank NVIDIA Corporation for donating the Tesla K40 GPU used for this research. This work was supported by the NRI Prize 1426452, the ONR Prize N00014-14-1-0156 and by the Microsoft Faculty Fellowship and the NSF Career Award to one of us (Saxena)."}], "references": [{"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["P. Abbeel", "A. Coates", "A. Ng"], "venue": "International Journal of Robotics Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Instance-based learning algorithms", "author": ["D.W. Aha", "D. Kibler", "M.K. Albert"], "venue": "Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1991}, {"title": "Robot web tools [ros topics", "author": ["B. Alexander", "K. Hsiao", "C. Jenkins", "B. Suay", "R. Toris"], "venue": "Robotics & Automation Magazine, IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "A survey of robot learning from demonstration", "author": ["B. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "RAS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Method for registration of 3-d shapes. In Robotics-DL tentative, pages 586\u2013606", "author": ["P.J. Besl", "N.D. McKay"], "venue": "International Society for Optics and Photonics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "Learning to localize objects with structured output regression", "author": ["M. Blaschko", "C. Lampert"], "venue": "In ECCV", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Bakebot: Baking cookies with the pr2", "author": ["M. Bollini", "J. Barry", "D. Rus"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Examples of objects and object parts. Each image shows the point cloud representation of an object. We overlaid some of its parts by CAD models for online Robobarista crowd-sourcing platform. Note that the actual underlying point-cloud of object parts contains much more noise and is not clearly segmented, and none of the models have access to overlaid model for inferring manipulation trajectory.  Human and robot perception in large-scale learning from demonstration", "author": ["C. Crick", "S. Osentoski", "G. Jay", "O.C. Jenkins"], "venue": "In HRI. ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Semantic grasping: Planning robotic grasps functionally suitable for an object manipulation task", "author": ["H. Dang", "P.K. Allen"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Learning concurrent motor skills in versatile solution spaces", "author": ["C. Daniel", "G. Neumann", "J. Peters"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Learning a dictionary of prototypical grasp-predicting parts from grasping experience", "author": ["R. Detry", "C.H. Ek", "M. Madry", "D. Kragic"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Learning the dynamics of doors for robotic manipulation", "author": ["F. Endres", "J. Trinkle", "W. Burgard"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Transfer of object shape knowledge across visual and haptic modalities", "author": ["G. Erdogan", "I. Yildirim", "R.A. Jacobs"], "venue": "In Proceedings of the 36th Annual Conference of the Cognitive Science Society,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Robot programming by demonstration with crowdsourced action fixes", "author": ["M. Forbes", "M.J.-Y. Chung", "M. Cakmak", "R.P. Rao"], "venue": "In Second AAAI Conference on Human Computation and Crowdsourcing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "The ecological approach to visual perception", "author": ["J.J. Gibson"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1986}, {"title": "Object detection with grammar models", "author": ["R. Girshick", "P. Felzenszwalb", "D. McAllester"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxe", "H. Lee", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Deep belief net learning in a long-range vision system for autonomous offroad driving", "author": ["R. Hadsell", "A. Erkan", "P. Sermanet", "M. Scoffier", "U. Muller", "Y. LeCun"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Reducing the dimensionality of data with neural", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "networks. Science,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Contact-reactive grasping of objects with partial shape information", "author": ["K. Hsiao", "S. Chitta", "M. Ciocarlie", "E. Jones"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Discriminative deep metric learning for face verification in the wild", "author": ["J. Hu", "J. Lu", "Y.-P. Tan"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Learning to recognize human activities from soft labeled data", "author": ["N. Hu", "Z. Lou", "G. Englebienne", "B. Krse"], "venue": "In Proceedings of Robotics: Science and Systems, Berke-  ley,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera", "author": ["S. Izadi", "D. Kim", "O. Hilliges", "D. Molyneaux", "R. Newcombe", "P. Kohli"], "venue": "In ACM Symposium on UIST,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Learning preferences for manipulation tasks from online coactive feedback", "author": ["A. Jain", "B. Wojcik", "T. Joachims", "A. Saxena"], "venue": "In International Journal of Robotics Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Cutting-plane training of structural svms", "author": ["T. Joachims", "T. Finley", "C.-N.J. Yu"], "venue": "Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Interactive segmentation, tracking, and kinematic modeling of unknown 3d articulated objects", "author": ["D. Katz", "M. Kazemi", "J. Andrew Bagnell", "A. Stentz"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Cloud-based robot grasping with the google object recognition engine", "author": ["B. Kehoe", "A. Matsukawa", "S. Candido", "J. Kuffner", "K. Goldberg"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Anticipating human activities using object affordances for reactive robotic response", "author": ["H. Koppula", "A. Saxena"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Semantic labeling of 3d point clouds for indoor scenes", "author": ["H. Koppula", "A. Anand", "T. Joachims", "A. Saxena"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "A kernelbased approach to direct action perception", "author": ["O. Kroemer", "E. Ugur", "E. Oztop", "J. Peters"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Unsupervised feature learning for 3d scene labeling", "author": ["K. Lai", "L. Bo", "D. Fox"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Deep learning for detecting robotic grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "Robotics: Science and Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Deepmpc: Learning deep latent features for model predictive control", "author": ["I. Lenz", "R. Knepper", "A. Saxena"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Learning contact-rich manipulation skills with guided policy search", "author": ["S. Levine", "N. Wagener", "P. Abbeel"], "venue": "IEEE International Conference on Robotics and Automation,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Towards total scene understanding: Classification, annotation and segmentation in an automatic framework", "author": ["L.-J. Li", "R. Socher", "L. Fei-Fei"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "Unsupervised learning of simultaneous motor primitives through imitation", "author": ["O. Mangin", "P.-Y. Oudeyer"], "venue": "In IEEE ICDL-EPIROB,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Exploiting similarities among languages for machine translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "A geometric approach to robotic laundry folding", "author": ["S. Miller", "J. Van Den Berg", "M. Fritz", "T. Darrell", "K. Goldberg", "P. Abbeel"], "venue": "International Journal of Robotics Research,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Tell me dave: Context-sensitive grounding of natural language to mobile manipulation instructions", "author": ["D. Misra", "J. Sung", "K. Lee", "A. Saxena"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Learning to embed songs and tags for playlist prediction", "author": ["J. Moore", "S. Chen", "T. Joachims", "D. Turnbull"], "venue": "In Conference of the International Society for Music Information Retrieval (ISMIR),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Scalable nearest neighbor algorithms for high dimensional data", "author": ["M. Muja", "D.G. Lowe"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Learning to select and generalize striking movements in robot table tennis", "author": ["K. M\u00fclling", "J. Kober", "O. Kroemer", "J. Peters"], "venue": "International Journal of Robotics Research,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2013}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "Learning and generalization of motor skills by learning from demonstration", "author": ["P. Pastor", "H. Hoffmann", "T. Asfour", "S. Schaal"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2009}, {"title": "Learning to plan for constrained manipulation from demonstrations", "author": ["M. Phillips", "V. Hwang", "S. Chitta", "M. Likhachev"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2013}, {"title": "Learning articulated motions from visual demonstration", "author": ["S. Pillai", "M. Walter", "S. Teller"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Ros: an opensource robot operating system", "author": ["M. Quigley", "K. Conley", "B. Gerkey", "J. Faust", "T. Foote", "J. Leibs", "R. Wheeler", "A.Y. Ng"], "venue": "In ICRA workshop on open source software,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2009}, {"title": "3D is here: Point Cloud Library (PCL)", "author": ["R. Rusu", "S. Cousins"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2011}, {"title": "Animating rotation with quaternion curves", "author": ["K. Shoemake"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1985}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E. Huang", "A. Ng", "C. Manning"], "venue": "In EMNLP,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2011}, {"title": "Convolutional-recursive deep learning for 3d object classification", "author": ["R. Socher", "B. Huval", "B. Bhat", "C. Manning", "A. Ng"], "venue": "In NIPS,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2012}, {"title": "Improved multimodal deep learning with variation of information", "author": ["K. Sohn", "W. Shang", "H. Lee"], "venue": "In NIPS,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2014}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R.R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2012}, {"title": "A probabilistic framework for learning kinematic models of articulated objects", "author": ["J. Sturm", "C. Stachniss", "W. Burgard"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2011}, {"title": "Un-  structured human activity detection from rgbd images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2012}, {"title": "Synthesizing manipulation sequences for under-specified tasks using unrolled markov random fields", "author": ["J. Sung", "B. Selman", "A. Saxena"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2014}, {"title": "Robobarista: Object part-based transfer of manipulation trajectories from crowd-sourcing in 3d pointclouds", "author": ["J. Sung", "S.H. Jin", "A. Saxena"], "venue": "In International Symposium on Robotics Research (ISRR),", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2015}, {"title": "Asking for help using inverse semantics", "author": ["S. Tellex", "R. Knepper", "A. Li", "T. Howard", "D. Rus", "N. Roy"], "venue": "Robotics: Science and Systems,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2014}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J. Tenenbaum", "V. De Silva", "J. Langford"], "venue": null, "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2000}, {"title": "Robotsfor. me and robots for you", "author": ["R. Toris", "S. Chernova"], "venue": "In Proceedings of the Interactive Machine Learning Workshop, Intelligent User Interfaces Conference,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2013}, {"title": "The robot management system: A framework for conducting human-robot interaction studies through crowdsourcing", "author": ["R. Toris", "D. Kent", "S. Chernova"], "venue": "Journal of Human-Robot Interaction,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2014}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "In ICML. ACM,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2004}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun", "Y. Singer"], "venue": null, "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2005}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2008}, {"title": "Predicting slippage and learning manipulation affordances through gaussian process regression", "author": ["F. Vina", "Y. Bekiroglu", "C. Smith", "Y. Karayiannidis", "D. Kragic"], "venue": "In Humanoids,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2008}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "In NIPS,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2005}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "In IJCAI,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2011}, {"title": "Combining force and visual feedback for physical interaction tasks in humanoid robots", "author": ["S. Wieland", "D. Gonzalez-Aguirre", "N. Vahrenkamp", "T. Asfour", "R. Dillmann"], "venue": "In Humanoid Robots,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2009}, {"title": "Hierarchical semantic labeling for task-relevant rgb-d perception", "author": ["C. Wu", "I. Lenz", "A. Saxena"], "venue": "In Robotics:  Science and Systems,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2014}, {"title": "Learning structural svms with latent variables", "author": ["C.-N. Yu", "T. Joachims"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2009}, {"title": "Adadelta: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2012}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga"], "venue": "In ICASSP,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2013}], "referenceMentions": [{"referenceID": 60, "context": "[61]) Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Thus, rather than relying on scene understanding techniques [8, 39, 19], we directly use 3D point-clouds for manipulation planning using machine learning algorithms.", "startOffset": 60, "endOffset": 71}, {"referenceID": 38, "context": "Thus, rather than relying on scene understanding techniques [8, 39, 19], we directly use 3D point-clouds for manipulation planning using machine learning algorithms.", "startOffset": 60, "endOffset": 71}, {"referenceID": 18, "context": "Thus, rather than relying on scene understanding techniques [8, 39, 19], we directly use 3D point-clouds for manipulation planning using machine learning algorithms.", "startOffset": 60, "endOffset": 71}, {"referenceID": 14, "context": "Humans are able to map similar concepts from different sensory system to the same concept using common representation between different modalities [15].", "startOffset": 147, "endOffset": 151}, {"referenceID": 44, "context": "Our algorithm allows for efficient inference because, given a new instruction and point-cloud, we only need to find the nearest trajectory to the projection of this pair in the learned embedding space, which can be done using fast nearest-neighbor algorithms [45].", "startOffset": 259, "endOffset": 263}, {"referenceID": 32, "context": "In the past, deep learning methods have shown impressive results for learning features in a wide variety of domains [33, 54, 21] and even learning cross-domain embeddings for, for example, language and image features [57].", "startOffset": 116, "endOffset": 128}, {"referenceID": 53, "context": "In the past, deep learning methods have shown impressive results for learning features in a wide variety of domains [33, 54, 21] and even learning cross-domain embeddings for, for example, language and image features [57].", "startOffset": 116, "endOffset": 128}, {"referenceID": 20, "context": "In the past, deep learning methods have shown impressive results for learning features in a wide variety of domains [33, 54, 21] and even learning cross-domain embeddings for, for example, language and image features [57].", "startOffset": 116, "endOffset": 128}, {"referenceID": 56, "context": "In the past, deep learning methods have shown impressive results for learning features in a wide variety of domains [33, 54, 21] and even learning cross-domain embeddings for, for example, language and image features [57].", "startOffset": 217, "endOffset": 221}, {"referenceID": 3, "context": "Furthermore, in contrast to previous approaches based on learning from demonstration (LfD) that learn a mapping from a state to an action [4], our work complements LfD as we focus on the entire manipulation motion, as opposed to a sequential state-action mapping.", "startOffset": 138, "endOffset": 141}, {"referenceID": 19, "context": "\u2022 We introduce a new algorithm, which learns an embedding space while enforcing a varying and loss-based margin, along with a new unsupervised pre-training method which outperforms standard pre-training algorithms [20].", "startOffset": 214, "endOffset": 218}, {"referenceID": 38, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 91, "endOffset": 107}, {"referenceID": 31, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 91, "endOffset": 107}, {"referenceID": 32, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 91, "endOffset": 107}, {"referenceID": 73, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 91, "endOffset": 107}, {"referenceID": 58, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 134, "endOffset": 142}, {"referenceID": 24, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 134, "endOffset": 142}, {"referenceID": 54, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 191, "endOffset": 199}, {"referenceID": 34, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 191, "endOffset": 199}, {"referenceID": 18, "context": "idea of using part-based transfers, the deformable part model [19] was effective in object detection.", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "On the other hand, direct perception approaches [18, 34] skip the intermediate object labels and directly perceive affordances based on the shape of the object.", "startOffset": 48, "endOffset": 56}, {"referenceID": 33, "context": "On the other hand, direct perception approaches [18, 34] skip the intermediate object labels and directly perceive affordances based on the shape of the object.", "startOffset": 48, "endOffset": 56}, {"referenceID": 8, "context": "Most works in robotic manipulation focus on task-specific manipulation of known objects\u2014for example, baking cookies with known tools [9] and folding the laundry [42] \u2013 or focus on learning specific motions such as grasping [30] and opening doors [14].", "startOffset": 133, "endOffset": 136}, {"referenceID": 41, "context": "Most works in robotic manipulation focus on task-specific manipulation of known objects\u2014for example, baking cookies with known tools [9] and folding the laundry [42] \u2013 or focus on learning specific motions such as grasping [30] and opening doors [14].", "startOffset": 161, "endOffset": 165}, {"referenceID": 29, "context": "Most works in robotic manipulation focus on task-specific manipulation of known objects\u2014for example, baking cookies with known tools [9] and folding the laundry [42] \u2013 or focus on learning specific motions such as grasping [30] and opening doors [14].", "startOffset": 223, "endOffset": 227}, {"referenceID": 13, "context": "Most works in robotic manipulation focus on task-specific manipulation of known objects\u2014for example, baking cookies with known tools [9] and folding the laundry [42] \u2013 or focus on learning specific motions such as grasping [30] and opening doors [14].", "startOffset": 246, "endOffset": 250}, {"referenceID": 59, "context": "Others [60, 43] focus on sequencing manipulation tasks assuming perfect manipulation primitives such as grasp and pour are available.", "startOffset": 7, "endOffset": 15}, {"referenceID": 42, "context": "Others [60, 43] focus on sequencing manipulation tasks assuming perfect manipulation primitives such as grasp and pour are available.", "startOffset": 7, "endOffset": 15}, {"referenceID": 57, "context": "For the more general task of manipulating new instances of objects, previous approaches rely on finding articulation [58, 50] or using interaction [29], but they are limited by tracking performance of a vision algorithm.", "startOffset": 117, "endOffset": 125}, {"referenceID": 49, "context": "For the more general task of manipulating new instances of objects, previous approaches rely on finding articulation [58, 50] or using interaction [29], but they are limited by tracking performance of a vision algorithm.", "startOffset": 117, "endOffset": 125}, {"referenceID": 28, "context": "For the more general task of manipulating new instances of objects, previous approaches rely on finding articulation [58, 50] or using interaction [29], but they are limited by tracking performance of a vision algorithm.", "startOffset": 147, "endOffset": 151}, {"referenceID": 10, "context": "Another approach using part-based transfer between objects has been shown to be successful for grasping [11, 13].", "startOffset": 104, "endOffset": 112}, {"referenceID": 12, "context": "Another approach using part-based transfer between objects has been shown to be successful for grasping [11, 13].", "startOffset": 104, "endOffset": 112}, {"referenceID": 37, "context": "[38] use a Gaussian mixture model to learn system dynamics, then use these to learn a manipulation policy using a deep network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] use a deep network to learn system dynamics for realtime model-predictive control.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Several successful approaches for teaching robots tasks, such as helicopter maneuvers [1] or table tennis [46], have been based on Learning from Demonstration (LfD) [4].", "startOffset": 86, "endOffset": 89}, {"referenceID": 45, "context": "Several successful approaches for teaching robots tasks, such as helicopter maneuvers [1] or table tennis [46], have been based on Learning from Demonstration (LfD) [4].", "startOffset": 106, "endOffset": 110}, {"referenceID": 3, "context": "Several successful approaches for teaching robots tasks, such as helicopter maneuvers [1] or table tennis [46], have been based on Learning from Demonstration (LfD) [4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 39, "context": "actions and separately relies on high level task composition [40, 12] or is often limited to previously seen objects [49, 48].", "startOffset": 61, "endOffset": 69}, {"referenceID": 11, "context": "actions and separately relies on high level task composition [40, 12] or is often limited to previously seen objects [49, 48].", "startOffset": 61, "endOffset": 69}, {"referenceID": 48, "context": "actions and separately relies on high level task composition [40, 12] or is often limited to previously seen objects [49, 48].", "startOffset": 117, "endOffset": 125}, {"referenceID": 47, "context": "actions and separately relies on high level task composition [40, 12] or is often limited to previously seen objects [49, 48].", "startOffset": 117, "endOffset": 125}, {"referenceID": 1, "context": "Unlike learning a model from demonstration, instancebased learning [2, 17] replicates one of the demonstrations.", "startOffset": 67, "endOffset": 74}, {"referenceID": 16, "context": "Unlike learning a model from demonstration, instancebased learning [2, 17] replicates one of the demonstrations.", "startOffset": 67, "endOffset": 74}, {"referenceID": 70, "context": "LMNN [72] learns a max-margin Mahalanobis distance for a unimodal input feature space.", "startOffset": 5, "endOffset": 9}, {"referenceID": 71, "context": "[73] learn linear mappings from image and language features to a common embedding space for automatic image annotation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[44] learn to map songs and natural language tags to a shared embedding space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33, 54]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 53, "context": "[33, 54]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 35, "context": "In robotics, deep learning has previously been successfully used for detecting grasps for novel objects in multi-channel RGB-D images [36] and for classifying terrain from longrange vision [21].", "startOffset": 134, "endOffset": 138}, {"referenceID": 20, "context": "In robotics, deep learning has previously been successfully used for detecting grasps for novel objects in multi-channel RGB-D images [36] and for classifying terrain from longrange vision [21].", "startOffset": 189, "endOffset": 193}, {"referenceID": 46, "context": "[47] use deep learning to learn features incorporating both video and audio modalities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[56] propose a new generative learning algorithm for multimodal data which improves robustness to missing modalities at inference time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] map different languages to a joint feature space for translation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "Srivastava and Salakhutdinov [57] map images and natural language \u201ctags\u201d to the same space for automatic annotation and retrieval.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": ") Our algorithm trains each layer to map similar cases to similar areas of its feature space, as opposed to other methods which either perform variational learning [22] or train for reconstruction [20].", "startOffset": 164, "endOffset": 168}, {"referenceID": 19, "context": ") Our algorithm trains each layer to map similar cases to similar areas of its feature space, as opposed to other methods which either perform variational learning [22] or train for reconstruction [20].", "startOffset": 197, "endOffset": 201}, {"referenceID": 23, "context": "[24] also use a deep network for metric learning for the task of face verification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 70, "context": "Similar to LMNN [72], Hu et al.", "startOffset": 16, "endOffset": 20}, {"referenceID": 23, "context": "[24] enforces a constant margin between distances among inter-class objects and among intra-class objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Many approaches to teaching robots manipulation and other skills have relied on demonstrations by skilled experts [4, 1].", "startOffset": 114, "endOffset": 120}, {"referenceID": 0, "context": "Many approaches to teaching robots manipulation and other skills have relied on demonstrations by skilled experts [4, 1].", "startOffset": 114, "endOffset": 120}, {"referenceID": 9, "context": "Among previous efforts to scale teaching to the crowd [10, 62, 27], Forbes et al.", "startOffset": 54, "endOffset": 66}, {"referenceID": 61, "context": "Among previous efforts to scale teaching to the crowd [10, 62, 27], Forbes et al.", "startOffset": 54, "endOffset": 66}, {"referenceID": 26, "context": "Among previous efforts to scale teaching to the crowd [10, 62, 27], Forbes et al.", "startOffset": 54, "endOffset": 66}, {"referenceID": 16, "context": "[17] employs a similar approach towards crowd-sourcing but collects multiple instances of similar table-top manipulation with same object.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "Others also build web-based platform for crowdsourcing manipulation [65, 66].", "startOffset": 68, "endOffset": 76}, {"referenceID": 64, "context": "Others also build web-based platform for crowdsourcing manipulation [65, 66].", "startOffset": 68, "endOffset": 76}, {"referenceID": 2, "context": "[3], but works on any standard web browser with OpenGL support and incorporates real pointclouds of various scenes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "These points are often obtained by stitching together a sequence of sensor data from an RGBD sensor [26].", "startOffset": 100, "endOffset": 104}, {"referenceID": 52, "context": "Translation is linearly interpolated and the quaternion is interpolated using spherical linear interpolation (Slerp) [53].", "startOffset": 117, "endOffset": 121}, {"referenceID": 16, "context": "[17]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Thus, we align the negative z-axis along gravity and align the x-axis along the principal axis of the object part using PCA [23].", "startOffset": 124, "endOffset": 128}, {"referenceID": 32, "context": "image recognition [33]) to which deep learning has mostly been applied [6].", "startOffset": 18, "endOffset": 22}, {"referenceID": 5, "context": "image recognition [33]) to which deep learning has mostly been applied [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 46, "context": "this trajectory match this point-cloud/language pair?\u201d Then, we can use a multimodal feed-forward deep network [47] to solve this problem [61].", "startOffset": 111, "endOffset": 115}, {"referenceID": 60, "context": "this trajectory match this point-cloud/language pair?\u201d Then, we can use a multimodal feed-forward deep network [47] to solve this problem [61].", "startOffset": 138, "endOffset": 142}, {"referenceID": 60, "context": "Compared to previous work that exhaustively runs a full network over all these combinations [61], our approach allows us to pre-embed all candidate trajectories into this common feature space.", "startOffset": 92, "endOffset": 96}, {"referenceID": 76, "context": "With W \u2019s as network weights, which are the learned parameters of our system, and a(\u00b7) as a rectified linear unit (ReLU) activation function [78], our model for projecting from point-cloud and language features to the shared embedding h is as follows:", "startOffset": 141, "endOffset": 145}, {"referenceID": 71, "context": "As in previous work [73], similarity is defined as sim(a, b) = a \u00b7 b, and the trajectory that maximizes the magnitude of similarity is selected:", "startOffset": 20, "endOffset": 24}, {"referenceID": 60, "context": "The previous approach to this problem [61] required projecting the combination of the current point-cloud and natural language instruction with every trajectory in the training set through the network during inference.", "startOffset": 38, "endOffset": 42}, {"referenceID": 70, "context": "A simple approach would be to train the network to distinguish these two sets by enforcing a finite distance (safety margin) between the similarities of these two sets [72], which can be written in the form of a constraint:", "startOffset": 168, "endOffset": 172}, {"referenceID": 66, "context": "Instead, similar to the cutting plane method for structural support vector machines [68], we find the most violating trajectory \u03c4 \u2032 \u2208 Ti,D for each training pair of (pi, li, \u03c4i \u2208 Ti,S) at each iteration.", "startOffset": 84, "endOffset": 88}, {"referenceID": 75, "context": "The average cost of each minibatch is back-propagated through all the layers of the deep neural network using the AdaDelta [77] algorithm.", "startOffset": 123, "endOffset": 127}, {"referenceID": 69, "context": "First, we pre-train the layers leading up to these layers using spare de-noising autoencoders [71, 78].", "startOffset": 94, "endOffset": 102}, {"referenceID": 76, "context": "First, we pre-train the layers leading up to these layers using spare de-noising autoencoders [71, 78].", "startOffset": 94, "endOffset": 102}, {"referenceID": 69, "context": "Standard pretraining methods, such as sparse denoising autoencoder [71, 78] would only pre-train h to reconstruct individual trajectories.", "startOffset": 67, "endOffset": 75}, {"referenceID": 76, "context": "Standard pretraining methods, such as sparse denoising autoencoder [71, 78] would only pre-train h to reconstruct individual trajectories.", "startOffset": 67, "endOffset": 75}, {"referenceID": 30, "context": "[31]) and not their rotations and gripper status.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17], we provide a trajectory that the system has already seen for another object as the initial starting trajectory to edit.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "For each object, we took raw RGB-D images with the Microsoft Kinect sensor and stitched them using Kinect Fusion [26] to form a denser point-cloud in order to incorporate different viewpoints of objects.", "startOffset": 113, "endOffset": 117}, {"referenceID": 65, "context": "2) Object Part Classifier: To test our hypothesis that classifying object parts as an intermediate step does not guarantee successful transfers, we trained an object part classifier using multiclass SVM [67] on point-cloud features \u03c6(p) including local shape features [32], histogram of curvatures [52], and distribution of points.", "startOffset": 203, "endOffset": 207}, {"referenceID": 31, "context": "2) Object Part Classifier: To test our hypothesis that classifying object parts as an intermediate step does not guarantee successful transfers, we trained an object part classifier using multiclass SVM [67] on point-cloud features \u03c6(p) including local shape features [32], histogram of curvatures [52], and distribution of points.", "startOffset": 268, "endOffset": 272}, {"referenceID": 51, "context": "2) Object Part Classifier: To test our hypothesis that classifying object parts as an intermediate step does not guarantee successful transfers, we trained an object part classifier using multiclass SVM [67] on point-cloud features \u03c6(p) including local shape features [32], histogram of curvatures [52], and distribution of points.", "startOffset": 298, "endOffset": 302}, {"referenceID": 62, "context": "\u03c6(\u03c4) applies Isomap [63] to interpolated \u03c4 for non-linear dimensionality reduction.", "startOffset": 20, "endOffset": 24}, {"referenceID": 27, "context": "VI) to train SSVM and used the cutting plane method to solve the SSVM optimization problem [28].", "startOffset": 91, "endOffset": 95}, {"referenceID": 57, "context": "Instead of explicitly trying to learn this structure, we encoded this internal structure as latent variable z \u2208 Z , composed of joint type, center of joint, and axis of joint [58].", "startOffset": 175, "endOffset": 179}, {"referenceID": 74, "context": "We used Latent SSVM [76] to train with z, learning the discriminant function F : P \u00d7 L \u00d7 T \u00d7 Z \u2192 R.", "startOffset": 20, "endOffset": 24}, {"referenceID": 57, "context": "6) Latent SSVM + Kinematic [58] 17.", "startOffset": 27, "endOffset": 31}, {"referenceID": 16, "context": "9) Task Similarity + Weights [17] 13.", "startOffset": 29, "endOffset": 33}, {"referenceID": 60, "context": "0) Deep Multimodal Network with Noise-handling without Embedding [61] 13.", "startOffset": 65, "endOffset": 69}, {"referenceID": 70, "context": "1) LMNN-like Cost Function [72] 15.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "Pairwise similarity is defined as a convex combination of the cosine similarity in bag-of-words representations of language and the average mutual point-wise distance of two pointclouds after a fixed number of iterations of the ICP [7] algorithm.", "startOffset": 232, "endOffset": 235}, {"referenceID": 16, "context": "[17] introduces a score function for weighting demonstrations based on weighted distance to the \u201cseed\u201d (expert) demonstration.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "More details about the model can be found in [61].", "startOffset": 45, "endOffset": 49}, {"referenceID": 70, "context": "9) LMNN [72]-like cost function: For all top layer fine-tuning and lower layer pre-training, we define the cost function without loss augmentation.", "startOffset": 8, "endOffset": 12}, {"referenceID": 70, "context": "Similar to LMNN [72], we give a finite margin between similarities.", "startOffset": 16, "endOffset": 20}, {"referenceID": 69, "context": "Instead, we pre-train each layer as stacked denoising autoencoders [71, 78].", "startOffset": 67, "endOffset": 75}, {"referenceID": 76, "context": "Instead, we pre-train each layer as stacked denoising autoencoders [71, 78].", "startOffset": 67, "endOffset": 75}, {"referenceID": 15, "context": "Is segmentation required for the system? Even with the state-of-the-art techniques [16, 33], detection of \u2018manipulable\u2019 object parts such as \u2018handles\u2019 and \u2018levers\u2019 in a point-", "startOffset": 83, "endOffset": 91}, {"referenceID": 32, "context": "Is segmentation required for the system? Even with the state-of-the-art techniques [16, 33], detection of \u2018manipulable\u2019 object parts such as \u2018handles\u2019 and \u2018levers\u2019 in a point-", "startOffset": 83, "endOffset": 91}, {"referenceID": 59, "context": "without Embedding [60] Deep Multimodal Embedding", "startOffset": 18, "endOffset": 22}, {"referenceID": 60, "context": "Comparisons of transfers between our model and the baseline (deep multimodal network without embedding [61]).", "startOffset": 103, "endOffset": 107}, {"referenceID": 34, "context": "cloud is by itself a challenging problem [35].", "startOffset": 41, "endOffset": 45}, {"referenceID": 67, "context": "This visualization is created by projecting all training data (point-cloud/language pairs and trajectories) of one of the cross-validation folds to h, then embedding them to 2-dimensional space using t-SNE [69].", "startOffset": 206, "endOffset": 210}, {"referenceID": 60, "context": "Although previous work [61] was able to visualize several nodes in the top layer, most were difficult to interpret.", "startOffset": 23, "endOffset": 27}, {"referenceID": 60, "context": "[61]", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "Our algorithm consistently outperforms the previous approach [61] and an LMNN-like cost function [72].", "startOffset": 61, "endOffset": 65}, {"referenceID": 70, "context": "Our algorithm consistently outperforms the previous approach [61] and an LMNN-like cost function [72].", "startOffset": 97, "endOffset": 101}, {"referenceID": 70, "context": "Should cost function be loss-augmented? When we changed the cost function for pre-training h and finetuning h to use a constant margin of 1 between relevant Ti,S and irrelevant Ti,D demonstrations [72], performance drops to 55.", "startOffset": 197, "endOffset": 201}, {"referenceID": 69, "context": "Pre-training the lower layers with the conventional stacked de-noising auto-encoder (SDA) algorithm [71, 78] increases performance to 62.", "startOffset": 100, "endOffset": 108}, {"referenceID": 76, "context": "Pre-training the lower layers with the conventional stacked de-noising auto-encoder (SDA) algorithm [71, 78] increases performance to 62.", "startOffset": 100, "endOffset": 108}, {"referenceID": 60, "context": "Does embedding improve efficiency? The previous model [61] had 749, 638 parameters to be learned, while our model has only 418, 975 (and still gives better performance.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "Time was measured on the same hardware, with a GPU (GeForce GTX Titan X), using the Theano library [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 44, "context": "We note that the only part of our algorithm\u2019s runtime which scales up with the amount of training data is the nearest-neighbor computation, for which there exist many efficient algorithms [45].", "startOffset": 188, "endOffset": 192}, {"referenceID": 8, "context": "For these experiments, a point-cloud is acquired from the head mounted Kinect sensor and each motion is executed on the specified arm using a Cartesian end-effector stiffness controller [9] in ROS [51].", "startOffset": 186, "endOffset": 189}, {"referenceID": 50, "context": "For these experiments, a point-cloud is acquired from the head mounted Kinect sensor and each motion is executed on the specified arm using a Cartesian end-effector stiffness controller [9] in ROS [51].", "startOffset": 197, "endOffset": 201}, {"referenceID": 67, "context": "Learned Deep Point-cloud/Language/Trajectory Embedding Space: Joint embedding space h3 after the network is fully fine-tuned, visualized in 2d using t-SNE [69] .", "startOffset": 155, "endOffset": 159}, {"referenceID": 67, "context": "Learned Point-cloud/Language Space: Visualization of the point-cloud/language layer h2,lp in 2d using t-SNE [69] after the network is fully fine-tuned.", "startOffset": 108, "endOffset": 112}, {"referenceID": 72, "context": "For such objects, correctly executing a transferred manipulation trajectory may require incorporating visual and/or force feedback [74, 70] in order for the execution to adapt exactly to the object.", "startOffset": 131, "endOffset": 139}, {"referenceID": 68, "context": "For such objects, correctly executing a transferred manipulation trajectory may require incorporating visual and/or force feedback [74, 70] in order for the execution to adapt exactly to the object.", "startOffset": 131, "endOffset": 139}], "year": 2016, "abstractText": "There is a large variety of objects and appliances in human environments, such as stoves, coffee dispensers, juice extractors, and so on. It is challenging for a roboticist to program a robot for each of these object types and for each of their instantiations. In this work, we present a novel approach to manipulation planning based on the idea that many household objects share similarly-operated object parts. We formulate the manipulation planning as a structured prediction problem and learn to transfer manipulation strategy across different objects by embedding point-cloud, natural language, and manipulation trajectory data into a shared embedding space using a deep neural network. In order to learn semantically meaningful spaces throughout our network, we introduce a method for pre-training its lower layers for multimodal feature embedding and a method for fine-tuning this embedding space using a loss-based margin. In order to collect a large number of manipulation demonstrations for different objects, we develop a new crowd-sourcing platform called Robobarista. We test our model on our dataset consisting of 116 objects and appliances with 249 parts along with 250 language instructions, for which there are 1225 crowd-sourced manipulation demonstrations. We further show that our robot with our model can even prepare a cup of a latte with appliances it has never seen before.", "creator": "LaTeX with hyperref package"}}}