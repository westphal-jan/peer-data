{"id": "1601.02745", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2016", "title": "Basic Reasoning with Tensor Product Representations", "abstract": "In this paper we present the initial development of a general theory for mapping inference in predicate logic to computation over Tensor Product Representations (TPRs; Smolensky (1990), Smolensky &amp; Legendre (2006)). After an initial brief synopsis of TPRs (Section 0), we begin with particular examples of inference with TPRs in the 'bAbI' question-answering task of Weston et al. (2015) (Section 1). We then present a simplification of the general analysis that suffices for the bAbI task (Section 2). Finally, we lay out the general treatment of inference over TPRs (Section 3). We also show the simplification in Section 2 derives the inference methods described in Lee et al. (2016); this shows how the simple methods of Lee et al. (2016) can be formally extended to more general reasoning tasks.", "histories": [["v1", "Tue, 12 Jan 2016 06:44:54 GMT  (654kb)", "http://arxiv.org/abs/1601.02745v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["paul smolensky", "moontae lee", "xiaodong he", "wen-tau yih", "jianfeng gao", "li deng"], "accepted": false, "id": "1601.02745"}, "pdf": {"name": "1601.02745.pdf", "metadata": {"source": "CRF", "title": "Basic Reasoning with Tensor Product Representations", "authors": ["Paul Smolensky"], "emails": ["smolensky@jhu.edu", "moontae@cs.cornell.edu", "deng}@microsoft.com"], "sections": [{"heading": null, "text": "(Smolensky) We (Smolensky & Legendre (Smolensky & Legendre (Smolensky & Legendre (2006))) We (Smolensky & Legendre). (Smolensky) We (Smolensky & Legendre). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky). (Smolensky)."}, {"heading": "1 BABI EXAMPLE", "text": "Consider the example in (2). \"(a, b, t)\" means \"a is at time t\" (or \"a is co-located with b at time t\"). (In Lee et al. (2016) the gloss is \"a belongs to b\" or \"a is contained in b.\" (The English question \"where was the apple before the kitchen?\" is a logical form (LF) that can be called \"the location x,\" for which there is the case that P (x) \"P (x)}}}. The English question\" where was the apple before the kitchen? \"is a logical form (LF) that is called\" the case x, \"that one [the apple] was t at any time and the apple to k [the kitchen] at the time t.\" (2)"}, {"heading": "2 SIMPLIFICATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Deriving the simplification for two-place predicates", "text": "As shown in Lee et al. (2016), this is the only relationship needed for most bAbI problem types (they are \"unirelational\"), so it is not necessary to encode them explicitly: All elements have the same initial tensor factor @. This simplification is presented in the third column of Table (14). However, the matrix algebra expression xyT and the tensor expression x y define exactly the same elements [xyT] jk = [x-y] jk. (14) Simplification is sufficient for the bAbI task: implicit is symbolic complete TPR simplification with predicate, implicit timestamp: 1 2 4 {@ (x, y, t1), t1 x-y-y-y-y-y-y-z-explicit."}, {"heading": "2.2 Deriving the simplification for three-place predicates", "text": "The analyses in Lee et al. (2016) of the questions in categories 2, 3 and 5 relate to the binding of 3 entities instead of 2. \"(Example: (16) The representation of\" Mary \"in the ranges 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1"}, {"heading": "2.3 Deriving the simplification for Path Finding (bAbI category 19)", "text": "(19) Path-Finding: Example ProblemSentence i LF: L (i) Full TPR Modela. The bedroom is south of the garden. (b, h) s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s"}, {"heading": "2.4 Performance of simplification on bAbI dataset", "text": "The simplification of the complete TPR argumentation analysis described above has been implemented and the results recorded in Lee et al. (2016) are briefly summarized in (21). (21) The performance of the simplification of the bAbI tasks 100% in all question categories except: C5: 99.8% C16: 99.5% Since the present analysis performs conclusions by programmed vector methods and not by learned network calculations, this performance cannot be directly compared with the performance of previous work devoted to the bAbI task (in particular Peng et. al (2015), which reached 66.4% / 97.9% and 17.3% / 87.0% respectively in tasks 17 and 19 with 1k / 10k training examples; these are the most difficult tasks where the previous best performance was 72% and 36% respectively). (Previous best performance on C5 / C16 was 99.3% / 100% by the closely monitored Memory Network of Weston, Chopra & Borra (2014)."}, {"heading": "3 GENERAL TREATMENT", "text": "(22) \u00b2 \u00b2 of query constructiona.b.c.As the specific example of query (13) from the general case (22) is described in (23). (23) derivation of query (13) from the general case (22) a. x. t \u2032, t \u00b2, t \u00b2, t \u00b2, t \u00b2 & @ (a, x, t \u2032, t \u00b2, (t \u00b2, \u00f8) b. e31, e32. p1 (c11, c21, e31) p1 = @; c11 = a, c21 = k, e31 = t \u00b2 & p2 (c12, x \u00b2, x \u00b2 \u00b2, e \u00b2 \u00b2) p2 = @; c 1 = a \u00b2, v \u00b2 (2 = a \u00b2, v \u00b2, 3 p1 p1 (c11, e31) p1 = square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square square"}], "references": [{"title": "Reasoning in vector space: An exploratory study of question answering", "author": ["Lee", "Moontae", "He", "Xiaodong", "Yih", "Wen-tau", "Gao", "Jianfeng", "Deng", "Li", "Smolensky", "Paul"], "venue": "Under review for", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Tensor product variable binding and the representation of symbolic structures in connectionist systems", "author": ["Smolensky", "Paul"], "venue": "Artificial Intelligence,", "citeRegEx": "Smolensky and Paul.,? \\Q1990\\E", "shortCiteRegEx": "Smolensky and Paul.", "year": 1990}, {"title": "The Harmonic Mind: From Neural Computation to OptimalityTheoretic Grammar. Volume I: Cognitive Architecture", "author": ["Smolensky", "Paul", "Legendre", "G\u00e9raldine"], "venue": null, "citeRegEx": "Smolensky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Smolensky et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "We also show the simplification in Section 2 derives the inference methods described in Lee et al. (2016); this shows how the simple methods of Lee et al.", "startOffset": 88, "endOffset": 106}, {"referenceID": 0, "context": "We also show the simplification in Section 2 derives the inference methods described in Lee et al. (2016); this shows how the simple methods of Lee et al. (2016) can be formally extended to more general reasoning tasks.", "startOffset": 88, "endOffset": 162}, {"referenceID": 0, "context": "(In Lee et al. (2016), the gloss is \u201ca belongs to b\u201d or \u201ca is contained in b\u201d.", "startOffset": 4, "endOffset": 22}, {"referenceID": 0, "context": "As shown in Lee et al. (2016), for most of the bAbI problem types, this is the only relation needed (they are \u201cuni-relational\u201d), so it is not necessary to encode it explicitly: all items have the same initial tensor factor @.", "startOffset": 12, "endOffset": 30}, {"referenceID": 0, "context": "This is exactly the form that transitive inference takes in Lee et al. (2016); e.", "startOffset": 60, "endOffset": 78}, {"referenceID": 0, "context": "The analyses in Lee et al. (2016) of questions in categories 2, 3 and 5 involve the binding of 3 entities rather than 2.", "startOffset": 16, "endOffset": 34}, {"referenceID": 0, "context": "(These assumptions apply to the implementation discussed in Lee et al. (2016).) Assume the generic case in which the {\u00eal} are linearly independent, and let the n-dimensional subspace of Rd that they span be E.", "startOffset": 60, "endOffset": 78}, {"referenceID": 0, "context": "In Lee et al. (2016), questions in category 5 are treated with an operation * that functions identically to \u2218, with d \u00d7 2d matrix V rather than U.", "startOffset": 3, "endOffset": 21}, {"referenceID": 0, "context": "In this sense, Path Finding is a \u201cmulti-relational\u201d problem, whereas all the simpler bAbI problem types, reducible to transitivity, are \u201cuni-relational\u201d \u2014 a main point of Lee et al. (2016).", "startOffset": 171, "endOffset": 189}, {"referenceID": 0, "context": "In the simplified approach implemented in Lee et al. (2016), a set of position vectors and direction matrices encoding the statements given in the problem is generated.", "startOffset": 42, "endOffset": 60}, {"referenceID": 0, "context": "The simplification of the full TPR reasoning analysis described above was implemented and the results reported in Lee et al. (2016) are briefly summarized in (21).", "startOffset": 114, "endOffset": 132}], "year": 2016, "abstractText": "In this paper we present the initial development of a general theory for mapping inference in predicate logic to computation over Tensor Product Representations (TPRs; Smolensky (1990), Smolensky & Legendre (2006)). After an initial brief synopsis of TPRs (Section 0), we begin with particular examples of inference with TPRs in the \u2018bAbI\u2019 question-answering task of Weston et al. (2015) (Section 1). We then present a simplification of the general analysis that suffices for the bAbI task (Section 2). Finally, we lay out the general treatment of inference over TPRs (Section 3). We also show the simplification in Section 2 derives the inference methods described in Lee et al. (2016); this shows how the simple methods of Lee et al. (2016) can be formally extended to more general reasoning tasks.", "creator": "Word"}}}