{"id": "1601.02828", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2016", "title": "Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation", "abstract": "This work presents a broad study on the adaptation of neural network acoustic models by means of learning hidden unit contributions (LHUC) -- a method that linearly re-combines hidden units in a speaker- or environment-dependent manner using small amounts of unsupervised adaptation data. We also extend LHUC to a speaker adaptive training (SAT) framework that leads to more adaptable DNN acoustic model, which can work in both a speaker-dependent and a speaker-independent manner, without the requirement to maintain auxiliary speaker-dependent feature extractors or to introduce significant speaker-dependent changes to the DNN structure. Through a series of experiments on four different speech recognition benchmarks (TED talks, Switchboard, AMI meetings and Aurora4) and over 270 test speakers we show that LHUC in both its test-only and SAT variants results in consistent word error rate reductions ranging from 5% to 23% relative depending on the task and the degree of mismatch between training and test data. In addition we have investigated the effect of the amount of adaptation data per speaker, the quality of adaptation targets when estimating transforms in an unsupervised manner, the complementarity to other adaptation techniques, one-shot adaptation, and an extension to adapting DNNs trained in a sequence discriminative manner.", "histories": [["v1", "Tue, 12 Jan 2016 12:33:56 GMT  (474kb,D)", "http://arxiv.org/abs/1601.02828v1", "13 pages, Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing"], ["v2", "Wed, 13 Jul 2016 17:47:07 GMT  (2236kb,D)", "http://arxiv.org/abs/1601.02828v2", "14 pages, 9 Tables, 11 Figues in IEEE/ACM Transactions on Audio, Speech and Language Processing, Vol. 24, Num. 8, 2016"]], "COMMENTS": "13 pages, Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["pawel swietojanski", "jinyu li", "steve renals"], "accepted": false, "id": "1601.02828"}, "pdf": {"name": "1601.02828.pdf", "metadata": {"source": "CRF", "title": "Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation", "authors": ["Pawel Swietojanski"], "emails": ["p.swietojanski@ed.ac.uk", "s.renals@ed.ac.uk", "jinyu.li@microsoft.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION AND SUMMARYSPEECH recognition accuracies have improved substantially over the last few years through the use of (deep) neural network (DNN) acoustic models. Hinton et al. [1] report word error rate (WER) techniques between 10-32% across a wide variety of tasks, compared to discriminatively trained Gaussian mixture model (GMM) based systems. These results use neural networks as part of hybrid DNN / HMM (hidden Markov model) systems [1] - [5] in which the neural network provides a scaled probability estimate to replace GMM, and as a tandem or bottleneck feature systems [6], [7] in which the neural network is used as a discriminatory trait extractor for a GMM-based system. For many tasks it has been observed that GMM-based systems (with tandem or bottleneck) adapted to the talker are more accurate than non-adapted DNM-hybrid systems."}, {"heading": "II. REVIEW OF NEURAL NETWORK ACOUSTIC ADAPTATION", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "III. LEARNING HIDDEN UNIT CONTRIBUTIONS (LHUC)", "text": "There are a number of approaches that prove to be universally applicable. (1) That is, there is a vector of random input variables that relate to a neural network. (2) That is, there is a vector of random input variables that relates to an arbitrarily small error in relation to a distance measurement, such as the mean square error (provided n is sufficiently large). (2) In (1) the ranges that relate to an arbitrarily small error in relation to a distance measurement, the mean square error (provided n is sufficiently large). (2) In (1) the ranges where an elementary non-linear operation is applied that forms an adaptive function that forms an adaptive function. (2) In (2) the ranges where an elementary non-linear approach is applied, the learning process is not an elementary one (2)."}, {"heading": "IV. SPEAKER ADAPTIVE TRAINING LHUC (SAT-LHUC)", "text": "When LHUC is used as a test-only fit, it is assumed that the set of speaker-independent basic functions is estimated on the basis of the training data, providing a good starting point for further adjustment to the underlying data distribution of the fit data (Fig. 1). However, a counter-example can be derived from this where this assumption fails: the uppermost graph of Fig. 2 shows training data exemplarily drawn from two competing distributions f1 (a) and f1 (b), where the linear recombination of the resulting base provides a poor approximation of fit data. This motivates the combination of LHUC with Adaptive Training (SAT) [57], in which the hidden units are trained to capture both good average representations and speakers-specific representations by estimating hidden units for each training speaker."}, {"heading": "V. EXPERIMENTAL SETUPS", "text": "We experimentally examined LHUC and SAT-LHUC long contexts using four different corpora: the TED talks corpus [15] following the IWSLT evaluation protocol (www.iwslt.org); the switchboard corpus of talking speech [17] (ldc.upenn.edu); the AMI conferences corpus [16], [58] (corpus. amiproject.org); and the Aurora4 corpus of read speech with artificially corrupted acoustic environments [18] (catalog.elra. info). Unless explicitly stated otherwise, the models share similar structure across the tasks - DNNs with 6 hidden layers (2,048 units in each) using a sigmoid non-linearity. The output logistic regression layer models the distribution of context-dependent states [5]."}, {"heading": "VI. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. LHUC hyperparameters", "text": "In fact, it is the case that we will be able to get to grips with the problems mentioned in order to solve them."}, {"heading": "B. SAT-LHUC", "text": "As described in Section IV, the SAT-LHUC training aims to regulate the hidden unit feature receptors so that they capture not only the average characteristics of the training data, but also the specific characteristics of the different distributions from which the data were drawn (for example, different training subjects).As a result, the model can be better tailored to invisible speakers by placing more emphasis on those units that were useful for training speakers with similar characteristics. In this work, we train SAT-LHUC models with frame level [14], segment level and speaker level clusters. For speaker and segment level, we decide which speakers or segments will be treated as SI or SD prior to training. For the frame level SAT-LHUC approach, the SI / SD decisions are made separately for each data point during training."}, {"heading": "C. Sequence model adaptation", "text": "This year, the time has come for us to be able to look for a solution."}, {"heading": "D. Other aspects of adaptation", "text": "In this context, it should be noted that this is one of the greatest challenges in the history of the European Union."}, {"heading": "E. Complementarity to feature normalisation", "text": "It is therefore of great interest to explore how complementary the proposed approaches to SAT training are with fMLR transformations. 3We compared LHUC and SAT-LHUC with SAT-fMLR training using TED tst2010 (Fig. 9, red curves).We compared both techniques, including a comparison in terms of the amount of data used to estimate any kind of transformation. fMLLR transforms an estimated 10s of unattended data, resulting in an increase in WHO compared to the SI-trained3Due, which we do not explicitly perform with other techniques such as auxiliary i-vector features or speaker codes; literature suggests that the use of i-vectors yields similar results [29] compared to fMLLR models."}, {"heading": "F. Adaptation Summary", "text": "In this section, we summarize our results by applying LHUC and SAT-LHUC to TED, AMI, and the switchboard. Table X contains the results of four IWSLT exams (dev2010, tst2011, and tst2013), and similar conclusions can be drawn from the experiments with LHUC and SAT-LHUC, which are effectively DNN and CNN oriented."}, {"heading": "VII. LHUC FOR FACTORISATION", "text": "We have used LHUC to adapt to both the loudspeaker and the acoustic environment. Alternatively, if multiple conditioning data is available for a loudspeaker, it is possible to define a set of common loudspeaker environments that transforms LHUC. Alternatively, we can estimate two sets of transformations - for loudspeakers rS and for environment rE - and then interpolate them linearly to derive a combined transformation r-SE, as follows: \"(r-lSE) = 1 \u2212 \u03b1)\" (rlE) \"(10) The idea is similar to the channel normalization applied by LHUC between distant and nearby scenarios [75], unless we use two independently estimated transformations. We have adapted base models of multiple conditioning models DNN models [76] to the loudspeaker (rS) and the environment (rE). The rS transformations have only been estimated to clean language; the environment has been similarly transformed."}, {"heading": "VIII. CONCLUSIONS", "text": "We presented the LHUC approach to unattended adaptation of neural network acoustic models in both test-only (LHUC) and SAT (SAT-LHUC) frameworks and evaluated these using four standardized speech recognition corpora: TED conversations as used in the IWSLT evaluations, AMI, Switchboard, and Aurora4. Our experimental results suggest that both LHUC and SAT-LHUC can offer significant improvements in WHO (5-23% relative to the test set and task). LHUC adaptation works well unattended and with small amounts of data (only 10 s), is complementary to feature normalization transformations such as SAT-fMLLR, and can be applied to unattended adaptation of sequenced DNN acoustic models using a cross-entropolar lens functionality that we have demonstrated to transform them in different ways."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G Hinton", "L Deng", "D Yu", "GE Dahl", "A Mohamed", "N Jaitly", "A Senior", "V Vanhoucke", "P Nguyen", "TN Sainath", "B Kingsbury"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, Nov 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Connectionist Speech Recognition: A Hybrid Approach", "author": ["H Bourlard", "N Morgan"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Connectionist probability estimators in HMM speech recognition", "author": ["S Renals", "N Morgan", "H Bourlard", "M Cohen", "H Franco"], "venue": "IEEE Trans Speech and Audio Processing, vol. 2, pp. 161\u2013174, 1994.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["F Seide", "X Chen", "D Yu"], "venue": "Proc. IEEE ASRU, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["GE Dahl", "D Yu", "L Deng", "A Acero"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Tandem connectionist feature extraction for conventional HMM systems", "author": ["H Hermansky", "DPW Ellis", "S Sharma"], "venue": "Proc. IEEE ICASSP, 2000, pp. 1635\u20131638.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Probabilistic and bottleneck features for LVCSR of meetings", "author": ["F Grezl", "M Karafiat", "S Kontar", "J Cernocky"], "venue": "Proc. IEEE ICASSP, 2007, pp. IV\u2013757\u2013IV\u2013760.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep convolutional neural networks for LVCSR", "author": ["TN Sainath", "A Mohamed", "B Kingsbury", "B Ramabhadran"], "venue": "Proc. IEEE ICASSP, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Revisiting hybrid and GMM- HMM system combination techniques", "author": ["P Swietojanski", "A Ghoshal", "S Renals"], "venue": "Proc. IEEE ICASSP, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Cambridge University transcription systems for the Multi-Genre Broadcast Challenge", "author": ["PC Woodland", "X Liu", "Y Qian", "C Zhang", "MJF Gales", "P Karanasou", "P Lanchantin", "L Wang"], "venue": "Proc. IEEE ASRU, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker adaptation for continuous density HMMs: A review", "author": ["PC Woodland"], "venue": "Proceedings of the ISCA workshop on adaptation methods for speech recognition, 2001, pp. 11\u201319.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Rapid and effective speaker adaptation of convolutional neural network based models for speech recognition", "author": ["O Abdel-Hamid", "H Jiang"], "venue": "Proc. Interspeech, pp. 1248\u20131252.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1252}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["P Swietojanski", "S Renals"], "venue": "Proc. IEEE SLT, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "SAT-LHUC: Speaker adaptive training for learning hidden unit contributions", "author": ["P Swietojanski", "S Renals"], "venue": "Proc. IEEE ICASSP, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Wit3: Web inventory of transcribed and translated talks", "author": ["M Cettolo", "C Girardi", "M Federico"], "venue": "Proc. EAMT, 2012, pp. 261\u2013268.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Unleashing the killer corpus: Experiences in creating the multi-everything AMI meeting corpus", "author": ["J Carletta"], "venue": "Language Resources and Evaluation, vol. 41, no. 2, pp. 181\u2013190, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "SWITCHBOARD: Telephone speech corpus for research and development", "author": ["JJ Godfrey", "EC Holliman", "J McDaniel"], "venue": "Proc. IEEE ICASSP. IEEE, 1992, pp. 517\u2013520.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1992}, {"title": "Performance analysis of the Aurora large vocabulary baseline system", "author": ["N Parihar", "J Picone", "D Pearce", "HG Hirsch"], "venue": "Proc. EUSIPCO, 2004.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Maximum likelihood linear transformations for HMMbased speech recognition", "author": ["MJF Gales"], "venue": "Computer Speech and Language, vol. 12, pp. 75\u201398, April 1998.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Deep belief networks using discriminative features for phone recognition", "author": ["A Mohamed", "TN Sainath", "G Dahl", "B Ramabhadran", "GE Hinton", "MA Picheny"], "venue": "Proc. IEEE ICASSP, 2011, pp. 5060\u20135063.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Transcribing meetings with the AMIDA systems", "author": ["T Hain", "L Burget", "J Dines", "PN Garner", "F Gr\u00e9zl", "A El Hannani", "M Karaf\u0131\u0301at", "M Lincoln", "V Wan"], "venue": "IEEE Trans Audio, Speech and Language Processing, vol. 20, pp. 486\u2013498, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Auto-encoder bottleneck features using deep belief networks", "author": ["TN Sainath", "B Kingsbury", "B Ramabhadran"], "venue": "Proc. IEEE ICASSP, 2012, pp. 4153\u20134156.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-level adaptive networks in tandem and hybrid ASR systems", "author": ["P Bell", "P Swietojanski", "S Renals"], "venue": "Proc. IEEE ICASSP, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Speaker adaptation for hybrid HMM\u2013ANN continuous speech recognition system", "author": ["J Neto", "L Almeida", "M Hochberg", "C Martins", "L Nunes", "S Renals", "T Robinson"], "venue": "Proc. Eurospeech, 1995, pp. 2171\u20132174.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Connectionist speaker normalization and adaptation", "author": ["V Abrash", "H Franco", "A Sankar", "M Cohen"], "venue": "Proc. Eurospeech, 1995, pp. 2183\u2013 2186.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "Adaptation of context-dependent deep neural networks for automatic speech recognition", "author": ["K Yao", "D Yu", "F Seide", "H Su", "L Deng", "Y Gong"], "venue": "Proc. IEEE SLT, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Front end factor analysis for speaker verification", "author": ["N Dehak", "PJ Kenny", "R Dehak", "P Dumouchel", "P Ouellet"], "venue": "IEEE Trans Audio, Speech and Language Processing, vol. 19, pp. 788\u2013798, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "iVectorbased discriminative adaptation for automatic speech recognition", "author": ["M Karafiat", "L Burget", "P Matejka", "O Glembek", "J Cernozky"], "venue": "Proc. IEEE ASRU, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G Saon", "H Soltau", "D Nahamoo", "M Picheny"], "venue": "Proc. IEEE ASRU, 2013, pp. 55\u201359.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "I-vector based speaker adaptation of deep neural networks for french broadcast audio transcription", "author": ["V Gupta", "P Kenny", "P Ouellet", "T Stafylakis"], "venue": "Proc. IEEE ICASSP, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptation of deep neural network acoustic models using factorised i-vectors", "author": ["P Karanasou", "Y Wang", "MJF Gales", "PC Woodland"], "venue": "Proc. Interspeech, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptive training of deep neural network acoustic models using i-vectors", "author": ["Y Miao", "H Zhang", "F Metze"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 23, no. 11, pp. 1938\u20131949, Nov 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1938}, {"title": "Using neural network front-ends on far field multiple microphones based speech recognition", "author": ["Y Liu", "P Zhang", "T Hain"], "venue": "Proc. IEEE ICASSP, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Recnorm: Simultaneous normalisation and classification applied to speech recognition", "author": ["JS Bridle", "S Cox"], "venue": "Advances in Neural Information Processing Systems 3, 1990, pp. 234\u2013240.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1990}, {"title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code", "author": ["O Abdel-Hamid", "H Jiang"], "venue": "Proc. IEEE ICASSP, 2013, pp. 4277\u20134280.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast adaptation of deep neural network based on discriminant codes for speech recognition", "author": ["S Xue", "O Abdel-Hamid", "J Hui", "L Dai", "Q Liu"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 22, no. 12, pp. 1713\u20131725, Dec 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptation of context dependent deep neural networks", "author": ["H Liao"], "venue": "In Proc. ICASSP. 2013, pp. 7947\u20137951, IEEE.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition", "author": ["D Yu", "K Yao", "H Su", "G Li", "F Seide"], "venue": "Proc. IEEE ICASSP, 2013, pp. 7893\u20137897.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Regularized sequence-level deep neural network model adaptation", "author": ["Y Huang", "Y Gong"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Singular value decomposition based low-footprint speaker adaptation and personalization for deep neural network", "author": ["J Xue", "J Li", "D Yu", "M Seltzer", "Y Gong"], "venue": "Proc. IEEE ICASSP, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptive training using deep neural networks", "author": ["T Ochiai", "S Matsuda", "X Lu", "C Hori", "S Katagiri"], "venue": "Proc. IEEE ICASSP, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Investigating online low-footprint speaker adaptation using generalized linear regression and click-through data", "author": ["Y Zhao", "J Li", "J Xue", "Y Gong"], "venue": "Proc. IEEE ICASSP, 2015.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Differentiable pooling for unsupervised speaker adaptation", "author": ["P Swietojanski", "S Renals"], "venue": "Proc. IEEE ICASSP, 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Hermitian polynomial for speaker adaptation of connectionist speech recognition systems", "author": ["SM Siniscalchi", "J Li", "CH Lee"], "venue": "IEEE Trans Audio, Speech, and Language Processing, vol. 21, pp. 2152\u20132161, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum a posteriori adaptation of network parameters in deep models", "author": ["Z Huang", "S M Siniscalchi", "I-F Chen", "J Wu", "C-H Lee"], "venue": "arXiv preprint arXiv:1503.02108, 2015.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Rapid adaptation for deep neural networks through multi-task learning", "author": ["Z Huang", "J Li", "S M Siniscalchi", "I-F Chen", "J Wu", "C-H Lee"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Structured output layer with auxiliary targets for context-dependent acoustic modelling", "author": ["P Swietojanski", "P Bell", "S Renals"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker adaptation of deep neural networks using a hierarchy of output layers", "author": ["R Price", "K Iso", "K Shinoda"], "venue": "Proc. IEEE SLT, 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K Hornik", "M Stinchcombe", "H White"], "venue": "Neural networks, vol. 2, no. 5, pp. 359\u2013366, 1989.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1989}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["K Hornik"], "venue": "Neural Networks, vol. 4, no. 2, pp. 251 \u2013 257, 1991.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1991}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["AR Barron"], "venue": "IEEE Transactions on Information Theory, vol. 39, no. 3, pp. 930\u2013945, 1993.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1993}, {"title": "Parameterised Sigmoid and ReLU Hidden Activation Functions for DNN Acoustic Modelling", "author": ["C Zhang", "PC Woodland"], "venue": "Proc. Interspeech, 2015.  SUBMITTED TO IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING  13", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Networks with trainable amplitude of activation functions", "author": ["E Trentin"], "venue": "Neural Networks, vol. 14, pp. 471\u2013493, 2001.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2001}, {"title": "Multi-basis adaptive neural network for rapid adaptation in speech recognition", "author": ["C Wu", "M Gales"], "venue": "Proc. IEEE ICASSP, 2015.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Cluster adaptive training for deep neural network", "author": ["T Tan", "Y Qian", "M Yin", "Y Zhuang", "K Yu"], "venue": "Proc. IEEE ICASSP, 2015.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2015}, {"title": "Context adaptive deep neural networks for fast acoustic model adaptation", "author": ["M Delcroix", "K Kinoshita", "T Hori", "T Nakatani"], "venue": "Proc. IEEE ICASSP, 2015.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "A compact model for speaker-adaptive training", "author": ["T Anastasakos", "J McDonough", "R Schwartz", "J Makhoul"], "venue": "Proc. ICSLP, 1996, pp. 1137\u2013 1140.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1996}, {"title": "Recognition and understanding of meetings: The AMI and AMIDA projects", "author": ["S Renals", "T Hain", "H Bourlard"], "venue": "Proc. of the IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU\u201907, Kyoto, 12 2007, IDIAP-RR 07-46.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2007}, {"title": "Overview of the IWSLT 2012 evaluation campaign", "author": ["M Federico", "M Cettolo", "L Bentivogli", "M Paul", "S St\u00fcker"], "venue": "Proc. IWSLT, 2012.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2012}, {"title": "The UEDIN system for the IWSLT 2014 evaluation", "author": ["P Bell", "P Swietojanski", "J Driesen", "M Sinclair", "F McInnes", "S Renals"], "venue": "Proc. IWSLT, 2014.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid acoustic models for distant and multichannel large vocabulary speech recognition", "author": ["P Swietojanski", "A Ghoshal", "S Renals"], "venue": "Proc. IEEE ASRU, 2013.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. M\u00fcller"], "venue": "Neural Networks: Tricks of the Trade, chapter 2. Springer, 1998.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 1998}, {"title": "Applying convolutional neural networks concepts to hybrid NN\u2013HMM model for speech recognition", "author": ["O Abdel-Hamid", "A-R Mohamed", "J Hui", "G Penn"], "venue": "Proc. IEEE ICASSP, 2012, pp. 4277\u20134280.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional neural networks for distant speech recognition", "author": ["P Swietojanski", "A Ghoshal", "S Renals"], "venue": "Signal Processing Letters, IEEE, vol. 21, no. 9, pp. 1120\u20131124, Sept. 2014.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K Vesely", "A Ghoshal", "L Burget", "D Povey"], "venue": "Proc. Interspeech, Lyon, France, August 2013.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2013}, {"title": "The Kaldi speech recognition toolkit", "author": ["D Povey", "A Ghoshal", "G Boulianne", "L Burget", "O Glembek", "N Goel", "M Hannemann", "P Motl\u0131\u0301\u010dek", "Y Qian", "P Schwarz", "J Silovsk\u00fd", "G Stemmer", "K Vesel\u00fd"], "venue": "Proc. IEEE ASRU, December 2011.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2011}, {"title": "Connectionist probability estimation in the DECIPHER speech recognition system", "author": ["S Renals", "N Morgan", "M Cohen", "H Franco"], "venue": "Proc. IEEE ICASSP, 1992.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1992}, {"title": "Catastrophic forgetting in connectionist networks: Causes, consequences and solutions", "author": ["RM French"], "venue": "Trends in Cognitive Sciences, vol. 3, pp. 128\u2013135, 1999.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 1999}, {"title": "A novel loss function for the overall risk criterion based discriminative training of HMM models", "author": ["J Kaiser", "B Horvat", "Z Kacic"], "venue": "Proc. ICSLP, 2000, pp. 887\u2013890.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2000}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["B Kingsbury"], "venue": "Proc. IEEE ICASSP, 2009, pp. 3761\u20133764.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2009}, {"title": "The NICT ASR system for IWSLT 2013", "author": ["C-L Huang", "PR Dixon", "S Matsuda", "Y Wu", "X Lu", "M Saiko", "C Hori"], "venue": "Proc. IWSLT, 2013.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2013}, {"title": "On combining i-vectors and discriminative adaptation methods for unsupervised speaker normalization in dnn acoustic models", "author": ["L Samarakoon", "K C Sim"], "venue": "Proc. IEEE ICASSP, 2016.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2016}, {"title": "Human vs machine spoofing detection on wideband and narrowband data", "author": ["M Wester", "Z Wu", "J Yamagishi"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2015}, {"title": "2000 NIST evaluation of conversational speech recognition over the telephone: English and Mandarin performance results", "author": ["J Fiscus", "WM Fisher", "AF Martin", "MA Przybocki", "DS Pallett"], "venue": "Proc. Speech Transcription Workshop. Citeseer, 2000.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2000}, {"title": "Towards utterancebased neural network adaptation in acoustic modeling", "author": ["I Himawan", "P Motlicek", "M Ferras", "S Madikeri"], "venue": "Proc. IEEE ASRU, 2015.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2015}, {"title": "An investigation of deep neural networks for noise robust speech recognition", "author": ["M Seltzer", "D Yu", "Y Wang"], "venue": "Proc. IEEE ICASSP, 2013.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2013}, {"title": "Annealed dropout training of deep networks", "author": ["SJ Rennie", "V Goel", "S Thomas"], "venue": "Proc. IEEE SLT, 2014.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout networks", "author": ["IJ Goodfellow", "D Warde-Farley", "M Mirza", "A Courville", "Y Bengio"], "venue": "arXiv:1302.4389, 2013.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["LJP van der Maaten", "GE Hinton"], "venue": "Journal of Machine Learning Research, vol. 9: 25792605, Nov 2008.  PLACE PHOTO HERE Pawel Swietojanski Biography text here. PLACE PHOTO HERE Jinyu Li Biography text here. PLACE PHOTO HERE  Steve Renals Biography text here.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Hinton et al [1] report word error rate (WER) reductions between 10\u201332% across a wide variety of tasks, compared with discriminatively trained Gaussian mixture model (GMM) based systems.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "These results use neural networks as part of both hybrid DNN/HMM (hidden Markov model) systems [1]\u2013[5] in which the neural network provides a scaled likelihood estimate to replace the GMM, and as tandem or bottleneck feature systems [6], [7] in which the neural network is used as a discriminative feature extractor for a GMM-based system.", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "These results use neural networks as part of both hybrid DNN/HMM (hidden Markov model) systems [1]\u2013[5] in which the neural network provides a scaled likelihood estimate to replace the GMM, and as tandem or bottleneck feature systems [6], [7] in which the neural network is used as a discriminative feature extractor for a GMM-based system.", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "These results use neural networks as part of both hybrid DNN/HMM (hidden Markov model) systems [1]\u2013[5] in which the neural network provides a scaled likelihood estimate to replace the GMM, and as tandem or bottleneck feature systems [6], [7] in which the neural network is used as a discriminative feature extractor for a GMM-based system.", "startOffset": 233, "endOffset": 236}, {"referenceID": 6, "context": "These results use neural networks as part of both hybrid DNN/HMM (hidden Markov model) systems [1]\u2013[5] in which the neural network provides a scaled likelihood estimate to replace the GMM, and as tandem or bottleneck feature systems [6], [7] in which the neural network is used as a discriminative feature extractor for a GMM-based system.", "startOffset": 238, "endOffset": 241}, {"referenceID": 7, "context": "For many tasks it has been observed that GMM-based systems (with tandem or bottleneck features) that have been adapted to the talker are more accurate than unadapted hybrid DNN/HMM systems [8]\u2013[10], indicating that the adaptation of DNN acoustic models is an important topic that merits investigation.", "startOffset": 189, "endOffset": 192}, {"referenceID": 9, "context": "For many tasks it has been observed that GMM-based systems (with tandem or bottleneck features) that have been adapted to the talker are more accurate than unadapted hybrid DNN/HMM systems [8]\u2013[10], indicating that the adaptation of DNN acoustic models is an important topic that merits investigation.", "startOffset": 193, "endOffset": 197}, {"referenceID": 10, "context": "Acoustic model adaptation [11] aims to normalise the mismatch between training and runtime data distributions that", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "In this paper we investigate unsupervised modelbased adaptation of DNN acoustic models to speakers and to acoustic environments, using a recently introduced method called Learning Hidden Unit Contributions (LHUC) [12]\u2013 [14].", "startOffset": 213, "endOffset": 217}, {"referenceID": 13, "context": "In this paper we investigate unsupervised modelbased adaptation of DNN acoustic models to speakers and to acoustic environments, using a recently introduced method called Learning Hidden Unit Contributions (LHUC) [12]\u2013 [14].", "startOffset": 219, "endOffset": 223}, {"referenceID": 13, "context": "We present the LHUC approach both in the context of test-only adaptation, and an extension to speaker-adaptive training (SAT), referred to as SAT-LHUC [14].", "startOffset": 151, "endOffset": 155}, {"referenceID": 14, "context": "We present an extensive experimental analysis using four standard corpora: TED talks [15], AMI [16], Switchboard [17] and Aurora4 [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "We present an extensive experimental analysis using four standard corpora: TED talks [15], AMI [16], Switchboard [17] and Aurora4 [18].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "We present an extensive experimental analysis using four standard corpora: TED talks [15], AMI [16], Switchboard [17] and Aurora4 [18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "We present an extensive experimental analysis using four standard corpora: TED talks [15], AMI [16], Switchboard [17] and Aurora4 [18].", "startOffset": 130, "endOffset": 134}, {"referenceID": 18, "context": "VI-D); complementarity with feature-space adaptation techniques based on maximum likelihood linear regression [19] (Sec.", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": "The dominant technique for estimating feature space transforms is constrained (feature-space) MLLR, referred to as CMLLR or fMLLR [19].", "startOffset": 130, "endOffset": 134}, {"referenceID": 0, "context": "This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches [1], [4], [8], [9], [20]\u2013[23].", "startOffset": 135, "endOffset": 138}, {"referenceID": 3, "context": "This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches [1], [4], [8], [9], [20]\u2013[23].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches [1], [4], [8], [9], [20]\u2013[23].", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches [1], [4], [8], [9], [20]\u2013[23].", "startOffset": 150, "endOffset": 153}, {"referenceID": 19, "context": "This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches [1], [4], [8], [9], [20]\u2013[23].", "startOffset": 155, "endOffset": 159}, {"referenceID": 22, "context": "This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches [1], [4], [8], [9], [20]\u2013[23].", "startOffset": 160, "endOffset": 164}, {"referenceID": 23, "context": "The linear input network (LIN) [24], [25] defines an additional speakerdependent layer between the input features and the first hidden layer, and thus has a similar effect to fMLLR.", "startOffset": 31, "endOffset": 35}, {"referenceID": 24, "context": "The linear input network (LIN) [24], [25] defines an additional speakerdependent layer between the input features and the first hidden layer, and thus has a similar effect to fMLLR.", "startOffset": 37, "endOffset": 41}, {"referenceID": 3, "context": "has been further developed to include the use of a tied variant of LIN in which each of the input frames is constrained to have the same linear transform \u2013 feature-space discriminative linear regression (fDLR) [4], [26].", "startOffset": 210, "endOffset": 213}, {"referenceID": 25, "context": "has been further developed to include the use of a tied variant of LIN in which each of the input frames is constrained to have the same linear transform \u2013 feature-space discriminative linear regression (fDLR) [4], [26].", "startOffset": 215, "endOffset": 219}, {"referenceID": 26, "context": "There has been considerable recent work exploring the use of i-vectors [27] for this purpose.", "startOffset": 71, "endOffset": 75}, {"referenceID": 27, "context": "I-vectors, which can be regarded as basis vectors which span a subspace of speaker variability, were first used for adaptation in a GMM framework by Karafiat et al [28].", "startOffset": 164, "endOffset": 168}, {"referenceID": 28, "context": "Saon et al [29] used i-vectors to augment the input features of DNN-based acoustic models, and showed that augmenting the input features with 100-dimensional i-vectors for each speaker resulted in a 10% relative reduction in WER on Switchboard (and a 6% reduction when the input features had been transformed using fMLLR).", "startOffset": 11, "endOffset": 15}, {"referenceID": 29, "context": "Gupta et al [30] obtained similar results, and Karanasou et al [31] presented an approach in which the ivectors were factorised into speaker and environment parts.", "startOffset": 12, "endOffset": 16}, {"referenceID": 30, "context": "Gupta et al [30] obtained similar results, and Karanasou et al [31] presented an approach in which the ivectors were factorised into speaker and environment parts.", "startOffset": 63, "endOffset": 67}, {"referenceID": 31, "context": "Miao et al [32] proposed to transform i-vectors using an auxiliary DNN which produced speaker-specific transforms of the original feature vectors, similar to fMLLR.", "startOffset": 11, "endOffset": 15}, {"referenceID": 32, "context": "Other examples of auxiliary features include the use of speaker-specific bottleneck features obtained from a speaker separation DNN used in a distant speech recognition task [33], the use of outof-domain tandem features [23], and speaker codes [34]\u2013[36] in which a specific set of units for each speaker is optimised.", "startOffset": 174, "endOffset": 178}, {"referenceID": 22, "context": "Other examples of auxiliary features include the use of speaker-specific bottleneck features obtained from a speaker separation DNN used in a distant speech recognition task [33], the use of outof-domain tandem features [23], and speaker codes [34]\u2013[36] in which a specific set of units for each speaker is optimised.", "startOffset": 220, "endOffset": 224}, {"referenceID": 33, "context": "Other examples of auxiliary features include the use of speaker-specific bottleneck features obtained from a speaker separation DNN used in a distant speech recognition task [33], the use of outof-domain tandem features [23], and speaker codes [34]\u2013[36] in which a specific set of units for each speaker is optimised.", "startOffset": 244, "endOffset": 248}, {"referenceID": 35, "context": "Other examples of auxiliary features include the use of speaker-specific bottleneck features obtained from a speaker separation DNN used in a distant speech recognition task [33], the use of outof-domain tandem features [23], and speaker codes [34]\u2013[36] in which a specific set of units for each speaker is optimised.", "startOffset": 249, "endOffset": 253}, {"referenceID": 36, "context": "Liao [37] investigated supervised and unsupervised adaptation of different weight subsets using a few minutes of adaptation data.", "startOffset": 5, "endOffset": 9}, {"referenceID": 37, "context": "Yu et al [38] have explored the use of regularisation for adapting the weights of a DNN, using the Kullback-Liebler (KL) divergence between the speaker-independent output distribution and the speakeradapted output distributions, resulting in a 3% relative improvement on Switchboard.", "startOffset": 9, "endOffset": 13}, {"referenceID": 38, "context": "This approach was also recently used to adapt all parameters of sequence-trained models [39].", "startOffset": 88, "endOffset": 92}, {"referenceID": 39, "context": "A variant of this approach reduces the number of speakerspecific parameters through a factorisation based on singular value decomposition [40].", "startOffset": 138, "endOffset": 142}, {"referenceID": 40, "context": "Ochiai et al [41] have also explored regularised speaker adaptive training with a speaker-dependent layer.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "Smaller subsets of the DNN weights may be modified, including output layer biases [26], the bias and slope of hidden units [42] or training the models with differentiable pooling operators [43], which are then adapted in SD fashion.", "startOffset": 82, "endOffset": 86}, {"referenceID": 41, "context": "Smaller subsets of the DNN weights may be modified, including output layer biases [26], the bias and slope of hidden units [42] or training the models with differentiable pooling operators [43], which are then adapted in SD fashion.", "startOffset": 123, "endOffset": 127}, {"referenceID": 42, "context": "Smaller subsets of the DNN weights may be modified, including output layer biases [26], the bias and slope of hidden units [42] or training the models with differentiable pooling operators [43], which are then adapted in SD fashion.", "startOffset": 189, "endOffset": 193}, {"referenceID": 43, "context": "Siniscalchi et al [44] also investigated the use of Hermite polynomial activation functions, whose parameters are estimated in a speaker adaptive fashion.", "startOffset": 18, "endOffset": 22}, {"referenceID": 44, "context": "One can also adapt the top layer in a Bayesian fashion resulting in a maximum a posteriori (MAP) approach [45], or address the sparsity of context-dependent tied-states when few adaptation data-points are available by using multi-task adaptation, using monophones to adapt the context-dependent output layer [46], [47].", "startOffset": 106, "endOffset": 110}, {"referenceID": 45, "context": "One can also adapt the top layer in a Bayesian fashion resulting in a maximum a posteriori (MAP) approach [45], or address the sparsity of context-dependent tied-states when few adaptation data-points are available by using multi-task adaptation, using monophones to adapt the context-dependent output layer [46], [47].", "startOffset": 308, "endOffset": 312}, {"referenceID": 46, "context": "One can also adapt the top layer in a Bayesian fashion resulting in a maximum a posteriori (MAP) approach [45], or address the sparsity of context-dependent tied-states when few adaptation data-points are available by using multi-task adaptation, using monophones to adapt the context-dependent output layer [46], [47].", "startOffset": 314, "endOffset": 318}, {"referenceID": 47, "context": "A similar approach, but using a hierarchical output layer (tied-states followed by monophones) rather than multi-task adaptation, has also been proposed [48].", "startOffset": 153, "endOffset": 157}, {"referenceID": 48, "context": "Under certain assumptions on the family of target functions f\u2217 (as well as on the model structure itself) the neural network can act as an universal approximator [49]\u2013[51].", "startOffset": 162, "endOffset": 166}, {"referenceID": 50, "context": "Under certain assumptions on the family of target functions f\u2217 (as well as on the model structure itself) the neural network can act as an universal approximator [49]\u2013[51].", "startOffset": 167, "endOffset": 171}, {"referenceID": 48, "context": "The properties also hold true when considering deeper (nested) models [49] (Corollaries 2.", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "Image reproduced from [14].", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "The amplitude is modelled using a function \u03be : R \u2192 R \u2013 typically a sigmoid with range (0, 2) [13], but an identity function could be used [52].", "startOffset": 93, "endOffset": 97}, {"referenceID": 51, "context": "The amplitude is modelled using a function \u03be : R \u2192 R \u2013 typically a sigmoid with range (0, 2) [13], but an identity function could be used [52].", "startOffset": 138, "endOffset": 142}, {"referenceID": 52, "context": "The idea of directly learning hidden unit amplitudes was proposed in the context of an adaptive learning rate schedule by Trentin [53], and was later applied to supervised speaker adaptation by Abdel-Hamid and Jiang [12].", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "The idea of directly learning hidden unit amplitudes was proposed in the context of an adaptive learning rate schedule by Trentin [53], and was later applied to supervised speaker adaptation by Abdel-Hamid and Jiang [12].", "startOffset": 216, "endOffset": 220}, {"referenceID": 12, "context": "The approach was extended to unsupervised adaptation, non-sigmoid nonlinearities, and large vocabulary speech recognition by Swietojanski and Renals [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 41, "context": "Other adaptive transfer function approaches for speaker adaptation have also been proposed [42], [44], as have \u201cbasis\u201d approaches [54]\u2013[56].", "startOffset": 91, "endOffset": 95}, {"referenceID": 43, "context": "Other adaptive transfer function approaches for speaker adaptation have also been proposed [42], [44], as have \u201cbasis\u201d approaches [54]\u2013[56].", "startOffset": 97, "endOffset": 101}, {"referenceID": 53, "context": "Other adaptive transfer function approaches for speaker adaptation have also been proposed [42], [44], as have \u201cbasis\u201d approaches [54]\u2013[56].", "startOffset": 130, "endOffset": 134}, {"referenceID": 55, "context": "Other adaptive transfer function approaches for speaker adaptation have also been proposed [42], [44], as have \u201cbasis\u201d approaches [54]\u2013[56].", "startOffset": 135, "endOffset": 139}, {"referenceID": 56, "context": "This motivates combining LHUC with speaker adaptive training (SAT) [57] in which the hidden units are trained to capture both good average representations and speakerspecific representations, by estimating speaker-specific hidden unit amplitudes for each training speaker.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "We experimentally investigated LHUC and SAT-LHUC using four different corpora: the TED talks corpus [15] following the IWSLT evaluation protocol (www.", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "org); the Switchboard corpus of conversational telephone speech [17] (ldc.", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "edu); the AMI meetings corpus [16], [58] (corpus.", "startOffset": 30, "endOffset": 34}, {"referenceID": 57, "context": "edu); the AMI meetings corpus [16], [58] (corpus.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "org); and the Aurora4 corpus of read speech with artificially corrupted acoustic environments [18] (catalog.", "startOffset": 94, "endOffset": 98}, {"referenceID": 4, "context": "The output logistic regression layer models the distribution of context-dependent clustered tied states [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 58, "context": "com) following the IWSLT ASR evaluation protocol [59] (iwslt.", "startOffset": 49, "endOffset": 53}, {"referenceID": 8, "context": "The training data consisted of 143 hours of speech (813 talks) and the systems follow our previously described recipe [9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "In this work however, compared to our previous works [9], [13], [43], our systems employ more accurate language models developed for our IWSLT\u20132014 systems [60]: in particular, the final reported results use a 4-gram language model estimated from 751 million words.", "startOffset": 53, "endOffset": 56}, {"referenceID": 12, "context": "In this work however, compared to our previous works [9], [13], [43], our systems employ more accurate language models developed for our IWSLT\u20132014 systems [60]: in particular, the final reported results use a 4-gram language model estimated from 751 million words.", "startOffset": 58, "endOffset": 62}, {"referenceID": 42, "context": "In this work however, compared to our previous works [9], [13], [43], our systems employ more accurate language models developed for our IWSLT\u20132014 systems [60]: in particular, the final reported results use a 4-gram language model estimated from 751 million words.", "startOffset": 64, "endOffset": 68}, {"referenceID": 59, "context": "In this work however, compared to our previous works [9], [13], [43], our systems employ more accurate language models developed for our IWSLT\u20132014 systems [60]: in particular, the final reported results use a 4-gram language model estimated from 751 million words.", "startOffset": 156, "endOffset": 160}, {"referenceID": 60, "context": "AMI: We follow the Kaldi GMM recipe described in [61]", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "For AMI, we also evaluate the effectiveness of LHUC and SAT-LHUC on AMs that utilise convolutional layers [8], [62], [63], where the CNN networks were trained as described in [64] but with 300 convolutional filters.", "startOffset": 106, "endOffset": 109}, {"referenceID": 61, "context": "For AMI, we also evaluate the effectiveness of LHUC and SAT-LHUC on AMs that utilise convolutional layers [8], [62], [63], where the CNN networks were trained as described in [64] but with 300 convolutional filters.", "startOffset": 111, "endOffset": 115}, {"referenceID": 62, "context": "For AMI, we also evaluate the effectiveness of LHUC and SAT-LHUC on AMs that utilise convolutional layers [8], [62], [63], where the CNN networks were trained as described in [64] but with 300 convolutional filters.", "startOffset": 117, "endOffset": 121}, {"referenceID": 63, "context": "For AMI, we also evaluate the effectiveness of LHUC and SAT-LHUC on AMs that utilise convolutional layers [8], [62], [63], where the CNN networks were trained as described in [64] but with 300 convolutional filters.", "startOffset": 175, "endOffset": 179}, {"referenceID": 64, "context": "Switchboard: We use the Kaldi GMM recipe [65], [66], using Switchboard\u20131 Release 2 (LDC97S62).", "startOffset": 41, "endOffset": 45}, {"referenceID": 65, "context": "Switchboard: We use the Kaldi GMM recipe [65], [66], using Switchboard\u20131 Release 2 (LDC97S62).", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "Aurora4: The Aurora 4 task is a small scale, medium vocabulary noise and channel ASR robustness task based on the Wall Street Journal corpus [18].", "startOffset": 141, "endOffset": 145}, {"referenceID": 12, "context": "LHUC and SAT-LHUC approaches (adapting the other way round \u2013 starting from the top layer \u2013 is much less effective [13]).", "startOffset": 114, "endOffset": 118}, {"referenceID": 37, "context": "This suggests that it is not necessary to carefully regularise the model \u2013 for example, by KullbackLeibler divergence training [38] which is usually required when adapting the weights of one or more layers in a network.", "startOffset": 127, "endOffset": 131}, {"referenceID": 66, "context": "08 and was later adjusted according to the newbob learning scheme [67].", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "In this work we train SAT-LHUC models with frame-level [14], segment-level and speaker-level clusters.", "startOffset": 55, "endOffset": 59}, {"referenceID": 13, "context": "However, the difference, as shown experimentally in [14], is mostly due to poorer quality adaptation targets resulting from the corresponding first pass SAT-LHUC systems rather than the differences in learned representations.", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "If different models for SI and SD decodes are acceptable, then further small gains in accuracy are observed [14].", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "1These results are compatible with further SAT-LHUC results using this tst2013 set in [14]", "startOffset": 86, "endOffset": 90}, {"referenceID": 67, "context": "trained using cross-entropy: a mismatched adaptation objective (here cross-entropy) can easily erase sequence information from the weight matrices due to the well-known effect of catastrophic forgetting [68] in neural networks.", "startOffset": 203, "endOffset": 207}, {"referenceID": 38, "context": "Indeed Huang and Gong [39] report no gain from adapting SE-DNN models with a KL divergence regularised [38] cross-entropy adaptation objective and supervised adaptation targets.", "startOffset": 22, "endOffset": 26}, {"referenceID": 37, "context": "Indeed Huang and Gong [39] report no gain from adapting SE-DNN models with a KL divergence regularised [38] cross-entropy adaptation objective and supervised adaptation targets.", "startOffset": 103, "endOffset": 107}, {"referenceID": 68, "context": "In this work we adapt state-level minimum Bayes risk (sMBR) [69], [70] sequence-trained models with LHUC approach and report results on TED tst2011 and tst2013 in Table VI.", "startOffset": 60, "endOffset": 64}, {"referenceID": 69, "context": "In this work we adapt state-level minimum Bayes risk (sMBR) [69], [70] sequence-trained models with LHUC approach and report results on TED tst2011 and tst2013 in Table VI.", "startOffset": 66, "endOffset": 70}, {"referenceID": 70, "context": "We compared also our adaptation results to the most accurate system of the IWSLT\u20132013 TED transcription evaluation, which performed both feature- and model-space speaker adaptation [71].", "startOffset": 181, "endOffset": 185}, {"referenceID": 40, "context": "For model-space adaptation that system used a method which adapts DNNs with a speaker-dependent layer [41].", "startOffset": 102, "endOffset": 106}, {"referenceID": 70, "context": "IWSLT2013 winner system (numbers taken from [71]) DNN (sMBR) + HUB4 + WSJ 15.", "startOffset": 44, "endOffset": 48}, {"referenceID": 40, "context": "1 +++++ SAT on DNN [41] 7.", "startOffset": 19, "endOffset": 23}, {"referenceID": 59, "context": "5 Our system [60] DNN (sMBR) + AMI data 9.", "startOffset": 13, "endOffset": 17}, {"referenceID": 70, "context": "This allows our single-model system to match a considerably more sophisticated post-processing pipeline [71], as outlined in Table VII.", "startOffset": 104, "endOffset": 108}, {"referenceID": 31, "context": "In related work [32] LHUC was employed using alignments obtained from an SIGMM system with a 8.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "3Due to space constraints we do not make an explicit comparisons to other techniques such as auxiliary i-vector features or speaker-codes; however, the literature suggest that the use of i-vectors give similar [29] results when compared to fMLLR trained models.", "startOffset": 210, "endOffset": 214}, {"referenceID": 31, "context": "Related recent studies also show LHUC is at least as good as the standard use of i-vector features [32], [72].", "startOffset": 99, "endOffset": 103}, {"referenceID": 71, "context": "Related recent studies also show LHUC is at least as good as the standard use of i-vector features [32], [72].", "startOffset": 105, "endOffset": 109}, {"referenceID": 3, "context": "feature-space discriminative linear regression (fDLR) [4], but neither of these matches SAT trained feature transform models.", "startOffset": 54, "endOffset": 57}, {"referenceID": 72, "context": "This could be due to the fact Switchboard data is narrow-band and as such contains less information for discrimination between speakers [73], especially when estimating relevant statistics from small amounts of unsupervised adaptation data.", "startOffset": 136, "endOffset": 140}, {"referenceID": 73, "context": "Another potential reason could be related to the fact that the Switchboard part of eval2000 is characterised by a large overlap between training and test speakers \u2013 36 out of 40 test speakers are observed in training [74], which limits the need for adaptation, but also enables models to learn much more accurate speaker-characteristics during supervised speaker adaptive training.", "startOffset": 217, "endOffset": 221}, {"referenceID": 74, "context": "The idea is similar to LHUC applied to channel normalisation between distant and close talking scenarios [75], except we use two independently estimated transforms.", "startOffset": 105, "endOffset": 109}, {"referenceID": 75, "context": "We adapted baseline multi-condition trained DNN models [76] to the speaker (rS) and the environment (rE).", "startOffset": 55, "endOffset": 59}, {"referenceID": 76, "context": "We also train more competitive models following Rennie et al [77]: Maxout [78] CNN models were trained using annealed dropout.", "startOffset": 61, "endOffset": 65}, {"referenceID": 77, "context": "We also train more competitive models following Rennie et al [77]: Maxout [78] CNN models were trained using annealed dropout.", "startOffset": 74, "endOffset": 78}, {"referenceID": 76, "context": "However, in contrast to [77], in this work we are", "startOffset": 24, "endOffset": 28}, {"referenceID": 76, "context": "5% in [77]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 78, "context": "Finally, we visualise the top hidden layer activations of the annealed dropout Maxout CNN using stochastic neighbourhood embedding (tSNE) [79] for one utterance recorded under clean and noisy (restaurant) conditions (Fig.", "startOffset": 138, "endOffset": 142}], "year": 2017, "abstractText": "This work presents a broad study on the adaptation of neural network acoustic models by means of learning hidden unit contributions (LHUC) \u2013 a method that linearly re-combines hidden units in a speakeror environment-dependent manner using small amounts of unsupervised adaptation data. We also extend LHUC to a speaker adaptive training (SAT) framework that leads to more adaptable DNN acoustic model, which can work in both a speaker-dependent and a speaker-independent manner, without the requirement to maintain auxiliary speakerdependent feature extractors or to introduce significant speakerdependent changes to the DNN structure. Through a series of experiments on four different speech recognition benchmarks (TED talks, Switchboard, AMI meetings and Aurora4) and over 270 test speakers we show that LHUC in both its test-only and SAT variants results in consistent word error rate reductions ranging from 5% to 23% relative depending on the task and the degree of mismatch between training and test data. In addition we have investigated the effect of the amount of adaptation data per speaker, the quality of adaptation targets when estimating transforms in an unsupervised manner, the complementarity to other adaptation techniques, one-shot adaptation, and an extension to adapting DNNs trained in a sequence discriminative manner.", "creator": "LaTeX with hyperref package"}}}