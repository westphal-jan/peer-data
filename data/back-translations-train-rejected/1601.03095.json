{"id": "1601.03095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2016", "title": "Submodular Optimization under Noise", "abstract": "We consider the problem of maximizing monotone submodular functions under noise, which to the best of our knowledge has not been studied in the past. There has been a great deal of work on optimization of submodular functions under various constraints, with many algorithms that provide desirable approximation guarantees. However, in many applications we do not have access to the submodular function we aim to optimize, but rather to some erroneous or noisy version of it. This raises the question of whether provable guarantees are obtainable in presence of error and noise. We provide initial answers, by focusing on the question of maximizing a monotone submodular function under cardinality constraints when given access to a noisy oracle of the function. We show that:", "histories": [["v1", "Tue, 12 Jan 2016 23:05:24 GMT  (691kb,D)", "https://arxiv.org/abs/1601.03095v1", null], ["v2", "Tue, 12 Apr 2016 22:24:46 GMT  (128kb,D)", "http://arxiv.org/abs/1601.03095v2", null], ["v3", "Fri, 4 Nov 2016 21:33:37 GMT  (126kb,D)", "http://arxiv.org/abs/1601.03095v3", null]], "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.GT", "authors": ["avinatan hassidim", "yaron singer"], "accepted": false, "id": "1601.03095"}, "pdf": {"name": "1601.03095.pdf", "metadata": {"source": "CRF", "title": "Submodular Optimization under Noise", "authors": ["Avinatan Hassidim", "Yaron Singer"], "emails": ["avinatan@cs.biu.ac.il", "yaron@seas.harvard.edu"], "sections": [{"heading": null, "text": "\u2022 For a cardinality restriction k \u2265 2 there is an approximation algorithm whose approximation algorithm is arbitrarily close to 1 \u2212 1 / e; \u2022 For k = 1 there is an algorithm whose approximation algorithm is arbitrarily close to 1 / 2. No randomized algorithm can obtain an approximation algorithm better than 1 / 2 + o (1); \u2022 If noise is counterproductive, no non-trivial approximation guarantee can be obtained. \u2022 Supported by ISF 1241 / 12; \u2020 Supported by NSF funding CCF-1301976, CAREER CCF-1452961, Google Faculty Research Award, Facebook FacultyAward.ar Xiv: 160 1.03 095v 3 [cs.D S] 4 November 2Contents"}, {"heading": "1 Introduction 1", "text": "1.1 Main result......................................................................................................................... 41.3 applications....................................................."}, {"heading": "2 Optimization for Large k 6", "text": "2.1. Smooth, greedy algorithms.........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3 Optimization for Small k 12", "text": "3.1 Combinatorial averages.................................................................................................. 133.3.. Smoothing guarantees............................................................................."}, {"heading": "4 Optimization for Very Small k 16", "text": "Smoothing guarantees.................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "6 Impossibility for Adversarial Noise 23", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 More related work 26", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 Acknowledgements 28", "text": "Annex 36"}, {"heading": "A Combinatorial Smoothing 37", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Optimization for Large k 45", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C Optimization for Small k 60", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D Optimization for Very Small k 72", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "E Noise Distributions 77", "text": "F Other examples 79"}, {"heading": "1 Introduction", "text": "This paper examines the effects of errors and noise on submodular optimization. (...) A function f: 2N \u2192 R, which refers to a soil group N of size n, is submodular if it can be defined in a way that it can be defined in a way. (...) In any case, it is possible that the submodular functions require a representation that is exponential in the size of the soil group, and the assumption is that we get access to a value oracle that has a set of S\\ T (a). (...) It is known that submodular functions require a representation that is exponential in the size of the soil group and that we get access to a value oracle that gives us a set of f (S)."}, {"heading": "1.1 Main result", "text": "We have to deal with the problem that we find ourselves in a position to make an insufficient payment for an insufficient payment. () We have in our hands an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment. () We have the payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment. () We have the payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment for an insufficient payment."}, {"heading": "1.2 Extensions", "text": "One of the appealing aspects of the noise model and algorithms is that they can easily be extended to a variety of related models. In Section 5 we discuss the application to additive noise, edge noise, correlated noise, information degradation, and approximate submodularity."}, {"heading": "1.3 Applications", "text": "In fact, the fact is that it is a question of a way in which it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what it is about the question, to what is about the question, to what extent it is about the question, to what it is about the question to what is about the question, to what is about the question to what is about the question, to what is about the question to what is about the question, to what is about the question, to what extent it is about the question to what is about the question, to what is about the question to what is about the question, to what is about the question to what is about the question, to what is about the question to what is to what extent it is about the question, to what is about the question to what is about the question, to what is about the question to what is the question, to what extent it is about the question to what is about the question, to what extent it is about the question, to what is to what extent it is about the question, to what is about the question, to what is the question to what is to what is to what is about the question, to what is to what is to what is to what is the question, to what is to what is to what is to what is the question"}, {"heading": "1.4 Paper organization", "text": "The most important technical contribution of the paper is the algorithms for the three different regimes of k. The presentation of the algorithms is contained in sections 2, 3 and 4, which can be read independently of each other. For each algorithm, we suppress evidence and additional quotations to the corresponding section in the appendix. All algorithms use smoothing arguments, which can be found in Appendix A. The smoothing arguments are used as a black box in the evidence of each algorithm and are not required for reading the main presentation. In section 5, we discuss extending the algorithms to related models. In section 6, we prove the result for contrary noise. The discussion of additional related work takes place in section 7.5."}, {"heading": "2 Optimization for Large k", "text": "In this section, we describe the SLICK-GREEDY algorithm, whose approximation guarantee is arbitrarily close to 1 \u2212 1 / e for sufficiently large k. The algorithm is deterministic and can be applied to any desired level of accuracy > 0 if the cardinality restriction is k (log n / 2), or more precisely, if k \u2265 3168 log n / 2. First, we describe and analyze the SMOOTH-GREEDY algorithm, which is then used by the SLICK-GREEDY algorithm as a subroutine."}, {"heading": "2.1 The Smooth Greedy Algorithm", "text": "We start with the description of the smoothing technique used by SMOOTH-GREEDY. We select an arbitrary quantity H (for a certain element a) and assume for the rest of this section that H is an arbitrary quantity of size, where \"depends on k.\" In the case where k \u2265 2400 log nwe '= 25 log n are used, and if k < 2400 log nwe are used \"= 33 log nwe.\" The exact choice for \"will be clarified later in this section. Intuitively,\" on the one hand, it is small enough for us to afford to sacrifice \"elements for smoothing out the noise, and on the other hand, it is\" large enough for taking all subsets to give us a large smoothing environment that allows us to apply concentration.Definition: \"for a set S N and some fixed quantities H N of size.\""}, {"heading": "2.1.1 The algorithm", "text": "\"We do not even have the idea that we will be able to find a solution for which we will find an optimal solution for an optimal solution.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\". \"\" \"We.\" \"\". \"\" \"\". \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \".\" \"\". \"\" \".\" \".\" \"\". \"\" \"\" \".\" \"\" \"\". \"\". \"\" \".\" \"\" \".\" \".\". \".\" \".\". \".\" \".\". \".\". \"\". \"\" \".\". \".\". \".\". \".\". \"\". \"\". \".\". \"\". \".\". \".\" \".\". \"\" \".\". \"\". \"\". \"\". \"\" \".\" \".\" \".\". \"\" \".\" \".\" \"\" \".\" \"\" \".\". \".\" \"\". \".\" \"\" \".\". \".\". \".\" \"\" \".\". \".\". \"\" \"\" \".\". \".\". \".\". \"\". \"\". \".\". \"\". \"\" \".\". \"\". \"\". \"\" \".\". \"\". \".\" \".\". \".\". \".\". \".\" \".\". \".\". \".\" \".\" \".\" \".\" \"\" \".\". \"\". \".\". \".\". \"\". \"\". \"\". \"\" \".\" \".\" \"\". \"\". \"\" \".\" \".\". \".\". \".\" \".\" \".\". \".\" \".\". \".\" \".\". \"\". \"\". \".\" \"\" \".\". \".\". \".\". \".\". \""}, {"heading": "2.1.2 Smoothing guarantees", "text": "The first step is to prove lemmas 2.1. This problem shows that at each step as SMOOTH-GREEDY, this element adds the element that maximizes the noisy value argmaxa / \u0441H F-A, this element almost maximizes the (non-noisy) smooth limit contribution FS, with a high probability. Lemma 2.1. For each fixed > 0, let us consider a -relevant iteration of SMOOTH-GREEDY, where S is the set of elements selected in previous iterations, and a series of elements that have a maximum / minimum H-F value (S-B). To prove the above problem, we use claims B.1, B.2 and B.3. The statements and evidence can be found in Appendix B and are best understood after reading section A in Appendix A."}, {"heading": "2.1.3 Approximation guarantee", "text": "Lemma 2.1 makes us forget about noise, at least for the rest of the analysis of SMOOTH-GREEDY. We can now focus on the consequences of selecting an element that maximizes (up to factor 1 \u2212 \u03b4) FS and not the true limit contribution fS. Claim 2.2. If FS (a) \u2265 (1 \u2212 \u03b4 2 / 4k. Suppose the iteration is -relevant and leave b \u2212 argmaxb / \u0445H fH (b).If FS (1 \u2212 \u03b4) FS (b?) FS (b?), then: fS (a) \u2265 (1 \u2212) fH (b?).Appendix is similar to Claim B.1. In this version we have a weaker condition, since FS (a) is not larger than FS (b?), but rather (1 \u2212 \u03b4) FS (b?).RES (b?), but the claim is less general as it just needs to be taken for b?. Let's use a slightly different approach to prove this claim."}, {"heading": "2.2 Slick Greedy: Optimal Approximation for Sufficiently Large k", "text": "The reason why SMOOTH-GREEDY cannot arbitrarily approximate 1 \u2212 1 / e is because a significant portion of the value can be attributed to the optimal solution H. This would be solved if we had a way to guarantee that the contribution of H is low. The idea behind SLICK-GREEDY is to obtain this kind of warranty. Intuitively, by choosing the \"best\" solution, we ensure that the contribution of the smoothing set is relatively low by performing a large, though constant number of cases of SMOOTH-GREEDY with different smoothing sets."}, {"heading": "2.2.1 The algorithm", "text": "We can describe the SMOOTH-GREEDY procedure as follows: \"We can describe the SMOOTH-GREEDY procedure as follows. We can describe the SMOOTH-GREEDY procedure as follows. We can follow the SMOOTH-GREEDY procedure. We can follow the solution Tj-GREEDY-GREEDY procedure. We have followed the SMOOTH-GREEDY procedure. We can follow the SMOOTH-GREEDY procedure. We compare the solution Tj-GREEDY procedure with the best Si-Hi-Hi procedure. We have seen so far that we are using the SMOOTH-GREEDY procedure."}, {"heading": "2.2.2 Generalizing guarantees of smooth greedy", "text": "Let Sl be the amount returned by SMOOTH-GREEDY initialized with Rl and Hl its smoothing quantity. Then, for each fixed > 0, if k \u2265 36 '/ 2 w.P., we have at least 1 \u2212 1 / n3: f (Sl-Hl) \u2265 (1 \u2212 1 / e \u2212 2 / 3) OPT."}, {"heading": "2.2.3 The smooth comparison procedure", "text": "We can now describe the SMOOTH-COMPARE method that we use in the algorithm. For a given set of Hij-N of size \"and two sets of Ti, Tj-N\\ Hij, we compare f \u00b2 (Ti \u00b2 H \u00b2 ij) with f \u00b2 (Tj \u00b2 H \u00b2 ij) for all H \u00b2 ij. We choose Ti if in most comparisons with H \u00b2 ij \u00b2 Hij (approximate) when we select this f \u00b2 (Ti \u00b2 H \u00b2 ij) for all H \u00b2 ij, and otherwise we choose Tj.10Algorithm 3 SMOOTH \u2212 COMPARE Input: Ti, Tj, Hij \u00b2 N\\ (Ti \u00b2 Tj), 1: approximate f \u00b2 for these elements (Ti \u00b2 H \u00b2 H \u00b2 ij) if they are approximate (Tj \u00b2 H \u00b2) if they are approximate."}, {"heading": "2.2.4 Approximation guarantee of SLICK GREEDY", "text": "Finally, all in all, we can prove the main result of this section (see appendix??).Theorem 2.1. Let f: 2N \u2192 R be a monotonous submodular function. For each fixed > 0, if k \u2265 3168 log n / 2, then given access to a loud oracle whose sound distribution has a generalized exponential tail, the SLICK-GREEDY algorithm returns a set that is a (1 \u2212 1 / e \u2212) approximation to maxS: | S | \u2264 k f (S), with a probability of at least 1 \u2212 1 / n.11"}, {"heading": "3 Optimization for Small k", "text": "If k is small, we cannot apply the smoothing technique from the previous section, as it requires the smoothing set to be included in the solution (log n). In this section, we describe the method of the tested mean, which can be applied to k-2N (1 /) and O (log n), leading to an approximation of 1 \u2212 1 / e. This result is achieved by applying a greedy algorithm to a replacement function F: 2N \u2192 R +, which we call the calculated mean of f. Using the replacement function makes it relatively easy to achieve the approximation of 1 \u2212 1 / e, albeit in anticipation. The most important technical challenge is the transition from a warranty held in expectation to a warranty that is held with high probability. This difficulty only limits the applicability of this method if k fluctuates between B (1 /) and O (log n) and the generalized exponential Tail property is greatly exploited."}, {"heading": "3.1 Combinatorial averaging", "text": "The sampled Mean method is based on mean values to find elements whose limit is high and which can then be added greedily to the solution. (Sampled Mean method comes from continuous optimization.) Consider the optimization of a function f: Rn \u2192 R, which for each point x: Rn \u2192 R has the value f: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: x: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c: c."}, {"heading": "3.2 The Sampled Mean Greedy Algorithm", "text": "The SM-GREEDY starts with the blank set S and looks at all bundles of size c-O (1 /) for each iteration to add to S. With each iteration, the algorithm first identifies the bundle A that maximizes the loud mean. After identifying A, it then looks at all possible bundles of Aij and takes the one whose noisy mean is the largest. We formally describe the algorithm below. Algorithm 4 SM-GREEDY Input: Budget k, Precision > 0, c-O (1) 1: S \u2190 2: during | S | < c \u00b7 K c \u00b2 do 3: A \u2190 argmaxB: | = c F (S-GREEDY Input: Budget k, Precision > 0, c \u00b2 O (1) 1: S \u00b2 2: during | S \u00b2 K c \u00b2 do 3: A \u00b2 argmaxB: = c \u00b2 argmaxB (S \u00b2 maxi) 4: S \u00b2 A \u00b2 S \u00b2 (2), S \u00b2 A \u00b2 A: maxi (S \u00b2)"}, {"heading": "3.3 Smoothing Guarantees", "text": "For each > 0 and any amount of S-N, leave A-arg maxA: | A | = 1 / fS (A).Then: (1 \u2212) fS (A?) \u2264 FS (A?) \u2264 fS (A?).The proof is in Appendix C and uses a natural property of submodular functions: the removal of a random element from a large group does not significantly affect its value as expected. Significant iterations. Similar to the previous section, we define an assumption about the iterations of the algorithm that allows us to apply the smoothing technique in this paragraph.13Definition. Letter B-argmaxB: | = c fS-armaxB: (B).An iteration of SM-GREEDY is -significant when selected for the given group S."}, {"heading": "3.4 Approximation Guarantee in Expectation", "text": "Lemma 3.3. Suppose that in each \u03b4 / 4-significant iteration of SM-GREEDY, if S are the elements selected in previous iterations, A-argmaxB: | B | = c F-iterations (S-B), the bundle A-significant iteration of SM-GREEDY, if S are the elements selected in previous iterations. Suppose S is the solution according to bk / cc iterations. Then, this problem implicitly proves an approximation guarantee that applies in expectation. This is simply because we know that if we have chosen A-A = A- {aj} equally randomly across all decisions of i-c = (1-1 / e-5\u03b4), we will get E [fS (A-A) > 1-cc-iterations (A) equally across all decisions of i-c []."}, {"heading": "3.5 From Expectation to High Probability", "text": "From lem 3.3.3 we know that A-argmaxB: \"A-argmaxB:\" A-argmaxB: \"A-argmaxi:\" A-argmaxi: \"A-argmaxi:\" A-argmaxi: \"A-argmaxi:\" A-argmaxi: \"A-argmaxi:\" A-argmaxi: \"A-argmaxi:\" A-argmaxi: \"A-argmaxB:\" A-argmaxB: \"A-argmaxB:\" A-argmaxB \":\" A-argmaxi: \"A-argmaxB:\" A-argmaxB \":\" A-argxi B: \"A-argxi B:\" A-argxi B: \"A-argxi B:\" A-argxi B: \"maxA-argxi B:\" A-argxi B: \"maxA-argxi B:\" A-argxi B: \"maxxi B:\" A-argxi B: \"maxxi B:\" A-argxi B: \"A-argxi A-argxi A:\" maxB: \"A-argxi B:\" A-argxi B: \"A-argxi A-argxi B:\" maxmaxB: \"A-argxi B:\" A-argxi B: \"A-argxi A:\" A-argxi A-argxi A: \"A-argmaxA-argmaxB:\" A-argmaxA-argmaxB: \"A:\" A-argmaxA: A-argmaxB A-argmaxB: A-argmaxA: A-argmaxA-argmaxA: A-argmaxA-A: A-A: A-argmaxB: A-A-argmaxB: A-argmaxB: A-A-A-argmaxB: A: A-A"}, {"heading": "4 Optimization for Very Small k", "text": "For small constants, we propose another algorithm that uses a different smoothing technique, the algorithm is simple and applies the same principles as those of the previous section. We show that this simple algorithm achieves an approximation ratio that is arbitrarily close to 1 \u2212 1 / e w.h.p at k > 2 and in anticipation at k = 2. For k = 1, we get arbitrarily close to 1 / 2, which is narrow. We show lower limits for small values of k, especially if k = 1 shows that no algorithm can achieve an expected approximation ratio better than 1 / 2 + o (1). All proofs and details are in Appendix D."}, {"heading": "4.1 Smoothing Guarantees", "text": "The smoothing is simple here: For each group A consider the smoothing neighborhood H (A) = {A-x: x / A-x}, F (A) = EX-H (A) [f (X)] and F-H (A) = EX-H (A) [f-x (X)].Lemma 4.1. Leave A-argmaxB: | B-k-F-A (B).Then for each fixed > 0-p-1 \u2212 e-p (2 (n-k)): F-A (1 \u2212) max B: | B-k-F (B)."}, {"heading": "4.2 An Approximation Algorithm for Very Small k", "text": "Approximate guarantee in expectation. The algorithm simply chooses the set A to be a random set of k elements from a random set of H (A), where A: | B | = k F (B).For each constant k and each fixed > 0 this is an approximation (k / (k + 1) \u2212) in expectation (see theorem D.1).High probability. To get a result that holds, we will consider a modest variant of the above algorithm. The algorithm enumerates all possible subsets of the quantity k \u2212 1 and identifies the set A: | argmaxB: | B | = k \u2212 1 F (B).The algorithm then returns a modest variant of the above algorithm: A: argmaxX \u2022 H (A) f (X).Theorem 4.2. For each submodular function f: 2N \u2192 R and each fixed > 0 and constant k, there is a (1 / \u2212 approximation) with a probability of at least S (S)."}, {"heading": "4.3 Information Theoretic Lower Bounds for Constant k", "text": "Surprisingly, even for k = 1 no algorithm can achieve an approximation better than 1 / 2, proving a separation between capital and small k. In claim D.2 we show that no randomized algorithm with a loud oracle can achieve an approximation better than 1 / 2 + O (1 / \u221a n) for maxa \u0435N f (a), and in claim D.3 an approximation better than (2k \u2212 1) / 2k + O (1 / \u221a n) for the optimal set of magnitude k. 3The dependence on this begins in claim C.2, where we are bound to the variation of c \u2212 1 setsA-i, and therefore the smoothing depends on c \u2265 4 /.16."}, {"heading": "5 Extensions", "text": "In this section we will look at enhancements to the noise model optimization. In particular, we will show that the algorithms can be applied to several related problems: additive noise, edge noise, correlated noise, information degradation, and approximate submodularity."}, {"heading": "5.1 Additive Noise", "text": "In the course of this work, we assumed that noise is multiplicative, i.e. we defined the noise oracle to return f-S-S-S-S-S-S-S. An alternative model is one where noise is additive, i.e. f-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S"}, {"heading": "5.2 Marginal Noise", "text": "An alternative noise model is one in which the noise acts on the marginal of the distribution. In this model, a query of the oracle is a pair of sentences S, T N and the oracle returns in the multiplicative marginal noise model and fS (T) + \u043fS, T in the additive marginal noise model. Adversarial additive marginal noise is generally impossible. If the error is adversarial and the noise is additive, the lower limit of 6.1 follows for each magnitude of noise. If we define the maximum magnitude of noise, we consider a function in which no element ever contributes more than, and then it does not help to obtain marginal information. Adversarial multiplicative marginal noise is approximable. If the marginal error is adversarial but multiplicative within the factor \u03b1, it is known that a quantity can be obtained."}, {"heading": "5.3 Correlated Noise", "text": "This means that we cannot engage in a uniform distribution. (...) We then have an interesting class of distributions for which we request optimal guarantees. (...) We have an optimal distribution for correlated distributions. (...) We have taken the first step of showing algorithms for the i.i.d.d. in the space model whether this assumption is necessary. (...) Even for the uniformity of distributions there are simple spatial distributions for which no algorithms are shown. (...) Theorem 5.1. (...) There are simple spatially correlated distributions for which no algorithms can be achieved. (...)"}, {"heading": "5.4 Information Degradation", "text": "The generalization to a case in which the algorithm receives more information each time, but 20 represents a degradation of the information, is simple: Whenever the algorithms presented here want to query a point several times and transmit the expected value of the point with all the information that one has to the algorithm, it makes sense to concentrate on the extreme case in which only the first query is helpful, as is usual in the literature on noisy optimization (e.g. [12])."}, {"heading": "5.5 Approximate Submodularity", "text": "That is, we assume that there is an underlying submodular function that we cannot understand in terms of the function classes we are aiming for. If we assume that the function is an adversarial approximation (1 \u00b1) of a submodular function, it is interesting to understand the limits of efficient optimization in terms of the function classes we are aiming for."}, {"heading": "6 Impossibility for Adversarial Noise", "text": "In this section, we show that there are very simple submodular functions for which no randomized algorithm with access to a -faulty oracle can obtain an adequate approximation guarantee with a subexponential number of queries to the oracle. Intuitively, the main idea behind this result is to show that a noisy oracle can make it difficult to distinguish between two functions whose values can be very far apart. The functions we use are similar to those used to demonstrate the subordinate limits of submodular optimization and learning [79, 84, 36, 95].Theorem 6.1. No randomized algorithm can achieve an approximation that is strictly better than O (n \u2212 1 + 2 + 2) to maximize the monotonous submodular functions using a cardinality constraint."}, {"heading": "7 More related work", "text": "In fact, the fact is that most of them are able to survive on their own, without there being a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which is a process, in which there is a process, in which there is a process, in which there is a process, in which is a process, in which there is a process, in which there is a process, in which is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which is a process, in which there is a process, in which there is a process, a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, a process, in which is a process, a process, a process, a process, a process in which is a process"}, {"heading": "8 Acknowledgements", "text": "A.H. was supported by ISF 1241 / 12; Y.S. was supported by the NSF grant CCF-1301976, CAREER CCF-1452961, a Google Faculty Research Award and a Facebook faculty gift. We thank Vitaly Feldman for pointing out the request for active learning. We are deeply indebted to Lior Seeman, who has carefully read earlier versions of the manuscript and made several valuable inspiration.28"}, {"heading": "A Combinatorial Smoothing", "text": "In this section, we illustrate a reasonable approach to evaluate the true value of a point that we will use in the neighborhood of the following values: \"A,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"\" S, \"\" \",\" \",\" \",\" \",\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \"\", \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \"\", \"\" \"\" \",\" \"\" \",\" \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\" \"\", \"\" \",\" \"\", \"\" \"\", \"\" \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\" \",\" \"\" \"\""}, {"heading": "B Optimization for Large k", "text": "The fS (a).S (a).S (b).S (b).S (b).S (a).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S.S (b).S).S (b).S).B (.S).S (.S).S (.S).S (.S).S (.S).S (.S).S (.S).B (b).S (b).S (S).S (b).S (S).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (b).S (.S (b).S (b).S (b).S (.S (.S (b).S (b).S (b).S (b).S (.S (.S (b).S (.S).S (b).S (b).S (b).S (.S (b).S (.S (b).S (.S (.S).S ("}, {"heading": "C Optimization for Small k", "text": "(A)..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "D Optimization for Very Small k", "text": "Let A-argmaxB (A-argmaxB: | B-argmaxB: | B-k (B) max B: | B-k (B) max B: | B-k (B) max B: | B-k-F (B) proof. The proof follows the same reasoning as the previous sections. Let A-k \u2212 \u2212 \u2212 B-k-F (B) max B: We will show that w.h.p. does not have a sentence B (B) for which F (B) < (1 \u2212) F (A?) F (A?) suggests A. The size of the smoothing quantity is t = n \u2212 k, and the upper limit is a noise multiplier.Note that the optimality of A? and submodularity of A (A?) implies that f (A?"}, {"heading": "An Approximation Algorithm for Very Small k", "text": "(A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A)"}, {"heading": "E Noise Distributions", "text": "As discussed in the introduction, our goal was to enable the noise distribution in the model to be potentially Gaussian, exponential, uniform and generally limited. It was important for us that the algorithm ignore the specific noise distribution and rely on its properties only in analysis. To achieve this, we introduce the class of generalized exponential tail distributions. We remember the definition from introduction. Definition. A noise distribution D has a generalized exponential distribution, if there is one, that for x > x0 the probability density function (x) = e \u2212 g (x), where g (x) = i aix\u03b1i's does not assume that all the distributions are integers, but only these are."}, {"heading": "F Additional Examples", "text": "In this section we will show some examples of how greedy and its variants are due to errors and noise.Greedy fails due to errors and mistakes. In this case, the classic problem is an example of maximizing a monotonous submodular functionality. In this case, it is an example of how greedy handles errors. In this case, there is a family of sets A, where all sets cover the same two sets, and another family of discounters B, each covering a single universe. In this case, it is an oracle that rates the sets as follows. In this case, there is a family of sets A, where all sets cover the same two sets, and another family of discounters B, each covering a single universe-egg-egg-egg-egg-egg-egg-egg-egg-egg-egg-egg-egg-egg-egg-egg-egg-egg-egg-egg-egg-egg."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We consider the problem of maximizing a monotone submodular function under noise.<lb>There has been a great deal of work on optimization of submodular functions under various<lb>constraints, resulting in algorithms that provide desirable approximation guarantees. In many<lb>applications, however, we do not have access to the submodular function we aim to optimize,<lb>but rather to some erroneous or noisy version of it. This raises the question of whether prov-<lb>able guarantees are obtainable in presence of error and noise. We provide initial answers, by<lb>focusing on the question of maximizing a monotone submodular function under a cardinality<lb>constraint when given access to a noisy oracle of the function. We show that:<lb>\u2022 For a cardinality constraint k \u2265 2, there is an approximation algorithm whose approxi-<lb>mation ratio is arbitrarily close to 1\u2212 1/e;<lb>\u2022 For k = 1 there is an algorithm whose approximation ratio is arbitrarily close to 1/2. No<lb>randomized algorithm can obtain an approximation ratio better than 1/2 + o(1);<lb>\u2022 If the noise is adversarial, no non-trivial approximation guarantee can be obtained. \u2217Supported by ISF 1241/12;<lb>\u2020Supported by NSF grant CCF-1301976, CAREER CCF-1452961, Google Faculty Research Award, Facebook Faculty<lb>Award.<lb>ar<lb>X<lb>iv<lb>:1<lb>60<lb>1.<lb>03<lb>09<lb>5v<lb>3<lb>[<lb>cs<lb>.D<lb>S]<lb>4<lb>N<lb>ov<lb>2<lb>01<lb>6", "creator": "LaTeX with hyperref package"}}}