{"id": "1601.03288", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2016", "title": "Predicting the Effectiveness of Self-Training: Application to Sentiment Classification", "abstract": "The goal of this paper is to investigate the connection between the performance gain that can be obtained by selftraining and the similarity between the corpora used in this approach. Self-training is a semi-supervised technique designed to increase the performance of machine learning algorithms by automatically classifying instances of a task and adding these as additional training material to the same classifier. In the context of language processing tasks, this training material is mostly an (annotated) corpus. Unfortunately self-training does not always lead to a performance increase and whether it will is largely unpredictable. We show that the similarity between corpora can be used to identify those setups for which self-training can be beneficial. We consider this research as a step in the process of developing a classifier that is able to adapt itself to each new test corpus that it is presented with.", "histories": [["v1", "Wed, 13 Jan 2016 15:55:36 GMT  (394kb,D)", "http://arxiv.org/abs/1601.03288v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vincent van asch", "walter daelemans"], "accepted": false, "id": "1601.03288"}, "pdf": {"name": "1601.03288.pdf", "metadata": {"source": "CRF", "title": "Predicting the Effectiveness of Self-Training: Application to Sentiment Classification", "authors": ["Vincent Van Asch", "Walter"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION IF the development and testing of techniques to enhance the performance of natural language processing systems, the choice of corpora to test the technique may influence the effectiveness of the new technique. Therefore, it is important to introduce a second dimension in addition to the performance values that can describe the selected corpora. We have decided to introduce similarity values in a self-training setup to identify the arrangements for which the self-training technique is useful."}, {"heading": "A. Self-training procedure", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "II. RELATED RESEARCH", "text": "The concepts of self-training were first introduced by [1], but more recent examples are given by [7], [8], and [2]. There are many variations of self-training. Reference [9] combines the concept of self-training with ensemble learning for the classification of citations. Instead of using a single classifier that needs to be trained, they use an ensemble of classifiers. Another interesting approach is to combine self-training with active learning [10]. Instead of using the entire training to label the unmarked data, a portion is kept apart. These heldout data are labeled along with the unmarked data in self-training step 1. The most trusted classified instances of the unmarked data are added along with the least confidential data. The intention is to select useful data in a more rigorous manner. It is possible to use similarities between companies as such to draw conclusions about them."}, {"heading": "III. EXPERIMENTAL DESIGN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Definitions", "text": "There are two levels of evaluation in this work. The first level is the self-training experiment and the second level is the evaluation of how well self-training gains can be predicted. To achieve maximum clarity, some explicit definitions are given here first. Similarity A number that expresses the degree to which two corpora are similar. The higher this number, the less similar is the corpora. The exact meaning of the similarity depends on the similarity measurement that is used. Labeling experiment consists of instances of labels. Often this is a training phase using1 Because higher values express less similarity, this is sometimes referred to as distance. However, the term distance entails certain mathematical properties that we do not require a similarity measurement."}, {"heading": "B. Corpus and labeling task", "text": "The goal is to create an entity that is looking for a solution to the problem."}, {"heading": "C. Similarity measures", "text": "An important question is how to measure the similarity between the corpora. Values in the vector indicate whether or not a particular character appears in the example text. In this way, an entire corpus can be reduced to a single vector, namely the centroid of all instance-based vectors in this corpus. If we have one vector for the test corpus and one for the training corpus, for example, the cosmic similarity between the two domain vectors can be calculated."}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Baseline systems", "text": "For our experiments, four different baselines are calculated: two baselines with one class and two straightforward baselines for learners. All result tables include the accuracy of the self-training gain forecast, the macro-averaged F score for performance prediction and the accuracy of performance prediction. We include the accuracy because it gives a general insight into the correct predictions. As the majority class has more influence on the accuracy of the self-training gain forecast and the accuracy ignores the accuracy, we also include the macro-averaged F score. This score gives the best sense of how well a system works. In a practical situation, a systems developer may be most interested in the precision of the self-training gain forecast. In fact, if a self-training setup leads to a performance gain, the developer wants this prediction to be4When qk = 0, the smoothing of the basic values of qk = 2 \u2212 2.5stytrust."}, {"heading": "B. Self-training gain prediction", "text": "The fact is that one sees oneself in a position to trump oneself, in the way in which one is able to trump oneself, in the way in which one is able to trump oneself, in the way in which one is able to trump oneself, in the way in which one seeks to trump oneself, in the way in which one sees oneself in a position to trump oneself, in the way in which one is able to trump oneself, in the way in which one is able to trump oneself."}, {"heading": "V. DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Limitations of similarity scores", "text": "Although this is a limitation on the practicality of the technique, predicting self-training gains seems to work best when information about a collection of self-training experiments is available. It may be useful to summarize some limitations that should be taken into account when applying similarity measures.c) Class marking independence: comparative yardsticks do not use class markers. Consider two corpora that are completely disjointed in terms of class markers but are very similar in the attribute space.A comparative yardstick is likely to underestimate the difference between the two corpora and overestimate labeling performance. Fortunately, there are labeling tasks (partial language marking, mood prediction,...) that this extreme situation is unlikely to occur, but it nonetheless means that the linearity between similarity and performance should be underestimated for each new task."}, {"heading": "B. Different experimental setups", "text": "It is about the question of whether and how such a development can occur, and about the question of whether and how such a development can occur. (...) It is about the question of whether and how such a development can occur. (...) It is about the question of whether and how such a development can occur. (...) It is about the question of whether and how such a development can occur. (...) It is about the question of whether and how such a development can occur. (...) It is about the question of whether and how such a development can occur. (...) It is about the question of whether and how such a development can occur. (...) It is about the question of whether and how such a development can occur. (...) It is about the question of whether and how such a development can occur. (...)"}, {"heading": "VI. CONCLUSION", "text": "In this work, we showed that self-training is a performance-enhancing technique for a strict selection of setups. Consider a system developer who wants to implement an online domain identification tool. He has a labeled corpus and a set of unlabeled corpus available. He does not know what techniques a user will submit and, as a result, he does not know which unlabeled domain corpus he should add to his labeled corpus domain. It is also very likely that the submitted corpus will be an invisible corpus. For the binary domain sentiment classification data, we have shown that he could evaluate the similarity between his training corpus, his non-labeled company and the invisible test corpus, and predict whether he should add the unlabeled data to his training corpus-domain-domain-domain-domain-classification data before he labels the test-corpus, we have shown that he could not evaluate the similarity between his corpus training corpus and his unlabeled corpus, his not share his training corpus."}, {"heading": "ACKNOWLEDGMENT", "text": "This research is funded by the Flanders Research Foundation (FWO project G.0478.10 - Statistical Relational Learning of Natural Language)."}], "references": [{"title": "Statistical parsing with a context-free grammar and word statistics,", "author": ["E. Charniak"], "venue": "Proceedings of the Fourteenth National Conference on Artificial Intelligence and Ninth Innovative Applications of Artificial Intelligence Conference", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Self-training without reranking for parser domain adaptation and its impact on semantic role labeling,", "author": ["K. Sagae"], "venue": "Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing. Uppsala,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Domain adaptation for statistical classifiers,", "author": ["H. Daum\u00e9 III", "D. Marcu"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Genres, registers, text types, domain, and styles: Clarifying the concepts and navigating a path through the BNC jungle,", "author": ["D.Y.W. Lee"], "venue": "Language Learning & Technology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Variation across speech and writing", "author": ["D. Biber"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "Preliminary recommendations on text typology,", "author": ["J. Sinclair", "J. Ball"], "venue": "Consiglio Nazionale delle Ricerche, Istituto di Linguistica Computazionale, Pisa, Italy, Expert Advisory Group on Language Engineering Standards (EAGLES) EAG\u2014TCWG\u2014TTYP/P,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Instance weighting for domain adaptation in NLP,", "author": ["J. Jiang", "C. Zhai"], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Any domain parsing: Automatic domain adaptation for natural language parsing,", "author": ["D. McClosky"], "venue": "Ph.D. dissertation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Ensemble-style self-training on citation classification,", "author": ["C. Dong", "U. Sch\u00e4fer"], "venue": "Proceedings of 5th International Joint Conference on Natural Language Processing. Chiang Mai, Thailand: Asian Federation of Natural Language Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Reserved self-training: A semisupervised sentiment classification method for chinese microblogs,", "author": ["Z. Liu", "X. Dong", "Y. Guan", "J. Yang"], "venue": "Proceedings of the Sixth International Joint Conference on Natural Language Processing. Nagoya, Japan: Asian Federation of Natural Language Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "The textual characteristics of traditional and open access scientific journals are similar,", "author": ["K. Verspoor", "K.B. Cohen", "L. Hunter"], "venue": "BMC Bioinformatics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Challenging stereotypes about academic writing: Complexity, elaboration, explicitness,", "author": ["D. Biber", "B. Gray"], "venue": "Journal of English for Academic Purposes,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Inducing features of random fields,", "author": ["S. Della Pietra", "V. Della Pietra", "J. Lafferty"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Unsupervised feature selection using feature similarity,", "author": ["P. Mitra", "C. Murthy", "S.K. Pal"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "On the effectiveness of the skew divergence for statistical language analysis,", "author": ["L. Lee"], "venue": "in 8th International Workshop on Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Toward a unified approach to statistical language modeling for chinese,", "author": ["J. Gao", "J. Goodman", "M. Li", "K.-F. Lee"], "venue": "Transactions on Asian Language Information Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Extracting discriminative concepts for domain adaptation in text mining,", "author": ["B. Chen", "W. Lam", "I. Tsang", "T.-L. Wong"], "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, ser. KDD \u201909", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Intelligent selection of language model training data,", "author": ["R.C. Moore", "W. Lewis"], "venue": "Proceedings of the ACL 2010 Conference Short Papers. Uppsala, Sweden: Association for Computational Linguistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Domain adaptation for parsing,", "author": ["B. Plank"], "venue": "Ph.D. dissertation, University of Groningen, the Netherlands,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Multiple source adaptation and the R\u00e9nyi divergence,", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "On measures of information and entropy,", "author": ["A. R\u00e9nyi"], "venue": "Proceedings of the 4th Berkeley Symposium on Mathematics, Statistics and Probability,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1961}, {"title": "Correlating natural language parser performance with statistical measures of the text,", "author": ["Y. Zhang", "R. Wang"], "venue": "Proceedings of the 32nd annual German conference on Advances in artificial intelligence. Paderborn, Germany: Springer-Verlag,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Using domain similarity for performance estimation,", "author": ["V. Van Asch", "W. Daelemans"], "venue": "Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing. Uppsala,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Soricut, \u201cAutomatic prediction of parser accuracy,", "author": ["S. Ravi", "K. Knight"], "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Honolulu, Hawaii: Association for Computational Linguistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "On information and sufficiency,", "author": ["S. Kullback", "R.A. Leibler"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1951}, {"title": "Measures of distributional similarity,", "author": ["L. Lee"], "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics. Maryland, USA: Association for Computational Linguistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "An empirical study on language model adaptation using a metric of domain similarity,", "author": ["W. Yuan", "J. Gao", "H. Suzuki"], "venue": "Natural Language Processing IJCNLP 2005, ser. Lecture Notes in Computer Science,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "SEM 2013 shared task: Semantic textual similarity,\u201d in Second Joint Conference on Lexical and Computational Semantics, Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity", "author": ["E. Agirre", "D. Cer", "M. Diab", "A. Gonzalez-Agirre", "W. Guo"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "DKPro similarity: An open source framework for text similarity,\u201d in Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations", "author": ["D. B\u00e4r", "T. Zesch", "I. Gurevych"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Biographies or blenders: Which resource is best for cross-domain sentiment analysis?", "author": ["N. Ponomareva", "M. Thelwall"], "venue": "Computational Linguistics and Intelligent Text Processing, ser. Lecture Notes in Computer Science,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Domain adaptation using domain similarity- and domain complexity-based instance selection for cross-domain sentiment analysis,", "author": ["R. Remus"], "venue": "Proceedings of the IEEE 12th International Conference on Data Mining Workshops. Brussels,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Biographies, Bollywood, Boomboxes and Blenders: Domain adaptation for sentiment classification,", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "Proceedings of the 45th Annual Meeting of the Association of  9 Computational Linguistics. Prague, Czech Republic: Association for Computational Linguistics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Making large-scale support vector machine learning practical,", "author": ["T. Joachims"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1999}, {"title": "Computer-intensive methods for testing hypotheses", "author": ["E.W. Noreen"], "venue": "New York, NY, USA: John Wiley,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1989}, {"title": "More accurate tests for the statistical significance of result differences,", "author": ["A. Yeh"], "venue": "Proceedings of the 18th International Conference on Computational Linguistics,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2000}, {"title": "Divergence measures based on the Shannon entropy,", "author": ["J. Lin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1991}, {"title": "Dutch dependency parser performance across domains,", "author": ["B. Plank", "G. van Noord"], "venue": "CLIN meeting,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Three corpora are needed for self-training: a labeled training corpus, a labeled test corpus, and an unlabeled additional corpus [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "Reference [2] argues that self-training is only beneficial in those situations for which the training and test data are sufficiently dissimilar, but other factors \u2013 most obviously labeling accuracy of the unlabeled data \u2013 may have an influence too.", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "For an introduction to domain adaptation see [3].", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "Reference [4], drawing upon [5] and the EAGLES initiative [6], mentions two types of parameters to categorize corpora: external (intended audience, purpose, and setting) and internal (lexical or grammatical (co)occurrence features).", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "Reference [4], drawing upon [5] and the EAGLES initiative [6], mentions two types of parameters to categorize corpora: external (intended audience, purpose, and setting) and internal (lexical or grammatical (co)occurrence features).", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "Reference [4], drawing upon [5] and the EAGLES initiative [6], mentions two types of parameters to categorize corpora: external (intended audience, purpose, and setting) and internal (lexical or grammatical (co)occurrence features).", "startOffset": 58, "endOffset": 61}, {"referenceID": 0, "context": "The concepts of self-training were first introduced by [1], but more recent examples are given by [7], [8], and [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "The concepts of self-training were first introduced by [1], but more recent examples are given by [7], [8], and [2].", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "The concepts of self-training were first introduced by [1], but more recent examples are given by [7], [8], and [2].", "startOffset": 103, "endOffset": 106}, {"referenceID": 1, "context": "The concepts of self-training were first introduced by [1], but more recent examples are given by [7], [8], and [2].", "startOffset": 112, "endOffset": 115}, {"referenceID": 8, "context": "Reference [9] combines the self-training concept with ensemble learning for citation classification.", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": "Another interesting approach is the combination of self-training with active learning [10].", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "It is possible to use similarities between corpora as such, using their outcome to draw inferences about them [11], [12], but an additional interesting usage involves applying similarities in a machine learning setup.", "startOffset": 110, "endOffset": 114}, {"referenceID": 11, "context": "It is possible to use similarities between corpora as such, using their outcome to draw inferences about them [11], [12], but an additional interesting usage involves applying similarities in a machine learning setup.", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "Similarities are used in natural language processing in various situations ranging from feature selection [13], [14] and measuring the similarity between two language models [15], [16] to training corpus creation [8], [17]\u2013[19].", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "Similarities are used in natural language processing in various situations ranging from feature selection [13], [14] and measuring the similarity between two language models [15], [16] to training corpus creation [8], [17]\u2013[19].", "startOffset": 112, "endOffset": 116}, {"referenceID": 14, "context": "Similarities are used in natural language processing in various situations ranging from feature selection [13], [14] and measuring the similarity between two language models [15], [16] to training corpus creation [8], [17]\u2013[19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 15, "context": "Similarities are used in natural language processing in various situations ranging from feature selection [13], [14] and measuring the similarity between two language models [15], [16] to training corpus creation [8], [17]\u2013[19].", "startOffset": 180, "endOffset": 184}, {"referenceID": 7, "context": "Similarities are used in natural language processing in various situations ranging from feature selection [13], [14] and measuring the similarity between two language models [15], [16] to training corpus creation [8], [17]\u2013[19].", "startOffset": 213, "endOffset": 216}, {"referenceID": 16, "context": "Similarities are used in natural language processing in various situations ranging from feature selection [13], [14] and measuring the similarity between two language models [15], [16] to training corpus creation [8], [17]\u2013[19].", "startOffset": 218, "endOffset": 222}, {"referenceID": 18, "context": "Similarities are used in natural language processing in various situations ranging from feature selection [13], [14] and measuring the similarity between two language models [15], [16] to training corpus creation [8], [17]\u2013[19].", "startOffset": 223, "endOffset": 227}, {"referenceID": 19, "context": "An example of training corpus creation is presented by [20].", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "They employ the R\u00e9nyi divergence [21] to create a combination of different training corpora in order to increase the labeling performance for a specific test corpus.", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "Similarities have also been used to predict the performance of machine learners [22], [23].", "startOffset": 80, "endOffset": 84}, {"referenceID": 22, "context": "Similarities have also been used to predict the performance of machine learners [22], [23].", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "A good example of such an application is the prediction of parsing accuracy [24].", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "Apart from the R\u00e9nyi divergence, some of the similarities that are used are perplexity, Kullback-Leibler divergence [25], and the Skew divergence [26].", "startOffset": 116, "endOffset": 120}, {"referenceID": 25, "context": "Apart from the R\u00e9nyi divergence, some of the similarities that are used are perplexity, Kullback-Leibler divergence [25], and the Skew divergence [26].", "startOffset": 146, "endOffset": 150}, {"referenceID": 18, "context": "Despite the fact that authors have shown that a similarity [19], [23] or a linear combination of similarities [8] can be successfully used to link the similarity between domains to the performance of a natural language processing system, no consensus exists about which similarity or combination of similarities is best suited for the task.", "startOffset": 59, "endOffset": 63}, {"referenceID": 22, "context": "Despite the fact that authors have shown that a similarity [19], [23] or a linear combination of similarities [8] can be successfully used to link the similarity between domains to the performance of a natural language processing system, no consensus exists about which similarity or combination of similarities is best suited for the task.", "startOffset": 65, "endOffset": 69}, {"referenceID": 7, "context": "Despite the fact that authors have shown that a similarity [19], [23] or a linear combination of similarities [8] can be successfully used to link the similarity between domains to the performance of a natural language processing system, no consensus exists about which similarity or combination of similarities is best suited for the task.", "startOffset": 110, "endOffset": 113}, {"referenceID": 26, "context": "For the research of [27], measuring the cross-entropy between two domains offered the best results when adapting a baseline language model to a new domain.", "startOffset": 20, "endOffset": 24}, {"referenceID": 27, "context": "Current research focuses on semantic textual similarity (STS) [28].", "startOffset": 62, "endOffset": 66}, {"referenceID": 28, "context": "An interesting software package in this context is the DKPro Similarity package [29], which implements various semantic similarities in addition to less complex string matching similarities.", "startOffset": 80, "endOffset": 84}, {"referenceID": 29, "context": "Similarity measures have been used on a range of corpora and currently we know of two papers that carry out domain similarity research on the same corpus that we use [30], [31].", "startOffset": 166, "endOffset": 170}, {"referenceID": 30, "context": "Similarity measures have been used on a range of corpora and currently we know of two papers that carry out domain similarity research on the same corpus that we use [30], [31].", "startOffset": 172, "endOffset": 176}, {"referenceID": 29, "context": "Reference [30] shows that instead of exploiting the correlation between the test/training similarity and the accuracy, one could also use the correlation between the similarity and the accuracy drop.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "Reference [30] also introduces the notion of domain complexity, which may be expressed by the percentage of rare words in a domain.", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "Examining the corpus of [32], they observe that less complex source domains tend to give a smaller accuracy drop on more complex target domains.", "startOffset": 24, "endOffset": 28}, {"referenceID": 30, "context": "Reference [31] investigates instance selection for the corpus", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "of [32].", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "The instances for this task are bag-of-word instances coming from the sentiment classification corpus of [32].", "startOffset": 105, "endOffset": 109}, {"referenceID": 32, "context": "02 [33].", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "Although the experiments are not directly comparable, the cross-domain accuracy is in the same region as reported by [32] meaning that the machine learner is not underperforming.", "startOffset": 117, "endOffset": 121}, {"referenceID": 31, "context": "The features of the corpus of [32] consist of tokens.", "startOffset": 30, "endOffset": 34}, {"referenceID": 33, "context": "3Using approximate randomization testing [34], [35].", "startOffset": 41, "endOffset": 45}, {"referenceID": 34, "context": "3Using approximate randomization testing [34], [35].", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "g the Kullback-Leibler divergence (KL; [25]):", "startOffset": 39, "endOffset": 43}, {"referenceID": 35, "context": "4 Apart from the Kullback-Leibler divergence, we also implemented the Jensen-Shannon divergence (JS; [36]):", "startOffset": 101, "endOffset": 105}, {"referenceID": 21, "context": "A fifth similarity measure that has been used is the simple Unknown Word Ratio (sUWR; [22], [37]).", "startOffset": 86, "endOffset": 90}, {"referenceID": 36, "context": "A fifth similarity measure that has been used is the simple Unknown Word Ratio (sUWR; [22], [37]).", "startOffset": 92, "endOffset": 96}, {"referenceID": 22, "context": "oped by [23].", "startOffset": 8, "endOffset": 12}], "year": 2016, "abstractText": "The goal of this paper is to investigate the connection between the performance gain that can be obtained by selftraining and the similarity between the corpora used in this approach. Self-training is a semi-supervised technique designed to increase the performance of machine learning algorithms by automatically classifying instances of a task and adding these as additional training material to the same classifier. In the context of language processing tasks, this training material is mostly an (annotated) corpus. Unfortunately self-training does not always lead to a performance increase and whether it will is largely unpredictable. We show that the similarity between corpora can be used to identify those setups for which self-training can be beneficial. We consider this research as a step in the process of developing a classifier that is able to adapt itself to each new test corpus that it is presented with.", "creator": "LaTeX with hyperref package"}}}