{"id": "1601.03317", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2016", "title": "Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model", "abstract": "Neural machine translation has shown very promising results lately. Most NMT models follow the encoder-decoder frame- work. To make encoder-decoder models more flexible, attention mechanism was introduced to machine translation and also other tasks like speech recognition and image captioning. We observe that the quality of translation by attention-based encoder-decoder can be significantly dam- aged when the alignment is incorrect. We attribute these problems to the lack of distortion and fertility models. Aiming to resolve these problems, we propose new variations of attention-based encoder- decoder and compare them with other models on machine translation. Our pro- posed method achieved an improvement of 2 BLEU points over the original attention- based encoder-decoder.", "histories": [["v1", "Wed, 13 Jan 2016 17:14:01 GMT  (1928kb,D)", "https://arxiv.org/abs/1601.03317v1", "11 pages"], ["v2", "Mon, 18 Jan 2016 04:58:59 GMT  (1932kb,D)", "http://arxiv.org/abs/1601.03317v2", "11 pages, updated details"], ["v3", "Fri, 22 Jan 2016 02:08:02 GMT  (1933kb,D)", "http://arxiv.org/abs/1601.03317v3", "11 pages, updated details"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shi feng", "shujie liu", "mu li", "ming zhou"], "accepted": false, "id": "1601.03317"}, "pdf": {"name": "1601.03317.pdf", "metadata": {"source": "CRF", "title": "Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model", "authors": ["Shi Feng", "Shanghai Jiao Tong", "Shujie Liu", "Mu Li", "Ming Zhou"], "emails": ["sjtufs@gmail.com", "shujliu@microsoft.com", "muli@microsoft.com", "mingzhou@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Most NMT methods follow the encoder decoder framework proposed by (Cho et al., 2014), which usually consists of two RNNs: The RNN encoder reads the source set and transforms it into vector representation; the RNN decoder takes the vector representation and generates the target sentence word for word; the decoder stops as soon as a special symbol for the end of the sentence is generated; this encoder decoder frame can be applied to general sequence-to-sequence tasks (Sutskever et al., the work he did as an internist at Microsoft Research.), such as answering and summarizing the text. For example, the encoder encoder with a CNN encoder is applied to the."}, {"heading": "2 Problems of Attention Mechanism", "text": "By training the attention mechanism encoder decoder model, we get an alignment from the target word to the source word. This alignment facilitates translation by allowing for reordering. However, since alignment for attention is not always accurate, we found that in many cases where alignment is wrong, translation quality is significantly impaired. We attribute this type of problem to the lack of explicit distortion and fertility models in the attention-based encoder decoder model."}, {"heading": "2.1 Lack of Distortion Model", "text": "In Fig. 2, we show an example of an alignment given by an attention mechanism where a misalignment in the middle of the sentence caused the translation to go wrong in retrospect. We focus on the later part of the sentence, where the correct translation should be:... \"and warn that the election to be held on 30 January next year would not be\" violent violence in Iraq \"and would end,\" which is translated into:... \"and warn that next year it would not be the end of the severe violence in Iraq.\" The alignment matrix indicates that the word \"\u20ac\" (meaning \"planned\") in the initial sentence is \"would,\" but the next attention has jumped to... \"(meaning\" choice \") while it should focus on\" next year \"or on the date. After this erroneous rearrangement, the meaning of the original sentence in the translation is distorted."}, {"heading": "2.2 Lack of Fertility Model", "text": "In SMT, the fertility model controls how many target words are translated by a source word. We observe two phenomena related to the lack of fertility model in the attention-based encoder decoder: the problem of repetition and the problem of obfuscation. Problem of repetition Fig. 3 shows an example of repetition problems in alignment. For consecutive words, the attention mechanism focused on the same position in the source set, which led to a repetition in the translation: \"the organization of Europe and the organization of cooperation in Europe.\" Problem of Fig. 4 shows an example of a cover problem in alignment. We see that part of the source set was ignored, resulting in a significant loss of content in the translation. These two problems are attributable to the lack of fertility model in NMT: In the first case, some source words are translated into too many targets, while in the second case some source words are presented as distorting."}, {"heading": "3 Attention-based Encoder-Decoder", "text": "We start by examining the RNN used in NMT papers and the encoder decoder with attention mechanisms (Bahdanau et al., 2014)."}, {"heading": "3.1 Gated Recurrent Unit", "text": "Gated Recurrent Unit (GRU) (Cho et al., 2014) is an RNN alternative similar to LSTM (Hochreiter and Schmidhuber, 1997), used in NMT papers (Cho et al., 2014; Bahdanau et al., 2014), and we will use GRU as RNN in our work. Like normal RNN, GRU calculates its hidden state hi based on the input xi and the former hidden state hi \u2212 1: hi = RNN (hi \u2212 1, xi) calculated with update gate and reset gate, formally defined by: ri = \u03c3 (W rxi + U rhi \u2212 1) h \u2032 i = tanh (ri \u0445 Uhi \u2212 1 + Wxi) zi = \u03c3 (W zxi + U zhi \u2212 1) hi = (1 \u2212 zi)."}, {"heading": "3.2 RNNSearch (Bahdanau et al., 2014)", "text": "The encoder used in RNNSEARCH (Bahdanau et al., 2014) is a bidirectional RNN. It consists of two independent RNN, one of which reads the source sentence from left to right, another from right to left: \u2212 \u2192 s i = RNN (\u2212 \u2192 s i \u2212 1, xi) \u2190 \u2212 s i = RNN (\u2190 \u2212 s i + 1, xi), where xi is the word embedding the source word at position i.The representation at position i is then defined as concatenation of \u2212 s i and \u2190 \u2212 s i: si = [\u2212 s i \u2212 s i] Decoder with Attention Unlike the decoder of (Cho et al., 2014), which occupies only the last representation, the decoder with attention mechanism can fully exploit the entire representation."}, {"heading": "4 Recurrent Attention Mechanisms", "text": "In Figure 5, we show an abstraction of the decoder attention structure. We note that the attention mechanism treats the states of the encoder as a set, not a sequence, while the source sentence sequence is decisive for the reorder. And, the state of the reorder given to the attention unit is all embedded in the hidden state of the decoder - the attention unit itself has no memory. Motivated by the analysis in paragraph 2.1, we propose to add recurring paths to the decoder attention structure to give the attention unit more information about the reorder. Instead of reminding the decoder what the state of the reorder is, the recurring attention mechanism explicitly keeps this information in mind."}, {"heading": "4.1 RecAtt", "text": "The decoder with RecAtt follows: ci = ATT (hi \u2212 1, ci \u2212 1 {sj}) hi = RNN (hi \u2212 1, yi \u2212 1, ci) Where the modified attention mechanism RecAtt follows: eij = v T tanh\u03b1 (hi \u2212 1, ci \u2212 1, sj) wij = exp (eij) \u2211 k exp (eik) ci = \u2211 j wijsjThe modified sum matching function is: \u03b1sum (a, b, c) = W \u03b1a + U\u03b1b + V \u03b1cWe find that RecAtt is purely content-related - the recurring attention information is the context vector and not the weights. We show in our experiments that it is very important to make the attention unit itself recurring in order to improve the translation performance from end to end."}, {"heading": "4.2 RNNAtt", "text": "RecAtt was developed with the aim of adding a distortion model. In RecAtt, only the previous attentively generated context is used in the recurring path, so that it only has a \"short-term memory.\" To make it more flexible and to have a longer memory, we propose RNNATT, as in Fig. 7. The attention unit now retains a hidden state and effectively becomes a complete RNN.ci = ATT OUT (qi \u2212 1, {sj}) qi = ATT RNN (qi \u2212 1, hi \u2212 1, ci) hi = RNN (yi \u2212 1, hi \u2212 1, ct), where ATT OUT is the original attention unit that applies the match function and Softmax, ATT RNN stands for the hidden state of the Qi calculation of the attention unit."}, {"heading": "5 Conditioned Decoder", "text": "To solve this problem, we propose the conditional decoder CONDDEC, which uses a condition vector to display which information was extracted from the source set. This can be regarded as an implicit fertility model, the condition vector can track how many target words are translated from each source word. We use a structure similar (Wen et al., 2015), in which a predefined condition is used to direct the generation of natural language. In contrast to this method, we use a traable condition initialized with the last encoder. \u2212 At each decoding step, the condition is updated with the decoding state and used to calculate the next decoder state. The decoder GRU with attention and the condition sdi is defined as an addition of an additional decay model vdi Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z decoder decoder"}, {"heading": "6 Related work", "text": "There are variations of attention mechanisms that repeat themselves similar to RECATT. In this section, these models are reviewed and the decoder attention structures are compared."}, {"heading": "6.1 InputFeed (Luong et al., 2015)", "text": "In (Luong et al., 2015), the authors examined several variations of the attention mechanism, including various match functions and local attention. We focus on the input-feeding method proposed in this paper because it is relapsing. INPUTFEED passes the previous attention output to the decoder along with the current attention output to further inform the decoder of previous alignment decisions.ci = ATT (hi \u2212 1, {si}) ht = RNN (hi \u2212 1, yi \u2212 1, ci, ci \u2212 1) This attention mechanism is purely content-based - the recurring information is the context given by the attention mechanism, not weights. Note that the recurring information is passed directly to the decoder outside the attention function, which distinguishes it from RecAtt, where the recurring information is passed to the attention unit."}, {"heading": "6.2 HybridAtt1(Chorowski et al., 2014)", "text": "In (Chorowski et al., 2014) the authors proposed an attention mechanism with a recurring path. In calculating the current weight set on encoder states, the attention unit takes the previous weights and punishes the jumping distance. It calculates the average attention center, the mi \u2212 1 = \u2211 j \u0445 w (i \u2212 1), j. Then it adjusts the weight of each encoder state based on its distance from this centre.mi \u2212 1 = \u2211 j \u00b7 wi \u2212 1, jeij = v T tanh\u03b1 (hi \u2212 1, sj) e \u2032 ij = Logistic (j \u2212 mi \u2212 1) \u00b7 exp (eij) wij = e \u2032 ij \u0445 k e \u2032 ikci = \u0445 j wijsjThis is a content-based attention with location-based recurring attention characterized by the use of an average attention center. Note that the recurring information outside the attention unit is used to adjust the weights of what distinguishes it from the receptor."}, {"heading": "6.3 HybridAtt2(Bahdanau et al., 2015)", "text": "In an paper (Bahdanau et al., 2015), the authors followed the previous and improved HYBRIDATT1 by integrating the recurring location information into the attention function. First, they extracted characteristic vectors gi by performing folds with the previous weights Q * wi \u2212 1, and then they used these characteristic vectors to predict new weights.gi = Q * wi \u2212 1 eij = v T tanh\u03b1 (hi \u2212 1, sj, gij) wij = exp (eij) \u2211 k exp (eik) ci = \u2211 j wijsjwhere \u0445 stands for folding. This is also a content-based attention mechanism with location-based, recurring attention. The difference between this method and HYBRIDATT1 is that the recurring information is integrated into the attention function. We note that HYBRIDATT1 and HYDATT2 are not used to improve the attention function on the new language recognition task, but rather on the overcognition task proposed in the MYBRIDATT2."}, {"heading": "7 Experimental Setup", "text": "In this section, we describe the data used in our experiments, our evaluation methods and our validation process. Datasets For training, we use NIST ChineseEnglish training set exclusive the Hong Kong Law and Hong Kong Hansard (0.5 m sentence pairs after exclusion). For validation, we use Nist2005 Dataset (1082 sentence pairs). For validation, we use Nist2003 Dataset (913 sentence pairs). Validation set is used only for early stopping and the training process Monitoring.Following (Bahdanau et al., 2014), we use source and target dictionaries of size 30,000, covering 97.4% and 98.9% of vocabularies. Out-of-of-vocabularies are provided with a special token < UNK >.Post-Processing We conduct post-processing We are based on the alignment SMSM."}, {"heading": "8 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Quantitative", "text": "RECATT performed best among NMT models, with and without post-processing. RECATT achieved an improvement in the BLEU score of 2.1 over the original RNNSEARCH. Note that RECATT also achieved the most improvements from post-processing, namely 5.04 BLEU points. In post-processing, we use a naive translation table that is generated exclusively from training data, so the effect of post-processing largely depends on the quality of the alignment. Thus, the gain from post-processing can be considered a measure of the quality of the attention-generated alignment, and as a result, RECATT improves the attention mechanism. CONDDEC has surpassed RNNSEARCH by 1 BLEU point, both with and without post-processing. All three of our proposed models have surpassed the phrase-based SMT baseline. The combination of CONDDEC with RECATT and RNNATT is in progress."}, {"heading": "8.2 Qualitative", "text": "This year, we will be able to start looking for a new partner who will be able to help us."}, {"heading": "9 Conclusions", "text": "To solve these problems, we proposed to add implicit distortion and fertility models to the attention-based encoder decoder. We proposed recurring attention mechanisms, RECATT and RNNATT for the distortion model and CONDDEC for the fertility model. We compared our models with other related variations. We evaluated our methods both quantitatively and qualitatively. In the Chinese-English translation, RECATT achieved an improvement of 2 BLEU points over the original attention mechanism and outperformed all related models. CONDDEC also outperformed the original attention mechanism by 1 BLEU point. By analyzing the alignment trix generated by the attention mechanism, we showed that our proposed methods contribute to solving the observed problems. We are working on combining CONDDEC with RECATT and RNNATT. We will also try alternative structures of recurring attention and to improve the end performance of NT."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1508.04395.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Audio chord recognition with recurrent neural networks", "author": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent."], "venue": "ISMIR, pages 335\u2013 340.", "citeRegEx": "Boulanger.Lewandowski et al\\.,? 2013", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2013}, {"title": "Abccnn: An attention based convolutional neural network for visual question answering", "author": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia."], "venue": "arXiv preprint arXiv:1511.05960.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "computational linguistics, 33(2):201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: First results", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.1602.", "citeRegEx": "Chorowski et al\\.,? 2014", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Maxout networks", "author": ["Ian J Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1302.4389.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1211.3711.", "citeRegEx": "Graves.,? 2012", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1684\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1508.04025.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Computational linguistics, 29(1):19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1211.5063.", "citeRegEx": "Pascanu et al\\.,? 2012", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Where to look: Focus regions for visual question answering", "author": ["Kevin J Shih", "Saurabh Singh", "Derek Hoiem."], "venue": "arXiv preprint arXiv:1511.07394.", "citeRegEx": "Shih et al\\.,? 2015", "shortCiteRegEx": "Shih et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "arXiv preprint arXiv:1411.4555.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "PeiHao Su", "David Vandyke", "Steve Young."], "venue": "arXiv preprint arXiv:1508.01745.", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko."], "venue": "arXiv preprint arXiv:1511.05234.", "citeRegEx": "Xu and Saenko.,? 2015", "shortCiteRegEx": "Xu and Saenko.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1502.03044.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Most NMT methods follow the encoder-decoder framework proposed by (Cho et al., 2014), which typically consists of two RNNs: the encoder RNN reads the source sentence and transform it into vector representation; the decoder RNN takes the vector representation and generates the target sentence word by word.", "startOffset": 66, "endOffset": 84}, {"referenceID": 19, "context": "After some modification, for example replacing the RNN encoder with a CNN, the model can also be applied to tasks like image captioning (Vinyals et al., 2014; Xu et al., 2015).", "startOffset": 136, "endOffset": 175}, {"referenceID": 22, "context": "After some modification, for example replacing the RNN encoder with a CNN, the model can also be applied to tasks like image captioning (Vinyals et al., 2014; Xu et al., 2015).", "startOffset": 136, "endOffset": 175}, {"referenceID": 0, "context": "To make it more flexible and generalize the fixed-length representation to a variablelength one, it was proposed to use attention mechanism for machine translation (Bahdanau et al., 2014).", "startOffset": 164, "endOffset": 187}, {"referenceID": 13, "context": ", between image objects and agent actions in the dynamic control problem (Mnih et al., 2014).", "startOffset": 73, "endOffset": 92}, {"referenceID": 0, "context": "In (Bahdanau et al., 2014), attention mechanism was applied to machine translation to learn an alignment between source words and target words.", "startOffset": 3, "endOffset": 26}, {"referenceID": 10, "context": "With the ability of learning alignments between different modalities from attention mechanism, attention-based encoder-decoder model is more powerful than just encoder-decoder and has been used for many tasks like question answering (Hermann et al., 2015), speech recognition (Bahdanau ar X iv :1 60 1.", "startOffset": 233, "endOffset": 255}, {"referenceID": 22, "context": ", 2014), image captioning (Xu et al., 2015) and visual question answering (Xu and Saenko, 2015; Chen et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 21, "context": ", 2015) and visual question answering (Xu and Saenko, 2015; Chen et al., 2015; Shih et al., 2015).", "startOffset": 38, "endOffset": 97}, {"referenceID": 3, "context": ", 2015) and visual question answering (Xu and Saenko, 2015; Chen et al., 2015; Shih et al., 2015).", "startOffset": 38, "endOffset": 97}, {"referenceID": 17, "context": ", 2015) and visual question answering (Xu and Saenko, 2015; Chen et al., 2015; Shih et al., 2015).", "startOffset": 38, "endOffset": 97}, {"referenceID": 0, "context": "We start by reviewing the RNN used in NMT papers and the encoder-decoder with attention mechanism from (Bahdanau et al., 2014).", "startOffset": 103, "endOffset": 126}, {"referenceID": 5, "context": "Gated Recurrent Unit (GRU) (Cho et al., 2014) is an RNN alternative similar to LSTM (Hochreiter and Schmidhuber, 1997).", "startOffset": 27, "endOffset": 45}, {"referenceID": 11, "context": ", 2014) is an RNN alternative similar to LSTM (Hochreiter and Schmidhuber, 1997).", "startOffset": 46, "endOffset": 80}, {"referenceID": 5, "context": "It was used in NMT papers (Cho et al., 2014; Bahdanau et al., 2014) and we will use GRU as RNN in our paper.", "startOffset": 26, "endOffset": 67}, {"referenceID": 0, "context": "It was used in NMT papers (Cho et al., 2014; Bahdanau et al., 2014) and we will use GRU as RNN in our paper.", "startOffset": 26, "endOffset": 67}, {"referenceID": 0, "context": "2 RNNSearch (Bahdanau et al., 2014) Encoder The encoder used in RNNSEARCH (Bahdanau et al.", "startOffset": 12, "endOffset": 35}, {"referenceID": 0, "context": ", 2014) Encoder The encoder used in RNNSEARCH (Bahdanau et al., 2014) is a bi-directional RNN.", "startOffset": 46, "endOffset": 69}, {"referenceID": 5, "context": "Decoder with Attention Unlike the decoder from (Cho et al., 2014) which takes only the last representation, the decoder with attention mechanism can make full use of the whole representation set sj .", "startOffset": 47, "endOffset": 65}, {"referenceID": 12, "context": "The match function can take on many forms, which is analyzed in (Luong et al., 2015).", "startOffset": 64, "endOffset": 84}, {"referenceID": 0, "context": "In our paper we use the sum match function, which is a more common choice as used in (Bahdanau et al., 2014).", "startOffset": 85, "endOffset": 108}, {"referenceID": 16, "context": "To predict a target word at position i, the decoder state hi is concatenated with ci and yi\u22121 and fed through deep out (Pascanu et al., 2012) with a single maxout hidden layer (Goodfellow et al.", "startOffset": 119, "endOffset": 141}, {"referenceID": 8, "context": ", 2012) with a single maxout hidden layer (Goodfellow et al., 2013), followed by a softmax.", "startOffset": 42, "endOffset": 67}, {"referenceID": 20, "context": "We use a structure similar to (Wen et al., 2015) where a predefined condition is used to guide natural language generation.", "startOffset": 30, "endOffset": 48}, {"referenceID": 12, "context": "1 InputFeed (Luong et al., 2015)", "startOffset": 12, "endOffset": 32}, {"referenceID": 12, "context": "In (Luong et al., 2015) the authors explored several variations of attention mechanism, including different match functions and local attention.", "startOffset": 3, "endOffset": 23}, {"referenceID": 6, "context": "2 HybridAtt1(Chorowski et al., 2014)", "startOffset": 12, "endOffset": 36}, {"referenceID": 6, "context": "In (Chorowski et al., 2014) the authors proposed an attention mechanism with a recurrent path.", "startOffset": 3, "endOffset": 27}, {"referenceID": 1, "context": "3 HybridAtt2(Bahdanau et al., 2015)", "startOffset": 12, "endOffset": 35}, {"referenceID": 1, "context": "In paper (Bahdanau et al., 2015) the authors followed the previous one and improved HYBRIDATT1 by integrating the recurrent location information into attention function.", "startOffset": 9, "endOffset": 32}, {"referenceID": 13, "context": "Other variations of attention mechanism with similar recurrent paths include (Mnih et al., 2014), (Chen et al.", "startOffset": 77, "endOffset": 96}, {"referenceID": 3, "context": ", 2014), (Chen et al., 2015).", "startOffset": 9, "endOffset": 28}, {"referenceID": 0, "context": "Following (Bahdanau et al., 2014), we use source and target dictionaries of size 30000, covering 97.", "startOffset": 10, "endOffset": 33}, {"referenceID": 14, "context": "We make a simple word-level translation table from the alignment result given by GIZA++ (Och and Ney, 2003) from the training set: for each source word, we choose the most frequently aligned target word.", "startOffset": 88, "endOffset": 107}, {"referenceID": 15, "context": "Evaluation Performance is evaluated by BLEU score (Papineni et al., 2002) over the test set.", "startOffset": 50, "endOffset": 73}, {"referenceID": 5, "context": "We compare 6 models, RNNSEARCH (Cho et al., 2014), HYBRIDATT2 (Bahdanau et al.", "startOffset": 31, "endOffset": 49}, {"referenceID": 1, "context": ", 2014), HYBRIDATT2 (Bahdanau et al., 2015), INPUTFEED (Luong et al.", "startOffset": 20, "endOffset": 43}, {"referenceID": 12, "context": ", 2015), INPUTFEED (Luong et al., 2015), and three proposed models, RECATT, RNNATT and CONDDEC.", "startOffset": 19, "endOffset": 39}, {"referenceID": 4, "context": "We benchmark the 6 NMT models with our implementation of hierarchical phrase-based SMT from (Chiang, 2007), with standard features, denoted as SMT.", "startOffset": 92, "endOffset": 106}, {"referenceID": 0, "context": "perl script from (Bahdanau et al., 2014).", "startOffset": 17, "endOffset": 40}, {"referenceID": 0, "context": "Following (Bahdanau et al., 2014), we use dropout rate 0.", "startOffset": 10, "endOffset": 33}, {"referenceID": 7, "context": "Each model is trained with AdaGrad (Duchi et al., 2011) on K40m GPU for approximately 4 days, finishing over 400000 updates, equivalent to 640 epochs.", "startOffset": 35, "endOffset": 55}, {"referenceID": 9, "context": "When testing trained models, we use beam search (Graves, 2012; Boulanger-Lewandowski et al., 2013; Sutskever et al., 2014) with beam size of 12.", "startOffset": 48, "endOffset": 122}, {"referenceID": 2, "context": "When testing trained models, we use beam search (Graves, 2012; Boulanger-Lewandowski et al., 2013; Sutskever et al., 2014) with beam size of 12.", "startOffset": 48, "endOffset": 122}, {"referenceID": 18, "context": "When testing trained models, we use beam search (Graves, 2012; Boulanger-Lewandowski et al., 2013; Sutskever et al., 2014) with beam size of 12.", "startOffset": 48, "endOffset": 122}], "year": 2016, "abstractText": "Neural machine translation has shown very promising results lately. Most NMT models follow the encoder-decoder framework. To make encoder-decoder models more flexible, attention mechanism was introduced to machine translation and also other tasks like speech recognition and image captioning. We observe that the quality of translation by attention-based encoder-decoder can be significantly damaged when the alignment is incorrect. We attribute these problems to the lack of distortion and fertility models. Aiming to resolve these problems, we propose new variations of attention-based encoderdecoder and compare them with other models on machine translation. Our proposed method achieved an improvement of 2 BLEU points over the original attentionbased encoder-decoder.", "creator": "TeX"}}}