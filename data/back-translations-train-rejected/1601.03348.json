{"id": "1601.03348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2016", "title": "EvoGrader: an online formative assessment tool for automatically evaluating written evolutionary explanations", "abstract": "EvoGrader is a free, online, on-demand formative assessment service designed for use in undergraduate biology classrooms. EvoGrader's web portal is powered by Amazon's Elastic Cloud and run with LightSIDE Lab's open-source machine-learning tools. The EvoGrader web portal allows biology instructors to upload a response file (.csv) containing unlimited numbers of evolutionary explanations written in response to 86 different ACORNS (Assessing COntextual Reasoning about Natural Selection) instrument items. The system automatically analyzes the responses and provides detailed information about the scientific and naive concepts contained within each student's response, as well as overall student (and sample) reasoning model types. Graphs and visual models provided by EvoGrader summarize class-level responses; downloadable files of raw scores (in .csv format) are also provided for more detailed analyses. Although the computational machinery that EvoGrader employs is complex, using the system is easy. Users only need to know how to use spreadsheets to organize student responses, upload files to the web, and use a web browser. A series of experiments using new samples of 2,200 written evolutionary explanations demonstrate that EvoGrader scores are comparable to those of trained human raters, although EvoGrader scoring takes 99% less time and is free. EvoGrader will be of interest to biology instructors teaching large classes who seek to emphasize scientific practices such as generating scientific explanations, and to teach crosscutting ideas such as evolution and natural selection. The software architecture of EvoGrader is described as it may serve as a template for developing machine-learning portals for other core concepts within biology and across other disciplines.", "histories": [["v1", "Wed, 13 Jan 2016 18:59:06 GMT  (1095kb)", "http://arxiv.org/abs/1601.03348v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kayhan moharreri", "minsu ha", "ross h nehm"], "accepted": false, "id": "1601.03348"}, "pdf": {"name": "1601.03348.pdf", "metadata": {"source": "META", "title": "EvoGrader: an online formative assessment tool for automatically evaluating written evolutionary explanations", "authors": ["Kayhan Moharreri", "Minsu Ha", "Ross H Nehm"], "emails": ["ross.nehm@stonybrook.edu"], "sections": [{"heading": null, "text": "EvoGrader's web portal is powered by Amazon's Elastic Cloud and runs with LightSIDE Lab's open source machine learning tools. EvoGrader's web portal allows biology teachers to upload a response file (.csv) that contains an unlimited number of evolutionary explanations written in response to 86 different ACORNS (Assessing COntextual Reasoning about Natural Selection) instruments. The system automatically analyzes the answers and provides detailed information about the scientific and naive concepts contained in each student's response, as well as general student (and random sample) argumentation models. Graphics and visual models provided by EvoGrader summarize class-level responses; downloadable raw data files (im.csv format) are also provided for more detailed analysis."}, {"heading": "Background", "text": "The move toward assessing complex tasks - such as explaining one's reasoning, constructing an argument, testing hypotheses, or defending an explanation - is becoming the new norm. For example, the 2012 Advanced Placement (AP) biology exam has doubled the number of open questions and drastically reduced the number of multiple choice questions (Duncan 2013). The new framework for K-12 Science * Correspondence: ross.nehm @ stonybrook.edu 3Center for Science and Mathematics Education, and Department of Ecology and Evolution, Stony Brook University, 11794 Stony Brook, USA Full list of author information is available at the end of the article."}, {"heading": "An online formative assessment tool", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "The Web portal", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "Using the EvoGrader system", "text": "Although it is obvious that the computing machinery used by EvoGrader is complex (see above), using the system is simple. Users just need to know how to: (1) use spreadsheets, (2) upload a CSV file to the web, and (3) use an online web application. Full step-by-step instructions are available in video format on the EvoGrader website, and written instructions are included in the supplementary file 1 to this essay. Overall, the system is designed to be usable by anyone with very basic computer skills."}, {"heading": "Research questions", "text": "This study goes beyond this previous work to analyze the effectiveness of new scoring models derived from a much larger and more diverse human scoring corpus and optimized for use in an online environment (i.e., in the EvoGrader online portal). In addition to evaluating how well EvoGrader evaluates the ACORNS scoring tools, we also examine how well EvoGrader evaluates the \"ACORNS-like\" items (i.e., not the 86 items developed for use in EvoGrader, but similar open-response evolution items). Since different evaluation approaches and contexts have been shown to influence students \"ideas (and the appropriate language used to express these ideas, see Nehm and Ha 2011), it is an open question whether EvoGrader might be able to effectively evaluate human responses to ACNS questions before they are used by other researchers."}, {"heading": "Methods", "text": "Human Scoring of Evolutionary Explanations: Concepts and Models In order to evaluate the results of the EvoGrader study, the results of the EvoGrader study were compared with those of humans. Two different categories of results were compared: First, the results of six normative scientific concepts (variation, inheritance, limited resources, differentiated survival / reproduction and non-adaptive ideas) and second, the results of three non-normative ideas (needs and goals) [teleology], use and discretion [usheritance], and adaptation / acceptance) (see Table 2 for detailed descriptions of these concepts). All of these ideas were commonly used in research on student thinking (e.g. Bishop and Anderson 1990; Nehm and Reilly 2007). In addition to studying concept patterns in the written answers, we examined the general thought patterns (i.e.), we used cognitive models."}, {"heading": "Results", "text": "iSe rf\u00fc die r die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc ("}, {"heading": "Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "The scoring efficacy of EvoGrader", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "EvoGrader in the classroom", "text": "This year, it is more than ever in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place."}, {"heading": "Conclusions", "text": "While it is clear that technological advances and the demand for richer and more informative types of tests have begun to change the field of education assessment in the United States (Duncan 2013), almost all evolutionary assessments remain in multiple-choice formats. Educators need to start adopting new tools and technologies and using them to develop more meaningful measures of evolutionary thinking and thinking (National Research Council 2012). EvoGrader is a small step in this important direction."}, {"heading": "Additional file", "text": "Additional Data 1: User Manual ACORNS: Assessing COntextual reasoning about natural selection; SMO: Sequential minimal optimization. Competting interests The authors declare that they have no competing interests. Authors \"contributions KM was responsible for computer system design, MH Harris was responsible for data analysis, and RHN was responsible for study design. MH and RHN were equally responsible for data collection. All authors were equally responsible for writing the manuscript. All authors read and approved the final manuscript. Acknowledgements We thank the National Science Foundation REESE grant 0909999 and NSF TUES grants 1338570 and 1322872 for funding this work. All opinions, results, and recommendations in this publication are those of the authors and do not necessarily reflect the views of the NSF."}], "references": [{"title": "A methodology for scoring open-ended architectural design problems", "author": ["II Bejar"], "venue": "Journal of Applied Psychology", "citeRegEx": "Bejar,? \\Q1991\\E", "shortCiteRegEx": "Bejar", "year": 1991}, {"title": "Student conceptions of natural selection and its role in evolution", "author": ["BA Bishop", "CW Anderson"], "venue": "Journal of Research in Science Teaching", "citeRegEx": "Bishop and Anderson,? \\Q1990\\E", "shortCiteRegEx": "Bishop and Anderson", "year": 1990}, {"title": "N-Gram-Based Text Categorization", "author": ["WB Cavnar", "JM Trenkle"], "venue": "Proceeding of the Symposium on Document Analysis and Information Retrieval", "citeRegEx": "Cavnar and Trenkle,? \\Q1994\\E", "shortCiteRegEx": "Cavnar and Trenkle", "year": 1994}, {"title": "Students\u2019 conceptions of natural selection and its role in evolution: Cases of replication and comparison", "author": ["SS Demastes", "J Settlage", "R Good"], "venue": "Journal of Research in Science Teaching", "citeRegEx": "Demastes et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Demastes et al\\.", "year": 1995}, {"title": "Why we need high-speed schools", "author": ["A Duncan"], "venue": "Scientific American", "citeRegEx": "Duncan,? \\Q2013\\E", "shortCiteRegEx": "Duncan", "year": 2013}, {"title": "Evolution: Education and Outreach", "author": ["Moharreri"], "venue": null, "citeRegEx": "Moharreri,? \\Q2014\\E", "shortCiteRegEx": "Moharreri", "year": 2014}, {"title": "Measuring nominal scale agreement among many raters", "author": ["J Fleiss"], "venue": "Psychological Bulletin", "citeRegEx": "Fleiss,? \\Q1971\\E", "shortCiteRegEx": "Fleiss", "year": 1971}, {"title": "Using Machine-Learning Methods to Detect key Concepts and Misconceptions of Evolution in students\u2019 Written Explanations. Paper to be Presented at the National Association for Research in Science Teaching", "author": ["M Ha", "RH Nehm"], "venue": null, "citeRegEx": "Ha and Nehm,? \\Q2012\\E", "shortCiteRegEx": "Ha and Nehm", "year": 2012}, {"title": "Applying computerized scoring models of written biological explanations across courses and colleges: Prospects and limitations", "author": ["M Ha", "RH Nehm", "M Urban-Lurain", "JE Merrill"], "venue": "CBE Life Sciences Education", "citeRegEx": "Ha et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ha et al\\.", "year": 2011}, {"title": "Optimizing Machine-Learning Models for Automated Computer Scoring of Natural Selection Concepts", "author": ["M Ha", "S Dennis", "RH Nehm"], "venue": "Paper in proceedings of the National Association for Research in Science", "citeRegEx": "Ha et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ha et al\\.", "year": 2013}, {"title": "What are they thinking? Automated analysis of student writing about acid\u2013base chemistry in introductory biology", "author": ["KC Haudek", "LB Prevost", "RA Moscarella", "J Merrill", "M Urban-Lurain"], "venue": "CBE Life Sciences Education", "citeRegEx": "Haudek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Haudek et al\\.", "year": 2012}, {"title": "The measurement of observer agreement for categorical data", "author": ["JR Landis", "GG Koch"], "venue": "Biometrics", "citeRegEx": "Landis and Koch,? \\Q1977\\E", "shortCiteRegEx": "Landis and Koch", "year": 1977}, {"title": "LightSIDE: Researcher\u2019s User Manual", "author": ["E Mayfield", "D Adamson", "C Ros\u00e9"], "venue": null, "citeRegEx": "Mayfield et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mayfield et al\\.", "year": 2013}, {"title": "Identifying and handling mislabelled instances", "author": ["F Muhlenbach", "S Lallich", "DA Zighed"], "venue": "Journal of Intelligent Information Systems", "citeRegEx": "Muhlenbach et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Muhlenbach et al\\.", "year": 2004}, {"title": "Item feature effects in evolution assessment", "author": ["RH Nehm", "M Ha"], "venue": "Journal of Research in Science Teaching", "citeRegEx": "Nehm and Ha,? \\Q2011\\E", "shortCiteRegEx": "Nehm and Ha", "year": 2011}, {"title": "Human vs. computer diagnosis of students\u2019 natural selection knowledge: testing the efficacy of text analytic software", "author": ["RH Nehm", "H Haertig"], "venue": "Journal of Science Education and Technology", "citeRegEx": "Nehm and Haertig,? \\Q2012\\E", "shortCiteRegEx": "Nehm and Haertig", "year": 2012}, {"title": "Biology majors\u2019 knowledge and misconceptions of natural selection", "author": ["RH Nehm", "L Reilly"], "venue": "BioScience", "citeRegEx": "Nehm and Reilly,? \\Q2007\\E", "shortCiteRegEx": "Nehm and Reilly", "year": 2007}, {"title": "Measuring knowledge of natural selection: a comparison of the CINS, an open\u2010response instrument, and an oral interview", "author": ["RH Nehm", "IS Schonfeld"], "venue": "Journal of Research in Science Teaching", "citeRegEx": "Nehm and Schonfeld,? \\Q2008\\E", "shortCiteRegEx": "Nehm and Schonfeld", "year": 2008}, {"title": "The future of natural selection knowledge measurement: a reply to Anderson et al", "author": ["RH Nehm", "IS Schonfeld"], "venue": "Journal of Research in Science Teaching", "citeRegEx": "Nehm and Schonfeld,? \\Q2010\\E", "shortCiteRegEx": "Nehm and Schonfeld", "year": 2010}, {"title": "Academic preparation in biology and advocacy for teaching evolution: biology versus non\u2010biology teachers", "author": ["RH Nehm", "SY Kim", "K Sheppard"], "venue": "Science Education", "citeRegEx": "Nehm et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nehm et al\\.", "year": 2009}, {"title": "Scoring Guide for the Open Response Instrument (ORI) and Evolutionary Gain and Loss Test (ACORNS)", "author": ["RH Nehm", "M Ha", "M Rector", "JE Opfer", "L Perrin", "J Ridgway", "K Mollohan"], "venue": "Technical Report of National Science Foundation REESE Project", "citeRegEx": "Nehm et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nehm et al\\.", "year": 2010}, {"title": "Reasoning about natural selection: diagnosing contextual competency using the ACORNS instrument", "author": ["RH Nehm", "EP Beggrow", "JE Opfer", "M Ha"], "venue": "The American Biology Teacher", "citeRegEx": "Nehm et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nehm et al\\.", "year": 2012}, {"title": "Transforming biology assessment with machine learning: automated scoring of written evolutionary explanations", "author": ["RH Nehm", "M Ha", "E Mayfield"], "venue": "Journal of Science Education and Technology", "citeRegEx": "Nehm et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nehm et al\\.", "year": 2012}, {"title": "Cognitive foundations for science assessment design: Knowing what students know about evolution", "author": ["JE Opfer", "RH Nehm", "M Ha"], "venue": "Journal of Research in Science Teaching", "citeRegEx": "Opfer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Opfer et al\\.", "year": 2012}, {"title": "Fast Training of Support Vector Machines Using Sequential Minimal Optimization", "author": ["J Platt"], "venue": null, "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "The inference of protein\u2013protein interactions by co-evolutionary analysis is improved by excluding the information about the phylogenetic relationships", "author": ["T Sato", "Y Yamanishi", "M Kanehisa", "H Toh"], "venue": null, "citeRegEx": "Sato et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sato et al\\.", "year": 2005}, {"title": "The relevance of recall and precision in user evaluation", "author": ["LT Su"], "venue": "Journal of the American Society for Information Science", "citeRegEx": "Su,? \\Q1994\\E", "shortCiteRegEx": "Su", "year": 1994}, {"title": "Computational identification of transcription factor binding sites via a transcription-factor-centric clustering (TFCC) algorithm", "author": ["Z Zhu", "Y Pilpel", "GM Church"], "venue": "Journal of Molecular Biology", "citeRegEx": "Zhu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 4, "context": "The 2012 Advanced Placement (AP) biology exam, for example, doubled the number of open-ended questions and dramatically reduced the number of multiple-choice questions (Duncan 2013).", "startOffset": 168, "endOffset": 181}, {"referenceID": 4, "context": "Secretary of Education notes that every year more and more assessments in K-12 education will move to an online environment, opening up additional opportunities for novel forms of knowledge and skill measurement (Duncan 2013).", "startOffset": 212, "endOffset": 225}, {"referenceID": 8, "context": ", clicker questions, midterm exams) remain the norm, greatly limiting the range of skills and competencies that can be assessed (Ha et al. 2011; Nehm and Haertig 2012).", "startOffset": 128, "endOffset": 167}, {"referenceID": 15, "context": ", clicker questions, midterm exams) remain the norm, greatly limiting the range of skills and competencies that can be assessed (Ha et al. 2011; Nehm and Haertig 2012).", "startOffset": 128, "endOffset": 167}, {"referenceID": 23, "context": "Such assessment tasks have been shown to be very useful for understanding student thinking processes (Opfer et al. 2012) and measuring evolutionary understanding (Nehm and Schonfeld 2008, 2010).", "startOffset": 101, "endOffset": 120}, {"referenceID": 3, "context": "ACORNS (and ACORNS-like) items are also useful for high school and undergraduate educators because they allow students to practice building scientific explanations, communicating their ideas through writing, and documenting their understanding of the core idea of natural selection (Demastes et al. 1995).", "startOffset": 282, "endOffset": 304}, {"referenceID": 15, "context": "science education (see [Nehm and Haertig 2012], and [Haudek et al.", "startOffset": 23, "endOffset": 46}, {"referenceID": 10, "context": "science education (see [Nehm and Haertig 2012], and [Haudek et al. 2012]).", "startOffset": 52, "endOffset": 72}, {"referenceID": 8, "context": "2012b], [Ha et al. 2011]).", "startOffset": 8, "endOffset": 24}, {"referenceID": 12, "context": "The back-end of EvoGrader relies on the supervised machine learning tools of LightSIDE Labs\u2019 open-source program known as LightSIDE (Mayfield et al. 2013).", "startOffset": 132, "endOffset": 154}, {"referenceID": 2, "context": "Bigram modeling (Cavnar and Trenkle 1994) may also be used and allows for the creation of double-word terms (e.", "startOffset": 16, "endOffset": 41}, {"referenceID": 24, "context": "Unpublished research (Nehm, unpublished data) suggests that Sequential Minimal Optimization (SMO) (Platt 1999) is the most effective algorithm for the corpus used in EvoGrader.", "startOffset": 98, "endOffset": 110}, {"referenceID": 13, "context": "The training corpus may contain the data misclassified by the cross-validation process (Muhlenbach et al. 2004).", "startOffset": 87, "endOffset": 111}, {"referenceID": 13, "context": "In such cases, we can remove the misclassified data from the original training corpus and re-train the scoring algorithm using the new corpus (Muhlenbach et al. 2004).", "startOffset": 142, "endOffset": 166}, {"referenceID": 7, "context": "Prior work has demonstrated the utility of machinelearning and text analysis methods for scoring written evolutionary explanations (e.g., Beggrow et al.; Ha and Nehm 2012; Nehm and Haertig 2012).", "startOffset": 131, "endOffset": 194}, {"referenceID": 15, "context": "Prior work has demonstrated the utility of machinelearning and text analysis methods for scoring written evolutionary explanations (e.g., Beggrow et al.; Ha and Nehm 2012; Nehm and Haertig 2012).", "startOffset": 131, "endOffset": 194}, {"referenceID": 16, "context": "All of these ideas have been commonly found in research on student thinking about evolution (e.g., Bishop and Anderson 1990; Nehm and Reilly 2007).", "startOffset": 92, "endOffset": 146}, {"referenceID": 1, "context": ", Bishop and Anderson 1990; Nehm and Reilly 2007). In addition to studying concept use patterns in the written responses, we also examined students\u2019 overall reasoning patterns (i.e., holistic models). Specifically, we used the cognitive models outlined by Nehm et al. (2009) to categorize each student response into one of four categories: a scientific model (including only normative scientific ideas), a mixed model (including both scientific and na\u00efve ideas), a na\u00efve model (including only nonnormative naive ideas), or no model (rephrasing the question, providing extraneous information, or not answering the question directly).", "startOffset": 2, "endOffset": 275}, {"referenceID": 1, "context": ", Bishop and Anderson 1990; Nehm and Reilly 2007). In addition to studying concept use patterns in the written responses, we also examined students\u2019 overall reasoning patterns (i.e., holistic models). Specifically, we used the cognitive models outlined by Nehm et al. (2009) to categorize each student response into one of four categories: a scientific model (including only normative scientific ideas), a mixed model (including both scientific and na\u00efve ideas), a na\u00efve model (including only nonnormative naive ideas), or no model (rephrasing the question, providing extraneous information, or not answering the question directly). The scoring rubrics of Nehm et al. (2010) were used to guide the production of human-generated scores.", "startOffset": 2, "endOffset": 675}, {"referenceID": 10, "context": "suggested by Landis and Koch (1977) and Fleiss (1971) were used in the evaluation of EvoGrader performance:", "startOffset": 13, "endOffset": 36}, {"referenceID": 6, "context": "suggested by Landis and Koch (1977) and Fleiss (1971) were used in the evaluation of EvoGrader performance:", "startOffset": 40, "endOffset": 54}, {"referenceID": 11, "context": "00 (Landis and Koch 1977) and", "startOffset": 3, "endOffset": 25}, {"referenceID": 6, "context": "00 (Fleiss 1971) (see Bejar (1991) and Ha et al.", "startOffset": 3, "endOffset": 16}, {"referenceID": 8, "context": "([Ha et al. 2011]) for additional details about these", "startOffset": 1, "endOffset": 17}, {"referenceID": 0, "context": "00 (Fleiss 1971) (see Bejar (1991) and Ha et al.", "startOffset": 22, "endOffset": 35}, {"referenceID": 9, "context": "Precision and recall are widely used measures in information retrieval studies (Ha et al. 2013; Su 1994).", "startOffset": 79, "endOffset": 104}, {"referenceID": 26, "context": "Precision and recall are widely used measures in information retrieval studies (Ha et al. 2013; Su 1994).", "startOffset": 79, "endOffset": 104}, {"referenceID": 7, "context": "Although Pearson correlation coefficients have been used to quantify such correspondence in previous studies (Beggrow et al.; Ha and Nehm 2012), Spearman\u2019s rank correlation is more appropriate in the present study because the range of scores for a single item was small and the distribution was not normal.", "startOffset": 109, "endOffset": 143}, {"referenceID": 27, "context": "9 to be \u2018nearly identical\u2019 (e.g., Sato et al. 2005; Zhu et al. 2002) and so we adopted this benchmark for our Spearman\u2019s rank correlation tests.", "startOffset": 27, "endOffset": 68}, {"referenceID": 6, "context": "75 kappa value that Fleiss (1971) considered to be an excellent kappa value.", "startOffset": 20, "endOffset": 34}, {"referenceID": 4, "context": "begun to change the field of educational assessment in the United States (Duncan 2013), nearly all evolution assess-", "startOffset": 73, "endOffset": 86}], "year": 2014, "abstractText": "EvoGrader is a free, online, on-demand formative assessment service designed for use in undergraduate biology classrooms. EvoGrader\u2019s web portal is powered by Amazon\u2019s Elastic Cloud and run with LightSIDE Lab\u2019s open-source machine-learning tools. The EvoGrader web portal allows biology instructors to upload a response file (.csv) containing unlimited numbers of evolutionary explanations written in response to 86 different ACORNS (Assessing COntextual Reasoning about Natural Selection) instrument items. The system automatically analyzes the responses and provides detailed information about the scientific and naive concepts contained within each student\u2019s response, as well as overall student (and sample) reasoning model types. Graphs and visual models provided by EvoGrader summarize class-level responses; downloadable files of raw scores (in .csv format) are also provided for more detailed analyses. Although the computational machinery that EvoGrader employs is complex, using the system is easy. Users only need to know how to use spreadsheets to organize student responses, upload files to the web, and use a web browser. A series of experiments using new samples of 2,200 written evolutionary explanations demonstrate that EvoGrader scores are comparable to those of trained human raters, although EvoGrader scoring takes 99% less time and is free. EvoGrader will be of interest to biology instructors teaching large classes who seek to emphasize scientific practices such as generating scientific explanations, and to teach crosscutting ideas such as evolution and natural selection. The software architecture of EvoGrader is described as it may serve as a template for developing machine-learning portals for other core concepts within biology and across other disciplines.", "creator": "Arbortext Advanced Print Publisher 9.1.440/W Unicode"}}}