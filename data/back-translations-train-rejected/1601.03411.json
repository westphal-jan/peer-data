{"id": "1601.03411", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2016", "title": "Analysis of Algorithms and Partial Algorithms", "abstract": "We present an alternative methodology for the analysis of algorithms, based on the concept of expected discounted reward. This methodology naturally handles algorithms that do not always terminate, so it can (theoretically) be used with partial algorithms for undecidable problems, such as those found in artificial general intelligence (AGI) and automated theorem proving. We mention new approaches to self-improving AGI and logical uncertainty enabled by this methodology.", "histories": [["v1", "Wed, 13 Jan 2016 21:17:42 GMT  (14kb)", "https://arxiv.org/abs/1601.03411v1", null], ["v2", "Sun, 28 Feb 2016 18:30:51 GMT  (16kb)", "http://arxiv.org/abs/1601.03411v2", null], ["v3", "Tue, 1 Mar 2016 19:21:41 GMT  (16kb)", "http://arxiv.org/abs/1601.03411v3", null], ["v4", "Sun, 8 May 2016 00:52:51 GMT  (28kb)", "http://arxiv.org/abs/1601.03411v4", null], ["v5", "Mon, 7 Aug 2017 01:30:46 GMT  (28kb)", "http://arxiv.org/abs/1601.03411v5", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DS", "authors": ["andrew macfie"], "accepted": false, "id": "1601.03411"}, "pdf": {"name": "1601.03411.pdf", "metadata": {"source": "CRF", "title": "Analysis of Algorithms and Partial Algorithms", "authors": ["Andrew MacFie"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 1,03 411v 5 [cs.A I] 7 Aug 201 7"}, {"heading": "1 Introduction: Shortcomings of Traditional Analysis of Algorithms", "text": "The (runtime) analysis of algorithms takes the following form. In the face of two algorithms A, B that solve the same problem, we find out which one is more efficient by asymptotically comparing the runtime sequences (an), (bn) [4,15]. This could be the use of worst-case problems or average fall times or even smoothed analyses [16]. We refer to this general method as traditional analysis of algorithms. As with any other model, the traditional analysis of algorithms is not perfect. Authors have pointed out [1,9] that comparing sequence endings avoids the arbitrariness of a certain range of input lengths, but leads us to say that a = n100 is better than bn = (\u2212 exp), which is wrong for practical purposes. Another problem with traditional analysis is illustrated by this situation: Say we have a function F: {0, 1}."}, {"heading": "2 Expected-Reward Analysis of Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Definition", "text": "We say that the score of A isS (A) = \u2211 \u0438 P ({\u03c9}) r (\u03c9) D (\u03c9)) = E (r \u00b7 cA)), where P is an appropriate reward, is a discount function [7], and r (\u03c9) is a reward (a.k.a. use value) associated with obtaining the solution to the problem. However, the expression S (A) can be interpreted as the expected reward that A receives when applied to a random input, and the practice of comparing scores between algorithms, which we call expectation reward analysis, indicates a more efficient algorithm. Functions D and r are arbitrary and can be set freely in the context of a particular application. E.g. in graphical user interface software, which we often want immediate responses, with the benefit rapidly diminishing."}, {"heading": "2.2 Theory and Practice", "text": "Traditional analysis of algorithms has an established literature that goes back decades and provides a number of techniques for performing traditional analyses of Q = Q = Q (Q = Q = 1) that have been developed for various problems. We are not developing a significant mathematical theory of expectation reward analysis here, but we are making some very brief initial notes.As an introductory example, we will consider the analysis of expectation reward applied to some known sorting algorithms. Let Sn determine the number of permutations of [1.. n] and let them be a uniform random element of Sn. We refer to the merge and quick sorting algorithms of M and Q as defined in [15], and se\u01d0 = E [exp] [exp (\u2212 x cM (n)]], qn = E [exp [exp [exp], \u03bb cQ (n)))))), where cA (n) is the number of comparison elements used by an algorithm that is an element of a comparison element."}, {"heading": "3 Self-Improving AI", "text": "The universality of the 0 \"problems allows us to consider the design and analysis of 0\" algorithms as a task that can itself be led to a 0 \"algorithm that induces a recursive self-improvement. Here, we present a possible concrete example of this term and discuss connections with AI.Computational problems with the Turing degree 0\" are Turing equivalent so without loss of generality in this section we assume that 0 \"algorithms are automated theorems. Specifically, we define a formal logic system that is consistent, and take the amount of inputs to be ZFC sentences, and the possible outputs to be detectable and unprovable.Let a predicate \u03b2 that \u03b2 (Z) holds a 0\" algorithm that is correct on detectable inputs and otherwise does not terminate. In pseudo-code, we write the instructions to pass some Z on input as Z, and if this function contains its implications. \""}, {"heading": "4 Future Work", "text": "We would like to apply the expectation reward analysis with different parameter values, probability variables and discount functions to both terminating and non-terminating algorithms. In particular, we would like to know if 0 \"algorithms can be analyzed practically. It may be possible to develop general mathematical tools and techniques to improve the practicality of these methods as they exist for traditional analyses; this is a comprehensive and open research goal. Recognition. The author thanks Zhicheng Gao, Nima Hoda, Patrick LaVictoire, Saran Neti and anonymous speakers for helpful comments."}], "references": [{"title": "Why philosophers should care about computational complexity", "author": ["S. Aaronson"], "venue": "Computability: G\u00f6del, Turing, Church, and Beyond", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Looper: Lightweight detection of infinite loops at runtime", "author": ["J. Burnim", "N. Jalbert", "C. Stergiou", "K. Sen"], "venue": "In International Conference on Automated Software Engineering", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Re: Proof theory on the eve of year", "author": ["S. Buss"], "venue": "http://www.ihes.fr/~carbone/papers/proofsurveyFeferman2000.html", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Introduction to Algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": "MIT Press, Cambridge, MA, third edn.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Handbook of Theoretical Computer Science, Vol", "author": ["P. van Emde Boas"], "venue": "A. pp. 1\u201366. MIT Press, Cambridge, MA, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1990}, {"title": "Analytic combinatorics", "author": ["P. Flajolet", "R. Sedgewick"], "venue": "Cambridge University Press, Cambridge", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Time discounting and time preference: A critical review", "author": ["S. Frederick", "G. Loewenstein", "T. O\u2019Donoghue"], "venue": "Journal of Economic Literature pp. 351\u2013401", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Toward a formal characterization of real-world general intelligence", "author": ["B. Goertzel"], "venue": "Proceedings of the 3rd Conference on Artificial General Intelligence, AGI. pp. 19\u201324", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Feasible functions", "author": ["Y. Gurevich"], "venue": "London Mathematical Society Newsletter 206, 6\u20137", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability", "author": ["M. Hutter"], "venue": "Springer, Berlin", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Gamma: Exploring Euler\u2019s Constant", "author": ["H. Julian"], "venue": "Princeton University Press", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "The Art of Computer Programming, Vol", "author": ["D.E. Knuth"], "venue": "1. Addison-Wesley, Reading, MA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "An Introduction to Kolmogorov Complexity and its Applications", "author": ["M. Li", "P. Vit\u00e1nyi"], "venue": "Springer Science & Business Media", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "G\u00f6del machines: Fully self-referential optimal universal selfimprovers", "author": ["J. Schmidhuber"], "venue": "Artificial General Intelligence, pp. 199\u2013226. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "An Introduction to the Analysis of Algorithms", "author": ["R. Sedgewick", "P. Flajolet"], "venue": "AddisonWesley", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Smoothed analysis: an attempt to explain the behavior of algorithms in practice", "author": ["D.A. Spielman", "S.H. Teng"], "venue": "Communications of the ACM 52(10), 76\u201384", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "The Mathematica Guidebook for Symbolics", "author": ["M. Trott"], "venue": "Springer Science & Business Media", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "Given two algorithms A, B that solve the same problem, we find which is more efficient by asymptotically comparing the running time sequences (an), (bn) [4,15].", "startOffset": 153, "endOffset": 159}, {"referenceID": 14, "context": "Given two algorithms A, B that solve the same problem, we find which is more efficient by asymptotically comparing the running time sequences (an), (bn) [4,15].", "startOffset": 153, "endOffset": 159}, {"referenceID": 15, "context": "This could be using worst-case or average-case running times or even smoothed analysis [16].", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "Authors have noted [1,9] that comparing sequence tails avoids the arbitrariness of any particular range of input lengths but leads us to say an = n 100 is superior to bn = ( 1 + exp(\u221210) n which is false for practical purposes.", "startOffset": 19, "endOffset": 24}, {"referenceID": 8, "context": "Authors have noted [1,9] that comparing sequence tails avoids the arbitrariness of any particular range of input lengths but leads us to say an = n 100 is superior to bn = ( 1 + exp(\u221210) n which is false for practical purposes.", "startOffset": 19, "endOffset": 24}, {"referenceID": 1, "context": "Such problems include string compression (Kolmogorov complexity), the halting problem in program analysis [2], algebraic simplification [17], program optimization, automated theorem proving, and Solomonoff induction (central to artificial general intelligence [13]).", "startOffset": 106, "endOffset": 109}, {"referenceID": 16, "context": "Such problems include string compression (Kolmogorov complexity), the halting problem in program analysis [2], algebraic simplification [17], program optimization, automated theorem proving, and Solomonoff induction (central to artificial general intelligence [13]).", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "Such problems include string compression (Kolmogorov complexity), the halting problem in program analysis [2], algebraic simplification [17], program optimization, automated theorem proving, and Solomonoff induction (central to artificial general intelligence [13]).", "startOffset": 260, "endOffset": 264}, {"referenceID": 2, "context": "theorem proving, Buss, describing the main open problems in proof theory [3], states, \u201cComputerized proof search .", "startOffset": 73, "endOffset": 76}, {"referenceID": 6, "context": "where P is a probability measure on \u03a9, D is a discount function [7], and r(\u03c9) is a reward (a.", "startOffset": 64, "endOffset": 67}, {"referenceID": 12, "context": "The measure P is also determined by a probability mass function on Z0+ if we weight equal-length inputs according to Solomonoff\u2019s universal distribution m [13], which is a particularly good general model, although computationally difficult.", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "Computational complexity theory often works with classes of problems whose definitions are equivalent for all \u201creasonable\u201d models of computation [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "This is simply the price of concreteness, and outside of complexity theory, traditional analysis of algorithms generally selects a particular model of computation and gives precise results that do not necessarily apply to other models [6].", "startOffset": 235, "endOffset": 238}, {"referenceID": 7, "context": "The expected-reward paradigm already appears in the analysis of artificial agents, rather than algorithms [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 14, "context": "We denote the algorithms mergesort and quicksort by M and Q, as defined in [15], and set", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "From [15], M makes the same number of comparisons for all inputs of length n \u2265 1: cM (\u03a0n) = n\u2308lg(n)\u2309+ n\u2212 2 \u2308lg(n)\u2309 , so (1) is immediate.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "Sedgewick and Flajolet [15] give an alternative expression for the running time of mergesort:", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "For the lower bound on qn, we use the probabilistic form of Jensen\u2019s inequality, qn = E [exp(\u2212\u03bbcQ(\u03a0n))] \u2265 exp(\u2212\u03bbE [cQ(\u03a0n)]) , noting that average-case analysis of quicksort [15] yields", "startOffset": 173, "endOffset": 177}, {"referenceID": 10, "context": "holds [11] (sharper bounds exist), so we have", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "In artificial general intelligence (AGI) it is desirable to have intelligent systems with the ability to make autonomous improvements to themselves [14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 9, "context": "If an AGI system such as an AIXI approximation [10] already uses a 0 algorithm Z to compute the universal distribution m, we can give the system the ability to improve Z over time by devoting some of its computational resources to running Improve.", "startOffset": 47, "endOffset": 51}], "year": 2017, "abstractText": "We present an alternative methodology for the analysis of algorithms, based on the concept of expected discounted reward. This methodology naturally handles algorithms that do not always terminate, so it can (theoretically) be used with partial algorithms for undecidable problems, such as those found in artificial general intelligence (AGI) and automated theorem proving. We mention an approach to self-improving AGI enabled by this methodology.", "creator": "LaTeX with hyperref package"}}}