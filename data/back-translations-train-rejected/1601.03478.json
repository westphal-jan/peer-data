{"id": "1601.03478", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2015", "title": "Deep Learning Applied to Image and Text Matching", "abstract": "The ability to describe images with natural language sentences is the hallmark for image and language understanding. Such a system has wide ranging applications such as annotating images and using natural sentences to search for images.In this project we focus on the task of bidirectional image retrieval: such asystem is capable of retrieving an image based on a sentence (image search) andretrieve sentence based on an image query (image annotation). We present asystem based on a global ranking objective function which uses a combinationof convolutional neural networks (CNN) and multi layer perceptrons (MLP).It takes a pair of image and sentence and processes them in different channels,finally embedding it into a common multimodal vector space. These embeddingsencode abstract semantic information about the two inputs and can be comparedusing traditional information retrieval approaches. For each such pair, the modelreturns a score which is interpretted as a similarity metric. If this score is high,the image and sentence are likely to convey similar meaning, and if the score is low then they are likely not to.", "histories": [["v1", "Mon, 14 Sep 2015 17:19:33 GMT  (376kb,D)", "http://arxiv.org/abs/1601.03478v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.CV", "authors": ["afroze ibrahim baqapuri"], "accepted": false, "id": "1601.03478"}, "pdf": {"name": "1601.03478.pdf", "metadata": {"source": "CRF", "title": "Deep Learning applied to Image and Text matching", "authors": ["Afroze Ibrahim Baqapuri"], "emails": ["afroze.baqapuri@epfl.ch", "francois.fleuret@idiap.ch", "cosatto@nec-labs.com"], "sections": [{"heading": "Deep Learning applied to Image and Text", "text": ""}, {"heading": "MASTER THESIS IN COMPUTER SCIENCE", "text": "Author: Afroze Ibrahim Baqapuri afroze.baqapuri @ epfl.chSupervisors: Dr. Fran\u00e7ois Fleuretfrancois.fleuret @ idiap.ch Dr. Eric Cosattocosatto @ nec-labs.com15. January 2016 ar Xiv: 160 1.03 478v 1 [cs.L G] 14 Sep 2015"}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 9", "text": "1.1 Organisation of the report............................... 10"}, {"heading": "2 Literature Review 13", "text": "2.1 Deep learning in images and computer vision............. 13 2.2 Deep learning in text and NLP..................... 16 2.3 Deep learning in image and text multimodal models......... 182.3.1 Introduction................. 18 2.3.2 Evaluation Metrics.............. 182.3.1 Introduction.......... 20 2.3.3 Image and text mapping........................"}, {"heading": "3 Resources 27", "text": "3.1 Flickr8K..........................................................................................................................."}, {"heading": "4 Research Methodology 31", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Proposed Model 37", "text": ".................................................................................................................................."}, {"heading": "6 Experiments and Results 45", "text": "6.1. Pre-processing.................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "7 Conclusion 57", "text": "7.1 Future work................................. 59Annexes 61"}, {"heading": "A Acronyms 61", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Extra Resources 62", "text": "B.1 MNIST......................................... 62 B.2 Caltech256................................ 62 B.3 ImageNet....................................................."}, {"heading": "C Software 64", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D Further results of I2T and T2I training methodologies 65", "text": "Bibliography 65"}, {"heading": "Abstract", "text": "In this project, we focus on the task of bidirectional image retrieval: such a system is able to retrieve an image based on a set (image search) and retrieve sentences based on an image retrieval (image description). We present a system based on a global ranking of objective functions that uses a combination of Convolutionary Neural Networks (CNN) and Multi-layered Perceptrons (MLP). It takes a pair of images and sentences and processes them in different channels to eventually embed them in a common multimodal vector space. These embedding encodes abstract semantic information about the two inputs and can be compared with traditional information retrieval approaches. For each such pair, the model delivers a score that is interpreted as a sequence size. If this score is high, image and sentence can be compared."}, {"heading": "Acknowledgments", "text": "I would like to thank NEC and EPFL for the opportunity to work on such an interesting project. Special thanks to Bing Bai, Iain Melvin, Eric Cosatto, Igor Durdanovic and Martin Renqiang Min for the many fruitful discussions."}, {"heading": "Introduction", "text": "The ability to describe images with natural language elements is the hallmark of the understanding of language. It has important applications such as searching for images with natural sentences and automatically capturing images. Advances in this area have far-reaching implications, as images are ubiquitous on the Internet, and we can describe an image with relative ease. However, this is not a trivial task. The main difficulty is that the two input modalities have very different statistical properties and cannot be directly compared."}, {"heading": "1.1 Organization of report", "text": "The rest of the final report is broken down as follows: In Chapter 2, we present an overview of the available scientific literature in this field. We start with deep learning from a historical perspective and discuss its main ideas and significance, first in the image community and then in word processing. Finally, we discuss recent previous work in the combined field of images and text as multimodal input. Then, in Chapter 3, we briefly describe some of the important resources used in our project, including datasets that we have trained and tested, as well as some publicly available tools that we have used in our larger model.In Chapter 4, we provide an overview of the research methodology and timeline of the experiments that we have conducted. This chapter serves as an optional section, allowing the reader to get over it without losing any necessary information in understanding our final models and experiments (reference to the chapter is made whenever we make a point that needs support from there). The material covers our experience with the problem, how it has affected the way we accepted the task."}, {"heading": "Literature Review", "text": "In this section we will provide a summary of important research that has been done in the field of deep learning over the last decade. Bibliography will comprise three parts: 1. Deep Learning in Images and Computer Vision.2. Deep Learning in Text and NLP.3. Deep Learning in Multimodal Image and Text Modeling. The last part is the most relevant to our task, but we believe that a background discussion is important to understand and appreciate the workings of this model."}, {"heading": "2.1 Deep learning in images and computer vi-", "text": "In fact, most people are able to decide for themselves what they want and what they want. It's not that they are able to decide for themselves what they want and what they want. It's that they don't want it. It's that they do it as if they want it."}, {"heading": "2.2 Deep learning in text and NLP", "text": "This year, it has come to the point where it only takes one year for it to come to a conclusion."}, {"heading": "2.3 Deep learning in image and text multimodal", "text": "Great progress has been made in the multi-level classification of images with single words or keywords, but the more difficult problem of linking images with complete natural sentences has only recently gained attention. Research in this area focuses mainly on two tasks, namely: 1. Mapping images and sentences into a combined space 2. Generating descriptions of images in relation to complete and natural sentences. The first presents it as a problem of obtaining information, while the second treats it as a problem of natural language generation."}, {"heading": "2.3.1 Introduction", "text": "[Hodosh et al., 2013] have written one of the groundbreaking papers in this area, which provides an interesting discussion of the problem, an in-depth comparison1https: / / code.google.com / p / word2vec / of the available data sets (including the type of data required for good modeling), an analysis of the modeling techniques used in the early stages of this field, and a detailed discussion of the various evaluation metrics used in various previous related work. [Hodosh et al., 2013] enter into the philosophy of image description by arguing that there are three different types of image descriptions: 1. Identify conceptual descriptions of what is shown in the image. They deal with the concrete descriptions of the scenes and entities depicted, their attributes and relationships, and events in which they participate."}, {"heading": "2.3.2 Evaluation Metrics", "text": "Since image description is a subjective task, the ideal setting for the evaluation of a system would be an averaged human assessment. However, since human assessment is expensive and slow, there are a number of metrics used in evaluating these systems. These metrics can be divided into two categories: \u2022 metrics for text generation systems. \u2022 metrics for the ranking systems.BLEU [Papineni et al., 2002] and ROUGE [Lin, 2004] metrics for automatic image description systems. Originally, they were standard metrics for machine translation and summary, but they were used to evaluate multiple image caption systems [Vinyals et al., 2014, Ordonez et al., 2011, Gupta et al., 2012]. Given a picture caption c and an image i with a series of reference labels Ri, the BLEU evaluation of a proposed image caption system (i) is based on the precision against Ri-grams."}, {"heading": "2.3.3 Image and text mapping", "text": "Unlike most other countries in the world, where most people are able to recognize and understand their identity, most countries in the world are unable to identify themselves. Most countries in the world, which are able to recognize their identity and identity, are able to recognize and understand themselves. Most countries in the world have the same problems that they want to solve. Most countries in the world have the same problems that they need to solve. Most countries in the world have the same problems that they need to solve. Most countries in the world have the same problems that they need to solve. Most countries in the world have the same problems that they need to solve. Most countries in the world have the same problems, but they need to solve. Most of them have the same problems that they need to solve \"that they need to solve\" that they need to solve. \""}, {"heading": "2.3.4 sentence generation from images", "text": "Now we will briefly discuss the related task of generating natural sentences from images. [Kiros et al., 2014] Although it is not the focus of our project, it is still too interesting to see the approach taken in this area, especially as there has been a growing interest in it lately. [Kiros et al., 2014] present a model in which they use a Convolutionary Neural Network (CNN) for this task. CNN has four input paths, one for an image and the other three for words. The idea is that the model learns to predict the next word in sequence, thus learning a common \"multimodal language model.\" Recurrent neural networks (RNN) are quite popular for text generation, and so many researchers use them in this task, albeit in different environments [Karpathy and Fei-Fei, 2014, Vinyals Input N, 2014]."}, {"heading": "Resources", "text": "We used various datasets and resources in our research and experiments, and we will provide a brief description of it below. Most of the high-quality dataset is collected through crowdsourcing methods characterized by human input. Although there are some large, auto-generated datasets (for example, by user comments when uploading images to photo-sharing websites), [Hodosh et al., 2013] convincingly argue that these descriptions do not align with the objectives of our task and that we do not use them (as many researchers in this area do)."}, {"heading": "3.1 Flickr8K", "text": "Flickr8K 1 is a dataset consisting of 8,092 images collected from the Flickr website [Rashtchian et al., 2010], each image has five captions describing the content of the image. Images in this dataset focus on humans or animals (mainly dogs) performing some action, and the images tend to have no known locations, but are selected manually to represent a variety of scenes and situations, and are crowdsourced by human captioners (five for each image) using Amazons Mechanical Turk 2. To avoid grammatical errors, captioners (who only came from the US) had to pass a brief quality test based on spelling and grammar, and were then asked to write sentences describing the scenes, situations, events and entities depicted (people, animals, other objects). To avoid grammatical errors, captioners (who only came from the US) had to collect the same modeling of the image in order to provide a more consistent description of the different people."}, {"heading": "3.2 Flickr30K", "text": "Flickr30K 4 is an extension of the Flickr8K dataset, consisting of 31,783 images collected by [Young et al., 2014]. Similar to its counterpart, each image is associated with five captions written by people using the Mechanical Turk of the Amazon. The images consist of everyday activities, events and scenarios. It is important that commentators are not familiar with the entities and circumstances depicted therein, in order to avoid making excessively specific comments. For example, the comments \"Three people pitching a tent\" are more relevant to the task than \"Our Journey to the Olympic Peninsula\" (for the same image - the latter is probably a comment given by a person familiar with the meaning of the image). Furthermore, different commentators use different levels of specificity, from the description of the overall situation (performing a piece of music) to specific actions (bowing to an image that is not necessary to synchronize with each other)."}, {"heading": "3.3 OverFeat", "text": "OverFeat 5 is a publicly available visual feature extractor based on the Convolutionary Neural Network submitted by [Sermanet et et al., 2014] to the large-scale ILSVRC-2013 object recognition competition. At the time of publication, it was ranked 4th in classification, 1st in localization, and 1st in recognition tasks for ILSVRC-13 datasets. We use the precise version of its model, which has 144 million free parameters and 5.4 billion connections and achieves a 14.18% performance on the validation set of the competition. Its network architecture is similar to the architecture of [Krizhevsky et al., 2012], with the addition that it is trained on images in multiple scales and has improved inference increments. 3http: / / nlp.cs.ilois.edu / HockenmaierGroup / 8k-overvfeat. 4res.html: http: / / nlp.linoiscann.cils.h / Denilois.Group / 8k-overvres.cres.html?"}, {"heading": "3.4 Word2vec", "text": "Word2vec 6 is a publicly available tool that provides an efficient implementation of the continuous learning of words and vector representations based on word skip programs. Model and implementation are based on the work of [Mikolov et al., 2013b]. In addition to implementation, they also offer vector representations of words and phrases that they learned by training this model on Google News Dataset (about 100 billion words), which are 300-dimensional vectors for 3 million words and phrases. An interesting feature of these vector representations is that they capture linear regularities in the language. For example, the result of vector calculation: vec (\"Madrid\") - vec (\"Spain\") + vec (\"France\") is vec closest to vec (\"Paris\").6https: / / code.google.com / p / word2vec Chapter 4"}, {"heading": "Research Methodology", "text": "Most of the experiments mentioned in this chapter are not directly involved in our final results, and so the reader cannot understand the way we approach the task, and how we arrived at our final model is repeated in the next chapter. The reason for including this chapter is that the reader understands how our experience with the problem affects the way we approach the task (including the reserves we face), and how we arrived at our final model. We decided to treat the problem of image descriptions as a retrieval task, and decided to follow a ranking, which was influenced by previous work in the NEC on semantic indexing. [Bai et al], this is a system based on retrieval."}, {"heading": "Proposed Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Abstract Architecture", "text": "Our model is inspired by SSI's previous work in the NEC (supervised semantic indexing) [Bai et al., 2009], which is a textual query system. We aim to extend this model to multimodal inputs: in particular images and text. We focus on bidirectional query. In other words, retrieving images based on a sentence query (image search) and retrieving sentences based on an image query (image description). The sentences are complete and natural, constructed by human commentators, as opposed to individual and isolated tags.The general abstract architecture of our complete model is illustrated in Figure 5.1. The two submodels PosNet and NegNet have identical architecture and they share weights. For a pair of text and image inputs, they calculate the score s, which is a measure of the similarity between the corresponding text and image (based on cosine distance)."}, {"heading": "Margin Ranking Criterion", "text": "In our system, the text model and the visual model are replaced by ANN (artificial neural networks). In the face of a sentence and an image, these ANNs are trained to transform them into low-dimensional, high-quality vector embedding. Vtxt means the vector generated from text input, and Vimg is the vector embedded from visual (image) input. By high quality, we mean that the vectors are embedded in a common embedding space that encodes semantic information at a high level about the content of the image and text, while details at the functional level are ignored (for example, that you are text and you are input). The advantage is that multimodal input is once embedded in a common space in which we can apply standard vector operations to measure their similarity."}, {"heading": "5.2 Details Visual and Textual models", "text": "This is because ANNs can be trained continuously, optimizing the global lens function and requiring minimal pre-processing. As discussed in Chapter 2, different ANN architectures also achieve state-of-the-art results in various machine learning tasks. We hope to reciprocate these results for our task. We use a Convolutionary Neural Network (CNN) for the visual model, as it has become ubiquitous in image recognition tasks. As discussed in Chapter 4, we experimented with forming a CNN from scratch (with random neural networks)."}, {"heading": "Si,j", "text": "In fact, most of us are able to go in search of a solution."}, {"heading": "5.3 Unique training methodology", "text": "As we explained above, we create a negative sample by taking a positive sample and then replacing the sentence (or image) with another random sentence (or image). In fact, we can choose two approaches: replace the sentence with another sentence (while retaining the image) or replace the image with another image (while retaining the sentence). It turns out that this choice makes a difference in the evaluation results, so from now on we will refer to this choice as a training method: txt1 = txt2, img1 6 = img2We will discuss the effects of this with empirical results in the next chapter: img1 = img2, txt1 6 = txt2T2I approach: txt1 = txt2, img1 6 = img2However, we would briefly mention that the I2T approach provides higher performance for image search systems, while this system can be a specialized system by nature (the task can be generated in a different way)."}, {"heading": "5.4 Comparison to other models", "text": "Deeply rooted CNNs are now ubiquitous with image recognition and feature extraction, and reach the state of the art in many important computer vision tasks. Therefore, like most recent work, we stick to CNNs for our visual model. However, instead of training the network from the ground up, we use a publicly available OverFeat model, which performed very well in ILSVRC-2013. Our original reason was not to waste any additional time training the model from the ground up on large data sets like ImageNet. They train the model with various special tricks such as multiple image fragments, multiple scales, maximizing image localization, introducing color and translation invariance with data augmentation. For this reason, the model provides excellent feature vector representation of images, and we believe that our good results are due in part to the high modeling strength and accuracy of our visual modeling models created by the high level of modeling and accuracy of our visual modeling."}, {"heading": "Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Preprocessing", "text": "The images have all been scaled to a constant dimension of 221 \u00d7 221, as the ANN must have a constantly large input. This is also the input dimension of OverFeat CNN, which we use in our model. We do not cut out the image to ensure that a visually significant object is not accidentally truncated, making its identification difficult. We convert all sentences into lowercase letters, then remove punctuation and any other non-alphanumeric characters from them, and reduce them to a sequence of individual words. We filter out the articles \"a,\" \"an,\" and \"the,\" believing that they do not contribute to the semantic value of the sentence. Finally, we limit the word vocabulary by the number of occurrences. For unigram BoW, our vocabulary contains 5000 words, while for branches and trigrams the occurrence of f is."}, {"heading": "6.2 Evaluation Metrics", "text": "In this section we will consider the results of our models on groups of mainly Flickr30K dataset and occasionally FLickr8K. Recall @ K (R @ K) is the most commonly used metric in this area. It is defined as the percentage of test queries for which a model returns the positive element among the top k results. It is useful in the context of the search, in which a user can be satisfied with the first k results containing a single relevant point. However, this definition is ambiguous if there is more than one positive result for each test query. This also applies to image annotation in our case, as each image matches 5 sets. Some researchers only consider the first of 5 sets in test cases (e.g. [Hodosh et al., 2013, Gong et al., 2014]), while others consider all 5 (e.g. [Karpathy et al., 2014]."}, {"heading": "6.3 Training Details", "text": "In all of our experiments, we set the parameters of the OverFeat network so that its weights do not update with the backflow. Generally, it takes 1-2 days (depending on the size of the model) for our model to be fully trained on the Flickr30K dataset using an 8-core (16 threads) CPU. Typically, the network runs 50 epochs before completion. From the Flickr8K and FLickr30K datasets, we separate random 1000 images (and corresponding sets) as a test set and 5% of the remaining examples as a pre-set validation set for model selection. We have tried several models and selected the meta parameters based on the provided validation set. We select the parameters mentioned in 6.1 for all of our experiments for fair comparison."}, {"heading": "6.4 Experimental Results", "text": "Figure 6.1 shows that our model performs well in validation (with errors below 6%) compared to our very first model that failed to generalize to validation, as seen in Figure 4.1. We can also see that validation performance is saturated around the 20th epoch, and we store the networks only to the point where validation performance is not yet saturated, in order to minimize the chance of overhauling validation at the training level.1lr decreases linearly to a factor of 0.01 over 100 epochs 2rectified linear units."}, {"heading": "6.4.1 I2T and T2I training approach", "text": "As explained in Chapter 5.3, we have tried to find methods for generating negative samples during training, namely: I2T and T2I. We note that this choice actually influences the performance of our system in a particular way; it specializes in a particular task. As shown in Table 6.2, using the I2T method leads to higher performance for caption tasks and the T2I method to higher performance for caption tasks. We compare performance across multiple model architectures and consistently notice the same phenomenon. So when we refer to image labels, we report I2T results, and for image processing, we report T2I results. The reason for this specialization is intuitive, that I2T is more consistent with image labels, and T2I with image search. In I2T, the images are the same in both positive and negative samples, while the sentences are different, so the system learns to detect changes in sentences better and thus ultimately perform a better comparison."}, {"heading": "6.4.2 Results of BoW model", "text": "First, we will report on the results of our BoW model, where our text model is just an MLP with unique word properties; all information about the temporal dependence of the sentence is lost; and we use a dictionary size of 5,000 words (after removing \"a,\" \"an,\" and \"the\"); and we use binary characteristics, so that there is no information about the multiple occurrence of words in a sentence; the hidden level of the text model is also fixed at 1,000; Table 6.3 shows the results on Flickr8K test set; and Table 6.4 shows the results on Flickr30K test set (both of the size of 1,000 images); and we use this model as our BoW model: Comparing the results from Chapter W, we are quite promising and the results from 7.Architectures are ready to follow."}, {"heading": "6.4.3 Word2Vec results", "text": "Next, we will examine the effects of Word2Vec embedding. As already mentioned, we have used it as a way to initialize our text model well."}, {"heading": "BoW model on Flickr8K", "text": "We are repeating the experiment on Flickr30K using the BoW model, with the only difference that we are using randomly initialized networks. This experiment is useful because we have proposed to apply n-gram models to the BoW model as a second step, but we are aware that there are no such publicly available n-gram embeddings. The results of the experiment are presented in Table 6.5, which we can compare with Table 6.4. A quick comparison of the performance indicators in these two tables shows that the word2vec initialization contributes to improving the accuracy of the system, but the effects are not extremely drastic. For example, R @ 10: avg _ txt increases by 3.3% for the image description, while image searches without this initialization increase by 3.86%. The results are significant to suggest the importance of using word2vec for the initialization of our models, but we believe that the gap can be bridged with more complicated models without this initialization."}, {"heading": "BoW model w/o Word2Vec", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.4.4 Results of n-gram models", "text": "In our experiments, we tried bigrams (2g), trigrams (3g) and trigrams + skip grams (tk3). In all of these cases, the vocabulary size is set to 50,000, which corresponds to the size of the input layer of the text model MLP. Table 6.6 shows a comparison between these three different types of models for our task. Results show that both bigrams and trigrams work better in different settings, while the inclusion of skip programs in the trigram model consistently resulted in the worst performance. Afterwards, we also experimented with binary or tf-idf rated characteristics. Intuition suggests that tf-idf encodes more information and should therefore be better than binary characteristics. However, empirical evidence suggests that binary characteristics performed better in most of our models."}, {"heading": "6.5 Are deep textual models necessary", "text": "We will examine whether a deep network is really necessary in the text model. We have examined several flat single-layer architectures in which the input of n-Grammys is linearly transformed into the embedding vector. Surprisingly, almost all the networks we have tried have achieved a performance close to what we have achieved with non-linearity. In addition to their performance, these shallow models were about 3 times faster to train (disregarding the time required for the OverFeat Feature Vector generation). We have achieved the best performance with a 300-dimensional embedding associated with 5,000 input devices. Also, the small supporting visual network does not contain a hidden layer, and the 4,096 dimensional OverFeat features are vectors that we transform linearly into the 300-dimensional dimensions. The result of this model is in Table 6.9."}, {"heading": "6.6 Comparison with existing systems", "text": "We would like to compare the performance of our model with existing state-of-the-art systems. However, in Chapter 6.2, we have found that the definition of the most common evaluation quantity (Recall @ K) for this task is fraught with ambiguities. Furthermore, researchers often differ in the way they present the results. We use all 5 sentences in evaluating our system. To compare the task for image annotation, we report R @ K avg _ txt and to compare the task for image search, R @ K any _ txt. Here, we follow the standard of [Karpathy et al., 2014], and report it to the best of our knowledge."}, {"heading": "SSE for textual model", "text": "Table 6.11 shows that our models are comparable to recent work in field3 [Frome et al., 2013] 4 [Socher et al., 2014] 5 [Karpathy et al., 2014] 6fragment alignment objective 7global ranking objective 8multi instance learning9 and are not far from the top results. In comparing the results, it is interesting to note that most recent research results have been incorporated into complicated textual modeling. For example, [Socher et al., 2014] use SDT-RNN, a dependency tree-based recursive neural network. They also try other types of recursive and recursive neural networks. In addition, [Karpathy et al., 2014] use dependency tree margins to fragment their theorems, etc. In contrast, we use simple textual models and achieve comparable results. Most of the strength of our model comes from the visual model and training methodology that specializes the performance of a particular task."}, {"heading": "Conclusion", "text": "In fact, we are able to behave in a way that we have done in the past: in the way that we have done it, the way that we have done it, and the way that we have done it, the way that we have done it, the way that we have done it, the way that we have done it, the way that we have done it, the way that we have done it, the way that we have done it, the way that we have done it, the way that we have done it, the way that we have done it, the way that we have done it, the way that we have done it, the way that we have done it."}, {"heading": "7.1 Future work", "text": "In fact, it is in such a way that we are in a position to put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in another world, in which we find ourselves in another world, in which we find ourselves in another world, in which we find ourselves in another world, in which we find ourselves in another world, but in which we find ourselves in another world, in which we find ourselves in another world, in which we find ourselves in which we find ourselves in which we live, in which we live in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live,"}, {"heading": "Appendix A", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Acronyms", "text": "ANN artificial neural networkBoW bag full of wordsCNN Convolutionary neural networkI2T image to textKCCA kernel conical correlation analyseR @ K recall at kMLP multi layer perceptronNLP natural language Procecesingrelu rectified linear unitRNN recurrent neural networkRNN recursive neural networkSGD stochastic gradient descentSSE Supervised sequence embedSSI Supervised semantic indexingT2I text to imagetf-tdf term frequency - inverse document frequency"}, {"heading": "Appendix B", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Extra Resources", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 MNIST", "text": "MNIST 1 is a database of handwritten digits consisting of 60,000 training examples and 10,000 test examples. The digits are size normalized and centered in a fixed size image of 28 x 28 pixels. The data set has a long history as a benchmark for measuring the performance of visual recognition systems."}, {"heading": "B.2 Caltech256", "text": "Caltech256 2 is an object recognition database of images. It consists of about 30,000 images in 256 categories (plus clutter), with categories of car, helicopter, airplane, dog, cat, elephant, spaghetti, rifle and other everyday objects varying from left to right, making the task relatively difficult. B.3 ImageNetImageNet 3 is a hierarchically organized image database on a large scale. It consists of a total of about 14 million images in 22,000 categories. In addition, it organizes an annual computer vision contest called ILSVRC (Image Net Large Scale Visual Recognition Challenge), which consists of three separate tasks: object recognition, localization and detection.1http: / / yann.lecun.com / exdb / mnist / 2http: / / www.vision.caltech.edu / Image _ Datasets / Caltech256 / 3http: / image-net.org /"}, {"heading": "B.4 Pascal VOC 2008", "text": "This was the first publicly available data set collected by [Farhadi et al., 2010] specifically for the caption ranking task. This data set consists of 1,000 images from the object recognition challenge posed by PASCAL VOC-2008. 50 images are randomly selected, belonging to each of the 20 categories. Each image is provided with 5 descriptive captions using Amazon's Mechanical Turk, resulting in 5,000 sentences. Although it is the first specific data set, it has a number of deficiencies that limit its usefulness, the range of its images is very limited, and the captions are relatively simple and sometimes contain grammatical and spelling errors [Hodosh et al., 2013]."}, {"heading": "Appendix C", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Software", "text": "MiLDe (Machine Learning Development Environment) is a software environment for the development and prototyping of applications based on machine learning and statistics."}, {"heading": "Appendix D", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Further results of I2T and T2I", "text": ""}, {"heading": "Image Annotation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Image Search", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Kernel independent component analysis", "author": ["Bach", "Jordan", "F.R. 2003] Bach", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bach et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2003}, {"title": "Supervised semantic indexing", "author": ["Bai et al", "B. 2009] Bai", "J. Weston", "D. Grangier", "R. Collobert", "K. Sadamasa", "Y. Qi", "O. Chapelle", "K. Weinberger"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "A neural probabilistic language model", "author": ["Bengio et al", "Y. 2003] Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Sentiment classification based on supervised latent n-gram analysis", "author": ["Bespalov et al", "D. 2011] Bespalov", "B. Bai", "Y. Qi", "A. Shokoufandeh"], "venue": "In Proceedings of the 20th ACM international conference on Information and knowledge management,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston", "R. 2008] Collobert", "J. Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng et al", "J. 2009] Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Farhadi et al", "A. 2010] Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome et al", "A. 2013] Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T Mikolov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Gong et al", "Y. 2014] Gong", "L. Wang", "M. Hodosh", "J. Hockenmaier", "S. Lazebnik"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Choosing linguistics over vision to describe images", "author": ["Gupta et al", "A. 2012] Gupta", "Y. Verma", "C. Jawahar"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton et al", "G. 2006] Hinton", "S. Osindero", "Teh", "Y.-W"], "venue": "Neural computation,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Hinton et al", "G.E. 2012] Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Hodosh et al", "M. 2013] Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Receptive fields and functional architecture of monkey striate cortex", "author": ["Hubel", "Wiesel", "D.H. 1968] Hubel", "T.N. Wiesel"], "venue": "The Journal of physiology,", "citeRegEx": "Hubel et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Hubel et al\\.", "year": 1968}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Karpathy", "Fei-Fei", "A. 2014] Karpathy", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1412.2306", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Karpathy et al", "A. 2014] Karpathy", "A. Joulin", "F.F.F. Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["Kiros et al", "R. 2014] Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105", "author": ["Krizhevsky et al", "A. 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Handwritten digit recognition with a back-propagation", "author": ["Le Cun et al", "B.B. 1990] Le Cun", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1990\\E", "shortCiteRegEx": "al. et al\\.", "year": 1990}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun et al", "Y. 1998] LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}, {"title": "Comparison of learning algorithms for handwritten digit recognition", "author": ["LeCun et al", "Y. 1995] LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. Muller", "E Sackinger"], "venue": "In International conference on artificial neural networks,", "citeRegEx": "al. et al\\.,? \\Q1995\\E", "shortCiteRegEx": "al. et al\\.", "year": 1995}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Lin", "2004] Lin", "C.-Y"], "venue": "In Text Summarization Branches Out: Proceedings of the ACL-04", "citeRegEx": "Lin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin et al", "2014] Lin", "T.-Y", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Baselines for image annotation", "author": ["Makadia et al", "A. 2010] Makadia", "V. Pavlovic", "S. Kumar"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Natural language processing with modular pdp networks and distributed lexicon", "author": ["Miikkulainen", "Dyer", "R. 1991] Miikkulainen", "M.G. Dyer"], "venue": null, "citeRegEx": "Miikkulainen et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Miikkulainen et al\\.", "year": 1991}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Mikolov et al", "T. 2013a] Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al", "T. 2013b] Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Ordonez et al", "V. 2011] Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni et al", "K. 2002] Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "al. et al\\.,? \\Q2002\\E", "shortCiteRegEx": "al. et al\\.", "year": 2002}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["Rashtchian et al", "C. 2010] Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "An investigation into the validity of some metrics for automatically evaluating natural language generation systems", "author": ["Reiter", "Belz", "E. 2009] Reiter", "A. Belz"], "venue": null, "citeRegEx": "Reiter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Reiter et al\\.", "year": 2009}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet et al", "P. 2014] Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "In International Conference on Learning Representations (ICLR", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207\u2013218", "author": ["Socher et al", "R. 2014] Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Socher et al", "R. 2011] Socher", "C.C. Lin", "C. Manning", "A.Y. Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Srivastava", "Salakhutdinov", "N. 2012] Srivastava", "R.R. Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2012}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals et al", "O. 2014] Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378", "author": ["Young et al", "P. 2014] Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "[Karpathy et al., 2014] implemented their model for image and sentence mapping to compare performance against their own system.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "Unlike previous work, [Karpathy et al., 2014] go on a finer level and map fragments of images (objects) and fragments of sentences (dependency tree relations) into a common embedding space.", "startOffset": 22, "endOffset": 45}, {"referenceID": 14, "context": "[Karpathy and Fei-Fei, 2014] build upon their prior work on image text matching using fragments of image and sentence [Karpathy et al., 2014] (discussed above), add a new features to increase the modeling capacity of their model.", "startOffset": 118, "endOffset": 141}, {"referenceID": 14, "context": "Also, unlike [Karpathy et al., 2014] we don\u2019t divide our images and sentences into fragments, but treat them as whole.", "startOffset": 13, "endOffset": 36}, {"referenceID": 14, "context": "[Karpathy et al., 2014]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "Here we will follow the standard by [Karpathy et al., 2014], and this is how they report it to the best of our knowledge.", "startOffset": 36, "endOffset": 59}, {"referenceID": 14, "context": ", 2014] 5 [Karpathy et al., 2014] 6fragment alignment objective 7global ranking objective 8multi instance learning", "startOffset": 10, "endOffset": 33}, {"referenceID": 14, "context": "Besides this [Karpathy et al., 2014] use dependency tree edges to fragment their sentences, and so on.", "startOffset": 13, "endOffset": 36}, {"referenceID": 14, "context": "9Some of these models have been re-implemented by [Karpathy et al., 2014]", "startOffset": 50, "endOffset": 73}, {"referenceID": 14, "context": "We expect to achieve performance improvement by this finetuning, as reported by [Karpathy et al., 2014] and seen in the Table 6.", "startOffset": 80, "endOffset": 103}], "year": 2016, "abstractText": "The ability to describe images with natural language sentences is the hallmark for image and language understanding. Such a system has wide ranging applications such as annotating images and using natural sentences to search for images. In this project we focus on the task of bidirectional image retrieval: such a system is capable of retrieving an image based on a sentence (image search) and retrieve sentence based on an image query (image annotation). We present a system based on a global ranking objective function which uses a combination of convolutional neural networks (CNN) and multi layer perceptrons (MLP). It takes a pair of image and sentence and processes them in different channels, finally embedding it into a common multimodal vector space. These embeddings encode abstract semantic information about the two inputs and can be compared using traditional information retrieval approaches. For each such pair, the model returns a score which is interpretted as a similarity metric. If this score is high, the image and sentence are likely to convey similar meaning, and if the score is low then they are likely not to. The visual input is modeled via deep convolutional neural network. On the other hand we explore three models for the textual module. The first one is bag of words with an MLP. The second one uses n-grams (bigram, trigrams, and a combination of trigram & skip-grams) with an MLP. The third is more specialized deep network specific for modeling variable length sequences (SSE). We report comparable performance to recent work in the field, even though our overall model is simpler. We also show that the training time choice of how we can generate our negative samples has a significant impact on performance, and can be used to specialize the bi-directional system in one particular task.", "creator": "LaTeX with hyperref package"}}}