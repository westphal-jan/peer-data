{"id": "1601.03650", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2016", "title": "Smoothing parameter estimation framework for IBM word alignment models", "abstract": "IBM models are very popular word alignment models in Machine Translation. They play critical roles in the systems of this field. These models follow Maximum Likelihood principle to estimate their parameters. However, in many case, the models will be too fit the training data that may result in wrong word alignments on testing data. Smoothing is a popular solution to the overfitting problem when the causes are rare events. While this technique is very common in Language Model which is another problem in Machine Translation, there is still lack of studies for the problem of word alignment. \\cite{moore2004improving} reported a study on a simple method of additive smoothing, in which the amount to add is learnt from annotated data. This basic technique gives a significant improvement over the unsmoothed version. With such a good motivation, in this paper, we propose a more general framework by varying the amount to add rather than adding only a constant amount as the original additive smoothing. In term of learning method, we also experience a method to learn the parameter of smoothing from unannotated data with a deep analysis and comparision between different learning methods.", "histories": [["v1", "Thu, 14 Jan 2016 16:30:09 GMT  (20kb)", "https://arxiv.org/abs/1601.03650v1", null], ["v2", "Thu, 25 Feb 2016 10:48:07 GMT  (21kb)", "http://arxiv.org/abs/1601.03650v2", null], ["v3", "Mon, 14 Mar 2016 04:10:51 GMT  (29kb)", "http://arxiv.org/abs/1601.03650v3", null], ["v4", "Wed, 27 Apr 2016 04:01:48 GMT  (38kb)", "http://arxiv.org/abs/1601.03650v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vuong van bui", "cuong anh le"], "accepted": false, "id": "1601.03650"}, "pdf": {"name": "1601.03650.pdf", "metadata": {"source": "META", "title": "Smoothing parameter estimation framework for IBM word alignment models", "authors": ["Vuong Van Bui", "Anh-Cuong Le"], "emails": ["vuongbv_56@vnu.edu.vn", "leanhcuong@tdt.edu.vn"], "sections": [{"heading": null, "text": "Based on the principle of maximum likelihood estimation to estimate their parameters, the models could easily go beyond the training data for sparse data. Although smoothing is a very popular solution in the Language Model, there is still a lack of studies on word alignment smoothing. In this essay, we propose a framework that generalizes the remarkable work Moore (2004) did in applying additive smoothing to text alignment models, allowing developers to customize the smoothing amount for each word pair, scaling the additional amount accordingly by a common factor that reflects how much the framework of the addition strategy relies on data according to performance. We also carefully review different performance criteria and propose a smoothed version of error counting that generally shows the best results.Keywords: text alignment, machine translation, sparse data, smoothing stimulation, parameter optimization."}, {"heading": "1. Introduction", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "2. Related work", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3. Formal Description of IBM models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Introduction of IBM Models", "text": "IBM models are very popular among word axis models. In these models, it is assumed that every word in the target sentence is the translation of a word in the source sentence. If the target word is not the translation of a word in the source sentence, it is assumed that it is the translation of a hidden word \"NULL,\" which is at position 0 as convention in each source sentence. There are 5 versions of IBM models from IBM Model 1 to IBM Model 5. Each model is an extension of the previous model with the introduction of new parameters. IBM Model 1 is the first and also the simplest model with only one parameter. However, this parameter, the word translation probability, which is the most important parameter, is also used in all later models. A better evaluation of this parameter leads to better later models. Since this paper uses only this parameter, only related specifications of IBM Model 1 are briefly discussed. Further details on explanations, evidence and later models can be found in many other sources, such as Brown (199b)."}, {"heading": "3.2. IBM Model 1 formulation", "text": "This section briefly introduces the formal description of IBM Model 1. With a source sentence e = length l containing the words e1, e2,.. | el and the target sentence f of length m containing the words f1, f2,... fm, the word alignment a is represented as a vector of length m: < a1, a2,.., am >, in which each target means an alignment link from eaj to fj. The model with the model parameter t states that with the given source sentence e, the probability that the translation in the target language is the sentence f with the word alignment a is: p (f, a; e; t) = alignment link of ej = 1t (fj | eaj) (1) the word constant is used to normalize the correct distribution of probability. It guarantees that the probabilities of all events add up to 1."}, {"heading": "4. Parameter Estimation, The Problems and The Current Solution", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Estimating the parameters with Expectation Maximization algorithm", "text": "The parameter of the word alignment model is usually not given in advance. Instead, all we have in the training data is often just a corpus of bilingual sentence pairs. Therefore, we must determine the parameter that maximizes the probability of these sentence pairs. The first step is to formulate this probability. The probability of a sentence pair (e, f) is: p (f | e; t) = p, p (f, e) = p, p (f, e) (7) = [l + 1) mm, j = 1t (fj | eaj) (8) = (l + 1) mm, j = 1l, p (fj | ei) (9) The probability of all the pairs in the training set is the product of the probabilities of the individual pairs with an assumption of conditional independence between the pairs with the given parameters. p (f (1), f (2), f)."}, {"heading": "4.2. Problems with Maximum Likelihood Estimation", "text": "There is hardly an ideal corpus with a multitude of words, phrases and structures that appear at high frequencies and are able to cover almost all cases of languages. Due to the complicated nature of languages, it often appears at many levels of structures such as words, phrases, etc. Each level has its own complexity and effect on general thrift. In this paper, we examine only the thrift of words. We believe that this study could motivate further studies on the thrift of more complex structures. In this section, the behavior of rare words is examined. \"Rare\" words are words that very rarely occur in the corpus."}, {"heading": "4.3. Moore\u2019s additive smoothing solution", "text": "Additive smoothing, often known as laplace smoothing, is therefore a basic and basic technique of smoothing. Although it is considered a bad technique in some applications such as the Language Model, Moore (2004) reports relatively good results for word alignment. As in the Expectation Maximization Algorithm maximization step, Moore (2004) applies the same method by adding a constant factor n to each number of pairs of words (e, f), thus increasing the total number of e by an amount of | F | times the additional factor, where | F | is the size of word smoothing for the language model by adding a constant factor to each number of pairs of words (e, f)."}, {"heading": "5. Our proposed general framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Bayesian view of Moore\u2019s additive smoothing", "text": "Additive smoothing can be explained in terms of the Maximum a Posteriori Principle. In this section we briefly present such an explanation. Details of the Maximum a Posteriori Principle can be found further in Chapter 5 of Murphy (2012). The parameter of the model has a distribution te (f) = t (f | e) for each source word e. As the Maximum a Posteriori Principle, the parameter value is chosen to maximize the posterior distribution p \u2212 \u2212 data) instead of p (data | t). However, the posterior word can actually be expressed in terms of the likelihood.p (t | data) p (data | data) p (data | t) p \u2212 quantitative distribution of the parameter p \u2212 data) instead of p (data | t)."}, {"heading": "5.2. Formal Description of the framework", "text": "Adding a constant amount to the count (e, f) for each f does not seem to make much sense. In many situations, some count should get more than others. Therefore, the amount to add for each count should be different, if necessary. This can be explained in the sense of the Bayean view above that it is not necessary to set all \u03b1i the same value for the previous distribution of the dirichlet. In this section, we propose a general framework that allows us to adjust the amount we need to add with the certainty that this will hardly diminish the quality of the model.We designate G (e, f) as the amount we need to add for each word pair (e, f).This function is usually specified manually by the user. With the general function specifying the amount we need to add, we have a more general model in which the parameter is estimated as t (f | e) = Count (f, e) + G = Count (f), + (f), (f), (), (f)."}, {"heading": "6. Learning the scaling factors", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Approaches to learn the scaling factor", "text": "In this section, several approaches are presented to learn the scaling factor. We assume the existence of an additional corpus of bilingual pairs (f, e) and the word alignments a between these pairs, which are commented manually. Although the traditional maximum likelihood principle considers the number of sentences to be maximized appropriate, the probability of both f and a given e being used in terms of development (k) is low. (k) The probability that both methods will be used in terms of development is low. (k) The probability that both f and a given e will be used in terms of development (k) is low. (k) Where t (f | e) is the parameter estimated as an expectation maximization algorithm with \u03bb G (e, f) is added at the maximization stage of each iteration. However, there is another popular method based on the error number that differs from the deducted approximation."}, {"heading": "6.2. Optimizational aspects of the approaches", "text": "Most optimization algorithms prefer continuous functions, but with continuous functions, a maximum of local functions is always ensured. However, because in anticipation of maximization algorithm, each iteration only includes basic continuous operators such as multiplication, division and addition. However, the function of the most likely parameter of the model for a given number of errors is also continuous. However, the continuity of the model parameter does not always lead to the continuity of the objective function. Indeed, this depends a lot on the nature of the objective mode of operation. The method of minimizing the number of errors and maximizing the probability are completely different aspects of continuity when optimization techniques are applied. The method of minimizing the alignment error counting is discrete due to the Argmax operator. The probability is continuous due to the multiplication of only continuous quantities. This means that the optimization in terms of probability is easier than that for the alignment error counting is always continuous function.Most of the optimization algorithms are continuous only."}, {"heading": "7. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1. The adding functions to experience", "text": "Most additive strategies are usually based on heuristic methods, and in this paper we will examine them empirically; the first additional method we experience is the addition of a scaled amount of the number of times the word e appears to the number of each pair (e, f), and it has the motivation that the same adding amount for each pair could only be suitable for a small number of pairs. In estimating the probability of word translation for a very rare word, this adding amount might be too high, while it is rather too low for very popular words. Therefore, we hope that applying a scaled amount of the number of source words would increase the result. This modifies the estimate as: t (f | e) = number (f, e) + number (e) + number (e) + number of words + F | (32), where ne is the number of times in which word e occurs in the corpus. The other adding method we want to learn is the addition of a scale of the cube coefficient to calculate a pair (this is a well-known coefficient = two words)."}, {"heading": "7.2. Performance of the adding functions and the objective functions", "text": "This year, it will be able to leave the country to transform it into another world."}, {"heading": "8. Conclusion", "text": "The maximum likelihood estimation is the traditional solution for estimating parameters of word alignment models. However, many papers have shown their weakness in estimating with sparse data. Smoothing is usually considered a good choice in this case. In this paper, a general framework was proposed that allows adjusting the additive amount for each case, rather than adding a constant amount like Moore's work. Inappropriate addition strategies do not harm the model due to a mechanism for adjusting the effects that adds sums to the estimate. We have demonstrated two additional addition strategies. Although the first method, which adds a scaled amount to the number of occurrences of source words, is not adequate, the result of alignment due to the scale factor is still virtually unchanged. The second strategy, which adds a scale amount of the cube coefficient of the word pair, is better than the first because it delivers the error rate of occurrences of source words frames while still does not have the best addition strategy among the three."}, {"heading": "Acknowledgements", "text": "This work is supported by the Nafosted project 102.012014.22."}], "references": [{"title": "Algorithms for minimization without derivatives", "author": ["R.P. Brent"], "venue": "Courier Corporation", "citeRegEx": "Brent,? \\Q2013\\E", "shortCiteRegEx": "Brent", "year": 2013}, {"title": "But dictionaries are data too", "author": ["P.F. Brown", "S.A. Della Pietra", "V.J. Della Pietra", "M.J. Goldsmith", "J. Hajic", "R.L. Mercer", "S. Mohanty"], "venue": "In Proceedings of the workshop on Human Language Technology,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "R.L. Mercer"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "Chen and Goodman,? \\Q1999\\E", "shortCiteRegEx": "Chen and Goodman", "year": 1999}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the royal statistical society. Series B (methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "A bit of progress in language modeling", "author": ["J.T. Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "Goodman,? \\Q2001\\E", "shortCiteRegEx": "Goodman", "year": 2001}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn,? \\Q2005\\E", "shortCiteRegEx": "Koehn", "year": 2005}, {"title": "Factored translation models", "author": ["P. Koehn", "H. Hoang"], "venue": "In EMNLPCoNLL,", "citeRegEx": "Koehn and Hoang,? \\Q2007\\E", "shortCiteRegEx": "Koehn and Hoang", "year": 2007}, {"title": "Guidelines for word alignment evaluation and manual alignment", "author": ["P. Lambert", "A. De Gispert", "R. Banchs", "J.B. Mari\u00f1o"], "venue": "Language Resources and Evaluation,", "citeRegEx": "Lambert et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lambert et al\\.", "year": 2005}, {"title": "Morphological analysis for statistical machine translation", "author": ["Lee", "Y.-S"], "venue": "In Proceedings of HLT-NAACL 2004: Short Papers,", "citeRegEx": "Lee and Y..S.,? \\Q2004\\E", "shortCiteRegEx": "Lee and Y..S.", "year": 2004}, {"title": "Improving ibm word-alignment model 1", "author": ["R.C. Moore"], "venue": "In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Moore,? \\Q2004\\E", "shortCiteRegEx": "Moore", "year": 2004}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "Murphy,? \\Q2012\\E", "shortCiteRegEx": "Murphy", "year": 2012}, {"title": "An efficient method for determining bilingual word classes. In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, pages 71\u201376", "author": ["F.J. Och"], "venue": null, "citeRegEx": "Och,? \\Q1999\\E", "shortCiteRegEx": "Och", "year": 1999}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F.J. Och"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Och,? \\Q2003\\E", "shortCiteRegEx": "Och", "year": 2003}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney"], "venue": "Computational linguistics,", "citeRegEx": "Och and Ney,? \\Q2003\\E", "shortCiteRegEx": "Och and Ney", "year": 2003}, {"title": "Combination of arabic preprocessing schemes for statistical machine translation", "author": ["F. Sadat", "N. Habash"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,", "citeRegEx": "Sadat and Habash,? \\Q2006\\E", "shortCiteRegEx": "Sadat and Habash", "year": 2006}, {"title": "Extensions to hmmbased statistical word alignment models", "author": ["K. Toutanova", "H.T. Ilhan", "C.D. Manning"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "Toutanova et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2002}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 10, "context": "In this paper, we propose a framework which generalizes the notable work Moore (2004) of applying additive smoothing to word alignment models.", "startOffset": 73, "endOffset": 86}, {"referenceID": 1, "context": "IBM Models presented in Brown et al. (1993b) are currently the most popular word alignment models.", "startOffset": 24, "endOffset": 45}, {"referenceID": 1, "context": "IBM Models presented in Brown et al. (1993b) are currently the most popular word alignment models. Based on the Maximum Likelihood Estimation principle, the parameters of IBM Models are estimated from training data with the Expectation Maximization algorithm presented in Dempster et al. (1977). This specialized algorithm is applied to determine the local maximum likelihood estimation by considering the word alignment unobserved variables of training data.", "startOffset": 24, "endOffset": 295}, {"referenceID": 1, "context": "IBM Models presented in Brown et al. (1993b) are currently the most popular word alignment models. Based on the Maximum Likelihood Estimation principle, the parameters of IBM Models are estimated from training data with the Expectation Maximization algorithm presented in Dempster et al. (1977). This specialized algorithm is applied to determine the local maximum likelihood estimation by considering the word alignment unobserved variables of training data. However, overfitting, a common problem in Machine Learning, occurs in word alignment models frequently. This issue appears when the model fits the training data too well but performs poorly on the testing data. The Maximum Likelihood estimation makes the parameters \u201cagree\u201d as much as possible with the training data and nothing else. This is not always appropriate, especially when there are not sufficient training data to obtain a reliable estimation. Even thousands of sentence pairs could still contain many rare structures due to the complexity of the languages. Therefore, the right solution should not rely completely on the training data. Before observing the data, we normally do have some prior knowledge of the languages. Integrating these features usually help reducing the problems caused by sparse data. Many methods have been developed to deal with sparse data. Bilingual dictionaries together with many other sorts of linguistic analysis such as morphological analysis, syntactic analysis have been well investigated in Brown et al. (1993a), Koehn and Hoang (2007) ,Sadat and Habash (2006) and Lee (2004).", "startOffset": 24, "endOffset": 1518}, {"referenceID": 1, "context": "IBM Models presented in Brown et al. (1993b) are currently the most popular word alignment models. Based on the Maximum Likelihood Estimation principle, the parameters of IBM Models are estimated from training data with the Expectation Maximization algorithm presented in Dempster et al. (1977). This specialized algorithm is applied to determine the local maximum likelihood estimation by considering the word alignment unobserved variables of training data. However, overfitting, a common problem in Machine Learning, occurs in word alignment models frequently. This issue appears when the model fits the training data too well but performs poorly on the testing data. The Maximum Likelihood estimation makes the parameters \u201cagree\u201d as much as possible with the training data and nothing else. This is not always appropriate, especially when there are not sufficient training data to obtain a reliable estimation. Even thousands of sentence pairs could still contain many rare structures due to the complexity of the languages. Therefore, the right solution should not rely completely on the training data. Before observing the data, we normally do have some prior knowledge of the languages. Integrating these features usually help reducing the problems caused by sparse data. Many methods have been developed to deal with sparse data. Bilingual dictionaries together with many other sorts of linguistic analysis such as morphological analysis, syntactic analysis have been well investigated in Brown et al. (1993a), Koehn and Hoang (2007) ,Sadat and Habash (2006) and Lee (2004).", "startOffset": 24, "endOffset": 1542}, {"referenceID": 1, "context": "IBM Models presented in Brown et al. (1993b) are currently the most popular word alignment models. Based on the Maximum Likelihood Estimation principle, the parameters of IBM Models are estimated from training data with the Expectation Maximization algorithm presented in Dempster et al. (1977). This specialized algorithm is applied to determine the local maximum likelihood estimation by considering the word alignment unobserved variables of training data. However, overfitting, a common problem in Machine Learning, occurs in word alignment models frequently. This issue appears when the model fits the training data too well but performs poorly on the testing data. The Maximum Likelihood estimation makes the parameters \u201cagree\u201d as much as possible with the training data and nothing else. This is not always appropriate, especially when there are not sufficient training data to obtain a reliable estimation. Even thousands of sentence pairs could still contain many rare structures due to the complexity of the languages. Therefore, the right solution should not rely completely on the training data. Before observing the data, we normally do have some prior knowledge of the languages. Integrating these features usually help reducing the problems caused by sparse data. Many methods have been developed to deal with sparse data. Bilingual dictionaries together with many other sorts of linguistic analysis such as morphological analysis, syntactic analysis have been well investigated in Brown et al. (1993a), Koehn and Hoang (2007) ,Sadat and Habash (2006) and Lee (2004).", "startOffset": 24, "endOffset": 1567}, {"referenceID": 1, "context": "IBM Models presented in Brown et al. (1993b) are currently the most popular word alignment models. Based on the Maximum Likelihood Estimation principle, the parameters of IBM Models are estimated from training data with the Expectation Maximization algorithm presented in Dempster et al. (1977). This specialized algorithm is applied to determine the local maximum likelihood estimation by considering the word alignment unobserved variables of training data. However, overfitting, a common problem in Machine Learning, occurs in word alignment models frequently. This issue appears when the model fits the training data too well but performs poorly on the testing data. The Maximum Likelihood estimation makes the parameters \u201cagree\u201d as much as possible with the training data and nothing else. This is not always appropriate, especially when there are not sufficient training data to obtain a reliable estimation. Even thousands of sentence pairs could still contain many rare structures due to the complexity of the languages. Therefore, the right solution should not rely completely on the training data. Before observing the data, we normally do have some prior knowledge of the languages. Integrating these features usually help reducing the problems caused by sparse data. Many methods have been developed to deal with sparse data. Bilingual dictionaries together with many other sorts of linguistic analysis such as morphological analysis, syntactic analysis have been well investigated in Brown et al. (1993a), Koehn and Hoang (2007) ,Sadat and Habash (2006) and Lee (2004). Although these approaches have many good behaviors in experiments, applying one known method of a language pair for another language pair is usually difficult due to its language dependencies.", "startOffset": 24, "endOffset": 1582}, {"referenceID": 1, "context": "IBM Models presented in Brown et al. (1993b) are currently the most popular word alignment models. Based on the Maximum Likelihood Estimation principle, the parameters of IBM Models are estimated from training data with the Expectation Maximization algorithm presented in Dempster et al. (1977). This specialized algorithm is applied to determine the local maximum likelihood estimation by considering the word alignment unobserved variables of training data. However, overfitting, a common problem in Machine Learning, occurs in word alignment models frequently. This issue appears when the model fits the training data too well but performs poorly on the testing data. The Maximum Likelihood estimation makes the parameters \u201cagree\u201d as much as possible with the training data and nothing else. This is not always appropriate, especially when there are not sufficient training data to obtain a reliable estimation. Even thousands of sentence pairs could still contain many rare structures due to the complexity of the languages. Therefore, the right solution should not rely completely on the training data. Before observing the data, we normally do have some prior knowledge of the languages. Integrating these features usually help reducing the problems caused by sparse data. Many methods have been developed to deal with sparse data. Bilingual dictionaries together with many other sorts of linguistic analysis such as morphological analysis, syntactic analysis have been well investigated in Brown et al. (1993a), Koehn and Hoang (2007) ,Sadat and Habash (2006) and Lee (2004). Although these approaches have many good behaviors in experiments, applying one known method of a language pair for another language pair is usually difficult due to its language dependencies. Language Model, which is another well known problem in Machine Translation, has sparse data as the main issue to deal with. The task of Language Model is to estimate how likely a sentence is produced by speakers in which the training data is hardly able to cover all cases. A method called \u201csmoothing\u201d is a very popular solution to the issue Chen and Goodman (1999), Goodman (2001).", "startOffset": 24, "endOffset": 2142}, {"referenceID": 1, "context": "IBM Models presented in Brown et al. (1993b) are currently the most popular word alignment models. Based on the Maximum Likelihood Estimation principle, the parameters of IBM Models are estimated from training data with the Expectation Maximization algorithm presented in Dempster et al. (1977). This specialized algorithm is applied to determine the local maximum likelihood estimation by considering the word alignment unobserved variables of training data. However, overfitting, a common problem in Machine Learning, occurs in word alignment models frequently. This issue appears when the model fits the training data too well but performs poorly on the testing data. The Maximum Likelihood estimation makes the parameters \u201cagree\u201d as much as possible with the training data and nothing else. This is not always appropriate, especially when there are not sufficient training data to obtain a reliable estimation. Even thousands of sentence pairs could still contain many rare structures due to the complexity of the languages. Therefore, the right solution should not rely completely on the training data. Before observing the data, we normally do have some prior knowledge of the languages. Integrating these features usually help reducing the problems caused by sparse data. Many methods have been developed to deal with sparse data. Bilingual dictionaries together with many other sorts of linguistic analysis such as morphological analysis, syntactic analysis have been well investigated in Brown et al. (1993a), Koehn and Hoang (2007) ,Sadat and Habash (2006) and Lee (2004). Although these approaches have many good behaviors in experiments, applying one known method of a language pair for another language pair is usually difficult due to its language dependencies. Language Model, which is another well known problem in Machine Translation, has sparse data as the main issue to deal with. The task of Language Model is to estimate how likely a sentence is produced by speakers in which the training data is hardly able to cover all cases. A method called \u201csmoothing\u201d is a very popular solution to the issue Chen and Goodman (1999), Goodman (2001). The idea is that, when estimating a probability, we will give a little mass to events that do not occur in the training data.", "startOffset": 24, "endOffset": 2158}, {"referenceID": 1, "context": "IBM Models presented in Brown et al. (1993b) are currently the most popular word alignment models. Based on the Maximum Likelihood Estimation principle, the parameters of IBM Models are estimated from training data with the Expectation Maximization algorithm presented in Dempster et al. (1977). This specialized algorithm is applied to determine the local maximum likelihood estimation by considering the word alignment unobserved variables of training data. However, overfitting, a common problem in Machine Learning, occurs in word alignment models frequently. This issue appears when the model fits the training data too well but performs poorly on the testing data. The Maximum Likelihood estimation makes the parameters \u201cagree\u201d as much as possible with the training data and nothing else. This is not always appropriate, especially when there are not sufficient training data to obtain a reliable estimation. Even thousands of sentence pairs could still contain many rare structures due to the complexity of the languages. Therefore, the right solution should not rely completely on the training data. Before observing the data, we normally do have some prior knowledge of the languages. Integrating these features usually help reducing the problems caused by sparse data. Many methods have been developed to deal with sparse data. Bilingual dictionaries together with many other sorts of linguistic analysis such as morphological analysis, syntactic analysis have been well investigated in Brown et al. (1993a), Koehn and Hoang (2007) ,Sadat and Habash (2006) and Lee (2004). Although these approaches have many good behaviors in experiments, applying one known method of a language pair for another language pair is usually difficult due to its language dependencies. Language Model, which is another well known problem in Machine Translation, has sparse data as the main issue to deal with. The task of Language Model is to estimate how likely a sentence is produced by speakers in which the training data is hardly able to cover all cases. A method called \u201csmoothing\u201d is a very popular solution to the issue Chen and Goodman (1999), Goodman (2001). The idea is that, when estimating a probability, we will give a little mass to events that do not occur in the training data. Although the smoothened model is not the strongest one to \u201cagree\u201d with the training data, it will perform better on the testing data than the un-smoothened model. In spite of being a very popular technique to deal with rare events, there is still lack of studies of applying smoothing to the word alignment problem. To the best of our knowledge, the only study on this subject is the work by Moore Moore (2004). A basic additive smoothing was utilized for the parameter of", "startOffset": 24, "endOffset": 2696}, {"referenceID": 1, "context": "For word alignment, the instance of the rare word problem is studied in Brown et al. (1993a) and Moore (2004).", "startOffset": 72, "endOffset": 93}, {"referenceID": 1, "context": "For word alignment, the instance of the rare word problem is studied in Brown et al. (1993a) and Moore (2004). In these papers, rare words act as \u201cgarbage collectors\u201d that tend to align to too many target words.", "startOffset": 72, "endOffset": 110}, {"referenceID": 1, "context": "For word alignment, the instance of the rare word problem is studied in Brown et al. (1993a) and Moore (2004). In these papers, rare words act as \u201cgarbage collectors\u201d that tend to align to too many target words. To deal with rare word problems, many researches utilized the linguistic information. One of the earliest works is Brown et al. (1993a) which used an external dictionary to improve the word alignment models.", "startOffset": 72, "endOffset": 348}, {"referenceID": 1, "context": "For word alignment, the instance of the rare word problem is studied in Brown et al. (1993a) and Moore (2004). In these papers, rare words act as \u201cgarbage collectors\u201d that tend to align to too many target words. To deal with rare word problems, many researches utilized the linguistic information. One of the earliest works is Brown et al. (1993a) which used an external dictionary to improve the word alignment models. Experiments show that this method also solves the problem of rare words. Another approach utilized the information provided by morphological analysis. Some of them are Koehn and Hoang (2007), Sadat and Habash (2006), Lee (2004).", "startOffset": 72, "endOffset": 611}, {"referenceID": 1, "context": "For word alignment, the instance of the rare word problem is studied in Brown et al. (1993a) and Moore (2004). In these papers, rare words act as \u201cgarbage collectors\u201d that tend to align to too many target words. To deal with rare word problems, many researches utilized the linguistic information. One of the earliest works is Brown et al. (1993a) which used an external dictionary to improve the word alignment models. Experiments show that this method also solves the problem of rare words. Another approach utilized the information provided by morphological analysis. Some of them are Koehn and Hoang (2007), Sadat and Habash (2006), Lee (2004).", "startOffset": 72, "endOffset": 636}, {"referenceID": 1, "context": "For word alignment, the instance of the rare word problem is studied in Brown et al. (1993a) and Moore (2004). In these papers, rare words act as \u201cgarbage collectors\u201d that tend to align to too many target words. To deal with rare word problems, many researches utilized the linguistic information. One of the earliest works is Brown et al. (1993a) which used an external dictionary to improve the word alignment models. Experiments show that this method also solves the problem of rare words. Another approach utilized the information provided by morphological analysis. Some of them are Koehn and Hoang (2007), Sadat and Habash (2006), Lee (2004). These works do not treat word as the smallest unit of translation.", "startOffset": 72, "endOffset": 648}, {"referenceID": 1, "context": "IBM Model 4 (Brown et al. (1993b)) constrains word distortion on the class of source words.", "startOffset": 13, "endOffset": 34}, {"referenceID": 1, "context": "IBM Model 4 (Brown et al. (1993b)) constrains word distortion on the class of source words. Distortion indicates how likely two words are translations of each other based on theirs positions. A better distortion estimation would result in a better alignment. Another work from Toutanova et al. (2002) utilized the word classes of both source words and target words.", "startOffset": 13, "endOffset": 301}, {"referenceID": 1, "context": "IBM Model 4 (Brown et al. (1993b)) constrains word distortion on the class of source words. Distortion indicates how likely two words are translations of each other based on theirs positions. A better distortion estimation would result in a better alignment. Another work from Toutanova et al. (2002) utilized the word classes of both source words and target words. It estimated the translation probability of pairs of classes as well as the word translation probability. This class translation probability is usually more reliable than the word translation probability. Aligning will be better, especially in the case of rare words when it encourages alignments to follow the class translation probability. Word classes may be part-of-speech tags which is usually obtained by running part-of-speech tagging software (such as the Stanford tagger in Toutanova et al. (2003)) or more usually the classes obtained by running a clustering algorithm which is language independent as in Och (1999).", "startOffset": 13, "endOffset": 873}, {"referenceID": 1, "context": "IBM Model 4 (Brown et al. (1993b)) constrains word distortion on the class of source words. Distortion indicates how likely two words are translations of each other based on theirs positions. A better distortion estimation would result in a better alignment. Another work from Toutanova et al. (2002) utilized the word classes of both source words and target words. It estimated the translation probability of pairs of classes as well as the word translation probability. This class translation probability is usually more reliable than the word translation probability. Aligning will be better, especially in the case of rare words when it encourages alignments to follow the class translation probability. Word classes may be part-of-speech tags which is usually obtained by running part-of-speech tagging software (such as the Stanford tagger in Toutanova et al. (2003)) or more usually the classes obtained by running a clustering algorithm which is language independent as in Och (1999). Smoothing is a popular technique to solve the problem of sparsity.", "startOffset": 13, "endOffset": 992}, {"referenceID": 1, "context": "IBM Model 4 (Brown et al. (1993b)) constrains word distortion on the class of source words. Distortion indicates how likely two words are translations of each other based on theirs positions. A better distortion estimation would result in a better alignment. Another work from Toutanova et al. (2002) utilized the word classes of both source words and target words. It estimated the translation probability of pairs of classes as well as the word translation probability. This class translation probability is usually more reliable than the word translation probability. Aligning will be better, especially in the case of rare words when it encourages alignments to follow the class translation probability. Word classes may be part-of-speech tags which is usually obtained by running part-of-speech tagging software (such as the Stanford tagger in Toutanova et al. (2003)) or more usually the classes obtained by running a clustering algorithm which is language independent as in Och (1999). Smoothing is a popular technique to solve the problem of sparsity. Language Model, which is another problem of Machine Translation, has a variety of smoothing methods. The classic paper Chen and Goodman (1999) gives a very good study of this solution for the Language Model problem.", "startOffset": 13, "endOffset": 1203}, {"referenceID": 1, "context": "IBM Model 4 (Brown et al. (1993b)) constrains word distortion on the class of source words. Distortion indicates how likely two words are translations of each other based on theirs positions. A better distortion estimation would result in a better alignment. Another work from Toutanova et al. (2002) utilized the word classes of both source words and target words. It estimated the translation probability of pairs of classes as well as the word translation probability. This class translation probability is usually more reliable than the word translation probability. Aligning will be better, especially in the case of rare words when it encourages alignments to follow the class translation probability. Word classes may be part-of-speech tags which is usually obtained by running part-of-speech tagging software (such as the Stanford tagger in Toutanova et al. (2003)) or more usually the classes obtained by running a clustering algorithm which is language independent as in Och (1999). Smoothing is a popular technique to solve the problem of sparsity. Language Model, which is another problem of Machine Translation, has a variety of smoothing methods. The classic paper Chen and Goodman (1999) gives a very good study of this solution for the Language Model problem. A large number of smoothing methods with extensive comparisons amongst them will be analyzed carefully in the paper. However, there is still lack of studies for applying smoothing techniques to the word alignment problem. The work of this paper is mostly an extended work of Moore (2004) which is the earliest study for this matter.", "startOffset": 13, "endOffset": 1564}, {"referenceID": 1, "context": "More details of explanations, proofs and the later models can be found in many other sources such as the classic paper Brown et al. (1993b)", "startOffset": 119, "endOffset": 140}, {"referenceID": 4, "context": "There is, instead, an iterative algorithm, Expectation Maximization algorithm (Dempster et al. (1977)), which is suitable for this particular kind of problem.", "startOffset": 79, "endOffset": 102}, {"referenceID": 8, "context": "This is also explained in Moore (2004) and particular examples of this behavior can be found in Brown et al.", "startOffset": 26, "endOffset": 39}, {"referenceID": 1, "context": "This is also explained in Moore (2004) and particular examples of this behavior can be found in Brown et al. (1993a). The overfitting is reflected in the action of estimating the new parameter merely from the expected count of links between words.", "startOffset": 96, "endOffset": 117}, {"referenceID": 10, "context": "Although it is considered as a poor technique in some applications like Language Model, reasonably good results for word alignment are reported in Moore (2004). As in the maximization step of Expectation Maximization algorithm, the maximum likelihood estimation of the word translation probability is:", "startOffset": 147, "endOffset": 160}, {"referenceID": 10, "context": "Employing ideas from Laplace smoothing for Language Model, Moore (2004) applied the same method by adding a constant factor n to every count of a word pair (e, f).", "startOffset": 59, "endOffset": 72}, {"referenceID": 11, "context": "Details of Maximum a Posteriori principle can be further found in chapter 5 of Murphy (2012). The parameter of the model has one distribution te(f) = t(f | e) for each source word e.", "startOffset": 79, "endOffset": 93}, {"referenceID": 0, "context": "Brent algorithm (Brent (2013)) is considered to be an appropriate solution for functions of a single variable as in our case.", "startOffset": 0, "endOffset": 30}, {"referenceID": 0, "context": "Brent algorithm (Brent (2013)) is considered to be an appropriate solution for functions of a single variable as in our case. Although the algorithm do not require the functions to be continuous, the performance will not be good in these cases with the same sort of pitfall caused by gaps. There is another solution for this problem that we can smooth the objective function. The work Och (2003) gives a very useful study case when learning parameters for the phrase-based translation models.", "startOffset": 0, "endOffset": 396}, {"referenceID": 12, "context": "Although word alignment models merely based on dice coefficient is inefficient (Och and Ney (2003)), adjusting the baseline parameter estimation by an appropriate time of this amount could be an improvement.", "startOffset": 80, "endOffset": 99}, {"referenceID": 6, "context": "We have experiments on the Europarl corpora (Koehn (2005)) of GermanEnglish and English-Spanish.", "startOffset": 45, "endOffset": 58}, {"referenceID": 6, "context": "We have experiments on the Europarl corpora (Koehn (2005)) of GermanEnglish and English-Spanish. We extract 100,000 bilingual sentence pairs from each corpus. For the German-English corpus, we have 150 annotated sentence pairs. For the English-Spanish corpus, the work in Lambert et al. (2005) gives us 500 annotated sentence pairs.", "startOffset": 45, "endOffset": 294}, {"referenceID": 12, "context": "This sort of annotated data is for Alignment Error Rate (AER) evaluation as in Och and Ney (2003). Denote the set of sure links S, the set of possible links P , and the set of links are decided by the word alignment model A, we have an adaptation for the common metric: precision and recall.", "startOffset": 79, "endOffset": 98}], "year": 2016, "abstractText": "IBM models are important word alignment models in Machine Translation. Based on the Maximum Likelihood Estimation principle to estimate their parameters, the models could easily overfit training data when data are sparse. Even though smoothing is a very popular solution in Language Model, there is still a lack of studies on smoothing for word alignment. In this paper, we propose a framework which generalizes the notable work Moore (2004) of applying additive smoothing to word alignment models. The framework allows developers to customize the smoothing amount for each pair of words. The added amount will be scaled appropriately by a common factor which reflects how much the framework trusts the adding strategy according to the performance on data. We also carefully examine various performance criteria and propose a smoothened version of the error count, which generally gives the best result.", "creator": "LaTeX with hyperref package"}}}