{"id": "1601.03651", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2016", "title": "Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation", "abstract": "Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent, compared with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolution neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluate our DRNNs on the SemEval-2010 Task 8, and achieve an $F_1$-score of 85.81%, outperforming state-of-the-art recorded results.", "histories": [["v1", "Thu, 14 Jan 2016 16:30:41 GMT  (279kb,D)", "http://arxiv.org/abs/1601.03651v1", null], ["v2", "Thu, 13 Oct 2016 07:11:46 GMT  (690kb,D)", "http://arxiv.org/abs/1601.03651v2", "Accepted by COLING-16"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["yan xu", "ran jia", "lili mou", "ge li", "yunchuan chen", "yangyang lu", "zhi jin"], "accepted": false, "id": "1601.03651"}, "pdf": {"name": "1601.03651.pdf", "metadata": {"source": "CRF", "title": "Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation", "authors": ["Yan Xu", "Ran Jia", "Lili Mou", "Ge Li", "Yunchuan Chen", "Yangyang Lu", "Zhi Jin"], "emails": ["jiaran1994@gmail.com", "doublepower.mou@gmail.com", "chenyunchuan11@mails.ucas.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "The classification of relationships between two entities in a given context is an important task in natural language processing (NLP). Let's take the following sentence as an example: \"Jewelry and other minor [valuables] e1 were locked in a [safe] e2 or a closet with a dead bar.\" The marked entities valuables and safe are from relation content containers (e1, e2). Relation classification plays a key role in various NLP applications and has become a hot research topic in recent years. Today, neural network-based approaches have made significant improvements in relation to equivalence, compared with traditional methods based on either human-designed traits (Kambhatla, 2004; Hendrickx et al., 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013). Zeng et al. (2014) and Xu et al."}, {"heading": "2 Related Work", "text": "Traditional methods for classifying relationships are mainly divided into two groups: feature-based or kernel-based. The former approaches extract different types of traits and feed them into a classifier. Kambhatla (2004) uses a maximum entropy model for trait combinations. Hendrickx et al. (2009) collects various traits, including lexical, syntactic, and semantic traits. In contrast, kernel-based methods do not have explicit trait representations, but require a predefined measure of similarity between two data samples. Bunescu and Mooney (2005) design a kernel along the shortest dependency path (CNDP) between two units, observing that the relationship relies heavily on SDPs. Plank and Musk (2013) combine structural information and semantic information in a tree core. Today, neural networks play an important role in this task."}, {"heading": "3 The Proposed Methodology", "text": "In this section we describe our methodology in detail. Section 3.1 provides an overall picture of our DRNN model. Section 3.2 and 3.3 describe deep recurrent neural networks. The proposed method of data multiplication is presented in Section 3.4. Finally, we present our educational goal in Section 3.5."}, {"heading": "3.1 Overview", "text": "Figure 2 shows the overall architecture of the DRNN model. In view of a sentence and its dependency parse tree, we follow 1 Xu et al. (2015b) and build our neural network on the shortest dependency path (SDP), which serves as the backbone. Specifically, an RNN collects information along each subpath, separated by the common ancestor of marked units. In addition, we use four information channels, namely word embeddings, POS1We use the Stanford parser (de Marneffe et al., 2006).Embeddings, grammatical embeddings, and WordNet embeddings. Unlike Xu et al. (2015b), we design deep RNNNNs with up to four hidden layers. These layers are stacked hierarchically to capture information at different levels of abstraction."}, {"heading": "3.2 Recurrent Neural Networks on Shortest Dependency Path", "text": "Compared to a raw word sequence or a whole parse tree, the shortest dependency path (SDP) between two entities has two main advantages. First, it reduces irrelevant information; second, grammatical relationships between words focus on the action and the agents in a sentence that is, of course, suitable for classifying relationships. Existing studies have shown the effectiveness of SDP (Ebrahimi and Dou, 2015; Liu et al., 2015; Xu et al., 2015b; Xu et al., 2015a); details are repeated here.Focused on the SDP, an RNN holds a hidden state vector h, which changes accordingly with each step of the input word. Strictly speaking, the hidden state is ht ht, for the t-th word in the subpath, recursively \u2212 and the current word embedding trxt."}, {"heading": "3.3 Deep Recurrent Neural Networks", "text": "Although an RNN, as described above, is, due to its iterative nature, capable of gathering information along a sequence (a substratum of our task), the machine learning community suggests that deep architectures may be better able to integrate information and capture different levels of abstraction. In this sense, single-layer RNNNs are actually flat in information processing (Hermans and Schrauwen, 2013; Irsoy and Cardie, 2014). In the relation classification task, words along SDPs provide information from different perspectives. In this sense, the labeled entities themselves are informative. On the other hand, the common ancestors of the entities (typically verbs) say how the two entities relate to each other. \u2212 Such heterogeneous information may require more complexity than a single layer that we hid Ni-layer in another layer."}, {"heading": "3.4 Data Augmentation", "text": "The data set of the relation classification SemEval-2010, which we use, contains only several thousand samples, which may not be fully suitable for the formation of deep RNN. To alleviate this problem, we propose a data extension technology for classifying relationships by using the direction level of relationships. The two partial paths [valuables] e1 \u2192 jewelry \u2192 locked in closet \u2190 [safe] e2in Figure 1, for example, can be assigned to the subject predicate and the object predicate components in the relationship content container (e1, e2). If we change the order of these two partial paths, we get [safe] e1 \u2192 closet \u2192 in \u2192 locked jewelry \u2190 [valuables] e2Then the relationship to container content (e1, e2), which is exactly the reverse of the context container (e1, e2)."}, {"heading": "3.5 Training Objective", "text": "For each recurring layer and embedding layer (above each sub-path for each channel), we apply a maximum pooling layer to collect information. In total, we have 40 pools that are concatenated for information integration and fed to a hidden layer. Finally, a Softmax layer gives the estimated probability that two sub-paths (sleft and sright) of relation r. For a single data sample i, we apply the standard entropy loss known as J (slefti, s right i, ri).With the data augmentation technology, our general training target isJ = m \u00b2 i = 1 J (slefti, s right i, ri) + J (s right i, s left i, r \u2212 1) + 1 \u00b2 W \u00b2 F, where r \u2212 1 refers to the reversal of the relationship r. m, is the number of data samples in the original training unit."}, {"heading": "4 Experiments", "text": "In this section we present our experiments in detail. Section 4.1 introduces the data set; Section 4.2 describes hyperparameter settings. We discuss data augmentation in Section 4.3 and rational use of RNNs in Section 4.4. Section 4.5 compares our DRNNs model with other methods in the literature. In Section 4.6 we have quantitative and qualitative analyses of how depth affects our model."}, {"heading": "4.1 Dataset", "text": "We evaluated our DRNs model based on the SemEval2010 Task 8 dataset, which is an established benchmark for classifying relations (Hendrickx et al., 2009).The dataset contains 8000 sets for training and 2717 for testing. We divided 800 samples from the training set for validation, which distinguishes 10 relationships as follows: \u2022 cause-effect \u2022 component totality \u2022 content containers \u2022 entity-destination \u2022 entity-origin \u2022 message-topic \u2022 member-collection \u2022 instrument-agency \u2022 product-producer \u2022 other The previous 9 relationships are directed, while the other class is undirected. In total, we have 19 different classes."}, {"heading": "4.2 Hyperparameter Setting", "text": "In this subsection, we present hyperparameter settings of our proposed model. To make a fair comparison, we have essentially followed the settings in Xu et al. (2015b). Word embedding was 200-dimensional and pre-trained with word2vec (Mikolov et al., 2013) on the Wikipedia corpus; embedding into other channels was randomly 50-dimensionally initialized; the hidden layers in each channel had the same number of units as their embedding (either 200 or 50); the penultimate hidden layer was 100-dimensional. A \"2 penalty of 10 \u2212 5 was also applied as in Xu et al. (2015b), but we chose the dropout rate by validation 2 implementation based on Mou et al. (2015).with a granularity of 5% for our model variants (with different depths).We also selected the depth of DR4 \u00b7 \u00b7 N2 by DR1, and DR5 by the highest validation (DR1)."}, {"heading": "4.3 Data Augmentation Details", "text": "As mentioned in Section 4.1, the dataset of Task 8 of SemEval-2010 contains an undirected class Other in addition to 9 directed relationships (18 classes). To increase data size, it is natural that the reverse other relationship also exists in the Other Class itself. However, if we increase all relationships, we observe a performance deterioration of 0.4% (Table 2). We assume that the Other Class mainly contains noise and is detrimental to our model. Then, we conducted another experiment in which we only increase the Other Class. The result confirms our assumption, since we obtained an even greater deterioration of 0.8% in this setting. Pilot experiments suggest that we should consider unfavorable noise when performing the data augmentation. If we only reverse the directed relationships and leave the Other Class intact, performance will be improved by a large distance of 1.7%. This shows that our proposed data augmentation technology contributes to mitigating the problem of data feebleness if we do not mitigate the impact of the data spareness."}, {"heading": "4.4 RNNs v.s. CNNs", "text": "Since both RNNs and CNNs are dominant neural models for NLP, we are curious to see if deep architecture is also beneficial for CNNs. We tried a CNN with a size 3 sliding window like Xu et al. (2015a); other settings were similar to our DRNs. The result is in Table 3. We observe that a single layer of CNN is also effective and yields an F1 score that is slightly worse than our RNN. But the deep architecture harms the performance of CNNs in this task. We provide a plausible explanation as follows: When a folding is performed, the beginning and end of a set are typically padded with a special symbol or simply zero. However, the shortest path of dependence between two entities is usually not very long (on average 4). Therefore, set boundaries can play a large role in folding, making CNNs vulnerable. On the contrary, RNNNs can deal with set boundaries gently, and the performance increases from 4.6 to 4.6."}, {"heading": "4.5 Results", "text": "Table 4 compares our DRNs model with other state-of-the-art methods. The first entry in the table represents the highest performance achieved by traditional, function-based methods. Hendrickx et al. (2009) feeds a variety of handcrafted functions into the SVM classifier and achieves an F1 score of 82.2%. Recent performance improvements on this data set are largely achieved using neural networks. Socher et al. (2012) build a recursive neural network on the constituency tree and achieve a comparable performance with Hendrickx et al. (2009). Further, they expand their recursive network with matrix-vector interaction and increase the F1score model to 82.4%. Ebrahimi and Dou (2015) restrict the network to SDP, which is slightly better than a sentence-wide recursive network. Xu et al al al. (2015b) initially introduce a type of grent neural network (M)."}, {"heading": "4.6 Analysis of DRNNs\u2019 Depth", "text": "In this section, we analyze the impact of depth in our DRNs models. We have tested the depth of the set (1, 2, 4, 5), and we have the result in Figure 3. As we can see, performance has increased from Depth 2 to Depth 4, and we have reached the highest point in Depth 4. Performance of Depth 5 has a slight deterioration, but it is still at a relatively high level. This is achieved through efficiency and modeling complexity, we have not tried to find deeper models (e.g. 6), and we have decided how RNN units in different layers are related to each other to fulfill the ultimate task of interest. This is achieved by tracing information from pooling layers."}, {"heading": "5 Conclusion", "text": "The DRNNs model, which consists of multiple layers of RNN, examines the representation space of different levels of abstraction. By visualizing the DRNNs units, we showed that high-level layers are able to integrate information relevant to target relationships. In addition, we have developed a data augmentation strategy using the directional effects of relations. When our DRNNs model is evaluated on the SemEval dataset, this results in a significant increase in performance. The F1 score generally improves with increasing depth and reaches its highest point when the depth is 4. Our result also suggests potential future directions for classifying neural networks: building deep architectures and enriching datasets."}], "references": [{"title": "A shortest path dependency kernel for relation extraction", "author": ["Razvan C. Bunescu", "Raymond J. Mooney."], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 724\u2013731.", "citeRegEx": "Bunescu and Mooney.,? 2005", "shortCiteRegEx": "Bunescu and Mooney.", "year": 2005}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine de Marneffe", "Bill MacCartney", "Christopher D. Manning."], "venue": "Proceedings of the International Conference on Language Resources and Evaluation, volume 6, pages 449\u2013454.", "citeRegEx": "Marneffe et al\\.,? 2006", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["C\u0131cero Nogueira dos Santos", "Bing Xiang", "Bowen Zhou."], "venue": "Proceedings of 53rd Annual Meeting of the Association for Computational Linguistics, pages 626\u2013634.", "citeRegEx": "Santos et al\\.,? 2015", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Chain based rnn for relation classification", "author": ["Javid Ebrahimi", "Dejing Dou."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1244\u20131249.", "citeRegEx": "Ebrahimi and Dou.,? 2015", "shortCiteRegEx": "Ebrahimi and Dou.", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 6645\u20136649. IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Simple customization of recursive neural networks for semantic relation classification", "author": ["Kazuma Hashimoto", "Makoto Miwa", "Yoshimasa Tsuruoka", "Takashi Chikayama."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Hashimoto et al\\.,? 2013", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2013}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen."], "venue": "Advances in Neural Information Processing Systems, pages 190\u2013198.", "citeRegEx": "Hermans and Schrauwen.,? 2013", "shortCiteRegEx": "Hermans and Schrauwen.", "year": 2013}, {"title": "Opinion mining with deep recurrent neural networks", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 720\u2013728.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations", "author": ["Nanda Kambhatla."], "venue": "Proceedings of the ACL 2004 on Interactive Poster and Demonstration Ses-", "citeRegEx": "Kambhatla.,? 2004", "shortCiteRegEx": "Kambhatla.", "year": 2004}, {"title": "A dependency-based neural network for relation classification", "author": ["Yang Liu", "Furu Wei", "Sujian Li", "Heng Ji", "Ming Zhou", "Houfeng WANG."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin."], "venue": "arXiv preprint arXiv:1504.01106.", "citeRegEx": "Mou et al\\.,? 2015", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Combining neural networks and log-linear models to improve relation extraction", "author": ["Thien Huu Nguyen", "Ralph Grishman."], "venue": "arXiv preprint arXiv:1511.05926.", "citeRegEx": "Nguyen and Grishman.,? 2015", "shortCiteRegEx": "Nguyen and Grishman.", "year": 2015}, {"title": "Embedding semantic similarity in tree kernels for domain adaptation of relation extraction", "author": ["Barbara Plank", "Alessandro Moschitti."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1498\u20131507.", "citeRegEx": "Plank and Moschitti.,? 2013", "shortCiteRegEx": "Plank and Moschitti.", "year": 2013}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Semantic relation classification via convolutional neural networks with simple negative sampling", "author": ["Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao."], "venue": "pages 536\u2013540.", "citeRegEx": "Xu et al\\.,? 2015a", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Xu et al\\.,? 2015b", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Factor-based compositional embedding models", "author": ["Mo Yu", "Matthew Gormley", "Mark Dredze."], "venue": "NIPS Workshop on Learning Semantics.", "citeRegEx": "Yu et al\\.,? 2014", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Relation classification via convolutional deep neural network", "author": ["Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335\u2013", "citeRegEx": "Zeng et al\\.,? 2014", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "sification, compared with traditional methods based on either human-designed features (Kambhatla, 2004; Hendrickx et al., 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013).", "startOffset": 86, "endOffset": 127}, {"referenceID": 6, "context": "sification, compared with traditional methods based on either human-designed features (Kambhatla, 2004; Hendrickx et al., 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013).", "startOffset": 86, "endOffset": 127}, {"referenceID": 0, "context": ", 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013).", "startOffset": 19, "endOffset": 72}, {"referenceID": 14, "context": ", 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013).", "startOffset": 19, "endOffset": 72}, {"referenceID": 0, "context": ", 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013). Zeng et al. (2014) and Xu et al.", "startOffset": 20, "endOffset": 93}, {"referenceID": 0, "context": ", 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013). Zeng et al. (2014) and Xu et al. (2015a), for example, utilize convolutional neural networks (CNNs) for relation classification.", "startOffset": 20, "endOffset": 115}, {"referenceID": 0, "context": ", 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013). Zeng et al. (2014) and Xu et al. (2015a), for example, utilize convolutional neural networks (CNNs) for relation classification. Xu et al. (2015b) apply long short term memory (LSTM)-based recurrent neural networks (RNNs) along the shortest dependency path.", "startOffset": 20, "endOffset": 221}, {"referenceID": 0, "context": ", 2009) or kernels (Bunescu and Mooney, 2005; Plank and Moschitti, 2013). Zeng et al. (2014) and Xu et al. (2015a), for example, utilize convolutional neural networks (CNNs) for relation classification. Xu et al. (2015b) apply long short term memory (LSTM)-based recurrent neural networks (RNNs) along the shortest dependency path. Nguyen and Grishman (2015) build model ensembles with gated recurrent unit (GRU)based RNNs and CNNs.", "startOffset": 20, "endOffset": 359}, {"referenceID": 4, "context": ", one layer of CNN or RNN, whereas evidence in the deep learning community suggests that deep architectures are more capable of information integration and abstraction (Graves et al., 2013; Hermans and Schrauwen, 2013; Irsoy and Cardie, 2014).", "startOffset": 168, "endOffset": 242}, {"referenceID": 7, "context": ", one layer of CNN or RNN, whereas evidence in the deep learning community suggests that deep architectures are more capable of information integration and abstraction (Graves et al., 2013; Hermans and Schrauwen, 2013; Irsoy and Cardie, 2014).", "startOffset": 168, "endOffset": 242}, {"referenceID": 8, "context": ", one layer of CNN or RNN, whereas evidence in the deep learning community suggests that deep architectures are more capable of information integration and abstraction (Graves et al., 2013; Hermans and Schrauwen, 2013; Irsoy and Cardie, 2014).", "startOffset": 168, "endOffset": 242}, {"referenceID": 17, "context": "Following Xu et al. (2015b), we use the shortest dependency path (SDP) as the backbone of our deep RNNs (Figure 1a).", "startOffset": 10, "endOffset": 28}, {"referenceID": 17, "context": "Such data augmentation technique further improves the performance without the need of leveraging additional data resources as in Xu et al. (2015a).", "startOffset": 129, "endOffset": 147}, {"referenceID": 7, "context": "Kambhatla (2004) uses a maximum entropy model for feature combination.", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "Hendrickx et al. (2009) collect various features, including lexical, syntactic as well as semantic features.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Bunescu and Mooney (2005) design a kernel along the shortest dependency path (SDP) between two entities by observing that the relation strongly relies on SDPs.", "startOffset": 0, "endOffset": 26}, {"referenceID": 0, "context": "Bunescu and Mooney (2005) design a kernel along the shortest dependency path (SDP) between two entities by observing that the relation strongly relies on SDPs. Plank and Moschitti (2013) combine structural information and semantic information in a tree kernel.", "startOffset": 0, "endOffset": 187}, {"referenceID": 10, "context": "Socher et al. (2011) design a recursive neural network along the constituency parse tree.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Hashimoto et al. (2013), on the basis of recursive networks, emphasize more on important phrases to improve performance.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Ebrahimi and Dou (2015) also use recursive networks, but their model is restricted to the SDP.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Ebrahimi and Dou (2015) also use recursive networks, but their model is restricted to the SDP. Xu et al. (2015b) first introduce gated recurrent networks, in particular LSTM, to this task.", "startOffset": 0, "endOffset": 113}, {"referenceID": 2, "context": "Ebrahimi and Dou (2015) also use recursive networks, but their model is restricted to the SDP. Xu et al. (2015b) first introduce gated recurrent networks, in particular LSTM, to this task. Zeng et al. (2014), on the other hand, apply CNNs to relation classification.", "startOffset": 0, "endOffset": 208}, {"referenceID": 2, "context": "Along this line, dos Santos et al. (2015) replace the common softmax loss function with a ranking loss in their CNN model.", "startOffset": 21, "endOffset": 42}, {"referenceID": 2, "context": "Along this line, dos Santos et al. (2015) replace the common softmax loss function with a ranking loss in their CNN model. Xu et al. (2015a) design a negative sampling method based on CNNs.", "startOffset": 21, "endOffset": 141}, {"referenceID": 2, "context": "Along this line, dos Santos et al. (2015) replace the common softmax loss function with a ranking loss in their CNN model. Xu et al. (2015a) design a negative sampling method based on CNNs. From the viewpoint of model ensembling, Liu et al. (2015) combine CNNs and recursive networks along the SDP, while Nguyen and Grishman (2015) incorporate CNNs with RNNs.", "startOffset": 21, "endOffset": 248}, {"referenceID": 2, "context": "Along this line, dos Santos et al. (2015) replace the common softmax loss function with a ranking loss in their CNN model. Xu et al. (2015a) design a negative sampling method based on CNNs. From the viewpoint of model ensembling, Liu et al. (2015) combine CNNs and recursive networks along the SDP, while Nguyen and Grishman (2015) incorporate CNNs with RNNs.", "startOffset": 21, "endOffset": 332}, {"referenceID": 17, "context": "Given a sentence and its dependency parse tree,1 we follow Xu et al. (2015b) and build our neural network on the shortest dependency path (SDP), which serves as a backbone.", "startOffset": 59, "endOffset": 77}, {"referenceID": 17, "context": "Different from Xu et al. (2015b), we design deep RNNs with up to four hidden layers.", "startOffset": 15, "endOffset": 33}, {"referenceID": 3, "context": "Existing studies have demonstrated the effectiveness of SDP (Ebrahimi and Dou, 2015; Liu et al., 2015; Xu et al., 2015b; Xu et al., 2015a); details are not repeated here.", "startOffset": 60, "endOffset": 138}, {"referenceID": 10, "context": "Existing studies have demonstrated the effectiveness of SDP (Ebrahimi and Dou, 2015; Liu et al., 2015; Xu et al., 2015b; Xu et al., 2015a); details are not repeated here.", "startOffset": 60, "endOffset": 138}, {"referenceID": 18, "context": "Existing studies have demonstrated the effectiveness of SDP (Ebrahimi and Dou, 2015; Liu et al., 2015; Xu et al., 2015b; Xu et al., 2015a); details are not repeated here.", "startOffset": 60, "endOffset": 138}, {"referenceID": 17, "context": "Existing studies have demonstrated the effectiveness of SDP (Ebrahimi and Dou, 2015; Liu et al., 2015; Xu et al., 2015b; Xu et al., 2015a); details are not repeated here.", "startOffset": 60, "endOffset": 138}, {"referenceID": 7, "context": "In this sense, single-layer RNNs are actually shallow in information processing (Hermans and Schrauwen, 2013; Irsoy and Cardie, 2014).", "startOffset": 80, "endOffset": 133}, {"referenceID": 8, "context": "In this sense, single-layer RNNs are actually shallow in information processing (Hermans and Schrauwen, 2013; Irsoy and Cardie, 2014).", "startOffset": 80, "endOffset": 133}, {"referenceID": 6, "context": "We evaluated our DRNNs model on the SemEval2010 Task 8 dataset,2 which is an established benchmark for relation classification (Hendrickx et al., 2009).", "startOffset": 127, "endOffset": 151}, {"referenceID": 11, "context": "Word embeddings were 200-dimensional, pretrained ourselves using word2vec (Mikolov et al., 2013) on the Wikipedia corpus; embeddings in other channels were 50-dimensional initialized randomly.", "startOffset": 74, "endOffset": 96}, {"referenceID": 16, "context": "For a fair comparison, we basically followed the settings in Xu et al. (2015b). Word embeddings were 200-dimensional, pretrained ourselves using word2vec (Mikolov et al.", "startOffset": 61, "endOffset": 79}, {"referenceID": 11, "context": "Word embeddings were 200-dimensional, pretrained ourselves using word2vec (Mikolov et al., 2013) on the Wikipedia corpus; embeddings in other channels were 50-dimensional initialized randomly. The hidden layers in each channel had the same number of units as their embeddings (either 200 or 50); the penultimate hidden layer was 100-dimensional. An `2 penalty of 10\u22125 was also applied as in Xu et al. (2015b), but we chose the dropout rate by validation", "startOffset": 75, "endOffset": 409}, {"referenceID": 12, "context": "Implementation based on Mou et al. (2015). Variant of Data augmentation F1 No Augmentation 83.", "startOffset": 24, "endOffset": 42}, {"referenceID": 6, "context": "2 (Hendrickx et al., 2009) depdency parse, Levin classes, PropBank, FanmeNet, NomLex-Plus, Google n-gram, paraphrases, TextRunner", "startOffset": 2, "endOffset": 26}, {"referenceID": 15, "context": "8 (Socher et al., 2011) + POS, NER, WordNet 77.", "startOffset": 2, "endOffset": 23}, {"referenceID": 16, "context": "1 (Socher et al., 2012) + POS, NER, WordNet 82.", "startOffset": 2, "endOffset": 23}, {"referenceID": 20, "context": "7 (Zeng et al., 2014) + position embeddings, WordNet 82.", "startOffset": 2, "endOffset": 21}, {"referenceID": 3, "context": "7 (Ebrahimi and Dou, 2015)", "startOffset": 2, "endOffset": 26}, {"referenceID": 19, "context": "6 (Yu et al., 2014) + dependency parsing, NER 83.", "startOffset": 2, "endOffset": 19}, {"referenceID": 18, "context": "(Xu et al., 2015b) Word + POS + GR + WordNet embeddings 83.", "startOffset": 0, "endOffset": 18}, {"referenceID": 10, "context": "0 (Liu et al., 2015) Word embeddings + NER 83.", "startOffset": 2, "endOffset": 20}, {"referenceID": 17, "context": "7 (Xu et al., 2015a) + negative sampling from NYT dataset 85.", "startOffset": 2, "endOffset": 20}, {"referenceID": 13, "context": "4 (Nguyen and Grishman, 2015) Word+POS+NER+WordNet embeddings, CNNs, RNNs + Voting 84.", "startOffset": 2, "endOffset": 29}, {"referenceID": 17, "context": "We tried a CNN with a sliding window of size 3 like Xu et al. (2015a); other settings were as our DRNNs.", "startOffset": 52, "endOffset": 70}, {"referenceID": 13, "context": "Socher et al. (2012) build a recursive neural network on the constituency tree and achieve a comparable performance with Hendrickx et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "(2012) build a recursive neural network on the constituency tree and achieve a comparable performance with Hendrickx et al. (2009). Further, they extend their recursive network with matrix-vector interaction and elevate the F1score to 82.", "startOffset": 107, "endOffset": 131}, {"referenceID": 3, "context": "Ebrahimi and Dou (2015) restrict the network to SDP, which is slight better than a sentence-wide recursive network.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "Ebrahimi and Dou (2015) restrict the network to SDP, which is slight better than a sentence-wide recursive network. Xu et al. (2015b) first introduce a type of gated recurrent neural network (LSTM) into this task and raise the F1-score to 83.", "startOffset": 0, "endOffset": 134}, {"referenceID": 15, "context": "From the perspective of convolution, Zeng et al. (2014) construct a CNN on the word sequence; they also integrate word position embeddings, which help a lot on the CNN architecture.", "startOffset": 37, "endOffset": 56}, {"referenceID": 2, "context": "dos Santos et al. (2015) propose a similar CNN model, named CRCNN, by replacing the common softmax cost function with a ranking-based cost function.", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": "dos Santos et al. (2015) propose a similar CNN model, named CRCNN, by replacing the common softmax cost function with a ranking-based cost function. By diminishing the impact of the Other class, they have achieved an F1-score of 84.1%. Along the line of CNNs, Xu et al. (2015a) design a straighward negative sampling method, which introduces additional samples from other corpora like the NYT dataset.", "startOffset": 4, "endOffset": 278}, {"referenceID": 2, "context": "dos Santos et al. (2015) propose a similar CNN model, named CRCNN, by replacing the common softmax cost function with a ranking-based cost function. By diminishing the impact of the Other class, they have achieved an F1-score of 84.1%. Along the line of CNNs, Xu et al. (2015a) design a straighward negative sampling method, which introduces additional samples from other corpora like the NYT dataset. Doing so greatly improves the performance to a high F1-score of 85.6%. Besides, two representative hybrid models of neural networks are designed by Liu et al. (2015) and Nguyen and Grishman (2015).", "startOffset": 4, "endOffset": 568}, {"referenceID": 2, "context": "dos Santos et al. (2015) propose a similar CNN model, named CRCNN, by replacing the common softmax cost function with a ranking-based cost function. By diminishing the impact of the Other class, they have achieved an F1-score of 84.1%. Along the line of CNNs, Xu et al. (2015a) design a straighward negative sampling method, which introduces additional samples from other corpora like the NYT dataset. Doing so greatly improves the performance to a high F1-score of 85.6%. Besides, two representative hybrid models of neural networks are designed by Liu et al. (2015) and Nguyen and Grishman (2015).", "startOffset": 4, "endOffset": 599}, {"referenceID": 17, "context": "Without using additional data resources (Xu et al., 2015a), or ensembling methods (Nguyen and Grishman, 2015), we outperform previous state-of-the-art results.", "startOffset": 40, "endOffset": 58}, {"referenceID": 13, "context": ", 2015a), or ensembling methods (Nguyen and Grishman, 2015), we outperform previous state-of-the-art results.", "startOffset": 32, "endOffset": 59}, {"referenceID": 16, "context": "Without the use of neural networks, Yu et al. (2014) propose a Feature-rich Compositional Embedding Model (FCM), which combines unlexicalized linguistic contexts and word embeddings.", "startOffset": 36, "endOffset": 53}], "year": 2017, "abstractText": "Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent, compared with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolution neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluate our DRNNs on the SemEval-2010 Task 8, and achieve an F1score of 85.81%, outperforming state-of-theart recorded results.", "creator": "TeX"}}}