{"id": "1601.03754", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2016", "title": "Dual-tree $k$-means with bounded iteration runtime", "abstract": "k-means is a widely used clustering algorithm, but for $k$ clusters and a dataset size of $N$, each iteration of Lloyd's algorithm costs $O(kN)$ time. Although there are existing techniques to accelerate single Lloyd iterations, none of these are tailored to the case of large $k$, which is increasingly common as dataset sizes grow. We propose a dual-tree algorithm that gives the exact same results as standard $k$-means; when using cover trees, we use adaptive analysis techniques to, under some assumptions, bound the single-iteration runtime of the algorithm as $O(N + k log k)$. To our knowledge these are the first sub-$O(kN)$ bounds for exact Lloyd iterations. We then show that this theoretically favorable algorithm performs competitively in practice, especially for large $N$ and $k$ in low dimensions. Further, the algorithm is tree-independent, so any type of tree may be used.", "histories": [["v1", "Thu, 14 Jan 2016 21:18:06 GMT  (52kb)", "http://arxiv.org/abs/1601.03754v1", "supplementary material included; submitted to ICML '16"]], "COMMENTS": "supplementary material included; submitted to ICML '16", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["ryan r curtin"], "accepted": false, "id": "1601.03754"}, "pdf": {"name": "1601.03754.pdf", "metadata": {"source": "META", "title": "Dual-tree k-means with bounded iteration runtime", "authors": ["Ryan R. Curtin"], "emails": ["ryan@ratml.org"], "sections": [{"heading": null, "text": "ar Xiv: 160 1,03 754v 1 [cs.D S] 1 4Ja n20 16"}, {"heading": "1. Introduction", "text": "Of all the cluster algorithms in use today, one of the simplest and most commonly used is the time-honored Kmean cluster algorithm, which is usually implemented by Lloyd's algorithm: When using a data set S, repeat the following two steps (a \"Lloyd iteration\") until the centroids converge each of the k clusters: 1. Assign each dot pi \u00b2 S to the cluster with the closest center. 2. Recalculate the centroids for each cluster based on the assignments of each dot in S. Clearly, a simple implementation of this algorithm takes O (kN) time, in which N = | S |. However, the number of iterations is not limited unless the user manually sets a maximum, and k means are not guaranteed to approach the world's best cluster. Despite these shortcomings, k means tend in practice to converge quickly to reasonable solutions."}, {"heading": "Submitted to the 33 rd International Conference on Machine", "text": "It is not only the way in which people move in the city, but also the way in which they move, and in which they move, and in which they move, and in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live."}, {"heading": "3. Tree-based algorithms", "text": "Dre rf\u00fc ide eeisrcnlhsrtee\u00fccnlhSrtee\u00fccnlhsrtee\u00fccnlhsrtee\u00fccnlhsrtee\u00fccnlhsrf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc-eaeaeaJnlhsrgne\u00fceaJnlrh-eaJnlrsrgne\u00fceaeaeaeaJnlrh-eaJnlrgnea-nlrlrsrteeaeoPnlrsrsrsrteeoioioioi.nlrsn"}, {"heading": "4. Pruning strategies", "text": "This year, it has come to the point where it can only take one year to move on to the next round."}, {"heading": "16: s \u2190 \u221e", "text": "17: otherwise if dmax (Nq, c) < ub (Nq) then 18: {We can improve the upper limit.} 19: ub (Nq) \u2190 dmax (Nq, Nr) 20: closest (Nq) \u2190 c21: {Check if all clusters (except one) are pruned.} 22: if pruned (Nq) = \u2190 23: return svia next (\u00b7). A simple algorithm to do this is given here as Algorithm 4 (UpdateCentroids (); it is a deep recursion through the tree that terminates a branch if a node belongs to a single cluster."}, {"heading": "6. Theoretical results", "text": "It is about the question of whether and in what form people will be able to put themselves into the world, and about the question of whether they are able to put themselves into the world. (...) It is about the question of whether the world is able to put itself into the world. (...) It is about the question of whether it is able to save the world. (...) It is about the question of whether it is able to save the world. (...) It is about the question of whether it is able to save the world. (...) It is about the question of whether it is able to save the world. (...) It is about the question of whether it is able to save the world. (...) It is about the question of whether it is able to save the world. (...) It is about the question of whether it is able to save the world. (...) It is about the question of whether it is able to save the world. (...) It is about the world, about the world, about the world, about the world, about the world. (...) It is about the world, about the world, about the world, about the world. (...) It is about the world, about the world, about the world, about the world, about the world, about the world."}, {"heading": "7. Experiments", "text": "The next thing to consider is the empirical performance of the algorithm. We use the publicly available kmeans program in mlpack (Curtin et al., 2013a); in our experiments we execute it as follows: $kmeans -i dataset.csv -I centroids.csv -C $-e $algorithm in which we use the number of clusters and $algorithms is the algorithm implemented in C + +. For the yinyang algorithms we use the authors \"implementation. We use a variety of values on the real world datasets; details are shown in Table 2; 1997; Lupton et al., 2001 the table also contains the time taken to build a kd tree (for blacklist and dualtree-kd)."}, {"heading": "8. Conclusion and future directions", "text": "Using four pruning strategies, we have developed a flexible, tree-independent k-mean algorithm that is the most powerful algorithm for large datasets and large k in small to medium dimensions. It is theoretically advantageous, has limited memory and can be used in conjunction with initial point selection and approximation programs for additional acceleration. However, there are still interesting future directions to pursue. First, parallelism: since our dual tree algorithm is agnostic in terms of the type of traversal used, we can use a parallel traversal (Curtin et al., 2013b), like an adapted version of a current parallel dual tree algorithm (Lee et al., 2012). Second, k-means and other spectral cluster techniques: our algorithm could be merged with the ideas of Curtin & Ram (2014) to perform further k-means."}, {"heading": "A. Supplementary material", "text": "Unfortunately, space constraints prevent an adequate explanation of every single point in the main work. This supplementary material is intended to clarify all parts of the dual k-means algorithm that space has not allowed in the main work."}, {"heading": "A.1. Updating the tree", "text": "In addition to updating the Zentroids, the limiting information contained within the tree must correspond to the processing strategies 3 and 4. b) b) b) b) b (max.) b) b (max.) b) b (max.) b) b (max.) b) b (max.) b) b (max.) b) b (max.) b) b) b (max.) b) b) b (max.) b) b (max.) b) b (max.) b) b) b (max.) b) b) b (max.) b) b) b (max. b) b) b) b (max. b) b) b (max. b) b) b (max. b) b) b (max.) b) b) b (max. b) b) b (max. b) b) b (max. b) b) b) b (max. b) b) b) b (max. b) b) b) b) b (b) b) b) b (max. (b) b) b) b (b) b) b (max. b) b) b) b (b) b) b) b) b (max. b) b) b) b) b) b) b (max. b) b) b) b) b (max) b) b) b (max) b) b) b) b (max) b (max) b) b) b) b (max) b) b (max) b) b (max) b) b (max) b) b) b (max) b) b) b (max) b) b (max) b) b) b (max) b) b (max) b) b) b (max) b) b (max) b) b (max) b) b (max) b) b) b (max) b (max) b) b) b (max) b) b (max) b) b) b) b (max (max) b) b) b) b (max. b) b) b) b) b (max (max. b) b) b) b) b (max. b) b) b) b) b (max. b) b) b)"}, {"heading": "A.2. Coalescing the tree", "text": "After UpdateTree () has been called, the tree must be merged to remove all nodes where canchange (\u00b7) = false can be achieved by a single run over the tree. A simple implementation is in algorithm 6. DecoalesceTree () can be implemented by simply restoring a pristine copy of the tree cached just before calling CoalesceTree ()."}, {"heading": "A.3. Correctness proof", "text": "It is as if the number of points (possibly zero) and the number of parent nodes (possibly zero) are connected to each other. \u2022 There is a node in each space tree with no parent; this is the root node of the tree. \u2022 Every point in S is contained in at least one node. \u2022 Every point in S is a node in every room that does not have parents. \u2022 There is a node in every room that does not have parents. \u2022 It is the root node of the tree. \u2022 Every point in S is contained in at least one node. \u2022 Every point in S is contained in at least one node. \u2022 It is the root node of the tree. \u2022 Every point in S is contained in at least one node. \u2022 Every node in S is in one node. \u2022 It is a node in every room that does not have parents. \u2022 It is the root node of the tree. \u2022 Every point in S is contained in at least one node."}], "references": [{"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Arthur and Vassilvitskii,? \\Q2007\\E", "shortCiteRegEx": "Arthur and Vassilvitskii", "year": 2007}, {"title": "An optimal algorithm for approximate nearest neighbor searching in fixed dimensions", "author": ["S. Arya", "D.M. Mount", "N.S. Netanyahu", "R. Silverman", "A.Y. Wu"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Arya et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Arya et al\\.", "year": 1998}, {"title": "Label embedding trees for large multi-class tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "In Advances in Neural Information Processing Systems 23 (NIPS \u201910),", "citeRegEx": "Bengio et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2010}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "Communications of the ACM,", "citeRegEx": "Bentley,? \\Q1975\\E", "shortCiteRegEx": "Bentley", "year": 1975}, {"title": "Cover trees for nearest neighbor", "author": ["A. Beygelzimer", "S.M. Kakade", "J. Langford"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning (ICML", "citeRegEx": "Beygelzimer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2006}, {"title": "Refining initial points for k-means clustering", "author": ["P.S. Bradley", "U.M. Fayyad"], "venue": "In Proceedings of the 15th International Conference on Machine Learning (ICML", "citeRegEx": "Bradley and Fayyad,? \\Q1998\\E", "shortCiteRegEx": "Bradley and Fayyad", "year": 1998}, {"title": "Concepts and effectiveness of the cover-coefficient-based clustering methodology for text databases", "author": ["F. Can", "E.A. Ozkarahan"], "venue": "ACM Transactions on Database Systems,", "citeRegEx": "Can and Ozkarahan,? \\Q1990\\E", "shortCiteRegEx": "Can and Ozkarahan", "year": 1990}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Visual categorization with bags of keypoints", "author": ["G. Csurka", "C. Dance", "L. Fan", "J. Willamowski", "C. Bray"], "venue": "InWorkshop on Statistical Learning in Computer Vision, ECCV,", "citeRegEx": "Csurka et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Csurka et al\\.", "year": 2004}, {"title": "Dual-tree fast exact max-kernel search", "author": ["R.R. Curtin", "P. Ram"], "venue": "Statistical Analysis and Data Mining,", "citeRegEx": "Curtin and Ram,? \\Q2014\\E", "shortCiteRegEx": "Curtin and Ram", "year": 2014}, {"title": "MLPACK: A scalable C++ machine learning library", "author": ["R.R. Curtin", "J.R. Cline", "N.P. Slagle", "W.B. March", "P. Ram", "N.A. Mehta", "A.G. Gray"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Curtin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Curtin et al\\.", "year": 2013}, {"title": "Tree-independent dual-tree algorithms", "author": ["R.R. Curtin", "W.B. March", "P. Ram", "D.V. Anderson", "A.G. Gray", "C.L. Isbell Jr."], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML", "citeRegEx": "Curtin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Curtin et al\\.", "year": 2013}, {"title": "Fast exact max-kernel search", "author": ["R.R. Curtin", "P. Ram", "A.G. Gray"], "venue": "In Proceedings of SIAM International Conference on Data Mining 2013 (SDM", "citeRegEx": "Curtin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Curtin et al\\.", "year": 2013}, {"title": "Plug-and-play dual-tree algorithm runtime analysis", "author": ["R.R. Curtin", "D. Lee", "W.B. March", "P. Ram"], "venue": "arXiv preprint arXiv:1501.05222,", "citeRegEx": "Curtin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Curtin et al\\.", "year": 2015}, {"title": "An automatic benchmarking system", "author": ["M. Edel", "A. Soni", "R.R. Curtin"], "venue": "In Proceedings of the NIPS 2014 Workshop on Software Engineering for Machine Learning,", "citeRegEx": "Edel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Edel et al\\.", "year": 2014}, {"title": "Using the triangle inequality to accelerate k-means", "author": ["C. Elkan"], "venue": "In Proceedings of the 20th International Conference on Machine Learning (ICML \u201903),", "citeRegEx": "Elkan,? \\Q2003\\E", "shortCiteRegEx": "Elkan", "year": 2003}, {"title": "A fast k-means implementation using coresets", "author": ["G. Frahling", "C. Sohler"], "venue": "International Journal of Computational Geometry & Applications,", "citeRegEx": "Frahling and Sohler,? \\Q2008\\E", "shortCiteRegEx": "Frahling and Sohler", "year": 2008}, {"title": "N-Body\u2019 problems in statistical learning", "author": ["A.G. Gray", "A.W. Moore"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gray and Moore,? \\Q2001\\E", "shortCiteRegEx": "Gray and Moore", "year": 2001}, {"title": "Nonparametric density estimation: Toward computational tractability", "author": ["A.G. Gray", "A.W. Moore"], "venue": "In SIAM International Conference on Data Mining (SDM),", "citeRegEx": "Gray and Moore,? \\Q2003\\E", "shortCiteRegEx": "Gray and Moore", "year": 2003}, {"title": "Making k-means even faster", "author": ["G. Hamerly"], "venue": "In Proceedings of the 2010 SIAM International Conference on Data Mining, pp", "citeRegEx": "Hamerly,? \\Q2010\\E", "shortCiteRegEx": "Hamerly", "year": 2010}, {"title": "Dimensionality, discriminability, density and distance distributions", "author": ["M.E. Houle"], "venue": "IEEE 13th International Conference on Data Mining Workshops (ICDMW),", "citeRegEx": "Houle,? \\Q2013\\E", "shortCiteRegEx": "Houle", "year": 2013}, {"title": "Finding nearest neighbors in growth-restricted metrics", "author": ["D.R. Karger", "M. Ruhl"], "venue": "In Proceedings of the Thirty-Fourth Annual ACM Symposium on Theory of Computing (STOC", "citeRegEx": "Karger and Ruhl,? \\Q2002\\E", "shortCiteRegEx": "Karger and Ruhl", "year": 2002}, {"title": "Scalable clustering algorithm for n-body simulations in a shared-nothing cluster", "author": ["Y.C. Kwon", "D. Nunley", "J.P. Gardner", "M. Balazinska", "B. Howe", "S. Loebman"], "venue": "In Scientific and Statistical Database Management,", "citeRegEx": "Kwon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kwon et al\\.", "year": 2010}, {"title": "A distributed kernel summation framework for general-dimension machine learning", "author": ["D. Lee", "R.W. Vuduc", "A.G. Gray"], "venue": "In Proceedings of the 2012 SIAM International Conference on Data Mining (SDM", "citeRegEx": "Lee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2012}, {"title": "An investigation of practical approximate nearest neighbor algorithms", "author": ["T. Liu", "A.W. Moore", "K. Yang", "A.G. Gray"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Liu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "The SDSS imaging pipelines", "author": ["R. Lupton", "J.E. Gunn", "Z. Ivezic", "G.R. Knapp", "S. Kent"], "venue": "In Astronomical Data Analysis Software and Systems X,", "citeRegEx": "Lupton et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lupton et al\\.", "year": 2001}, {"title": "Very fast em-based mixture model clustering using multiresolution kd-trees", "author": ["A.W. Moore"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Moore,? \\Q1999\\E", "shortCiteRegEx": "Moore", "year": 1999}, {"title": "Accelerating exact kmeans algorithms with geometric reasoning", "author": ["D. Pelleg", "A.W. Moore"], "venue": "In Proceedings of KDD", "citeRegEx": "Pelleg and Moore,? \\Q1999\\E", "shortCiteRegEx": "Pelleg and Moore", "year": 1999}], "referenceMentions": [{"referenceID": 24, "context": "Algorithms of this sort include the work of Pelleg and Moore (1999), Elkan (2003), Hamerly (2010), and Ding et al.", "startOffset": 55, "endOffset": 68}, {"referenceID": 15, "context": "Algorithms of this sort include the work of Pelleg and Moore (1999), Elkan (2003), Hamerly (2010), and Ding et al.", "startOffset": 69, "endOffset": 82}, {"referenceID": 15, "context": "Algorithms of this sort include the work of Pelleg and Moore (1999), Elkan (2003), Hamerly (2010), and Ding et al.", "startOffset": 69, "endOffset": 98}, {"referenceID": 15, "context": "Algorithms of this sort include the work of Pelleg and Moore (1999), Elkan (2003), Hamerly (2010), and Ding et al. (2015). However, the scaling of these algorithms can make them problematic for the case of large k and large N .", "startOffset": 69, "endOffset": 122}, {"referenceID": 15, "context": "Existing techniques include the brute-force implementation, the blacklist algorithm (Pelleg & Moore, 1999), Elkan\u2019s algorithm (2003), and Hamerly\u2019s algorithm (2010), as well as the recent Yinyang k-means algorithm (Ding et al.", "startOffset": 108, "endOffset": 133}, {"referenceID": 15, "context": "Existing techniques include the brute-force implementation, the blacklist algorithm (Pelleg & Moore, 1999), Elkan\u2019s algorithm (2003), and Hamerly\u2019s algorithm (2010), as well as the recent Yinyang k-means algorithm (Ding et al.", "startOffset": 108, "endOffset": 165}, {"referenceID": 22, "context": "Clusterings for n-body simulations on astronomical data often involve several thousand clusters (Kwon et al., 2010).", "startOffset": 96, "endOffset": 115}, {"referenceID": 23, "context": "Pelleg and Moore (1999) report several hundred clusters in a subset of 800k objects from the SDSS dataset.", "startOffset": 11, "endOffset": 24}, {"referenceID": 7, "context": "Csurka et al. (2004) extract vocabularies from image sets using k-means with k \u223c 1000.", "startOffset": 0, "endOffset": 21}, {"referenceID": 7, "context": "Coates et al. (2011) show that k-means can work surprisingly well for unsupervised feature learning for images, using k as large as 4000 on 50000 images.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "18000 unique labels (Bengio et al., 2010).", "startOffset": 20, "endOffset": 41}, {"referenceID": 2, "context": "18000 unique labels (Bengio et al., 2010). Can and Ozkarahan (1990) suggest that the number of clusters in text data is directly related to the size of the vocabulary, suggesting k \u223c mN/t where m is the vocabulary size, n is the number of documents, and t is the number of nonzero entries in the term matrix.", "startOffset": 21, "endOffset": 68}, {"referenceID": 3, "context": "This approach is applicable to a surprising variety of other problems, too (Bentley, 1975; Moore, 1999; Curtin et al., 2013c).", "startOffset": 75, "endOffset": 125}, {"referenceID": 26, "context": "This approach is applicable to a surprising variety of other problems, too (Bentley, 1975; Moore, 1999; Curtin et al., 2013c).", "startOffset": 75, "endOffset": 125}, {"referenceID": 3, "context": "The same reason is responsible for the impressive speedups obtained for other single-tree algorithms, such as nearest neighbor search (Bentley, 1975; Liu et al., 2004).", "startOffset": 134, "endOffset": 167}, {"referenceID": 24, "context": "The same reason is responsible for the impressive speedups obtained for other single-tree algorithms, such as nearest neighbor search (Bentley, 1975; Liu et al., 2004).", "startOffset": 134, "endOffset": 167}, {"referenceID": 10, "context": "A recent result generalizes the class of dual-tree algorithms, simplifying their expression and development (Curtin et al., 2013b). Any dual-tree algorithm can be decomposed into three parts: a type of space tree, a pruning dual-tree traversal, and a point-to-point BaseCase() function and node-to-node Score() function that determines when pruning is possible. Precise definitions and details of the abstraction are given by Curtin et al. (2013b), but for our purposes, this means that we can describe a dual-tree k-means algorithm entirely with a straightforward BaseCase() function and Score() function.", "startOffset": 109, "endOffset": 448}, {"referenceID": 4, "context": "The two types of trees we will explicitly consider in this paper are the kd-tree and the cover tree (Beygelzimer et al., 2006), but it should be remembered that the algorithm as provided is sufficiently general to work with any other type of tree.", "startOffset": 100, "endOffset": 126}, {"referenceID": 4, "context": "The two types of trees we will explicitly consider in this paper are the kd-tree and the cover tree (Beygelzimer et al., 2006), but it should be remembered that the algorithm as provided is sufficiently general to work with any other type of tree. Therefore, we standardize notation for trees: a tree is denoted with T , and a node in the tree is denoted by N . Each node in a tree may have children; the set of children of Ni is denoted Ci. In addition, each node may hold some points; this set of points is denoted Pi. Lastly, the set of descendant points of a node Ni is denoted D i . The descendant points are all points held by descendant nodes, and it is important to note that the set Pi is not equivalent to D p i . This notation is taken from Curtin et al. (2013b) and is detailed more comprehensively there.", "startOffset": 101, "endOffset": 774}, {"referenceID": 15, "context": "Thus, we will pursue four pruning strategies, each based on or related to earlier work (Pelleg & Moore, 1999; Elkan, 2003; Hamerly, 2010).", "startOffset": 87, "endOffset": 137}, {"referenceID": 19, "context": "Thus, we will pursue four pruning strategies, each based on or related to earlier work (Pelleg & Moore, 1999; Elkan, 2003; Hamerly, 2010).", "startOffset": 87, "endOffset": 137}, {"referenceID": 15, "context": "then cj will own pq next iteration (Elkan, 2003).", "startOffset": 35, "endOffset": 48}, {"referenceID": 4, "context": "Our results are with respect to the expansion constant ck of the centroids (Beygelzimer et al., 2006), which is a measure of intrinsic dimension.", "startOffset": 75, "endOffset": 101}, {"referenceID": 13, "context": "Our results also depend on the imbalance of the tree it(T ), which in practice generally scales linearly in N (Curtin et al., 2015).", "startOffset": 110, "endOffset": 131}, {"referenceID": 4, "context": ") Cover trees have O(N) nodes (Beygelzimer et al., 2006); because CoalesceTree(), DecoalesceTree(), UpdateCentroids(), and UpdateTree() can be performed in one pass of the tree, these steps may each be completed in O(N) time.", "startOffset": 30, "endOffset": 56}, {"referenceID": 13, "context": "Recent results show that dual-tree algorithms that use the cover tree may have their runtime easily bounded (Curtin et al., 2015).", "startOffset": 108, "endOffset": 129}, {"referenceID": 4, "context": "This meshes with earlier theoretical results (Beygelzimer et al., 2006; Curtin et al., 2015; Ram et al., 2009a) and earlier empirical results (Gray & Moore, 2003; 2001) that suggest that an answer can be obtained for a single query point in O(1) time.", "startOffset": 45, "endOffset": 111}, {"referenceID": 13, "context": "This meshes with earlier theoretical results (Beygelzimer et al., 2006; Curtin et al., 2015; Ram et al., 2009a) and earlier empirical results (Gray & Moore, 2003; 2001) that suggest that an answer can be obtained for a single query point in O(1) time.", "startOffset": 45, "endOffset": 111}, {"referenceID": 13, "context": "First, runtime bounds for cover trees are known to be loose (Curtin et al., 2015).", "startOffset": 60, "endOffset": 81}, {"referenceID": 25, "context": "We use a variety of k values on mostly real-world datasets; details are shown in Table 2 (Lichman, 2013; Zhang et al., 1997; Lupton et al., 2001).", "startOffset": 89, "endOffset": 145}, {"referenceID": 14, "context": "These simulations were performed on a modest consumer desktop with an Intel i5 with 16GB RAM, using mlpack\u2019s benchmarking system (Edel et al., 2014).", "startOffset": 129, "endOffset": 148}, {"referenceID": 1, "context": "Further, because our algorithm is treeindependent, we may use tree structures that are tailored to high-dimensional data (Arya et al., 1998)\u2014 including ones that have not yet been developed.", "startOffset": 121, "endOffset": 140}, {"referenceID": 23, "context": ", 2013b), such as an adapted version of a recent parallel dual-tree algorithm (Lee et al., 2012).", "startOffset": 78, "endOffset": 96}, {"referenceID": 20, "context": "Recently, more general notions of intrinsic dimensionality have been proposed (Houle, 2013; Amsaleg et al., 2015); these may enable tighter and more descriptive runtime bounds.", "startOffset": 78, "endOffset": 113}, {"referenceID": 10, "context": "The first direction is parallelism: because our dual-tree algorithm is agnostic to the type of traversal used, we may use a parallel traversal (Curtin et al., 2013b), such as an adapted version of a recent parallel dual-tree algorithm (Lee et al., 2012). The second direction is kernel k-means and other spectral clustering techniques: our algorithm may be merged with the ideas of Curtin & Ram (2014) to perform kernel k-means.", "startOffset": 144, "endOffset": 402}], "year": 2016, "abstractText": "k-means is a widely used clustering algorithm, but for k clusters and a dataset size of N , each iteration of Lloyd\u2019s algorithm costs O(kN) time. Although there are existing techniques to accelerate single Lloyd iterations, none of these are tailored to the case of large k, which is increasingly common as dataset sizes grow. We propose a dual-tree algorithm that gives the exact same results as standard k-means; when using cover trees, we use adaptive analysis techniques to, under some assumptions, bound the single-iteration runtime of the algorithm as O(N + k log k). To our knowledge these are the first subO(kN) bounds for exact Lloyd iterations. We then show that this theoretically favorable algorithm performs competitively in practice, especially for large N and k in low dimensions. Further, the algorithm is treeindependent, so any type of tree may be used.", "creator": "LaTeX with hyperref package"}}}