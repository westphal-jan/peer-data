{"id": "1601.03797", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2016", "title": "ActiveClean: Interactive Data Cleaning While Learning Convex Loss Models", "abstract": "Data cleaning is often an important step to ensure that predictive models, such as regression and classification, are not affected by systematic errors such as inconsistent, out-of-date, or outlier data. Identifying dirty data is often a manual and iterative process, and can be challenging on large datasets. However, many data cleaning workflows can introduce subtle biases into the training processes due to violation of independence assumptions. We propose ActiveClean, a progressive cleaning approach where the model is updated incrementally instead of re-training and can guarantee accuracy on partially cleaned data. ActiveClean supports a popular class of models called convex loss models (e.g., linear regression and SVMs). ActiveClean also leverages the structure of a user's model to prioritize cleaning those records likely to affect the results. We evaluate ActiveClean on five real-world datasets UCI Adult, UCI EEG, MNIST, Dollars For Docs, and WorldBank with both real and synthetic errors. Our results suggest that our proposed optimizations can improve model accuracy by up-to 2.5x for the same amount of data cleaned. Furthermore for a fixed cleaning budget and on all real dirty datasets, ActiveClean returns more accurate models than uniform sampling and Active Learning.", "histories": [["v1", "Fri, 15 Jan 2016 02:02:00 GMT  (2988kb,D)", "http://arxiv.org/abs/1601.03797v1", "Pre-print"]], "COMMENTS": "Pre-print", "reviews": [], "SUBJECTS": "cs.DB cs.LG", "authors": ["sanjay krishnan", "jiannan wang", "eugene wu", "michael j franklin", "ken goldberg"], "accepted": false, "id": "1601.03797"}, "pdf": {"name": "1601.03797.pdf", "metadata": {"source": "CRF", "title": "ActiveClean: Interactive Data Cleaning While Learning Convex Loss Models", "authors": ["Sanjay Krishnan", "Jiannan Wang", "Eugene Wu", "Michael J. Franklin", "Ken Goldberg"], "emails": ["goldberg}@berkeley.edu", "ewu@cs.columbia.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "There are a number of approaches at stake, both in terms of the way in which the data are realized and in terms of the way in which they are realized. (1) The way in which the data are realized is very different. (2) The way in which the data are realized is very different. (3) The way in which the data are realized is very different. (4) The way in which the data are realized is very different.) The way in which they are realized, in which they are realized is very different. (5) The way in which they are realized, in which they are realized is very different. (5) The way in which they are realized, in which they are realized is very different. (5) The way in which they are realized is very different."}, {"heading": "2. BACKGROUND AND PROBLEM SETUP", "text": "In this section, the iterative process of data cleansing and training is formalized and an application example is presented."}, {"heading": "2.1 Predictive Modeling", "text": "The user provides a relation R and wants a model based on the data in R. This work focuses on a class of well-analyzed predictive analysis problems, which can be expressed as minimizing convex loss functions. Problems of convex loss minimization include generalized linear models (including linear and logistic regression), supporting vector machines, and indeed means and medians are also special cases. We assume that the user provides a Featurizer F (\u00b7) that maps each record r-R to a feature vector x and a label y. In designated training examples {(xi, yi)} Ni = 1, the problem is to find a vector of model parameters by minimizing a loss function to all training examples."}, {"heading": "2.2 Data Cleaning", "text": "We look at corruption that affects the attribute values of datasets, not at errors that affect multiple datasets at the same time, such as record duplication or structure such as scheme transformation. Examples of supported cleanups include batch-solving common inconsistencies (e.g. merging \"USA\" and \"USA\"), filtering outliers (e.g. removing records with values > 1e6), and standardizing attribute semantics (e.g. \"1.2 miles\" and \"1.93 km\"). We are particularly interested in errors that are difficult or time-consuming to clean up, and require the analyst to examine a faulty dataset and determine the appropriate actionable knowledge of the current best model. We present this process as clean (\u00b7) that can be applied to a dataset r (or a set of records) to restore the clean dataset."}, {"heading": "2.3 Iteration", "text": "As an example of howClean (\u00b7) fits into an iterative analysis process, consider an analyst training as regression and identification of outliers. If it examines one of the outliers, it detects that the basic data (before the featurisation) has a formatting inconsistency that leads to an incorrect parsing of the numerical values. It applies a batch solution (i.e. Clean (\u00b7)) to all outliers with the same error and retrains the model. This iterative process can be described as the following pseudo-code loop: 1. Init (iter) 2. current _ model = Train (R) 3. For each t in {1,... iter} (a) dirty _ sample = Identify (R, current _ model) (b) clean _ sample = clean (dirty _ sample) (c) current _ model = Train (R) 3. Output current: current _ modelWhile we already identify Train (R, current _ model) (Update) b, clean _ modeled = (4.)"}, {"heading": "2.4 Challenges", "text": "Correctness: Suppose the analyst has implemented an identification (\u00b7) function that returns dirty datasets, the simple application of data cleaning is to repair corruption on the spot and realign the model after each repair. Suppose k N datasets are cleaned, but all remaining dirty datasets are retained in the datasets. Figure 1 highlights the dangers of this approach on a very simple dirty dataset and a linear regression model, i.e., the best fit line for two variables. One of the variables is systematically corrupted with a translation in the X axis (Figure 1a), the dirty data is highlighted in red and the clean data in blue, and they are shown with their respective best fit lines. After cleaning only two data points (Figure 1b), the best fit line in the X axis is corrupted (Figure 1a)."}, {"heading": "2.5 The Need For Automation", "text": "By automating the iterative process, ActiveClean ensures reliable models with convergence guarantees. The analyst initially initializes ActiveClean with a dirty model. ActiveClean carefully selects small data packets that are cleaned based on data that is likely to be dirty and affect the model. The analyst applies data cleanup to these packets, and ActiveClean updates the model with a step-by-step optimization technique. In previous work, machine learning has been used to improve the efficiency of data cleanup [37, 38, 16]. Human input, either to clean or validate automated cleaning, is often expensive and impractical for large datasets. A model can learn rules from a small group of examples that have been cleaned (or validated) by a human, and active learning is a technique to carefully select the set of examples in order to learn the most accurate model that can still be used in order to prick the cleaners."}, {"heading": "2.6 Use Case: Dollars for Docs [2]", "text": "ProPublica collected a dataset of corporate donations to physicians to analyze conflicts of interest. They reported that some physicians received more than $500,000 in travel, meals, and consulting costs. [4] ProPublica consistently curated and cleaned a dataset from the Centers for Medicare and Medicaid Services that listed nearly 250,000 research donations, and aggregated those donations from physicians, pharmaceuticals, and pharmaceutical companies. We collected the raw unaggregated data and investigated whether suspicious donations could be predicted using a model. This problem is typical of analysis scenarios based on observational data from the fields of finance, insurance, medicine, and investigative journalism. The dataset includes the following schema: Contr ibut ion (p i _ s p e c c i c i a l y, drug _ name, device _ name, corpora t ion, amount, d i spute, s spute, s t a t a t a t a t a t a t t t s s s): The dataset of laboratory laboratories ip _ ip is the following laboratory name, ip i."}, {"heading": "3. PROBLEM FORMALIZATION", "text": "This section formalizes the issues raised in the documents.3.1 Notation and SetupThe user represents a relation R, a cleaner C (\u00b7), a feature F (\u00b7), and a convex loss problem defined by the loss \u03c6 (\u00b7). Total k records are cleaned in batches of size b, so there will be kb iterations.We use the following notation to resubmit relevant intermediate states: \u2022 Dirty model: \u03b8 (d) is the model that is trained on R (without cleaning) with the featurizer F (\u00b7) and the loss \u03c6 (\u00b7). This serves as initialization for ActiveClean. \u2022 Dirty records: Rdirty R is the subset of records that are still dirty. As more data is cleaned Rdirty \u2192 {}. \u2022 Clean records: Rclean R is the subset of records that are clean."}, {"heading": "3.2 Problem 1. Correct Update Problem", "text": "In view of the newly cleaned data Sclean and the currently best model \u03b8 (t), the problem of the model update is the calculation \u03b8 (t + 1). \u03b8 (t + 1) will have some errors in relation to the true model \u03b8 (c), which we call an error: Error (\u03b8 (t + 1)) = Error (t + 1) \u2212 \u03b8 (c). Since a data sample is cleaned, it is only useful to talk about expected errors. We call the update algorithm \"reliable\" when the expected error is above the limit due to a monotonously decreasing function \u00b5 of the amount of cleaned data: E (Error (new) = O (\u00b5 (Sclean |))) Intuitively, \"reliable\" means that more cleaning should imply more accuracy. The problem of the correct update is to reliably update the model TB (t) with a sample of cleaned data."}, {"heading": "3.3 Problem 2. Efficiency Problem", "text": "The efficiency problem is to select Sclean in such a way that the expected error E (error (\u03b8 (t))) is minimized. ActiveClean uses previously cleaned data to estimate the value of the data cleanup on new data sets. It then draws a sample of the data sets S Rdirty. This is an uneven sample where each data set r has a sample probability p (r) based on the estimates. We derive the optimal sample distribution for the SGD updates and show how the theoretical optimum can be approximately achieved. The efficiency problem is to select a sample distribution p (\u00b7) across all data sets in such a way that the expected error w.r.t is transferred to the model when trained on completely clean data."}, {"heading": "4. ARCHITECTURE", "text": "This section introduces the ActiveClean architecture."}, {"heading": "4.1 Overview", "text": "Figure 2 illustrates the ActiveClean architecture. The dotted boxes describe optional components that the user can provide to improve the efficiency of the system. (4.1.1 Required user input model: The user provides a predictive model (e.g. SVM) specified as a convex loss optimization problem \u03c6 (\u00b7) and a Featurizer F (\u00b7) that maps a record to its feature vector x and its label y. Cleaning function: The user provides a function C (\u00b7) (implemented via software or crowdsourcing) that maps dirty records as defined in Section??. Batches: The data is cleaned in batches of size b and the user can change these settings if he wishes more or less frequent modeling updates. The choice of b affects the convergence rate. Section 5 discusses the efficiency and convergence of different trade-offs, which is irical."}, {"heading": "4.2 Example", "text": "The following example illustrates how a user would use ActiveClean to address the use case in Section 2.6: EXAMPLE 1. The analyst chooses to use an SVM model and manually cleans the records (the C (\u00b7)). ActiveClean first selects a sample of 50 records (the default) to view the analyst. It identifies a subset of 15 dirty records, corrects them by normalizing the drug and company names using a search engine, and corrects the labels with typographical or incorrect values. The system then uses the cleaned records to update the current best model and the next sample of 50. The analyst can stop at any time and use the improved model to predict the likelihood of a donation."}, {"heading": "5. UPDATES WITH CORRECTNESS", "text": "This section describes an algorithm for reliable model updates; the updater assumes that it is given a sample of data from Rdirty where i-Sdirty has a known sampling probability p (i); Sections 6 and 7 show how p (\u00b7) can be optimized; and the analysis in this section applies to all sample distributions p (\u00b7) > 0."}, {"heading": "5.1 Geometric Derivation", "text": "The updated algorithm intuitively follows the convex geometry of the problem. Let's consider the problem in one dimension (i.e., the parameter \u03b8 is a scalar value), so that the goal is to find the minimum point (\u03b8) of a curve l (\u03b8). However, the consequence of dirty data is that the false loss function is optimized. Figure 3A illustrates the consequences of the optimization. The red dashed line shows the loss function on the dirty data. The optimization of the loss function finds that at the minimum point (red star) the true loss function (w.r.t to the clean data) is in blue, so the optimal value on the dirty data is indeed a suboptimal point on the clean curve (red circle).The optimal clean model is visualized as a yellow star. The first question is which direction to update the clean models (d) (i.e., leftor right)."}, {"heading": "5.2 Model Update Algorithm", "text": "In summary, the algorithm was initialized with \u03b8 (0) = \u03b8 (d), which is the dirty model. There are three parameters set by the user, the budget k, the batch size b and the step size \u03b3. In the following section, we provide references from the convex optimization literature that allow the user to select these values accordingly. In each iteration t = {1,..., T}, the cleaning is applied to a stack of data b selected from the group of candidates for dirty data sets Rdirty. Subsequently, an average gradient is estimated from the cleaned batch and the model is updated. Iterations continue until k = T \u00b7 b datasets are cleaned.1. Calculate the gradient above the sample of newly clean data and call the result gS (\u03b8 (t)). 2. Calculate the average gradient over all already clean data in Rclean R = dirty \u2212 2001, and call the result (C)."}, {"heading": "5.3 Analysis with Stochastic Gradient Descent", "text": "The update algorithms can be described as a class of very well studied algorithms called the Stochastic Gradient Descent. SGD provides a theoretical framework for understanding and analyzing the update rule and analyzing the error. (SGD) A major difference to traditional SGD models is that ActiveClean applies a complete gradient step to the already clean data and compares it to a stochastic gradient step (i.e., calculated from a sample).ActiveClean iterations can perform multiple passes over the clean data. The update algorithms can be considered a variant of SGD that materializes the clean value."}, {"heading": "5.4 Example", "text": "This example describes an application of the update algorithm. EXAMPLE 2. Remember that the analyst has a dirty SVM model on the dirty data \u03b8 (d). She decides that she has a budget to clean 100 datasets and decides to clean the 100 datasets in stacks of 10 each (based on how fast she can clean the data and how often she wants to see an updated result). All data is first treated with Rdirty = R and Rclean = \u2205. The course of a basic SVM results from the following function: [x (c) i, y (c) i \u00b7 x, if y \u00b7 x) \u2264 10, if y (i). For each iteration t, a sample of 10 datasets S is drawn from Rdirty. ActiveClean then applies the cleaning function to the sample: [x (c) i, y (c) i (c) i (i) i (c) i (c) i (i) i) (i) i))."}, {"heading": "6. EFFICIENCY WITH SAMPLING", "text": "The updater received a sample with the probabilities p (\u00b7). For each distribution with p (\u00b7) > 0, we can maintain the correctness. ActiveClean uses a sampling algorithm that selects the most valuable data sets to make them more likely to be cleaned."}, {"heading": "6.1 Oracle Sampling Problem", "text": "Remember that the convergence rate of an SGD algorithm is limited by \u03c32, which is the variance of the gradient. Intuitively, the variance measures how accurately the gradient is estimated from a uniform sample. Other sample distributions, while maintaining the expected value of the sample, may have a lower variance. Therefore, the oracle sample problem is defined as a search by sample distributions to find the minimum variance sample distribution.DEFINITION 1 (ORACLE SAMPLING PROBLEM). Given a number of dirty data, it requires finding dirty, dirty sample probabilities p (r) in such a way that over all samples S of size k it is minimized: E (ORACLE SAMPLING PROBLEM) However, it can be shown that the optimal distribution over datasets in Rdirty would affect the probabilities proportional to: c (c) (i) (i) (i) the intuition of Gdirty would represent the probability of: c (c) (i) (i) (i) the problem that is more reliable."}, {"heading": "6.2 Dirty Gradient Solution", "text": "There is no such oracle, and one solution is to use the w.r.t gradient to the dirty data: pi \u043d\u044b\u0435 \u03c6 (x (d) i, y (d) i, \u03b8 (t)). It turns out that the solution works reasonably well in practice on our experimental datasets and has been studied in machine learning as an expected gradient length heuristics. [31] The contribution in this paper is to integrate this heuristics with statistically correct updates. Intuitively, however, an approximation of the oracle as close as possible can lead to improved prioritization. The following section describes two components, the detector and the estimator, which can be used to improve the convergence rate. Our experiments suggest that convergence can be improved up to two-fold in the use of these optional optimizations (Section 8.3.2)."}, {"heading": "7. OPTIMIZATIONS", "text": "In this section, we describe two optimization approaches, the detector and the estimator, which improve the efficiency of the cleaning process. Both approaches aim to increase the probability that the sampler will select dirty records that move the model most towards a truly clean model after cleaning it. The detector will learn the characteristics that distinguish dirty records from clean records, while the Estimator is designed to estimate the amount that cleaning a given dirty record will move the model towards the true optimal model."}, {"heading": "7.1 The Detector", "text": "The detector provides two important aspects of a dataset: (1) whether the dataset is dirty, and (2) if it is dirty, which is wrong with the dataset; the sampler can (1) select a subset of dirty datasets to query in each batch, and the estimator can (2) estimate the value of data cleanup based on other datasets with the same corruption; ActiveClean supports two types of detectors: a priori and adaptive. In the former, we assume that we know the set of dirty datasets and how they are dirty a priori for ActiveClean, while the latter adaptive enumerates characteristics of the dirty data in the context of running ActiveClean.7.1.1 A Priori Detector For many types of contamination, such as missing attribute values and limitation violations, it is possible to efficiently list a set of corrupt datasets and determine how the datasets will be corrupted."}, {"heading": "7.2 Adaptive Detection", "text": "The challenge in formulating this problem is that the detector must describe how the data is dirty (e.g. it is a priori case), and the detector achieves this by categorizing corruption into u-classes, which are corruption categories that do not necessarily match the characteristics, but each record is classified with a maximum of one category. When using adaptive detection, the repair step must clean the data and report which of the u-classes the corrupted records belong to. If an example (x, y) of the repair step labels it with one of the clean, 1, 2, u + 1 classes (including one for \"not dirty\").It is possible that u-classes are detected as other types of dirt."}, {"heading": "7.3 The Estimator", "text": "The estimator calculates the average change in all gradients. This estimator can be highly inaccurate as it also uses records known to be clean. ActiveClean leverages the detector for an average change between these two extremes. The estimator calculates the average change and selectively corrects the average change in gradients."}, {"heading": "To turn the result into a probability distribution, ActiveClean normalizes over all dirty records.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8. EXPERIMENTS", "text": "First, the experiments evaluate how different types of corrupt data benefit from data cleanup, then the experiments examine different prioritization and model update schemes for progressive data cleanup, and finally, ActiveClean is consistently evaluated in a number of real-world data cleanup scenarios."}, {"heading": "8.1 Experimental Setup and Notation", "text": "In the neeisrmtlrmnUe\u00fceegnn nvo rde eeisrmtlrmUeaeegnln rf\u00fc ide eeisrmtlrVrteee\u00fceegnln rf\u00fc ide eeisrmtlrteeaeVnlrrrrrrrrrlrteeeeeeeeeeeaeVnlrrrrrrlrrrrrlrrrrrlrrrrrrrrrrrrrrrrrrln rf\u00fc rrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "8.2 Does Data Cleaning Matter?", "text": "This year it is more than ever before."}, {"heading": "8.4 ActiveClean: Adaptive Detection", "text": "This experiment examines how the results of the previous experiment change when an adaptive detector is used instead of the a priori detector. Let's remember that in systematic corruption, 3 of the most informative features were corrupted, so we group these problems into 9 classes. We use an all-against-one SVM to learn categorization. 8.4.1 Basic Performance Figure 9 superimposes a curve (referred to as AC + C) representing ActiveClean with a classifier instead of a priori detection on the convergence diagrams in the previous Ex periments. Initially, ActiveClean is comparable to active learning, but as the classifier becomes more effective, performance detection improves. Over both sets, with the 500 datasets on the curve, adaptive ActiveClean is a 30% higher model that does not adjust ScriveClean."}, {"heading": "8.5 Estimation", "text": "The next experiment compares estimation techniques: (1) \"linear regression\" trains a linear regression model that predicts the clean gradient as a function of the dirty gradient, (2) \"average gradient\" that does not use detection to apply the estimate, (3) \"average characteristic change\" uses detection but not linearization, and (4) the linear approximation of the Taylor series. Figure 10b measures how accurately each estimation technique estimates the gradient as a function of the number of cleaned datasets on the EEG dataset. The estimation error is measured using the relative L2 error with the true gradient. The proposed Taylor series provides more accurate information for small purification variables. Linear regression and the average characteristic modification technique eventually produce comparable, but much more data after purification. Summary: Linear gradient estimates are more accurate when estimated from small samples."}, {"heading": "8.6 Real World Scenarios", "text": "This year it is more than ever before in the history of the city."}, {"heading": "9. RELATED WORK", "text": "In fact, most people will be able to move to another world in which they are able to live, in which they want to live."}, {"heading": "10. DISCUSSION AND FUTURE WORK", "text": "The experimental results indicate the following conclusions about ActiveClean: (1) when the data corruption rate is relatively low (e.g. 5%), ActiveClean cleans fewer data sets than Active Learning or SampleClean to achieve the same model accuracy, (2) all optimizations in ActiveClean (importance of sampling, detection, and estimation) result in significantly more accurate models for small samples, (3) only when corruption rates are very high (e.g. 50%), SampleClean exceeds ActiveClean, and (4) two real-life scenarios show similar accuracy improvements, where ActiveClean provides significantly more accurate models than SampleClean or Active Learning for the same number of data sets."}, {"heading": "11. CONCLUSION", "text": "The increasing popularity of predictive models in data analysis poses additional challenges in managing dirty data. Progressive data cleaning in this environment is prone to errors due to the mixing of dirty and clean data, sensitivity to sample size, and low error rate. The key finding of ActiveClean is that an important class of predictive models, so-called convex loss models (e.g. linear regression and SVMs), can be trained and cleaned simultaneously. As a result, there are verifiable guarantees for the convergence and error limits of ActiveClean. ActiveClean also includes numerous optimizations such as: utilizing the information from the data cleaning model on samples, detecting dirty data to avoid sampling clean data, and batching updates. The experimental results are promising as they suggest that these optimizations can significantly reduce the cost of data cleaning on Facebook, if Acisco's cleaning budgets are sparse for Visco, and for Visco."}, {"heading": "12. REFERENCES", "text": "[1] Berkeley data analytics stack.https: / / amplab.cs.berkeley.edu / software /. [2] Dollars for docs. http: / / projects.propublica.org / open-payments /. [3] For big-data scientists, \"janitor work\" is key hurdle to insights. http: / / www.nytimes.com / 2014 / 08 / 18 / technology / for-big-datascientists-hurdle-to-insights-is-janitor-work.html. [4] A pharma payment a day keeps docs \"finances\" okay. https: / www.propublica.org / article / a-pharma-payment-a-day-finances-ok. [5] A. Alexandrov, R. Bergmann, S. Ewen, J. Freytag, F. Hueske, A. Heise, O. Kao, M. Leich, U.S. readers."}, {"heading": "A. SET-OF-RECORDS CLEANING MODEL", "text": "On paper, we formalized the data cleanup specified by the analyst as follows: We take a sample of the data sets Sdirty and apply the data cleanup C (\u00b7). C is applied to a data set and produces a clean data set: Sclean = {C (r): Sclean = Sclean (r): Sclean = Sclean (r): Sclean = Sclean (r): Sclean = Sclean (r): Sclean = Sclean (r): Sclean = Sclean (r): Sclean = Sclean) Sclean = Sclean = Sclean (r) Sclean) Sclean = Sclean = Sclean (r) Sclean) Sclean = Sclean = Sclean (r) Sclean = Sclean)"}, {"heading": "B. STOCHASTIC GRADIENT DESCENT", "text": "The first problem is to select the weights \u03b1 and \u03b2 (on average already clean and newly cleaned data) in such a way that the estimate of the gradient is unbiased; the batch Sdirty is only obtained from Rdirty. Since the quantities of Rdirty and its supplement are known, it follows that the gradient can be combined with the already clean data gC and the recently cleaned data gS as follows: g (\u03b8t) = | Rdirty | \u00b7 gS + | Rclean | R | Therefore, the gradient estimate g (\u03b8) is unbiased if gS is an unbiased estimate of: 1 | Rdirty | Rclean | R |, \u03b2 = | Rdirty | LEMMA 1. The gradient estimate g (\u03b8) is unbiased if gS is an unbiased estimate of: 1 | Rdirty | PROOF SKETCH."}, {"heading": "C. NON-CONVEX LOSSES", "text": "We acknowledge that non-convex losses are becoming increasingly popular in the Neural Network and Deep Learning literature, but even for these losses, gradient descend techniques still apply. Instead of converging to a global optimum, they converge to a locally optimal value. Likewise, ActiveClean will converge to the locally optimal value of the dirty model. Therefore, it is more difficult to judge the results. Different initializations will lead to different local optimizations and therefore result in a complex dependence on the initialization with the dirty model. This problem is not well-founded for ActiveClean, and any gradient technique suffers from this challenge of general nonconvex losses, and we hope to explore this more closely in the future. IMPORTANT SAMPLING This problem describes the optimal distribution across a number of scalars: LEMMA 2. Given a set of real numbers A = {a1,..., we will leave it."}, {"heading": "A\u0302 be a sample with replacement of A of size k. If \u00b5 is the mean A\u0302, the sampling distribution that minimizes the variance of \u00b5,", "text": "The variance of this estimate results from: V ar (\u00b5) = E (\u00b52) \u2212 E (\u00b5) 2Since the estimate is unbiased, we can remove E (\u00b5) by the average of A: V ar (\u00b5) = E (\u00b52) \u2212 A \u04182Because the A number is deterministic, we can remove this term during the minimization process. Furthermore, we can write E (\u00b52) as follows: E (\u00b52) = 1 n2 n \u2211 i a2i piThen we can solve the following optimization problem (removal of the proportionality of 1n2) via weight group P = {p (ai)}: min P N \u00b2 i a2i pisubject to: P > 0, \u2211 P = 1Application of Lagrange multipliers, an equivalent problem of 1n2) via weight group P = {p (ai)}: min P N \u00b2 i = 2v2 subject pito: P > vai = 0 v2, an equivalent problem."}, {"heading": "E. LINEARIZATION", "text": "If this attribute (c) = f (d) + f (d) \u00b7 \u00b7 (d \u2212 c) +... disregarding the terms of higher order, the linear term f \u00b2 (d) \u00b7 (d \u2212 c) is finally a linear function in each attribute and designation. We only need to know the change in each attribute to estimate the change in value. In our case, the function f is the gradient. So the resulting linearization is: p (x (c) i, y (c) i, i) i (record) i). If we take the expected value: p (x, y, \u03b8) + x (x)."}, {"heading": "F. EXAMPLE MX , MY", "text": "Linear regression: When we refer to the partial derivatives \u03b8 in relation to x, Mx: Mx [i, i] = 2x [i] + \u2211 i 6 = j \u03b8 [j] x [j] \u2212 yMx [i, j] = \u03b8 [j] x [i]. Similarly, my [i, 1] = x [i] Logistic regression: [i, y] = (h (x, y) \u2212 y) xwhereh (z) = 11 + e \u2212 zwe can describe this as: My [i, 1] = 11 + e\u03b8T x \u00b2 (x, y [i] Logistic regression: (h) \u00b2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (x) \u2212 y) xwhereh (z) = 11 + e \u2212 zwe can describe this as: h\u03b8 (x) = 11 + e\u03b8T x \u00b2 (x, y, \u03b8) = (h, y) = (h, y) = (h, y) [in component form], c \u00b7 \u00b7 \u00b7 y (c) = bus (x huttle = y) (x, y) (y) (y) (x horizon = x i, y) = 1, y (y) = 1, y (y)."}, {"heading": "G. AGGREGATE QUERIES AS CONVEX LOSSES", "text": "GVG and SUM-SERVICES are a special case of lost minimization: If we define the following loss, it is easy to verify whether the optimum mean is \u00b5:????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "H. EXPERIMENTAL COMPARISON", "text": "H.1 Robust Logistic Regression We use the algorithm of Feng et al. for robust logistic regression.1. Input: Contaminated training samples {(x1, y1),..., (xn, yn)} an upper limit for the number of outliers n, the number of outliers n and the sample dimension p.2. Initialization: Set T = 4 \u221a log p / n + logn / n3. Removal of samples (xi, yi) whose magnitude satisfies the problem of regulated logistic regression."}, {"heading": "I. DOLLARS FOR DOCS SETUP", "text": "The Dollars for Docs Dataset have the following scheme: Contr ibut ion > p i _ s p e c i t y, drug _ name, device _ name, corpora t ion, amount, d i spute, s t a t u s) To mark suspicious medicines, we used the status attribute. When the status was \"covered,\" it was a permitted contribution under the researcher's declared protocol. When the status was \"not covered,\" it meant that it was an impermissible contribution under the researcher's declared protocol. The rest of the textual attributes were provided with a bag-ofwords model, and the numerical amounts and attributes in dispute were treated as numbers."}, {"heading": "J. MNIST SETUP", "text": "We include the visualization of the errors we created for the MNIST experiment. We created these errors in MATLAB by taking the grayscale version of the image (a 64 x 64 matrix) and corrupting it by block removal and blurring."}], "references": [{"title": "The stratosphere platform for big data analytics", "author": ["A. Alexandrov", "R. Bergmann", "S. Ewen", "J. Freytag", "F. Hueske", "A. Heise", "O. Kao", "M. Leich", "U. Leser", "V. Markl", "F. Naumann", "M. Peters", "A. Rheinl\u00e4nder", "M.J. Sax", "S. Schelter", "M. H\u00f6ger", "K. Tzoumas", "D. Warneke"], "venue": "VLDB J., 23(6),", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Progressive approach to relational entity resolution", "author": ["Y. Altowim", "D.V. Kalashnikov", "S. Mehrotra"], "venue": "PVLDB, 7(11),", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Query-oriented data cleaning with oracles", "author": ["M. Bergman", "T. Milo", "S. Novgorodov", "W.C. Tan"], "venue": "SIGMOD Conference,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey", "author": ["D.P. Bertsekas"], "venue": "CoRR, abs/1507.01030,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic gradient descent tricks", "author": ["L. Bottou"], "venue": "Neural Networks: Tricks of the Trade - Second Edition.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Tupleware: Distributed machine learning on small clusters", "author": ["A. Crotty", "A. Galakatos", "T. Kraska"], "venue": "IEEE Data Eng. Bull., 37(3),", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "JMLR, 13,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Model-driven data acquisition in sensor networks", "author": ["A. Deshpande", "C. Guestrin", "S. Madden", "J.M. Hellerstein", "W. Hong"], "venue": "VLDB,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["P. Drineas", "M. Magdon-Ismail", "M.W. Mahoney", "D.P. Woodruff"], "venue": "JMLR, 13,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust logistic regression and classification", "author": ["J. Feng", "H. Xu", "S. Mannor", "S. Yan"], "venue": "NIPS,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "The elements of statistical learning, volume 1", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Springer series in statistics Springer, Berlin,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Corleone: Hands-off crowdsourcing for entity matching", "author": ["C. Gokhale", "S. Das", "A. Doan", "J.F. Naughton", "N. Rampalli", "J. Shavlik", "X. Zhu"], "venue": "SIGMOD,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Incremental record linkage", "author": ["A. Gruenheid", "X.L. Dong", "D. Srivastava"], "venue": "PVLDB, 7(9),", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Active learning as non-convex optimization", "author": ["A. Guillory", "E. Chastain", "J. Bilmes"], "venue": "International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimating the number and sizes of fuzzy-duplicate clusters", "author": ["A. Heise", "G. Kasneci", "F. Naumann"], "venue": "CIKM Conference,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Declarative support for sensor data cleaning", "author": ["S.R. Jeffery", "G. Alonso", "M.J. Franklin", "W. Hong", "J. Widom"], "venue": "Pervasive Computing,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Enterprise data analysis and visualization: An interview study", "author": ["S. Kandel", "A. Paepcke", "J.M. Hellerstein", "J. Heer"], "venue": "IEEE Trans. Vis. Comput. Graph., 18(12),", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "A methodology for learning, analyzing, and mitigating social influence bias in recommender systems", "author": ["S. Krishnan", "J. Patel", "M.J. Franklin", "K. Goldberg"], "venue": "RecSys,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Stale view cleaning: Getting fresh answers from stale materialized views", "author": ["S. Krishnan", "J. Wang", "M.J. Franklin", "K. Goldberg", "T. Kraska"], "venue": "PVLDB, 8(12),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning accurate kinematic control of cable-driven surgical robots using data cleaning and gaussian process regression", "author": ["J. Mahler", "S. Krishnan", "M. Laskey", "S. Sen", "A. Murali", "B. Kehoe", "S. Patil", "J. Wang", "M. Franklin", "P. Abbeel", "K.Y. Goldberg"], "venue": "CASE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "ERACER: a database approach for statistical inference and data cleaning", "author": ["C. Mayfield", "J. Neville", "S. Prabhakar"], "venue": "SIGMOD Conference,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Scaling up crowd-sourcing to very large datasets: A case for active learning", "author": ["B. Mozafari", "P. Sarkar", "M.J. Franklin", "M.I. Jordan", "S. Madden"], "venue": "PVLDB, 8(2),", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Query strategies for evading convex-inducing classifiers", "author": ["B. Nelson", "B.I.P. Rubinstein", "L. Huang", "A.D. Joseph", "S.J. Lee", "S. Rao", "J.D. Tygar"], "venue": "JMLR, 13,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "TKDE, 22(10),", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Progressive duplicate  detection", "author": ["T. Papenbrock", "A. Heise", "F. Naumann"], "venue": "IEEE Trans. Knowl. Data Eng., 27(5),", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "University of Wisconsin, Madison, 52:11,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "The interpretation of interaction in contingency tables", "author": ["E.H. Simpson"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1951}, {"title": "A sample-and-clean framework for fast and accurate query processing on dirty data", "author": ["J. Wang", "S. Krishnan", "M.J. Franklin", "K. Goldberg", "T. Kraska", "T. Milo"], "venue": "SIGMOD Conference,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Incremental entity resolution on rules and data", "author": ["S.E. Whang", "H. Garcia-Molina"], "venue": "VLDB J., 23(1),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Is feature selection secure against training data poisoning", "author": ["H. Xiao", "B. Biggio", "G. Brown", "G. Fumera", "C. Eckert", "F. Roli"], "venue": "In ICML,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Don\u2019t be scared: use scalable automatic repairing with maximal likelihood and bounded changes", "author": ["M. Yakout", "L. Berti-Equille", "A.K. Elmagarmid"], "venue": "SIGMOD Conference,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Guided data repair", "author": ["M. Yakout", "A.K. Elmagarmid", "J. Neville", "M. Ouzzani", "I.F. Ilyas"], "venue": "PVLDB, 4(5),", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Stochastic optimization with importance sampling for regularized loss minimization", "author": ["P. Zhao", "T. Zhang"], "venue": "ICML,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Machine Learning on large and growing datasets is a key data management challenge with significant interest in both industry and academia [1, 5, 10, 20].", "startOffset": 138, "endOffset": 152}, {"referenceID": 5, "context": "Machine Learning on large and growing datasets is a key data management challenge with significant interest in both industry and academia [1, 5, 10, 20].", "startOffset": 138, "endOffset": 152}, {"referenceID": 16, "context": "Data often arrive dirty, including missing, incorrect, or inconsistent attributes, and analysts widely report that data cleaning and other forms of pre-processing account for up to 80% of their effort [3, 22].", "startOffset": 201, "endOffset": 208}, {"referenceID": 29, "context": "While data cleaning is an extensively studied problem, the predictive modeling setting poses a number of new challenges: (1) high dimensionality can amplify even a small amount of erroneous records [36], (2) the complexity can make it difficult to trace the consequnces of an error, and (3) there are often subtle technical cconditions (e.", "startOffset": 198, "endOffset": 202}, {"referenceID": 15, "context": "For example, battery-powered sensors can transmit unreliable measurements when battery levels are low [21].", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": ", typos), and unintentional cognitive biases [23].", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "ActiveClean is inspired by the recent success of progressive data cleaning where a user can gradually clean more data until the desired accuracy is reached [6, 34, 30, 17, 26, 38, 37].", "startOffset": 156, "endOffset": 183}, {"referenceID": 28, "context": "ActiveClean is inspired by the recent success of progressive data cleaning where a user can gradually clean more data until the desired accuracy is reached [6, 34, 30, 17, 26, 38, 37].", "startOffset": 156, "endOffset": 183}, {"referenceID": 24, "context": "ActiveClean is inspired by the recent success of progressive data cleaning where a user can gradually clean more data until the desired accuracy is reached [6, 34, 30, 17, 26, 38, 37].", "startOffset": 156, "endOffset": 183}, {"referenceID": 12, "context": "ActiveClean is inspired by the recent success of progressive data cleaning where a user can gradually clean more data until the desired accuracy is reached [6, 34, 30, 17, 26, 38, 37].", "startOffset": 156, "endOffset": 183}, {"referenceID": 20, "context": "ActiveClean is inspired by the recent success of progressive data cleaning where a user can gradually clean more data until the desired accuracy is reached [6, 34, 30, 17, 26, 38, 37].", "startOffset": 156, "endOffset": 183}, {"referenceID": 31, "context": "ActiveClean is inspired by the recent success of progressive data cleaning where a user can gradually clean more data until the desired accuracy is reached [6, 34, 30, 17, 26, 38, 37].", "startOffset": 156, "endOffset": 183}, {"referenceID": 30, "context": "ActiveClean is inspired by the recent success of progressive data cleaning where a user can gradually clean more data until the desired accuracy is reached [6, 34, 30, 17, 26, 38, 37].", "startOffset": 156, "endOffset": 183}, {"referenceID": 10, "context": "Convex loss minimization problems are amenable to a variety of incremental optimization methodologies with provable guarantees (see Friedman, Hastie, and Tibshirani [15] for an introduction).", "startOffset": 165, "endOffset": 169}, {"referenceID": 26, "context": "Aggregates over mixtures of different populations of data can result in spurious relationships due to the well-known phenomenon called Simpson\u2019s paradox [32].", "startOffset": 153, "endOffset": 157}, {"referenceID": 27, "context": "This approach is similar to SampleClean [33], which was proposed to approximate the results of aggregate queries by applying them to a clean sample of data.", "startOffset": 40, "endOffset": 44}, {"referenceID": 30, "context": "Efficiency: Conversely, hypothetically assume that the analyst has implemented a correct Update(\u00b7) primitive and implements Identify(\u00b7) with a technique such as Active Learning to select records to clean [37, 38, 16].", "startOffset": 204, "endOffset": 216}, {"referenceID": 31, "context": "Efficiency: Conversely, hypothetically assume that the analyst has implemented a correct Update(\u00b7) primitive and implements Identify(\u00b7) with a technique such as Active Learning to select records to clean [37, 38, 16].", "startOffset": 204, "endOffset": 216}, {"referenceID": 11, "context": "Efficiency: Conversely, hypothetically assume that the analyst has implemented a correct Update(\u00b7) primitive and implements Identify(\u00b7) with a technique such as Active Learning to select records to clean [37, 38, 16].", "startOffset": 204, "endOffset": 216}, {"referenceID": 30, "context": "Machine learning has been applied in prior work to improve the efficiency of data cleaning [37, 38, 16].", "startOffset": 91, "endOffset": 103}, {"referenceID": 31, "context": "Machine learning has been applied in prior work to improve the efficiency of data cleaning [37, 38, 16].", "startOffset": 91, "endOffset": 103}, {"referenceID": 11, "context": "Machine learning has been applied in prior work to improve the efficiency of data cleaning [37, 38, 16].", "startOffset": 91, "endOffset": 103}, {"referenceID": 4, "context": "It is well known that even for an arbitrary initialization SGD makes significant progress in less than one epoch (a pass through the entire dataset) [9].", "startOffset": 149, "endOffset": 152}, {"referenceID": 6, "context": "1 Convergence Conditions and Properties Convergence properties of batch SGD formulations have been well studied [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 6, "context": "The convergence rates of SGD are also well analyzed [11, 8, 39].", "startOffset": 52, "endOffset": 63}, {"referenceID": 3, "context": "The convergence rates of SGD are also well analyzed [11, 8, 39].", "startOffset": 52, "endOffset": 63}, {"referenceID": 32, "context": "The convergence rates of SGD are also well analyzed [11, 8, 39].", "startOffset": 52, "endOffset": 63}, {"referenceID": 25, "context": "It turns out that the solution works reasonably well in practice on our experimental datasets and has been studied in Machine Learning as the Expected Gradient Length heuristic [31].", "startOffset": 177, "endOffset": 181}, {"referenceID": 9, "context": "Robust Logistic Regression [14].", "startOffset": 27, "endOffset": 31}, {"referenceID": 27, "context": "SampleClean (SC) [33].", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": "Active Learning (AL) [18].", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "Within each iteration, examples are prioritized by distance to the decision boundary (called Uncertainty Sampling in [31]).", "startOffset": 117, "endOffset": 121}, {"referenceID": 1, "context": "Progressive data cleaning is a well studied problem especially in the context of entity resolution [6, 34, 30, 17].", "startOffset": 99, "endOffset": 114}, {"referenceID": 28, "context": "Progressive data cleaning is a well studied problem especially in the context of entity resolution [6, 34, 30, 17].", "startOffset": 99, "endOffset": 114}, {"referenceID": 24, "context": "Progressive data cleaning is a well studied problem especially in the context of entity resolution [6, 34, 30, 17].", "startOffset": 99, "endOffset": 114}, {"referenceID": 12, "context": "Progressive data cleaning is a well studied problem especially in the context of entity resolution [6, 34, 30, 17].", "startOffset": 99, "endOffset": 114}, {"referenceID": 20, "context": "Over the last 5 years a number of new results have expanded the scope and practicality of progressive data cleaning [26, 38, 37].", "startOffset": 116, "endOffset": 128}, {"referenceID": 31, "context": "Over the last 5 years a number of new results have expanded the scope and practicality of progressive data cleaning [26, 38, 37].", "startOffset": 116, "endOffset": 128}, {"referenceID": 30, "context": "Over the last 5 years a number of new results have expanded the scope and practicality of progressive data cleaning [26, 38, 37].", "startOffset": 116, "endOffset": 128}, {"referenceID": 31, "context": "There are a number of other works that use machine learning to improve the efficiency and/or reliability of data cleaning [38, 37, 16].", "startOffset": 122, "endOffset": 134}, {"referenceID": 30, "context": "There are a number of other works that use machine learning to improve the efficiency and/or reliability of data cleaning [38, 37, 16].", "startOffset": 122, "endOffset": 134}, {"referenceID": 11, "context": "There are a number of other works that use machine learning to improve the efficiency and/or reliability of data cleaning [38, 37, 16].", "startOffset": 122, "endOffset": 134}, {"referenceID": 30, "context": "train a model that evaluates the likelihood of a proposed replacement value [37].", "startOffset": 76, "endOffset": 80}, {"referenceID": 31, "context": "Machine learning is also increasingly applied to make automated repairs more reliable with human validation [38].", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "Machine learning can extrapolate rules from a small set of examples cleaned by a human (or humans) to uncleaned data [16, 38].", "startOffset": 117, "endOffset": 125}, {"referenceID": 31, "context": "Machine learning can extrapolate rules from a small set of examples cleaned by a human (or humans) to uncleaned data [16, 38].", "startOffset": 117, "endOffset": 125}, {"referenceID": 21, "context": "This approach can be coupled with active learning [27] to learn an accurate model with the fewest possible number of examples.", "startOffset": 50, "endOffset": 54}, {"referenceID": 27, "context": "SampleClean [33] applies data cleaning to a sample of data, and estimates the results of aggregate queries.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "Sampling has also been applied to estimate the number of duplicates in a relation [19].", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "explore the problem of query-oriented data cleaning [7], where given a query, they clean data relevant to that query.", "startOffset": 52, "endOffset": 55}, {"referenceID": 7, "context": "studied data acquisition in sensor networks [12].", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "[21] explored similar prioritization based on value of information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "explored how samples of materialized views can be maintained similar to how models are updated with a sample of clean data in this work [24].", "startOffset": 136, "endOffset": 140}, {"referenceID": 32, "context": "Stochastic Optimization and Active Learning: Zhao and Tong recently proposed using importance sampling in conjunction with stochastic gradient descent [39].", "startOffset": 151, "endOffset": 155}, {"referenceID": 8, "context": "This line of work builds on prior results in linear algebra that show that some matrix columns are more informative than others [13], and Active Learning which shows that some labels are more informative that others [31].", "startOffset": 128, "endOffset": 132}, {"referenceID": 25, "context": "This line of work builds on prior results in linear algebra that show that some matrix columns are more informative than others [13], and Active Learning which shows that some labels are more informative that others [31].", "startOffset": 216, "endOffset": 220}, {"referenceID": 25, "context": "Active Learning largely studies the problem of label acquisition [31], and recently the links between Active Learning and Stochastic optimization have been studied [18].", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "Active Learning largely studies the problem of label acquisition [31], and recently the links between Active Learning and Stochastic optimization have been studied [18].", "startOffset": 164, "endOffset": 168}, {"referenceID": 23, "context": "Transfer Learning and Bias Mitigation: ActiveClean has a strong link to a field called Transfer Learning and Domain Adaptation [29].", "startOffset": 127, "endOffset": 131}, {"referenceID": 19, "context": "explored a calibration problem in which data was systematically corrupted [25] and proposed a rule-based technique for cleaning data.", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "[23]) have the same structure, systematically corrupted data that is feeding into a model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Secure Learning: ActiveClean is also related to work in adversarial learning [28], where the goal is to make models robust to adversarial data manipulation.", "startOffset": 77, "endOffset": 81}, {"referenceID": 29, "context": "This line of work has extensively studied methodologies for making models private to external queries and robust to malicious labels [36], but the data cleaning problem explores more general corruptions than just malicious labels.", "startOffset": 133, "endOffset": 137}], "year": 2016, "abstractText": "Data cleaning is often an important step to ensure that predictive models, such as regression and classification, are not affected by systematic errors such as inconsistent, out-of-date, or outlier data. Identifying dirty data is often a manual and iterative process, and can be challenging on large datasets. However, many data cleaning workflows can introduce subtle biases into the training processes due to violation of independence assumptions. We propose ActiveClean, a progressive cleaning approach where the model is updated incrementally instead of re-training and can guarantee accuracy on partially cleaned data. ActiveClean supports a popular class of models called convex loss models (e.g., linear regression and SVMs). ActiveClean also leverages the structure of a user\u2019s model to prioritize cleaning those records likely to affect the results. We evaluate ActiveClean on five real-world datasets UCI Adult, UCI EEG, MNIST, Dollars For Docs, and WorldBank with both real and synthetic errors. Our results suggest that our proposed optimizations can improve model accuracy by up-to 2.5x for the same amount of data cleaned. Furthermore for a fixed cleaning budget and on all real dirty datasets, ActiveClean returns more accurate models than uniform sampling and Active Learning.", "creator": "LaTeX with hyperref package"}}}