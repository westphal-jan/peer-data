{"id": "1601.03805", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2016", "title": "Matrix Neural Networks", "abstract": "Traditional neural networks assume vectorial inputs as the network is arranged as layers of single line of computing units called neurons. This special structure requires the non-vectorial inputs such as matrices to be converted into vectors. This process can be problematic. Firstly, the spatial information among elements of the data may be lost during vectorisation. Secondly, the solution space becomes very large which demands very special treatments to the network parameters and high computational cost. To address these issues, we propose matrix neural networks (MatNet), which takes matrices directly as inputs. Each neuron senses summarised information through bilinear mapping from lower layer units in exactly the same way as the classic feed forward neural networks. Under this structure, back prorogation and gradient descent combination can be utilised to obtain network parameters efficiently. Furthermore, it can be conveniently extended for multimodal inputs. We apply MatNet to MNIST handwritten digits classification and image super resolution tasks to show its effectiveness. Without too much tweaking MatNet achieves comparable performance as the state-of-the-art methods in both tasks with considerably reduced complexity.", "histories": [["v1", "Fri, 15 Jan 2016 03:33:35 GMT  (3636kb,D)", "http://arxiv.org/abs/1601.03805v1", "20 pages, 5 figures"], ["v2", "Fri, 9 Dec 2016 01:47:07 GMT  (3652kb,D)", "http://arxiv.org/abs/1601.03805v2", "20 pages, 5 figures"]], "COMMENTS": "20 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["junbin gao", "yi guo", "zhiyong wang"], "accepted": false, "id": "1601.03805"}, "pdf": {"name": "1601.03805.pdf", "metadata": {"source": "CRF", "title": "Matrix Neural Networks", "authors": ["Junbin Gao", "Yi Guo"], "emails": ["junbin.gao@sydney.edu.au", "y.guo@westernsydney.edu.au", "zhiyong.wang@sydney.edu.au"], "sections": [{"heading": "1 Introduction", "text": "It is one of the biggest junctions in the history of the European Union."}, {"heading": "2 Matrix Neural Network Model", "text": "The basic model of a MatNet layer is the following bilinear figure Y = \u03c3 (UXV T + B) + E, (2.1), where U, V, B and E are matrices with compatible dimensions, U and V are connection weights, B is the offset of the current layer, \u03c3 (\u00b7) is the activation function acting on each element of the matrix and E is the error."}, {"heading": "2.1 Network Structure", "text": "The MatNet consists of several layers of neurons in the form of (2.1) l = l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l,"}, {"heading": "2.2 Optimisation", "text": "We collect all the unknown variables, i.e. the network parameters of each layer here. They are U (l), V (l), B (l) for l = 1,.., L, and uk and tbk for the output layer. Write the parameters of each layer as \u0432 (l). From equivalent. (2,2) it is easy to see that the information is passed on in exactly the same way as the traditional paid neural networks. The underpinning mechanism is the bilinear mapping in (2,3), which preserves the matrix shape throughout the network. This suggests that the optimization used in traditional neural networks, i.e. the reverse propagation (BP) and the gradient descend combination for MatNet can be used. All we need to do is to obtain the derivation of the cost function w.r.t (l), which the network can be passed backwards."}, {"heading": "2.3 Regularisation", "text": "Although MatNet has greatly reduced the solution space by already using bilinear mapping in (2,3), some techniques that are routinely used in traditional neural networks can still be used to restrict the solution in the direction of the desired pattern. Firstly, weight decay, i.e. holding the size of the weights to the connections, mainly U (l) and V (l). 2F), where it is a non-negative regularization parameter and the sum of the Frobenius norms also includes the output layer, i.e. the size of the weights to the connections, mainly U (l) and V (l). 2F), is usually used for this purpose, i.e. the integration of the weight decay, i.e. one can immediately think of the savings constraints on the weights, in order to cut off some connections between the elements that are similar to the DropConnect in [36], it turns out that savings layer is not promoted by the norms."}, {"heading": "3 Multimodal Matrix Neural Networks", "text": "We have the basics of MatNet from the discussion above. Now, we move on to extending MatNet to multimodal cases for applying the superresolution of the image. (The expansion is so simple that it contains more than one input matrix simultaneously on the input layer.) Conceptually, we have more than one input layer side by side for different modalities, and they send all the information to the common hidden layers through separate connections [22]. It turns out that the input layer is sufficient for three layers of MatNet, i.e., input layer, hidden layer and output layer, and it works in auto-encoder [14] mode, which means a regression MatNet that reproduces the input layer in the output layer. This requires that the output layer has the same amount of modalities as the input layer. Although we only have a three-layer regression of multimodal MatNet layer and output layer, it is not difficult to extend the output layer to the multimodal layer in any other way."}, {"heading": "4 Experimental Evaluation", "text": "In this section, we apply MatNet to the handwritten classification of the MNIST digits and the superresolution of the image. The network settings are somewhat arbitrary, or in other words, we did not optimize the number of layers and neurons in each layer in these tests. For handwritten number recognition, MatNet was configured as a classification network, i.e. the output layer was a vector of Softmax functions as in Equation (2.6) and (2.7) of length 10 (for 10 digits).It contained 2 hidden layers of 20 x 20 and 16 x 16 neurons respectively. As the number of layers and neurons was very conservative, we turned off the thrift restriction. MatNet was on board with a manifold restriction of the unit of measurement. Therefore, we excluded the removal of weights. For the superresolution task, the only hidden layer of size 10 x 10, so only 3 layer was the net activation function in both networks."}, {"heading": "4.1 MNIST Handwritten Digits Classification", "text": "The MNIST database is available at http: / / yann.lecun.com / exdb / mnist /. The entire database contains 60,000 training samples and 10,000 test samples, and each digit is a 28 x 28 grayscale image. We use all training samples to model and test all test samples. Figure 1 shows the weights, U (l) and V (l) and the bias B (l) in hidden layers. Figure 2 shows the first 100 test digits and hidden shift outputs. Checkboard effects can be seen from the hidden shift output in Figure 2 (b). The final classification rate is 97.3%, i.e. a 2.7% error rate, which is below the best MNIST performance of DropConnect with a 0.21% error rate."}, {"heading": "4.2 Image Super Resolution", "text": "For image superresolution, we must then use the multimodal MatNet, which is detailed in section 3. Training is the following: From a series of high-resolution images, each of which we stamped down by bicubic interpolation to the ratio of 1 / s, where s is the goal to increase the scaling factor. In this experiment, s = 2. From these images scaled down, we have scanned patches, say 15, from their feature images, i.e. first and second derivatives along x and y directions, 4 feature images for each. These are the modalities from X2 to X5. We have also scanned the same size patches from the original high-resolution images as X1. See Eq. (3.1). This data was fed into multimodal MatNet for training. To obtain a high-resolution image, we used the following method. We have scanned the image by Bikubicubic interpolation to the ratio of high-resolution image CbYb and then the CbYb space are the CbYe light component."}, {"heading": "5 Discussion", "text": "The most striking advantage of MatNet over traditional vector-based neural work is that it drastically reduces the complexity of the optimization problem while achieving a performance comparable to that of the most modern methods. This was demonstrated in the application of MNIST handwritten number classification and image superresolution. As we have mentioned several times in the text, MatNet was not specifically optimized for the tasks we showed in the experiment section. There is a lot of potential for further improvements. Many techniques used for deep networks can easily be applied to MatNet with appropriate adaptation, such as reLU activation, max pooling, etc., which will certainly become our future research."}, {"heading": "6 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Backpropagation Algorithm for Regression", "text": "We work out the derived formulas for all parameters (2.2), so that we can see that l = l (1), l (2), l (2), l (2), l (2), l (1), l (2), l (1), l (1), l (2), l (1), l (2), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1), l, l (1, l, 1, l, l, l, l (1, 1, 1, l, 1, l (1), l (1), l (1), l (1), l (1), l (1), l (1), l (1, l (1), l (1), l (1), l (1, l (1), l (1), l (1, l), l (1), l (1, l (1, l), l (1, l), l (1), l (1, l (1), l (1), l (1), l (1), l (1, l (1), l (1), l (1), l (1), l (1, l (1, l), l (1, l (1, l), l (1), l (1), l (1, l (1), l (1), l (1), l (1, l (1), l (1), l (1, l (1), l (1), l (1, l (1), l (1), l (1), l (1, l, l (1), l (1, l (1), l, l (1), l (1), l (1,"}, {"heading": "6.2 Backpropagation Algorithm for Classification", "text": "The only difference between regression and classification mnnet is in the last layer where the output at layer L + 1 is a vector of dimension K. This is the connection between this output layer and layer L between a vector and the matrix variable X (L) of dimensions IL \u00b7 JL.After (2.7) we have the following two cases for calculating the output layer N (L) nk: Case 1: k = k (L) nk = (L) nk = (K) nk (K) nk = (K) nk (N) nk (K) nk (N) nk (K) \u2212 exp (N) nk) exp (N) exp (K) exp (K) exp (K) exk (K) exk (N) exk (N) exk (K) exk (K) exk (n) exk (n) exk (n) exk (n) exk (K (n) exk (K) exk (n) exp (n) n (K (K) exk (n) exp (n) n (K (K) exp (n) n (K (K) exp (n) n (K (K) exp (n) n (K (n) n) exp (K (n) nk (n) nk (n) nk) nk (n) nk (n) nk (n) nk (n) nk) nk (n) nk (n) nk (n) nk (n) nk (n) nk (n) nk (n) nk) nk (n) nk (n) nk (n) nk (K (n) nk (n) nk (n) nk (nk (n) (n) nk (n) nk (K (n) nk (n) (nk (n) nk (n) (n) nk (n) (n) nk (K (n)."}, {"heading": "6.3 Sparsity", "text": "R-R = R-R = R-R = R = R = R = R + R + R + R + R + R + R = R + R = R + R = R + R = R + R = R + R = R = R + R = R = R + R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R = R"}, {"heading": "6.4 BP Algorithm for Multimodal MatNet Autoencoder", "text": "To train the multimodal MatNet autocoder, we need to work out the derivatives of L taking into account all the parameters. First, we define the derivative of L with respect to the variables of the output layer \u03b42ij = X-j i-X-j i.Now, we propagate these derivatives from the output layer to the hidden layer according to the network structure and define them as \"1i = D-j\" = 1 \"Sj (2 ij-p)\" (RjYiSTj + Cj)) RTj = D-j = 1 \"RTj\" (2 ij-p \"(X-j i))) SjThen it is not difficult to prove that the implementation of L-p = 1N-p (X-j formula)\" i-p \"= 1 (X-j-j-j-i)\" (X-j-j-i) \") SjY T i (6,20) and the development of L-j-p (X-j-p) and the development of L-j-p (p)."}, {"heading": "6.5 Sparsity in Multimodal MatNet Autoencoder", "text": "E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E"}], "references": [{"title": "Pedestrian detection with a large-field-of-view deep network", "author": ["Anelia Angelova", "Alex Krizhevsky", "Vincent Vanhoucke"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Symmetry-invariant optimization in deep networks", "author": ["V. Badrinarayanan", "B. Mishra", "R. Cipolla"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Deep networks for motor control functions", "author": ["Max Berniker", "Konrad P Kording"], "venue": "Frontiers in computational neuroscience,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)", "author": ["Thomas M. Cover", "Joy A. Thomas"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Le Cun", "B Boser", "John S Denker", "D Henderson", "Richard E Howard", "W Hubbard", "Lawrence D Jackel"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1990}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Misha Denil", "Loris Bazzani", "Hugo Larochelle", "Nando de Freitas"], "venue": "Neural Computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "From deep learning to episodic memories: Creating categories of visual experiences", "author": ["Jigar Doshi", "Zsolt Kira", "Alan Wagner"], "venue": "In Proceedings of the Third Annual Conference on Advances in Cognitive Systems ACS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "An improved implementation of the lbfgs algorithm for automatic history matching", "author": ["Guohua Gao", "Albert C Reynolds"], "venue": "In SPE Annual Technical Conference and Exhibition. Society of Petroleum Engineers,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Introduction to special issue on neuromorphic computing", "author": ["Dan Hammerstrom", "Vijaykrishnan Narayanan"], "venue": "ACM Journal on Emerging Technologies in Computing Systems (JETC),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "P.R. Salakhutdinov"], "venue": "Science, 313:504\u2013507,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Tensor principal component analysis via sum-of-squares proofs", "author": ["Samuel B Hopkins", "Jonathan Shi", "David Steurer"], "venue": "arXiv preprint arXiv:1507.03269,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Applied logistic regression, volume 398", "author": ["David W Hosmer Jr.", "Stanley Lemeshow", "Rodney X Sturdivant"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["Jianxiong Xiao"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML-", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y. Ng"], "venue": "In Proceedings of the Twenty-Eighth International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data", "author": ["P. Paatero", "U. Tapper"], "venue": "values. Environmetrics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1994}, {"title": "A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128k synapses", "author": ["Ning Qiao", "Hesham Mostafa", "Federico Corradi", "Marc Osswald", "Fabio Stefanini", "Dora Sumislawska", "Giacomo Indiveri"], "venue": "Frontiers in neuroscience,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Rodieck. The vertebrate retina: principles of structure and function", "author": ["W Robert"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1973}, {"title": "The perceptron - a perceiving and recognizing automaton", "author": ["Frank Rosenblatt"], "venue": "Technical report, Cornell Aeronautical Laboratory,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1957}, {"title": "The brain", "author": ["Robert F. Service"], "venue": "chip. Science,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Sparse autoencoders for word decoding from magnetoencephalography", "author": ["Michelle Shu", "Alona Fyshe"], "venue": "In Proceedings of the third NIPS Workshop on Machine Learning and Interpretation in NeuroImaging (MLINI),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Deep learning in diagnosis of brain disorders", "author": ["Heung-Il Suk", "Dinggang Shen"], "venue": "In Recent Progress in Brain and Cognitive Engineering,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["Robert Tibshirani"], "venue": "Journal of Royoal Statistical Society,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "Creation of a deep convolutional auto-encoder in caffe", "author": ["Volodymyr Turchenko", "Artur Luczak"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Aiding drug design with deep neural networks", "author": ["Thomas Unterthiner", "Andreas Mayr", "G\u00fcnter Klambauer", "Marvin Steijaert", "J\u00f6rg K Wegner", "Hugo Ceulemans", "Sepp Hochreiter"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Deep learning for drug target prediction", "author": ["Thomas Unterthiner", "Andreas Mayr", "G\u00fcnter Klambauer", "Marvin Steijaert", "J\u00f6rg K Wegner", "Hugo Ceulemans", "Sepp Hochreiter"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Common subspace for model and similarity: Phrase learning for caption generation from images", "author": ["Yoshitaka Ushiku", "Masataka Yamaguchi", "Yusuke Mukuta", "Tatsuya Harada"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "The Nature of Statistical Learning", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1995}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Encoding voxels with deep learning", "author": ["Panqu Wang", "Vicente Malave", "Ben Cipollini"], "venue": "The Journal of Neuroscience,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "The emergence of face-selective units in a model that has never seen a face.yaminscohenhongkanwisherdicarlo2015", "author": ["Daniel Yamins", "Michael Cohen", "Ha Hong", "Nancy Kanwisher", "James Di- Carlo"], "venue": "Journal of vision,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Image super-resolution via sparse representation", "author": ["Jianchao Yang", "John Wright", "Thomas Huang", "Yi Ma"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Neural self talk: Image understanding via continuous questioning and answering", "author": ["Yezhou Yang", "Yi Li", "Cornelia Fermuller", "Yiannis Aloimonos"], "venue": "arXiv preprint arXiv:1512.03460,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Fast nonnegative matrix/tensor factorization based on low-rank approximation", "author": ["Guoxu Zhou", "A Cichocki", "Shengli Xie"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "1 Introduction Neural networks especially deep networks [13, 19] have attracted a lot of attention recently due to their superior performance in several machine learning tasks such as face recognition, image understanding and language interpretation.", "startOffset": 56, "endOffset": 64}, {"referenceID": 0, "context": "The applications of neural netowrks go far beyond artificial intelligence domain, stretching to autonomous driving systems [1, 18], pharmaceutical", "startOffset": 123, "endOffset": 130}, {"referenceID": 17, "context": "The applications of neural netowrks go far beyond artificial intelligence domain, stretching to autonomous driving systems [1, 18], pharmaceutical", "startOffset": 123, "endOffset": 130}, {"referenceID": 30, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 9, "endOffset": 17}, {"referenceID": 31, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 9, "endOffset": 17}, {"referenceID": 4, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 32, "endOffset": 51}, {"referenceID": 9, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 32, "endOffset": 51}, {"referenceID": 27, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 32, "endOffset": 51}, {"referenceID": 35, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 32, "endOffset": 51}, {"referenceID": 36, "context": "research [32, 33], neuroscience [5, 10, 29, 37, 38] among others.", "startOffset": 32, "endOffset": 51}, {"referenceID": 16, "context": "Because of its usefulness and tremendous application potential, some open source software packages are made available for research such as caffe [17, 31] and Theano [4].", "startOffset": 145, "endOffset": 153}, {"referenceID": 29, "context": "Because of its usefulness and tremendous application potential, some open source software packages are made available for research such as caffe [17, 31] and Theano [4].", "startOffset": 145, "endOffset": 153}, {"referenceID": 3, "context": "Because of its usefulness and tremendous application potential, some open source software packages are made available for research such as caffe [17, 31] and Theano [4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 11, "context": "Furthermore, there are even efforts to build integrated circuits for neural networks [12, 24, 27].", "startOffset": 85, "endOffset": 97}, {"referenceID": 22, "context": "Furthermore, there are even efforts to build integrated circuits for neural networks [12, 24, 27].", "startOffset": 85, "endOffset": 97}, {"referenceID": 25, "context": "Furthermore, there are even efforts to build integrated circuits for neural networks [12, 24, 27].", "startOffset": 85, "endOffset": 97}, {"referenceID": 24, "context": "Evolving from the simplest perceptron [26] to the most sophisticated deep learning neural networks [19], the basic structure of the most widely used neural networks remains almost the same, i.", "startOffset": 38, "endOffset": 42}, {"referenceID": 5, "context": "hierarchical layers of computing units (called neurons) with feed forward information flow from previous layer to the next layer [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 33, "context": "This is the well known model complexity against learning capacity dilemma [35].", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "This is an analogy to the neurons in retina sensing visual signal which are organised in layers of matrix like formation [25].", "startOffset": 121, "endOffset": 125}, {"referenceID": 7, "context": "It is worth of pointing out that the convolutional neural network (ConvNet) [8, 20] works on images (matrices) directly.", "startOffset": 76, "endOffset": 83}, {"referenceID": 18, "context": "It is worth of pointing out that the convolutional neural network (ConvNet) [8, 20] works on images (matrices) directly.", "startOffset": 76, "endOffset": 83}, {"referenceID": 19, "context": "sigmoid, tanh, and rectified linear unit (reLU) [21] to generate its output for the next layer.", "startOffset": 48, "endOffset": 52}, {"referenceID": 38, "context": "As we will show in Section 3, this process is straightforward with great possibility to embrace other modalities such as natural languages for image understanding [40] and automated caption generation [34].", "startOffset": 163, "endOffset": 167}, {"referenceID": 32, "context": "As we will show in Section 3, this process is straightforward with great possibility to embrace other modalities such as natural languages for image understanding [40] and automated caption generation [34].", "startOffset": 201, "endOffset": 205}, {"referenceID": 37, "context": "Surprisingly for super resolution task, MatNet has superior results already in terms of peak signal to noise ratio (PSNR) compared to the state-of-the-art methods such as the sparse representation (SR) [39].", "startOffset": 202, "endOffset": 206}, {"referenceID": 15, "context": "4) is the softmax that is frequently used in logistic regression [16].", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "This certainly connects MatNet to matrix or tensor factorisation type of algorithms such as principal component analysis [15, 23, 41] broadening the understanding of MatNet.", "startOffset": 121, "endOffset": 133}, {"referenceID": 21, "context": "This certainly connects MatNet to matrix or tensor factorisation type of algorithms such as principal component analysis [15, 23, 41] broadening the understanding of MatNet.", "startOffset": 121, "endOffset": 133}, {"referenceID": 39, "context": "This certainly connects MatNet to matrix or tensor factorisation type of algorithms such as principal component analysis [15, 23, 41] broadening the understanding of MatNet.", "startOffset": 121, "endOffset": 133}, {"referenceID": 10, "context": "Once the gradients are computed, then any gradient descent algorithm such as the limited memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS) [11] can be readily used to find the sub-optimum given an initialisation.", "startOffset": 137, "endOffset": 141}, {"referenceID": 34, "context": "One may immediately think of the sparsity constraint on the weights to cut off some connections between layers similar to the DropConnect in [36].", "startOffset": 141, "endOffset": 145}, {"referenceID": 28, "context": "It turns out that it is not trivial to incorporate sparsity constraint manifested by sparsity encouraging norms such as `1 norm favourably used in sparse regressions [30].", "startOffset": 166, "endOffset": 170}, {"referenceID": 34, "context": "The dropping in [36] in implemented by a 0/1 mask sampled from Bernoulli distribution.", "startOffset": 16, "endOffset": 20}, {"referenceID": 26, "context": "Through (approximately) enforcing the constraint elementwise \u03c1 (l) ij = \u03c1, one can achieve sparsity in reducing the number of neurons [28].", "startOffset": 134, "endOffset": 138}, {"referenceID": 6, "context": "The deviation is quantified as the following akin to Kullback-Leibler divergence or entropy [7]:", "startOffset": 92, "endOffset": 95}, {"referenceID": 20, "context": "Conceptually, we have more than one input layer standing side by side for different modalities and they all send the information to the shared hidden layers through separate connections [22].", "startOffset": 186, "endOffset": 190}, {"referenceID": 13, "context": ", input layer, hidden layer and output layer, and it works on autoencoder [14] mode meaning a regression MatNet reproducing the input in output layer.", "startOffset": 74, "endOffset": 78}, {"referenceID": 37, "context": "We applied MatNet to the data set used in SR [39], both for training and testing.", "startOffset": 45, "endOffset": 49}], "year": 2017, "abstractText": "Traditional neural networks assume vectorial inputs as the network is arranged as layers of single line of computing units called neurons. This special structure requires the non-vectorial inputs such as matrices to be converted into vectors. This process can be problematic. Firstly, the spatial information among elements of the data may be lost during vectorisation. Secondly, the solution space becomes very large which demands very special treatments to the network parameters and high computational cost. To address these issues, we propose matrix neural networks (MatNet), which takes matrices directly as inputs. Each neuron senses summarised information through bilinear mapping from lower layer units in exactly the same way as the classic feed forward neural networks. Under this structure, back prorogation and gradient descent combination can be utilised to obtain network parameters efficiently. Furthermore, it can be conveniently extended for multimodal inputs. We apply MatNet to MNIST handwritten digits classification and image super resolution tasks to show its effectiveness. Without too much tweaking MatNet achieves comparable performance as the state-of-the-art methods in both tasks with considerably reduced complexity.", "creator": "LaTeX with hyperref package"}}}