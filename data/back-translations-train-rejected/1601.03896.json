{"id": "1601.03896", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2016", "title": "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures", "abstract": "Automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities. In this survey, we classify the existing approaches based on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space. We provide a detailed review of existing models, highlighting their advantages and disadvantages. Moreover, we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machine-generated image descriptions. Finally we extrapolate future directions in the area of automatic image description generation.", "histories": [["v1", "Fri, 15 Jan 2016 12:50:32 GMT  (4282kb)", "http://arxiv.org/abs/1601.03896v1", "To appear in JAIR"], ["v2", "Mon, 24 Apr 2017 09:47:20 GMT  (4282kb)", "http://arxiv.org/abs/1601.03896v2", "Journal of Artificial Intelligence Research 55, 409-442, 2016"]], "COMMENTS": "To appear in JAIR", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["raffaella bernardi", "ruket cakici", "desmond elliott", "aykut erdem", "erkut erdem", "nazli ikizler-cinbis", "frank keller", "adrian muscat", "barbara plank"], "accepted": false, "id": "1601.03896"}, "pdf": {"name": "1601.03896.pdf", "metadata": {"source": "CRF", "title": "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures", "authors": ["Raffaella Bernardi", "Ruket Cakici", "Desmond Elliott", "Aykut Erdem", "Nazli Ikizler-Cinbis", "Frank Keller", "Adrian Muscat", "Barbara Plank"], "emails": ["bernardi@disi.unitn.it", "ruken@ceng.metu.edu.tr", "d.elliott@uva.nl", "aykut@cs.hacettepe.edu.tr", "erkut@cs.hacettepe.edu.tr", "nazli@cs.hacettepe.edu.tr", "keller@inf.ed.ac.uk", "adrian.muscat@um.edu.mt", "bplank@cst.dk"], "sections": [{"heading": null, "text": "ar Xiv: 160 1,03 89"}, {"heading": "1. Introduction", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "2. Image Description Models", "text": "An image description can be analyzed in several dimensions (Shatford, 1986; Jaimes & Chang, 2000).We follow Hodosh, Young, and Hockenmaier (2013) and assume that the descriptions of interest in this survey article verbalize the visual and conceptual information presented in the image, i.e. descriptions referring to the units depicted, their attributes and relationships, and the actions in which they are participation.Outside the scope of automatic image description, non-visual descriptions that provide background information or refer to objects that are not depicted in the image (e.g. the place where the image was taken or who took the image) are also not relevant to common approaches to image description, perceptual descriptions that reflect global visual features at a low level (e.g. the predominant color in the image or the type of media such as photography, drawing, animation, etc.)."}, {"heading": "2.1 Description as Generation from Visual Input", "text": "The general approach of the studies in this group is to first predict the most likely meaning of a particular image by analyzing its visual content and then generate a sentence that reflects that meaning. All models in this category achieve this using the following general pipeline architecture: 1. Computer vision techniques are used to classify the type of scene, to recognize the objects present in the image, their attributes and the relationships that exist between them, and to recognize the actions that take place on the spot. 2. There follows a generational phase in which the detector exits are transformed into words or phrases, which are then combined to produce a natural language description of the image, using techniques derived from the production of natural language (e.g. templates, grammars, grammars, grammars, grammars). The approaches examined in this section include an explicit representation of descriptions that are distinguished and implied by the language models described in Section 2.2 and 2.3."}, {"heading": "2.2 Description as a Retrieval in Visual Space", "text": "The studies in this group pose the problem of automatically generating the description of an image according to certain rules or schemes. An approach that refers to the query image (i.e., the new image to be described); this is illustrated in Figure 2. In other words, these systems use similarity in visual space to transfer descriptions to the query image. Compared to models that generate descriptions directly (Section 2.1), retrieval models typically require a large amount of training data to provide relevant descriptions. In terms of their algorithmic components, visual approaches typically follow a pipeline of three main steps: 1. Represent the given query image with specific visual properties. Retrieve a set of training images based on similarity in feature space. 3. Re-rank the descriptions of candidate images by continuing to use visual and / textual information contained in the retrieval set."}, {"heading": "2.3 Description as a Retrieval in Multimodal Space", "text": "This year, it has come to the point where it is only a matter of time before a result is achieved."}, {"heading": "2.4 Comparison of Existing Approaches", "text": "The discussion in the previous subsections makes it clear that each approach to image description has its particular strengths and weaknesses. For example, the methods that view the task as a generational problem (Section 2.1) have an advantage over other approaches, as they can produce new sentences to describe a particular image. However, their success depends greatly on how accurately they assess the visual content and how well they are able to verbalize that content. Specifically, they explicitly use computer vision techniques to predict the most likely meaning of a particular image; these methods have limited accuracy in practice, so they are unable to identify the most important objects and their attributes, then no valid description can be generated."}, {"heading": "3. Datasets and Evaluation", "text": "There is a wide range of data sets for automatic image description research. The images in these data sets are linked to text descriptions and differ from each other in certain aspects such as the size, format of the descriptions and the way the descriptions were collected. At this point, we review common approaches to data collection, the data sets themselves, and evaluation yardsticks for comparing generated descriptions with relevant texts. The data sets are summarized in Table 2, and examples of images and descriptions are in Figure 5. Readers can also refer to the data set collection by Ferraro, Mostafazadeh, Huang, Vanderwende, Devlin, Galley, and Mitchell (2015) for an analysis similar to ours. It provides a basic comparison of some of the existing language and image data sets. It is not limited to automatic image descriptions and reports some simple statistics and quality metrics such as perplexity, syntactical complexity, and abstract word ratios."}, {"heading": "3.1 Image-Description Datasets", "text": "This medium-scale dataset consists of 1,000 images taken by people on Amazon Mechanical Turk (AMT) Service.The Visual and Linguistic Treebank, 2010) and includes objects from different visual classes, such as humans, animals and vehicles. Each image is associated with five descriptions provided by people on Amazon Mechanical Turk (AMT) Service.The Visual and Linguistic Treebank, 2010) and includes objects from different visual classes, such as humans, animals and vehicles. Each image is associated with five descriptions provided by people on Amazon Mechanical Turk (AMT) Service.The Visual and Linguistic Treebank Treebank (VLT2K). Elliott & Keller, 2013 makes use of images from the Pascal 2010 Action Recognition Dataset Dataset. It auctions these images with three, twin descriptions per image. These descriptions were collected on AMT."}, {"heading": "3.2 Image-Caption Datasets", "text": "Image descriptions verbalize what is shown in the image, i.e. they refer to the objects, actions and attributes that are depicted, mention the type of scene, etc. Image captions, on the other hand, are typically associated with images, which contain information that is not shown in the picture. http: / / www.facebook.com / Image Collection / http: / / http: / / www.facebook.com / Image Collection / http: / / www.facebook.com / Image Collection / Images in a newspaper or museum typically contain cultural or historical texts, i.e. captions that do not contain descriptions.The BBC News dataset (Feng & Lapata, 2008) was one of the earliest image and text collections. Images in a newspaper or museum typically contain cultural or historical texts, i.e. captions that do not describe description.The BBC News dataset (Feng & Lapata, 2008) was one of the earliest collections of images and joint news articles, typically included cultural or historical texts, i.e. captions that do not describe description.The BBC News dataset (Feng & Lapata, 2008) was one of the earliest collections of images and text collections of images, and the Feng & Lapata website contains a restriction of the News Corporation, and a restriction of the News Corporation, which included a restriction of the news item (Feng & Lapata, 2008)."}, {"heading": "3.3 Collecting Datasets", "text": "The images for these datasets were obtained either from an existing task in the Computer Vision community - the Pascal Challenge (Everingham et al., 2010) was used for the Pascal1K and VLT2K datasets - directly from Flickr, in the case of Flickr8K / 30K, MS COCO, SBU1M Captions and De'ja Image Captions datasets, or crowdsourced, in the case of the Abstract Scenes dataset. Captions in caption texts are usually obtained from Amazon Mechanical Turk or Crowdflower; while the texts in caption datasets are obtained from photo sharing websites, such as Flickr, or from news providers. Captions are usually collected without financial incentive because they are written by those sharing their own images, or by journalists. Crowd sourcing datasets are the descriptions of workers performing a simple task (patching) or are created by news providers."}, {"heading": "3.4 Evaluation Measures", "text": "Assessing the results of a natural language generation (NLG) is a fundamentally difficult task (GE & White, 2007; Reiter & Belz, 2009).The most common method of assessing the quality of automatically generated text is subjective evaluation by human experts. (2013) Text is typically judged in terms of grammar and content, indicating how syntactically correct and relevant the text is, or how relevant the image descriptions are to images. (2013) Text is also sometimes tested, especially when a surface realization technique is involved in the generation.Automatically generated descriptions of images can be evaluated using the same NLG techniques.Judges are provided with the image as well as the description during the evaluation assignments.Subjective human evaluations of machine-generated image descriptions are often performed using questions."}, {"heading": "4. Future Directions", "text": "As this survey shows, the CV and NLP community is seeing an increasing interest in automatic image description systems. Recent advances in deep learning models for images and text have seen significant improvements in the quality of automatically generated descriptions. However, a number of challenges remain for image description research. Below, we discuss future directions from which this research direction is likely to benefit."}, {"heading": "4.1 Datasets", "text": "The earliest work on image description used relatively small datasets (Farhadi et al., 2010; Kulkarni et al., 2011; Elliott & Keller, 2013). Recently, the introduction of Flickr30K, MS COCO, and other large datasets enabled the formation of more complex models such as neural networks. Nevertheless, the area is likely to benefit from larger and more diversified datasets that share a common, unified, comprehensive vocabulary. Vinyals et al. (2015) argue that learning an MS COCO model and applying it to datasets collected in different environments such as SBU1M Captions or Pascal1K significantly impairs performance and does not make transfer learning between datasets as effective as expected. They show that learning an MS COCO model and applying it to datasets collected in different environments such as SBU1M Captions or Pascal1K is likely to be more comprehensive than data exchange."}, {"heading": "4.2 Measures", "text": "The development of automated metrics that can mimic human judgments in assessing the suitability of image descriptions is perhaps the most urgent need in the field of image description (Elliott & Keller, 2014), a need that can be dramatically observed in the recent evaluation results of the MS COCO Challenge. According to existing metrics, including the most recent CIDER measurement (Vedantam et al., 2015), several automated metrics exceed the human upper limit (this upper limit shows how similar human descriptions are to each other), but the counterintuitive nature of this result is confirmed by the fact that when human assessments are used for evaluation, the results of even the best system are judged to be worse for most of the time than a human-generated description (Fang et al., 2015). However, since conducting human judgement experiments is costly, there is a great need for improved automated metrics that correlate more strongly with human judgments than human assessments (he does not show an optimum BL7 ECHEK)."}, {"heading": "4.3 Diversity and Originality", "text": "This situation has been demonstrated by Devlin et al. (2015), who show that their best model can only generate 47.0% unambiguous descriptions. Systems that generate diverse and original descriptions that not only repeat what they have already seen, but also infer the underlying semantics, therefore remain an open challenge. Chen and Zitnick (2015) and related approaches take a step toward these limitations by introducing description and visual generations of representation.Jas and Parikh (2015) introduce the notion of image specificity, arguing that the domain of image descriptions is not uniform, certain images are more specific than others. Descriptions of non-specific images tend to vary greatly, as people tend to describe an unspecific scene from different angles. This notion and its effects should be examined in detail."}, {"heading": "4.4 Further Tasks", "text": "Another open challenge is visual question-answering (VQA). While natural language question-answering based on texts has long been an important goal of NLP research (e.g. Liang, Jordan, & Klein, 2012; Fader, Zettlemoyer, & Etzioni, 2013; Richardson, Burges, & Renshaw, 2013; Fader, Zettlemoyer, & Etzioni, 2014), answering questions about images is a recent task."}, {"heading": "5. Conclusions", "text": "In this study, we discuss recent advances in automatic image description and closely related issues. We review and analyze much of the existing work by highlighting common features and differences between the existing research work. In particular, we categorize the related work into three groups: (i) the direct generation of descriptions from images, (i) the retrieval of images from a visual space, and (iii) the retrieval of images from multimodal (shared visual and linguistic) space. In addition, we provided a brief overview of the existing corpora and automatic evaluation measures, and discussed some future directions for visual and language research. Compared to conventional keyword-based image descriptions (using object recognition, attribute recognition, scene labeling, etc.), automatic image description systems generate more human explanations of visual content that provide a more complete picture of the scene. Advances in this area could lead to smarter artificial image processing systems (using object recognition, attribution recognition, scene labeling, etc.), which in turn could interact with the scenes generated by the environment, and therefore make inferences about the environment."}], "references": [{"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": null, "citeRegEx": "Banerjee and Lavie,? \\Q2005\\E", "shortCiteRegEx": "Banerjee and Lavie", "year": 2005}, {"title": "Automatic attribute discovery and characterization from noisy web data", "author": ["T.L. Berg", "A.C. Berg", "J. Shih"], "venue": "In European Conference on Computer Vision", "citeRegEx": "Berg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Berg et al\\.", "year": 2010}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference", "citeRegEx": "Chatfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "D\u00e9j\u00e0 image-captions: A corpus of expressive descriptions in repetition", "author": ["J. Chen", "P. Kuznetsova", "D. Warren", "Y. Choi"], "venue": "In North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Chen and Zitnick,? \\Q2015\\E", "shortCiteRegEx": "Chen and Zitnick", "year": 2015}, {"title": "Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation: Position Papers", "author": ["R. Dale", "White", "M.E. (Eds"], "venue": null, "citeRegEx": "Dale et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dale et al\\.", "year": 2007}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["M. Denkowski", "A. Lavie"], "venue": "In Conference of the European Chapter of the Association for Computational Linguistics Workshop on Statistical Machine Translation", "citeRegEx": "Denkowski and Lavie,? \\Q2014\\E", "shortCiteRegEx": "Denkowski and Lavie", "year": 2014}, {"title": "Language Models for Image Captioning: The Quirks and What Works", "author": ["J. Devlin", "H. Cheng", "H. Fang", "S. Gupta", "L. Deng", "X. He", "G. Zweig", "M. Mitchell"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Devlin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Describing images using inferred visual dependency representations", "author": ["D. Elliott", "A.P. de Vries"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Elliott and Vries,? \\Q2015\\E", "shortCiteRegEx": "Elliott and Vries", "year": 2015}, {"title": "Image Description using Visual Dependency Representations", "author": ["D. Elliott", "F. Keller"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Elliott and Keller,? \\Q2013\\E", "shortCiteRegEx": "Elliott and Keller", "year": 2013}, {"title": "Comparing Automatic Evaluation Measures for Image Description", "author": ["D. Elliott", "F. Keller"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Elliott and Keller,? \\Q2014\\E", "shortCiteRegEx": "Elliott and Keller", "year": 2014}, {"title": "Query-by-Example Image Retrieval using Visual Dependency Representations", "author": ["D. Elliott", "V. Lavrenko", "F. Keller"], "venue": "In International Conference on Computational Linguistics", "citeRegEx": "Elliott et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2014}, {"title": "The PASCAL Visual Object Classes (VOC) Challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "Paraphrase-driven learning for open question answering", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining", "citeRegEx": "Fader et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Fang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "In European Conference on Computer Vision", "citeRegEx": "Farhadi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2010}, {"title": "Automatic Image Annotation Using Auxiliary Text Information", "author": ["Y. Feng", "M. Lapata"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Feng and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Feng and Lapata", "year": 2008}, {"title": "Automatic caption generation for news images", "author": ["Y. Feng", "M. Lapata"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Feng and Lapata,? \\Q2013\\E", "shortCiteRegEx": "Feng and Lapata", "year": 2013}, {"title": "A survey of current datasets for vision and language research", "author": ["F. Ferraro", "N. Mostafazadeh", "T. Huang", "L. Vanderwende", "J. Devlin", "M. Galley", "M. Mitchell"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Ferraro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ferraro et al\\.", "year": 2015}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "A. Yuille"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Gao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2015}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Geman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Geman et al\\.", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Improving ImageSentence Embeddings Using Large Weakly Annotated Photo Collections", "author": ["Y. Gong", "L. Wang", "M. Hodosh", "J. Hockenmaier", "S. Lazebnik"], "venue": "In European Conference on Computer Vision", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "The IAPR TC-12 benchmark: A new evaluation resource for visual information systems", "author": ["M. Grubinger", "P. Clough", "H. Muller", "T. Deselaers"], "venue": "In International Conference on Language Resources and Evaluation", "citeRegEx": "Grubinger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Grubinger et al\\.", "year": 2006}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["S. Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Guadarrama et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guadarrama et al\\.", "year": 2013}, {"title": "Choosing linguistics over vision to describe images", "author": ["A. Gupta", "Y. Verma", "C.V. Jawahar"], "venue": "In AAAI Conference on Artificial Intelligence", "citeRegEx": "Gupta et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2012}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "venue": "Neural Computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Sentence-based image description with scalable, explicit models", "author": ["M. Hodosh", "J. Hockenmaier"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition Workshops", "citeRegEx": "Hodosh and Hockenmaier,? \\Q2013\\E", "shortCiteRegEx": "Hodosh and Hockenmaier", "year": 2013}, {"title": "Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": null, "citeRegEx": "Hotelling,? \\Q1936\\E", "shortCiteRegEx": "Hotelling", "year": 1936}, {"title": "A conceptual framework for indexing visual information at multiple levels", "author": ["A. Jaimes", "Chang", "S.-F"], "venue": "In IST SPIE Internet Imaging", "citeRegEx": "Jaimes et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Jaimes et al\\.", "year": 2000}, {"title": "Image specificity", "author": ["M. Jas", "D. Parikh"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Jas and Parikh,? \\Q2015\\E", "shortCiteRegEx": "Jas and Parikh", "year": 2015}, {"title": "Guiding the long-short term memory model for image caption generation", "author": ["X. Jia", "E. Gavves", "B. Fernando", "T. Tuytelaars"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Jia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2015}, {"title": "Image retrieval using scene graphs", "author": ["J. Johnson", "R. Krishna", "M. Stark", "Li", "L.-J", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Karpathy and Fei.Fei,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Fei.Fei", "year": 2015}, {"title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Towards coherent natural language description of video streams", "author": ["M.U.G. Khan", "L. Zhang", "Y. Gotoh"], "venue": "In International Conference on Computer Vision Workshops", "citeRegEx": "Khan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2011}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "In Advances in Neural Information Processing Systems Deep Learning Workshop", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Generating Natural-Language Video Descriptions Using Text-Mined Knowledge", "author": ["N. Krishnamoorthy", "G. Malkarnenkar", "R. Mooney", "K. Saenko", "S. Guadarrama"], "venue": "In Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Krishnamoorthy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krishnamoorthy et al\\.", "year": 2013}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Kulkarni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Collective Generation of Natural Image Descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kuznetsova et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2012}, {"title": "TREETALK: Composition and compression of trees for image descriptions", "author": ["P. Kuznetsova", "V. Ordonezz", "T.L. Berg", "Y. Choi"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Kuznetsova et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2014}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Lampert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2009}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Lazebnik et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lazebnik et al\\.", "year": 2006}, {"title": "Phrase-based image captioning", "author": ["R. Lebret", "P.O. Pinheiro", "R. Collobert"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Lebret et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebret et al\\.", "year": 2015}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T.L. Berg", "A.C. Berg", "Y. Choi"], "venue": "In The SIGNLL Conference on Computational Natural Language Learning", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2012}, {"title": "Automatic evaluation of summaries using n-gram cooccurrence statistics", "author": ["Lin", "C.-Y", "E. Hovy"], "venue": "In Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Lin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2008}, {"title": "Generating multi-sentence natural language descriptions of indoor scenes", "author": ["D. Lin", "S. Fidler", "C. Kong", "R. Urtasun"], "venue": "In British Machine Vision Conference", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Microsoft COCO: Common objects in context", "author": ["Lin", "T.-Y", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In European Conference on Computer Vision", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Lowe,? \\Q2004\\E", "shortCiteRegEx": "Lowe", "year": 2004}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["L. Ma", "Z. Lu", "H. Li"], "venue": "In AAAI Conference on Artificial Intelligence", "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Malinowski and Fritz,? \\Q2014\\E", "shortCiteRegEx": "Malinowski and Fritz", "year": 2014}, {"title": "Towards a visual turing challenge", "author": ["M. Malinowski", "M. Fritz"], "venue": "In Advances in Neural Information Processing Systems Workshop on Learning Semantics", "citeRegEx": "Malinowski and Fritz,? \\Q2014\\E", "shortCiteRegEx": "Malinowski and Fritz", "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Learning like a child: Fast novel visual concept learning from sentence descriptions of images", "author": ["J. Mao", "X. Wei", "Y. Yang", "J. Wang", "Z. Huang", "A.L. Yuille"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Nonparametric Method for Data-driven Image Captioning", "author": ["R. Mason", "E. Charniak"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Mason and Charniak,? \\Q2014\\E", "shortCiteRegEx": "Mason and Charniak", "year": 2014}, {"title": "Financial incentives and the \u201dperformance of crowds", "author": ["W.A. Mason", "D.J. Watts"], "venue": "In ACM SIGKDD Workshop on Human Computation", "citeRegEx": "Mason and Watts,? \\Q2009\\E", "shortCiteRegEx": "Mason and Watts", "year": 2009}, {"title": "Midge: generating image descriptions from computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A.C. Berg", "K. Yamaguchi", "T.L. Berg", "K. Stratos", "III Daume"], "venue": "In Conference of the European Chapter of the Association for Computational Linguistics", "citeRegEx": "Mitchell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "The impact of frequency on summarization", "author": ["A. Nenkova", "L. Vanderwende"], "venue": "Tech. rep., Microsoft Research", "citeRegEx": "Nenkova and Vanderwende,? \\Q2005\\E", "shortCiteRegEx": "Nenkova and Vanderwende", "year": 2005}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Oliva and Torralba,? \\Q2001\\E", "shortCiteRegEx": "Oliva and Torralba", "year": 2001}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ordonez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Learning to Interpret and Describe Abstract Scenes", "author": ["L.M.G. Ortiz", "C. Wolff", "M. Lapata"], "venue": "In Conference of the North American Chapter of the Association of Computational Linguistics", "citeRegEx": "Ortiz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ortiz et al\\.", "year": 2015}, {"title": "Studies in Iconology", "author": ["E. Panofsky"], "venue": null, "citeRegEx": "Panofsky,? \\Q1939\\E", "shortCiteRegEx": "Panofsky", "year": 1939}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Expressing an image stream with a sequence of natural sentences", "author": ["C. Park", "G. Kim"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Park and Kim,? \\Q2015\\E", "shortCiteRegEx": "Park and Kim", "year": 2015}, {"title": "The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding", "author": ["G. Patterson", "C. Xu", "H. Su", "J. Hays"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Patterson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Patterson et al\\.", "year": 2014}, {"title": "Simple image description generator via a linear phrase-based model", "author": ["P. Pinheiro", "R. Lebret", "R. Collobert"], "venue": "In International Conference on Learning Representations Workshop", "citeRegEx": "Pinheiro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pinheiro et al\\.", "year": 2015}, {"title": "Weakly supervised learning of interactions between humans and objects", "author": ["A. Prest", "C. Schmid", "V. Ferrari"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Prest et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Prest et al\\.", "year": 2012}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies Workshop on Creating Speech and Language Data with Amazon\u2019s", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": null, "citeRegEx": "Rashtchian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "An investigation into the validity of some metrics for automatically evaluating natural language generation systems", "author": ["E. Reiter", "A. Belz"], "venue": "Computational Linguistics,", "citeRegEx": "Reiter and Belz,? \\Q2009\\E", "shortCiteRegEx": "Reiter and Belz", "year": 2009}, {"title": "Building Natural Language Generation Systems", "author": ["E. Reiter", "R. Dale"], "venue": null, "citeRegEx": "Reiter and Dale,? \\Q2006\\E", "shortCiteRegEx": "Reiter and Dale", "year": 2006}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "In International Conference on Machine Learningt Deep Learning Workshop", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["M. Richardson", "C.J. Burges", "E. Renshaw"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "A dataset for movie description", "author": ["A. Rohrbach", "M. Rohrback", "N. Tandon", "B. Schiele"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Rohrbach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2015}, {"title": "Translating Video Content to Natural Language Descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Rohrbach et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2013}, {"title": "Generating semantically precise scene graphs from textual descriptions for improved image retrieval", "author": ["S. Schuster", "R. Krishna", "A. Chang", "L. Fei-Fei", "C.D. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing Vision and Language Workshop", "citeRegEx": "Schuster et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 2015}, {"title": "Analyzing the subject of a picture: A theoretical approach", "author": ["S. Shatford"], "venue": "Cataloging & Classification Quarterly,", "citeRegEx": "Shatford,? \\Q1986\\E", "shortCiteRegEx": "Shatford", "year": 1986}, {"title": "Indoor segmentation and support inference from RGBD images", "author": ["N. Silberman", "P. Kohli", "D. Hoiem", "R. Fergus"], "venue": "In European Conference on Computer Vision", "citeRegEx": "Silberman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silberman et al\\.", "year": 2012}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of im- ages using unaligned text corpora", "author": ["R. Socher", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Socher and Fei.Fei,? \\Q2010\\E", "shortCiteRegEx": "Socher and Fei.Fei", "year": 2010}, {"title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A. Ng"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild", "author": ["J. Thomason", "S. Venugopalan", "S. Guadarrama", "K. Saenko", "R. Mooney"], "venue": "In International Conference on Computational Linguistics", "citeRegEx": "Thomason et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Thomason et al\\.", "year": 2014}, {"title": "Common subspace for model and similarity: Phrase learning for caption generation from images", "author": ["Y. Ushiku", "M. Yamaguchi", "Y. Mukuta", "T. Harada"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Ushiku et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ushiku et al\\.", "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Vedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Im2Text and Text2Im: Associating Images and Texts for Cross-Modal Retrieval", "author": ["Y. Verma", "C.V. Jawahar"], "venue": "In British Machine Vision Conference", "citeRegEx": "Verma and Jawahar,? \\Q2014\\E", "shortCiteRegEx": "Verma and Jawahar", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "A Distributed Representation Based Query Expansion Approach for Image Captioning", "author": ["S. Yagcioglu", "E. Erdem", "A. Erdem", "R. Cakici"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Yagcioglu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yagcioglu et al\\.", "year": 2015}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y. Yang", "C.L. Teo", "III Daume", "Y. Aloimonos"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Yang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "Grouplet: A structured image representation for recognizing human and object interactions", "author": ["B. Yao", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Yao and Fei.Fei,? \\Q2010\\E", "shortCiteRegEx": "Yao and Fei.Fei", "year": 2010}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "See No Evil, Say No Evil: Description Generation from Densely Labeled Images", "author": ["M. Yatskar", "M. Galley", "L. Vanderwende", "L. Zettlemoyer"], "venue": "In Joint Conference on Lexical and Computation Semantics", "citeRegEx": "Yatskar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yatskar et al\\.", "year": 2014}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "Visual madlibs: Fill in the blank description generation and question answering", "author": ["L. Yu", "E. Park", "A.C. Berg", "T.L. Berg"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Y. Zhu", "R. Kiros", "R. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}, {"title": "Learning the visual interpretation of sentences", "author": ["C.L. Zitnick", "D. Parikh", "L. Vanderwende"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Zitnick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zitnick et al\\.", "year": 2013}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C.L. Zitnick", "D. Parikh"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Zitnick and Parikh,? \\Q2013\\E", "shortCiteRegEx": "Zitnick and Parikh", "year": 2013}], "referenceMentions": [{"referenceID": 70, "context": "An interesting intermediate approach involves the annotation of image streams with sequences of sentences, see the work of Park and Kim (2015).", "startOffset": 123, "endOffset": 143}, {"referenceID": 82, "context": "An image description can be analyzed in several different dimensions (Shatford, 1986; Jaimes & Chang, 2000).", "startOffset": 69, "endOffset": 107}, {"referenceID": 82, "context": "An image description can be analyzed in several different dimensions (Shatford, 1986; Jaimes & Chang, 2000). We follow Hodosh, Young, and Hockenmaier (2013) and assume that the descriptions that are of interest for this survey article are the ones that verbalize visual and conceptual information depicted in the image, i.", "startOffset": 70, "endOffset": 157}, {"referenceID": 17, "context": "Elliott and de Vries (2015) X Fang et al. (2015) X", "startOffset": 30, "endOffset": 49}, {"referenceID": 43, "context": "Figure 1: The automatic image description generation system proposed by Kulkarni et al. (2011).", "startOffset": 72, "endOffset": 95}, {"referenceID": 18, "context": "In terms of the representations used, existing models have conceptualized images in a number of different ways, relying on spatial relationships (Farhadi et al., 2010), corpus-based relationships (Yang et al.", "startOffset": 145, "endOffset": 167}, {"referenceID": 93, "context": ", 2010), corpus-based relationships (Yang et al., 2011), or spatial and visual attributes (Kulkarni et al.", "startOffset": 36, "endOffset": 55}, {"referenceID": 43, "context": ", 2011), or spatial and visual attributes (Kulkarni et al., 2011).", "startOffset": 42, "endOffset": 65}, {"referenceID": 18, "context": "Another group of papers utilizes an abstract image representation in the form of meaning tuples which capture different aspects of an image: the objects detected, the attributes of those detections, the spatial relations between them, and the scene type (Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012).", "startOffset": 254, "endOffset": 358}, {"referenceID": 93, "context": "Another group of papers utilizes an abstract image representation in the form of meaning tuples which capture different aspects of an image: the objects detected, the attributes of those detections, the spatial relations between them, and the scene type (Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012).", "startOffset": 254, "endOffset": 358}, {"referenceID": 43, "context": "Another group of papers utilizes an abstract image representation in the form of meaning tuples which capture different aspects of an image: the objects detected, the attributes of those detections, the spatial relations between them, and the scene type (Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012).", "startOffset": 254, "endOffset": 358}, {"referenceID": 49, "context": "Another group of papers utilizes an abstract image representation in the form of meaning tuples which capture different aspects of an image: the objects detected, the attributes of those detections, the spatial relations between them, and the scene type (Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012).", "startOffset": 254, "endOffset": 358}, {"referenceID": 63, "context": "Another group of papers utilizes an abstract image representation in the form of meaning tuples which capture different aspects of an image: the objects detected, the attributes of those detections, the spatial relations between them, and the scene type (Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012).", "startOffset": 254, "endOffset": 358}, {"referenceID": 67, "context": "While initial work using VDRs has relied on a corpus of manually annotated VDRs for training, more recent approaches induce VDRs automatically based on the output of an object detector (Elliott & de Vries, 2015) or the labels present in abstract scenes (Ortiz et al., 2015).", "startOffset": 253, "endOffset": 273}, {"referenceID": 15, "context": "In terms of the representations used, existing models have conceptualized images in a number of different ways, relying on spatial relationships (Farhadi et al., 2010), corpus-based relationships (Yang et al., 2011), or spatial and visual attributes (Kulkarni et al., 2011). Another group of papers utilizes an abstract image representation in the form of meaning tuples which capture different aspects of an image: the objects detected, the attributes of those detections, the spatial relations between them, and the scene type (Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012). More recently, Yatskar et al. (2014) proposed to generate descriptions from denselylabeled images, which incorporate object, attribute, action, and scene annotations.", "startOffset": 146, "endOffset": 672}, {"referenceID": 15, "context": "Similar in spirit is the work by Fang et al. (2015), which does not rely on prior labeling of objects, attributes, etc.", "startOffset": 33, "endOffset": 52}, {"referenceID": 11, "context": "The first framework to explicitly represent how the structure of an image relates to the structure of its description is the Visual Dependency Representations (VDR) method proposed by Elliott and Keller (2013). A VDR captures the spatial relations between the objects in an image in the form of a dependency graph.", "startOffset": 184, "endOffset": 210}, {"referenceID": 101, "context": "An example is Zitnick and Parikh\u2019s (2013) dataset, see Section 3 for details.", "startOffset": 14, "endOffset": 42}, {"referenceID": 43, "context": "by Lin et al. (2015), who parse images into scene graphs, which are similar to VDRs and represent the relations between the objects in a scene.", "startOffset": 3, "endOffset": 21}, {"referenceID": 39, "context": "Examples include the works by Kulkarni et al. (2011) and Li et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 39, "context": "Examples include the works by Kulkarni et al. (2011) and Li et al. (2011), which both generate descriptions using n-gram language models trained on a subset of Wikipedia.", "startOffset": 30, "endOffset": 74}, {"referenceID": 15, "context": "The approach of Fang et al. (2015) is similar, but uses a maximum entropy language model instead of an n-gram model to generate descriptions.", "startOffset": 16, "endOffset": 35}, {"referenceID": 15, "context": "The approach of Fang et al. (2015) is similar, but uses a maximum entropy language model instead of an n-gram model to generate descriptions. This gives the authors more flexibility in handling the output of the word detectors that are at the core of their model. Recent image description work using recurrent neural networks (RNNs) can also be regarded as relying on language modeling. A classical RNN is a language model: it captures the probability of generating a given word in a string, given the words generated so far. In an image description setup, the RNN is trained to generate the next word given not only the string so far, but also a set of image features. In this setting, the RNN is therefore not purely a language model (as in the case of an n-gram model, for instance), but it is a hybrid model that relies on a representation that incorporates both visual and linguistic features. We will return to this in more detail in Section 2.3. A second set of approaches use sentence templates to generate descriptions. These are (typically manually) pre-defined sentence frames in which open slots need to be filled with labels for objects, relations, or attributes. For instance, Yang et al. (2011) fill in a sentence template by selecting the likely objects, verbs, prepositions, and scene types based on a Hidden Markov Model.", "startOffset": 16, "endOffset": 1210}, {"referenceID": 11, "context": "The generation model of Elliott and Keller (2013) parses an image into a VDR, and then traverses the VDRs to fill the slots of sentence templates.", "startOffset": 24, "endOffset": 50}, {"referenceID": 11, "context": "The generation model of Elliott and Keller (2013) parses an image into a VDR, and then traverses the VDRs to fill the slots of sentence templates. This approach also performs a limited from of content selection by learning associations between VDRs and syntactic dependency trees at training time; these associations then allow to select the most appropriate verb for a description at test time. Other approaches have used more linguistically sophisticated approaches to generation. Mitchell et al. (2012) over-generate syntactically well-formed sentence fragments and then recombine these using a tree-substitution grammar.", "startOffset": 24, "endOffset": 506}, {"referenceID": 11, "context": "The generation model of Elliott and Keller (2013) parses an image into a VDR, and then traverses the VDRs to fill the slots of sentence templates. This approach also performs a limited from of content selection by learning associations between VDRs and syntactic dependency trees at training time; these associations then allow to select the most appropriate verb for a description at test time. Other approaches have used more linguistically sophisticated approaches to generation. Mitchell et al. (2012) over-generate syntactically well-formed sentence fragments and then recombine these using a tree-substitution grammar. A related approach has been pursued by Kuznetsova et al. (2014), where tree-fragments are learnt from a training set of existing descriptions and then these fragments are combined at test time to form new descriptions.", "startOffset": 24, "endOffset": 689}, {"referenceID": 11, "context": "The generation model of Elliott and Keller (2013) parses an image into a VDR, and then traverses the VDRs to fill the slots of sentence templates. This approach also performs a limited from of content selection by learning associations between VDRs and syntactic dependency trees at training time; these associations then allow to select the most appropriate verb for a description at test time. Other approaches have used more linguistically sophisticated approaches to generation. Mitchell et al. (2012) over-generate syntactically well-formed sentence fragments and then recombine these using a tree-substitution grammar. A related approach has been pursued by Kuznetsova et al. (2014), where tree-fragments are learnt from a training set of existing descriptions and then these fragments are combined at test time to form new descriptions. Another linguistically expressive model has recently been proposed by Ortiz et al. (2015). The authors model image description as machine translation over VDR\u2013sentence pairs and perform explicit content selection and surface realization using an integer linear program over linguistic constraints.", "startOffset": 24, "endOffset": 934}, {"referenceID": 11, "context": "The generation model of Elliott and Keller (2013) parses an image into a VDR, and then traverses the VDRs to fill the slots of sentence templates. This approach also performs a limited from of content selection by learning associations between VDRs and syntactic dependency trees at training time; these associations then allow to select the most appropriate verb for a description at test time. Other approaches have used more linguistically sophisticated approaches to generation. Mitchell et al. (2012) over-generate syntactically well-formed sentence fragments and then recombine these using a tree-substitution grammar. A related approach has been pursued by Kuznetsova et al. (2014), where tree-fragments are learnt from a training set of existing descriptions and then these fragments are combined at test time to form new descriptions. Another linguistically expressive model has recently been proposed by Ortiz et al. (2015). The authors model image description as machine translation over VDR\u2013sentence pairs and perform explicit content selection and surface realization using an integer linear program over linguistic constraints. The systems presented so far aimed at directly generating novel descriptions. However, as argued by Hodosh et al. (2013), framing image description as a natural language generation (NLG) task makes it difficult to objectively evaluate the quality of novel descriptions", "startOffset": 24, "endOffset": 1263}, {"referenceID": 66, "context": "Figure 2: The description model based on retrieval from visual space proposed by Ordonez et al. (2011).", "startOffset": 81, "endOffset": 103}, {"referenceID": 32, "context": "as it \u201cintroduces a number of linguistic difficulties that detract attention from the underlying image understanding problem\u201d (Hodosh et al., 2013).", "startOffset": 126, "endOffset": 147}, {"referenceID": 66, "context": "One of the first model to follow this approach was the Im2Text model of Ordonez et al. (2011). GIST (Oliva & Torralba, 2001) and Tiny Image (Torralba, Fergus, & Freeman, 2008)", "startOffset": 72, "endOffset": 94}, {"referenceID": 54, "context": "To retrieve visually similar images, the authors employ simple RGB and HSV color histograms, Gabor and Haar descriptors, GIST and SIFT (Lowe, 2004) descriptors as image features.", "startOffset": 135, "endOffset": 147}, {"referenceID": 43, "context": "The model proposed by Kuznetsova et al. (2012) first runs the detectors and the classifiers used in the re-ranking step of the Im2Text model on a query image to extract and represent its semantic content.", "startOffset": 22, "endOffset": 47}, {"referenceID": 29, "context": "The method of Gupta et al. (2012) is another phrase-based approach.", "startOffset": 14, "endOffset": 34}, {"referenceID": 29, "context": "The method of Gupta et al. (2012) is another phrase-based approach. To retrieve visually similar images, the authors employ simple RGB and HSV color histograms, Gabor and Haar descriptors, GIST and SIFT (Lowe, 2004) descriptors as image features. Then, instead of using visual object detectors or scene classifiers, they rely only on the textual information in the descriptions of the visually similar images to extract the visual content of the input image. Specifically, the candidate descriptions are segmented into phrases of a certain type such as (subject, verb), (subject, prep, object), (verb, prep, object), (attribute, object), etc. Those that best describe the input image are determined according to a joint probability model based on image similarity and Google search counts, and the image is represented by triplets of the form {((attribute1, object1), verb), (verb, prep, (attribute2, object2)), (object1, prep, object2)}. In the end, the description is generated using the three top-scoring triplets based on a fixed template. To increase the quality of the descriptions, the authors also apply syntactic aggregation and some subject and predicate grouping rules before the generation step. Patterson et al. (2014) were the first to present a large-scale scene attribute dataset in the computer vision community.", "startOffset": 14, "endOffset": 1232}, {"referenceID": 60, "context": "Mason and Charniak\u2019s (2014) description generation approach differs from the models discussed above in that it formulates description generation as an extractive summarization problem, and it selects the output description by considering only the textual information in the final re-ranking step.", "startOffset": 0, "endOffset": 28}, {"referenceID": 60, "context": "Mason and Charniak\u2019s (2014) description generation approach differs from the models discussed above in that it formulates description generation as an extractive summarization problem, and it selects the output description by considering only the textual information in the final re-ranking step. In particular, the authors represented images by using the scene attributes descriptor of Patterson et al. (2014). Once the visually similar images are identified from the training set, in the next step, the conditional probabilities of observing a word in the description of the query image are estimated via non-parametric density estimation using the descriptions of the retrieved images.", "startOffset": 0, "endOffset": 411}, {"referenceID": 60, "context": "Mason and Charniak\u2019s (2014) description generation approach differs from the models discussed above in that it formulates description generation as an extractive summarization problem, and it selects the output description by considering only the textual information in the final re-ranking step. In particular, the authors represented images by using the scene attributes descriptor of Patterson et al. (2014). Once the visually similar images are identified from the training set, in the next step, the conditional probabilities of observing a word in the description of the query image are estimated via non-parametric density estimation using the descriptions of the retrieved images. The final output description is then determined by using two different extractive summarization techniques, one depending on the SumBasic model (Nenkova & Vanderwende, 2005) and the other based on Kullback-Leibler divergence between the word distributions of the query and the candidate descriptions. Yagcioglu et al. (2015) proposed an average query expansion approach which is based on compositional distributed semantics.", "startOffset": 0, "endOffset": 1014}, {"referenceID": 8, "context": "The approach of Devlin et al. (2015) also utilizes CNN activations as the global image descriptor and performs k-nearest neighbor retrieval to determine the images from the training set that are visually similar to the query image.", "startOffset": 16, "endOffset": 37}, {"referenceID": 8, "context": "The approach of Devlin et al. (2015) also utilizes CNN activations as the global image descriptor and performs k-nearest neighbor retrieval to determine the images from the training set that are visually similar to the query image. It then selects a description from the candidate descriptions associated with the retrieved images that best describes the images that are similar to the query image, just like the approaches by Mason and Charniak (2014) and Yagcioglu et al.", "startOffset": 16, "endOffset": 453}, {"referenceID": 8, "context": "The approach of Devlin et al. (2015) also utilizes CNN activations as the global image descriptor and performs k-nearest neighbor retrieval to determine the images from the training set that are visually similar to the query image. It then selects a description from the candidate descriptions associated with the retrieved images that best describes the images that are similar to the query image, just like the approaches by Mason and Charniak (2014) and Yagcioglu et al. (2015). Their approach differs in terms of how they represent the similarity between description and how they select the best candidate over the whole set.", "startOffset": 16, "endOffset": 481}, {"referenceID": 32, "context": "The third group of studies casts image description generation again as a retrieval problem, but from a multimodal space (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014).", "startOffset": 120, "endOffset": 185}, {"referenceID": 85, "context": "The third group of studies casts image description generation again as a retrieval problem, but from a multimodal space (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014).", "startOffset": 120, "endOffset": 185}, {"referenceID": 39, "context": "The third group of studies casts image description generation again as a retrieval problem, but from a multimodal space (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014).", "startOffset": 120, "endOffset": 185}, {"referenceID": 32, "context": "Figure 3: Image descriptions as a retrieval task as proposed in Hodosh et al. (2013); Socher et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 32, "context": "Figure 3: Image descriptions as a retrieval task as proposed in Hodosh et al. (2013); Socher et al. (2014); Karpathy et al.", "startOffset": 64, "endOffset": 107}, {"referenceID": 32, "context": "Figure 3: Image descriptions as a retrieval task as proposed in Hodosh et al. (2013); Socher et al. (2014); Karpathy et al. (2014). (Image Source http://nlp.", "startOffset": 64, "endOffset": 131}, {"referenceID": 18, "context": "In an earlier study the authors proposed to learn a common meaning space (Farhadi et al., 2010) consisting of a triple representation of the form \u3008object, action, scene\u3009.", "startOffset": 73, "endOffset": 95}, {"referenceID": 33, "context": "(2013) use KCCA, a kernelized version of CCA, Canonical Correlation Analysis (Hotelling, 1936), to learn the joint space.", "startOffset": 77, "endOffset": 94}, {"referenceID": 31, "context": "In this section, we first discuss the seminal paper of Hodosh et al. (2013) on description retrieval, and then present more recent approaches that combine a retrieval approach with some form of natural language generation.", "startOffset": 55, "endOffset": 76}, {"referenceID": 31, "context": "In this section, we first discuss the seminal paper of Hodosh et al. (2013) on description retrieval, and then present more recent approaches that combine a retrieval approach with some form of natural language generation. Hodosh et al. (2013) map both images and sentences into a common space.", "startOffset": 55, "endOffset": 244}, {"referenceID": 18, "context": "In an earlier study the authors proposed to learn a common meaning space (Farhadi et al., 2010) consisting of a triple representation of the form \u3008object, action, scene\u3009. The representation was thus limited to a set of pre-defined discrete slot fillers, which was given as training information. Instead, Hodosh et al. (2013) use KCCA, a kernelized version of CCA, Canonical Correlation Analysis (Hotelling, 1936), to learn the joint space.", "startOffset": 74, "endOffset": 325}, {"referenceID": 18, "context": "In an earlier study the authors proposed to learn a common meaning space (Farhadi et al., 2010) consisting of a triple representation of the form \u3008object, action, scene\u3009. The representation was thus limited to a set of pre-defined discrete slot fillers, which was given as training information. Instead, Hodosh et al. (2013) use KCCA, a kernelized version of CCA, Canonical Correlation Analysis (Hotelling, 1936), to learn the joint space. CCA takes a training dataset of image-sentence pairs, i.e., Dtrain = {\u3008i, s\u3009}, thus input from two different feature spaces, and finds linear projections into a newly induced common space. In KCCA, kernel functions map the original items into higher-order space in order to capture the patterns needed to associate image and text. KCCA has been shown previously to be successful in associating images (Hardoon, Szedmak, & ShaweTaylor, 2004) or image regions (Socher & Fei-Fei, 2010) with individual words or set of tags. Hodosh et al. (2013) compare their KCCA approach to a nearest-neighbor (NN) baseline that uses unimodal text and image spaces, without constructing a joint space.", "startOffset": 74, "endOffset": 982}, {"referenceID": 85, "context": "Their final model integrates both global (sentence and image-level) as well as finer-grained information and outperforms previous approaches, such as DT-RNN (Socher et al., 2014).", "startOffset": 157, "endOffset": 178}, {"referenceID": 32, "context": "Description generation systems are difficult to evaluate, therefore the studies reviewed above treat the problem as a retrieval and ranking task (Hodosh et al., 2013; Socher et al., 2014).", "startOffset": 145, "endOffset": 187}, {"referenceID": 85, "context": "Description generation systems are difficult to evaluate, therefore the studies reviewed above treat the problem as a retrieval and ranking task (Hodosh et al., 2013; Socher et al., 2014).", "startOffset": 145, "endOffset": 187}, {"referenceID": 9, "context": "To alleviate this problem, recent models have been developed that are extensions of multimodal spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick, 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015; Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 172, "endOffset": 341}, {"referenceID": 41, "context": "To alleviate this problem, recent models have been developed that are extensions of multimodal spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick, 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015; Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 172, "endOffset": 341}, {"referenceID": 48, "context": "To alleviate this problem, recent models have been developed that are extensions of multimodal spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick, 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015; Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 172, "endOffset": 341}, {"referenceID": 90, "context": "To alleviate this problem, recent models have been developed that are extensions of multimodal spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick, 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015; Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 172, "endOffset": 341}, {"referenceID": 91, "context": "To alleviate this problem, recent models have been developed that are extensions of multimodal spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick, 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015; Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 172, "endOffset": 341}, {"referenceID": 76, "context": "Socher et al. (2014) use neural networks for building sentence and image vector representations that are then mapped into a common embedding space.", "startOffset": 0, "endOffset": 21}, {"referenceID": 30, "context": "The authors show that their model outperforms previously used KCCA approaches such as Hodosh and Hockenmaier (2013). Karpathy et al.", "startOffset": 86, "endOffset": 116}, {"referenceID": 30, "context": "The authors show that their model outperforms previously used KCCA approaches such as Hodosh and Hockenmaier (2013). Karpathy et al. (2014) extend the previous multi-modal embeddings model.", "startOffset": 86, "endOffset": 140}, {"referenceID": 30, "context": "The authors show that their model outperforms previously used KCCA approaches such as Hodosh and Hockenmaier (2013). Karpathy et al. (2014) extend the previous multi-modal embeddings model. Rather than directly mapping entire images and sentences into a common embedding space, their model embeds more fine-grained units, i.e., fragments of images (objects) and sentences (dependency tree fragments), into a common space. Their final model integrates both global (sentence and image-level) as well as finer-grained information and outperforms previous approaches, such as DT-RNN (Socher et al., 2014). A similar approach is pursued by Pinheiro et al. (2015), who propose a bilinear phrase-based model that learns a mapping between image representations and sentences.", "startOffset": 86, "endOffset": 658}, {"referenceID": 30, "context": "The authors show that their model outperforms previously used KCCA approaches such as Hodosh and Hockenmaier (2013). Karpathy et al. (2014) extend the previous multi-modal embeddings model. Rather than directly mapping entire images and sentences into a common embedding space, their model embeds more fine-grained units, i.e., fragments of images (objects) and sentences (dependency tree fragments), into a common space. Their final model integrates both global (sentence and image-level) as well as finer-grained information and outperforms previous approaches, such as DT-RNN (Socher et al., 2014). A similar approach is pursued by Pinheiro et al. (2015), who propose a bilinear phrase-based model that learns a mapping between image representations and sentences. A constrained language model is then used to generate from this representation. A conceptually related approach is pursued by Ushiku et al. (2015): the authors use a common subspace model which maps all feature vectors associated with the same phrase into nearby regions of the space.", "startOffset": 86, "endOffset": 915}, {"referenceID": 9, "context": "To alleviate this problem, recent models have been developed that are extensions of multimodal spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick, 2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015; Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015). Kiros et al. (2015) introduced a general encoder-decoder framework for image description ranking and generation, illustrated in Figure 4.", "startOffset": 195, "endOffset": 363}, {"referenceID": 41, "context": "Figure 4: The encoder-decoder model proposed by Kiros et al. (2015).", "startOffset": 48, "endOffset": 68}, {"referenceID": 85, "context": "(2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al.", "startOffset": 42, "endOffset": 63}, {"referenceID": 37, "context": "In Kiros et al.\u2019s (2015) encoder-decoder model, the vision space is projected into the embedding space of the LSTM hidden states; a pairwise ranking loss is minimized to learn the ranking of images and their descriptions.", "startOffset": 3, "endOffset": 25}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture.", "startOffset": 106, "endOffset": 128}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM.", "startOffset": 106, "endOffset": 490}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al.", "startOffset": 106, "endOffset": 594}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al.", "startOffset": 106, "endOffset": 683}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval.", "startOffset": 106, "endOffset": 731}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval. Subsequent work includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al.", "startOffset": 106, "endOffset": 850}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval. Subsequent work includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al. (2015), who are very similar to the one proposed by Kiros et al.", "startOffset": 106, "endOffset": 876}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval. Subsequent work includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al. (2015), who are very similar to the one proposed by Kiros et al. (2015) and achieve comparable results on standard datasets.", "startOffset": 106, "endOffset": 941}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval. Subsequent work includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al. (2015), who are very similar to the one proposed by Kiros et al. (2015) and achieve comparable results on standard datasets. Mao, Wei, Yang, Wang, Huang, and Yuille (2015b) propose an interesting extension of Mao et al.", "startOffset": 106, "endOffset": 1042}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval. Subsequent work includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al. (2015), who are very similar to the one proposed by Kiros et al. (2015) and achieve comparable results on standard datasets. Mao, Wei, Yang, Wang, Huang, and Yuille (2015b) propose an interesting extension of Mao et al.\u2019s (2015a) model for the learning of novel visual concepts.", "startOffset": 106, "endOffset": 1099}, {"referenceID": 8, "context": "Work that has been carried out at the same time and is similar to the latter is described in the paper by Donahue et al. (2015). The authors propose a model that is also based on the LSTM neural architecture. However, rather than projecting the vision space into the embedding space of the hidden states, the model takes a copy of the static image and the previous word directly as input, that is then fed to a stack of four LSTMs. Another LSTM-based model is proposed by Jia et al. (2015), who added semantic image information as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior DT-RNN model (Socher et al., 2014); in turn, Donahue et al. (2015) report that they outperform Kiros et al. (2015) on the task of image description retrieval. Subsequent work includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al. (2015), who are very similar to the one proposed by Kiros et al. (2015) and achieve comparable results on standard datasets. Mao, Wei, Yang, Wang, Huang, and Yuille (2015b) propose an interesting extension of Mao et al.\u2019s (2015a) model for the learning of novel visual concepts. Karpathy and Fei-Fei (2015) improve on previous models by proposing a deep visualsemantic alignment model with a simpler architecture and objective function.", "startOffset": 106, "endOffset": 1176}, {"referenceID": 5, "context": "Another model that can generate novel sentences is proposed in Chen and Zitnick (2015). In contrast to the previous work, their model dynamically builds a visual representation of the scene as a description is being generated.", "startOffset": 63, "endOffset": 87}, {"referenceID": 88, "context": "The model of Xu et al. (2015) is closely related in that it also uses an RNN-based architecture in which the visual representations are dynamically updated.", "startOffset": 13, "endOffset": 30}, {"referenceID": 88, "context": "The model of Xu et al. (2015) is closely related in that it also uses an RNN-based architecture in which the visual representations are dynamically updated. Xu et al.\u2019s (2015) model incorporates an attentional component, which gives it a way of determining which regions in an image are salient, and it can focus its description on those regions.", "startOffset": 13, "endOffset": 176}, {"referenceID": 47, "context": "The general RNN-based ranking and generation approach is also followed by Lebret et al. (2015). Here, the main innovation is on the linguistic side: they employ a bilinear model to learn a common space of image features and syntactic phrases (noun phrases, verb phrases, and prepositional phrases).", "startOffset": 74, "endOffset": 95}, {"referenceID": 47, "context": "The general RNN-based ranking and generation approach is also followed by Lebret et al. (2015). Here, the main innovation is on the linguistic side: they employ a bilinear model to learn a common space of image features and syntactic phrases (noun phrases, verb phrases, and prepositional phrases). A Markov model is then utilized to generate sentences from these phrase embedding. On the visual side, standard CNN-based features are used. This results in an elegant modeling framework, whose performance is broadly comparable to the state of the art. Finally, two important directions that are less explored are: portability and weakly supervised learning. Verma and Jawahar (2014) evaluate the portability of a bi-directional model based on topic models, showing that performance significantly degrades.", "startOffset": 74, "endOffset": 683}, {"referenceID": 26, "context": "Gong et al. (2014) propose an approach based on weak supervision that transfers knowledge from millions of weakly annotated images to improve the accuracy of description retrieval.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "The training set also needs to be diverse (in addition to being large), in order for visual retrieval-based approaches to produce image descriptions that are adequate for novel test images (Devlin et al., 2015).", "startOffset": 189, "endOffset": 210}, {"referenceID": 74, "context": "The Pascal1K sentence dataset (Rashtchian et al., 2010) is a dataset which is commonly used as a benchmark for evaluating the quality of description generation systems.", "startOffset": 30, "endOffset": 55}, {"referenceID": 44, "context": "Kuznetsova et al. (2014) ran a human judgments study on 1,000 images from this dataset.", "startOffset": 0, "endOffset": 25}, {"referenceID": 74, "context": "Pascal1K (Rashtchian et al., 2010) 1,000 5 No Partial VLT2K (Elliott & Keller, 2013) 2,424 3 Partial Partial Flickr8K (Hodosh & Hockenmaier, 2013) 8,108 5 Yes No Flickr30K (Young et al.", "startOffset": 9, "endOffset": 34}, {"referenceID": 97, "context": ", 2010) 1,000 5 No Partial VLT2K (Elliott & Keller, 2013) 2,424 3 Partial Partial Flickr8K (Hodosh & Hockenmaier, 2013) 8,108 5 Yes No Flickr30K (Young et al., 2014) 31,783 5 No No Abstract Scenes (Zitnick & Parikh, 2013) 10,000 6 No Complete IAPR-TC12 (Grubinger et al.", "startOffset": 145, "endOffset": 165}, {"referenceID": 27, "context": ", 2014) 31,783 5 No No Abstract Scenes (Zitnick & Parikh, 2013) 10,000 6 No Complete IAPR-TC12 (Grubinger et al., 2006) 20,000 1\u20135 No Segmented MS COCO (Lin et al.", "startOffset": 95, "endOffset": 119}, {"referenceID": 53, "context": ", 2006) 20,000 1\u20135 No Segmented MS COCO (Lin et al., 2014) 164,062 5 Soon Partial", "startOffset": 40, "endOffset": 58}, {"referenceID": 66, "context": "BBC News (Feng & Lapata, 2008) 3,361 1 No No SBU1M Captions (Ordonez et al., 2011) 1,000,000 1 Possibly No D\u00e9j\u00e0-Image Captions (Chen et al.", "startOffset": 60, "endOffset": 82}, {"referenceID": 4, "context": ", 2011) 1,000,000 1 Possibly No D\u00e9j\u00e0-Image Captions (Chen et al., 2015) 4,000,000 Varies No No", "startOffset": 52, "endOffset": 71}, {"referenceID": 32, "context": "The Flickr8K dataset (Hodosh et al., 2013) and its extended version Flickr30K dataset (Young et al.", "startOffset": 21, "endOffset": 42}, {"referenceID": 97, "context": ", 2013) and its extended version Flickr30K dataset (Young et al., 2014) contain images from Flickr, comprising approximately 8,000 and 30,000 images, respectively.", "startOffset": 51, "endOffset": 71}, {"referenceID": 53, "context": "The MS COCO dataset (Lin et al., 2014) currently consists of 123,287 images with five different descriptions per image.", "startOffset": 20, "endOffset": 38}, {"referenceID": 0, "context": "Extensions of MS COCO are currently under development, including the addition of questions and answers (Antol et al., 2015).", "startOffset": 103, "endOffset": 123}, {"referenceID": 52, "context": "One paper (Lin et al., 2015) uses an the NYU dataset (Silberman, Kohli, Hoiem, & Fergus, 2012), which contains 1,449 indoor scenes with 3D object segmentation.", "startOffset": 10, "endOffset": 28}, {"referenceID": 26, "context": "The IAPR-TC12 dataset introduced by Grubinger et al. (2006) is one of the earliest multi-modal datasets and contains 20,000 images with descriptions.", "startOffset": 36, "endOffset": 60}, {"referenceID": 68, "context": "A caption provides personal, cultural, or historical context for the image (Panofsky, 1939).", "startOffset": 75, "endOffset": 91}, {"referenceID": 20, "context": "Feng and Lapata (2008) harvested 3,361 news articles from the British Broadcasting Corporation News website, with the constraint that the article includes an image and a caption.", "startOffset": 0, "endOffset": 23}, {"referenceID": 20, "context": "Feng and Lapata (2008) harvested 3,361 news articles from the British Broadcasting Corporation News website, with the constraint that the article includes an image and a caption. The SBU1M Captions dataset introduced by Ordonez et al. (2011) differs from the previous datasets in that it is a web-scale dataset containing approximately one million captioned images.", "startOffset": 0, "endOffset": 242}, {"referenceID": 4, "context": "The D\u00e9j\u00e0-Image Captions dataset (Chen et al., 2015) contains 4,000,000 images with 180,000 near-identical captions harvested from Flickr.", "startOffset": 32, "endOffset": 51}, {"referenceID": 4, "context": "For instance, the sentences the bird flies in blue sky and a bird flying into the blue sky were normalized to bird fly IN blue sky (Chen et al., 2015).", "startOffset": 131, "endOffset": 150}, {"referenceID": 14, "context": "The images for these datasets have either been sourced from an existing task in the computer vision community \u2013 the Pascal challenge (Everingham et al., 2010) was used to the Pascal1K and VLT2K datasets \u2013 directly from Flickr, in the case of Flickr8K/30K, MS COCO, SBU1M Captions, and D\u00e9j\u00e0-Image Captions datasets, or crowdsourced, in the case of the Abstract Scenes dataset.", "startOffset": 133, "endOffset": 158}, {"referenceID": 32, "context": "Further options are available to control the quality of the collected texts: a minimum performance rate for workers is a common choice; and a pre-task selection quiz may be used to determine whether workers have a sufficient grasp of the English language (Hodosh et al., 2013).", "startOffset": 255, "endOffset": 276}, {"referenceID": 12, "context": "The images for these datasets have either been sourced from an existing task in the computer vision community \u2013 the Pascal challenge (Everingham et al., 2010) was used to the Pascal1K and VLT2K datasets \u2013 directly from Flickr, in the case of Flickr8K/30K, MS COCO, SBU1M Captions, and D\u00e9j\u00e0-Image Captions datasets, or crowdsourced, in the case of the Abstract Scenes dataset. The texts in image\u2013description datasets are usually crowd-sourced from Amazon Mechanical Turk or Crowdflower; whereas the texts in image\u2013caption datasets have been harvested from photo-sharing sites, such as Flickr, or from news providers. Captions are usually collected without financial incentive because they are written by the people sharing their own images, or by journalists. Crowd-sourcing the descriptions of images involves defining a simple task that can be performed by untrained workers. Examples of the task guidelines used by Hodosh et al. (2013) and Elliott and Keller (2013) are given in Figure 6.", "startOffset": 134, "endOffset": 938}, {"referenceID": 11, "context": "(2013) and Elliott and Keller (2013) are given in Figure 6.", "startOffset": 11, "endOffset": 37}, {"referenceID": 11, "context": "(2013) and Elliott and Keller (2013) are given in Figure 6. In both instances, care was taken to clearly inform the potential workers about the expectations for the task. In particular, explicit instructions were given on how the descriptions should be written, and examples of good texts were provided. In addition, Hodosh et al. provided more extensive examples to explain what would constitute unsatisfactory texts. Further options are available to control the quality of the collected texts: a minimum performance rate for workers is a common choice; and a pre-task selection quiz may be used to determine whether workers have a sufficient grasp of the English language (Hodosh et al., 2013). The issue of remuneration for crowd-sourced workers is controversial, and higher payments do not always lead to better quality in a crowd-sourced environment (Mason &Watts, 2009). Rashtchian et al. (2010) paid $0.", "startOffset": 11, "endOffset": 902}, {"referenceID": 11, "context": "(2013) and Elliott and Keller (2013) are given in Figure 6. In both instances, care was taken to clearly inform the potential workers about the expectations for the task. In particular, explicit instructions were given on how the descriptions should be written, and examples of good texts were provided. In addition, Hodosh et al. provided more extensive examples to explain what would constitute unsatisfactory texts. Further options are available to control the quality of the collected texts: a minimum performance rate for workers is a common choice; and a pre-task selection quiz may be used to determine whether workers have a sufficient grasp of the English language (Hodosh et al., 2013). The issue of remuneration for crowd-sourced workers is controversial, and higher payments do not always lead to better quality in a crowd-sourced environment (Mason &Watts, 2009). Rashtchian et al. (2010) paid $0.01/description, Elliott and Keller (2013) paid $0.", "startOffset": 11, "endOffset": 952}, {"referenceID": 32, "context": "Source Appendix of the work by Hodosh et al. (2013)", "startOffset": 31, "endOffset": 52}, {"referenceID": 43, "context": "\u2022 The description accurately describes the image (Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al., 2013).", "startOffset": 49, "endOffset": 182}, {"referenceID": 49, "context": "\u2022 The description accurately describes the image (Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al., 2013).", "startOffset": 49, "endOffset": 182}, {"referenceID": 63, "context": "\u2022 The description accurately describes the image (Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al., 2013).", "startOffset": 49, "endOffset": 182}, {"referenceID": 44, "context": "\u2022 The description accurately describes the image (Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al., 2013).", "startOffset": 49, "endOffset": 182}, {"referenceID": 32, "context": "\u2022 The description accurately describes the image (Kulkarni et al., 2011; Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al., 2013).", "startOffset": 49, "endOffset": 182}, {"referenceID": 63, "context": "\u2022 The description has no incorrect information (Mitchell et al., 2012).", "startOffset": 47, "endOffset": 70}, {"referenceID": 49, "context": "\u2022 The description is relevant for this image (Li et al., 2011; Yang et al., 2011).", "startOffset": 45, "endOffset": 81}, {"referenceID": 93, "context": "\u2022 The description is relevant for this image (Li et al., 2011; Yang et al., 2011).", "startOffset": 45, "endOffset": 81}, {"referenceID": 49, "context": "\u2022 The description is creatively constructed (Li et al., 2011).", "startOffset": 44, "endOffset": 61}, {"referenceID": 63, "context": "\u2022 The description is human-like (Mitchell et al., 2012).", "startOffset": 32, "endOffset": 55}, {"referenceID": 43, "context": "This approach to evaluation has been subject to much discussion and critique (Kulkarni et al., 2011; Hodosh et al., 2013; Elliott & Keller, 2014).", "startOffset": 77, "endOffset": 145}, {"referenceID": 32, "context": "This approach to evaluation has been subject to much discussion and critique (Kulkarni et al., 2011; Hodosh et al., 2013; Elliott & Keller, 2014).", "startOffset": 77, "endOffset": 145}, {"referenceID": 11, "context": "Elliott and Keller analyzed the correlation between human judgments and automatic evaluation measures for retrieved and system-generated image descriptions in the Flickr8K and VLT2K datasets. They showed that sentence-level unigram BLEU, which at that point in time was the de facto standard measure for image description evaluation, is only weakly correlated with human judgments. Meteor (Banerjee & Lavie, 2005), a less frequently used translation evaluation measure, exhibited the highest correlation with human judgments. However, Kuznetsova et al. (2014) found that unigram BLEU was more strongly correlated with human judgments than Meteor for image caption generation.", "startOffset": 0, "endOffset": 560}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al.", "startOffset": 33, "endOffset": 59}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al.", "startOffset": 33, "endOffset": 109}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al.", "startOffset": 33, "endOffset": 192}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al.", "startOffset": 33, "endOffset": 250}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al. (2014) Generation SBU1M Human, BLEU, Meteor Mason and Charniak (2014) VisRetrieval SBU1M Human, BLEU Patterson et al.", "startOffset": 33, "endOffset": 328}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al. (2014) Generation SBU1M Human, BLEU, Meteor Mason and Charniak (2014) VisRetrieval SBU1M Human, BLEU Patterson et al.", "startOffset": 33, "endOffset": 391}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al. (2014) Generation SBU1M Human, BLEU, Meteor Mason and Charniak (2014) VisRetrieval SBU1M Human, BLEU Patterson et al. (2014) VisRetrieval SBU1M BLEU Socher et al.", "startOffset": 33, "endOffset": 446}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al. (2014) Generation SBU1M Human, BLEU, Meteor Mason and Charniak (2014) VisRetrieval SBU1M Human, BLEU Patterson et al. (2014) VisRetrieval SBU1M BLEU Socher et al. (2014) MultRetrieval Pascal1K mRank, R@k Verma and Jawahar (2014) MultRetrieval IAPR, SBU1M, Pascal1K BLEU, ROUGE, P@k Yatskar et al.", "startOffset": 33, "endOffset": 491}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al. (2014) Generation SBU1M Human, BLEU, Meteor Mason and Charniak (2014) VisRetrieval SBU1M Human, BLEU Patterson et al. (2014) VisRetrieval SBU1M BLEU Socher et al. (2014) MultRetrieval Pascal1K mRank, R@k Verma and Jawahar (2014) MultRetrieval IAPR, SBU1M, Pascal1K BLEU, ROUGE, P@k Yatskar et al.", "startOffset": 33, "endOffset": 550}, {"referenceID": 8, "context": "(2012) Generation Pascal1K Human Elliott and Keller (2013) Generation VLT2K Human, BLEU Hodosh et al. (2013) MultRetrieval Pascal1K, Flickr8K Human, BLEU, ROUGE, mRank, R@k Gong et al. (2014) MultRetrieval SBU1M, Flickr30K R@k Karpathy et al. (2014) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr Kuznetsova et al. (2014) Generation SBU1M Human, BLEU, Meteor Mason and Charniak (2014) VisRetrieval SBU1M Human, BLEU Patterson et al. (2014) VisRetrieval SBU1M BLEU Socher et al. (2014) MultRetrieval Pascal1K mRank, R@k Verma and Jawahar (2014) MultRetrieval IAPR, SBU1M, Pascal1K BLEU, ROUGE, P@k Yatskar et al. (2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al.", "startOffset": 33, "endOffset": 625}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al.", "startOffset": 39, "endOffset": 63}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al.", "startOffset": 39, "endOffset": 150}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al.", "startOffset": 39, "endOffset": 225}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al.", "startOffset": 39, "endOffset": 284}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al.", "startOffset": 39, "endOffset": 343}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al.", "startOffset": 39, "endOffset": 411}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al.", "startOffset": 39, "endOffset": 482}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al.", "startOffset": 39, "endOffset": 567}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al. (2015) MultRetrieval Flickr30K, COCO BLEU, R@k Lin et al.", "startOffset": 39, "endOffset": 619}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al. (2015) MultRetrieval Flickr30K, COCO BLEU, R@k Lin et al. (2015) Generation NYU ROUGE Mao et al.", "startOffset": 39, "endOffset": 677}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al. (2015) MultRetrieval Flickr30K, COCO BLEU, R@k Lin et al. (2015) Generation NYU ROUGE Mao et al. (2015a) MultRetrieval IAPR, Flickr30K, COCO BLEU, mRank, R@k Ortiz et al.", "startOffset": 39, "endOffset": 717}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al. (2015) MultRetrieval Flickr30K, COCO BLEU, R@k Lin et al. (2015) Generation NYU ROUGE Mao et al. (2015a) MultRetrieval IAPR, Flickr30K, COCO BLEU, mRank, R@k Ortiz et al. (2015) Generation Abstract Scenes Human, BLEU, Meteor Pinheiro et al.", "startOffset": 39, "endOffset": 790}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al. (2015) MultRetrieval Flickr30K, COCO BLEU, R@k Lin et al. (2015) Generation NYU ROUGE Mao et al. (2015a) MultRetrieval IAPR, Flickr30K, COCO BLEU, mRank, R@k Ortiz et al. (2015) Generation Abstract Scenes Human, BLEU, Meteor Pinheiro et al. (2015) MultRetrieval COCO BLEU Ushiku et al.", "startOffset": 39, "endOffset": 860}, {"referenceID": 5, "context": "(2014) Generation Own data Human, BLEU Chen and Zitnick (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Donahue et al. (2015) MultRetrieval Flickr30K, COCO Human, BLEU, mRank, R@k Devlin et al. (2015) VisRetrieval COCO BLEU, Meteor Elliott and de Vries (2015) Generation VLT2K, Pascal1K BLEU, Meteor Fang et al. (2015) Generation COCO Human, BLEU, ROUGE, Meteor, CIDEr Jia et al. (2015) Generation Flickr8K/30K, COCO BLEU, Meteor Karpathy and Fei-Fei (2015) MultRetrieval Flickr8K/30K, COCO BLEU, Meteor, CIDEr, mRank, R@k Kiros et al. (2015) MultRetrieval Flickr8K/30K R@k Lebret et al. (2015) MultRetrieval Flickr30K, COCO BLEU, R@k Lin et al. (2015) Generation NYU ROUGE Mao et al. (2015a) MultRetrieval IAPR, Flickr30K, COCO BLEU, mRank, R@k Ortiz et al. (2015) Generation Abstract Scenes Human, BLEU, Meteor Pinheiro et al. (2015) MultRetrieval COCO BLEU Ushiku et al. (2015) Generation Pascal1K, IAPR, SBU1M, COCO BLEU", "startOffset": 39, "endOffset": 905}, {"referenceID": 88, "context": "The number of reference texts for each testing image was either five or 40, based on the insight that some measures may benefit from larger reference sets (Vedantam et al., 2015).", "startOffset": 155, "endOffset": 178}, {"referenceID": 32, "context": "The models that approach the description generation problem from a cross-modal retrieval perspective (Hodosh & Hockenmaier, 2013; Hodosh et al., 2013; Socher et al., 2014; Gong et al., 2014; Karpathy et al., 2014; Verma & Jawahar, 2014) are also able to use measures from information retrieval, such as median rank (mRank), precision at k (S@k), or recall at k (R@k) to evaluate the descriptions they return, in addition to the text-similarity measures reported above.", "startOffset": 101, "endOffset": 236}, {"referenceID": 85, "context": "The models that approach the description generation problem from a cross-modal retrieval perspective (Hodosh & Hockenmaier, 2013; Hodosh et al., 2013; Socher et al., 2014; Gong et al., 2014; Karpathy et al., 2014; Verma & Jawahar, 2014) are also able to use measures from information retrieval, such as median rank (mRank), precision at k (S@k), or recall at k (R@k) to evaluate the descriptions they return, in addition to the text-similarity measures reported above.", "startOffset": 101, "endOffset": 236}, {"referenceID": 26, "context": "The models that approach the description generation problem from a cross-modal retrieval perspective (Hodosh & Hockenmaier, 2013; Hodosh et al., 2013; Socher et al., 2014; Gong et al., 2014; Karpathy et al., 2014; Verma & Jawahar, 2014) are also able to use measures from information retrieval, such as median rank (mRank), precision at k (S@k), or recall at k (R@k) to evaluate the descriptions they return, in addition to the text-similarity measures reported above.", "startOffset": 101, "endOffset": 236}, {"referenceID": 39, "context": "The models that approach the description generation problem from a cross-modal retrieval perspective (Hodosh & Hockenmaier, 2013; Hodosh et al., 2013; Socher et al., 2014; Gong et al., 2014; Karpathy et al., 2014; Verma & Jawahar, 2014) are also able to use measures from information retrieval, such as median rank (mRank), precision at k (S@k), or recall at k (R@k) to evaluate the descriptions they return, in addition to the text-similarity measures reported above.", "startOffset": 101, "endOffset": 236}, {"referenceID": 18, "context": "The earliest work on image description used relatively small datasets (Farhadi et al., 2010; Kulkarni et al., 2011; Elliott & Keller, 2013).", "startOffset": 70, "endOffset": 139}, {"referenceID": 43, "context": "The earliest work on image description used relatively small datasets (Farhadi et al., 2010; Kulkarni et al., 2011; Elliott & Keller, 2013).", "startOffset": 70, "endOffset": 139}, {"referenceID": 18, "context": "The earliest work on image description used relatively small datasets (Farhadi et al., 2010; Kulkarni et al., 2011; Elliott & Keller, 2013). Recently, the introduction of Flickr30K, MS COCO and other large datasets has enabled the training of more complex models such as neural networks. Still, the area is likely to benefit from larger and diversified datasets that share a common, unified, comprehensive vocabulary. Vinyals et al. (2015) argue that", "startOffset": 71, "endOffset": 440}, {"referenceID": 88, "context": "According to existing measures, including the latest CIDEr measure (Vedantam et al., 2015), several automatic methods outperform the human upper bound (this upper bound indicates how similar human descriptions are to each other).", "startOffset": 67, "endOffset": 90}, {"referenceID": 17, "context": "The counterintuitive nature of this result is confirmed by the fact that when human judgments are used for evaluation, the output of even the best system is judged as worse than a human generated description for most of the time (Fang et al., 2015).", "startOffset": 229, "endOffset": 248}, {"referenceID": 32, "context": "The human judgments were obtained from human experts (Hodosh et al., 2013).", "startOffset": 53, "endOffset": 74}, {"referenceID": 7, "context": "This situation has been demonstrated by Devlin et al. (2015), who show that their best model is able to generate only 47.", "startOffset": 40, "endOffset": 61}, {"referenceID": 5, "context": "Chen and Zitnick (2015) and related approaches take a step towards addressing such limitations by coupling description and visual representation generation.", "startOffset": 0, "endOffset": 24}, {"referenceID": 77, "context": "Following this effort, several datasets on this task are being released: DAQUAR (Malinowski & Fritz, 2014a) was compiled from scene depth images and mainly focuses on questions about the type, quantity and color of objects; COCOQA (Ren et al., 2015) was constructed by converting image descriptions to VQA format over a subset of images from the MS COCO dataset; the Freestyle Multilingual Image Question Answering (FM-IQA) Dataset (Gao et al.", "startOffset": 231, "endOffset": 249}, {"referenceID": 23, "context": ", 2015) was constructed by converting image descriptions to VQA format over a subset of images from the MS COCO dataset; the Freestyle Multilingual Image Question Answering (FM-IQA) Dataset (Gao et al., 2015), Visual Madlibs dataset (Yu, Park, Berg, & Berg, 2015) and the VQA dataset (Antol et al.", "startOffset": 190, "endOffset": 208}, {"referenceID": 0, "context": ", 2015), Visual Madlibs dataset (Yu, Park, Berg, & Berg, 2015) and the VQA dataset (Antol et al., 2015), were again built for images from MS COCO, but this time question-answer pairs are collected via human annotators in a freestyle paradigm.", "startOffset": 83, "endOffset": 103}, {"referenceID": 54, "context": "Towards achieving this goal, Malinowski and Fritz (2014a) propose a Bayesian framework that connects natural language questionanswering with the visual information extracted from image parts.", "startOffset": 29, "endOffset": 58}, {"referenceID": 27, "context": "Currently, among the available benchmark datasets, only the IAPR-TC12 dataset (Grubinger et al., 2006) has multilingual descriptions (in English and German).", "startOffset": 78, "endOffset": 102}], "year": 2016, "abstractText": "Automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities. In this survey, we classify the existing approaches based on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space. We provide a detailed review of existing models, highlighting their advantages and disadvantages. Moreover, we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machine-generated image descriptions. Finally we extrapolate future directions in the area of automatic image description generation.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}