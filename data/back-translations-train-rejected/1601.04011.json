{"id": "1601.04011", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2016", "title": "Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization", "abstract": "We tighten the sample complexity of empirical risk minimization (ERM) associated with a class of generalized linear models that include linear and logistic regression. In particular, we conclude that ERM attains the optimal sample complexity for linear regression. Our analysis relies on a new notion of stability, called preconditioned stability, which may be of independent interest.", "histories": [["v1", "Fri, 15 Jan 2016 17:32:44 GMT  (17kb)", "https://arxiv.org/abs/1601.04011v1", null], ["v2", "Tue, 2 Feb 2016 11:46:18 GMT  (18kb)", "http://arxiv.org/abs/1601.04011v2", null], ["v3", "Tue, 11 Oct 2016 12:29:09 GMT  (16kb)", "http://arxiv.org/abs/1601.04011v3", null], ["v4", "Sun, 16 Apr 2017 12:15:33 GMT  (16kb)", "http://arxiv.org/abs/1601.04011v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alon gonen", "shai shalev-shwartz"], "accepted": false, "id": "1601.04011"}, "pdf": {"name": "1601.04011.pdf", "metadata": {"source": "CRF", "title": "Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization", "authors": ["Alon Gonen", "Shai Shalev-Shwartz"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 1,04 011v 4 [cs.L G] 16 A"}, {"heading": "1 Introduction", "text": "Central to statistical learning theory is the concept of (algorithmic) stability. Since its introduction by [4], deep connections have been established between the generalization ability and the algorithmic stability of a learning algorithm. It has been demonstrated by [22, 18] that stability characterizes learning ability. Furthermore, in expectation of a certain notion of stability is exactly the same as the generalization error of an algorithm (namely the gap between real loss and traction loss).In general linear learning problems, a prominent geometric property that exceeds the stability rate is the conditional number of the loss function. While uniform convergence limits ([21] [chapter 4]) usually have limits of this scale of 1 {n, where n is the size of the sample, well-conditioned problems allow faster (stability) rates linearly with 1 {n. The caveat is that the typical (large-scale) machine learning problem in science is conditioned by the poorly conditioned function, while we let the next part of the definition be the conditional one."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Setup", "text": "\"We are looking at the problem of minimizing the risk associated with a generalized linear regression.\" \"We,\" he says, \"must, however, hold us to the limits.\" \"We,\" he says, \"must hold us to the limits.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\". \"\" \"We.\" \"\" \".\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\" \".\" \"\" \".\" \"\". \"\" \".\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\". \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\" \"\""}, {"heading": "2.2 Stability", "text": "In this section we review basic definitions and results on stability., \",\", \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" we, \"\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\""}, {"heading": "2.2.1 Stability of Well-conditioned Objectives", "text": "\"We assume that it is the lowest eigenvalue of C.\" We define the functional condition as the ratio between the square Lipschitz parameter and the strong excess parameter: \"We assume that there is the lowest eigenvalue of C.\" We define the functional condition as the ratio between the square Lipschitz parameter and the strong excess parameter: \"We assume that the conditional number of the objective is defined as the product between the square Lipschitz parameter and the strong eccentricity parameter.\" We define the conditional number of the objective as the product between the empirical and the functional conditional number: \"We assume that we define the conditional number of the objective as the product between the square Lipschitz parameter and the strong excess parameter.\""}, {"heading": "3 Preconditioned Stability", "text": "We are now in a position to describe our main result. Let P be a (symmetric) positive definitive matrix, SP being the education we receive when we choose the definition of the average stability of Equation (4). (2) Our main risk is that the crucial observation is that empirical risk minimization with respect to domain WP is equivalent to ERM w.r.t. S with respect to domain W w.r.t. S with respect to domain W wpw, w \"P 1 {2wq P WP rns."}, {"heading": "4 Some Implications", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Linear Regression", "text": "Conclusion 2 (linear regression) Consider linear regression as formulated in Example (1).The expected excess risk of the ERM is limited by ES \"Dn\" 1rLpw \"q\" Lpw \"qs\" pSq \"4Y 2dn.\" If we compare the limits in (11) and episode 2, we see that the dependence on the geometry of X and W is replaced by the optimal empirical conditional number. As we have mentioned above, this gap tends to be huge in practice. As we discuss in Section 5, standard limits for this setting depend on the geometry of X and W. On the contrary, it results from the generalized Kauchy-Black inequality that applies to any choice of a standard on Rd. \""}, {"heading": "4.2 The Average Stability of Stochastic Gradient Descent", "text": "One of the most widely used algorithms in machine learning is the Stochastic Gradient Descent (SGD). Besides its simplicity, its popularity is also based on its generalization capabilities ([21] [Section 14,5]). Recently, [9] he examined the (uniform) stability of SGD in various constellations. Following our notation, Theorem 3,9 of his paper implies a limit of maxi 2\u03c1, 2i \u03b3n on the uniform stability, where it is the strong convexity of the entire object, and for all i P rns \u03c1 i is the Lipschitz parameter of i. As the proof of Lemma 3 shows, SGD can be limited by an equilibrium of p \u00b2 q and vice versa is at best an (approximate) xi} 2. Specifically, the limit depends on the choice of the coordinate system. As implied in [5] [Theorem 3,2], SGD can be considered in our context as a (non-hazard) SM coordinate system in which is the HM 1."}, {"heading": "5 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Slower rates", "text": "One of the most direct techniques for determining the limits of excess risk is the analysis of the Rademacher complexity ([3]) of the corresponding predictor class. In our environment, these techniques were used by [11] to determine the limits of order 1 {? n with respect to the generalization error of the ERM. We call these rates slower because of the lower dependence on the sample size. [3] In fact, under mild additional assumptions to X, two sentences X and W, which meet our assumptions, can be represented in this way. Suppose that X is symmetrical (i.e. x P X iff'x P X) and 0 P intpX q. Then it is known ([7] that X induces a standard to Rd by the Minkowsky function} x}: \"ppxq\" inf tt P tBu. It is immediate that the closed unit ball tx:} x \u00b2 itself is X."}, {"heading": "5.2 Dependence on Norm", "text": "Since both the uniform and the average stability of the ERM are limited by its generalization error ([22]), the limits of [11] translate into limits of the average stability. In contrast to our fast rates, the exact limits depend on the geometry of the X and W quantities. For example: a) If both X and W are the Euclidean standard sphere in Rd, then the obtained bound balance is 1 {? n. b) If X is the unit of 8 and W the unit of 1, then the obtained bound balance is a logpdq {n."}, {"heading": "5.3 Lower bounds on the excess risk", "text": "Lower limits for stochastic minimization of exp-concave functions have been investigated in [16]. For our setting, Theorem 2 in this paper implies a limit for the excessive risk of any algorithm. In the special case of linear regression with X \"tx P Rd:} x\" 2% 1u, W \"tw P Rd:} w} 2% Bu, Y\" r'Y, Y s (13) [23], the lower limit between mintY 2, B2'dY 2 n, BY? n u \u00b2 proved to be due to the ERM generalization error. The most leftmost term is trivially reached by the predictor w \"0. The middle term is achieved by combining the Vovk-Azoury-Warmuth predictor ([1, 24]) with the standard online-to-batch conversions ([6])."}, {"heading": "5.4 Stability and Regularization", "text": "Previous work ([4, 22]) examined the rate of uniform stability in different environments. For our environment, their limits in terms of expected risk are identical to the limit in term 3. As we have explained above, these fast rates are often inferior to the so-called slower rates due to the dependence on the empirical state number. The standard approach to solving this problem is to add a regularization term to the target. By adding the regularization term \u03bb} w} 2 to the target, one effectively increases the eigenvalues of C \u00b2 by \u03bb, and consequently the total state number decreases. However, as explained in [21] [Section 13.4], this modification does not usually retain the fast rates.4"}, {"heading": "5.5 Stability and Exp-concavity", "text": "In fact, the Online Newton Step (ONS) of [10] is designed for online minimization. As explained in [21] [Section 13.3], we must ensure that by optimally managing this compromise, we no longer obtain fast rates (i.e., the stability rate is 1 {? n rather than 1 {n]. The exp-concave functions achieve improved (logarithmic) limits that resemble the regrets limits for strongly convex functions ([10]). The online-to-batch analysis of [16] provides a limited excess risk that coincides with our logarithmic factors."}, {"heading": "5.6 Other Techniques and High-Probability Bounds", "text": "The limit of the expected excess risk in episode 1 can be reached by using two additional techniques. Both techniques also result in high probability limits. Next, we examine the corresponding results. A recent follow-up work of [17] set a limit on the O-pd logpnq'logp1 {\u03b4q {nq} on the excess risk of the ERM, where \u03b4 is the confidence parameter.6. It also showed how to get rid of the logpnq factor by increasing the confidence of the ERM. The evidence centers around an amber condition that applies based on the assumption of the Exp concavity. Another alternative is the limitation of the excess risk by the local radiator complexity (LRC) of the associated class of predictors (e.g. by the use of episode 5.3 in [2]). In our environment, one can derive limits of the LRC (e.g. by the use of [14]) that coincide with our limits."}, {"heading": "Acknowledgments", "text": "We thank Iliya Tolstikhin for pointing out the alternative proof of logical episode 1 on the basis of local Rademacher complexities."}], "references": [{"title": "Relative loss bounds for on-line density estimation with the exponential family of distributions", "author": ["Katy S Azoury", "Manfred K Warmuth"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Local rademacher complexities", "author": ["Peter L Bartlett", "Olivier Bousquet", "Shahar Mendelson"], "venue": "Annals of Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Stability and generalization", "author": ["Olivier Bousquet", "Andr\u00e9 Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Convex optimization: Algorithms and complexity", "author": ["S\u00e9bastien Bubeck"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Nicolo Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "A course in functional analysis, volume 96", "author": ["John B Conway"], "venue": "Springer Science & Business Media,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Solving ridge regression using sketched preconditioned svrg", "author": ["Alon Gonen", "Francesco Orabona", "Shai Shalev-Shwartz"], "venue": "arXiv preprint arXiv:1602.02350,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["Sham M Kakade", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Algorithmic stability and sanity-check bounds for leave-one-out cross-validation", "author": ["Michael Kearns", "Dana Ron"], "venue": "Neural Computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Averaging expert predictions", "author": ["Jyrki Kivinen", "Manfred K Warmuth"], "venue": "In Computational Learning Theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Oracle inequalities in empirical risk minimization and sparse recovery problems: Lecture notes", "author": ["V Koltchinskii"], "venue": "Technical report,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Fast rates for exp-concave empirical risk minimization", "author": ["Tomer Koren", "Kfir Levy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Lower and upper bounds on the generalization of stochastic exponentially concave optimization", "author": ["Mehrdad Mahdavi", "Lijun Zhang", "Rong Jin"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "From exp-concavity to variance control: O (1/n) rates and onlineto-batch conversion with high probability", "author": ["Nishant A Mehta"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization", "author": ["Sayan Mukherjee", "Partha Niyogi", "Tomaso Poggio", "Ryan Rifkin"], "venue": "Advances in Computational Mathematics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Beyond logarithmic bounds in online learning", "author": ["Francesco Orabona", "Nicolo Cesa-Bianchi", "Claudio Gentile"], "venue": "In AISTATS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Learnability, stability and uniform convergence", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "The sample complexity of learning linear predictors with the squared loss", "author": ["Ohad Shamir"], "venue": "arXiv preprint arXiv:1406.5143,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Competitive on-line statistics", "author": ["Volodya Vovk"], "venue": "International Statistical Review,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}], "referenceMentions": [{"referenceID": 11, "context": "Abstract We show that the average stability notion introduced by [12, 4] is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses.", "startOffset": 65, "endOffset": 72}, {"referenceID": 3, "context": "Abstract We show that the average stability notion introduced by [12, 4] is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses.", "startOffset": 65, "endOffset": 72}, {"referenceID": 8, "context": "b) We strengthen the recent bounds of [9] on the stability rate of the Stochastic Gradient Descent algorithm.", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "Since being introduced by [4], deep connections between the generalization ability and the algorithmic stability of a learning algorithm have been established.", "startOffset": 26, "endOffset": 29}, {"referenceID": 21, "context": "It was shown by [22, 18] that stability characterizes learnability.", "startOffset": 16, "endOffset": 24}, {"referenceID": 17, "context": "It was shown by [22, 18] that stability characterizes learnability.", "startOffset": 16, "endOffset": 24}, {"referenceID": 20, "context": "While uniform convergence bounds ([21][Chapter 4]) mostly yield bounds that scale with 1{?n, where n is the size of the sample, well-conditioned problems admit faster (stability) rates that scale linearly with 1{n.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "Our main example is the following formulation of linear regression ([19]).", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "More generally, our setting captures all known exp-concave functions ([13]).", "startOffset": 70, "endOffset": 74}, {"referenceID": 20, "context": ", the bound on the uniform stability in [21][Corollary 13.", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "using [20][Lemma 2.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "This matches the bound in [21][Corollary 13.", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": ", see the empirical study in [8]).", "startOffset": 29, "endOffset": 32}, {"referenceID": 20, "context": "Besides its computational simplicity, its popularity stems also from its generalization abilities ([21][Section 14.", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "Recently, [9] studied the (uniform) stability of SGD in various settings.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "As implied by [5][Theorem 3.", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "1 Slower rates One of the most direct techniques for establishing bounds on the excess risk is via analyzing the Rademacher complexity ([3]) of the associated class of predictors.", "startOffset": 136, "endOffset": 139}, {"referenceID": 10, "context": "In our setting, these techniques have been employed by [11] to establish bounds of order 1{?n on the generalization error of ERM.", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Then it is known ([7]) that X induces a norm on R through the Minkowsky functional }x} :\u201c ppxq \u201c inf tt P R : x P tBu .", "startOffset": 18, "endOffset": 21}, {"referenceID": 21, "context": "2 Dependence on Norm Note that since both the uniform and the average stability of ERM are bounded above by its generalization error ([22]), the bounds of [11] translate into bounds on the average stability.", "startOffset": 134, "endOffset": 138}, {"referenceID": 10, "context": "2 Dependence on Norm Note that since both the uniform and the average stability of ERM are bounded above by its generalization error ([22]), the bounds of [11] translate into bounds on the average stability.", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "3 Lower bounds on the excess risk Lower bounds for stochastic minimization of exp-concave functions have been studied in [16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 22, "context": "For the special case of linear regression with X \u201c tx P R : }x}2 \u010f 1u, W \u201c tw P R : }w}2 \u010f Bu, Y \u201c r \u0301Y, Y s (13) [23] proved the lower bound \u03a9 \u0301", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "The middle term is attained by combining the Vovk-Azoury-Warmuth forecaster ([1, 24]) with standard online-to-batch conversions ([6]).", "startOffset": 77, "endOffset": 84}, {"referenceID": 23, "context": "The middle term is attained by combining the Vovk-Azoury-Warmuth forecaster ([1, 24]) with standard online-to-batch conversions ([6]).", "startOffset": 77, "endOffset": 84}, {"referenceID": 5, "context": "The middle term is attained by combining the Vovk-Azoury-Warmuth forecaster ([1, 24]) with standard online-to-batch conversions ([6]).", "startOffset": 129, "endOffset": 132}, {"referenceID": 10, "context": "Last, the right term is attained by ERM, as implied by the aforementioned upper bound of [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 22, "context": "However, since in the setting of [23] (Equation (13)) the magnitude of the predictions is not uniformly upper bounded by Y , no contradiction arises.", "startOffset": 33, "endOffset": 37}, {"referenceID": 3, "context": "4 Stability and Regularization Previous work ([4, 22]) studied the rate of uniform stability in various settings.", "startOffset": 46, "endOffset": 53}, {"referenceID": 21, "context": "4 Stability and Regularization Previous work ([4, 22]) studied the rate of uniform stability in various settings.", "startOffset": 46, "endOffset": 53}, {"referenceID": 20, "context": "However, as explained in [21][Section 13.", "startOffset": 25, "endOffset": 29}, {"referenceID": 9, "context": "Indeed, the Online Newton Step (ONS) of [10], which has been designed for online minimization Namely, when tuning \u03bb, we need to ensure that any \u01eb{2-approximate minimizer with respect to the regularized objective is also an \u01eb-approximate minimizer with respect to the unregularized objective.", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "As explained in [21][Section 13.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "of exp-concave functions, achieves improved (logarithmic) regret bounds that resemble the regret bounds for strongly convex functions ([10]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "The online-to-batch analysis of [16] yields a bound on the excess risk that coincides with our bounds up to logarithmic factors.", "startOffset": 32, "endOffset": 36}, {"referenceID": 14, "context": "This question was answered affirmatively by [15].", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "While exp-concavity is weaker than strong convexity, [15][section 4.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "The above interpretation of [15] inspired us to make one step forward and directly show that regularization is not required as long as a related (preconditioned) problem is well conditioned.", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "The upper bound of [15] on the excess risk scales with 24\u03b2d \u1fb1n \u201c 24\u03b2d\u03c1 \u03b1n (recall that the exp-concavity parameter \u1fb1 is equal to \u03b1{\u03c12).", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "This resolves the question raised by [15] regarding the necessity of the smoothness assumption.", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "Note that for linear regression, the smoothness is 1, making our bounds identical to the bounds of [15] for this special case.", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "As discussed in [15], it is difficult to translate bounds on the average stability into high-probability bounds (while preserving the fast rate and introducing only logarithmic dependence on 1{\u03b4).", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "A recent follow-up work by [17] established a bound of \u00d5pd logpnq ` logp1{\u03b4q{nq on the excess risk of ERM, where \u03b4 is the confidence parameter.", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "3 in [2]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 13, "context": ", using [14]) which coincide with our bounds.", "startOffset": 8, "endOffset": 12}], "year": 2017, "abstractText": "We show that the average stability notion introduced by [12, 4] is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses. In other words, when analyzing the stability rate of a given algorithm, we may assume the optimal preconditioning of the data. This implies that, at least from a statistical perspective, explicit regularization is not required in order to compensate for ill-conditioned data, which stands in contrast to a widely common approach that includes a regularization for analyzing the sample complexity of generalized linear models. Several important implications of our findings include: a) We demonstrate that the excess risk of empirical risk minimization (ERM) is controlled by the preconditioned stability rate. This immediately yields a relatively short and elegant proof for the fast rates attained by ERM in our context. b) We strengthen the recent bounds of [9] on the stability rate of the Stochastic Gradient Descent algorithm.", "creator": "LaTeX with hyperref package"}}}