{"id": "1601.04155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2016", "title": "Brain-Inspired Deep Networks for Image Aesthetics Assessment", "abstract": "Image aesthetics assessment has been challenging due to its subjective nature. Being extensively inspired by the scientific advances in the human visual perception and neuroaesthetics, we design the Brain-Inspired Deep Networks (BDN) for this task. BDN first learns attributes through the parallel supervised pathways, on a variety of selected feature dimensions. A high-level synthesis network is trained to associate and transform those attributes into the overall aesthetics rating. We then extend BDN to predicting the distribution of human ratings, since aesthetics ratings often vary somewhat from observer to observer. Another highlight is our first-of-its-kind study of label-preserving transformations in the context of aesthetics assessment, which leads to effective data augmentation approaches. Experimental results on the AVA dataset show that our biological inspired, task-specific BDN model leads to significantly improved performances, compared to other state-of-the-art models with the same or higher parameter capacity.", "histories": [["v1", "Sat, 16 Jan 2016 10:59:40 GMT  (2510kb,D)", "http://arxiv.org/abs/1601.04155v1", null], ["v2", "Tue, 15 Mar 2016 03:46:27 GMT  (2539kb,D)", "http://arxiv.org/abs/1601.04155v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["zhangyang wang", "shiyu chang", "florin dolcos", "diane beck", "ding liu", "thomas s huang"], "accepted": false, "id": "1601.04155"}, "pdf": {"name": "1601.04155.pdf", "metadata": {"source": "CRF", "title": "Brain-Inspired Deep Networks for Image Aesthetics Assessment", "authors": ["Zhangyang Wang", "Florin Dolcos", "Diane Beck", "Shiyu Chang", "Thomas S. Huang"], "emails": ["t-huang1}@illinois.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "1.1. Related Work", "text": "Most people who are able to identify themselves are able to identify themselves, most of them are able to identify themselves, most of them are able to identify themselves, most of them are able to identify themselves, most of them are able to identify themselves, most of them are able to identify themselves, most of them are able to identify themselves, most of them are able to identify themselves, most of them are able to identify themselves, most of them are able to identify themselves, most of them are able to identify themselves, most of them are able to identify themselves, most of them are able to identify themselves."}, {"heading": "2. Biological Inspirations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Summary of Scientistic Advances", "text": "Recent advances in neuroaesthetics suggest that human perception of aesthetics is a highly complicated and systematic process. Multiple parallel processing strategies, comprising more than a dozen retinal ganglion cell types, can be found in the retina. Each ganglion cell type tiles the retina to focus on a particular type of trait and provides complete representation across the entire visual field [26]. Retinal ganglion cells project parallel from the retina, through the lateral genicled nucleus of the thalamus into the primary visual cortex. Beyond the primary visual cortex, the primary visual cortex receives parallel inputs from the thalamus and uses modularity defined spatially and by cell-specific connectivity to support these inputs into new parallel outputs. Beyond the primary visual cortex, visual combinations are distinct, but distinct, interdoral and distinct behavioral targets."}, {"heading": "2.2. Principled Design Insights", "text": "So far, the computer model of deep learning has been closely linked to a class of theories of brain development. [13] For example, the design of CNNs follows the discovery of general human visual mechanisms [27], suggesting the usefulness of ideas borrowed from neurobiological processes. However, all current \"deep\" models remain extremely simple compared to the breadth and complexity of biological information processing. [29] It is shown that a single neuron is likely to be more complex than an entire CNN [29], not to mention our lack of knowledge of the electrochemical properties of cells and Internet neurons. We argue that it is neither impractical nor necessary to design a model that accurately reproduces the full perceptual process in the human brain, we take the typical example of a human being able to fly without the complexity and fluidity of fluttering wings. Starting from the underlying neuroscientific principles [6, 26] we conclude the following visual, but important, multi-purpose models that inspire our brains are:"}, {"heading": "3. Brain-Inspired Deep Networks", "text": "The architecture of Brain-Inspired Deep Networks (BDN) is illustrated in Figure 1, which is divided into two phases based on the above findings. In short, we first learn about attributes through parallel (supervised) paths through the selected feature dimensions and then correlate them with general aesthetic assessments in the high-level synthesis network."}, {"heading": "3.1. Attribute Learning via Parallel Pathways", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Selecting Feature Dimensions", "text": "We first select characteristic dimensions that are highly related to the assessment of aesthetics. Despite the lack of strict rules, it is believed that certain visual features appeal to people more than others. [9] We use these photographic or psychological-inspired characteristic dimensions as precursors and force BDN to \"focus on them.\" The previous work [9, 18] identified a number of aesthetically discriminatory features, among which light exposure, saturation, and hue play indispensable roles. We assume that the RGB data of each image is converted into the HSV color space, generating two-dimensional matrices IH, IS, and IV, each having the same size as the original image. In addition, many characteristics of the photographic style influence the aesthetic judgments of people. [9] Six sets of photographic styles have been proposed, including the rule of the third composition, textures, shapes, and low sharpness (DOF)."}, {"heading": "3.1.2 Parallel Supervised Pathways", "text": "Among the 17 feature dimensions, the simplest three, IH, IS and IV, are directly derived from the input. However, the remaining 14 feature dimensions are not well-defined in quality; their attributes are not easy to extract.1 The 14 photographic styles are selected specifically on the AVA datasets. We do not believe they represent all aesthetically relevant visual information and plan to have more photographic styles with annotations. For each style category as feature dimension, we create binary individual labels by labeling images with style notation as \"1\" and otherwise \"0,\" following many previous works [25, 10]. In [22], the authors used style labels to train a style classification column in their network that serves as a static regulation and is not aligned with the general prediction. Instead, we design a special architecture called supervised paths. Each path is modeled as a convolutionary image (CNN)."}, {"heading": "3.2. Training The High-Level Synthesis Network", "text": "Finally, we simulate the high-level association and synthesis of the brain using a larger FCNN. Its architecture is similar to Fig. 2, except that the first three sinuous layers each have 128 channels instead of 64. The high-level synthesis network takes the attributes of all parallel paths as inputs and gives the overall evaluation of aesthetics."}, {"heading": "4. Predicting The Distribution Representation", "text": "Most existing studies [9] apply a scalar value to represent the predicted aesthetic quality, which seems insufficient to capture the true subjective nature. For example, two images with the same mean could exhibit very different deviations between the evaluators. Typically, an image with a large valuation variation is more likely to be angular, or subject to interpretation. [22] Assigned images with binary aesthetic labels, i.e. high quality and low quality, could be very different by swelling their mean ratings, which allow less informative monitoring due to the large variation within the class. [32] It has been proposed to represent the ratings as a distribution on predefined ordinary basic ratings. \u2212 However, such a structural label could be very noisy due to the rough grid of basic ratings, the limited sample size (number of ratings) per image, and the lack of a shift in the robustness of their L2-based losses."}, {"heading": "5. Study Label-Preserving Transformations", "text": "In the formation of deep networks, the most common approach to reducing over-adaptation is to artificially enlarge the data set by using marker-preserving transformations [3]. In [19], image translations and horizontal reflections are generated, while the intensities of the RGB channels are altered, both of which do not appear to change the labels of the object categories. Other alternatives, such as random noise, rotations, deformations, and scales, are also widely accepted by recent deepening-based object identifications. However, so far, there have been few studies identifying marker-preserving transformations for the assessment of image aesthetics, such as those that do not significantly alter human aesthetics, taking into account valuation-based labels, are highly subjective. In [22], motivated by their need to create fixed inputs, the authors will select randomly cropped local regions from image aesthetics that are empirically treated as data augmentation."}, {"heading": "6. Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Settings", "text": "We implement our models on the basis of the Cuda-Convnet package [19]. ReLU nonlinearity is applied as well as the dropout. Batch size is set at 128. Since BDN is fully convolutional, there is no need to normalize the input size. Fine-tuning the entire BDN model typically takes about a day. For IH, IS and IV, we reduce it to 1 / 4 of the original size to improve training efficiency. We find that performance is hardly affected, which is understandable since human perception of these features is insensitive to scale changes. Adapting the learning rates in such a hierarchical model requires particular attention. We first train the 14 parallel learning paths with identical learning rates:"}, {"heading": "6.2. Binary Rating Prediction", "text": "We compare BDN with the state-of-the-art RAPID model for binary aesthetics assessment prediction. Benefiting from our completely revolutionary architecture, the BDN model has a much lower parameter capacity than RAPID, which relies on fully connected layers. In addition, we manually design three base networks, all of which have exactly the same parameter capacity as BDN: \u2022 Baseline Full Convolutionary Network (BFCN) first binds the constellation of 14 layers horizontally, creating a three-layer, fully revolutionary network, each layer possessing 64 \u00d7 14 = 896 filter channels. The attribute learning part is trained in an uncontrolled manner, and then linked to the above five binding ratings models, with which we provide binding ratings for binary predictions, with both layers. The overall accuracy is compared in Table 3."}, {"heading": "6.3. Rating Distribution Prediction", "text": "To the best of our knowledge, it replaces all state-of-the-art models running on the latest large-format datasets, BDN is the only one that takes into account the prediction of the evaluation distribution. We use the BDN binary prediction as initialization and train only the high-level synthesis network with the loss defined in Eqn. (4) Then we compare the predicted distributions with the basic truth of the AVA test set. We also include two other BDN variants as baselines in this task: \u2022 BDN with the Softmax loss of the evaluation vectors (BDN-Soft-D) makes the only change in architecture by modifying the global average summary of the high-level network to be 10-channel. Its results are compared with the raw evaluation distribution under the conventional soft-max loss (i.e., cross-entropy function). \u2022 BDN with the loss of the evaluation engines (Soft-DD)."}, {"heading": "7. Conclusion", "text": "The biologically inspired task-specific architecture of BDN results in superior performance compared to other state-of-the-art models with the same or higher parameter capacity. Since it has been observed in Figures 4 and 5 that emotions and contexts can alter aesthetic judgment, our immediate next work is to consider the two factors for a broader framework."}], "references": [{"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning, 2(1):1\u2013127", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "A framework for photo-quality assessment and enhancement based on visual aesthetics", "author": ["S. Bhattacharya", "R. Sukthankar", "M. Shah"], "venue": "Proceedings of the international conference on Multimedia, pages 271\u2013280. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Rank analysis of incomplete block designs the method of paired comparisons", "author": ["R.A. Bradley", "M.E. Terry"], "venue": "Biometrika, 39(3-4):324\u2013345", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1952}, {"title": "The neural foundations of aesthetic appreciation", "author": ["C.J. Cela-Conde", "L. Agnati", "J.P. Huston", "F. Mora", "M. Nadal"], "venue": "Progress in neurobiology, 94(1):39\u201348", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Prospects for a cognitive neuroscience of visual aesthetics", "author": ["A. Chatterjee"], "venue": "Bulletin of Psychology and the Arts, 4(2):55\u2013 60", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Neuroaesthetics: a coming of age story", "author": ["A. Chatterjee"], "venue": "Journal of Cognitive Neuroscience, 23(1):53\u201362", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to photograph", "author": ["B. Cheng", "B. Ni", "S. Yan", "Q. Tian"], "venue": "Proceedings of the international conference on Multimedia, pages 291\u2013300. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Studying aesthetics in photographic images using a computational approach", "author": ["R. Datta", "D. Joshi", "J. Li", "J.Z. Wang"], "venue": "Computer Vision\u2013ECCV 2006, pages 288\u2013301. Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "High level describable attributes for predicting aesthetics and interestingness", "author": ["S. Dhar", "V. Ordonez", "T.L. Berg"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1657\u20131664. IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv preprint arXiv:1310.1531", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Photo quality assessment with dcnn that understands image well", "author": ["Z. Dong", "X. Shen", "H. Li", "X. Tian"], "venue": "MultiMedia Modeling, pages 524\u2013535. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Rethinking innateness: A connectionist perspective on development", "author": ["J.L. Elman"], "venue": "volume 10. MIT press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Information processing in the primate retina: circuitry and coding", "author": ["G. Field", "E. Chichilnisky"], "venue": "Annu. Rev. Neurosci., 30:1\u201330", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Multiple aesthetic attribute assessment by exploiting relations among aesthetic attributes", "author": ["Z. Gao", "S. Wang", "Q. Ji"], "venue": "Proceedings of the 5th ACM on International Conference on Multimedia Retrieval, pages 575\u2013578. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "A model of saliency-based visual attention for rapid scene analysis", "author": ["L. Itti", "C. Koch", "E. Niebur"], "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence, (11):1254\u20131259", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Aesthetics and emotions in images", "author": ["D. Joshi", "R. Datta", "E. Fedorovskaya", "Q.-T. Luong", "J.Z. Wang", "J. Li", "J. Luo"], "venue": "Signal Processing Magazine, IEEE, 28(5):94\u2013115", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "The design of high-level features for photo quality assessment", "author": ["Y. Ke", "X. Tang", "F. Jing"], "venue": "Computer Vision and Pat-  tern Recognition, 2006 IEEE Computer Society Conference on, volume 1, pages 419\u2013426. IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv preprint arXiv:1312.4400", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Distinctive image features from scaleinvariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision, 60(2):91\u2013110", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Rapid: Rating pictorial aesthetics using deep learning", "author": ["X. Lu", "Z. Lin", "H. Jin", "J. Yang", "J.Z. Wang"], "venue": "Proceedings of the ACM International Conference on Multimedia, pages 457\u2013466. ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Content-based photo quality assessment", "author": ["W. Luo", "X. Wang", "X. Tang"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 2206\u20132213. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Assessing the aesthetic quality of photographs using generic image descriptors", "author": ["L. Marchesotti", "F. Perronnin", "D. Larlus", "G. Csurka"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 1784\u20131791. IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Ava: A largescale database for aesthetic visual analysis", "author": ["N. Murray", "L. Marchesotti", "F. Perronnin"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2408\u20132415. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Parallel processing strategies of the primate visual system", "author": ["J.J. Nassi", "E.M. Callaway"], "venue": "Nature Reviews Neuroscience, 10(5):360\u2013372", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "et al", "author": ["J.D. Power", "A.L. Cohen", "S.M. Nelson", "G.S. Wig", "K.A. Barnes", "J.A. Church", "A.C. Vogel", "T.O. Laumann", "F.M. Miezin", "B.L. Schlaggar"], "venue": "Functional network organization of the human brain. Neuron, 72(4):665\u2013678", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "How parallel is visual processing in the ventral pathway? Trends in cognitive sciences", "author": ["G.A. Rousselet", "S.J. Thorpe", "M. Fabre-Thorpe"], "venue": "8(8):363\u2013370", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Functional topography in the human cerebellum: a meta-analysis of neuroimaging studies", "author": ["C.J. Stoodley", "J.D. Schmahmann"], "venue": "Neuroimage, 44(2):489\u2013501", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "A feature-integration theory of attention", "author": ["A.M. Treisman", "G. Gelade"], "venue": "Cognitive psychology, 12(1):97\u2013136", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1980}, {"title": "Computational models of visual attention", "author": ["J.K. Tsotsos", "A. Rothenstein"], "venue": "Scholarpedia, 6(1):6201", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to predict the perceived visual quality of photos", "author": ["O. Wu", "W. Hu", "J. Gao"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 225\u2013232. IEEE", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 2018\u20132025. IEEE", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "Automated assessment or rating of pictorial aesthetics has many applications, such as in an image retrieval system or a picture editing software [8].", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "Those features are designed under the guidance of photography and psychological rules, such as ruleof-thirds composition, depth of field (DOF), and colorfulness [9, 18].", "startOffset": 161, "endOffset": 168}, {"referenceID": 17, "context": "Those features are designed under the guidance of photography and psychological rules, such as ruleof-thirds composition, depth of field (DOF), and colorfulness [9, 18].", "startOffset": 161, "endOffset": 168}, {"referenceID": 21, "context": "[22] proposed the Rating Pictorial Aesthetics using Deep Learning (RAPID) model, with impressive accuracies on the Aesthetic Visual Analysis (AVA) dataset [25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[22] proposed the Rating Pictorial Aesthetics using Deep Learning (RAPID) model, with impressive accuracies on the Aesthetic Visual Analysis (AVA) dataset [25].", "startOffset": 155, "endOffset": 159}, {"referenceID": 31, "context": "They have not yet studied more precise predictions, such as finer-grain ratings or rating distributions [32].", "startOffset": 104, "endOffset": 108}, {"referenceID": 4, "context": "Furthermore, the study of the cognitive and neural underpinnings of aesthetic appreciation by means of neuroimaging techniques yields some promise for understanding human aesthetics [5].", "startOffset": 182, "endOffset": 185}, {"referenceID": 6, "context": "Although the results of these studies have been somewhat divergent, a hierarchical set of core mechanisms involved in aesthetic preference have been identified [7].", "startOffset": 160, "endOffset": 163}, {"referenceID": 0, "context": "Whereas deep learning is well known to be inspired by and analogous to brain mechanisms [1], there is hardly any work providing the synergy between the neuroaesthetics knowledge and the advances of learning-based aesthetics assessment models.", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "BDN clearly distinguishes itself from prior models, for its unique architecture inspired by the hierarchical information processing phases from the neuroaesthetics study [5].", "startOffset": 170, "endOffset": 173}, {"referenceID": 24, "context": "We examine the BDN model on the large-scale AVA dataset [25], for both binary rating prediction and rating distribution prediction tasks, and confirms its superiority over a few competitive methods with the same or higher parameter capacity.", "startOffset": 56, "endOffset": 60}, {"referenceID": 24, "context": "com [25].", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "[9] first casted the image aesthetics assessment problem as a classification or regression problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The earliest work [9, 18] extracted various handcrafted features, including low-level image statistics such as distributions of edges and color histograms, and high-level photographic rules such as the rule of thirds.", "startOffset": 18, "endOffset": 25}, {"referenceID": 17, "context": "The earliest work [9, 18] extracted various handcrafted features, including low-level image statistics such as distributions of edges and color histograms, and high-level photographic rules such as the rule of thirds.", "startOffset": 18, "endOffset": 25}, {"referenceID": 1, "context": "A part of subsequent efforts, such as [2, 10, 23], focus on improving the quality of those features.", "startOffset": 38, "endOffset": 49}, {"referenceID": 9, "context": "A part of subsequent efforts, such as [2, 10, 23], focus on improving the quality of those features.", "startOffset": 38, "endOffset": 49}, {"referenceID": 22, "context": "A part of subsequent efforts, such as [2, 10, 23], focus on improving the quality of those features.", "startOffset": 38, "endOffset": 49}, {"referenceID": 23, "context": "Generic image features [24], such as SIFT and Fisher Vector [21], have also been applied to predict aesthetics.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "Generic image features [24], such as SIFT and Fisher Vector [21], have also been applied to predict aesthetics.", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "Deep networks [1] attempt to emulate the underlying complex neural mechanisms of human perception, and display the ability to describe image content from the primitive level (low-level features) to the abstract level (high-level features).", "startOffset": 14, "endOffset": 17}, {"referenceID": 21, "context": "The RAPID model [22] is among the first to apply deep convolutional neural networks (CNN) [19] to the aesthetics rating prediction, where the features are automatically learned.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "The RAPID model [22] is among the first to apply deep convolutional neural networks (CNN) [19] to the aesthetics rating prediction, where the features are automatically learned.", "startOffset": 90, "endOffset": 94}, {"referenceID": 24, "context": "They further improved the model by exploring style annotations [25] associated with images.", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "In fact, even the hidden activations from a generic CNN architecture prove to work reasonably well for aesthetics features [12].", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "For example, RAPID [22] simply divided all samples as aesthetic or unaesthetic, and trained a binary classification model.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "However, it is common for different users to rate visual subjects inconsistently or even oppositely due to the subjective problem nature [18].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "emotion [17], it is difficult for individuals to reliably convert their experiences to a single rating, resulting in noisy estimates of real aesthetic responses.", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "In [32], Wu et.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "[15] formulated the aesthetic assessment as a multi-label task, where multiple aesthetic attributes were predicted jointly via bayesian networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Each ganglion cell type tiles the retina to focus on one specific kind of feature, and provide a complete representation across the entire visual field [26].", "startOffset": 152, "endOffset": 156}, {"referenceID": 26, "context": "Beyond primary visual cortex, separate but interacting dorsal and ventral streams perform distinct computations on similar visual information to support distinct behavioural goals [27].", "startOffset": 180, "endOffset": 184}, {"referenceID": 4, "context": "Independent groups of cells with different functions are brought into temporary association, by a so-called \u201cbinding\u201d mechanism [5], for the final decision-making.", "startOffset": 128, "endOffset": 131}, {"referenceID": 29, "context": "More specifically, from the retina to the prefrontal cortex, the human visual processing system will first conduct a very rapid holistic image analysis [30, 16, 31].", "startOffset": 152, "endOffset": 164}, {"referenceID": 15, "context": "More specifically, from the retina to the prefrontal cortex, the human visual processing system will first conduct a very rapid holistic image analysis [30, 16, 31].", "startOffset": 152, "endOffset": 164}, {"referenceID": 30, "context": "More specifically, from the retina to the prefrontal cortex, the human visual processing system will first conduct a very rapid holistic image analysis [30, 16, 31].", "startOffset": 152, "endOffset": 164}, {"referenceID": 13, "context": "The divergence comes at a later stage, in how the low-level visual features are further processed through parallel pathways [14] before being utilized.", "startOffset": 124, "endOffset": 128}, {"referenceID": 27, "context": "For example, there is evidence [28] that neurons in V1 code for relatively simple features such as local contours and colors, whereas neurons in TE fire in response to more abstractive features, that encode the scene\u2019s gist and/or saliency information and act as a holistic signature of the input.", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "So far, the computational model of deep learning has been closely related to a class of theories of brain development [13].", "startOffset": 118, "endOffset": 122}, {"referenceID": 26, "context": "For example, the design of CNNs follows the discovery of general human vision mechanisms [27], indicating the usefulness of ideas borrowed from neurobiological processes.", "startOffset": 89, "endOffset": 93}, {"referenceID": 28, "context": "It is demonstrated that a single neuron is probably more complex than an entire CNN [29], not to mention our lack of knowledge in the cells\u2019 electrochemical properties and inter-neuron interactions.", "startOffset": 84, "endOffset": 88}, {"referenceID": 5, "context": "Starting from the the underlying neuroscience principles [6, 26], we conclude the following simplified, but important insights that inspire our model:", "startOffset": 57, "endOffset": 64}, {"referenceID": 25, "context": "Starting from the the underlying neuroscience principles [6, 26], we conclude the following simplified, but important insights that inspire our model:", "startOffset": 57, "endOffset": 64}, {"referenceID": 4, "context": "Step 2 and 3 are derived from the existing neuroaesthetics advances, that aesthetics judgments evidently involve multiple pathways, which could connect from related perception tasks [5, 7, 6].", "startOffset": 182, "endOffset": 191}, {"referenceID": 6, "context": "Step 2 and 3 are derived from the existing neuroaesthetics advances, that aesthetics judgments evidently involve multiple pathways, which could connect from related perception tasks [5, 7, 6].", "startOffset": 182, "endOffset": 191}, {"referenceID": 5, "context": "Step 2 and 3 are derived from the existing neuroaesthetics advances, that aesthetics judgments evidently involve multiple pathways, which could connect from related perception tasks [5, 7, 6].", "startOffset": 182, "endOffset": 191}, {"referenceID": 8, "context": "Despite the lack of firm rules, certain visual features are believed to please humans more than others [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "The previous work [9, 18] has identified a set of aesthetically discriminative features, among which the light exposure, saturation and hue play indispensable roles.", "startOffset": 18, "endOffset": 25}, {"referenceID": 17, "context": "The previous work [9, 18] has identified a set of aesthetically discriminative features, among which the light exposure, saturation and hue play indispensable roles.", "startOffset": 18, "endOffset": 25}, {"referenceID": 8, "context": "[9] proposed six sets of photographic styles, including the rule of thirds composition, textures, shapes, and shallow depth-of-field (DOF).", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "For each style category as a feature dimension, we create binary individual labels, by labelling images with the style annotation as \u201c1\u201d and otherwise \u201c0\u201d, which follows many previous work [25, 10].", "startOffset": 189, "endOffset": 197}, {"referenceID": 9, "context": "For each style category as a feature dimension, we create binary individual labels, by labelling images with the style annotation as \u201c1\u201d and otherwise \u201c0\u201d, which follows many previous work [25, 10].", "startOffset": 189, "endOffset": 197}, {"referenceID": 21, "context": "In [22], the authors utilized style labels to train a style classier column in their network, which serves as a static regularization and is not jointly tuned with the overall prediction.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "The choice of FCNN is motivated by the spatial localitypreserving characteristics of human brain\u2019s low-level visual perception [27].", "startOffset": 127, "endOffset": 131}, {"referenceID": 32, "context": "We construct a 4-layer Stacked Convolutional Auto Encoder (SCAE): its first 2 layers follows the same topology as the conv1 and conv2 layers, and the last 2 layers are mirror-symmetrical deconvolutional layers [33].", "startOffset": 210, "endOffset": 214}, {"referenceID": 10, "context": "The strategy is based on the common belief that the lower layers of CNNs learn generalpurpose features, such as edges and contours, which could be adapted for extensive high-level tasks [11].", "startOffset": 186, "endOffset": 190}, {"referenceID": 19, "context": "It is followed by the global average pooling [20] step, to be correlated with the binary labels.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "In the end, the conv3 layer activations of each pathway are utilized as attributes [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 8, "context": "Most existing studies [9] apply a scalar value to represent the predicted aesthetics quality, which appears insufficient to capture the true subjective nature.", "startOffset": 22, "endOffset": 25}, {"referenceID": 21, "context": "[22] assigned images with binary aesthetics labels, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] suggested to represent the ratings as a distribution on pre-defined ordinal basic ratings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "The previous study of the AVA datasets [25] reveals two important facts:", "startOffset": 39, "endOffset": 43}, {"referenceID": 24, "context": "According to [25], Gaussian functions perform adequately good approximations to fit the rating distributions of 99.", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "Assuming the underlaying distribution N1(\u03bc1, \u03c31) and the predicted distribution N2(\u03bc2, \u03c32), their difference could be calculated by the classical Kullback-Leibler (KL) divergence [3]:", "startOffset": 179, "endOffset": 182}, {"referenceID": 0, "context": "But different from the binary prediction task where the output denotes a Bernoulli distribution over [0, 1] labels, the two elements in the output here denote the predicted mean and variance, respectively.", "startOffset": 101, "endOffset": 107}, {"referenceID": 2, "context": "When training deep networks, the most common approach to reduce overfitting is to artificially enlarge the dataset using label-preserving transformations [3].", "startOffset": 154, "endOffset": 157}, {"referenceID": 18, "context": "In [19], Table 2.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "63 Alter RGB Perturbed the intensities of the RGB channels [19] 0.", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "In [22], motivated by their need to create fixed-size inputs, the authors created randomly-cropped local regions from training images, which could be empirically treated as data augmentation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "We fit a Bradley-Terry [4] model to estimate the subjective scores for each method so that they can be ranked.", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "With groundtruth set as score 1, each transformation will receive a score between [0, 1].", "startOffset": 82, "endOffset": 88}, {"referenceID": 18, "context": "We implement our models based on the cuda-convnet package [19].", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "For binary prediction, we follow RAPID [22] to quantize images\u2019 mean ratings into binary values.", "startOffset": 39, "endOffset": 43}, {"referenceID": 21, "context": "2The accuracies of RAPID are from the RDCNN results in Table 3 [22] Table 4.", "startOffset": 63, "endOffset": 67}, {"referenceID": 2, "context": ", relative entropy) [3].", "startOffset": 20, "endOffset": 23}, {"referenceID": 24, "context": "While the raw ratings can be noisy due to both the coarse rating grid and the limited rating number, we are able to obtain a more robust estimation of the underlying rating distribution, with the aid of the strong Gaussian prior from the AVA study [25].", "startOffset": 248, "endOffset": 252}], "year": 2017, "abstractText": "Image aesthetics assessment has been challenging due to its subjective nature. Being extensively inspired by the scientific advances in the human visual perception and neuroaesthetics, we design the Brain-Inspired Deep Networks (BDN) for this task. BDN first learns attributes through the parallel supervised pathways, on a variety of selected feature dimensions. A high-level synthesis network is trained to associate and transform those attributes into the overall aesthetics rating. We then extend BDN to predicting the distribution of human ratings, since aesthetics ratings often vary somewhat from observer to observer. Another highlight is our first-of-its-kind study of label-preserving transformations in the context of aesthetics assessment, which leads to effective data augmentation approaches. Experimental results on the AVA dataset show that our biological inspired, task-specific BDN model leads to significantly improved performances, compared to other state-of-the-art models with the same or higher parameter capacity.", "creator": "LaTeX with hyperref package"}}}