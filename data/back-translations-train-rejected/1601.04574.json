{"id": "1601.04574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2016", "title": "SimpleDS: A Simple Deep Reinforcement Learning Dialogue System", "abstract": "This paper presents 'SimpleDS', a simple and publicly available dialogue system trained with deep reinforcement learning. In contrast to previous reinforcement learning dialogue systems, this system avoids manual feature engineering by performing action selection directly from raw text of the last system and (noisy) user responses. Our initial results, in the restaurant domain, show that it is indeed possible to induce reasonable dialogue behaviour with an approach that aims for high levels of automation in dialogue control for intelligent interactive agents.", "histories": [["v1", "Mon, 18 Jan 2016 15:37:22 GMT  (140kb,D)", "http://arxiv.org/abs/1601.04574v1", "International Workshop on Spoken Dialogue Systems (IWSDS), 2016"]], "COMMENTS": "International Workshop on Spoken Dialogue Systems (IWSDS), 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["heriberto cuay\\'ahuitl"], "accepted": false, "id": "1601.04574"}, "pdf": {"name": "1601.04574.pdf", "metadata": {"source": "CRF", "title": "SimpleDS: A Simple Deep Reinforcement Learning Dialogue System", "authors": ["Heriberto Cuay\u00e1huitl"], "emails": ["hc213@hw.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Nearly two decades ago, the community of (spoken) dialogue systems adopted the Reinforcement Learning (RL) paradigm because it offered the opportunity to treat dialogue design as an optimization problem, and because RL-based systems can improve their performance with experience over time. Although a large number of methods for training (spoken) dialogue systems using RL have been proposed, the question \"how dialogue strategies can be trained in an efficient, scalable, and effective way across domains?\" remains an open problem. One limitation of current approaches is the fact that RL-based dialogue systems still require a high level of human intervention (from system developers), as opposed to automation of dialogue design. The formation of a system of this kind requires a systems developer to develop a set of features to describe the dialogue state, a set of measures to control interaction, and a performance function to reward or punish the action process, all of these elements need to be developed carefully or a good policy."}, {"heading": "2 Deep Reinforcement Learning for Dialogue Control", "text": "A Reinforcement Learning (RL) agent learns its behavior from interacting with an environment and the physical or virtual agents within Q = Q + rt Q, where situations are mapped to actions by maximizing a long-term reward signal [4]. An RL agent is typically characterized by: (i) a finite or infinite series of states Q = {si}; (ii) a finite or infinite series of actions by the user A = {a j}; (iii) a state transitional function T (s, a, s), which describes the next state s \"and the action of the current state s\"; (iv) a reward function R (s, a), which specifies the reward given to the agent for choosing an action in the state s; and (v) a policy approach: S \u2192 A, which defines a mapping of states to actions. The goal of an RL is to choose a reward agent by maximizing its actions."}, {"heading": "4 Summary", "text": "We describe a publicly accessible dialog system motivated by the idea that future dialog systems should be trained without any intervention from system developers. In contrast to earlier dialog systems to enhance learning, SimpleDS selects dialog actions directly from the raw (loud) text of the last system and user reactions. It remains to be shown how far one can go with such an approach. Future work will include (a) comparing different model architectures, training parameters and reward functions; (b) expanding or improving the capabilities of the proposed dialog system; (c) training deep learning agents in other (larger) areas [7, 8, 9]; (d) evaluating end-to-end systems with real users; (e) comparing or combining different types of neural networks [10] and (e) performing rapid learning based on parallel calculations."}, {"heading": "Acknowledgments", "text": "The funding from the project \"STAC: Strategic Conversation\" No. 269427 of the European Research Council (ERC) is gratefully accepted."}], "references": [{"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Automating spoken dialogue management design using machine learning: An industry perspective", "author": ["T. Paek", "R. Pieraccini"], "venue": "Speech Communication 50(8-9)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Strategic dialogue management via deep reinforcement learning", "author": ["H. Cuay\u00e1huitl", "S. Keizer", "O. Lemon"], "venue": "NIPS Deep Reinforcement Learning Workshop.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Algorithms for Reinforcement Learning", "author": ["C. Szepesv\u00e1ri"], "venue": "Morgan and Claypool Pub.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "ConvNetJS: A javascript library for training deep learning models", "author": ["A. Karpathy"], "venue": "https://github.com/karpathy/convnetjs", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluation of a hierarchical reinforcement learning spoken dialogue system", "author": ["H. Cuay\u00e1huitl", "S. Renals", "O. Lemon", "H. Shimodaira"], "venue": "Computer Speech & Language 24(2)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Spatially-aware dialogue control using hierarchical reinforcement learning", "author": ["H. Cuay\u00e1huitl", "N. Dethlefs"], "venue": "TSLP 7(3)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonstrict hierarchical reinforcement learning for interactive systems and robots", "author": ["H. Cuay\u00e1huitl", "I. Kruijff-Korbayov\u00e1", "N. Dethlefs"], "venue": "TiiS 4(3)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A.W. Senior", "H. Sak"], "venue": "ICASSP.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In particular, the field of Deep Reinforcement Learning (DRL) targets feature learning and policy learning simultaneously\u2014which reduces the effort in feature engineering [1].", "startOffset": 170, "endOffset": 173}, {"referenceID": 1, "context": "This is relevant because the vast majority of previous RL-based dialogue systems make use of carefully engineered features to represent the dialogue state [2].", "startOffset": 155, "endOffset": 158}, {"referenceID": 2, "context": "Motivated by the advantages of DRL methods over traditional RL methods, in this paper we present an extended dialogue system, recently applied to strategic dialogue management[3], that makes use of raw noisy text\u2014without any engineered features to represent the dialogue state.", "startOffset": 175, "endOffset": 178}, {"referenceID": 3, "context": "A Reinforcement Learning (RL) agent learns its behaviour from interaction with an environment and the physical or virtual agents within it, where situations are mapped to actions by maximising a long-term reward signal [4].", "startOffset": 219, "endOffset": 222}, {"referenceID": 0, "context": "To induce the Q function above we use Deep Reinforcement Learning as in [1], which approximates Q\u2217 using a multilayer convolutional neural network.", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "the learning algorithm Deep Q-Learning with Experience Replay described in [1].", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "The SimpleDS learning agent is based on the ConvNetJS tool [5], which implements the algorithm \u2018Deep Q-", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "Learning with experience replay\u2019 proposed by [1].", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "The hidden layers use Rectified Linear Units to normalise their weights [6].", "startOffset": 72, "endOffset": 75}, {"referenceID": 6, "context": "Future work includes to (a) compare different model architectures, training parameters and reward functions; (b) extend or improve the abilities of the proposed dialogue system; (c) train deep learning agents in other (larger scale) domains [7, 8, 9]; (d) evaluate end-to-end systems with real users; (e) compare or combine different types of neural nets [10]; and (e) perform fast learning based on parallel computing.", "startOffset": 241, "endOffset": 250}, {"referenceID": 7, "context": "Future work includes to (a) compare different model architectures, training parameters and reward functions; (b) extend or improve the abilities of the proposed dialogue system; (c) train deep learning agents in other (larger scale) domains [7, 8, 9]; (d) evaluate end-to-end systems with real users; (e) compare or combine different types of neural nets [10]; and (e) perform fast learning based on parallel computing.", "startOffset": 241, "endOffset": 250}, {"referenceID": 8, "context": "Future work includes to (a) compare different model architectures, training parameters and reward functions; (b) extend or improve the abilities of the proposed dialogue system; (c) train deep learning agents in other (larger scale) domains [7, 8, 9]; (d) evaluate end-to-end systems with real users; (e) compare or combine different types of neural nets [10]; and (e) perform fast learning based on parallel computing.", "startOffset": 241, "endOffset": 250}, {"referenceID": 9, "context": "Future work includes to (a) compare different model architectures, training parameters and reward functions; (b) extend or improve the abilities of the proposed dialogue system; (c) train deep learning agents in other (larger scale) domains [7, 8, 9]; (d) evaluate end-to-end systems with real users; (e) compare or combine different types of neural nets [10]; and (e) perform fast learning based on parallel computing.", "startOffset": 355, "endOffset": 359}], "year": 2016, "abstractText": "This paper presents SimpleDS, a simple and publicly available dialogue system trained with deep reinforcement learning. In contrast to previous reinforcement learning dialogue systems, this system avoids manual feature engineering by performing action selection directly from raw text of the last system and (noisy) user responses. Our initial results, in the restaurant domain, report that it is indeed possible to induce reasonable behaviours with such an approach that aims for higher levels of automation in dialogue control for intelligent interactive agents.", "creator": "LaTeX with hyperref package"}}}