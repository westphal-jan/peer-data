{"id": "1601.04667", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2016", "title": "Proactive Message Passing on Memory Factor Networks", "abstract": "We introduce a new type of graphical model that we call a \"memory factor network\" (MFN). We show how to use MFNs to model the structure inherent in many types of data sets. We also introduce an associated message-passing style algorithm called \"proactive message passing\"' (PMP) that performs inference on MFNs. PMP comes with convergence guarantees and is efficient in comparison to competing algorithms such as variants of belief propagation. We specialize MFNs and PMP to a number of distinct types of data (discrete, continuous, labelled) and inference problems (interpolation, hypothesis testing), provide examples, and discuss approaches for efficient implementation.", "histories": [["v1", "Mon, 18 Jan 2016 19:38:51 GMT  (1414kb,D)", "http://arxiv.org/abs/1601.04667v1", "35 pages, 13 figures"]], "COMMENTS": "35 pages, 13 figures", "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["patrick eschenfeldt", "dan schmidt", "stark draper", "jonathan yedidia"], "accepted": false, "id": "1601.04667"}, "pdf": {"name": "1601.04667.pdf", "metadata": {"source": "CRF", "title": "Proactive Message Passing on Memory Factor Networks Proactive Message Passing on Memory Factor Networks", "authors": ["Patrick Eschenfeldt", "Dan Schmidt"], "emails": ["peschen@mit.edu", "dan.schmidt@disneyresearch.com", "stark.draper@utoronto.ca", "yedidia@disneyresearch.com"], "sections": [{"heading": null, "text": "Keywords: machine learning, optimization, pattern recognition models, vision and scene understanding, image restoration"}, {"heading": "1. Introduction and motivations", "text": "In this paper, we present \"Storage Factor Networks\" (MFNs) and the \"Proactive Message Transmission\" (PMP) algorithm. Our goal is to combine the ability of message transmission with algorithms that enable large-scale problem solving in the most efficient way, generalizing algorithms with the ability of machine learning. Factor graphics and message transmission (Kschischang et al., 2001; Loeliger et al., 2007; Koller and Friedman, 2009; Yedidia et al., 2005; Sudderth and Freeman, 2008) have proven to be an extremely effective combination when faced with an inference task involving a large number of local constraints. In applications such as error correction decoding (Richardson and Urbanke, 2008), these local constraints have a pronounced effect. As in January 2016, he is using analog devices Lyric, LabMA 022.160 Xiv:"}, {"heading": "2. Memory factor networks", "text": "In fact, it is true that it is a real person, but it is also true that it is a real person in which it is a real person who is able to identify himself. (...) In fact, it is true that he is able to put himself at the centre. (...) In fact, it is true that he is able to put himself at the centre. (...) In fact, it is true that he is able to put himself at the centre. (...) It is true that he is able to take himself into his own hands. \"(...)"}, {"heading": "3. Proactive message passing", "text": "In our efforts to minimize the PMP algorithm (1), we are now conducting the proactive message that delegates deliver to an idealized political convention, where the goal of the convention is to determine the party platform. A party platform consists of a set of positions on issues where each topic corresponds to a variable in analogy, and an attitude that represents a value for the variable. Each delegate corresponds to a factor, and each delegate deals with only a (generally small) subset of issues."}, {"heading": "3.1 PMP specification", "text": "It is about the question to what extent a change can occur, when the question is whether and how a change should occur. (...) It is about the question to what extent a change can occur. (...) It is about the question to what extent a change can occur. (...) It is about the question to what extent a change can occur. (...) It is about the question to what extent a change should occur. (...) It is about the question to what extent a change can occur. (...) It is about the question to what extent a change should occur. (...) It is about the question to what extent a change should occur. (...) It is about the question to what extent a change should occur. (...) It is about the question to what extent a change should occur. (...) It is about the question to what extent a change should occur. (...) It is about the question to what extent a change should occur. (...) It is about the question to what extent a change should occur. (...) It is about the question to what extent a change should occur."}, {"heading": "3.2 Message-passing formulation", "text": "The answer to this question is: \"What is the ground for the decision?\" (\"What is the ground?\") (\"What is the ground?\") (\"What is the ground?\") (\"What is the ground?\") (\"What is the ground?\") (\"What is the ground?\") (\"What is the ground?\") (\"What is the ground?\") (\"What is the ground?\") (\"What is the ground?\") (\"What is the ground?\" (\"What is the ground?\") (\"What is the ground?\") (\"What is the ground?\") (\"What is the ground?\") (\"What is the ground?\" (\"What is the ground?\") (\"What is the ground?\") (\"The ground (\" The ground) (\"The ground) (\" The ground) (\"The ground) (\" The ground) (\"The ground) (\" The ground) (\"The ground) (\" The ground) (The ground) (\"The ground) (The ground) (The ground) (The ground) (\" What is the ground (\"What is the ground?\" (\"What is the ground?\" (\"What is the ground?\") (\"What is the ground (\" What is the ground (\"What is the ground?\") (\"What is the ground) (\" What is the ground (\"What is the ground?\") (\"What is the ground (\" What is the ground) (\"What is the ground) (\" What is the ground (\"What is the ground?\" (\") (\" What is the ground) (What is the ground) (\"What is the ground) (What is the ground (\" The ground) (The ground) (\"(\" What is the ground) (What is the ground) (\"The ground) (\" The ground) (\"The ground) (The ground) (\" The ground) (The ground) (The ground) (The ground) (The ground (The ground) (The ground) (The ground) (\"What is the ground) (\" What is the ground (\"(\" What is the ground) (\"What is the ground) (\" The ground) (\"(\" What is the ground is the ground) (\"The ground) ("}, {"heading": "3.3 Parallel updates and simultaneous voting", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4. Types of variables and objective functions", "text": "In this section, we will consider several options for the Xi alphabet and the cost factors of variables xi, in particular, we will consider various cases of minimization x-i = arg min xi \u2211 b-B-i (xi, v b i) w b i (10) with a fixed set of voices vBi for some B Ni. Minimization of the above form is used in PMP both in calculating the final variable settings to return, and in calculating the opinion of a factor via (8), which is the version of (3) that can give a message-conveying interpretation. It is therefore of great importance that minimization in (10) is comprehensible. In fact, for our decisions on alphabets and cost functions, we receive explicit solutions that allow us to write (3) in a simple message delivery form, according to our discussion in Sec. 3.2. The messages come from all adjacent variable nodes and sum up the voices (and associated weights) of the memory factors that comprise each of these variables."}, {"heading": "4.1 Real variables with quadratic cost", "text": "First, we consider the case where xi-R and the local cost functions are square, i.e. vice versa (x, v) = (x-v) 2. For a square local cost function, we take the derivative with respect to xi and set the result to zero to find the minimization value of xi: x-i = x-v = x-v | 2. In Appendix A, we show that if we apply this result to update the opinion of the vectors for factor a, then (3) is a weighted combination of voices. Note that this result extends to the complex case xi-C with the value of xi (x, v) = x-v."}, {"heading": "4.2 Integer variables with linear cost", "text": "We look at the situation where xi-Z and mismatch cause costs: Z-Z is absolute difference, i.e., we look at the situation where xi-Z and mismatch cause costs: Z-Z is absolute difference, i.e., we look at the setting where weights are all identical. We will argue that the median will be a median of voices that minimizes the cost. In fact, the median is a set of values that are not greater than the lower end of the range and at least half of the voices have values that are as large as the upper end of the range. Generally, the median value is the set of integers for which at least half of the voices are not larger than the lower end of the range and have at least half of the values that are as large as the upper end of the range. So, x-z is Mmed."}, {"heading": "4.3 Labels with histograms", "text": "In this setting, we consider the local cost functions as indicators. (x, v) = I (x 6 = v). As with integers, we impose the restriction that weights associated with the same variable must be the same. In this setting, we take the local cost functions as indicators. (x, v) = I (x 6 = v). We minimize the cost (10) by selecting x-i as one of the most common tuning values in v B i. If we chose a different (less frequently observed) tuning value, the cost would increase with the reduction in the number of voices. Therefore, it is optimal to select x-i as an element of the tuning modes that we call Mmod (vBi)."}, {"heading": "4.4 Mixing variable types", "text": "If a factor of adjacent variables of different types (real, integer, label), the problem of opinion update (3) becomes a combination of the problem types described above. In this situation, the sum of i-Na can be divided into sums of different types of variables. For example, if Na = B-C with i-B, which corresponds to complex variables, and i-C, which corresponds to integer variables, we can reexpress (3) asoa = arg min o-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i."}, {"heading": "5. Types of Memory Factors", "text": "In this section, we describe two ways to construct memory factors - a simple approach using \"memory tables\" and a slightly more complicated, but perhaps more scalable approach using reduction to a low-dimensional subspace."}, {"heading": "5.1 Memory Table Factors", "text": "A memory table is simply a database of examples, each copy encoding a valid configuration of variables bordering on the factor in question. Storage factors, which are memory tables, are trivial to train. If we get a number of examples from which we learn the local structure, we simply store xa = {xi: i Na} for each copy. We consider each of these stored local snapshots to be \"memory.\" Formally, the memory table corresponding to factor a corresponds to a database of La (generally) different memories {\u00b5al: l [La]}. Each memory is a vector of length | Na |, whose jtest element is \u00b5al, j. Each element of this vector corresponds to a certain variable, which is neighbor factor a. If \u00b5al, j refers to the variable xk (k) deniv is the jtest element of Na), then \u00b5al, jtest element of Xk is the jtest element of which may be factor a. The memory table may correspond to the factor, which is neighbor."}, {"heading": "5.2 Subspace Factors", "text": "Another form of learned structure can be used for memory factors to refer to a lower space (or a subset thereof). (We refer to memory factors that force a dimensionality reduction as \"subspace factors.\") In particular, we look at linear subspaces that take the form of transformations of hidden variables z, z, z, z, Rp, etc. when the variables are complex, we use complex matrices and hidden variables. Moreover, we often limit ourselves to a subset of subspace, so that Z, R and Y, R, etc. We will give an example later where Z = R + and Wij _ R + restrict our subspace factor to the positive (cone)."}, {"heading": "6. Applications", "text": "In this section, we describe a number of applications that demonstrate the basic mechanisms and capabilities of memory factor networks, and compare the behavior of MFNs with that of subspace factors using memory tables."}, {"heading": "6.1 Face reconstruction", "text": "Our first application is to reconstruct missing or noisy data in a two-dimensional color image. We use the FEI (fei) dataset of 400 52 x 72 pixel images, each of which manually aligns a single face so that facial features are in the same pixel positions on each image. We use 320 of these images as a training set and have 80 available for testing. We will describe a memory factor network (with a memory table version and a subspace factor version) that represents such images and can be used for a variety of tasks with similar images. The set of variables we are interested in are the red, green and blue pixel values for each pixel position in an image, with some examples adding additional variables representing the gray value of each pixel. Therefore, variables are non-negative real numbers. They are also normalized to at most one, but we do not explicitly set this in the MFN, and in rare cases simply shorten it to a larger value."}, {"heading": "6.1.1 Factor layout", "text": "The factors we use for this problem cover square grids of pixels at a certain point in the image. A basic structure is to create three factors (representing the red, green and blue channels) for 8 x 8 squares of pixels in the image, with squares representing every 4 x 4 pixels, so that each factor divides a 4 x 4 pixel square with three adjacent factors of the same type. At corners and edges, we follow the same pattern, but shorten the factors as needed. This structure essentially creates three parallel factor networks representing the three color channels, since none of these factors associated with 8 x 8 pixels is adjacent to variables of multiple colors. To link the networks, we introduce factors that include variables of all three colors. So that these factors are not too large, we need to cover them 4 x 4 pixels squares (so that they have 3 x 16 x 48 visible variables)."}, {"heading": "6.1.2 Learning memories", "text": "In the case of memory tables, the implementation of these factors is simple: Each table contains the corresponding pixel values of each face in the training set. For subspace factors, we must learn the matrix W (after selecting a certain order for the variables so that they are represented as a vector). Since pixel values are not negative, we choose the nonnegative matrix factorization (NMF) for this learning step. To be precise, for a subspace factor with n variables, we choose a value p < n from a training set of m images, construct an n \u00d7 m matrix X in which the ith column contains the pixel values at the appropriate positions for the ith training image, and use a standard NMF algorithm to approximate factorization."}, {"heading": "6.1.3 Missing data", "text": "In this case, we are in a position to go in search of a solution."}, {"heading": "6.1.4 Noisy data", "text": "If \"known\" pixel values have been disturbed in any way, we will prefer the smoothed result obtained by the general application of our algorithm, with equal weights between factors and proofs (or even additional weight for factors)."}, {"heading": "6.1.5 Results summary", "text": "We performed PMP with the settings described for each of the problems described above on each of the 80 test images and calculated the mean square error of the color pixel values (over the portion of the image used in the MFN). Statistics summarizing the results are shown in Table 1, and images for the best and worst examples of each problem are available in Appendix B."}, {"heading": "6.2 Music reconstruction", "text": "Another application of MFNs is the processing of audio files. Again, the question of interest can be to reconstruct missing data or smooth out noisy data, and some design decisions will be different depending on the application, but first we describe the general process we use to transform an audio file into a format suitable for an MFN. For our data set, we use 9-second clips of randomly generated music that we downloaded from Otomata (Oto), with 142 training samples and 20 test samples. All clips have a sampling rate of 40k Hz. To limit RAM usage, a random subset of training samples was used in each run of PMP, with each sample independently recorded with a probability of 0.3."}, {"heading": "6.2.1 Creating a spectrogram", "text": "Our MFNs work on a spectrogram representation of the audio files, so we use the Short-Term Fourier Transformation (STFT) to process our audio samples. This process begins by splitting the audio into a series of overlapping frames of a given length. Overlapping can be varied by changing the \"hop size,\" which determines the distance between the start time of consecutive frames. We use an image length of 50ms and a hop size of 25ms. After splitting the frames, the data in each frame is multiplied by a window function: we use the Hanning window. Each frame won is then individually transformed into Fourier by transforming a matrix of complex values in which each line represents a frame and each line represents a signal frequency. Frequency range and resolution is determined by the sampling rate of the audio frequencies and the length of the frames."}, {"heading": "6.2.2 Factor layout", "text": "Since the perceived quality of a sound generally depends on all the frequencies present in that sound (possibly non-adjacent frequencies), we consider memory factors that cover the entire frequency spectrum over a small period of time. This allows the factor to learn the entire frequency profile of a particular sound, which often includes many distant frequencies. In our main test case of randomly generated music, we also find that the absolute time position has little significance, since we have no a primary expectation that, for example, the last two seconds of a sample should be different from the first two seconds. Instead of learning different memory factors for different parts of the spectrogram, we learn a single subspace matrix or set of memory from all time positions of all training examples, and share this among all the memory factors in the network. Note that this leads to a very large training set, even for a relatively small number of music samples.We note that this time dependence is a specific feature of our choice of the audio signal, and is not something that applies to any particular audio signal in a particular instance."}, {"heading": "6.2.3 Missing music", "text": "Analogous to the distance of the eyes from a face, we consider the problem of inference to fill gaps in a piece of music. In this situation, we will attach considerable weight to the evidence image, preferring to maintain the original signal where possible. It will also be generally advantageous to select relatively wide memory factors (10-20 spectrogram pixels) to maximize the connection between the newly generated signal and the original signal it matches. For example, spectrograms for a reconstruction problem see Fig. 5. For the construction of missing music, memory tables have significantly better properties than subspace factors in our tests. The memory table network selects complete notes or sequences of notes and tries to select those that best fit the surrounding music, while subspace factors generally correspond to the prevailing frequencies of the surrounding music, but do not replicate the shape of notes, but rather a kind of flat sums that they are forced to compose themselves frequently with a subspace factor that we believe is a reason for them to be placed on a subspace behavior."}, {"heading": "6.2.4 Noisy music", "text": "To simulate this problem, we add normal random noise to our test sample. Since many regions of the spectrogram have a value of zero in the absence of noise, when evidence is significantly weighted in relation to memory factors, there will still be some level of noise at the end of the algorithm. To combat this effect, we run PMP twice, first with a great deal of weight on the evidence and without voices of factors, and then again with very little weight for the evidence, but taking into account the final voices of the factors as first voices in the second run. This allows factors to exploit the evidence without dominating it. Since factors must primarily correspond to the signal under the noise, rather than coordinate across time intervals, we use very narrow factors (2 pixels) for noise problems. An example of a noisy music problem can be seen in Fig. 6. Again, we see that memory tables are better able to try to maintain the rich structure of the notes, even though they hold more memory factors than the music structure when they hold much more of the main tuning factors."}, {"heading": "6.2.5 Results summary", "text": "We performed PMP with the settings described for the two problems described above on each of the 20 test clips and calculated the mean square error of the spectrogram pixel values. Statistics summarizing the results are presented in Table 2, and images for the best and worst examples of each problem are available in Appendix B."}, {"heading": "6.3 Handwritten digit classification", "text": "In fact, most of them are able to decide for themselves what they want."}, {"heading": "6.4 Restoration of corrupted images", "text": "A natural application for MFNs that use memory tables is to reconstruct previously seen images when presented with versions of them that have been corrupted by noise or deletion. We use a similar network as the facial reconstruction application and for data weLevel 0 Level 1 Level 2 Level 3 use the CIFAR-10 dataset of 32 x 32 RGB images (Krizhevsky, 2009), which contains 50,000 images. The provided caption of the images is irrelevant to our problem. After all 50,000 images have been read, one of the previously seen images is randomly selected and applied to each color channel value of each pixel with a standard deviation of 40. In addition, a randomly generated blob of 144 pixels is completely deleted from the center of the image (which does not provide any evidence).This image is then presented to the MFN, which restores the image in its serial version, with the applied blob of 144 pixels being the most convenient for both the application and the region of 9000 pixels because this is also the choice of 9xels."}, {"heading": "7. Conclusions and future work", "text": "We have introduced a new approach here to combine conclusions with learning from experience. Storage factor networks offer an easy way to store examples from experience, while proactive message delivery through a trust-based scheme for prioritizing factor updates provides a reliable way to align with a good optimum of storage factor network cost functionality. We consider the algorithms and applications demonstrated here merely a first foray into the possibilities raised by this approach. Therefore, factor diagrams that combine storage factor nodes with more conventional factor nodes encoding known statistical dependencies or limitations could be considered. The PTP algorithm is well suited for MFNs, but one could still consider other approaches, such as those based on belief propagation or variational approaches (Wainwright and Jordan, 2008), or those based on the method of changing directions of multipliers (Boyplier), such as those available in 2011, including many deep video images."}, {"heading": "Appendix A: Derivation of opinion update for real variables with quadratic cost", "text": "In this section, we derive the messages in the message-sharing version of PMP for real variables and square costs. (3) Under these decisions, we specialize (3) in the definition of (2) + (2) + (2) + (2), (2), (2), (2), (2) and (2), in which we optimize the derivative with respect to xi (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (,, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (, (,), (, (,), (,), (, (, (,), (,), (, (,), (, (, (,), (,), (,), (, (,), (, (,), (, (,), (, (, (,), (, (, (, (,), (, (,), (, (,), (, (, (, (,), (, (, (, (,), (, (,), (, (,), (,), (, (, (, (, (,), (, (, (, (, (, (,), (,), (,), (,), (, (,), (, (, (, (, (, (,), (,), (, (, (, (, (, (,), ("}, {"heading": "Appendix B: Reconstruction results", "text": "In order for the reader to have a reasonable sense of the range of possible outcomes, including the nature of artifacts obtained through proactive messaging to storage factor networks, we insert here images of the best and worst solutions to a variety of problems and for each type of factor (memory table or subspace), first for reconstructing facial images and then for reconstructing music spectrograms."}], "references": [{"title": "Complex-valued autoencoders", "author": ["P. Baldi", "Z. Lu"], "venue": "Neural Networks,", "citeRegEx": "Baldi and Lu.,? \\Q2012\\E", "shortCiteRegEx": "Baldi and Lu.", "year": 2012}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "An improved three-weight messagepassing algorithm", "author": ["N. Derbinsky", "J. Bento", "V. Elser", "J.S. Yedidia"], "venue": null, "citeRegEx": "Derbinsky et al\\.,? \\Q1961\\E", "shortCiteRegEx": "Derbinsky et al\\.", "year": 1961}, {"title": "Codes on graphs: Normal realizations", "author": ["G.D. Forney"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Forney.,? \\Q2001\\E", "shortCiteRegEx": "Forney.", "year": 2001}, {"title": "Learning low-level vision", "author": ["W.T. Freeman", "E.C. Pasztor", "O.T. Carmichael"], "venue": "Int. J. Computer Vision,", "citeRegEx": "Freeman et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Freeman et al\\.", "year": 2000}, {"title": "Example-based super-resolution", "author": ["W.T. Freeman", "T.R. Jones", "E.C. Pasztor"], "venue": "IEEE Computer Graphics and Applications,", "citeRegEx": "Freeman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Freeman et al\\.", "year": 2002}, {"title": "The unreasonable effectiveness of data", "author": ["A. Halevy", "P. Norvig", "F. Pereira"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "Halevy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Halevy et al\\.", "year": 2009}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman.,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman.", "year": 2009}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Masters Thesis, University of Toronto,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "H. Loeliger"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Kschischang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kschischang et al\\.", "year": 2001}, {"title": "The factor graph approach to model-based signal processing", "author": ["H. Loeliger", "J. Dauwels", "J. Hu", "S. Korl", "L. Ping", "F.R. Kschischang"], "venue": "Proc. of the IEEE,", "citeRegEx": "Loeliger et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loeliger et al\\.", "year": 2007}, {"title": "Modern Coding Theory", "author": ["T.J. Richardson", "R. Urbanke"], "venue": null, "citeRegEx": "Richardson and Urbanke.,? \\Q2008\\E", "shortCiteRegEx": "Richardson and Urbanke.", "year": 2008}, {"title": "Signal and image processing with belief propagation", "author": ["E. Sudderth", "W.T. Freeman"], "venue": "IEEE Signal Process. Mag.,", "citeRegEx": "Sudderth and Freeman.,? \\Q2008\\E", "shortCiteRegEx": "Sudderth and Freeman.", "year": 2008}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan.,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan.", "year": 2008}, {"title": "Constructing free energy approximations and generalized belief propagation algorithms", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Yedidia et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 9, "context": "Factor graphs and message passing (Kschischang et al., 2001; Loeliger et al., 2007; Koller and Friedman, 2009; Yedidia et al., 2005; Sudderth and Freeman, 2008) have proved to be an extremely effective combination when one is faced with an inference task wherein global problem structure decomposes into a large set of local constraints.", "startOffset": 34, "endOffset": 160}, {"referenceID": 10, "context": "Factor graphs and message passing (Kschischang et al., 2001; Loeliger et al., 2007; Koller and Friedman, 2009; Yedidia et al., 2005; Sudderth and Freeman, 2008) have proved to be an extremely effective combination when one is faced with an inference task wherein global problem structure decomposes into a large set of local constraints.", "startOffset": 34, "endOffset": 160}, {"referenceID": 7, "context": "Factor graphs and message passing (Kschischang et al., 2001; Loeliger et al., 2007; Koller and Friedman, 2009; Yedidia et al., 2005; Sudderth and Freeman, 2008) have proved to be an extremely effective combination when one is faced with an inference task wherein global problem structure decomposes into a large set of local constraints.", "startOffset": 34, "endOffset": 160}, {"referenceID": 14, "context": "Factor graphs and message passing (Kschischang et al., 2001; Loeliger et al., 2007; Koller and Friedman, 2009; Yedidia et al., 2005; Sudderth and Freeman, 2008) have proved to be an extremely effective combination when one is faced with an inference task wherein global problem structure decomposes into a large set of local constraints.", "startOffset": 34, "endOffset": 160}, {"referenceID": 12, "context": "Factor graphs and message passing (Kschischang et al., 2001; Loeliger et al., 2007; Koller and Friedman, 2009; Yedidia et al., 2005; Sudderth and Freeman, 2008) have proved to be an extremely effective combination when one is faced with an inference task wherein global problem structure decomposes into a large set of local constraints.", "startOffset": 34, "endOffset": 160}, {"referenceID": 11, "context": "In applications such as error correction decoding (Richardson and Urbanke, 2008), these local constraints have a", "startOffset": 50, "endOffset": 80}, {"referenceID": 4, "context": "in their VISTA (\u201cVision by Image/Scene training\u201d) approach (Freeman et al., 2000).", "startOffset": 59, "endOffset": 81}, {"referenceID": 5, "context": "The main application considered using the VISTA approach was example-based super-resolution (Freeman et al., 2002).", "startOffset": 92, "endOffset": 114}, {"referenceID": 3, "context": "Perhaps other than the edge weights, the above description is standard; it quite neatly fits into the formalism of a \u201cnormal\u201d factor graph due to Forney (Forney, 2001).", "startOffset": 153, "endOffset": 167}, {"referenceID": 14, "context": "This feature of guaranteed convergence to at least a local optimum is not shared by most message-passing algorithms operating on factor graphs containing cycles (Yedidia et al., 2005).", "startOffset": 161, "endOffset": 183}, {"referenceID": 0, "context": "Because our variables are now complex valued, to learn subspace factors we follow the work of Baldi and Lu (Baldi and Lu, 2012) and use a PCA approach.", "startOffset": 107, "endOffset": 127}, {"referenceID": 8, "context": "use the CIFAR-10 dataset of 32 \u00d7 32 RGB images (Krizhevsky, 2009), containing 50,000 images.", "startOffset": 47, "endOffset": 65}, {"referenceID": 13, "context": "The PMP algorithm is well-suited to MFNs, but one might nevertheless consider using other approaches, such as those based on belief propagation or variational approaches (Wainwright and Jordan, 2008), or those based on the alternating directions method of multipliers (Boyd et al.", "startOffset": 170, "endOffset": 199}, {"referenceID": 1, "context": "The PMP algorithm is well-suited to MFNs, but one might nevertheless consider using other approaches, such as those based on belief propagation or variational approaches (Wainwright and Jordan, 2008), or those based on the alternating directions method of multipliers (Boyd et al., 2011; Derbinsky et al., 2013).", "startOffset": 268, "endOffset": 311}, {"referenceID": 6, "context": "Finally, an important open question is how well the approach described here can take advantage of massive amounts of data, as are often now available (Halevy et al., 2009).", "startOffset": 150, "endOffset": 171}], "year": 2016, "abstractText": "We introduce a new type of graphical model that we call a \u201cmemory factor network\u201d (MFN). We show how to use MFNs to model the structure inherent in many types of data sets. We also introduce an associated message-passing style algorithm called \u201cproactive message passing\u201d (PMP) that performs inference on MFNs. PMP comes with convergence guarantees and is efficient in comparison to competing algorithms such as variants of belief propagation. We specialize MFNs and PMP to a number of distinct types of data (discrete, continuous, labelled) and inference problems (interpolation, hypothesis testing), provide examples, and discuss approaches for efficient implementation.", "creator": "LaTeX with hyperref package"}}}