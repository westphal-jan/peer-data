{"id": "1601.04692", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2016", "title": "Spectral Theory of Unsigned and Signed Graphs. Applications to Graph Clustering: a Survey", "abstract": "This is a survey of the method of graph cuts and its applications to graph clustering of weighted unsigned and signed graphs. I provide a fairly thorough treatment of the method of normalized graph cuts, a deeply original method due to Shi and Malik, including complete proofs. The main thrust of this paper is the method of normalized cuts. I give a detailed account for K = 2 clusters, and also for K &gt; 2 clusters, based on the work of Yu and Shi. I also show how both graph drawing and normalized cut K-clustering can be easily generalized to handle signed graphs, which are weighted graphs in which the weight matrix W may have negative coefficients. Intuitively, negative coefficients indicate distance or dissimilarity. The solution is to replace the degree matrix by the matrix in which absolute values of the weights are used, and to replace the Laplacian by the Laplacian with the new degree matrix of absolute values. As far as I know, the generalization of K-way normalized clustering to signed graphs is new. Finally, I show how the method of ratio cuts, in which a cut is normalized by the size of the cluster rather than its volume, is just a special case of normalized cuts.", "histories": [["v1", "Mon, 18 Jan 2016 20:57:41 GMT  (1079kb,D)", "http://arxiv.org/abs/1601.04692v1", "122 pages. arXiv admin note: substantial text overlap witharXiv:1311.2492"]], "COMMENTS": "122 pages. arXiv admin note: substantial text overlap witharXiv:1311.2492", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["jean gallier"], "accepted": false, "id": "1601.04692"}, "pdf": {"name": "1601.04692.pdf", "metadata": {"source": "CRF", "title": "Spectral Theory of Unsigned and Signed Graphs Applications to Graph Clustering: a Survey", "authors": ["Jean Gallier"], "emails": ["jean@cis.upenn.edu"], "sections": [{"heading": null, "text": "In fact, most of them are able to abide by the rules they have imposed on themselves, and they are able to abide by the rules they have given themselves. (...) It is as if they were able to break the rules. (...) It is as if they were able to break the rules. (...) It is as if they were able to break the rules. (...) It is as if they were able to break the rules. (...) It is as if they are able to break the rules. (...) It is as if they are able to break the rules. (...) It is as if they are able to break the rules. (...) It is as if they are able to break the rules. (...)"}, {"heading": "1 Introduction 7", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Graphs and Graph Laplacians; Basic Facts 19", "text": "2.1 Directed graphs, undirected graphs, weighted graphs............ 19 2.2 Lapland graph matrices................... 27"}, {"heading": "3 Spectral Graph Drawing 37", "text": "3.1 Graph Drawing and Energy Minimization................. 37 3.2 Examples of Graph Drawings...................."}, {"heading": "4 Graph Clustering 45", "text": "4.1 Graph Clustering using Normalized Cuts............... 45 4.2 Special Case: 2-Way Clustering using Normalized Cuts........... 46 4.3 K-Way Clustering using Normalized Cuts..................... 56 4.4 K-Way Clustering; Using The Dependencies Among X1,..... XK....... 70 4.5 Discrete Solution Close to a Continuous Approximation......... 73"}, {"heading": "5 Signed Graphs 87", "text": "5.1 Signed charts and signed rags.................................. 87 5.2 Signed normalized sections................................. 90 5.3 Balanced charts.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "6 Graph Clustering Using Ratio Cuts 103", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Rayleigh Ratios and the Courant-Fischer Theorem 107", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Riemannian Metrics on Quotient Manifolds 113", "text": "Bibliography 11956 CONTENTS Chapter 1IntroductionIn the fall of 2012, my friend Kurt Reillag suggested that I should be ashamed to know so little about graph laplakers and normalized graph sections. These comments are the result of my efforts to correct this situation. I will begin by reviewing the basic concepts of graph theory. Although the graph laplaker is basically associated with an undirected graph, I will review the definition of both directed and undirected graphs. For directed and undirected graphs, I will define the degree matrix D, the incidence matrix B, and the adjacent matrix A. Then I will define a weighted graph. This is a pair (V, W), where V is a finite set of nodes, and W is an m symmetric matrix with non-negative entries and zero diagonal entries (with notation)."}, {"heading": "8 CHAPTER 1. INTRODUCTION", "text": "The quantitative linkages (A, A) = links (A, A) (where A = V \u2212 A denotes the addition of A to V = > incidence) measures how many linkages escape from A (and A). We define the intersection of A ascut (A) = links (A, A).The terms of the intersection are shown in Figure 1.2. The above concepts play a crucial role in the theory of normalized sections. Then, I introduce the (unnormalized) graph L of a directed graph G in an \"old way,\" showing that for each orientation of a graph G, BB > = D \u2212 Lis an invariant. I also define the (unnormalized) graph L of a weighted graph L as L = (V, W) as L \u2212 W. I show that the notion of incidence matrix can be generalized."}, {"heading": "10 CHAPTER 1. INTRODUCTION", "text": "In this case, we can look at the more general situation in which people are not necessarily identical. This can be done by a symmetrical relationship between the different types of people who are able to identify themselves, and between the different types of people who are able to identify themselves. It can be done by a symmetrical relationship between the different types of people who are able to identify themselves, and between the different types of people who are able to identify themselves. In this case, we can look at the more general situation in which people are not necessarily identical. It can be done by a symmetrical relationship between the different types of people who are able to identify themselves, and who are able to differ."}, {"heading": "12 CHAPTER 1. INTRODUCTION", "text": "The first step is to express our optimization problem in matrix form. In the case of two clusters, a single vector Xx = > PNX can be used to describe the partition (A1, A2) = (A, A). We need to choose the structure of this vector so that it is important to be explicitly pointed out. First, I have realized that it is important to select a vector representation that is invariant among multiplication."}, {"heading": "14 CHAPTER 1. INTRODUCTION", "text": "We describe a selection (A1,., AK) of the group of nodes V = > PNj = j = j PN = = j PN = = j PN = = j PN = = j PN = = = j PN = = = = j PN = = > K = > K = > K = > K = 1 = 1 = 2, we assume that the vector Xj is the two different real numbers. The vector Xj is an indicator vector for Aj in the sense that we are for i = 1,., N and i = 1,., and where we are both different numbers. The choice {aj, bj} for j is an indicator vector for Aj in the sense that we are for i = 1,., N, xji = {aj, if we choose {aj, bj} for j = 1,."}, {"heading": "16 CHAPTER 1. INTRODUCTION", "text": "As applies in the case of unsigned, weighted graphs: For each orientation G\u03c3 of the underlying graphics of the AK group (V, W), there is a weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted, weighted,"}, {"heading": "18 CHAPTER 1. INTRODUCTION", "text": "Consequently, all that needs to be done to replace the normalized laplac sym with the non-normalized laplacian L (and omit the step of considering the problem (and 1). In the case of signed diagrams, we define the signed ratio section sRcut (A1,.., AK) of the partition (A1,.., AK) assRcut (A1,.., AK) = K \u00b2 j = 1 section (Aj, Aj) | Aj | + 2 \u00b2 K \u00b2 j = 1 link \u2212 (Aj,., Aj) | Aj |.Since we still have (Xj) > LXj = a2j (cut (Aj, Aj) + 2 links \u2212 (Aj, Aj) we know how positive lapendices Rcut is (A1,., Appendix J = 1 (Xj) > prographs = a2j (Aj, Aj) + 2 links \u2212 (Aj, Aj), Aj \u2212 Aj)."}, {"heading": "2.1 Directed Graphs, Undirected Graphs, Incidence", "text": "Matrices, Adjacency Matrices, Weighted GraphsDefinition 2.1. A directed graph is a pair of G = (V, E), where V = {v1,..., vm} is a set of nodes or vertices, and E V \u00b7 V is a set of ordered pairs of different nodes (i.e. pairs (u, v) \u0445 V \u00b7 V with u 6 = v), called edges. In the face of an arbitrary edge e = (u, v) we leave s (e) = u the source of e and t (e) = v the target of e.Note: Since an edge is a pair (u, v) with u 6 = v, self-loops are not allowed. Also, there is at most one edge from a node u to a node v. Such graphs are sometimes referred to as simple graphs. An example of a directed graph is shown in Figure 2.1.119."}, {"heading": "20 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "For each node of the world the degree d (v) of v (v) v (v) v (v) v (v) v (r) v (v) v (v) v (n) v (n) v (n) v (n).D (G) n (G) n (G) n (G) n (G) n (G) n (G) n (G) n (G) n (G) n n (G) n (G) n (G) n (G) n (G) n (G) n (G) n) n (G) n (G) n (G) n n (G) n n (G) n n (G) n (G) n (G) n (G) n (G) n (G) n (G) n (G) n (G) n (G) n (G) n (G n) n (G n) n (G n) n (G n) n (G n) n (G n) n (G n) n (G n) n (G) n (G) n (G) n (G) n (G) n (G) n (G) n (G) n (G) n) n (G n) n (G n) n (G n) n (G n) n (G n) n (G n) n (G) n (G n) n (G n) n (G) n) n (G n) n (G n) n (G n) n (G n) n (G n) n (G n) n (G n) n (n) n) n (G n) n (G n) n (G n (n) n (n) n) n (G n (n) n (G n) n (n) n (n) n (G n) n (n) n (G n) n (G n) n (n) n n (G n) n (G n (n) n (G n) n (G n) n (G n) n (G n) n (G n (G n) n) n (G n (G n) n) n (G n (G n) n (G n (G n) n (G n) n) n ("}, {"heading": "22 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "Definition 2.5. Specifies an (undirected) graph G = (V, E) so that v0 = u, vk = v, and {vi, vi + 1} an edge in E for all i with 0 \u2264 i \u2264 k \u2212 1. The integer k is the length of the path. A path is closed if u = v. The graph G is connected if for two different nodes u, v \u0445 V, there is a path from u to v.Remark: The terminology walk or chain is often used instead of path, with the word path reserved for the case where the nodes vi are all different except that v0 = vk when the path is closed. The binary relationship on V \u00b7 V is defined so that u and v are related as vridence."}, {"heading": "2.1. DIRECTED GRAPHS, UNDIRECTED GRAPHS, WEIGHTED GRAPHS 23", "text": "If G = (V, E) is a directed or undirected graph, and we often use the notation matrix A of G, we can observe that the binary relationship between G and G is symmetrical if G is an undirected graph, but in general it is not symmetrical if G is a directed graph. If G = (V, E) is an undirected graph, then the adjacency matrix A of G is symmetrical if G is an undirected graph, but in general it is not symmetrical if G is a directed graph. If G = (V, E) is an undirected graph, then the adjacency matrix of G is to be considered as a linear map from RV to RV."}, {"heading": "24 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "Proof. (According to Godsil and Royle [10], Section 8.3 = > Column G = > Column G = > Column G = > Column C. Since B > is a n \u00b7 m matrix, we have havem = dim (Ker (B >)) + rank (B >), i.e. rank (B >) = m \u2212 c. Since B and B > have the same rank, rank (B) = m \u2212 c, as claimed. A vector z \u2212 Rm belongs to the core of B > iff B > iff z = 0 iff z > B = 0. With respect to the definition of B, for each edge {vi, vj} of G corresponding to the oriented edge ({vi, vj}) has zero entries except for a + 1 and a \u2212 1 in position i or vice versa."}, {"heading": "2.1. DIRECTED GRAPHS, UNDIRECTED GRAPHS, WEIGHTED GRAPHS 25", "text": "Note: Since wi i = 0, these graphs have no self-loops. We can imagine the matrix W as a generalized adjacence matrix. In the case where wi i = 0, 1} represents a graph as defined in 2.4.We can imagine the weight wi j of an edge {vi, vj} as a degree of similarity (or affinity) in an image or as a cost in a network. An example of a weighted graph is shown in Figure 2.3. The thickness of an edge corresponds to the size of its weights.Encrypt pairs of relationships as a weighted graph For each node vi \u0109V, the degree d (vi) of vi is the sum of the weights of the edges adjazent to vi: d (vi) = m \u0445j = 1 wi j.Note that in the above sum only nodes vj are such that there is an edge."}, {"heading": "26 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "In view of any subset of nodes A V, we define the volume vol (A) of A as the sum of the weights of all edges adjacent to nodes in A: vol (A) = \u2211 vi-A d (vi) = \u2211 vi-A m-J = 1 wi j.Note: Yu and Shi [24] use the notation degree (A) instead of vol (A). The terms degree and volume are in Figure 2.4.Note that vol (A) = 0 if A consists of isolated vertices, i.e. if wi j = 0 for all vi-A. Therefore, it is best to assume that G does not have isolated verticals. In view of two subsets A, B V (not necessarily different), we define links (A, B) by links (A, B) = links (A, B) = links (A, A) = many links (A) and the links (A) (A) (A), which are many (A), A = many links (A), A (many), A = many (A), many (A), A = many (A) nodes."}, {"heading": "2.2. LAPLACIAN MATRICES OF GRAPHS 27", "text": "The term \"cut\" is often referred to as an association of A. Sure, cut (A) + assoc (A) = vol (A). The term \"cut\" is illustrated in Figure 2.5. We now define the most important concept of these notes: the laplac matrix of a graph. In fact, as we shall see, it exists in several flavors."}, {"heading": "2.2 Laplacian Matrices of Graphs", "text": "Let's start with directed graphs, although, as we will see, graphs Laplaker are fundamentally associated with undirected graphs; the key sentence below shows how BB > refers to the adjacence matrix A. We reproduce the proof in Gaul [7] (see also Godsil and Royle [10]). Sentence 2.2. For any directed graph G, if B is the incidence matrix of G, A is the adjacence matrix of G, and D is the degree matrix such that Di i = d (vi), thenBB > = D \u2212 A. Consequently, BB > is symmetrical and positively semidefined regardless of the orientation of G and D \u2212 A; that is, the eigenvalues of D \u2212 A are real and not negative. Proof: The entry BB > i j is the inner product of the ith series bi and the jth series bj of B. If i = j, then asbi k = + 1, if s (ek) vi = otherwise (t = vi)."}, {"heading": "28 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "We see that bi \u00b7 bi = d (vi). If i 6 = j, then bi \u00b7 bj 6 = 0 \u2212 1 \u2212 1 \u2212 1 \u2212 1 = 1 \u2212 1 = 1 \u2212 1 = 1 \u2212 1 \u2212 1 \u2212 2 = 1 \u2212 2 = 1 \u2212 2 = 1 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 3 \u2212 3 \u2212 3 \u2212 3 \u2212 4 \u2212 4 \u2212 4 \u2212 4 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 5 \u2212 5 5 5 \u2212 5 5 5 \u2212 5 5 5 \u2212 5 5 \u2212 5 5 \u2212 5 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5 \u2212 5"}, {"heading": "2.2. LAPLACIAN MATRICES OF GRAPHS 29", "text": "For all x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "30 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "Consequently, B\u03c3 (B\u03c3) > is independent of the orientation of the underlying diagram of G and"}, {"heading": "L = D \u2212W is symmetric and positive semidefinite; that is, the eigenvalues of L = D \u2212W are real and nonnegative.", "text": "Note: In view of any orientation of the underlying graph of a weighted graph G = (V, W) (if B is the incidence matrix of G, then B > defines a kind of discrete covariant derivative: RV \u00b7 X (G) \u00b7 RV on the set of 0 forms, which is only the set of functions RV. (vh, vi) | whi 6 = 0}, as a kind of discrete tangent space on vi, we see the set of edges with source or endpoint vi, TviG = (vi, vj) | wide (vh, vi) | whi 6 = 0}, as a kind of discrete tangent space on vi. The disjunction union of the tangent spaces TviG is the discrete tangent bundle TG. A discrete vector field is then a function X: V \u2192 TG, which is assigned to each vertex f."}, {"heading": "2.2. LAPLACIAN MATRICES OF GRAPHS 31", "text": "Space RX (G) \u00b7 V (and the inner product on RV) to define the adjugated eigenvalue: RX (G) \u00b7 V \u2192 RV (G) \u00b7 V by < F, f > = < F, f >, f >, for all f eigenvalues and all F peculiarities: RX (G) \u00b7 V. Then the Laplacian matrix: RV \u2192 RV is actually equal to L. Another way to prove that L is the degree matrix associated with W is to evaluate the square shape x > Lx.Proposition 2.4. For each m x \u00b7 m symmetrical matrix W = (wij), if we have L = D \u2212 W is the degree matrix associated with W, then havex > Lx = 12 m = positive properties i, j = 1 wi j (xi \u2212 xj) 2 for all x \u00b7 Rm.Consequently, x \u2212 Lx does not depend on the diagrams \u2212."}, {"heading": "32 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "The number c of the contiguous components K1,.., Kc of the underlying diagram of G is equal to the dimension of the zero space of L, which is equal to the multiplicity of the eigenvalue 0. Furthermore, the zero space of L contains a base consisting of indicator vectors of the contiguous components of G, i.e. vectors (f1,.., fm), so that fj = 1 iff vj Ki and fj = 0 otherwise.Proof. Since L = BB > for the incidence matrix B is connected to each oriented diagram of G, and since L and B > have the same zero space, according to Proposition 2.1, the dimension of the zero space of L is equal to the number c of the contiguous components of G and the indicator vectors of G."}, {"heading": "2.2. LAPLACIAN MATRICES OF GRAPHS 33", "text": "Since the unnormalized Laplacian L can be written as L = BB >, where B is the incidence matrix of any oriented graph derived from the underlying graph of G = (V, W), if we use letBsym = D \u2212 1 / 2B, we get Lsym = BsymB > eigeneigendiagram. In particular, for each individual eigendiagram Bsym = U\u0445 V > of Bsym (with U on m \u00d7 m orthogonal matrix, \u03a3 a \"diagonal\" m \u00b7 n matrix of singular values and V on n eigendiagram Bsym Lsym > of Bsym value are the squares of the highest m singular values of Bsym, and the vectors in U are normal eigenvectors of Lsym in relation to these eigenvalues and V on n eigensyrix."}, {"heading": "34 CHAPTER 2. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS", "text": "(8) If the underlying graph of G is not a complete graph, then \u03bd2 \u2264 1. Furthermore, the underlying graph of G is a complete graph iff \u03bd2 = m m m \u2212 1. (9) If m \u2265 2 and the underlying graph of G are connected, then \u03bd2 > 0. (10) If m \u2265 2 and the underlying graph of G have no isolated vertices, then \u03bdm \u2265 mm \u2212 1. Proof. (1) We have Lsym = D \u2212 1 / 2LD \u2212 1 / 2 LD \u2212 2 / 2 eigenspectrum is a symmetric inversible matrix (since it is an invertable diagonal matrix). It is a well-known fact of linear algebra that if B is an invertable matrix, then a semidefinitive S symmetric, positive semidefinitive symmetric, positive semidefinitive symmetric symmetric formula is iff."}, {"heading": "2.2. LAPLACIAN MATRICES OF GRAPHS 35", "text": "(5) Since D \u2212 1 is invertible, we have Lu = 0 iff D \u2212 1Lu = Lrwu = 0. Similarly, since D \u2212 1 / 2 is invertible, we have Lu = 0 iff D \u2212 1 / 2LD \u2212 1 / 2D1 / 2u = 0 iff D1 / 2u Ker (Lsym). (6) Since L1 = 0, we get Lrw1 = D \u2212 1L1 = 0. That D1 / 21 is in the zero space of Lsym follows from (2). Properties (7) - (10) are proven in Chung [4] (Chapter 1). A version of Proposition 2.5 also applies to the diagram Lapulllacians Lsym and Lrw. This is easily derived from the fact that Proposition 2.1 is applicable to the underlying diagram of a weighted diagram."}, {"heading": "3.1 Graph Drawing and Energy Minimization", "text": "G = (V, E) is an undirected graph. It is often desirable to draw a graph, usually in the plane, but possibly in 3D, and it turns out that the graph Laplacian can be used to design surprisingly good methods. Say: V | = m. The idea is to assign a point \u03c1 (vi) in Rn to the vertex vi-V, and for each vi-V point to draw a line segment between the points \u03c1 (vi) and \u03c1 (vj) if there is an edge {vi, vj}. Thus, a graph drawing is a function \u03c1: V \u2192 R. We define the matrix of a chart either as m-n matrix R, whose ith row consists of the row vector \u03c1 (vi), which corresponds to the point that vi represents in Rn. Typically, we want n < m; in fact, n should be much smaller than m. We can assume that each representation is a zero, that each column is calibrated > 1."}, {"heading": "38 CHAPTER 3. SPECTRAL GRAPH DRAWING", "text": "The following proposition is shown in God's favor = > R = > R = > R = > W = > W = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "3.1. GRAPH DRAWING AND ENERGY MINIMIZATION 39", "text": "If R is the matrix of a diagram in Rn, then for any invertible matrix M, the map that Ray (vi) assigns to M is another graph of G, and these two drawings convey the same amount of information. From this point of view, a graph drawing becomes an orthogonal drawing through the column space of R. Therefore, it is reasonable to assume that the columns of R are orthogonal in pairs and that they have the unit length. Such a matrix fulfills the equation R > R = I, and the corresponding drawing is called an orthogonal drawing. This condition also excludes trivial drawings. The following result tells us how to find minimal energy orthogonal graphic drawings, provided that Equation 3.2 is met. Let G = (V, W) be a weighted graph with the same energy. If L = D \u2212 W is the (unstandardized) Laplacian of L. and if the eigenvalues of G are the eigenvalues of L."}, {"heading": "40 CHAPTER 3. SPECTRAL GRAPH DRAWING", "text": "1. Calculate the two smallest eigenvalues \u03bb2 \u2264 \u03bb3 of the graph Laplacian L (it is possible that \u03bb3 = \u03bb2, if \u03bb2 is a multiple eigenvalue); 2. Calculate two units eigenvectors u2, u3 associated with \u03bb2 and \u03bb3, and leave R = [u2 u3] the matrix m \u00b7 2 with u2 and u3 as column.3. Place the vertex vi at the point whose coordinates are the ith line of R, i.e. (Ri1, Ri2).This method generally yields pleasing results, but be careful that there is no guarantee that different images will be assigned to different nodes, since R may have identical rows."}, {"heading": "3.2 Examples of Graph Drawings", "text": "We now give a series of examples with Matlab, some of which have been borrowed or adapted by Spielman. Example 1. Consider the graph with four nodes whose adjacence matrix is A = 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0. We use the following program to calculate u2 and u3:"}, {"heading": "A = [0 1 1 0; 1 0 0 1; 1 0 0 1; 0 1 1 0];", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D = diag(sum(A));", "text": "L = D - A; [v, e] = eigs (L); gplot (A, v (:, [3 2])) hold; gplot (A, v (:, [3 2]), 'o') The graph of Example 1 is shown in Figure 3.1. The function eigs (L) calculates the six largest eigenvalues of L in descending order and the corresponding eigenvectors. It turns out that \u03bb2 = \u03bb3 = 2 is a double eigenvalue. Example 2. Consider graph G2, which is shown in Figure 2.2 by the Adjacence MatrixA = 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0. We use the following program to calculate u2 and u3:"}, {"heading": "3.2. EXAMPLES OF GRAPH DRAWINGS 41", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A = [0 1 1 0 0; 1 0 1 1 1; 1 1 0 1 0; 0 1 1 0 1; 0 1 0 1 0];", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D = diag(sum(A));", "text": "L = D - A; [v, e] = eig (L); gplot (A, v (:, [2 3]) hold on to gplot (A, v (:, [2 3]), \"o ') The function eig (L) (without s at the end) calculates the eigenvalues of L in increasing order. The result of the drawing of the graph is shown in Figure 3.2. Note that the node v2 is associated with the point (0, 0), so the difference between this drawing and the drawing in Figure 2.2 is that the drawing in Figure 3.2 is not convex."}, {"heading": "42 CHAPTER 3. SPECTRAL GRAPH DRAWING", "text": "Example 3. Consider the ring graph defined by the adjacency matrix A in the Matlab program:"}, {"heading": "A = diag(ones(1, 11),1);", "text": "A = A + A '; A (1, 12) = 1; A (12, 1) = 1;"}, {"heading": "D = diag(sum(A));", "text": "L = D - A; [v, e] = eig (L); gplot (A, v (:, [2 3]))) stop at gplot (A, v (:, [2 3]), \"o ')) Note that we get a very nice ring; see Figure 3.3. Again, \u03bb2 = 0.2679 is a double eigenvalue (and the same applies to the next eigenvalue pairs, except the last one, \u03bb12 = 4). Example 4. In this example, supplemented by Spielman, we create 20 randomly selected points in the unit square, calculate their Delaunay triangulation, then the adjustment matrix of the corresponding graph, and finally draw the graph using the second and third eigenvalues of the laptop player."}, {"heading": "A = zeros(20,20);", "text": "xy = rand (20, 2); trigs = delay (xy (:, 1), xy (:, 2)); elemtrig = ones (3) - eye (3); for i = 1: length (trigs), A (trigs (i,:), trigs (i,:))) = elementary; end"}, {"heading": "3.2. EXAMPLES OF GRAPH DRAWINGS 43", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A = double(A >0); gplot(A,xy)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D = diag(sum(A));", "text": "L = D - A; [v, e] = eigs (L, 3,'sm'); Figure (2) gplot (A, v (:, [2 1]))) hold on gplot (A, v (:, [2 1]), 'o') The Delaunay triangulation of the set of 20 points and the drawing of the corresponding diagram are shown in Figure 3.4. The diagram on the right looks more beautiful than the diagram on the left, but is no longer planar. Example 5. Our last example, also borrowed from Spielman [21], corresponds to the skeleton of the \"Buckyball,\" a geodesic dome invented by the architect Richard Buckminster Fuller (1895-1983)."}, {"heading": "A = full(bucky);", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D = diag(sum(A));", "text": "L = D - A; [v, e] = eig (L); gplot (A, v (:, [2 3]); gplot (A, v (:, [2 3]), \"o\") Figure 3.5 shows a graph of the buckyball. This image appears somewhat squashed for two reasons: firstly, it is actually a three-dimensional graph; secondly, \u03bb2 = 0.2434 is a triple representation."}, {"heading": "44 CHAPTER 3. SPECTRAL GRAPH DRAWING", "text": "eigenvalue. (In fact, the Laplaker of L has many multiple eigenvalues.) What we really should do is draw this graph in R3 using three orthonormal eigenvectors associated with \u03bb2. A 3D image of the graph of the buckyball is created by the following Matlab program, and its image is shown in Figure 3.6. It looks better! [x, y] = gplot (A, v (:, [2 3])); [x, z] = gplot (A, v (:, [2 4]); plot3 (x, y, z)"}, {"heading": "4.1 Graph Clustering Using Normalized Cuts", "text": "If the data is given in the form of a similarity diagram G, where the weight wi j between two nodes vi and vj is a measure of the similarity of vi and vj, the problem can be specified as follows: Find a partition (A1,.., AK) of the set of nodes V into different groups, so that the edges between different groups have a very low weight (indicating that the points in different clusters are unequal), and the edges within a group have a high weight (indicating that the points within the same cluster are similar).The above diagram Cluster problem can be formalized as an optimization problem by using the term of the section called at the end of Section 2.1. If a subset A of the set of nodes V is specified, remember that we define the intersection (A) = Links (A) = Links (A, A) = Efficiency."}, {"heading": "46 CHAPTER 4. GRAPH CLUSTERING", "text": "An example of a weighted graph and a division of its nodes into two clusters is shown in Figure 4.1.15Encode Pairwise Relationships as a Weighted Graph16Cut the graph into two piecesesFigure 4.1: A weighted graph and its division into two clusters. One way to get around this problem is to normalize the sections by any measure of each subset of Ai. One way is to use the size (number of elements) of Ai. Another is to use the volume vol (Ai) of Ai. A solution using the second measure (for K = 2) was proposed and examined in a policy paper by Shi and Malik."}, {"heading": "48 CHAPTER 4. GRAPH CLUSTERING", "text": "The above is equivalent to (a \u2212 b) 2\u03b1 (d \u2212 \u03b1) = \u03b1da2 = = b = > b = = b = > b = (d \u2212 \u03b1) db2 (which can be rewritten) (\u03b1d (d \u2212 \u03b1))) + b2 (d2 \u2212 \u2212 \u2212 b2) ab = 0, that is, a2\u03b12 + b2 (d \u2212 \u03b1) ab = 2 + 2\u03b1 (d \u2212 \u03b1) ab = 2\u03b1 (d \u2212 \u03b1) 2 + 2\u03b1 (d \u2212 \u03b1) ab = 2\u03b1 (d \u2212 \u03b1) 2 + 2\u03b1 (d \u2212 \u03b1) ab = 0, that is, a2\u03b12 (d \u2212 \u03b1) 2 + 2\u03b1 (d \u2212 \u03b1) ab = 0. (\u2020) Note that the condition (\u2020) is applied to a vector X whose components are a = a = = = = = b =."}, {"heading": "52 CHAPTER 4. GRAPH CLUSTERING", "text": "rf\u00fc ide rf\u00fc the green for the green for the green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green-green for the green for the green-green for the green-green for the green-green for the green for the green-green for the green-green for the green for the green-green for the green for the green-green for the green-green for the green-green for the green-green for the green for the green-green for the green-green for the green for the green-green for the green-green for the green-green for the green for the green-green for the green-green for the green for the green-green for the green for the green for the green for the green-green for the green for the green for the green-green for the green for the green for the green for the green-green for the green for the green for the green for the green for the green for the green for the green for the green-green for the green for the green for the green-green for the green for the green for the green for the green-green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green-green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for"}, {"heading": "54 CHAPTER 4. GRAPH CLUSTERING", "text": "Since a complete graph has many symmetries, it is not surprising that there are many different solutions. (In fact, the study of eigenvectors of U3 shows very unbalanced solutions.) For graphs where the number of edges is very large and the number of edges is O (N2), calculating the SVD of the incidence matrix B is not practical. Instead, we calculate an SVD for Lsym that seems to be more stable that the diagonalization of Lsym. Our naive algorithm treats zero as a positive entry. Now, we use the fact that thatb = \u2212 a d \u2212 \u03b1 is a better solution to search for a vector X."}, {"heading": "56 CHAPTER 4. GRAPH CLUSTERING", "text": "(2) It's the only solution we have. (3) The final answer when we implement these algorithms, and there seems to be a divine solution. (3) It's the last answer when we implement these algorithms, and there seems to be a divine solution. (3) It's the only solution we have. (4) It's the only solution we have. (4) It's the only solution we have. (4) It's the only solution we have. (5) The last answer when we implement these algorithms, and there seems to be a divine solution."}, {"heading": "58 CHAPTER 4. GRAPH CLUSTERING", "text": "Da (Xj) > LXj (Xj) = > DXj = (aj \u2212 bj) = = > 2 intersection (Aj, Aj) \u03b1ja2j + (d \u2212 \u03b1j) b2jand vol (Aj) = \u03b1j, then (Aj, Aj) vol (Aj) = (Xj) > LXj (Xj) > DXj = 1,.., K, we must have (a2j \u2212 2ajbj) one (aj \u2212 bj) 2\u03b1ja2j + (d \u2212 \u03b1j) b2j = 1\u03b1j (Aj) = 1,., K.So we must have (a2j \u2212 2ajbj + b2j) a character other than 0."}, {"heading": "60 CHAPTER 4. GRAPH CLUSTERING", "text": "It is not immediately obvious how to form this condition in the matrix. Let DIAG be the function that returns the diagonal matrix (with the diagonal entries of the matrix XX) (with the diagonal entries of A), DIAG (A) = diag (a1,. ann), for each square matrix A = (ai j). Then the condition for the rows of X is zero."}, {"heading": "62 CHAPTER 4. GRAPH CLUSTERING", "text": "As in the case of K = 2, to be rigorous, the solutions are really K-tuples of dots in RPN \u2212 1, so our solution set is reallyP (K) = {(P (X1) \u2212 \u2212 \u2212 j = > eigenK = > eigenK = > strange value. Considering this, we have our first formula of K-way clustering a graph using normalized sections, called problem PNC1 (the notation PNCX will be subject to Yu [23], Section 2.1): K-way clustering a graph using Xp, version 1: Problem PNC1minimize K = 1 (Xj) > LXj (the notation PNCX) > DXj (X i) > DXj. > DXj = 0, 1 \u2264 i."}, {"heading": "64 CHAPTER 4. GRAPH CLUSTERING", "text": "Since the Rayleigh ratios (Xj) (Xj) > DXj) \u2212 1 / 2Xj) \u2212 1 / 2Xj (((X1) \u2212 X / 2Xj) > DXj) \u2212 1 / 2Xj (X1) \u2212 1 / 2Xj (X1) \u2212 X (X1) > X \u2212 X (X1) > X \u2212 X (X1) \u2212 X \u2212 X (X1) \u2212 X \u2212 X (X1) \u2212 X \u2212 X (X1) > X \u2212 X \u2212 X (X1) \u2212 X \u2212 X (X1) \u2212 X \u2212 X \u2212 X (X1) > X \u2212 X (X1) \u2212 X (X1) \u2212 X (X1) > X \u2212 X (X1) > X \u2212 X (X1 / 2X1)."}, {"heading": "68 CHAPTER 4. GRAPH CLUSTERING", "text": "For some invertible matrix on St (k, n), the manifold St (k, n) is the manifold St (k, n) consists of the set of orthogonal K-frames in Rn, that is, the k-tuples of orthonormal vectors (u1,. uk) with ui-Rn. For the manifold St (n, n), the orthogonal group O (n) is identical to the orthogonal group O (n). For us, the orthogonal group O (n), the group SO (n) acts transitively on St (n), and St (k, n) is identical to the orthogonal group O (n)."}, {"heading": "70 CHAPTER 4. GRAPH CLUSTERING", "text": "This means that we are all orthogonal to D1, and that each Zj has both positive and negative coordinates, for j = 2., K.The conditions (Zi) > DZj = 0., ZK are all orthogonal to D1, and thus that each Zj has both positive and negative coordinates, for j = 2., K.The conditions (Zi) > DZj = 0., ZK are all orthogonal to D1, and so that each Zj has both positive and negative coordinates, for j = 2., K.The conditions (Zi) > DZj are explicit."}, {"heading": "72 CHAPTER 4. GRAPH CLUSTERING", "text": "Again, we can assume that the non-zero entries in X1,.., XK are a positive real c-R position, so we have \u03b11 + \u00b7 \u00b7 \u00b7 + XK = c1, and if (A1,.., AK) is the division of V we want, we write \u03b1j = vol (Aj). We have \u03b11 + \u00b7 \u00b7 + \u03b1K = d = 1 > D1. Since (Xj) > DXj = \u03b1jc 2, we normalize the Xj situation so that (Xj) > DXj = \u00b7 \u00b7 \u00b7 (XK) > DXK = c2, and we look at X = [X1 \u221a 1 X2 itely \u03b12 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 X K. Then we have the following result: Proposition 4.2. If X = [X1 \u221a 1 \u03b11 (X2) = thermal position (\u00b7 XK) = Z = orthogonal, then there is an orgonal problem."}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 73", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.5 Finding a Discrete Solution Close to a Continuous", "text": "The next step is to find an exact solution (P (X1),.., P (XK), P (K), P (K), P (K), P (K), which comes closest to our approximate solution (Z1),.. \u2212, ZK), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K, K, K, K, K, K (K, K, K, K, K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K (K), K (K), K (K), K (K), K (K), K (K (K), K (K), K (K), K (K), K (K), K (K, K, K (K), K (K), K (K (K, K), K (K), K (K, K), K (K, K, K (K, K, K, K, K), K (K, K), K (K, K, K (K, K), K (K (K, K, K, K, K), K (K, K (K, K), K (K), K (K (K, K), K (K), K (K (K, K), K (K, K, K, K (K, K), K (K, K, K), K (K (K), K (K, K), K (K ("}, {"heading": "74 CHAPTER 4. GRAPH CLUSTERING", "text": "This choice remains to follow from Yu [23] and the previous discussion > Q > Q > Q > Q > Q (2), we are looking for pairs (X \u2212 Q) with X \u2212 ZQ and where Q \u2212 K matrix with not zero and pairs of orthogonal columns (A > A). Yu [23] and Shi [24] are considering where Q \u2212 ZQ (K). A D \"re F is the Frobenius standard of A, A > A. tS\" o, tS \". tS.\".tS. \".t.z.\".tmi., \"D\" i \"r\" i \"tmi\" K, \"D\" K \"R\" R \"R\" R \"R\".K \"D\" R \"D\" D \"W,\" W \"W\" W, \"W\" W \"W, W\" W \"D W, W\" W \"W\" W tW, W \"W\" W \"W, D\" W \"W\" W \"W, W\" W \"W\" tmi, W \"tmi, W\" e \"W\" W \"W\" W, W \"W\" W \"W. D. D. W. W. W. W."}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 75", "text": "Proposition 4.3. For each n \u00b7 n matrix A and each orthogonal matrix > R > > V > V > V = V >, havemax {tr (QA) | Q \u2022 O (n) = \u03c31 + \u00b7 \u00b7 \u00b7 + \u03c3n, where \u03c31 \u2265 \u00b7 \u2265 \u03c3n are the singular values of A. Furthermore, this maximum is reached by Q = V >, where A = U\u0432 V > for A.Proof. is an orthogonal matrix. Matrix Z = V > is an orthogonal matrix, i.e. | zij | 1 for 1 \u2264 i, j \u2264 n, and \u03a3 is a diagonal matrix, so that we have a diagonal matrix (Z\u03a3 V) = z11\u04451 + \u00b7 \u00b7 \u00b7 Proxima."}, {"heading": "76 CHAPTER 4. GRAPH CLUSTERING", "text": "The proof: Since \"Z\" is a diagonal matrix, we have \"X,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"Z,\" \"\" Z, \"\" \"Z,\" \"\" Z, \"\" Z, \"\" \"Z,\" \"\" Z, \"\" \"Z,\" Z, \"\" Z, \"\" Z, \"\" Z, \"\" Z, \"\" Z, \"\" Z, \"\" \"Z,\" \"\" Z, \"\" Z, \"\" Z, \"\" \"Z,\" \"Z,\" \"\" Z, \"\" Z, \"\" \"Z,\" \"\" Z, \"\" Z, \"\" \"Z,\" \"\" Z, \"Z,\" \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"Z,\" Z, \"\" Z, \",\" \"\", \"Z,\", \"Z,\", \"Z,\", \"Z,\" Z, \",\", \",\", \"Z,\""}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 77", "text": "Minimizing \"X - ZQ\" F is equivalent to maximizingtr (Q > Z > X) = tr ((ZQ) > X) = * (X (ZQ) >), and since the ith line of X contains a single unequal entry a in column ji (1 \u2264 ji \u2264 K), the quantity tr (XY >) is maximized iff jiji is maximized for i = 1,.., N; this is achieved by selecting a column index \"such that yi\" is maximum for the ith line of X. To find the shape of \"X,\" we first find a matrix X by selecting a single unequal entry xij = 1 in line i so that it is maximum by the following method."}, {"heading": "78 CHAPTER 4. GRAPH CLUSTERING", "text": "The fact that X has found a solution to the problem (2) is determined by Yu (2), QQ (3), Q (3), Q (3), Q (3), Q (4), Q (4), Q (4), Q (4), Q (4), Q (4), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5), Q (5, Q (5), Q (5), Q (5), Q (5, Q (5), Q (5), Q (5), Q (5, Q (5), Q (5), Q (5, Q (5), Q (5), Q (5, Q (5), Q (5, Q (5), Q (5), Q (5, Q (5), Q (5, Q (5), Q (5, Q (5), Q (5, Q (5), Q (5, Q (5), Q (5, Q (5), Q (5), Q (5, Q (5), Q (5, Q (5, Q (5), Q (5, Q (5), Q (5), Q (5), Q (5, Q (5, Q (5), Q (5, Q (5), Q (5), Q (5, Q"}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 79", "text": "\"We are the first, we are the second, third, fourth, fourth, fourth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth, sixth."}, {"heading": "80 CHAPTER 4. GRAPH CLUSTERING", "text": "The intuition behind this method is that if a continuous solution Z can be sent by a rigid motion close to a discrete solution X, then many lines Z, which are considered as vectors in RK, should be almost orthogonal. Thus, ZR should contain at least K lines that are well aligned with the canonical base vectors, and these lines are good candidates for some lines of the discrete solution X.The algorithm given in Yu [23] requires a small correction, since rows are not removed from Z when added to R, which can result in the same line being repeated to R. Given the N-K matrix Z (whose columns all have the same norm), we calculate a matrix R whose columns are certain rows of Z. We use a vector c-RN to track the inner products of all rows of Z \u2212 with columns R1 \u2212."}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 81", "text": "At the end of the above process, we normalize the columns of R to obtain a matrix that we call R2. After some experiments, we have determined that it is desirable to proceed from a variant of the continuous solution Z, which we obtain by solving the problem. We have implemented three methods. 1. We are trying to shift the columns of Z by some diagonal invertible matrix, so that the columns of Z are used by some diagonal invertible matrix. Since the vector of the columns of Z is used by a diagonal invertible matrix, the use of K = diag (1,.., \"K\"), which is the least quadratic problem, can be as much as possible in the least quadratic sense."}, {"heading": "82 CHAPTER 4. GRAPH CLUSTERING", "text": "3. We use a more drastic method than (2), which consists in normalizing the lines of Z."}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 83", "text": "If we apply the method (using method 3 to find the initial R) to the graph assigned to matrix W1 in Figure 4.6 for K = 4 clusters, the algorithm converges in 3 steps and we find the clusters shown in Figure 4.7. The solution Z of the relaxed problem is Z = \u2212 21.3146 \u2212 0.0000 19.4684 \u2212 15.4303 \u2212 4.1289 0.0000 16,7503 \u2212 15.4303 \u2212 21.3146 \u2212 32,7327 \u2212 19.4684 \u2212 15.4303 \u2212 23.4303 16.7150 \u2212 15.4303 \u2212 9.3547 \u2212 15.4303 \u2212 15.4150 0.0000 9.3547 \u2212 15.4303 \u2212 15.3303 \u2212 32.719.327 \u2212 15.4303 \u2212 15.450427 \u2212 15.4503Q \u2212 327 \u2212 15.715.4303 \u2212 15.430355 \u2212 15.4303 \u2212 15.4303 \u2212 3303 \u2212 15.4303.We find the following sequence for Q 4303 \u2212 32.319.327 \u2212 15.4304 \u2212 15.415.4303 \u2212 15.4304 \u2212 15.4303 \u2212 15.415.4303 \u2212 3303 \u2212 15.415.430.430.4 \u2212 15.430.4 \u2212 32.3 \u2212 15.430.4 \u2212 15.4303 \u2212 15.430.4 \u2212 430.4 \u2212 16.430.3 \u2212 15.4303 \u2212 15.430.3 \u2212 15.430.3 \u2212 15.430.4 \u2212 15.430.3 \u2212 15.430.3 \u2212 15.430.4 \u2212 15.430.3 \u2212 15.430.4 \u2212 15.430.3 \u2212 15.430.3 \u2212 15.430.3 \u2212 15.430.4 \u2212 15.430.3 \u2212 15.430.3 \u2212 15.430.4 \u2212 327.3 \u2212 15.430.3 \u2212 15.430.3 \u2212 15.430.3 \u2212 15.430.4 \u2212 327.4 \u2212 15.430.4 \u2212 327.4 \u2212 327.4 \u2212 327.4 \u2212 15.430.4 \u2212 15.430.4 \u2212 327.4 \u2212 327.3 \u2212 15.430.3 \u2212 327.4 \u2212 327.3 \u2212 15.430.4 \u2212 327.3 \u2212"}, {"heading": "84 CHAPTER 4. GRAPH CLUSTERING", "text": "This is the first Q = 0,0000 \u2212 10,3162 30,4065 6,3600 0,00000 \u2212 1,3742 22,2703 \u2212 6,1531 \u2212 32,7327 \u2212 32,6044 \u2212 1,2967 2,58840.0000 \u2212 1,3742 22,2703 \u2212 6,1531 0,0000 = 8,9576 8.0309 \u2212 23,8653 \u2212 23,1455 \u2212 20,5505 \u2212 5,0065 \u2212 9,3982 32,7327 \u2212 32,6044 \u2212 1,2967 2,5884 \u2212 1,5884 23,1455 \u2212 20,5505 \u2212 5,0065 \u2212 9,3982 \u2212 1,7520 \u2212 7,2027 \u2212 25,6776 X = 0 \u2212 \u2212 51,0065 \u2212 32,6044 \u2212 1,5884 \u2212 1,0055 \u2212 20,5505 \u2212 5,005 \u2212 5,005 \u2212 5.001,007 \u2212 1,007 \u2212 1,007 280,0180,018 0,000,000,0- 0,0- 0,01- 0,01- 0,01- 10,00,00,000,1- 10,001- 1000,001- 10,000,001- 10,000,001- - - 10,000,000,000,- - - - - 0,000,000,000,000,000,- - - - - - - - - - 0,000,000,000,000,018 0,000,018 0,000,000,000,000,0- 0,000,0- 0,0- 0,0- 0,000,0- 0,018 0,000,000,000,000,0- 0,000,0- 0,000,0- 0,000,000,000,001,001 - 0,001 - 0,000,000,000,001 - 0,000,000,001 - 0,000,001 - 0,000,000,000,001 - 0,000,000,000,000,001 - 0,000,000,000,000,001 - 0,000,000,000,000,001 - 0,000,000,001 - 0,000,000,000,000,000,000,000,001 - 0,000,000,000,000,001 - 0,000,000,000,000,000,000,000,00"}, {"heading": "4.5. DISCRETE SOLUTION CLOSE TO A CONTINUOUS APPROXIMATION 85", "text": "In this example, the columns of all X entries were non-zero, but this can happen if, for example, we apply the algorithm to the graphs of Figure 4.6 to find K = 5 clusters shown in Figure 4.8.We find that the initial value for Z entries Q = -5.7716 \u2212 27.5934 0.0000 \u2212 9.3618 \u2212 0.0000 5.5839 \u2212 20.2099 \u2212 29.7044 \u2212 1.2471 \u2212 0.0000 \u2212 2.3489 1.1767 \u2212 0.0000 \u2212 29.5880 \u2212 29.7044 5.5839 \u2212 20.2099 29.7044 \u2212 1.2471 0.0000 21.6574 \u2212 7.2879 0.0000 8.1289 \u2212 8.5287 4.5433 \u2212 0.0000 \u2212 0.0000 \u2212 2.3489 1.1767 \u2212 0.0000 \u2212 29.7080 \u2212 29.7080 \u2212 29.7044 \u2212 0.2471 0.0000 21.6574 \u2212 0.0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"}, {"heading": "5.1 Signed Graphs and Signed Laplacians", "text": "Intuitively, a positive weight edge means similarity or proximity to its endpoints. For many reasons, it is desirable to allow negative weight edges, the intuition being that a negative weight indicates dissimilarity or distance.Weighted graphs, for which the weight matrix is a symmetrical matrix in which negative and positive entries are allowed, are called signed graphs. Such graphs (with weights (\u2212 1, 0, + 1)) were introduced by Harary [12] back in 1953 to model social relationships in which dislike, indifference, and sympathy play a role. Of course, the problem of clustering the nodes of a signed graph \u2212 arises as a generalization of the cluster problem for weighted graphs. From our perspective, we would like to know whether clustering with normalized cuts can be extended to signed graphs."}, {"heading": "88 CHAPTER 5. SIGNED GRAPHS", "text": "In fact, many results in this section come from Kunegis and al. [16]. However, it should be noted that in the above papers only clustering is considered \u2212 \u2212 \u2212 \u2212 \u2212 This requires a modification of the concept of the normalized section. This new idea is perfectly reasonable, as we will see shortly. If (V, W) is a signed graph where W is a m \u00b7 m symmetrical matrix with zero diagonal entries and with the other entries wij R is arbitrary, for each node vi-V, which is signed links vi-V, which is signed links vi-V-V-V, the signed link is vi-V-V-V, the signed link is sub-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V."}, {"heading": "5.1. SIGNED GRAPHS AND SIGNED LAPLACIANS 89", "text": "For a diagram without isolated vertices, we have d (vi) > 0 for i = 1,.., m, i.e. D \u2212 1 / 2is well defined.The marked laplaker is symmetrically positive semidefinity.As for the laplaker of a weight matrix (with non-negative entries), this can be done in two ways.The first method consists in defining a term of the incidence matrix for a marked diagram and appears in Hou [14].Definition 5.1. For a given predefined diagram G = (V, W), with V = {v1,.,., vm}, if {e1,.., en} the edges of the underlying diagram of G (remember that {vi, vj} one edge of this diagram is iff wij 6 = 0), for an oriented diagram Gvi, which is reached by indicating an orientation to the underlying graph of G < the incient of G is G."}, {"heading": "L = D \u2212W is symmetric and positive semidefinite; that is, the eigenvalues of L = D \u2212W are real and nonnegative.", "text": "Another way to prove that L is positively semidefined is to evaluate the square shape x > Lx. We need this calculation to find out what the new concept of normalized intersection is. For each real \u03bb-R, we define sgn (\u03bb) bysgn (\u03bb) = + 1 if \u03bb > 0 \u2212 1 if \u03bb < 0 if \u03bb = 0, Proposition 5,2. If we allow for each m \u00d7 m symmetrical matrix W = (wij) L = D \u2212 W, where D is the signed degree matrix associated with W, then we have a Vex > Lx = 12 m \u2211 i, j = 1 | wij | (xi \u2212 sgn (wij) xj) 2 for all x-Rm. Consequently, L is positively semidefined."}, {"heading": "90 CHAPTER 5. SIGNED GRAPHS", "text": "Proof: We have x > Lx = x > Dx \u2212 x > Wx = m \u2211 i = 1 dix 2 i \u2212 m \u2211 i, j = 1 wijxixj = m \u2211 i, j = 1 (| wij | x2i \u2212 wijxixj) = m \u2211 i, j = 1 (| wij | (x2i \u2212 sgn (wij) xixj) = 12 (m \u2211 i, j = 1 | wij | (x2i \u2212 2sgn (wij) xixj + x2j))) = 12 m \u2211 i, j = 1 | wij | (xi \u2212 sgn (wij) xj) 2, and this quantity is not negative."}, {"heading": "5.2 Signed Normalized Cuts", "text": "The sum on the righthand side splits into four parts: (1) S1 = 2 (1) S1 (1) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) S2 (2) (2) S2 (2) S2 (2) S2 (2) (2) S2 (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2 (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2 (2) (2) (2) (2 (2) (2) (2 (2) (2) (2 (2) (2) (2) (2 (2) (2) (2 (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) ("}, {"heading": "92 CHAPTER 5. SIGNED GRAPHS", "text": "Based on previous calculations, we have Ncut (A1,.., AK) = K \u2211 j = 1 (Xj) > LXj (Xj) > DXj.where X is the N \u00b7 K matrix whose jste column is Xj. Therefore, this is the same problem as in Chapter 4, where L is replaced by L and D. Note that minimizing sNcut (A1,., AK) amounts to minimizing the number of positive and negative edges between clusters, and also minimizing the number of negative edges within clusters. This second minimization does not capture the intuition connected by a negative edge together (they do not \"like\" each other; they should be far apart)."}, {"heading": "5.3 Balanced Graphs", "text": "Since Ncut (A1,..., AK) = K \u2211 j = 1 (Xj) > LXj (Xj) > DXj, the entire machinery of sections 4.3 and 4.5 can be applied, where D is replaced by D and L by L. However, there is a new phenomenon, namely that L can be positive."}, {"heading": "5.3. BALANCED GRAPHS 93", "text": "Consequently, 1 is not always a eigenvector of L. As observed by Kunegis et al. [16], it is also possible to characterize for which signed graphs the laplacian L is unambiguously positive. Such graphs are \"cousins\" of bipartite graphs and were introduced by Harary [12]. Since a graph is the union of its associated components, we limit ourselves to linked graphs. Definition 5.3. In view of a signed graph G = (V, W) with negative weights whose underlying graph is connected, let us say that G is balanced when there is a division of its nodes V into two blocks V1 and V2, so that all positive edges connect nodes within V1 or V2, and negative edges connect nodes between V1 and V2.An example of a balanced graph is shown in Figure 5.1 on the left, in the negative node V2, vvv2, in the positive node V2, vary from the positive edges of V5 and V6, vary from this {4 to v8)."}, {"heading": "94 CHAPTER 5. SIGNED GRAPHS", "text": "In the eeisrVnree\u00fcgn nvo edn eVrlrrrrrrrrrrte\u00fceegnr rf\u00fc ide eVrlrrrrrrrte\u00fceegnr vno edn eVrrrrrrlrrtee\u00fceeervnlrteeeeeeeeeeeeeeeeeeeeeVnre\u00fcgr ni nde eVrlrrrrrrrrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeVnrrrrrrrrrrrrrrrrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeVno rf\u00fc ide ide eVnrrrrrrrrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeVno eeeeeeeeeeeeeeeeeVno eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "5.3. BALANCED GRAPHS 95", "text": "Nodes in V1, and the value \u2212 c at all nodes in V2, so it must be a multiple of the vector u given by ui = + 1 iff vi-V1 and ui = \u2212 1 iff vi-V2. \u2212 Therefore \u2212 dim (Ker (B >)) = 1, and rank = m \u2212 1. The third part of the proposal has already been shown. \u2212 Note: A simple modification of the evidence for Proposition 5.5 shows that if there are c1 components that contain only positive edges, c2 components that are balanced graphs, and c3 components that are unbalanced (and contain some negative edges), thenc1 + c2 = m \u2212 rank (B). \u2212 Since by Proposition 5.1 we have L = BB > for each incidence matrix B associated with an orientation of the underlying graph of G, and c3 components that are not balanced, c2 and c2 (and some edges) \u2212 B."}, {"heading": "96 CHAPTER 5. SIGNED GRAPHS", "text": "It gives us the bipartition ({v1, v2, v4, v7, v8}, [v3, v6, v9}) as indicated by Proposition 5.5.The signed P2 of the unbalanced graph G2 is in fact of L2 = > 0-1 0-1 0-1 0-1 0-1 0-1 0-1 0-1 0-1 0-1 0-1 0-1 0-1 0-1 0-1 0-1 0-3 The eigenvalues of L2 are0.5175, 1.5016, 1.7029, 2.7058, 3.7284, 4.9604, 5.6026, 7.0888, 8.1921.The matrix L2 is actually positive (since G2 is balanced)."}, {"heading": "98 CHAPTER 5. SIGNED GRAPHS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.5. SIGNED GRAPH DRAWING 99", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.5 Signed Graph Drawing", "text": "According to Kunegis et al. [16], if our goal is to draw a pre-drawn diagram G = (V, W) with m-nodes (V, W), a natural method of interpreting negative weights is to assume that the endpoints vi and vj of an edge with a negative weight should be far apart, which can be achieved if instead of mapping the point f (vj) to vj, we assign the point \u2212 n (vj).Then, if R is the m \u00b7 n matrix of a diagram from G to Rn, the energy function E (R) is redefined to beE (R) = {vi, vj}."}, {"heading": "G5 = G3; G5(1,2) = -1; G5(2,1) = -1;", "text": "The graph we have received is shown on the left and the graph we have received is shown on the right in Figure 5.7. Positive edges are shown in blue and negative edges when shown in rot.5.5. SIGNED GRAPH DRAWING 101The third example is the signed graph G6 defined by the weight matrix given by the following graph: nn = 24; G6 = diag (One (1, nn), 1); G6 = G6 + G6 (1, nn + 1) = 1; G6 (G6, nn + 1) = 1; G6 (12.11) = -1; G6 (2.6 (2.6) = -1; G1 = -1; G1.6 (7.6) = (-12) = (G1 = 1.11); G1 = 1.1; G1 (1.1); G1 = 1.11 (-1)."}, {"heading": "Appendix A", "text": "Rayleigh Ratios and the Courant-Fischer TheoremThe most important property of symmetric matrices is that they have real eigenvalues and that they can be diagonalized in relation to an orthogonal matrix. (So if A is an n = > symmetric matrix, then it has n real eigenvalues \u03bb1,.), \u03bbn (not necessarily distinguishable), and there is an orthonormal basis of eigenvectors (u1,., un) (for proof see Gaul [8]). Another fact that is often used in optimization is that the eigenvalues of a symmetric matrix are characterized in relation to what is known as the Rayleigh ratio, defined by R (x) = x > Axx > x > x, x > Rn, x 6 = 0.The following statement is often used to prove the correctness of various optimizations or approximation problems."}, {"heading": "Appendix B", "text": "It is a matter of time until there is a solution. (...) It is a matter of time until there is a solution. (...) It is a matter of time until there is a solution. (...) It is a matter of time until there is a solution. (...) It is a matter of time until there is a solution. (...) It is a matter of time until there is a solution. (...) It is a matter of time until there is a solution. (...) It is a matter of time until there is a solution. (...) It is a matter of time until there is a solution. (...) It is a matter of time until there is a solution. (...) It is a matter of time until there is a solution. (...) It is a matter of time until there is a solution. (...) It is a question of time until there is a solution. (...) It is a question of time. (...) It is a question of time. (...) It is a question of time."}], "references": [{"title": "Correlation clustering", "author": ["Nikhil Bansal", "Avrim Blum", "Shuchi Chawla"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "G\u00e9om\u00e9trie diff\u00e9rentielle: vari\u00e9t\u00e9s, courbes et surfaces. Collection Math\u00e9matiques", "author": ["Marcel Berger", "Bernard Gostiaux"], "venue": "Puf, second edition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "Spectral Graph Theory, volume 92 of Regional Conference Series in Mathematics", "author": ["Fan R.K. Chung"], "venue": "AMS, first edition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Correlation clustering with partial information", "author": ["Eric D. Demaine", "Nicole Immorlica"], "venue": "Working Notes of the 6th International Workshop on Approximation Algorithms for Combinatorial Problems, LNCS", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Discrete Mathematics. Universitext", "author": ["Jean H. Gallier"], "venue": "Springer Verlag, first edition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Geometric Methods and Applications", "author": ["Jean H. Gallier"], "venue": "For Computer Science and Engineering. TAM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Riemannian Geometry", "author": ["S. Gallot", "D. Hulin", "J. Lafontaine"], "venue": "Universitext. Springer Verlag, second edition", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "Algebraic Graph Theory", "author": ["Chris Godsil", "Gordon Royle"], "venue": "GTM No. 207. Springer Verlag, first edition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Gene and F", "author": ["H. Golub"], "venue": "Van Loan, Charles. Matrix Computations. The Johns Hopkins University Press, third edition", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "On the notion of balance of a signed graph", "author": ["Frank Harary"], "venue": "Michigan Math. J.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1953}, {"title": "Bounds for the least laplacian eigenvalue of a signed graph", "author": ["Jao Ping Hou"], "venue": "Acta Mathematica Sinica,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Spectral surface reconstruction from noisy point clouds", "author": ["Ravikrishna Kolluri", "Jonathan R. Shewchuk", "James F. O\u2019Brien"], "venue": "In Symposium on Geometry Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Spectral analysis of signed graphs for clustering, prediction and visualization", "author": ["J\u00e9r\u00f4me Kunegis", "Stephan Schmidt", "Andreas Lommatzsch", "J\u00fcrgen Lerner", "Ernesto William De Luca", "Sahin Albayrak"], "venue": "In SDM\u201910,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Introduction to Smooth Manifolds", "author": ["John M. Lee"], "venue": "GTM No. 218. Springer Verlag, first edition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Semi-Riemannian Geometry With Applications to Relativity", "author": ["Barrett O\u2019Neill"], "venue": "Pure and Applies Math.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1983}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Spectral graph theory", "author": ["Daniel Spielman"], "venue": "In Uwe Naumannn and Olaf Schenk, editors,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "A tutorial on spectral clustering", "author": ["von Luxburg Ulrike"], "venue": "Statistics and Computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Computational Models of Perceptual Organization", "author": ["Stella X. Yu"], "venue": "PhD thesis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Multiclass spectral clustering", "author": ["Stella X. Yu", "Jianbo Shi"], "venue": "In 9th International Conference on Computer", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}], "referenceMentions": [{"referenceID": 8, "context": "Following Godsil and Royle [10], we prove that E(R) = tr(R>LR),", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "We give a number examples of graph drawings, many of which are borrowed or adapted from Spielman [21].", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "This beautiful and deeply original method first published in Shi and Malik [20], has now come to be a \u201ctextbook chapter\u201d of computer vision and machine learning.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "This method was extended to K \u2265 3 clusters by Stella Yu in her dissertation [23], and is also the subject of Yu and Shi [24].", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "This method was extended to K \u2265 3 clusters by Stella Yu in her dissertation [23], and is also the subject of Yu and Shi [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "A solution using the volume vol(Ai) of Ai (for K = 2) was proposed and investigated in a seminal paper of Shi and Malik [20].", "startOffset": 120, "endOffset": 124}, {"referenceID": 19, "context": "Subsequently, Yu (in her dissertation [23]) and Yu and Shi [24] extended the method to K > 2 clusters.", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "Subsequently, Yu (in her dissertation [23]) and Yu and Shi [24] extended the method to K > 2 clusters.", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "In view of the above, we have our first formulation of K-way clustering of a graph using normalized cuts, called problem PNC1 (the notation PNCX is used in Yu [23], Section 2.", "startOffset": 159, "endOffset": 163}, {"referenceID": 11, "context": "The idea to use positive degrees of nodes in the degree matrix of a signed graph with weights (\u22121, 0, 1) occurs in Hou [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "The natural step of using absolute values of weights in the degree matrix is taken by Kolluri, Shewchuk and O\u2019Brien [15] and Kunegis et al.", "startOffset": 116, "endOffset": 120}, {"referenceID": 13, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16], we show that the signed Laplacian L is positive definite iff G is unbalanced, which means that it contains some cycle with an odd number of negative edges.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "As far as I know, except for a short section in one of Gilbert Strang\u2019s book, and von Luxburg [22] excellent survey on spectral clustering, there is no comprehensive writing on the topic of graph cuts.", "startOffset": 94, "endOffset": 98}, {"referenceID": 8, "context": "(After Godsil and Royle [10], Section 8.", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": "Remark: Yu and Shi [24] use the notation degree(A) instead of vol(A).", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "We reproduce the proof in Gallier [7] (see also Godsil and Royle [10]).", "startOffset": 34, "endOffset": 37}, {"referenceID": 8, "context": "We reproduce the proof in Gallier [7] (see also Godsil and Royle [10]).", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "For more on the properties of the Fiedler number, see Godsil and Royle [10] (Chapter 13) and Chung [4].", "startOffset": 71, "endOffset": 75}, {"referenceID": 3, "context": "For more on the properties of the Fiedler number, see Godsil and Royle [10] (Chapter 13) and Chung [4].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "Properties (7)\u2013(10) are proved in Chung [4] (Chapter 1).", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "As explained in Godsil and Royle [10], we can imagine building a physical model of G by connecting adjacent vertices (in R) by identical springs.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "The following proposition is shown in Godsil and Royle [10].", "startOffset": 55, "endOffset": 59}, {"referenceID": 8, "context": "We present the proof given in Godsil and Royle [10] (Section 13.", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "Some of these are borrowed or adapted from Spielman [21].", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "We use the following program to compute u2 and u3: A = [0 1 1 0; 1 0 0 1; 1 0 0 1; 0 1 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eigs(L); gplot(A, v(:,[3 2])) hold on; gplot(A, v(:,[3 2]),\u2019o\u2019) The graph of Example 1 is shown in Figure 3.", "startOffset": 153, "endOffset": 158}, {"referenceID": 1, "context": "We use the following program to compute u2 and u3: A = [0 1 1 0; 1 0 0 1; 1 0 0 1; 0 1 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eigs(L); gplot(A, v(:,[3 2])) hold on; gplot(A, v(:,[3 2]),\u2019o\u2019) The graph of Example 1 is shown in Figure 3.", "startOffset": 153, "endOffset": 158}, {"referenceID": 2, "context": "We use the following program to compute u2 and u3: A = [0 1 1 0; 1 0 0 1; 1 0 0 1; 0 1 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eigs(L); gplot(A, v(:,[3 2])) hold on; gplot(A, v(:,[3 2]),\u2019o\u2019) The graph of Example 1 is shown in Figure 3.", "startOffset": 183, "endOffset": 188}, {"referenceID": 1, "context": "We use the following program to compute u2 and u3: A = [0 1 1 0; 1 0 0 1; 1 0 0 1; 0 1 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eigs(L); gplot(A, v(:,[3 2])) hold on; gplot(A, v(:,[3 2]),\u2019o\u2019) The graph of Example 1 is shown in Figure 3.", "startOffset": 183, "endOffset": 188}, {"referenceID": 1, "context": "A = [0 1 1 0 0; 1 0 1 1 1; 1 1 0 1 0; 0 1 1 0 1; 0 1 0 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 121, "endOffset": 126}, {"referenceID": 2, "context": "A = [0 1 1 0 0; 1 0 1 1 1; 1 1 0 1 0; 0 1 1 0 1; 0 1 0 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 121, "endOffset": 126}, {"referenceID": 1, "context": "A = [0 1 1 0 0; 1 0 1 1 1; 1 1 0 1 0; 0 1 1 0 1; 0 1 0 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 151, "endOffset": 156}, {"referenceID": 2, "context": "A = [0 1 1 0 0; 1 0 1 1 1; 1 1 0 1 0; 0 1 1 0 1; 0 1 0 1 0]; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 151, "endOffset": 156}, {"referenceID": 1, "context": "A = diag(ones(1, 11),1); A = A + A\u2019; A(1, 12) = 1; A(12, 1) = 1; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 125, "endOffset": 130}, {"referenceID": 2, "context": "A = diag(ones(1, 11),1); A = A + A\u2019; A(1, 12) = 1; A(12, 1) = 1; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 125, "endOffset": 130}, {"referenceID": 1, "context": "A = diag(ones(1, 11),1); A = A + A\u2019; A(1, 12) = 1; A(12, 1) = 1; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 155, "endOffset": 160}, {"referenceID": 2, "context": "A = diag(ones(1, 11),1); A = A + A\u2019; A(1, 12) = 1; A(12, 1) = 1; D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on gplot(A, v(:, [2 3]),\u2019o\u2019)", "startOffset": 155, "endOffset": 160}, {"referenceID": 1, "context": "A = double(A >0); gplot(A,xy) D = diag(sum(A)); L = D - A; [v, e] = eigs(L, 3, \u2019sm\u2019); figure(2) gplot(A, v(:, [2 1])) hold on gplot(A, v(:, [2 1]),\u2019o\u2019)", "startOffset": 110, "endOffset": 115}, {"referenceID": 0, "context": "A = double(A >0); gplot(A,xy) D = diag(sum(A)); L = D - A; [v, e] = eigs(L, 3, \u2019sm\u2019); figure(2) gplot(A, v(:, [2 1])) hold on gplot(A, v(:, [2 1]),\u2019o\u2019)", "startOffset": 110, "endOffset": 115}, {"referenceID": 1, "context": "A = double(A >0); gplot(A,xy) D = diag(sum(A)); L = D - A; [v, e] = eigs(L, 3, \u2019sm\u2019); figure(2) gplot(A, v(:, [2 1])) hold on gplot(A, v(:, [2 1]),\u2019o\u2019)", "startOffset": 140, "endOffset": 145}, {"referenceID": 0, "context": "A = double(A >0); gplot(A,xy) D = diag(sum(A)); L = D - A; [v, e] = eigs(L, 3, \u2019sm\u2019); figure(2) gplot(A, v(:, [2 1])) hold on gplot(A, v(:, [2 1]),\u2019o\u2019)", "startOffset": 140, "endOffset": 145}, {"referenceID": 17, "context": "Our last example, also borrowed from Spielman [21], corresponds to the skeleton of the \u201cBuckyball,\u201d a geodesic dome invented by the architect Richard Buckminster Fuller (1895\u20131983).", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "A = full(bucky); D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on; gplot(A,v(:, [2 3]), \u2019o\u2019)", "startOffset": 77, "endOffset": 82}, {"referenceID": 2, "context": "A = full(bucky); D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on; gplot(A,v(:, [2 3]), \u2019o\u2019)", "startOffset": 77, "endOffset": 82}, {"referenceID": 1, "context": "A = full(bucky); D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on; gplot(A,v(:, [2 3]), \u2019o\u2019)", "startOffset": 107, "endOffset": 112}, {"referenceID": 2, "context": "A = full(bucky); D = diag(sum(A)); L = D - A; [v, e] = eig(L); gplot(A, v(:, [2 3])) hold on; gplot(A,v(:, [2 3]), \u2019o\u2019)", "startOffset": 107, "endOffset": 112}, {"referenceID": 1, "context": "[x, y] = gplot(A, v(:, [2 3])); [x, z] = gplot(A, v(:, [2 4])); plot3(x,y,z)", "startOffset": 23, "endOffset": 28}, {"referenceID": 2, "context": "[x, y] = gplot(A, v(:, [2 3])); [x, z] = gplot(A, v(:, [2 4])); plot3(x,y,z)", "startOffset": 23, "endOffset": 28}, {"referenceID": 1, "context": "[x, y] = gplot(A, v(:, [2 3])); [x, z] = gplot(A, v(:, [2 4])); plot3(x,y,z)", "startOffset": 55, "endOffset": 60}, {"referenceID": 3, "context": "[x, y] = gplot(A, v(:, [2 3])); [x, z] = gplot(A, v(:, [2 4])); plot3(x,y,z)", "startOffset": 55, "endOffset": 60}, {"referenceID": 16, "context": "A solution using the second measure (the volume) (for K = 2) was proposed and investigated in a seminal paper of Shi and Malik [20].", "startOffset": 127, "endOffset": 131}, {"referenceID": 19, "context": "Subsequently, Yu (in her dissertation [23]) and Yu and Shi [24] extended the method to K > 2 clusters.", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "Subsequently, Yu (in her dissertation [23]) and Yu and Shi [24] extended the method to K > 2 clusters.", "startOffset": 59, "endOffset": 63}, {"referenceID": 18, "context": "This is the choice adopted in von Luxburg [22].", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "Shi and Malik [20] use a = 1, b = \u2212 \u03b1 d\u2212 \u03b1 = \u2212 k 1\u2212 k ,", "startOffset": 14, "endOffset": 18}, {"referenceID": 1, "context": "Another choice found in the literature (for example, in Belkin and Niyogi [2]) is", "startOffset": 74, "endOffset": 77}, {"referenceID": 16, "context": "Unfortunately, this is an NP-complete problem, as shown by Shi and Malik [20].", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "which is the solution presented in von Luxburg [22].", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "This choice also corresponds to the scaled partition matrix used in Yu [23] and Yu and Shi [24].", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "This choice also corresponds to the scaled partition matrix used in Yu [23] and Yu and Shi [24].", "startOffset": 91, "endOffset": 95}, {"referenceID": 19, "context": "X1K = 1N is used in Yu [23].", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "This normalization step is used by Yu [23] in the search for a discrete solution closest to a solution of a relaxation of our original problem.", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "However, it is well known that XX is the orthogonal projection of R onto the range of X (see Gallier [8], Section 14.", "startOffset": 101, "endOffset": 104}, {"referenceID": 19, "context": "In view of the above, we have our first formulation of K-way clustering of a graph using normalized cuts, called problem PNC1 (the notation PNCX is used in Yu [23], Section 2.", "startOffset": 159, "endOffset": 163}, {"referenceID": 19, "context": "This second option is the one chosen by Yu [23] and Yu and Shi [24] (actually, they work with 1 K (K \u2212 \u03bc(X, .", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "This second option is the one chosen by Yu [23] and Yu and Shi [24] (actually, they work with 1 K (K \u2212 \u03bc(X, .", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "In fact, since the eigenvalues of Lsym are in the range [0, 2], the eigenvalues of 2I\u2212Lsym = I+ D\u22121/2WD\u22121/2 are also in the range [0, 2] (that is, I+D\u22121/2WD\u22121/2 is positive semidefinite).", "startOffset": 56, "endOffset": 62}, {"referenceID": 1, "context": "In fact, since the eigenvalues of Lsym are in the range [0, 2], the eigenvalues of 2I\u2212Lsym = I+ D\u22121/2WD\u22121/2 are also in the range [0, 2] (that is, I+D\u22121/2WD\u22121/2 is positive semidefinite).", "startOffset": 130, "endOffset": 136}, {"referenceID": 15, "context": "Furthermore, both St(k, n) and G(k, n) are naturally reductive homogeneous manifolds (for the Stiefel manifold, when n \u2265 3), and G(k, n) is even a symmetric space (see O\u2019Neill [18]).", "startOffset": 176, "endOffset": 180}, {"referenceID": 8, "context": "This is not quite so, and Godsil and Royle [10] provide a rigorous proof using Proposition A.", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "A similar observation is made in Yu [23] and Yu and Shi [24] (but beware that in these works \u03b1 = vol(A)/ \u221a d).", "startOffset": 36, "endOffset": 40}, {"referenceID": 20, "context": "A similar observation is made in Yu [23] and Yu and Shi [24] (but beware that in these works \u03b1 = vol(A)/ \u221a d).", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "Inspired by Yu [23] and the previous discussion, given a solution Z of problem (\u22172), we look for pairs (X,Q) with X \u2208 X and where Q is a K\u00d7K matrix with nonzero and pairwise orthogonal columns, with \u2016X\u2016F = \u2016Z\u2016F , that minimize \u03c6(X,Q) = \u2016X \u2212 ZQ\u2016F .", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "Yu [23] and Yu and Shi [24] consider the special case where Q \u2208 O(K).", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Yu [23] and Yu and Shi [24] consider the special case where Q \u2208 O(K).", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "3 (with A = Z>X and Q = R>), we get the following result (see Golub and Van Loan [11], Section 12.", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "The above method is essentially the method described in Yu [23] and Yu and Shi [24], except that in these works (in which X,Z and Y are denoted by X\u2217, X\u0303\u2217, and X\u0303, respectively) the entries in X belong to {0, 1}; as described above, for row i, the index ` corresponding to the entry +1 is given by arg max 1\u2264j\u2264K X\u0303(i, j).", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "The above method is essentially the method described in Yu [23] and Yu and Shi [24], except that in these works (in which X,Z and Y are denoted by X\u2217, X\u0303\u2217, and X\u0303, respectively) the entries in X belong to {0, 1}; as described above, for row i, the index ` corresponding to the entry +1 is given by arg max 1\u2264j\u2264K X\u0303(i, j).", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "The method due to Yu and Shi (see Yu [23] and Yu and Shi [24]) to find X \u2208 K and Q = R\u039b with R \u2208 O(K) and \u039b diagonal invertible that minimize \u03c6(X,Q) = \u2016X \u2212 ZQ\u2016F is to alternate steps during which either Q is held fixed (step PODX) or X is held fixed (step PODR), except that Yu and Shi consider the special case where \u039b = I.", "startOffset": 37, "endOffset": 41}, {"referenceID": 20, "context": "The method due to Yu and Shi (see Yu [23] and Yu and Shi [24]) to find X \u2208 K and Q = R\u039b with R \u2208 O(K) and \u039b diagonal invertible that minimize \u03c6(X,Q) = \u2016X \u2212 ZQ\u2016F is to alternate steps during which either Q is held fixed (step PODX) or X is held fixed (step PODR), except that Yu and Shi consider the special case where \u039b = I.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "The method advocated by Yu [23] is to pick K rows of Z that are as orthogonal to each other as possible and to make a matrix R whose columns consist of these rows", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "The algorithm given in Yu [23] needs a small correction, because rows are not removed from Z when they are added to R, which may cause the same row to be added several times to R.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "Such graphs (with weights (\u22121, 0,+1)) were introduced as early as 1953 by Harary [12], to model social relations involving disliking, indifference, and liking.", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "A simple remedy is to use the absolute values of the weights in the degree matrix! This idea applied to signed graph with weights (\u22121, 0, 1) occurs in Hou [14].", "startOffset": 155, "endOffset": 159}, {"referenceID": 12, "context": "Kolluri, Shewchuk and O\u2019Brien [15] take the natural step of using absolute values of weights in the degree matrix in their original work on surface reconstruction from noisy point clouds.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "[16] appear to be the", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The first method consists in defining a notion of incidence matrix for a signed graph, and appears in Hou [14].", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "[16] deal with a different notion of cut, namely ratio cut (in which vol(A) is replaced by the size |A| of A), and only for two clusters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Correlation clustering was first introduced and studied for complete graphs by Bansal, Blum and Chawla [1].", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "Demaine and Immorlica [5] consider the same problem for arbitrary weighted graphs, and they give an O(log n)-approximation algorithm based on linear programming.", "startOffset": 22, "endOffset": 25}, {"referenceID": 13, "context": "[16], it is also possible to characterize for which signed graphs the Laplacian L is positive definite.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Such graphs are \u201ccousins\u201d of bipartite graphs and were introduced by Harary [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "The following proposition was first proved by Harary [12].", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "[16]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Hou [14] gives bounds on the smallest eigenvalue of an unbalanced graph.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "4 in Hou [14]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "[16]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16], if our goal is to draw a signed graph G = (V,W ) with m nodes, a natural way to interpret negative weights is to assume that the endpoints vi and vj of an edge with a negative weight should be placed far apart, which can be achieved if instead of assigning the point \u03c1(vj) \u2208 R to vj, we assign the point \u2212\u03c1(vj).", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": ", un) (for a proof, see Gallier [8]).", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "This section relies heavily on Gallot, Hulin, Lafontaine [9] and Lee [17], which contain thorough expositions and should be consulted for details.", "startOffset": 57, "endOffset": 60}, {"referenceID": 14, "context": "This section relies heavily on Gallot, Hulin, Lafontaine [9] and Lee [17], which contain thorough expositions and should be consulted for details.", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "The following proposition gives necessary and sufficient conditions for a discrete group to act freely and properly often found in the literature (for instance, O\u2019Neill [18], Berger and Gostiaux [3], and do Carmo [6], but beware that in this last reference Hausdorff separation is not required!).", "startOffset": 169, "endOffset": 173}, {"referenceID": 2, "context": "The following proposition gives necessary and sufficient conditions for a discrete group to act freely and properly often found in the literature (for instance, O\u2019Neill [18], Berger and Gostiaux [3], and do Carmo [6], but beware that in this last reference Hausdorff separation is not required!).", "startOffset": 195, "endOffset": 198}, {"referenceID": 14, "context": "\u201d However, as pointed out by Lee ([17], just before Proposition 9.", "startOffset": 34, "endOffset": 38}, {"referenceID": 7, "context": "For a proof, see Gallot, Hulin, Lafontaine [9] (Theorem 1.", "startOffset": 43, "endOffset": 46}, {"referenceID": 14, "context": "88) or Lee [17] (Theorem 9.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "For a complete proof see Gallot, Hulin, Lafontaine [9] (Proposition 2.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "4 can be found in Gallot, Hulin, Lafontaine [9] (Proposition 2.", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "Now, if (M, g) is a connected Riemannian manifold, recall that we define the distance d(p, q) between two points p, q \u2208M as d(p, q) = inf{L(\u03b3) | \u03b3 : [0, 1]\u2192M}, where \u03b3 is any piecewise C-curve from p to q, and", "startOffset": 149, "endOffset": 155}, {"referenceID": 7, "context": "The Hopf-Rinow Theorem (see Gallot, Hulin, Lafontaine [9], Theorem 2.", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "d(p, q) = inf{L(\u03b3) | \u03b3 : [0, 1]\u2192M is a geodesic}.", "startOffset": 25, "endOffset": 31}], "year": 2016, "abstractText": "This is a survey of the method of graph cuts and its applications to graph clustering of weighted unsigned and signed graphs. I provide a fairly thorough treatment of the method of normalized graph cuts, a deeply original method due to Shi and Malik, including complete proofs. I also cover briefly the method of ratio cuts, and show how it can be viewed as a special case of normalized cuts. I include the necessary background on graphs and graph Laplacians. I then explain in detail how the eigenvectors of the graph Laplacian can be used to draw a graph. This is an attractive application of graph Laplacians. The main thrust of this paper is the method of normalized cuts. I give a detailed account for K = 2 clusters, and also for K > 2 clusters, based on the work of Yu and Shi. I also show how both graph drawing and normalized cut K-clustering can be easily generalized to handle signed graphs, which are weighted graphs in which the weight matrix W may have negative coefficients. Intuitively, negative coefficients indicate distance or dissimilarity. The solution is to replace the degree matrix D by the matrix D in which absolute values of the weights are used, and to replace the Laplacian L = D \u2212W by the signed Laplacian L = D \u2212W . The signed Laplacian L is always positive semidefinite, and it may be positive definite (for unbalanced graphs, see Chapter 5). As far as I know, the generalization of K-way normalized clustering to signed graphs is new. Finally, I show how the method of ratio cuts, in which a cut is normalized by the size of the cluster rather than its volume, is just a special case of normalized cuts. All that needs to be done is to replace the normalized Laplacian Lsym by the unormalized Laplacian L. This is also true for signed graphs (where we replace Lsym by L). Three points that do not appear to have been clearly articulated before are elaborated: 1. The solutions of the main optimization problem should be viewed as tuples in the K-fold cartesian product of projective space RPN\u22121. 2. When K > 2, the solutions of the relaxed problem should be viewed as elements of the Grassmannian G(K,N). 3. Two possible Riemannian distances are available to compare the closeness of solutions: (a) The distance on (RPN\u22121)K . (b) The distance on the Grassmannian. I also clarify what should be the necessary and sufficient conditions for a matrix to represent a partition of the vertices of a graph to be clustered.", "creator": "LaTeX with hyperref package"}}}