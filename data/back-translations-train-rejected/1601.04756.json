{"id": "1601.04756", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2016", "title": "Improved Sampling Techniques for Learning an Imbalanced Data Set", "abstract": "This paper presents the performance of a classifier built using the stackingC algorithm in nine different data sets. Each data set is generated using a sampling technique applied on the original imbalanced data set. Five new sampling techniques are proposed in this paper (i.e., SMOTERandRep, Lax Random Oversampling, Lax Random Undersampling, Combined-Lax Random Oversampling Undersampling, and Combined-Lax Random Undersampling Oversampling) that were based on the three sampling techniques (i.e., Random Undersampling, Random Oversampling, and Synthetic Minority Oversampling Technique) usually used as solutions in imbalance learning. The metrics used to evaluate the classifier's performance were F-measure and G-mean. F-measure determines the performance of the classifier for every class, while G-mean measures the overall performance of the classifier. The results using F-measure showed that for the data without a sampling technique, the classifier's performance is good only for the majority class. It also showed that among the eight sampling techniques, RU and LRU have the worst performance while other techniques (i.e., RO, C-LRUO and C-LROU) performed well only on some classes. The best performing techniques in all data sets were SMOTE, SMOTERandRep, and LRO having the lowest F-measure values between 0.5 and 0.65. The results using G-mean showed that the oversampling technique that attained the highest G-mean value is LRO (0.86), next is C-LROU (0.85), then SMOTE (0.84) and finally is SMOTERandRep (0.83). Combining the result of the two metrics (F-measure and G-mean), only the three sampling techniques are considered as good performing (i.e., LRO, SMOTE, and SMOTERandRep).", "histories": [["v1", "Mon, 18 Jan 2016 23:31:12 GMT  (429kb)", "http://arxiv.org/abs/1601.04756v1", "7 pages, 10 figures, 16th Philippine Computing Science Congress (PCSC 2016)"]], "COMMENTS": "7 pages, 10 figures, 16th Philippine Computing Science Congress (PCSC 2016)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["maureen lyndel c lauron", "jaderick p pabico"], "accepted": false, "id": "1601.04756"}, "pdf": {"name": "1601.04756.pdf", "metadata": {"source": "CRF", "title": "Improved Sampling Techniques for Learning an Imbalanced Data Set", "authors": ["Maureen Lyndel C. Lauron", "Jaderick P. Pabico"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 1.04 756v 1 [cs.L G] 18 Jan 2016"}, {"heading": "Categories and Subject Descriptors", "text": "H.2.8 [Database Management]: Database Application -"}, {"heading": "Data Mining", "text": "Permission to make digital or printed copies of all or part of this work for personal use or teaching is granted at no charge, provided that copies are not made or distributed for profit or commercial gain, and that copies bear this notice and the full quote on the first page. Any other reproduction, republication, server publication or distribution to lists requires prior express permission and / or a fee. PCSC 2016 16-18 March 2016, Puerto Princesa City, Palawan. Copyright 2016 ACM x-xxxxx-xx... $15.00."}, {"heading": "General Terms", "text": "Imbalance Learning, Scanning Techniques"}, {"heading": "Keywords", "text": "Classification, Ensemble learning, StapleC"}, {"heading": "1. INTRODUCTION", "text": "It is natural that the data set has a distorted distribution because there are far more advanced students than students who go away. In the case of this study, there are twelve minority classes (see Figure 1). Scanning techniques have been used as solutions to multi-class incapacity; the problem with the extreme imbalances is minority incapacity (see Figure 1)."}, {"heading": "2. SAMPLING TECHNIQUES", "text": "Sampling techniques for solving the imbalance learning problem simply modify an unbalanced dataset so that it has a reasonably balanced distribution. Some common sampling techniques used in imbalance learning are random undersampling (RU), random oversampling (RO), and synthetic oversampling technique (SMOTE) [4]."}, {"heading": "2.1 Sampling Techniques in the Literature", "text": "2.1.1 Random subsampling The mechanics of the RU, as its name suggests, randomly moves instances from all classes except the class with the least number of instances (most minority class) until the number of instances is the same for all classes.2.1.2 Random oversampling While the subsampling removes data from the original data, the RO adds data by randomly replicating the instances of the twelve minority classes until the number of instances is the same for all classes.2.1.3 Synthetic oversampling techniques for minorities The SMOTE, like the RO, also adds new data, but in a different way [1]. To create a synthetic instance xnew, a random instance x \u00b2 is selected based on similarities between existing instances of the minority class. This technique uses the k-next neighbor algorithm to find the k-next neighbors of an instance xi in the minority class."}, {"heading": "2.2 Proposed Sampling Techniques", "text": "The five new sampling techniques proposed in this work are based on the above sampling techniques. 2.2.1 SMOTERandRep The first technique is called SMOTERandRep, an abbreviation for SMOTE Random Replacement. This technique is based on the idea of SMOTE, because it also tricks the minority class and creates synthetic instances. Faced with an instance xi of a minority class U and its k-next neighbor U, it creates a new instance by copying the characteristics of xi and randomly logging 2 attribute values with the values from the randomly selected k-next neighbor U, where a total number of attributes and whose k-value U is also log2 a.2.2.2 Lax Random Undersampling The second technique is called Lax Random Undersampling (LRRU) an attribute technique with the values from the randomly selected k-next neighbor U, where a RRRRRRU is the total number of attributes and the RRRRU is also called the total number of attributes from RRR2, and the second underling is 2.2, and the second technique is called the second and underling is the second technique."}, {"heading": "3. METHODOLOGY", "text": "The student data used in this study for the UPLB classification problem was collected by the Office of the University Registry (OUR) of the UPLB and by five different universities of the University. The data collected identified 13 possible classification of an instance of a student and these are different (SHC), transfer to another UP campus (TUP), transfer to another school (TAS), scholastic delinquency warning (SDW), scholastic delinquency sample (SDD), scholastic delinquency dismissal (SDD), scholastic delinquency permanent disqualification (SPQ), reading of disqualified and dismissed students (RAM), absence of absence (LOA), honorable dismissal from discretion (HDM), warning and sampling of sampling (SMP) and sampling (SDU)."}, {"heading": "4. RESULTS", "text": "As expected, the performance of the classifier using the original data is good for the majority class (COS) alone, as shown in Figure 9. Of the eight sampling methods, RU and LRU showed the worst performance for all classes, while other techniques (i.e. RO, C-LRUO and C-LROU) performed well only in some classes. Of the eight sampling methods for all classes, the best performing were SMOTE, SMOTERandRep and LRO with the lowest values of the F measurement class (SDW) of 0.65, 0.52 and 0.61, respectively. The result implies that sampling methods actually improve the performance of the classifier and that oversampling techniques are preferable to subsample methods. In terms of the overall performance of the classifier, the lowest G averages were those of the original data, LRU, and RU, as shown in Figure 10. The top sampling methods with the highest G averages were LO (0.86-mean), the lowest (SMRU) and lowest (SMRU)."}, {"heading": "5. CONCLUSION", "text": "The results showed that sampling methods actually improved the performance of the classifier for all minority classes. Between the two techniques (RO and RU), the results also showed that RO is preferable, for obvious reasons, since RU removes majority class instances, as the classifier is generated, important concepts about the majority class were missing, leading to better performance in some minority classes, but jeopardizing performance in the majority class. The result of this study also showed that LRO, SMOTE, SMOTER and Rep were the most powerful techniques. To further verify this result, it is proposed that the next iterations of this study use larger, unbalanced multi-class data and apply the validation method of holding and waiting instead of layered ten-fold cross-validation."}], "references": [{"title": "Smote: synthetic minority over-sampling technique", "author": ["Nitesh V. Chawla", "Kevin W. Bowyer", "Lawrence O. Hall", "W. Philip Kegelmeyer"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "From data mining to knowledge discovery in databases", "author": ["Usama Fayyad", "Gregory Piatetsky-Shapiro", "Padhraic Smyth"], "venue": "AI magazine,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "The weka data mining software: An update", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": "SIGKDD Explor. Newsl.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Learning from imbalanced data", "author": ["Haibo He", "Edwardo Garcia"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "This problem has drawn enough attention from both industry and academia, thus many state-of-the-art solutions now exists such as sampling techniques, cost sensitive methods, and kernel-based methods [4].", "startOffset": 199, "endOffset": 202}, {"referenceID": 3, "context": "Meanwhile, the two techniques can be used only with a specific learning algorithm, for example, kernel-based methods is implemented with support vector machines and some cost sensitive methods with adaptive boosting [4].", "startOffset": 216, "endOffset": 219}, {"referenceID": 1, "context": "There are many machine learning algorithms developed already for the classification task but each algorithm typically suits some classification problems better than others; and this is why there is no universal data mining method for all problem types [2].", "startOffset": 252, "endOffset": 255}, {"referenceID": 3, "context": "Some sampling techniques commonly used in imbalance learning are random undersampling (RU), random oversampling (RO), and synthetic minority oversampling technique (SMOTE) [4].", "startOffset": 172, "endOffset": 175}, {"referenceID": 0, "context": "The SMOTE just like RO also adds new data but in a different manner [1].", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "random instance x \u2032 is chosen from the k-nearest neighbors, then the corresponding feature vector difference is multiplied with a random number between [0,1] and finally this vector is added to xi", "startOffset": 152, "endOffset": 157}, {"referenceID": 2, "context": "Waikato Environment for Knowledge Analysis (WEKA) is an open source (available under GNU General Public License) and cross-platform (written in Java) data mining software for data mining tasks [3].", "startOffset": 193, "endOffset": 196}], "year": 2016, "abstractText": "This paper presents the performance of a classifier built using the stackingC algorithm in nine different data sets. Each data set is generated using a sampling technique applied on the original imbalanced data set. Five new sampling techniques are proposed in this paper (i.e., SMOTERandRep, Lax Random Oversampling, Lax Random Undersampling, Combined-Lax Random Oversampling Undersampling, and Combined-Lax Random Undersampling Oversampling) that were based on the three sampling techniques (i.e., Random Undersampling, Random Oversampling, and Synthetic Minority Oversampling Technique) usually used as solutions in imbalance learning. The metrics used to evaluate the classifier\u2019s performance were F-measure and G-mean. Fmeasure determines the performance of the classifier for every class, while G-mean measures the overall performance of the classifier. The results using F-measure showed that for the data without a sampling technique, the classifier\u2019s performance is good only for the majority class. It also showed that among the eight sampling techniques, RU and LRU have the worst performance while other techniques (i.e., RO, C-LRUO and C-LROU) performed well only on some classes. The best performing techniques in all data sets were SMOTE, SMOTERandRep, and LRO having the lowest Fmeasure values between 0.5 and 0.65. The results using Gmean showed that the oversampling technique that attained the highest G-mean value is LRO (0.86), next is C-LROU (0.85), then SMOTE (0.84) and finally is SMOTERandRep (0.83). Combining the result of the two metrics (F-measure and G-mean), only the three sampling techniques are considered as good performing (i.e., LRO, SMOTE, and SMOTERandRep)", "creator": "LaTeX with hyperref package"}}}