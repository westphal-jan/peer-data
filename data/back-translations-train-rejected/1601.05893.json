{"id": "1601.05893", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2016", "title": "GeoTextTagger: High-Precision Location Tagging of Textual Documents using a Natural Language Processing Approach", "abstract": "Location tagging, also known as geotagging or geolocation, is the process of assigning geographical coordinates to input data. In this paper we present an algorithm for location tagging of textual documents. Our approach makes use of previous work in natural language processing by using a state-of-the-art part-of-speech tagger and named entity recognizer to find blocks of text which may refer to locations. A knowledge base (OpenStreatMap) is then used to find a list of possible locations for each block. Finally, one location is chosen for each block by assigning distance-based scores to each location and repeatedly selecting the location and block with the best score. We tested our geolocation algorithm with Wikipedia articles about topics with a well-defined geographical location that are geotagged by the articles' authors, where classification approaches have achieved median errors as low as 11 km, with attainable accuracy limited by the class size. Our approach achieved a 10th percentile error of 490 metres and median error of 54 kilometres on the Wikipedia dataset we used. When considering the five location tags with the greatest scores, 50% of articles were assigned at least one tag within 8.5 kilometres of the article's author-assigned true location. We also tested our approach on Twitter messages that are tagged with the location from which the message was sent. Twitter texts are challenging because they are short and unstructured and often do not contain words referring to the location they were sent from, but we obtain potentially useful results. We explain how we use the Spark framework for data analytics to collect and process our test data. In general, classification-based approaches for location tagging may be reaching their upper accuracy limit, but our precision-focused approach has high accuracy for some texts and shows significant potential for improvement overall.", "histories": [["v1", "Fri, 22 Jan 2016 07:09:54 GMT  (498kb,D)", "http://arxiv.org/abs/1601.05893v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.DB cs.IR", "authors": ["shawn brunsting", "hans de sterck", "remco dolman", "teun van sprundel"], "accepted": false, "id": "1601.05893"}, "pdf": {"name": "1601.05893.pdf", "metadata": {"source": "CRF", "title": "GeoTextTagger: High-Precision Location Tagging of Textual Documents using a Natural Language Processing Approach", "authors": ["Shawn Brunsting", "Hans De Sterck", "Remco Dolman", "Teun van Sprundel"], "emails": ["*hans.desterck@monash.edu"], "sections": [{"heading": null, "text": "In this paper, we present an algorithm for positioning text documents that can point to locations. Our approach uses previous work in natural language processing by recognizing a state-of-the-art part of the word tagger and named entity to find blocks of text. An OpenStreatMap is then used to find a list of possible locations for each of these blocks of text. Finally, a location is selected for each block of text by assigning distance-based ratings to each location and repeatedly selecting the location and block of text with the highest score. We have tested our geolocation algorithm with Wikipedia articles on topics with a well-defined geographical location that are geotagged by the authors, with the classification approaches reaching average errors as low as 11 km."}, {"heading": "1 Introduction", "text": "This paper examines the problem of extracting location data from text. The aim is to assign high-precision geographic coordinates to locations mentioned in texts 1 / 35ar Xiv: 160 1.05 893v 1 [cs.A I] 2 2Ja n20 16 from various sources. In this section, some background information and previous work in the field of location marking are discussed. In the literature, this is also referred to as geotagging or geolocalization. Section 2 describes in detail the geolocalization algorithm we have developed to address this problem. We test this algorithm using geotagged Wikipedia articles in Section 3 and geotagged Twitter messages in Section 4. Finally, we summarize the results and discuss some future work in Section 5. Annex A lists the external software used in the implementation of our project. Annex B describes how the Wikipedia test data was obtained for Section 3, and Annex C describes how the Twitter test data was compiled for Section 4."}, {"heading": "1.1 Previous Work using Classification Approaches", "text": "Most studies in positioning tagging formulate it as a classification problem and use various machine learning approaches to solve it. Classification problems begin with a defined set of classes. In positioning tagging, these classes can take many forms, including cities, countries or territories within a range of latitude and longitude coordinates. The aim of the classification problems is to assign a class (or ranking of the most likely classes) to each text. Note that this problem is obviously most relevant to texts that actually speak about a specific location or set of locations, but you can try to use the resulting algorithms to use any arbitrary amount of text to geotag. Several essays in positioning literature [1, 2] use test records from Wikipedia articles on topics with a well-defined geographical location, and for which the Wikipedia authors have added a geographical tag to each article. For these geographical coordinated Wikipedia articles, the geographical articles, the articles."}, {"heading": "1.2 Drawbacks of Classification Approaches", "text": "It was therefore decided that formulating our problem as a classification problem would not be feasible because our goal would be to achieve a high level of precision. For example, if a text mentions the CN Tower in the city of Toronto, then we want to return the coordinates of that building, not the coordinates for Toronto. Formulating this as a classification problem with this level of precision would ultimately require defining a class for every designated location in the world. Furthermore, in order to apply these machine learning approaches, we would need to have a large training set, ideally with multiple sample texts for each class. Obtaining this training data would be difficult, and even if the data were available, it would probably not be mathematically possible to train such a model. Generally, we did not want to apply an approach that relies heavily on training data, as we want our location tagger to be as general as possible. A tagger trained with Wikipedia articles could perform worse if other types of text, such as tweets or news articles."}, {"heading": "1.3 Natural Language Processing", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "2 Methods", "text": "For example, in a text that says, \"Bob drove from Waterloo to Toronto,\" we want to find the names Waterloo and Toronto and determine which places are meant by these words. Does Waterloo mean a city in Ontario, Iowa, Belgium or elsewhere? Mentioning Toronto, which may refer to another city in Ontario, suggests that the correct answer is Waterloo, Ontario. Section 2.3 describes how names like Waterloo and Toronto are extracted from the text using a section of text formally described in this section. Section 2.1 gives a comprehensive overview of the algorithm. Section 2.2 defines the terminology we will use in the rest of the paper. Section 2.3 describes how names like Waterloo and Toronto are extracted from the text by using parts of the word tagging4 / 35 and being called entity recognition. Section 2.4 describes how we find out that geographic names like Waterloo can refer to multiple places (e.g. Ontario, Ontario, Iowa and a data base)."}, {"heading": "2.1 Overview", "text": "Algorithm 1 provides a general overview of the steps in the geolocation algorithm that we propose in this paper. Algorithm 1 Simplified overview of the geolocation algorithm 1: Extract potential location references from the text. This is described in detail in Section 2.3. 2: Search for any potential location reference in a knowledge base. This results in a list of result types that are suitable for the reference. This is described in Section 2.4. 3: Determine for each potential location reference to which the knowledge base is most likely to relate, and resolve conflicting interpretations of word sequences. This is referred to as disambiguity and is described in Section 2.5. Section 2.2 in detail to define some terminologies that allow us to write algorithm 1 more precisely. Section 2.6 summarizes Section 2 to give a more detailed version of Algorithm 1."}, {"heading": "2.2 Terminology", "text": "Before proceeding with the description of our geolocation algorithm, we need to define our terms precisely. \"Consider, for example, the following short text:\" Let's go shopping in one of the three cities. \"A term is a word or a sequence of adjacent words in the text. If a sequence of words occurs several times in the text, each event is treated as a different term. Our algorithm will first take into account each sequence of words that occur in the text, i.e. it will first take into account all terms (sequences of words) of length one, two, three, etc."}, {"heading": "2.3 Location Extraction", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2.4 Searching the Knowledge Base", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to fulfil them."}, {"heading": "2.5 Disambiguation", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2.6 Full geolocation algorithm", "text": "Now that each step in algorithm 2 has been explained in more detail, we can write a more comprehensive summary of the full geo-location algorithm. This is given by algorithm 5. Note that filtering in steps 4-18 to obtain the relevant term set T can be modified and possibly made more efficient, depending on the type of document you want to localize. However, the filtering steps in algorithm 5 are organized in such a way that the geo-location algorithm can be applied to a wide class of texts, including Wikipedia articles, Twitter messages, Kijiji classifieds, etc. At the end of algorithm 5, we have a collection of terms that are cited in the text along with a single position for each. Scores can be recalculated for these results using the same scoring feature that was used for the disambiguity. (Note that all weights will be 1 at this point) You can then sort the text in order for the places mentioned in the ranking to be listed."}, {"heading": "3 Results for Wikipedia Test Data", "text": "In this section we describe the performance of our geolocation algorithm with each of the eight scoring functions and the two disambiguation algorithms discussed in Section 2.5 for a Wikipedia dataset. 21 / 35"}, {"heading": "3.1 Test Data", "text": "We tested our geolocation algorithm with a subset of English geotagged Wikipedia articles, i.e. articles on topics with a clearly defined geographical location, in which the authors provided latitude and longitude coordinates of the primary \"true location\" of the article. These data required a considerable amount of pre-processing, as described in Appendix B. Our full dataset contained 920,176 geotagged Wikipedia articles, but the tests in this section used only a sample of 5,976 articles. This subset was used due to time and resource constraints (in particular, the one-second request-per-second limit for the free nominatim servers discussed in Section 2.4), and the subset was large enough to thoroughly test the capabilities of our geolocation algorithm as specified below."}, {"heading": "3.2 Filtering by NER and POS Tags", "text": "Section 2.3 describes how we filter our set of initial terms to arrive at the term Set T, which we do not clearly define. See also Algorithm 5 (Steps 1-24) for a summary of the filter steps. If the text contains NER LOCATION tags, then we only use terms that have at least one word with that tag (Steps 12-13). If there are no LOCATION tags, then we only hold terms that occur after POS prepositions (Steps 15-17). If there are no terms that follow prepositions, then we maintain the initial list of terms that is based on POS nouns (Steps 4-11). We have executed our geolocation algorithm on 5,976 articles. Of these articles, we have rejected 500 directly because the various text parsers used by the NER and POS taggers have produced inconsistent tools.3 Table 9 shows the tests, as we have often pulled the 5.4s from the respective items in our positions."}, {"heading": "3.3 Comparison of Disambiguation Algorithms", "text": "\"The inflation rate has doubled in the last ten years.\" \"The inflation rate has tripled in the last ten years.\" \"The inflation rate has tripled in the last ten years.\" \"The inflation rate has tripled in the last ten years.\" \"The inflation rate has tripled in the last ten years.\" \"The inflation rate has tripled in the last ten years.\" \"The inflation rate has tripled in the last ten years.\" \"The inflation rate has doubled in the last ten years.\" \"The inflation rate has tripled in the last ten years.\" \"The inflation rate has tripled in the last ten.\" The inflation rate has tripled in the last ten. \"The inflation rate has tripled in the last ten.\" The inflation rate has tripled in the last ten years. \""}, {"heading": "3.4 Further Analysis with the Winning Algorithm", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "3.5 Analysis of Top 5 Results", "text": "In previous sections, we used the distance between the true position and the result with the largest value as a measurement of the largest errors. This may not be the best approach, as our algorithm provides more information in a list of several locations mentioned in the text. So far, we have used only one of these locations to calculate the error. However, if a text describes a river and mentions the cities that lie along that river, then the geolocation algorithm would produce locations for the cities as well as a location for the river. The \"true place\" for the article would be a single point somewhere along the river. If one of the cities along the river has the highest score in our list of results, that city will be far away from the point assigned to the author of the article, then this can give a very large error value, although our output can be considered accurate. It might be more meaningful to consider a case like this than a group of places returned by the geolocation algorithm."}, {"heading": "4 Results for Twitter Test Data", "text": "We look at geotagged tweets, i.e. tweets that contain in their metadata the geographical location from which they were sent. We will try to locate those tweets (or concatenations of tweets sent by the same user) solely on the basis of the text in the tweets and judge which fraction of the tweets (or concatenations) is located by our algorithm to a location near the place from which the tweets were sent (which we consider to be the \"true location\" for evaluating our geolocalization algorithm for tweets). Locating tweets is inherently challenging: tweets are short and unstructured and less likely to contain NER-LOCATIONs. This means that our geolocalization algorithm will rely more often on POS prepositions and nouns (see Algorithm 5). In addition, unlike Wikipedia articles about well-defined geographical units that were specifically located in the places where the authors were most likely to have actually located (see the most of the results)."}, {"heading": "4.1 Test Data", "text": "We have compiled a Twitter dataset containing almost all of the geotagged tweets from Canada, the US, the Netherlands and Australia for the period of February, March and April 2015 \u2022 The dataset contains tweets from 3,764,507 unique users. The dataset was collected using the Twitter API via the Spark data analysis framework as explained in Appendix C. It is interesting to first consider what fraction of the geotagged tweets contain words or word sequences that can be interpreted (using a knowledge base) as places within a reasonable distance from the place from which the tweet was sent. Specifically, we looked at a 10% selection of tweets from February 2015 that we collected consisting of 3,466,922 geotagged tweets from our larger dataset. For each tweet, we considered all word sequences of up to 5 words (no POS or NER was used; we only considered any sequence of 5 or fewer consecutive words)."}, {"heading": "4.2 Results", "text": "Since we cannot expect high retrieval accuracy for individual tweets, we will first consider concatenations of tweets sent by the same user, but approximately 255.5 km long, which may have more useful NER LOCATIONs or POS nouns / prepositions to base the geolocation. In our initial tests, we combined the text of all user tweets into one text for each user. For each user, we set the latitude and longitude of the user to represent the meaning 29 / 35 latitude and longitude of all user tweets in the dataset. We took this into account for a sample of 882 users and found the following results: \u2022 10th percentiles: 603 km \u2022 25th percentiles: 2,222-2,247 km (Note: we report the range for the 16 different methods) \u2022 50th percentiles: 6,646-6,665 kmNote that this is, as expected, poorly compared to Wikipedia articles."}, {"heading": "5 Discussion", "text": "This text was written by the authors themselves and written by the readers themselves. It is able to deal with the situation in which it finds itself."}, {"heading": "A External Software", "text": "The following software was used in the implementation of our project. \u2022 Apache Spark version 1.4.0. \u2022 Based on the work of [17]. Available at http: / / spark.apache.org /. \u2022 Nominatim. It was queried using the API described at http: / / wiki.openstreetmap. org / wiki / Nominatim. \u2022 PHP Stanford NLP. October 18, 2014 version. Written by Anthony Gentile. Available at https: / / github.com / agentile / PHP-Stanford-NLP. Used as a PHPwrapper to access the Stanford POS tagger and NER. \u2022 Stanford Log-linear Part-Of-Speech Tagger version 3.5.2 (written April 20, 2015)."}, {"heading": "B Wikipedia Data", "text": "At the time of writing, data dumps of English Wikipedia could be obtained from https: / / dumps.wikimedia.org / enwiki /. This paper uses articles from a dump of English Wikipedia on June 2, 2015 (filename enwiki-20150602-pages-articles.xml.bz2). Location tags were also obtained from the June 2, 2015 dump, and are available as a separate download from the same website (filename enwiki-20150602-geo tags.sql.gz). Our full dataset contained 920,176 geotagged Wikipedia articles, and our tests used a sample of 5,976 articles. The raw dump file was processed using WikiExtractor (https: / github.com / attardi / wikiextractor)."}, {"heading": "C Twitter Data", "text": "Twitter provides an API that allows applications to stream a fraction of the publicly available tweets (https: / / dev.twitter.com / streaming), and each application can receive up to about 1% of the public tweets, although the number of tweets received can be customized using the API. Apache Spark's streaming library (http: / / spark.apache. org / streaming /) includes a wrapper for the Twitter API that allows applications to process those tweets using Spark. A Spark streaming application was written to receive a tweet and store it in parquet files for further processing, and this program was run for several months to obtain the dataset that Section 4.Note that the Twitter API applications can specifically request geotagged tweets (i.e. tweets that contain the location from which the tweet was sent as part of the tweets), but Sparks Wrapper for Twitter implements this query for this tweeting feature (the paper was modified for this Spark request)."}, {"heading": "D Code and Data", "text": "The code for our geolocation algorithm can be found at https: / / github.com / spotzi / Geotagger.The Spark programs we use to pre-process Wikipedia data and collect and pre-process Twitter data can be found at https: / / github.com / sjbrunst / GeoTextTagger data. The Wikipedia and Twitter data we used for our tests can also be found at https: / / github.com / sjbrunst / GeoTextTagger data."}], "references": [{"title": "Simple Supervised Document Geolocation with Geodesic Grids. In: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL): Human Language Technologies (HLT)", "author": ["BP Wing", "J. Baldridge"], "venue": "Stroudsburg, PA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Text-based Twitter User Geolocation Prediction", "author": ["B Han", "P Cook", "T. Baldwin"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-speech Tagger", "author": ["Toutanova K", "Manning CD"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Feature-rich Part-of-speech Tagging with a Cyclic Dependency Network", "author": ["K Toutanova", "D Klein", "CD Manning", "Y. Singer"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLT-NAACL). Stroudsburg, PA,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling", "author": ["JR Finkel", "T Grenager", "C. Manning"], "venue": "Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Named entity extraction and disambiguation for informal text: the missing link", "author": ["Habib Morgan MB"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Learning to link with wikipedia", "author": ["Milne D", "Witten IH"], "venue": "Proceedings of the 17th ACM conference on Information and knowledge management", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Collective annotation of Wikipedia entities in web text", "author": ["S Kulkarni", "A Singh", "G Ramakrishnan", "S. Chakrabarti"], "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Robust disambiguation of named entities in text", "author": ["J Hoffart", "MA Yosef", "I Bordino", "H F\u00fcrstenau", "M Pinkal", "M Spaniol"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics;", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "AGDISTISgraph-based disambiguation of named entities using linked data. In: The Semantic Web\u2013ISWC", "author": ["R Usbeck", "ACN Ngomo", "M R\u00f6der", "D Gerber", "SA Coelho", "S Auer"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Entity linking meets word sense disambiguation: a unified approach", "author": ["A Moro", "A Raganato", "R. Navigli"], "venue": "Transactions of the Association for Computational Linguistics", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "An online tool for accurate disambiguation of named entities in text and tables", "author": ["MA Yosef", "J Hoffart", "I Bordino", "M Spaniol", "Aida Weikum G"], "venue": "Proceedings of the VLDB Endowment", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Openstreetmap: User-generated street maps", "author": ["M Haklay", "P. Weber"], "venue": "Pervasive Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Part-Of-Speech Tagging Guidelines for the Penn Treebank Project (3rd revision)", "author": ["B. Santorini"], "venue": "Philadelphia, PA, USA: Department of Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "Twitter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data", "author": ["L Derczynski", "A Ritter", "S Clark", "K. Bontcheva"], "venue": "Recent Advances in Natural Language Processing;", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Resilient Distributed Datasets: A Fault-tolerant Abstraction for In-memory Cluster Computing", "author": ["M Zaharia", "M Chowdhury", "T Das", "A Dave", "J Ma", "M McCauley"], "venue": "Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation (NSDI). Berkeley, CA,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "For example, several papers in the location tagging literature [1, 2] use test data sets of Wikipedia articles about topics with a well-defined geographical location, and for which the Wikipedia authors have added a geographical location tag to each article.", "startOffset": 63, "endOffset": 69}, {"referenceID": 0, "context": "Wing and Baldridge created classes using simple geodesic grids of varying sizes [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "This is an improvement over the previous work [1].", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "focused on location tagging of Twitter messages [3].", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "[4, 5].", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[4, 5].", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "The other type of model is known as the bidirectional model [5].", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Habib [7] explored both the extraction and disambiguation steps for named entity recognition, along with the link between these steps.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "Named entities which refer to locations are called toponyms, and a major portion of Habib\u2019s [7] work discusses the extraction and disambiguation of toponyms in semi-formal text.", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": ", [8\u201312] and references therein.", "startOffset": 2, "endOffset": 8}, {"referenceID": 7, "context": ", [8\u201312] and references therein.", "startOffset": 2, "endOffset": 8}, {"referenceID": 8, "context": ", [8\u201312] and references therein.", "startOffset": 2, "endOffset": 8}, {"referenceID": 9, "context": ", [8\u201312] and references therein.", "startOffset": 2, "endOffset": 8}, {"referenceID": 10, "context": ", [8\u201312] and references therein.", "startOffset": 2, "endOffset": 8}, {"referenceID": 8, "context": "For example, the AIDA system [10,13] is a general framework for collective disambiguation exploiting the prior probability of an entity being mentioned, similarity between the context of the mention and its candidates, and the coherence among candidate entities for all mentions together.", "startOffset": 29, "endOffset": 36}, {"referenceID": 11, "context": "For example, the AIDA system [10,13] is a general framework for collective disambiguation exploiting the prior probability of an entity being mentioned, similarity between the context of the mention and its candidates, and the coherence among candidate entities for all mentions together.", "startOffset": 29, "endOffset": 36}, {"referenceID": 12, "context": "Our approach, which uses the OpenStreetMap knowledge base [14], is specific to geolocation, and provides a high-precision and versatile method for geolocation of texts.", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "The full list of possible tags is given in [15], but our algorithm only looks for a subset of tags which are relevant to our problem.", "startOffset": 43, "endOffset": 47}, {"referenceID": 12, "context": "OpenStreetMap is used as the knowledge base for this paper [14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "1: Tag the input text using the POS and NER taggers from [4\u20136].", "startOffset": 57, "endOffset": 62}, {"referenceID": 3, "context": "1: Tag the input text using the POS and NER taggers from [4\u20136].", "startOffset": 57, "endOffset": 62}, {"referenceID": 4, "context": "1: Tag the input text using the POS and NER taggers from [4\u20136].", "startOffset": 57, "endOffset": 62}, {"referenceID": 5, "context": "For example, Habib [7] demonstrated that there is a feedback loop between the extraction step and the disambiguation step in named entity recognition.", "startOffset": 19, "endOffset": 22}, {"referenceID": 14, "context": "[16] when geolocating tweets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "distance-based scoring, to more general named entity disambiguation frameworks such as AIDA [10,13].", "startOffset": 92, "endOffset": 99}, {"referenceID": 11, "context": "distance-based scoring, to more general named entity disambiguation frameworks such as AIDA [10,13].", "startOffset": 92, "endOffset": 99}, {"referenceID": 15, "context": "Based on the work of [17].", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "Based on the work of [4] and [5].", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "Based on the work of [4] and [5].", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "Based on the work of [6].", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": "4 It is worth noting that our set of articles is different from the one created by Wing and Baldridge [1], which was also used by Roller et al.", "startOffset": 102, "endOffset": 105}, {"referenceID": 15, "context": "These two data sets were joined using Apache Spark, which is a scalable cluster computing system [17].", "startOffset": 97, "endOffset": 101}], "year": 2016, "abstractText": "Location tagging, also known as geotagging or geolocation, is the process of assigning geographical coordinates to input data. In this paper we present an algorithm for location tagging of textual documents. Our approach makes use of previous work in natural language processing by using a state-of-the-art part-of-speech tagger and named entity recognizer to find blocks of text which may refer to locations. A knowledge base (OpenStreatMap) is then used to find a list of possible locations for each of these blocks of text. Finally, one location is chosen for each block of text by assigning distance-based scores to each location and repeatedly selecting the location and block of text with the best score. We tested our geolocation algorithm with Wikipedia articles about topics with a well-defined geographical location that are geotagged by the articles\u2019 authors, where classification approaches have achieved median errors as low as 11 km. However, the maximum accuracy of these approaches is limited by the class size, so future work may not yield significant improvement. Our algorithm tags a location to each block of text that was identified as a possible location reference, meaning a text is typically assigned multiple tags. When we considered only the tag with the highest distancebased score, we achieved a 10th percentile error of 490 metres and median error of 54 kilometres on the Wikipedia dataset we used. When we considered the five location tags with the greatest scores, we found that 50% of articles were assigned at least one tag within 8.5 kilometres of the article\u2019s author-assigned true location. We also tested our approach on a set of Twitter messages that are tagged with the location from which the message was sent. This dataset is more challenging than the geotagged Wikipedia articles, because Twitter texts are shorter, tend to contain unstructured text, and may not contain information about the location from where the message was sent in the first place. Nevertheless, we make some interesting observations about potential use of our geolocation algorithm for this type of input. We explain how we use the Spark framework for data analytics to collect and process our test data. In general, classification-based approaches for location tagging may be reaching their upper limit for accuracy, but our precision-focused approach has high accuracy for some texts and shows significant potential for improvement overall.", "creator": "LaTeX with hyperref package"}}}