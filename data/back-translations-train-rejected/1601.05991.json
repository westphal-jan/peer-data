{"id": "1601.05991", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2016", "title": "Speech vocoding for laboratory phonology", "abstract": "Using phonological speech vocoding, we propose a platform for exploring relations between phonology and speech processing, and in broader terms, for exploring relations between the abstract and physical structures of a speech signal. Our goal is to make a step towards bridging phonology and speech processing and to contribute to the program of Laboratory Phonology. We show three application examples for laboratory phonology: compositional phonological speech modelling, a comparison of phonological systems and an experimental phonological parametric text-to-speech (TTS) system. The featural representations of the following three phonological systems are considered in this work: (i) Government Phonology (GP), (ii) the Sound Pattern of English (SPE), and (iii) the extended SPE (eSPE). Comparing GP- and eSPE-based vocoded speech, we conclude that the latter achieves slightly better results than the former. However, GP - the most compact phonological speech representation - performs comparably to the systems with a higher number of phonological features. The parametric TTS based on phonological speech representation, and trained from an unlabelled audiobook in an unsupervised manner, achieves intelligibility of 85% of the state-of-the-art parametric speech synthesis. We envision that the presented approach paves the way for researchers in both fields to form meaningful hypotheses that are explicitly testable using the concepts developed and exemplified in this paper. On the one hand, laboratory phonologists might test the applied concepts of their theoretical models, and on the other hand, the speech processing community may utilize the concepts developed for the theoretical phonological models for improvements of the current state-of-the-art applications.", "histories": [["v1", "Fri, 22 Jan 2016 13:22:10 GMT  (2015kb,D)", "https://arxiv.org/abs/1601.05991v1", null], ["v2", "Mon, 18 Apr 2016 12:06:21 GMT  (2016kb,D)", "http://arxiv.org/abs/1601.05991v2", null], ["v3", "Thu, 15 Sep 2016 08:26:38 GMT  (2087kb,D)", "http://arxiv.org/abs/1601.05991v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["milos cernak", "stefan benus", "alexandros lazaridis"], "accepted": false, "id": "1601.05991"}, "pdf": {"name": "1601.05991.pdf", "metadata": {"source": "CRF", "title": "Speech vocoding for laboratory phonology", "authors": ["Milos Cernaka", "Stefan Benus", "Alexandros Lazaridis"], "emails": ["milos.cernak@idiap.ch"], "sections": [{"heading": null, "text": "Using phonological speech vocabulary, we propose a platform for the exploration of the relationship between phonology and speech processing, and more broadly for the exploration of the relationship between the abstract and physical structures of a speech signal. Our goal is to take a step towards bridging phonology and speech processing and to contribute to the Laboratory Phonology Program. We present three examples of laboratory phonology applications: (i) Compositional phonological speech modeling, (ii) the comparison of phonological systems and an experimental phonological text-to-speech system (TTS). Feature representations of the following three phonological systems are taken into account in this work: (i) government phonology (GP), (ii) the sound pattern of English (SPE) and (iii) the extended SPE (eSPE)."}, {"heading": "1. Introduction", "text": "In this sense, the articulating activity and the resulting acoustic speech signal are constantly varying. On the other hand, this continuous signal must be perceived as contrasting at the same time. Traditionally, these two aspects have been studied within phonetics and phonology. Following significant successes of this dichotomatic approach, for example in speech synthesis and recognition, we have seen many advances in the understanding and formal modelling of the relationship between these two aspects in recent decades, e.g. the phonology program (Pierrehumbert et al.) or the renewed interest in the approaches to synthesis (Hirst, 2011; Poeppel, 2010). The aim of this paper is to follow these developments by tracing relationships between (abstract) and physical structures."}, {"heading": "2. Phonological systems", "text": "Phonology is interpreted in this paper as a formal model that represents cognitive (unconscious) knowledge of the systematic sound patterns of a language by native speakers; the two traditional components of such models are i) system primitives, i.e. the units of representation for cognitively relevant objects such as sounds or syllables; and ii) a series of permissible operations on these representations that are capable of generating the observed patterns. Of course, these two facets are closely linked and interdependent. In this paper, we will focus on the theory of representation."}, {"heading": "2.1. Segmental representations", "text": "In the tradition of James and Halle (1956), phonemes are able to think and act from within themselves; in the past, they are able to determine themselves; in the present, they are able to determine themselves; in the present, they are able to determine themselves; in the present, they are able to determine themselves; in the future, it will be so that they are able to determine themselves; and in the future, they will be able to determine themselves."}, {"heading": "2.2. Representing CMUbet with SPE and GP", "text": "In order to characterize the phoneme inventory of American English in the SPE and GP frameworks, we have adopted the approach of King and Taylor (2000) with some modifications, using the reduced set of 39 phonemes in the CMUbet system1. The main difference in SPE-like representations is that we have added a [rising] feature that is used to distinguish diphthongs from monophthongs. In the original SPE framework, this difference was treated with the [long] feature and the surface representation of diphthongs was derived from long monophthongs by a rule. Given the absence of the \"rule module\" in our approach to synthesis, we opted for a uniform feature specification of the full vowel inventory of American English using the additional feature. This enabled diphthongs to form a natural class with vowels rather than with vowels [j, w] as in King and Taylor (2000)."}, {"heading": "3. Phonological vocoding", "text": "Both SPE and GP phonological systems represent a telephone through a combination of phonological features. For example, a consonant [j] is articulated by means of the mediodorsal part of the tongue [+ dorsally] in the mediopalatal part of the vocal tract [+ high], which is produced by vibrating the vocal fold [+ voiced] at the same time. These three features then include the phonological representation for [j] in the SPE system. Cernak et al. (2015) recently developed a phonological vocoder for low-bit-rate parametric speech coding as a cascaded artificial neural network consisting of a speech analyzer and synthesizers using a common phonological speech representation. Figure 1 shows a sequential processing of the vocoding, which is briefly presented in the following text."}, {"heading": "3.1. Phonological analysis", "text": "Phonological analysis begins with a speech analysis that turns speech samples into a sequence of acoustic characteristic observations. In this step of speech analysis, conventional conceptual coefficients can be used. Then, a bank of phonological analyzers realized as neural network classifiers converts the acoustic characteristic observation sequence X into a sequence of vectors Z = {z1,..., zN} in which a vector zn = [z1n,., zkn,.,.. zKn] T consists of phonological characteristics posterior probabilities named after the phonological background characteristics. zkn background characteristics are probabilities that the k-th characteristic of the symmetric unit occurs (versus not occurs), and they compose the information transferred to the side of the synthesizer in the order in which the phonological background signal is transferred to the synthetical values (the next one is synthesized)."}, {"heading": "3.2. Phonological synthesis", "text": "The phonological synthesizer is based on a Deep Neural Network (DNN), which learns the highly complex regression problem of mapping the posterior language parameters zkn to the language parameters. Specifically, it consists of two arithmetic steps. The first step is a DNN forward pass, which generates the language parameters, and the second is a conversion of the language parameters into the language samples. The second step of synthesis is based on an open source LPC re-synthesis with minimal phase complex cepstrum glottal model estimates (Garner et al., 2015). The modeled language parameters are as follows: \u2022 pn: static line spectral pairs (LSPs) 24th order plus gain \u2022 log (rn): a harmonic-to-noise (HNR) ratio, \u2022 and tn, log (mn): two glottal model parameters - angle and size of a glottal polar, gain (trrn)."}, {"heading": "4. Experimental setup for laboratory phonology", "text": "This section describes the experimental protocol of phonological analysis and synthesis. Table 1 presents the data used in these experiments; the analyzer is trained on the corpora of continuous speech recognition of The Wall Street Journal (WSJ0 and WSJ1, 1992); the 284-sentence set of 37,514 statements was used, divided into 90% training and 10% cross-validation sets; the synthesizer is trained and evaluated on an English audiobook \"Anna Karenina\" by Leo Tolstoy2, which is approximately 36 hours.The recordings were divided into 238 sections, and we used Sections 1-209 as a training set, 210-230 as a development set, and 231-238 as a test set, each lasting 3 hours."}, {"heading": "4.1. Phonological analysis", "text": "The analyzer is based on a database of phonological analyzers realized as neural network classifiers - multilayer perceptrons (MLPs) - and assesses phonological background characteristics. Each MLP is designed to classify a binary phonological property. First, we trained a phoneme-based automatic speech recognition system using features of Perceptual Linear Prediction (PLP).The phoneme set, which consists of 40 phonemes (including \"sil,\" which represents silence), was defined by the pronunciation dictionary of the CMU. The remaining 10% was used for crossword identification. We associated trilingual models with the HTS variant (Zen et al., 2007) of the HTK toolkit based on the 90% subset of the Sit s 284. The remaining 10% subset was used for cross-validation."}, {"heading": "4.1.1. Alignment", "text": "The bootstrapping alignment was used to train the bootstrapping MLP using 39 PLP functions of the order with the temporal context of 9 consecutive frames and a Softmax output function. Empirically, the architecture of the MLP was determined, 3-hidden layer 351 \u00d7 2000 \u00d7 500 \u00d7 2000 \u00d7 40 (input 351 = 39 \u00d7 9 and output 40 is the number of phonemes) was realigned using a hybrid speech decoder fed with the phoneme posteriors. After two iterations of the MLP training and realignment, the best phoneme alignment of the speech data was achieved. This realignment increased the cross-validation accuracy of the MLP training from 77.54% to 82.36%."}, {"heading": "4.1.2. Training of the bank of phonological analysers", "text": "The representation given in Appendix A was used to map the phonemes of the best alignment with phonological characteristics for the training of the analyzers; the K analyzers were trained with the frame alignment having two output units, with the k-th phonological feature occurring or not; the analytical MLPs were finally trained with the same input characteristics used for the alignment of MLP, and with the same network architecture, except for 2 output units instead of 40. The output encoding the phonological feature is used as a vector of phonological decimal places (see Tables 2, 3 and 4 show classification accuracies at the image level of GP, SPE and eSPE MLPs for the output of the \"Feature occurs.\" The accuracies are specified for training and cross validation sets (see Table 1 for datasets); the performance of the analyzers is high, with an average cross-validation accuracy of 95.9%, with the number of features used being increased by 95.5%)."}, {"heading": "4.2. Phonological synthesis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1. Training", "text": "The speech signals from the training and cross-validation sets of the Tolstoy database, sampled at 16 kHz, framed by 25 ms windows with 10 ms image shift, were used to extract both the DNN input and output functions. Input functions, phonological background functions zn, were generated by the phonological analyzer trained on the WSJ database. The temporal context of 11 consecutive frames resulted in input function vectors of dimensions 132 (12 x 11 x 1), 165 (15 x 11 x 1) and 231 (21 x 11 x 1) for the GP, SPE and eSPE schemes, respectively. Output functions, the LPC language parameters, were extracted by the Speech Signal Processing (SSP) Python Toolkit3."}, {"heading": "4.2.2. Synthesis", "text": "There are three ways to create a specific tone: (i) by Z from the language, (ii) by Z from the text, or (iii) by compositional speech modeling; in the first two cases, we speak of this generation as network-based; the language parameters generated by the upstream DNN pass are smoothed using dynamic features and pre-calculated (global) variances, and a formant improvement is performed to mitigate the smoothing of formant frequencies; in the second case, speech sounds are generated using compositional phonological speech models, which are presented next in Section 5.1. In short, we use a single3https: / / github.com / idiap / ssponological feature as active input, and the rest is set to zeros. This produces an artificial tone that characterizes the input phonological feature; then the language is generated by this feature, by referring the composition to the particular component, we refer to the sound synthesize."}, {"heading": "5. Application examples for laboratory phonology", "text": "In this section we show how to use the phonological vocoder described in Section 4, a) for compositional phonological speech modeling (Section 5.1), b) for comparing phonological systems (Section 5.2) and c) as a parametric phonological TTS system (Section 5.3.2)."}, {"heading": "5.1. Compositional phonological speech models", "text": "Virtanen et al. (2015) investigate the constructive composition of speech signals, i.e. the representation of the speech signal as non-negative linear combinations of atomic units (\"atoms\"), which are also not negative, to ensure that such a combination does not lead to subtraction or diminishment.The power of the sum of uncorrelated atomic signals in any frequency band is the sum of the forces of the individual signals within that band.The central point is the definition of the tone atoms that are used as compositional models.Following this line of research, we assume that the acoustic representation of the phonological features generated by a phonological vocoder forms a series of speech signal atoms (the phonological sound components) that define the telephones. We call these sound components phonological atoms phonological atoms. It is possible to produce the atoms for any phonological system."}, {"heading": "5.2. Comparison of the phonological systems", "text": "We begin with context-independent vocoding in Section 5.2.1, i.e. the vocoding of isolated speech noises, and continue with context-dependent vocoding in Section 5.2.2."}, {"heading": "5.2.1. Context-independent vocoding", "text": "This year, it has come to the point where it only takes one year to find a solution."}, {"heading": "5.2.2. Context-dependent vocoding", "text": "The previous experiments showed that the isolated noises were used by the subjects. In this evaluation, the segmental errors relating to the context-dependent vocodings were shown. We also used a subjective evaluation test (Loizou, 2011), which was suitable for comparing two different systems. Listeners were confronted with the samples from two systems, namely with their preferences."}, {"heading": "5.3. Experimental parametric phonological TTS", "text": "In this section we show how compositional phonological speech models can be combined to produce arbitrary speech sounds, and how to synthesize continuous language from canonical phonological representation."}, {"heading": "5.3.1. Generation of sounds from unseen language", "text": "In this experiment, we randomly selected the GP system to demonstrate the phonological composition of new speech sounds. Harris (1994) claims that the fusion and division of prime numbers constitutes the phonological description of sound. We chose the phonological rule number 29 [I, U, E] \u2192 y and [A, I, U, E] \u2192 \u00ba of Harris (1994) and tried to synthesize non-English sounds by the composition of the phonological atoms involved. According to Section 5.1, we claim that new sounds can be produced by mixing the corresponding atoms over time. Table B.2 demonstrates the synthesis of standard sounds of German sounds [y] and English phonological atoms as they are generated in equation. 1. For wsn = 1, it can easily be done with available free tools, e.g.: [y] the synthesis of synchronous-m I.wav U.wav E.wav y.wav"}, {"heading": "5.3.2. Continuous speech synthesis", "text": "This year, it has reached the stage where it will be able to take the lead in order to achieve the objectives I have mentioned."}, {"heading": "6. Conclusions", "text": "The proposal consists of a cascade-like phonological analysis and synthesis. Objective and subjective assessments support the hypothesis that most informative features - with the best coverage of acoustic space (see Confusion Matrices of Context-Independent Vocoding in Sec. 5.2.1) - achieve the best application examples of our proposed approach (see Context Vocoding in Sec. 5.2.2), in which the characteristics are verified in both directions. We have shown three application examples for our proposed approach. First, we compared three systems of phonological representations and concluded that the eSPE achieves slightly better results than the other two. Our results support other current work that is suitable for phonological analysis."}, {"heading": "7. Acknowledgements", "text": "We would like to thank Dr. Phil N.Garner from the Idiap Research Institute for his valuable comments on this paper, which was carried out with the support of the Swiss NSF as part of the funding measure CRSII2 141903: Spoken Interaction with Interpretation in Switzerland (SIWIS) and the funding measure 2 / 0197 / 15 of the Scientific Granting Agency in Slovakia."}, {"heading": "8. References", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix A. Mapping of the phonological features to CMUbet", "text": "Tables A.1, A.2 and A.3 show the assignment of phonological characteristics to the phonemes used in this work."}, {"heading": "Appendix B. Demonstrating speech synthesis samples", "text": "Tables B.1 and B.2 contain recordings demonstrating the phonological atoms of general medicine and their composition, and Tables B.3 Recordings demonstrating phonological speech synthesis."}], "references": [{"title": "Analysis by Synthesis: A (Re-)Emerging", "author": ["T.G. References Bever", "D. Poeppel"], "venue": null, "citeRegEx": "Bever and Poeppel,? \\Q2010\\E", "shortCiteRegEx": "Bever and Poeppel", "year": 2010}, {"title": "Composition of Deep and Spiking Neural Networks for Very Low Bit Rate Speech Coding", "author": ["M. Cernak", "A. Lazaridis", "A. Asaei", "P.N. Garner", "August"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing.", "citeRegEx": "Cernak et al\\.,? 2016", "shortCiteRegEx": "Cernak et al\\.", "year": 2016}, {"title": "Phonological vocoding using artificial neural networks", "author": ["M. Cernak", "B. Potard", "P.N. Garner", "Apr."], "venue": "In: Proc. of ICASSP. IEEE. URL https://publidiap.idiap.ch/index.php/publications/show/ 3070", "citeRegEx": "Cernak et al\\.,? 2015", "shortCiteRegEx": "Cernak et al\\.", "year": 2015}, {"title": "Segmental vocoder-going beyond the phonetic approach", "author": ["J. Cernocky", "G. Baudoin", "G. Chollet", "May"], "venue": "In: Proc. of ICASSP. Vol. 2. IEEE, pp. 605\u2013608 vol.2. URL http://dx.doi.org/10.1109/icassp.1998.675337", "citeRegEx": "Cernocky et al\\.,? 1998", "shortCiteRegEx": "Cernocky et al\\.", "year": 1998}, {"title": "The Sound Pattern of English", "author": ["N. Chomsky", "M. Halle"], "venue": null, "citeRegEx": "Chomsky and Halle,? \\Q1968\\E", "shortCiteRegEx": "Chomsky and Halle", "year": 1968}, {"title": "The Internal Organization of Speech Sounds", "author": ["G.N. Clements", "E. Hume"], "venue": null, "citeRegEx": "Clements and Hume,? \\Q1995\\E", "shortCiteRegEx": "Clements and Hume", "year": 1995}, {"title": "A simple continuous excitation model for parametric vocoding", "author": ["P.N. Garner", "M. Cernak", "B. Potard", "Jan."], "venue": "Tech. Rep. Idiap-RR-03-2015, Idiap. URL http://publications.idiap.ch/index.php/publications/ show/2955", "citeRegEx": "Garner et al\\.,? 2015", "shortCiteRegEx": "Garner et al\\.", "year": 2015}, {"title": "Articulatory copy synthesis from cine x-ray films", "author": ["C. Gendrot", "M. Adda-Decker"], "venue": "Proc. of Interspeech", "citeRegEx": "Gendrot and Adda.Decker,? \\Q2005\\E", "shortCiteRegEx": "Gendrot and Adda.Decker", "year": 2005}, {"title": "Cortical oscillations and speech processing: emerging computational principles and operations", "author": ["Giraud", "A.-L.L.", "D. Poeppel", "Apr."], "venue": "Nature neuroscience 15 (4), 511\u2013517. URL http://view.ncbi.nlm.nih.gov/pubmed/22426255", "citeRegEx": "Giraud et al\\.,? 2012", "shortCiteRegEx": "Giraud et al\\.", "year": 2012}, {"title": "The SIWIS database: a multilingual speech database with acted emphasis", "author": ["Goldman", "J.-P.", "Honnet", "P.-E.", "R. Clark", "P.N. Garner", "M. Ivanova", "A. Lazaridis", "H. Liang", "T. Macedo", "B. Pfister", "M.S. Ribeiro", "E. Wehrli", "J. Yamagishi", "Sep."], "venue": "In: Proc. of Interspeech.", "citeRegEx": "Goldman et al\\.,? 2016", "shortCiteRegEx": "Goldman et al\\.", "year": 2016}, {"title": "Autosegmental Phonology", "author": ["J. Goldsmith"], "venue": "Ph.D. thesis,", "citeRegEx": "Goldsmith,? \\Q1976\\E", "shortCiteRegEx": "Goldsmith", "year": 1976}, {"title": "Articulatory copy synthesis using a nineparameter vocal tract model", "author": ["C. Goodyear", "D. Wei", "May"], "venue": "In: Proc. of ICASSP. Vol. 1. IEEE, pp. 385\u2013388 vol. 1.", "citeRegEx": "Goodyear et al\\.,? 1996", "shortCiteRegEx": "Goodyear et al\\.", "year": 1996}, {"title": "English Sound Structure, 1st Edition", "author": ["J. Harris", "Dec."], "venue": "Wiley-Blackwell. URL", "citeRegEx": "Harris and Dec.,? 1994", "shortCiteRegEx": "Harris and Dec.", "year": 1994}, {"title": "The elements of phonological representation", "author": ["J. Harris", "G. Lindsey"], "venue": null, "citeRegEx": "Harris and Lindsey,? \\Q1995\\E", "shortCiteRegEx": "Harris and Lindsey", "year": 1995}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh", "Jul."], "venue": "Neural Comput. 18 (7), 1527\u20131554. URL http://dx.doi.org/10.1162/neco.2006.18.7.1527", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "The analysis by synthesis of speech melody: from data to models", "author": ["D. Hirst"], "venue": "Journal of Speech Sciences", "citeRegEx": "Hirst,? \\Q2011\\E", "shortCiteRegEx": "Hirst", "year": 2011}, {"title": "The influence of glottal waveform on the naturalness of speech from a parallel formant synthesizer", "author": ["J. Holmes", "Jun"], "venue": "Audio and Electroacoustics, IEEE Transactions on 21 (3), 298\u2013305.", "citeRegEx": "Holmes and Jun,? 1973", "shortCiteRegEx": "Holmes and Jun", "year": 1973}, {"title": "Fundamentals of Language", "author": ["R. Jakobson", "M. Halle"], "venue": null, "citeRegEx": "Jakobson and Halle,? \\Q1956\\E", "shortCiteRegEx": "Jakobson and Halle", "year": 1956}, {"title": "Constituent structure and government in phonology", "author": ["J. Kaye", "J. Lowenstamm", "Vergnaud", "J.-R"], "venue": "Phonology", "citeRegEx": "Kaye et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Kaye et al\\.", "year": 1990}, {"title": "Detection of phonological features in continuous speech using neural networks", "author": ["S. King", "P. Taylor", "Oct."], "venue": "Computer Speech & Language 14 (4), 333\u2013353. URL http://dx.doi.org/10.1006/csla.2000.0148", "citeRegEx": "King et al\\.,? 2000", "shortCiteRegEx": "King et al\\.", "year": 2000}, {"title": "The CMU Arctic speech databases", "author": ["J. Kominek", "A. Black"], "venue": "Proc. of 5th ISCA Speech Synthesis Workshop. pp", "citeRegEx": "Kominek and Black,? \\Q2004\\E", "shortCiteRegEx": "Kominek and Black", "year": 2004}, {"title": "Mel-cepstral distance measure for objective speech quality assessment", "author": ["R.F. Kubichek", "May"], "venue": "In: Proc. of ICASSP. Vol. 1. IEEE, pp. 125\u2013128 29", "citeRegEx": "Kubichek and May,? 1993", "shortCiteRegEx": "Kubichek and May", "year": 1993}, {"title": "A Course in Phonetics, 7th Edition", "author": ["P. Ladefoged", "K. Johnson", "Jan."], "venue": "Cengage Learning. URL", "citeRegEx": "Ladefoged et al\\.,? 2014", "shortCiteRegEx": "Ladefoged et al\\.", "year": 2014}, {"title": "Articulatory copy synthesis from cine x-ray films", "author": ["Y. Laprie", "M. Loosvelt", "S. Maeda", "R. Sock", "F. Hirsch"], "venue": "Proc. of Interspeech", "citeRegEx": "Laprie et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Laprie et al\\.", "year": 2013}, {"title": "Suprasegmental Phonology", "author": ["W. Leben"], "venue": "Ph.D. thesis,", "citeRegEx": "Leben,? \\Q1973\\E", "shortCiteRegEx": "Leben", "year": 1973}, {"title": "A very low bit rate speech coder based on a recognition/synthesis paradigm", "author": ["Lee", "K.-S.", "R. Cox", "Jul"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing 9 (5), 482\u2013491.", "citeRegEx": "Lee et al\\.,? 2001", "shortCiteRegEx": "Lee et al\\.", "year": 2001}, {"title": "USTC system for Blizzard Challenge 2006 - an improved HMM-based speech synthesis method", "author": ["Ling", "Z.-H", "Wu", "Y.-J", "Wang", "Y.-P", "L. Qin", "R.-H"], "venue": "Proc. of Blizzard Challenge workshop", "citeRegEx": "Ling et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2006}, {"title": "Speech Quality Assessment", "author": ["P.C. Loizou"], "venue": null, "citeRegEx": "Loizou,? \\Q2011\\E", "shortCiteRegEx": "Loizou", "year": 2011}, {"title": "Acoustic analysis of German vowels in the Kiel Corpus of Read Speech", "author": ["M. P\u00e4tzold", "A.P. Simpson"], "venue": "Arbeitsberichte des Instituts fu\u0308r Phonetik und digitale Sprachverarbeitung der Universita\u0308t", "citeRegEx": "P\u00e4tzold and Simpson,? \\Q1997\\E", "shortCiteRegEx": "P\u00e4tzold and Simpson", "year": 1997}, {"title": "The design for the wall street journal-based CSR corpus", "author": ["D.B. Paul", "J.M. Baker"], "venue": "Proceedings of the workshop on Speech and Natural Language. HLT \u201991. Association for Computational Linguistics,", "citeRegEx": "Paul and Baker,? \\Q1992\\E", "shortCiteRegEx": "Paul and Baker", "year": 1992}, {"title": "Conceptual foundations of phonology as a laboratory science", "author": ["J.B. Pierrehumbert", "M.E. Beckman", "D.R. Ladd"], "venue": null, "citeRegEx": "Pierrehumbert et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Pierrehumbert et al\\.", "year": 2000}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely", "Dec."], "venue": "In: Proc. of ASRU. IEEE SPS, iEEE Catalog No.: CFP11SRW-USB.", "citeRegEx": "Povey et al\\.,? 2011", "shortCiteRegEx": "Povey et al\\.", "year": 2011}, {"title": "The representation of features and relations in non-linear phonology", "author": ["E.C. Sagey"], "venue": "Ph.D. thesis,", "citeRegEx": "Sagey,? \\Q1986\\E", "shortCiteRegEx": "Sagey", "year": 1986}, {"title": "Acoustic modeling based on the MDL principle for speech recognition", "author": ["K. Shinoda", "T. Watanabe"], "venue": "Proc. of Eurospeech", "citeRegEx": "Shinoda and Watanabe,? \\Q1997\\E", "shortCiteRegEx": "Shinoda and Watanabe", "year": 1997}, {"title": "Experiments on Cross-Language Attribute Detection and Phone Recognition With Minimal Target-Specific Training Data", "author": ["S.M. Siniscalchi", "Lyu", "D.-C.", "T. Svendsen", "Lee", "C.-H.", "Mar."], "venue": "IEEE Trans. on Audio, Speech, and Language Processing 20 (3), 875\u2013887. URL http://dx.doi.org/10.1109/tasl.2011.2167610", "citeRegEx": "Siniscalchi et al\\.,? 2012", "shortCiteRegEx": "Siniscalchi et al\\.", "year": 2012}, {"title": "Speech parameter generation from HMM using dynamic features", "author": ["K. Tokuda", "T. Kobayashi", "S. Imai", "May"], "venue": "In: Proc. of ICASSP. Vol. 1. IEEE, pp. 660\u2013663 vol.1. URL http://dx.doi.org/10.1109/icassp.1995.479684", "citeRegEx": "Tokuda et al\\.,? 1995", "shortCiteRegEx": "Tokuda et al\\.", "year": 1995}, {"title": "A very low bit rate speech coder using HMM-based speech recognition/synthesis techniques", "author": ["K. Tokuda", "T. Masuko", "J. Hiroi", "T. Kobayashi", "T. Kitamura", "May"], "venue": "In: Proc. of ICASSP. Vol. 2. IEEE, pp. 609\u2013612 vol.2. URL http://dx.doi.org/10.1109/icassp.1998.675338", "citeRegEx": "Tokuda et al\\.,? 1998", "shortCiteRegEx": "Tokuda et al\\.", "year": 1998}, {"title": "Compositional Models for Audio Processing: Uncovering the structure of sound mixtures", "author": ["T. Virtanen", "J.F. Gemmeke", "B. Raj", "P. Smaragdis", "Mar."], "venue": "IEEE Signal Processing Magazine 32 (2), 125\u2013144. URL http://dx.doi.org/10.1109/msp.2013.2288990", "citeRegEx": "Virtanen et al\\.,? 2015", "shortCiteRegEx": "Virtanen et al\\.", "year": 2015}, {"title": "Boosting attribute and phone estimation accuracies with deep neural networks for detectionbased speech recognition", "author": ["D. Yu", "S. Siniscalchi", "L. Deng", "Lee", "C.-H.", "March"], "venue": "In: Proc. of ICASSP. IEEE SPS. URL http://research.microsoft.com/apps/pubs/default.aspx?id= 157585", "citeRegEx": "Yu et al\\.,? 2012", "shortCiteRegEx": "Yu et al\\.", "year": 2012}, {"title": "The HMM-based Speech Synthesis System Version 2.0", "author": ["H. Zen", "T. Nose", "J. Yamagishi", "S. Sako", "T. Masuko", "A. Black", "K. Tokuda"], "venue": "Proc. of ISCA", "citeRegEx": "Zen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zen et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 30, "context": "the program of Laboratory Phonology (Pierrehumbert et al., 2000) or the renewed interest in the approaches based on Analysis by Synthesis (Hirst, 2011; Bever and Poeppel, 2010).", "startOffset": 36, "endOffset": 64}, {"referenceID": 15, "context": ", 2000) or the renewed interest in the approaches based on Analysis by Synthesis (Hirst, 2011; Bever and Poeppel, 2010).", "startOffset": 81, "endOffset": 119}, {"referenceID": 0, "context": ", 2000) or the renewed interest in the approaches based on Analysis by Synthesis (Hirst, 2011; Bever and Poeppel, 2010).", "startOffset": 81, "endOffset": 119}, {"referenceID": 23, "context": "in the formant (Holmes, 1973), or articulatory (Goodyear and Wei, 1996; Laprie et al., 2013) domains, to phones (Tokuda et al.", "startOffset": 47, "endOffset": 92}, {"referenceID": 36, "context": ", 2013) domains, to phones (Tokuda et al., 1998; Lee and Cox, 2001), and syllables (Cernocky et al.", "startOffset": 27, "endOffset": 67}, {"referenceID": 3, "context": ", 1998; Lee and Cox, 2001), and syllables (Cernocky et al., 1998), are used in sequential processing.", "startOffset": 42, "endOffset": 65}, {"referenceID": 0, "context": ", 2000) or the renewed interest in the approaches based on Analysis by Synthesis (Hirst, 2011; Bever and Poeppel, 2010). The goal of this paper is to follow these developments by proposing a platform for exploring relations between the mental (abstract) and physical structures of the speech signal. In this, we aim at mutual cross-fertilisation between phonology, as a quest for understanding and modelling of cognitive abilities that underlie systematic patterns in our speech, and speech processing, as a quest for natural, robust, and reliable automatic systems for synthesising and recognising speech. As a first step in this direction we examine a cascaded speech analysis and synthesis approach (known also as vocoding) based on phonological representations and how this might inform both quests mentioned above. In parametric vocoding speech segments of different time-domain granularity, ranging from speech frames, e.g. in the formant (Holmes, 1973), or articulatory (Goodyear and Wei, 1996; Laprie et al., 2013) domains, to phones (Tokuda et al., 1998; Lee and Cox, 2001), and syllables (Cernocky et al., 1998), are used in sequential processing. In addition to these segments, phonological representations have also been shown to be useful for speech processing e.g. by King and Taylor (2000). In our work, we explore a direct link between phonological features and their engineered acoustic realizations.", "startOffset": 95, "endOffset": 1305}, {"referenceID": 34, "context": "Secondly, phonological representations are inherently multilingual (Siniscalchi et al., 2012).", "startOffset": 67, "endOffset": 93}, {"referenceID": 1, "context": "In this work, we propose to use the phonological vocoding of Cernak et al. (2015) and other advances of speech processing for testing certain aspects of phonological theories.", "startOffset": 61, "endOffset": 82}, {"referenceID": 13, "context": "\u2022 The Government Phonology (GP) features (Harris and Lindsey, 1995) describing sounds by fusing and splitting of 11 primes.", "startOffset": 41, "endOffset": 67}, {"referenceID": 4, "context": "\u2022 The Sound Pattern of English (SPE) system with 13 features established from natural (articulatory) features (Chomsky and Halle, 1968).", "startOffset": 110, "endOffset": 135}, {"referenceID": 38, "context": "\u2022 The extended SPE system (eSPE) (Yu et al., 2012; Siniscalchi et al., 2012) consisting of 21 phonological features.", "startOffset": 33, "endOffset": 76}, {"referenceID": 34, "context": "\u2022 The extended SPE system (eSPE) (Yu et al., 2012; Siniscalchi et al., 2012) consisting of 21 phonological features.", "startOffset": 33, "endOffset": 76}, {"referenceID": 24, "context": "These proposals started with the representation of lexical tones (Leben, 1973; Goldsmith, 1976), continued with the featural geometry approach (Sagey, 1986; Clements and Hume, 1995) and received novel formal treatments in the theories of Dependency and Government Phonology (GP), e.", "startOffset": 65, "endOffset": 95}, {"referenceID": 10, "context": "These proposals started with the representation of lexical tones (Leben, 1973; Goldsmith, 1976), continued with the featural geometry approach (Sagey, 1986; Clements and Hume, 1995) and received novel formal treatments in the theories of Dependency and Government Phonology (GP), e.", "startOffset": 65, "endOffset": 95}, {"referenceID": 32, "context": "These proposals started with the representation of lexical tones (Leben, 1973; Goldsmith, 1976), continued with the featural geometry approach (Sagey, 1986; Clements and Hume, 1995) and received novel formal treatments in the theories of Dependency and Government Phonology (GP), e.", "startOffset": 143, "endOffset": 181}, {"referenceID": 5, "context": "These proposals started with the representation of lexical tones (Leben, 1973; Goldsmith, 1976), continued with the featural geometry approach (Sagey, 1986; Clements and Hume, 1995) and received novel formal treatments in the theories of Dependency and Government Phonology (GP), e.", "startOffset": 143, "endOffset": 181}, {"referenceID": 14, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles.", "startOffset": 20, "endOffset": 46}, {"referenceID": 4, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles.", "startOffset": 50, "endOffset": 75}, {"referenceID": 4, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles. In the former model of Jakobson and Halle (1956), 12 basic perceptual-acoustic domains (e.", "startOffset": 50, "endOffset": 177}, {"referenceID": 4, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles. In the former model of Jakobson and Halle (1956), 12 basic perceptual-acoustic domains (e.g. acute-grave, or compact-diffuse) define the space for characterising all the phonemes. The model uses polar opposites for these 12 continua, which are necessarily relational, and thus languagespecific. Hence, a vowel characterised as, e.g. grave in one language might be phonetically different from the same grave vowel in another language since their grave quality might be at a different point of the acute-grave continuum. The latter system of Chomsky and Halle (1968), known also as SPE, differed from the former system of Jakobson and Halle (1956) in two fundamental aspects relevant for this paper.", "startOffset": 50, "endOffset": 693}, {"referenceID": 4, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles. In the former model of Jakobson and Halle (1956), 12 basic perceptual-acoustic domains (e.g. acute-grave, or compact-diffuse) define the space for characterising all the phonemes. The model uses polar opposites for these 12 continua, which are necessarily relational, and thus languagespecific. Hence, a vowel characterised as, e.g. grave in one language might be phonetically different from the same grave vowel in another language since their grave quality might be at a different point of the acute-grave continuum. The latter system of Chomsky and Halle (1968), known also as SPE, differed from the former system of Jakobson and Halle (1956) in two fundamental aspects relevant for this paper.", "startOffset": 50, "endOffset": 774}, {"referenceID": 4, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles. In the former model of Jakobson and Halle (1956), 12 basic perceptual-acoustic domains (e.g. acute-grave, or compact-diffuse) define the space for characterising all the phonemes. The model uses polar opposites for these 12 continua, which are necessarily relational, and thus languagespecific. Hence, a vowel characterised as, e.g. grave in one language might be phonetically different from the same grave vowel in another language since their grave quality might be at a different point of the acute-grave continuum. The latter system of Chomsky and Halle (1968), known also as SPE, differed from the former system of Jakobson and Halle (1956) in two fundamental aspects relevant for this paper. First, it took the articulatory production mechanism as the underlying principle of phoneme organisation; hence, in their 13 basic binary features, we talk about the position (or activity) of the active articulators rather than percepts they create. Second, SPE assumed that the flat, unstructured binary feature specifications are language independent and characterise the set of possible phonemes in languages of the world. The developments of phonological theory after SPE focused on both the theory of representations as well as the operations. The most relevant for this paper are proposals for establishing the non-linear nature of phonological representations, i.e., the fact that individual features are not strictly linked to the linear sequence of sounds but may span greater domains or occupy independent non-overlapping tiers. These proposals started with the representation of lexical tones (Leben, 1973; Goldsmith, 1976), continued with the featural geometry approach (Sagey, 1986; Clements and Hume, 1995) and received novel formal treatments in the theories of Dependency and Government Phonology (GP), e.g. Kaye et al. (1990), Harris (1994).", "startOffset": 50, "endOffset": 1969}, {"referenceID": 4, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles. In the former model of Jakobson and Halle (1956), 12 basic perceptual-acoustic domains (e.g. acute-grave, or compact-diffuse) define the space for characterising all the phonemes. The model uses polar opposites for these 12 continua, which are necessarily relational, and thus languagespecific. Hence, a vowel characterised as, e.g. grave in one language might be phonetically different from the same grave vowel in another language since their grave quality might be at a different point of the acute-grave continuum. The latter system of Chomsky and Halle (1968), known also as SPE, differed from the former system of Jakobson and Halle (1956) in two fundamental aspects relevant for this paper. First, it took the articulatory production mechanism as the underlying principle of phoneme organisation; hence, in their 13 basic binary features, we talk about the position (or activity) of the active articulators rather than percepts they create. Second, SPE assumed that the flat, unstructured binary feature specifications are language independent and characterise the set of possible phonemes in languages of the world. The developments of phonological theory after SPE focused on both the theory of representations as well as the operations. The most relevant for this paper are proposals for establishing the non-linear nature of phonological representations, i.e., the fact that individual features are not strictly linked to the linear sequence of sounds but may span greater domains or occupy independent non-overlapping tiers. These proposals started with the representation of lexical tones (Leben, 1973; Goldsmith, 1976), continued with the featural geometry approach (Sagey, 1986; Clements and Hume, 1995) and received novel formal treatments in the theories of Dependency and Government Phonology (GP), e.g. Kaye et al. (1990), Harris (1994). These latter approaches posit the so called primes, or basic elements, that are monovalent (c.", "startOffset": 50, "endOffset": 1984}, {"referenceID": 13, "context": "Harris and Lindsey (1995) call this GP assumption the autonomous interpretation hypothesis.", "startOffset": 0, "endOffset": 26}, {"referenceID": 1, "context": "Cernak et al. (2015) have recently designed a phonological vocoder for a low bit rate parametric speech coding as a cascaded artificial neural network composed of speech analyser and synthesizer that use a shared phonological speech representation.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "For example, the binary nature of the phonological features considered by Cernak et al. (2015) allowed for using binary values of the phonological features instead of continuous values, which resulted only in minimal perceptual degradation of speech quality.", "startOffset": 74, "endOffset": 95}, {"referenceID": 6, "context": "The second stage of synthesis is based on an open-source LPC re-synthesis with minimum-phase complex cepstrum glottal model estimation (Garner et al., 2015).", "startOffset": 135, "endOffset": 156}, {"referenceID": 35, "context": "The generated speech parameter vectors \u2013 pn, tn, log(rn) and log(mn) for the n-th frame \u2013 from the first computational step are smoothed using dynamic features and pre-computed (global) variances (Tokuda et al., 1995), and formant enhancement (Ling et al.", "startOffset": 196, "endOffset": 217}, {"referenceID": 26, "context": ", 1995), and formant enhancement (Ling et al., 2006) is performed to compensate for over-smoothing of the formant frequencies.", "startOffset": 33, "endOffset": 52}, {"referenceID": 29, "context": "The analyser is trained on the Wall Street Journal (WSJ0 and WSJ1) continuous speech recognition corpora (Paul and Baker, 1992).", "startOffset": 105, "endOffset": 127}, {"referenceID": 39, "context": "The three-state, cross-word triphone models were trained with the HTS variant (Zen et al., 2007) of the HTK toolkit on the 90% subset of the si tr s 284 set.", "startOffset": 78, "endOffset": 96}, {"referenceID": 33, "context": "We tied triphone models with decision tree state clustering based on the minimum description length (MDL) criterion (Shinoda and Watanabe, 1997).", "startOffset": 116, "endOffset": 144}, {"referenceID": 14, "context": "The DNN was initialised using (K \u00d7 11) \u00d7 1024 \u00d7 1024 \u00d7 1024\u00d71024 Deep Belief Network pre-training by contrastive divergence with 1 sampling step (CD1) (Hinton et al., 2006).", "startOffset": 151, "endOffset": 172}, {"referenceID": 31, "context": "The 4 hidden layers DNN with a linear output function was then trained using a mini-batch based stochastic gradient descent algorithm with mean square error cost function of the KALDI toolkit (Povey et al., 2011).", "startOffset": 192, "endOffset": 212}, {"referenceID": 27, "context": "We employed a subjective evaluation listening test (Loizou, 2011), suitable for comparing two different systems.", "startOffset": 51, "endOffset": 65}, {"referenceID": 7, "context": "The mean value of the first formant of the natural [\u0153] takes values between 509 Hz (Gendrot and Adda-Decker, 2005) and 550 Hz (P\u00e4tzold and Simpson, 1997).", "startOffset": 83, "endOffset": 114}, {"referenceID": 28, "context": "The mean value of the first formant of the natural [\u0153] takes values between 509 Hz (Gendrot and Adda-Decker, 2005) and 550 Hz (P\u00e4tzold and Simpson, 1997).", "startOffset": 126, "endOffset": 153}, {"referenceID": 20, "context": "To demonstrate the potential of our parametric phonological TTS system, we randomly selected three utterances from a slt subset of the CMUARCTIC speech database (Kominek and Black, 2004), and used their text labels to generate continuous speech.", "startOffset": 161, "endOffset": 186}, {"referenceID": 9, "context": "The SUSs were taken from SIWIS database Goldman et al. (2016). The length of the sentences varied from 6 to 8 words.", "startOffset": 40, "endOffset": 62}, {"referenceID": 1, "context": "Cernak et al. (2016) have recently shown that major degradation of speech quality in speech synthesis based on the phonological speech representation comes from the LPC re-synthesis.", "startOffset": 0, "endOffset": 21}, {"referenceID": 38, "context": "Our results thus support other recent work showing that eSPE is suitable for phonological analysis, for speech recognition and language identification tasks (Yu et al., 2012; Siniscalchi et al., 2012).", "startOffset": 157, "endOffset": 200}, {"referenceID": 34, "context": "Our results thus support other recent work showing that eSPE is suitable for phonological analysis, for speech recognition and language identification tasks (Yu et al., 2012; Siniscalchi et al., 2012).", "startOffset": 157, "endOffset": 200}], "year": 2016, "abstractText": "Using phonological speech vocoding, we propose a platform for exploring relations between phonology and speech processing, and in broader terms, for exploring relations between the abstract and physical structures of a speech signal. Our goal is to make a step towards bridging phonology and speech processing and to contribute to the program of Laboratory Phonology. We show three application examples for laboratory phonology: compositional phonological speech modelling, a comparison of phonological systems and an experimental phonological parametric text-to-speech (TTS) system. The featural representations of the following three phonological systems are considered in this work: (i) Government Phonology (GP), (ii) the Sound Pattern of English (SPE), and (iii) the extended SPE (eSPE). Comparing GPand eSPE-based vocoded speech, we conclude that the latter achieves slightly better results than the former. However, GP \u2013 the most compact phonological speech representation \u2013 performs comparably to the systems with a higher number of phonological features. The parametric TTS based on phonological speech representation, and trained from an unlabelled audiobook in an unsupervised manner, achieves intelligibility of 85% of the state-of-the-art parametric speech synthesis. We envision that the presented approach paves the way for researchers in both fields to form meaningful hypotheses that are explicitly testable using the concepts developed and exemplified in this paper. On the one hand, laboratory phonologists might test the applied concepts of their theoretical models, and on the other hand, the speech processing community may utilize \u2217Corresponding author Email address: milos.cernak@idiap.ch (Milos Cernak) Preprint submitted to Elsevier September 16, 2016 ar X iv :1 60 1. 05 99 1v 3 [ cs .C L ] 1 5 Se p 20 16 the concepts developed for the theoretical phonological models for improvements of the current state-of-the-art applications.", "creator": "LaTeX with hyperref package"}}}