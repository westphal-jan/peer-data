{"id": "1601.06071", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2016", "title": "Bitwise Neural Networks", "abstract": "Based on the assumption that there exists a neural network that efficiently represents a set of Boolean functions between all binary inputs and outputs, we propose a process for developing and deploying neural networks whose weight parameters, bias terms, input, and intermediate hidden layer output signals, are all binary-valued, and require only basic bit logic for the feedforward pass. The proposed Bitwise Neural Network (BNN) is especially suitable for resource-constrained environments, since it replaces either floating or fixed-point arithmetic with significantly more efficient bitwise operations. Hence, the BNN requires for less spatial complexity, less memory bandwidth, and less power consumption in hardware. In order to design such networks, we propose to add a few training schemes, such as weight compression and noisy backpropagation, which result in a bitwise network that performs almost as well as its corresponding real-valued network. We test the proposed network on the MNIST dataset, represented using binary features, and show that BNNs result in competitive performance while offering dramatic computational savings.", "histories": [["v1", "Fri, 22 Jan 2016 16:59:01 GMT  (87kb,D)", "http://arxiv.org/abs/1601.06071v1", "This paper was presented at the International Conference on Machine Learning (ICML) Workshop on Resource-Efficient Machine Learning, Lille, France, Jul. 6-11, 2015"]], "COMMENTS": "This paper was presented at the International Conference on Machine Learning (ICML) Workshop on Resource-Efficient Machine Learning, Lille, France, Jul. 6-11, 2015", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["minje kim", "paris smaragdis"], "accepted": false, "id": "1601.06071"}, "pdf": {"name": "1601.06071.pdf", "metadata": {"source": "META", "title": "Bitwise Neural Networks", "authors": ["Minje Kim", "Paris Smaragdis"], "emails": ["MINJE@ILLINOIS.EDU", "PARIS@ILLINOIS.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to unfold in a way that they have done in the past, namely, by behaving in a way that they have done in the past. (...) Most of them have behaved in a way that they have done in the past. (...) Most of them have behaved in a way that they have done in the past. (...) Most of them have behaved in a way that they have done in the past. (...) Most of them have behaved in a way that they have done in the past. (...) Most of them have behaved in a way that they have done. (...) Most of them have behaved in a way that they have done. (...)"}, {"heading": "2. Feedforward in Bitwise Neural Networks", "text": "In fact, in a country where most people are able to move to another world, in which they are able to integrate, it is the case that you will be able to do so."}, {"heading": "3. Training Bitwise Neural Networks", "text": "We first train some compressed network parameters, then retrain them for BNNs by means of noisy back propagation."}, {"heading": "3.1. Real-valued Networks with Weight Compression", "text": "First, we form a real network, which either has bitwise input or real input in the range of \u2212 1 and + 1. A special part of this network is that we limit the weights to have values between \u2212 1 and + 1 by wrapping them in tanh. Similarly, if we choose tanh for activation, we can say that the network is a relaxed version of the corresponding bipolar BNN. With this weight compression technique, the relaxed forward motion during training is defined as follows: ali = tanh (b-l i) + Kl \u2212 1-j tanh (w-li, j-l-1, (5) z-li = tanh (ali), (6), with all binary values in (1) and (2) being real for the time being: W-l-RKl \u2212 1 (w-l), b-l \u2212 1, b-l-RKl \u2212 1, and z-l-RKl \u2212 1."}, {"heading": "3.2. Training BNN with Noisy Backpropagation", "text": "Since we have trained a real evaluated network with an appropriate range of weights, the next thing we do is train the actual bitwise network, the training procedure is similar to that with quantified weights (Fiesler et al., 1990; Hwang & Sung, 2014), except that the values we are dealing with are bits and the operations on them are bitwise. To this end, we first initialize all the real evaluated parameters, W and b, with the parameters learned from the previous section. Then, we set a thrift parameter that indicates the proportion of zeros after binarization. Then, we divide the parameters into three groups: + 1, 0, or \u2212 1. Therefore, the limit \u03b2, e.g. wlij = \u2212 1, if w, lij < \u2212 \u03b2 is determined. Note that the number of zero weights | w, lij, and the number of zero weights are considered binding."}, {"heading": "4. Experiments", "text": "In this section, we will go over the details and results of the handwritten number recognition task on the MNIST dataset (LeCun et al., 1998) with the proposed BNN system. Throughout the training, we take over the softmax output layer for these multiclass classification cases. All networks have three hidden layers with 1024 units per layer. From the first round of training, we receive a regular failure network with the same setting proposed in (Srivastava et al., 2014), except for the fact that we used the hyperbolic tangent for both weight compression and activation to make the network suitable for initializing the following bipolar bitwise network. The number of iterations from 500 to 1 000 was sufficient to build a baseline. The first row of Table 1 shows the performance of the real binary network with 64bits that we enter in the second."}, {"heading": "5. Conclusion", "text": "In this thesis, we propose a bitwise version of artificial neural networks, where all inputs, weights, distortions, hidden units and outputs can be represented with single bits and operated with simple bitwise logic. Such a network is very efficient in terms of computing and can be valuable in situations with limited resources, especially in cases where variables and floating-point operations are prohibitively expensive. In the future, we plan to investigate a bitwise version of convolutional neural networks, where efficient computing is more desirable."}], "references": [{"title": "A survey on context-aware systems", "author": ["M. Baldauf", "S. Dustdar", "F. Rosenberg"], "venue": "International Journal of Ad Hoc and Ubiquitous Computing,", "citeRegEx": "Baldauf et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Baldauf et al\\.", "year": 2007}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Low precision arithmetic for deep learning", "author": ["M. Courbariaux", "Y. Bengio", "David", "J.-P"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Approximations by superpositions of sigmoidal functions", "author": ["G. Cybenko"], "venue": "Mathematics of Control, Signals, and Systems,", "citeRegEx": "Cybenko,? \\Q1989\\E", "shortCiteRegEx": "Cybenko", "year": 1989}, {"title": "Weight discretization paradigm for optical neural networks", "author": ["E. Fiesler", "A. Choudry", "H.J. Caulfield"], "venue": "In The Hague\u201990,", "citeRegEx": "Fiesler et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Fiesler et al\\.", "year": 1990}, {"title": "On learning \u03bc-perceptron networks with binary weights", "author": ["M. Golea", "M. Marchand", "T.R. Hancock"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Golea et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Golea et al\\.", "year": 1992}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["K. Hornik"], "venue": "Neural Networks,", "citeRegEx": "Hornik,? \\Q1991\\E", "shortCiteRegEx": "Hornik", "year": 1991}, {"title": "Fixed-point feedforward deep neural network design using weights", "author": ["K. Hwang", "W. Sung"], "venue": "IEEE Workshop on Signal Processing Systems (SiPS),", "citeRegEx": "Hwang and Sung,? \\Q2014\\E", "shortCiteRegEx": "Hwang and Sung", "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["W.S. McCulloch", "W.H. Pitts"], "venue": "The Bulletin of Mathematical Biophysics,", "citeRegEx": "McCulloch and Pitts,? \\Q1943\\E", "shortCiteRegEx": "McCulloch and Pitts", "year": 1943}, {"title": "Computational limitations on learning from examples", "author": ["L. Pitt", "L.G. Valiant"], "venue": "Journal of the Association for Computing Machinery,", "citeRegEx": "Pitt and Valiant,? \\Q1988\\E", "shortCiteRegEx": "Pitt and Valiant", "year": 1988}, {"title": "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["D. Soudry", "I. Hubara", "R. Meir"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Soudry et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Soudry et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "An experimental study on speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "Dai", "L.-R", "Lee", "C.-H"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "According to the universal approximation theorem, a single hidden layer with a finite number of units can approximate a continuous function with some mild assumptions (Cybenko, 1989; Hornik, 1991).", "startOffset": 167, "endOffset": 196}, {"referenceID": 7, "context": "According to the universal approximation theorem, a single hidden layer with a finite number of units can approximate a continuous function with some mild assumptions (Cybenko, 1989; Hornik, 1991).", "startOffset": 167, "endOffset": 196}, {"referenceID": 6, "context": "cated function, Deep Neural Networks (DNN) achieve the goal by learning a hierarchy of features in their multiple layers (Hinton et al., 2006; Bengio, 2009).", "startOffset": 121, "endOffset": 156}, {"referenceID": 1, "context": "cated function, Deep Neural Networks (DNN) achieve the goal by learning a hierarchy of features in their multiple layers (Hinton et al., 2006; Bengio, 2009).", "startOffset": 121, "endOffset": 156}, {"referenceID": 14, "context": ", 2012), speech enhancement (Xu et al., 2014), etc, it is also the case that the relatively bigger networks with more parameters than before call for more resources (processing power, memory, battery time, etc), which are sometimes critically constrained in applications running on embedded devices.", "startOffset": 28, "endOffset": 45}, {"referenceID": 0, "context": "Examples of those applications span from context-aware computing, collecting and analysing a variety of sensor signals on the device (Baldauf et al., 2007), to always-on computer vision applications (e.", "startOffset": 133, "endOffset": 155}, {"referenceID": 4, "context": "Most of the effort is focused on training networks whose weights can be transformed into some quantized representations with a minimal loss of performance (Fiesler et al., 1990; Hwang & Sung, 2014).", "startOffset": 155, "endOffset": 197}, {"referenceID": 2, "context": "It was also shown that 10 bits and 12 bits are enough to represent gradients and storing weights for implementing the stateof-the-art maxout networks even for training the network (Courbariaux et al., 2014).", "startOffset": 180, "endOffset": 206}, {"referenceID": 5, "context": "Although learning those bitwise weights as a Boolean concept is an NPcomplete problem (Pitt & Valiant, 1988), the bitwise networks have been studied in the limited setting, such as \u03bcperceptron networks where an input node is allowed to be connected to one and only one hidden node and its final layer is a union of those hidden nodes (Golea et al., 1992).", "startOffset": 334, "endOffset": 354}, {"referenceID": 12, "context": "A more practical network was proposed in (Soudry et al., 2014) recently, where the posterior probabilities of the binary weights were sought using the Expectation Back Propagation (EBP) scheme, which is similar to backpropagation in its form, but has some advantages, such as parameterfree learning and a straightforward discretization of the weights.", "startOffset": 41, "endOffset": 62}, {"referenceID": 4, "context": "The training procedure is similar to the ones with quantized weights (Fiesler et al., 1990; Hwang & Sung, 2014), except that the values we deal with are all bits, and the operations on them are bitwise.", "startOffset": 69, "endOffset": 111}, {"referenceID": 9, "context": "In this section we go over the details and results of the hand-written digit recognition task on the MNIST data set (LeCun et al., 1998) using the proposed BNN system.", "startOffset": 116, "endOffset": 136}, {"referenceID": 13, "context": "From the first round of training, we get a regular dropout network with the same setting suggested in (Srivastava et al., 2014), except the fact that we used the hyperbolic tangent for both weight compression and activation to make Table 1.", "startOffset": 102, "endOffset": 127}], "year": 2016, "abstractText": "Based on the assumption that there exists a neural network that efficiently represents a set of Boolean functions between all binary inputs and outputs, we propose a process for developing and deploying neural networks whose weight parameters, bias terms, input, and intermediate hidden layer output signals, are all binary-valued, and require only basic bit logic for the feedforward pass. The proposed Bitwise Neural Network (BNN) is especially suitable for resourceconstrained environments, since it replaces either floating or fixed-point arithmetic with significantly more efficient bitwise operations. Hence, the BNN requires for less spatial complexity, less memory bandwidth, and less power consumption in hardware. In order to design such networks, we propose to add a few training schemes, such as weight compression and noisy backpropagation, which result in a bitwise network that performs almost as well as its corresponding realvalued network. We test the proposed network on the MNIST dataset, represented using binary features, and show that BNNs result in competitive performance while offering dramatic computational savings.", "creator": "LaTeX with hyperref package"}}}