{"id": "1601.06569", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2016", "title": "Towards Resolving Unidentifiability in Inverse Reinforcement Learning", "abstract": "We consider a setting for Inverse Reinforcement Learning (IRL) where the learner is extended with the ability to actively select multiple environments, observing an agent's behavior on each environment. We first demonstrate that if the learner can experiment with any transition dynamics on some fixed set of states and actions, then there exists an algorithm that reconstructs the agent's reward function to the fullest extent theoretically possible, and that requires only a small (logarithmic) number of experiments. We contrast this result to what is known about IRL in single fixed environments, namely that the true reward function is fundamentally unidentifiable. We then extend this setting to the more realistic case where the learner may not select any transition dynamic, but rather is restricted to some fixed set of environments that it may try. We connect the problem of maximizing the information derived from experiments to submodular function maximization and demonstrate that a greedy algorithm is near optimal (up to logarithmic factors). Finally, we empirically validate our algorithm on an environment inspired by behavioral psychology.", "histories": [["v1", "Mon, 25 Jan 2016 11:50:43 GMT  (251kb,D)", "http://arxiv.org/abs/1601.06569v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kareem amin", "satinder singh"], "accepted": false, "id": "1601.06569"}, "pdf": {"name": "1601.06569.pdf", "metadata": {"source": "CRF", "title": "Towards Resolving Unidentifiability in Inverse Reinforcement Learning", "authors": ["Kareem Amin", "Satinder Singh"], "emails": ["amkareem@umich.edu", "baveja@umich.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "We assume that the (unknown) reward function of an agent who behaves optimally in the context of a Markov decision-making process is unlimited. The most basic formulation of the problem asks: Given a known environment1 E, and an optimal reward through the policy of reward, one can deduce from this the reward function R, which makes such behavior optimal for the MDP (E, R). However, the problem is fundamental to almost any study that involves behavior modeling. Consider an experimental psychologist who tries to understand the internal motivations of a subject, say a mouse, or consider a marketer who observes user behavior on a website in the hope of understanding the potential value for various perpetrators. As noted by Ng and Russell, a fundamental complication of the general goals of reward that we determine is the exact function of reward."}, {"heading": "2. SETTING AND PRELIMINARIES", "text": "We designate an environment of a tuple E = (S, P, \u03b3), where S = {1,..., d} is a finite series of states in which the agent can find himself, A is a finite series of actions available to the agent, and P is a collection of transition dynamics for each a-A reward, so that P = {Pa} a-A reward for each a-A reward, so that P = {Pa} a-A reward represents the probability of transferring to the state s \u2032 when selecting measures a. The discount factor of the agent is a (0, 1).We represent a reward function of the agent as a vector R (s), which indicates the payout for reaching the state. Note that a common choice of the marcovian fixes the environment E with reward function R."}, {"heading": "3. ON IDENTIFICATION", "text": "In this section, we will give a more nuanced characterization of what it means to identify a reward function. We will argue that there are several types of uncertainty involved in the identification of R, which we categorize as representative unidentifiability and experimental unidentifiability \u2212 \u2212 \u2212 \u2212 \u2212 Furthermore, we will argue that the first type of reward is somewhat superficial and should be ignored, while the second type of reward can be ignored. We will begin with a definition of R and R \u00b2 reward functions defined in the same state space. We will say that R and R \u00b2 are behavioral equivalents if for each environment (also defined on S), the actor whose reward function R behaves identically with the actor whose reward function is R. Two reward vectors R defined on S are behavioral equivalents, denoted R \u00b2 if for each type of action, transition function, and discount."}, {"heading": "No amount of experimentation can remove the representational unidentifiability from the setting, depicted here by the darker shaded region. (c) Nevertheless, adding the constraints K(E\u2032, \u03c0\u2032)", "text": "In other words, he claims that the outstanding task in IRL is a member of [R], not the literal R. Secondly, if the true reward function R is not constant (i.e. R 6 [~ 0]), he demands that the algorithm identifies R (up to representative decisions). However, if the agent actually represents a reward function R, the reward function R is not constant (i.e. R 6 [~ 0]), he demands that the algorithm identifies R (up to representative decisions). However, if the agent really has a reward function of ~ 0, the algorithm may output anything. In other words, the algorithm is only allowed to behave arbitrarily if the agent behaves arbitrarily. 5We also note that Definition 2 can be loosened to give an idea of a closer identification we are rewarding."}, {"heading": "4. OMNIPOTENT EXPERIMENTER SETTING", "text": "Formally, any environment E selected by the experimenter belongs to a class U \u0443 that contains an environment (S, A, P, \u03b3) for any group of transition dynamics P to S. We call this the almighty experimenter setting. We will describe an algorithm for the almighty experimenter setting that identifies R by using only O (log (d /)) experiments. Although the almighty experimenter is extremely powerful, the result shows that the guarantee obtained in a repeated IRL environment can be much stronger than in a standard IRL environment with a single environment. Furthermore, it illustrates the distinction between experimental indeterminability and representative indeterminability."}, {"heading": "4.1 Omnipotent Identification Algorithm", "text": "The algorithm runs in two phases, both of which involve simple binary searches. The first stage identifies states smin, smax so that R (smin) = Rmin and R (smax) = Rmax. The second stage identifies for each s'S such a transition that R (s) = \u03b1sRmin + (1 \u2212 \u03b1s) Rmax. The second stage therefore identifies in describing the algorithm that R (s) = \u03b1sRmin + (1 \u2212 \u03b1s) Rmax. The environment selected by the algorithm is entirely determined by its choices for Pa1 and Pa2. If indeed, in the omnipotent experimental environment, one can reduce the remaining actions in A by making the remaining actions either a1 or a2. 6We first address the task of smax. Suppose we have two candidate s and s."}, {"heading": "5. RESTRICTED EXPERIMENTER SETTING", "text": "We now consider an environment in which the experimenter has a limited universe of environments to choose from. U does not have to contain every possible transition dynamics, a prerequisite for executing the binary search algorithm of the previous section. Therefore, the best the experimenter could ever hope for is to try every environment in U. Thus, the experimenter receives all available information about the reward function of Agent R. Therefore, we are more interested in maximizing the information gained by the experimenter while minimizing the number of experiments performed. In practice, observing an agent can be expensive or difficult to obtain, and so, even for a small budget of experiments B, the learner wants to select environments from U that maximize the experimental indeterminability. Once a sequence of experiments E has been observed, we know that R is consistent with the observed sequence, if and only if R, K (E) is present."}, {"heading": "5.1 Generalized Selection Heuristics", "text": "In the standard environment (single environment) \u2212 \u2212 R (1) (there are two natural selection pairs), the learner must now make a choice between one of the rewards in K (E, \u03c0). Heuristics proposed by [10] are motivated by the idea that for a given state s, the reward function, which maximizes the difference in Q value between the observed measures in state s, \u03c0 (s) and any other action a 6 = \u03c0 (s), provides the strongest explanation for the behavior observed by the agent. Therefore, a reasonable linear selection criterion is to maximize the sum of these differences between states. Adding a regulation term promotes the selection of reward functions, which are also sparse. Adding these together, the default selection is heuristic for a single environment IRL to maximize R: The results of S (min a6 = \u03c0 (s) (s) (hay term, which are also sparse. Adding them together, the default selection criterion an individual environment IRL is to maximize R: the results of S (min a6 = \u03c0 (s) (hay term, which are \u2212 a \u2212 Pa) (\u2212 \u2212)."}, {"heading": "5.2 Adaptive Experimentation", "text": "We must first decide what we mean by \"informative.\" We propose that for a series of experiments E (either in the basic or trajectory setting) is a natural goal to minimize the mass of the resulting space of possible rewards K (E) in relation to any measurement (or distribution) E. This corresponds to the natural goal of reducing the volume of K (E) as much as possible. So we define: Vol\u00b5 (E) = [R).K (E) d\u00b5 (R).K (E) We will find it convenient to consider this as a maximization problem E (E), and therefore we define f (E) = V \u2212 Vol\u00b5 (E).K (E).P (E).P (E).( E).P (E).E (E).P. (E).P. (.P).P. (.P).P. (.P).P. (.P. (.P).P. (.P).P (.P (.P).E (.E).E (.E. (E).E (E).E (E).P. (.P (.P).P (.P.).P (.P (.P).P (.P).P (.P (.P).P (.P).P (.P (.P).P (.P).E (.E).E (.E).E (.E (.E).E (.E).E (.E (.E).E (.E (.E).E (.E).E (.E (.E).E (.E).E (.E (.E).E (.E).E (.E (.E (.E).E (.E).E (.E (.E).E (.E). P.).E (.E (.E.).E (.E (.E).E (.E.).E (.P.).P (.P (.P.).P (.P (.P.).P (.P (.P (.P (.P."}, {"heading": "6. EXPERIMENTAL ANALYSIS", "text": "We imagine that we have an agent who will be able to define a particular environment, and so we will alternately refer to an environment that is considered a labyrinth. To motivate the value of repeated experiments, we remember a restricted environment for the learner. For example, the learner cannot make an action cause the agent to travel from the bottom corner of the laboratory to the top corner. The learner can change the dynamics of the environment as much as he can construct it."}, {"heading": "7. CONCLUSIONS", "text": "First, we separate the causes of the indeterminability of IRL problems into two classes: representative and experimental. We argue that the indeterminability of representations is superficial, which leads us to redefine the problem of identification in IRL according to Definition 2. While previous work does not distinguish between these two classes, we show that this allows algorithms to be designed to eliminate experimental indeterminability while providing formal guarantees. In the process, we derive a new model for IRL in which the learner can observe behavior in multiple environments, a model that we consider interesting in itself, but also crucial for eliminating experimental indeterminability. We provide an algorithm for a very powerful learner who can observe the behavior of agents in any environment, and show that the algorithm -identifies a reward of agents defined on d states, while implementing the behavior only on O (log /) environments."}, {"heading": "A. PROOF OF THEOREM 3", "text": "Theorem 6: For all R, R \".Rd, R.\" we concentrate on the case in which both R \".R\".R \".B\".B \".B\".B \".B\".B \".B\".B \".B\".B \".B\".B \".B\".B \".B\".B \".B\".B \".B\".B \".B\".B \".B\".B \".B\".B \".B\".B \".B\".B \".B.B.\".B.B. \".B.B\".B \".B\".B \".B\".B \".B\".B \".D\".D \".D\".D \".B\".B \".B\".B \".B\".D \".D\".D \".D\".D. B \".B\".B \".D\".D \".D\".D."}, {"heading": "B. PROOF OF GREEDY\u2019S PERFORMANCE", "text": "In view of the definition of K (E + o) = K (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E) (E (E) (E) (E) (E) (E (E) (E) (E (E) (E (E) (E) (E (E) (E (E) (E (E) (E) (E (E) (E (E) (E) (E (E) (E (E) (E (E) (E) (E) (E (E) (E (E (E) (E) (E) (E (E) (E) (E (E) (E (E) (E) (E) (E) (E (E) (E) (E) (E) (E) (E) (E (E) (E) (E (E) (E) (E) (E) (E) (E) (E) (E) (E) (E (E) (E) (E"}], "references": [{"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["P. Abbeel", "A. Coates", "M. Quigley", "A.Y. Ng"], "venue": "Advances in neural information processing systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Making rational decisions using adaptive utility elicitation", "author": ["U. Chajewska", "D. Koller", "R. Parr"], "venue": "In AAAI/IAAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Learning for control from multiple demonstrations", "author": ["A. Coates", "P. Abbeel", "A.Y. Ng"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Apprenticeship learning for helicopter control", "author": ["A. Coates", "P. Abbeel", "A.Y. Ng"], "venue": "Communications of the ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Adaptive submodularity: A new approach to active learning and stochastic optimization", "author": ["D. Golovin", "A. Krause"], "venue": "In COLT, pages 333\u2013345,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Interactive submodular set cover", "author": ["A. Guillory", "J. Bilmes"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Active learning for reward estimation in inverse reinforcement learning", "author": ["M. Lopes", "F. Melo", "L. Montesano"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Hit-and-run mixes fast", "author": ["L. Lov\u00e1sz"], "venue": "Mathematical Programming,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A.Y. Ng", "S.J. Russell"], "venue": "In Icml,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Bayesian inverse reinforcement learning", "author": ["D. Ramachandran", "E. Amir"], "venue": "Urbana, 51:61801,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Maximum margin planning", "author": ["N.D. Ratliff", "J.A. Bagnell", "M.A. Zinkevich"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Regret-based reward elicitation for markov decision processes", "author": ["K. Regan", "C. Boutilier"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Robust policy computation in reward-uncertain mdps using nondominated policies", "author": ["K. Regan", "C. Boutilier"], "venue": "In AAAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Eliciting additive reward functions for markov decision processes", "author": ["K. Regan", "C. Boutilier"], "venue": "In IJCAI Proceedings-International Joint Conference on Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Preference elicitation and inverse reinforcement learning", "author": ["C.A. Rothkopf", "C. Dimitrakakis"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Effective reinforcement learning for mobile robots", "author": ["W.D. Smart", "L.P. Kaelbling"], "venue": "In Robotics and Automation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "A game-theoretic  approach to apprenticeship learning", "author": ["U. Syed", "R.E. Schapire"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Theory of games and economic behavior (60th Anniversary Commemorative Edition)", "author": ["J. Von Neumann", "O. Morgenstern"], "venue": "Princeton university press,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B.D. Ziebart", "A.L. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "In AAAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}], "referenceMentions": [{"referenceID": 9, "context": "Inverse reinforcement learning (IRL), first introduced by Ng and Russell ([10]), is concerned with the problem of inferring the (unknown) reward function of an agent behaving optimally in a Markov decision process.", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "The most basic formulation of the problem asks: given a known environment E, and an optimal agent policy \u03c0, can we deduce the reward function R which makes \u03c0 optimal for the MDP (E,R)? IRL has seen a number of applications in the development of autonomous systems, such as autonomous vehicle operation, where even a cooperative (human) agent might have great difficultly describing her incentives [17, 2, 1, 5].", "startOffset": 397, "endOffset": 410}, {"referenceID": 1, "context": "The most basic formulation of the problem asks: given a known environment E, and an optimal agent policy \u03c0, can we deduce the reward function R which makes \u03c0 optimal for the MDP (E,R)? IRL has seen a number of applications in the development of autonomous systems, such as autonomous vehicle operation, where even a cooperative (human) agent might have great difficultly describing her incentives [17, 2, 1, 5].", "startOffset": 397, "endOffset": 410}, {"referenceID": 0, "context": "The most basic formulation of the problem asks: given a known environment E, and an optimal agent policy \u03c0, can we deduce the reward function R which makes \u03c0 optimal for the MDP (E,R)? IRL has seen a number of applications in the development of autonomous systems, such as autonomous vehicle operation, where even a cooperative (human) agent might have great difficultly describing her incentives [17, 2, 1, 5].", "startOffset": 397, "endOffset": 410}, {"referenceID": 4, "context": "The most basic formulation of the problem asks: given a known environment E, and an optimal agent policy \u03c0, can we deduce the reward function R which makes \u03c0 optimal for the MDP (E,R)? IRL has seen a number of applications in the development of autonomous systems, such as autonomous vehicle operation, where even a cooperative (human) agent might have great difficultly describing her incentives [17, 2, 1, 5].", "startOffset": 397, "endOffset": 410}, {"referenceID": 9, "context": "Since the true reward function is fundamentally unidentifiable, much of the previous work in IRL has been concerned with the development of heuristics which prefer certain rewards as better explanations for behavior than others [10, 20, 11].", "startOffset": 228, "endOffset": 240}, {"referenceID": 19, "context": "Since the true reward function is fundamentally unidentifiable, much of the previous work in IRL has been concerned with the development of heuristics which prefer certain rewards as better explanations for behavior than others [10, 20, 11].", "startOffset": 228, "endOffset": 240}, {"referenceID": 10, "context": "Since the true reward function is fundamentally unidentifiable, much of the previous work in IRL has been concerned with the development of heuristics which prefer certain rewards as better explanations for behavior than others [10, 20, 11].", "startOffset": 228, "endOffset": 240}, {"referenceID": 9, "context": "Prior work in IRL has mostly focused on inferring an agent\u2019s reward function from data acquired from a fixed environment [10, 2, 4, 20, 11, 18, 14].", "startOffset": 121, "endOffset": 147}, {"referenceID": 1, "context": "Prior work in IRL has mostly focused on inferring an agent\u2019s reward function from data acquired from a fixed environment [10, 2, 4, 20, 11, 18, 14].", "startOffset": 121, "endOffset": 147}, {"referenceID": 3, "context": "Prior work in IRL has mostly focused on inferring an agent\u2019s reward function from data acquired from a fixed environment [10, 2, 4, 20, 11, 18, 14].", "startOffset": 121, "endOffset": 147}, {"referenceID": 19, "context": "Prior work in IRL has mostly focused on inferring an agent\u2019s reward function from data acquired from a fixed environment [10, 2, 4, 20, 11, 18, 14].", "startOffset": 121, "endOffset": 147}, {"referenceID": 10, "context": "Prior work in IRL has mostly focused on inferring an agent\u2019s reward function from data acquired from a fixed environment [10, 2, 4, 20, 11, 18, 14].", "startOffset": 121, "endOffset": 147}, {"referenceID": 17, "context": "Prior work in IRL has mostly focused on inferring an agent\u2019s reward function from data acquired from a fixed environment [10, 2, 4, 20, 11, 18, 14].", "startOffset": 121, "endOffset": 147}, {"referenceID": 13, "context": "Prior work in IRL has mostly focused on inferring an agent\u2019s reward function from data acquired from a fixed environment [10, 2, 4, 20, 11, 18, 14].", "startOffset": 121, "endOffset": 147}, {"referenceID": 7, "context": "Previous applications of active learning to IRL have considered settings where, in a single environment, the learner can query the agent for its action in some state [8], or for information about its reward [13].", "startOffset": 166, "endOffset": 169}, {"referenceID": 12, "context": "Previous applications of active learning to IRL have considered settings where, in a single environment, the learner can query the agent for its action in some state [8], or for information about its reward [13].", "startOffset": 207, "endOffset": 211}, {"referenceID": 11, "context": "There is prior work on using data collected from multiple \u2014 but exogenously fixed \u2014 environments to predict agent behavior [12].", "startOffset": 123, "endOffset": 127}, {"referenceID": 19, "context": "There are also applications where methods for single-environment MDPs have been adapted to multiple environments [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "In the economics literature, the problem of inferring an agent\u2019s utility from behavior has long been studied under the heading of utility or preference elicitation [3, 19, 15, 16, 13, 15].", "startOffset": 164, "endOffset": 187}, {"referenceID": 18, "context": "In the economics literature, the problem of inferring an agent\u2019s utility from behavior has long been studied under the heading of utility or preference elicitation [3, 19, 15, 16, 13, 15].", "startOffset": 164, "endOffset": 187}, {"referenceID": 14, "context": "In the economics literature, the problem of inferring an agent\u2019s utility from behavior has long been studied under the heading of utility or preference elicitation [3, 19, 15, 16, 13, 15].", "startOffset": 164, "endOffset": 187}, {"referenceID": 15, "context": "In the economics literature, the problem of inferring an agent\u2019s utility from behavior has long been studied under the heading of utility or preference elicitation [3, 19, 15, 16, 13, 15].", "startOffset": 164, "endOffset": 187}, {"referenceID": 12, "context": "In the economics literature, the problem of inferring an agent\u2019s utility from behavior has long been studied under the heading of utility or preference elicitation [3, 19, 15, 16, 13, 15].", "startOffset": 164, "endOffset": 187}, {"referenceID": 14, "context": "In the economics literature, the problem of inferring an agent\u2019s utility from behavior has long been studied under the heading of utility or preference elicitation [3, 19, 15, 16, 13, 15].", "startOffset": 164, "endOffset": 187}, {"referenceID": 9, "context": "Theorem 1 (Ng, Russell [10]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "Thus, a binary search can be conducted on each \u03b1s \u2208 [0, 1] independently in order to determine an approximation of the \u03b1\u2217 s such that R(s) = \u03b1 \u2217 sRmin + (1\u2212 \u03b1\u2217 s)Rmax.", "startOffset": 52, "endOffset": 58}, {"referenceID": 9, "context": "The heuristic suggested by [10] is motivated by the idea that for a given state s, the reward function that maximizes the difference in Q-value between the observed action in state s, \u03c0(s), and any other action a 6= \u03c0(s), gives the strongest explanation of the behavior observed from the agent.", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "There are other selection rules for the single-environment setting, which are generalizable to the repeated experimentation setting, including heuristics for the infinite state setting, trajectory heuristics, as well as approaches already adapted to multiple environments [12].", "startOffset": 272, "endOffset": 276}, {"referenceID": 9, "context": "Due to space constraints, we discuss only the foundational approach of [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "Furthermore, as we will see later in this section, the objective is a monotone submodular function, an assumption well-studied in the active learning literature [7, 6], allowing us to prove guarantees for a greedy algorithm.", "startOffset": 161, "endOffset": 167}, {"referenceID": 5, "context": "Furthermore, as we will see later in this section, the objective is a monotone submodular function, an assumption well-studied in the active learning literature [7, 6], allowing us to prove guarantees for a greedy algorithm.", "startOffset": 161, "endOffset": 167}, {"referenceID": 6, "context": "al ([7]), in their work on interactive set cover.", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "from the uniform distribution on [0, 1].", "startOffset": 33, "endOffset": 39}, {"referenceID": 8, "context": "In the policy observation setting, 1000 samples are first drawn from the consistent set K(E) using a hit-and-run sampler [9], which is an MCMC method for uniformly sampling highdimensional convex sets in polynomial time.", "startOffset": 121, "endOffset": 124}, {"referenceID": 9, "context": "This problem occurs in standard IRL, and one approach ([10]) is to select a large lambda before this transition, hence our choice of \u03bb = 0.", "startOffset": 55, "endOffset": 59}], "year": 2016, "abstractText": "We consider a setting for Inverse Reinforcement Learning (IRL) where the learner is extended with the ability to actively select multiple environments, observing an agent\u2019s behavior on each environment. We first demonstrate that if the learner can experiment with any transition dynamic on some fixed set of states and actions, then there exists an algorithm that reconstructs the agent\u2019s reward function to the fullest extent theoretically possible, and that requires only a small (logarithmic) number of experiments. We contrast this result to what is known about IRL in single fixed environments, namely that the true reward function is fundamentally unidentifiable. We then extend this setting to the more realistic case where the learner may not select any transition dynamic, but rather is restricted to some fixed set of environments that it may try. We connect the problem of maximizing the information derived from experiments to active submodular function maximization, and demonstrate that a greedy algorithm is near optimal (up to logarithmic factors). Finally, we empirically validate our algorithm on an environment inspired by behavioral psychology.", "creator": "LaTeX with hyperref package"}}}