{"id": "1601.06615", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2016", "title": "A Taxonomy of Deep Convolutional Neural Nets for Computer Vision", "abstract": "Traditional architectures for solving computer vision problems and the degree of success they enjoyed have been heavily reliant on hand-crafted features. However, of late, deep learning techniques have offered a compelling alternative -- that of automatically learning problem-specific features. With this new paradigm, every problem in computer vision is now being re-examined from a deep learning perspective. Therefore, it has become important to understand what kind of deep networks are suitable for a given problem. Although general surveys of this fast-moving paradigm (i.e. deep-networks) exist, a survey specific to computer vision is missing. We specifically consider one form of deep networks widely used in computer vision - convolutional neural networks (CNNs). We start with \"AlexNet\" as our base CNN and then examine the broad variations proposed over time to suit different applications. We hope that our recipe-style survey will serve as a guide, particularly for novice practitioners intending to use deep-learning techniques for computer vision.", "histories": [["v1", "Mon, 25 Jan 2016 14:25:07 GMT  (1306kb,D)", "http://arxiv.org/abs/1601.06615v1", "Published in Frontiers in Robotics and AI (this http URL)"]], "COMMENTS": "Published in Frontiers in Robotics and AI (this http URL)", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["suraj srinivas", "ravi kiran sarvadevabhatla", "konda reddy mopuri", "nikita prabhu", "srinivas s s kruthiventi", "r venkatesh babu"], "accepted": false, "id": "1601.06615"}, "pdf": {"name": "1601.06615.pdf", "metadata": {"source": "CRF", "title": "A Taxonomy of Deep Convolutional Neural Nets for Computer Vision", "authors": ["Suraj Srinivas", "Ravi Kiran Sarvadevabhatla", "Konda Reddy Mopuri", "Nikita Prabhu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 Introduction to Convolutional Neural Networks", "text": "The idea of a Convolutional Neural Network (CNN) is not new; this model had proved good for handwritten number recognition by [LeCun et al., 1998], but fell out of favour due to the inability of these networks to scale to much larger images, largely due to memory and hardware constraints and the unavailability of large amounts of training data. As computing power increased thanks to the widespread availability of GPUs and the introduction of large-scale datasets such as the ImageNet (see [Russakovsky et al., 2015]) and the MIT Places dataset (see [Zhou et al., 2014]), it was possible to train larger, more complex models, as first demonstrated by the popular AlexNet model previously discussed."}, {"heading": "2.1 Building Blocks of CNNs", "text": "In this section, we look at the basic building blocks of CNNs in general. This assumes that the reader is familiar with traditional neural networks, which we will call \"fully connected layers\" in this article. Figure 1 shows a representation of weights in the AlexNet model. While the first five layers are wavy, the last three layers are fully connected layers."}, {"heading": "2.1.1 Why convolutions?", "text": "In fact, most people who are able to move are able to move, to move and to move, to move, to move, to move, to move and to move, to move, to move and to move, to move and to move, to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move."}, {"heading": "2.1.2 Max-Pooling", "text": "The neocognitron model inspired the modeling of simple cells as coils. In the same way, the complex cells can be modeled as a max pooling operation. One can imagine this process as a max filter, in which each n \u00b7 n region is replaced by its maximum value. This process serves two purposes: 1. It selects the highest activation in a local region and thus ensures a low degree of spatial invariance. This corresponds to the process of complex cells. 2. It reduces the size of the activation for the next layer by a factor n2. If the activation size is smaller, we need a smaller number of parameters that we will have to learn in the later layers."}, {"heading": "2.1.3 Non-linearity", "text": "Deep networks usually consist of coils followed by a nonlinear operation after each layer. This is necessary because cascading linear systems (such as coils) are another linear system. Nonlinearity between layers ensures that the model is more meaningful than a linear model. In theory, no nonlinearity is more meaningful than any other as long as it is continuous, limited, and monotonously increasing (see [Hornik, 1991]). Traditional forward-facing neural networks use the sigmoid (\u03c3 (x) = 1 1 + e \u2212 x) or the tanh (tanh (x) = ex \u2212 e + e \u2212 x) nonlinearity. Modern revolutionary networks, however, use the ReLU (x) = max (0, x)))) nonlinearity. CNNs with this nonlinearity model have evolved faster, as [Nair and Hinton, 2010] have shown."}, {"heading": "2.2 Depth", "text": "The Universal Approximation Theorem by [Hornik, 1991] states that a neural network with a single hidden layer is sufficient to model a continuous function. [Bengio, 2009], however, showed that such networks require an exponentially large number of neurons compared to a neural network with many hidden layers. [Romero et al., 2014] and [Ba and Caruana, 2014] explicitly showed that a deeper neural network can be trained much better than a comparatively flat network. Although the motivation for creating deeper networks was clear, for a long time researchers did not have an algorithm that could efficiently train neural networks with more than three layers. [Hinton et al., 2006] with the introduction of greedy layer-soft pre-training courses by [Hinton et al., this enabled researchers to train much deeper networks, which played an important role in bringing the so-called deep learning systems into the mainstream machine learning layers of modern Alexe4 and Alexe7 networks, respectively."}, {"heading": "2.3 Learning algorithm", "text": "A powerful, expressive model is no use without an algorithm to efficiently learn the parameters of the model. Greedy, layered training approaches in the pre-AlexNet era tried to create such an efficient algorithm. However, for computer vision tasks, it turned out that a simpler, supervised training method was sufficient to learn a powerful model. Learning is generally done by minimizing certain loss functions. Classification-based tasks use the Softmax loss function or the sigmoid cross entropy function, while those that are regression use the Euclidean error function. In the example of Figure 1, the output of the FC8 layer is trained to represent one of a thousand classes of the dataset."}, {"heading": "2.3.1 Gradient-based optimization", "text": "Neural networks are generally trained using the backpropogation algorithm (see [Rumelhart et al., 1988]), which uses the chain rule to speed up the calculation of the gradient for gradient descent (GD). However, for datasets with thousands (or more) of data points, the use of GD is impractical. In such cases, an approach called Stochastic Gradient Descent (SGD) is used. It has been found that training with SGD generalizes much better than training with GD. However, one drawback is that the convergence of SGD is very slow. To counteract this, SGD is typically used with a mini-batch, in which the mini-batch typically contains a small number of data points (approximately 100). Momentum (see [Polyak, 1964]) belongs to a family of methods aimed at accelerating convergence of GSD."}, {"heading": "2.3.2 Dropout", "text": "Common approaches such as \"1 or\" 2 regulation of the weights of the neural network have proved insufficient in this regard. \"Dropout\" is a powerful regulation method introduced by [Hinton et al., 2012] and has been proven to work well for large neural networks. To exploit dropouts, we randomly eject neurons with a probability p during training. As a result, only a random subset of neurons are trained in a single iteration of the SGD. At test times, we use all neurons, but simply multiply the activation of each neuron with p to take scaling into account. [Hinton et al., 2012] showed that this method corresponds to the formation of a large ensemble of neural networks with common parameters, and then use their geometric means to achieve a single prediction.Many extensions, such as \"DropConnect\" by another networker who worked better in 2013, have proved insufficient."}, {"heading": "2.4 Tricks to increase performance", "text": "Although the techniques and components described above are theoretically sound, certain tricks are crucial to achieving state-of-the-art performance. It is widely known that machine learning is more powerful when more data is available. Data augmentation is a process in which some geometric transformations are applied to training data to increase their number.Some examples of commonly used geometric transformations include random cropping, RGB jittering, image rotation and small rotations. It has been found that using augmented data typically increases performance by about 3% (see [Chatfield et al., 2014]).Also known is the fact that a group of models performs better than one. Therefore, it is common to train multiple CNNs and make their predictions on average at test time.The use of ensembles typically increases accuracy by 1-2% (see [Simonyan and Zisserman, 4b and 2014], [Szegeal]."}, {"heading": "2.5 Putting it all together: AlexNet", "text": "The building blocks discussed above largely describe AlexNet as a whole. As shown in Figure 1, only layers 1,2 and 5 contain maximum pooling, while dropouts are only applied to the last two fully interconnected layers, as they contain most of the parameters. Layer 1 and 2 also contain Local Reaction Normalization, which was not discussed, as [Chatfield et al., 2014] showed that their absence has no effect on performance. This network was trained on ILSVRC training data in 2012, which included 1.2 million training images from 1000 classes, which were trained on 2 GPUs over the course of a month. Today, the same network can be trained with more powerful GPUs in less than a week (see [Chatfield et al., 2014]). Hyperparameters of learning algorithms such as learning speed, dynamics, failure rate and weight loss were manually matched. It is also interesting to point out trends in the nature of features learned in different layers."}, {"heading": "2.6 Using Pre-trained CNNs", "text": "One of the main reasons for the success of the AlexNet model was that it was possible to use the pre-trained model directly for various other tasks for which it was not originally intended. It became amazingly easy to download a learned model and then easily adapt it to the respective application. We describe two ways of using models in this way."}, {"heading": "2.6.1 Fine-tuning", "text": "Considering a model that is trained for image classification, the question is how to modify it to perform a different (but related) task. The answer is simply to use the trained weights as initialization and perform SGD again for this new task. Typically, one uses a learning rate that is much lower than that used to learn the original mesh. If the new task is very similar to the task of image classification (with similar categories), one does not need to re-learn many layers. The number of layers that can be re-learned also depends on the number of data points available to learn the new task. The more data, the higher the number of layers that can be re-learned. The reader is encouraged to refer to [Yosinski et al., 2014] for more precise guidelines."}, {"heading": "2.6.2 CNN activations as features", "text": "As mentioned earlier, the later levels in AlexNet seem to learn visual semantic attributes, which are crucial for performing a 1000-way classification, and since they represent a variety of classes, the FC7 activation of an image can be used as a generic feature descriptor, which has proven to be better than handmade features such as SIFT or HoG for various computer vision tasks. [Donahue et al., 2013] first introduced the idea of using CNN activations as features and conducted tests to determine their suitability for different tasks. [Babenko et al., 2014] suggested using the activation of fully connected layers for image recovery, which they called \"Neural Codes.\" [Razavian et al., 2014] used these activations for different tasks and concluded that standard CNN functions can serve as a hard-to-beat baseline for many tasks. [Hariharmal, 2014] Activations used across layers."}, {"heading": "2.7 Improving AlexNet", "text": "AlexNet's performance motivated a number of CNN-based approaches, all aimed at improving performance beyond AlexNet. Just as AlexNet was the winner of the ILSVRC challenge in 2012, a CNN-based network trick called Overfeat by [Sermanet et et al., 2013a] was the most powerful player in ILSVRC-2013. Their key finding was that the formation of a revolutionary network for simultaneous classification, localization and detection of objects in images can increase the classification accuracy and detection and localization accuracy of all tasks. Given its multitask learning paradigm, we discuss Overfeat when we discuss hybrid CNNs and multi-task learning in Section 3.5.GoogleNet by [Szegedy et al., 2014], the most powerful player in ILSVRC-2014, notes that very deep networks can lead to significant increases in classification performance. As naively increasing the number of layers, they lead to a number of design parameters, \"using a number of authors."}, {"heading": "3 CNN Flavours", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Region-based CNNs", "text": "Most CNNs trained for image recognition are trained with a dataset of images containing a single object. At test time, even in the case of multiple objects, CNN can still predict a single class. Finally, this inherent problem with the design of CNNs is not limited to image classification alone. For example, the problem of object recognition and localization requires not only classification of the image, but also estimation of the class and exact position of the object (s) present in the image. Object recognition is a challenge because we potentially want to detect multiple objects of different sizes within a single image. It usually requires editing the image patchwise, looking for the presence of objects. Neural networks have been used in this way to detect specific objects such as faces in [Vaillant et al., 1994] and [Rowley et al., 1998] and for pedestrians through [Sermanet al, 2013c]."}, {"heading": "3.2 Fully Convolutional Networks", "text": "In fact, most people who fight for the rights of women and men are fighting for the rights of men and women, but not for the rights of women and men, but for the rights of women and men who fight for the rights of men and women."}, {"heading": "3.3 Multi-modal networks", "text": "The success of CNNs on standard RGB tasks is of course extended to work on other perceptual modalities such as RGB-D and motion information in the videos. Recently, there has been increasing evidence of the successful adaptation of CNNs to learn successive representation patterns from depth images. [Socher et al., 2012] used information from colors and depth models to address the problem of classification. In their approach, a single layer of CNN extracts forms low-level properties separated from both the RGB and depth images. These low-level properties are used in a set of RNNNs for embedding in a lower dimension.The confrontation of the resulting features forms the input to the final soft layer. Couprie et al, 2013] extends the CNN method of [Farabet et al] to designate indoor scenes by treating depth information as an additional channel of information."}, {"heading": "3.4 CNNs with RNNs", "text": "This loop structure allows the RNN to have an internal memory and to learn temporal patterns in data. Figure 5 shows the unrolled version of a simple RNN applied to a toy example of sequence addition. The problem is defined as follows: Include a positive number corresponding to the input in due time. Output in due time is considered by St = t, i = 1 aiWe consider a very simple RNN with only one hidden layer. The RNN can be described by equations."}, {"heading": "3.4.1 Action recognition", "text": "Detecting human actions from video has long been a central problem in the tasks of video understanding and video surveillance. Actions that extend over a limited period of time are excellent candidates for a common CNN-RNN model. In particular, we discuss the model proposed by [Donahue et al., 2014]. They use RGB and optical flow functions to jointly train a variant of Alexnet in combination with a single layer of LSTM (256 hidden units). Single frames of the video are scanned, routed through the trained network and classified individually, and the final prediction is achieved by averaging all frames."}, {"heading": "3.4.2 Image and video captioning", "text": "Another important component of scene understanding is the textual description of images and videos. Relevant textual descriptions also help supplement image information and form useful queries to retrieve. RNNs (LSTMs) have long been used for machine translation (see [Bahdanau et al., 2014, Cho et al., 2014], which has motivated their use for the purpose of capturing images. [Vinyals et al., 2014] have developed an end-to-end system by first encoding an image using a CNN feature and then using the encoded image as input into a language that RNN generates. [Carpathy and Fei-Fei, 2014] suggest a multi-modal deep network that balances out various interesting regions of the image, represented by a CNN feature, with associated words. The learned correspondence is then used as input into a language that RNN generates."}, {"heading": "3.4.3 Visual Question answering", "text": "A true understanding of an image should enable a system not only to make a statement about it, but also to answer related questions. Therefore, answering questions based on visual concepts in an image is the next natural step for machine understanding of algorithms. However, this requires that the system model both the text question and the image representation before it generates an answer based on both the question and the image. A combination of CNN and LSTM has also proven effective in this task, as evidenced by the work of [Malinowski et al., 2015], which trains an LSTM layer to accept both the question and a CNN representation of the image and generate the answer. [Gao et al., 2015] use two LSTM's with common weighting along with a CNN for the task. Their experiments are conducted on a multilingual dataset that includes Chinese questions and answers along with its English translation to provide a visual answer to the scenes [Antol, 2015]."}, {"heading": "3.5 Hybrid learning methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.5.1 Multi-task learning", "text": "Multitask learning is essentially a machine learning paradigm, where the goal is to train the learning system to perform multiple tasks at the same time. Multitask learning frameworks tend to take advantage of shared representations that exist between tasks to achieve better generalization performance than counterparts designed solely for a single task. CNNs use multiple task learning with different approaches. A class of approaches uses a multiple task loss function with hyperparameters that typically regulate task losses. [Girshick, 2015] use multiple task loss to train their network together for classification and Bounding Box regression tasks, thereby improving object recognition performance. [Zhang et al, 2014] propose a face recognition network that adaptively weights auxiliary tasks (e.g. head task estimation, gender classification, age assessment)."}, {"heading": "3.5.2 Similarity learning", "text": "Apart from classification, CNNs can also be used for tasks such as metric learning and rank learning. Instead of asking CNN to identify objects, we can instead ask it to verify whether two images contain the same object or not. In other words, we ask CNN to learn which images are similar and which are not. Image search is an application where such questions are routinely asked. Structurally, Siamese networks are similar to dual-stream networks that were previously discussed, but the difference is that both \"streams\" have identical weights. Siamese networks are made up of two separate (but identical) networks in which two images are fed in as input. Their activations are combined at a later level, and the output of the network consists of a single number or metric that is an idea of the distance between the images. Training is performed in such a way that images that are considered to be similar have a lower output score than images that are considered to be different. [Bromesiet et, a fictional idea later used to differentiate between these networks]."}, {"heading": "4 Open problems", "text": "In this section we briefly mention some open research problems in the field of deep learning, particularly those of interest to computer vision. Several of these problems are already being addressed in several papers. \u2022 Training CNNs requires tuning a large number of hyperparameters, including those relating to model architecture. \u2022 [Nguyen et al., 2014] showed that artificial images can be produced that lead to high confidence, false prediction. In a similar work line [Szegedy et al., 2013] it was shown that natural images can be imperceptibly modified to produce a completely different classification label. Although [Goodfellow et al., 2014b] attempts were made to reduce the impact of such contradictory examples, it remains to be seen whether this can be completely eliminated. \u2022 We are known to produce a large classification mark."}, {"heading": "5 Concluding remarks", "text": "In this article, we have explored the use of deep learning networks - especially revolutionary neural networks - for computer vision, which has allowed complicated hand-guided algorithms to be replaced by single monolithic algorithms trained in end-to-end ways. However, despite our best efforts, it may not be possible to capture the full range of deep learning research - even for computer vision - in this essay. We refer the reader to other reviews, particularly those by [Bengio, 2009], [LeCun et al., 2015] and [Schmidhuber, 2015]. These reviews are more focused on deep learning in general, while ours is more focused on computer vision. We hope that our article will be useful for visionaries who are beginning to engage with deep learning."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["Ba", "Caruana", "J. 2014] Ba", "R. Caruana"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Neural codes for image retrieval", "author": ["Babenko et al", "A. 2014] Babenko", "A. Slesarev", "A. Chigorin", "V. Lempitsky"], "venue": "Computer Vision ECCV 2014,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau et al", "D. 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Lucas-kanade 20 years on: A unifying framework", "author": ["Baker", "Matthews", "S. 2004] Baker", "I. Matthews"], "venue": "International journal of computer vision,", "citeRegEx": "Baker et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Baker et al\\.", "year": 2004}, {"title": "Deep generative stochastic networks trainable by backprop. arXiv preprint arXiv:1306.1091", "author": ["Bengio et al", "Y. 2013] Bengio", "E. Thibodeau-Laufer", "G. Alain", "J. Yosinski"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Bishop", "C.M. 2006] Bishop"], "venue": null, "citeRegEx": "Bishop and Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop and Bishop", "year": 2006}, {"title": "Signature verification using a siamese time delay neural network", "author": ["Bromley et al", "J. 1993] Bromley", "J.W. Bentz", "L. Bottou", "I. Guyon", "Y. LeCun", "C. Moore", "E. S\u00e4ckinger", "R. Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q1993\\E", "shortCiteRegEx": "al. et al\\.", "year": 1993}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["Chatfield et al", "K. 2014] Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In Proceedings of the British Machine Vision Conference,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Chen et al", "2014] Chen", "L.-C", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv preprint arXiv:1412.7062", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al", "K. 2014] Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Chopra et al", "S. 2005] Chopra", "R. Hadsell", "Y. LeCun"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "al. et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al. et al\\.", "year": 2005}, {"title": "Memory bounded deep convolutional networks. arXiv preprint arXiv:1412.1442", "author": ["Collins", "Kohli", "M.D. 2014] Collins", "P. Kohli"], "venue": null, "citeRegEx": "Collins et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2014}, {"title": "Indoor semantic segmentation using depth information. CoRR, abs/1301.3572", "author": ["Couprie et al", "C. 2013] Couprie", "C. Farabet", "L. Najman", "Y. LeCun"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Histograms of oriented gradients for human detection", "author": ["Dalal", "Triggs", "N. 2005] Dalal", "B. Triggs"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Dalal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dalal et al\\.", "year": 2005}, {"title": "Predicting parameters in deep learning", "author": ["Denil et al", "M. 2013] Denil", "B. Shakibi", "L. Dinh", "M.A. Ranzato", "N. de Freitas"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Multi-task learning of facial landmarks and expression", "author": ["Devries et al", "T. 2014] Devries", "K. Biswaranjan", "G.W. Taylor"], "venue": "In Computer and Robot Vision (CRV),", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Donahue et al", "J. 2014] Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4389", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Donahue et al", "J. 2013] Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv preprint arXiv:1310.1531", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al", "J. 2011] Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Learning hierarchical features for scene labeling", "author": ["Farabet et al", "C. 2013a] Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Learning hierarchical features for scene labeling", "author": ["Farabet et al", "C. 2013b] Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["Gao et al", "H. 2015] Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "arXiv preprint arXiv:1505.05612", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick et al", "R. 2014] Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Finding action tubes", "author": ["Gkioxari", "Malik", "G. 2015] Gkioxari", "J. Malik"], "venue": "In CVPR", "citeRegEx": "Gkioxari et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gkioxari et al\\.", "year": 2015}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Gong et al", "Y. 2014a] Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "Computer Vision ECCV 2014,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Gong et al", "Y. 2014b] Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "In Computer Vision ECCV", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Goodfellow et al", "I. 2014a] Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572", "author": ["Goodfellow et al", "I.J. 2014b] Goodfellow", "J. Shlens", "C. Szegedy"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves et al", "A. 2009] Graves", "M. Liwicki", "S. Fern\u00e1ndez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves et al", "A. 2013] Graves", "Mohamed", "A.-r", "G. Hinton"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Learning rich features from RGB-D images for object detection and segmentation", "author": ["Gupta et al", "S. 2014] Gupta", "Ross", "P. Arbelaez", "J. Malik"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["Hariharan et al", "B. 2014] Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "arXiv preprint arXiv:1411.5752", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He et al", "K. 2015] He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1502.01852", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531", "author": ["Hinton et al", "G. 2015] Hinton", "O. Vinyals", "J. Dean"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton et al", "G.E. 2006] Hinton", "S. Osindero", "Teh", "Y.-W"], "venue": "Neural computation,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Hinton et al", "G.E. 2012] Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber", "S. 1997] Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["Hubel", "Wiesel", "D.H. 1962] Hubel", "T.N. Wiesel"], "venue": "The Journal of physiology,", "citeRegEx": "Hubel et al\\.,? \\Q1962\\E", "shortCiteRegEx": "Hubel et al\\.", "year": 1962}, {"title": "Deep structured output learning for unconstrained text recognition. CoRR, abs/1412.5903", "author": ["Jaderberg et al", "M. 2014a] Jaderberg", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Jaderberg et al", "M. 2014b] Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1405.3866", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Aggregating local image descriptors into compact codes", "author": ["Jegou et al", "H. 2012] Jegou", "F. Perronnin", "M. Douze", "J. Sanchez", "P. Perez", "C. Schmid"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "A biologically inspired system for action recognition", "author": ["Jhuang et al", "H. 2007] Jhuang", "T. Serre", "L. Wolf", "T. Poggio"], "venue": "In Computer Vision,", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "3d convolutional neural networks for human action recognition", "author": ["Ji et al", "S. 2013] Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Fei-Fei", "A. 2014] Karpathy", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1412.2306", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["Karpathy et al", "A. 2014] Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba", "D. 2014] Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Welling", "D.P. 2013] Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky et al", "A. 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Deepfix: A fully convolutional neural network for predicting human eye fixations", "author": ["Kruthiventi et al", "S.S. 2015] Kruthiventi", "K. Ayush", "R.V. Babu"], "venue": "arXiv preprint arXiv:1510.02927", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Hmdb: a large video database for human motion recognition", "author": ["Kuehne et al", "H. 2011] Kuehne", "H. Jhuang", "E. Garrote", "T. Poggio", "T. Serre"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Deep convolutional inverse graphics", "author": ["Kulkarni et al", "T.D. 2015] Kulkarni", "W. Whitney", "P. Kohli", "J.B. Tenenbaum"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun et al", "Y. 1998] LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}, {"title": "Heterogeneous multi-task learning for human pose estimation with deep convolutional neural network", "author": ["Li et al", "S. 2015a] Li", "Liu", "Z.-Q", "A. Chan"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Deepsaliency: Multi-task deep neural network model for salient object detection", "author": ["Li et al", "X. 2015b] Li", "L. Zhao", "L. Wei", "M. Yang", "F. Wu", "Y. Zhuang", "H. Ling", "J. Wang"], "venue": "arXiv preprint arXiv:1510.05484", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Predicting eye fixations using convolutional neural networks", "author": ["Liu et al", "N. 2015] Liu", "J. Han", "D. Zhang", "S. Wen", "T. Liu"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long et al", "J. 2015] Long", "E. Shelhamer", "T. Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Maas et al", "A.L. 2013] Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "In Proc. ICML,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Ask your neurons: A neural-based approach to answering questions about images. arXiv preprint arXiv:1505.01121", "author": ["Malinowski et al", "M. 2015] Malinowski", "M. Rohrbach", "M. Fritz"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Learning like a child: Fast novel visual concept learning from sentence descriptions of images. arXiv preprint arXiv:1504.06692", "author": ["Mao et al", "J. 2015] Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["Mao et al", "J. 2014] Mao", "W. Xu", "Y. Yang", "J. Wang", "A. Yuille"], "venue": "arXiv preprint arXiv:1412.6632", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Object level deep feature pooling for compact image representation", "author": ["Mopuri", "Babu", "K. 2015] Mopuri", "R. Babu"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "Mopuri et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mopuri et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Hinton", "V. 2010] Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable", "author": ["Nguyen et al", "A. 2014] Nguyen", "J. Yosinski", "J. Clune"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Jointly modeling embedding and translation to bridge video and language. arXiv preprint arXiv:1505.01861", "author": ["Pan et al", "Y. 2015] Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["Razavian et al", "A.S. 2014] Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550", "author": ["Romero et al", "A. 2014] Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Neural network-based face detection", "author": ["Rowley et al", "H.A. 1998] Rowley", "S. Baluja", "T. Kanade"], "venue": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,", "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}, {"title": "Learning representations by back-propagating errors", "author": ["Rumelhart et al", "D.E. 1988] Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling,", "citeRegEx": "al. et al\\.,? \\Q1988\\E", "shortCiteRegEx": "al. et al\\.", "year": 1988}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Russakovsky et al", "O. 2015] Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A. Berg", "L. Fei-Fei"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229", "author": ["Sermanet et al", "P. 2013a] Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet et al", "P. 2013b] Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "CoRR, abs/1312.6229", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Pedestrian detection with unsupervised multi-stage feature learning", "author": ["Sermanet et al", "P. 2013c] Sermanet", "K. Kavukcuoglu", "S. Chintala", "Y. LeCun"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Two-stream convolutional networks for action recognition", "author": ["Simonyan", "Zisserman", "K. 2014a] Simonyan", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Zisserman", "K. 2014b] Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Snoek et al", "J. 2012] Snoek", "H. Larochelle", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Convolutional-recursive deep learning for 3d object classification", "author": ["Socher et al", "R. 2012] Socher", "B. Huval", "B. Bath", "C.D. Manning", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Data-free parameter pruning for deep neural networks", "author": ["Srinivas", "Babu", "S. 2015] Srinivas", "R.V. Babu"], "venue": "Proceedings of the British Machine Vision Conference (BMVC),", "citeRegEx": "Srinivas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2015}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever et al", "I. 2013] Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199", "author": ["Szegedy et al", "C. 2013] Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Taigman et al", "Y. 2014] Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Selective search for object recognition", "author": ["Uijlings et al", "J. 2013] Uijlings", "K. van de Sande", "T. Gevers", "A. Smeulders"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Original approach for the localisation of objects in images. IEE Proceedings-Vision", "author": ["Vaillant et al", "R. 1994] Vaillant", "C. Monrocq", "Y. Le Cun"], "venue": "Image and Signal Processing,", "citeRegEx": "al. et al\\.,? \\Q1994\\E", "shortCiteRegEx": "al. et al\\.", "year": 1994}, {"title": "Sequence to sequence\u2013video", "author": ["Venugopalan et al", "S. 2015] Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729", "author": ["Venugopalan et al", "S. 2014] Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Large-scale optimization of hierarchical features for saliency prediction in natural images", "author": ["Vig et al", "E. 2014] Vig", "M. Dorr", "D. Cox"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent et al", "P. 2010] Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals et al", "O. 2014] Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan et al", "L. 2013] Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Multi-modal unsupervised feature learning for rgb-d scene labeling", "author": ["Wang et al", "A. 2014a] Wang", "J. Lu", "G. Wang", "J. Cai", "Cham", "T.-J"], "venue": "Computer Vision \u2013 ECCV 2014,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Action recognition with improved trajectories", "author": ["Wang", "Schmid", "H. 2013] Wang", "C. Schmid"], "venue": "In IEEE International Conference on Computer", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Learning fine-grained image similarity with deep ranking", "author": ["Wang et al", "J. 2014b] Wang", "Y. Song", "T. Leung", "C. Rosenberg", "J. Wang", "J. Philbin", "B. Chen", "Y. Wu"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Action recognition with trajectory-pooled deepconvolutional descriptors", "author": ["Wang et al", "L. 2015a] Wang", "Y. Qiao", "X. Tang"], "venue": "In CVPR,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Fast dropout training", "author": ["Wang", "Manning", "S. 2013] Wang", "C. Manning"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Deep joint task learning for generic object extraction. CoRR, abs/1502.00743", "author": ["Wang et al", "X. 2015b] Wang", "L. Zhang", "L. Lin", "Z. Liang", "W. Zuo"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "CNN: single-label to multi-label", "author": ["Wei et al", "Y. 2014] Wei", "W. Xia", "J. Huang", "B. Ni", "J. Dong", "Y. Zhao", "S. Yan"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Deepflow: Large displacement optical flow with deep matching", "author": ["Weinzaepfel et al", "P. 2013] Weinzaepfel", "J. Revaud", "Z. Harchaoui", "C. Schmid"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Evaluating bag-of-visual-words representations in scene classification", "author": ["Yang et al", "J. 2007] Yang", "Jiang", "Y.-G", "A.G. Hauptmann", "Ngo", "C.-W"], "venue": "In Proceedings of the international workshop on Workshop on multimedia information retrieval,", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski et al", "J. 2014] Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Learning to compare image patches via convolutional neural networks. arXiv preprint arXiv:1504.03641", "author": ["Zagoruyko", "Komodakis", "S. 2015] Zagoruyko", "N. Komodakis"], "venue": null, "citeRegEx": "Zagoruyko et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zagoruyko et al\\.", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Fergus", "M. 2014] Zeiler", "R. Fergus"], "venue": "Computer Vision ECCV 2014,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Learning and transferring multi-task deep representation for face alignment. CoRR, abs/1408.3967", "author": ["Zhang et al", "Z. 2014] Zhang", "P. Luo", "C.C. Loy", "X. Tang"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Conditional random fields", "author": ["Zheng et al", "S. 2015] Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P. Torr"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Learning deep features for scene recognition using places database", "author": ["Zhou et al", "B. 2014] Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 43, "context": "A more recent work by [Karpathy et al., 2014] propose a set of techniques to fuse the appearance information present from a stack of consecutive frames in a video.", "startOffset": 22, "endOffset": 45}], "year": 2016, "abstractText": "Traditional architectures for solving computer vision problems and the degree of success they enjoyed have been heavily reliant on hand-crafted features. However, of late, deep learning techniques have offered a compelling alternative \u2013 that of automatically learning problem-specific features. With this new paradigm, every problem in computer vision is now being re-examined from a deep learning perspective. Therefore, it has become important to understand what kind of deep networks are suitable for a given problem. Although general surveys of this fast-moving paradigm (i.e. deep-networks) exist, a survey specific to computer vision is missing. We specifically consider one form of deep networks widely used in computer vision convolutional neural networks (CNNs). We start with \u201cAlexNet\u201d as our base CNN and then examine the broad variations proposed over time to suit different applications. We hope that our recipe-style survey will serve as a guide, particularly for novice practitioners intending to use deep-learning techniques for computer vision.", "creator": "LaTeX with hyperref package"}}}