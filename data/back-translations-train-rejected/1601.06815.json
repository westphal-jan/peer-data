{"id": "1601.06815", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2016", "title": "Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add", "abstract": "Convolutional neural networks (CNNs) are currently state-of-the-art for various classification tasks, but are computationally expensive. Propagating through the convolutional layers is very slow, as each kernel in each layer must sequentially calculate many dot products for a single forward and backward propagation which equates to $\\mathcal{O}(N^{2}n^{2})$ per kernel per layer where the inputs are $N \\times N$ arrays and the kernels are $n \\times n$ arrays. Convolution can be efficiently performed as a Hadamard product in the frequency domain. The bottleneck is the transformation which has a cost of $\\mathcal{O}(N^{2}\\log_2 N)$ using the fast Fourier transform (FFT). However, the increase in efficiency is less significant when $N\\gg n$ as is the case in CNNs. We mitigate this by using the \"overlap-and-add\" technique reducing the computational complexity to $\\mathcal{O}(N^2\\log_2 n)$ per kernel. This method increases the algorithm's efficiency in both the forward and backward propagation, reducing the training and testing time for CNNs. Our empirical results show our method reduces computational time by a factor of up to 16.3 times the traditional convolution implementation for a 8 $\\times$ 8 kernel and a 224 $\\times$ 224 image.", "histories": [["v1", "Mon, 25 Jan 2016 21:29:11 GMT  (1114kb,D)", "http://arxiv.org/abs/1601.06815v1", "British Machine Vision Conference 2015"]], "COMMENTS": "British Machine Vision Conference 2015", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["tyler highlander", "andres rodriguez"], "accepted": false, "id": "1601.06815"}, "pdf": {"name": "1601.06815.pdf", "metadata": {"source": "CRF", "title": "HIGHLANDER, RODRIGUEZ: EFFICIENT TRAINING OF CNNS USING FFT AND OAA 1 Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add", "authors": ["Tyler Highlander", "Andres Rodriguez"], "emails": ["highlander.2@wright.edu", "andres.rodriguez.8@us.af.mil"], "sections": [{"heading": "1 Introduction", "text": "In recent years, it has become clear that the problem is not only a problem, but also a problem that affects many people, not only a problem, but also a problem that affects many people, but also a problem for which they themselves are responsible. \"I think it is a problem,\" he says, \"but it is a problem that we cannot solve.\" The problem is not only that it is a problem, but also that it is a problem. \""}, {"heading": "2 Overlap-and-Add", "text": "In OaA, the complexity of each project in N2 / n2 (rounded up) blocks is equal to the kernel size n \u00d7 n. A confrontation between each block and the kernel is calculated and the results are overlapped and added. Figure 1 shows a simple 1-D spatial confrontation overlap method (which is easy to generalize to 2-D. The input confrontation is initially split into smaller blocks that are the size of the kernel. Minor confrontations are calculated between the kernel and the block inputs. The resulting confrontations are overlapped by n \u2212 1, where n is the length of the kernel and is added to produce the same results as a traditional spatial confrontation. Figure 1: 1-D overlaps and add confrontation Each confrontation in OaA can be efficiently calculated in the frequency domain."}, {"heading": "3 Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Training consistency", "text": "In this experiment, we use each method of convolutions to train the CNN LeNet-5 [6] architecture using the MNIST [7] dataset. The goal of this experiment is to empirically demonstrate that the methods are equivalent. Each network is trained for only 100 epochs, as all we need on 1github.com / THighlander show is consistency. Each CNN network is trained five times with each type of folding technique, and their averages of classification rate are shown in Table 2. As expected, all three methods averaged within 0.07% of each other. The reason for this difference is the random initialization of the parameters during training of each CNN."}, {"heading": "3.2 Time vs. number of kernels", "text": "In this experiment, we compare the total propagation time required by a revolutionary layer as the number of nuclei in the layer increases. To compare the computing time, we note that additional channels in the revolutionary layer can be treated as a multiplicative factor in the number of nuclei, i.e., what is required for confronting a C channel with a set of K channels is equivalent to confronting a single channel input array with a set of 1 channel cores. In our experiments, the input array of size 32 \u00d7 32 and each kernel input array with a size of 5 \u00d7 5, as well as the speed-up factor of FFTconv and OaAconv, is compared to the advance."}, {"heading": "3.3 Time vs. kernel size", "text": "In this experiment, we vary the size of the kernel while keeping the input size constant. The size of the input field is 64 x 64. The number of cores used is kept constant at 100. Figures 4 and 5 show the speed over spaceConv vs. core size for forward and backward propagation respectively. The core sizes vary from 1 to 64 with a discrete step of 1. Each \"core size experiment\" is repeated ten times and the results are average. Figure 4: Speed over spaceConv vs. core size in forward propagation Figure 5: Speed over spaceConv vs. core size in backward propagation In forward propagation in Figure 4, the performance of FFTconv and OaAconv converge as expected at 64. An interesting aspect of this diagram is the different power peaks at different core sizes. This is because the FFT software [2] is optimal when converging FFTconv and OaAconv as expected at 64."}, {"heading": "3.4 Time vs. input size", "text": "In this experiment, we test the performance of multiple input variables while maintaining a constant core size at 5 \u00d7 5. The input varied from 4 x 4 to 256 x 256 with a discrete step of 4 x 4 for the forward propagation experiment and a discrete step of 8 x 8 for the backward propagation experiment. Various discrete steps are used to highlight the best performing size of the FFT algorithm for each propagation experiment. Each experiment of the \"input size\" is repeated ten times and the results are averaged. Figures 6 and 7 show the velocity over spatial propagation Conv versus the input size for forward and backward propagation, respectively. Figure 6: Acceleration over spatial propagation Conv vs. input size in forward propagation For inputs greater than 8 x 8 input propagation sizes (the typical scenario for CNN architectures), OaAconv always outperforms other methods."}, {"heading": "4 Conclusion", "text": "In this paper, we demonstrated that OaAconv can improve the efficiency of CNNs over conventional folds and folds via a Hadamard product in the frequency domain. Although OaAconv needs to compute more transformations than FFTconv, the fact that the transformations are smaller outweighs the cost of having to compute more. To make fair comparisons, we conducted our experiments on individual threads. In practice, OaA should be implemented in such a way that each block folding is calculated in parallel and each FFT is implemented with a GPU FFT library, e.g. cuFFT [9], to achieve maximum performance. In future work, we plan to optimize the FFT implementations for small format transformations and design a CNN completely in the frequency domain to eliminate the bottleneck of transformations."}, {"heading": "Acknowledgements", "text": "We would like to thank Prof. Mateen Rizki of Wright State University for the helpful discussions and the Air Force Office for Scientific Research (AFOSR) for providing the main funding for this work through LRIR 14Y06COR."}], "references": [{"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Fftw: An adaptive software architecture for the FFT", "author": ["Matteo Frigo", "Steven G Johnson"], "venue": "In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Computer Science Department,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Mnist handwritten digit database", "author": ["Yann LeCun", "Corinna Cortes"], "venue": "AT&T Labs [Online]. http://yann. lecun. com/exdb/mnist,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Fast training of convolutional networks through FFTs", "author": ["Michael Mathieu", "Mikael Henaff", "Yann LeCun"], "venue": "International Conference on Learning Representations,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Cufft library", "author": ["CUDA Nvidia"], "venue": "http://docs.nvidia.com/cuda/cufft", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Discrete-time signal processing, volume 2. Prentice-hall", "author": ["Alan V Oppenheim", "Ronald W Schafer", "John R Buck"], "venue": "Englewood Cliffs,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Michael Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "In International Conference on Learning Representations. CBLS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Fast convolutional nets with fbfft: A gpu performance evaluation", "author": ["Nicolas Vasilache", "Jeff Johnson", "Michael Mathieu", "Soumith Chintala", "Serkan Piantino", "Yann LeCun"], "venue": "International Conference on Learning Representations,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Convolutional neural networks (CNNs) achieved state-of-the-art classification rates on various datasets [1, 4, 7], but require significant computational resources.", "startOffset": 104, "endOffset": 113}, {"referenceID": 3, "context": "Convolutional neural networks (CNNs) achieved state-of-the-art classification rates on various datasets [1, 4, 7], but require significant computational resources.", "startOffset": 104, "endOffset": 113}, {"referenceID": 6, "context": "Convolutional neural networks (CNNs) achieved state-of-the-art classification rates on various datasets [1, 4, 7], but require significant computational resources.", "startOffset": 104, "endOffset": 113}, {"referenceID": 4, "context": "For example, AlexNet [5] has over 60 million free parameters trained with stochastic gradient descent requiring thousands of forward and backward propagations through a network with 5 convolutional layers.", "startOffset": 21, "endOffset": 24}, {"referenceID": 11, "context": "A more recent CNN, GoogLeNet [12], has various layers within layers amounting to 59 convolutional layers.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "With over one million training images in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [1] and hundreds of epochs needed for training each CNN, reducing the complexity of the convolution operation reduces training time.", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "[8] demonstrated that this reduces the training and testing time of CNNs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "In this paper, we propose to use the overlap-and-add (OaA) technique [10] to further reduce the training and testing complexity to O(N2 log2 n) per kernel.", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "Note that the overlapand-save [10] is a similar technique that may be marginally faster but has the same complexity.", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "We can get additional speed up by taking advantage of the NVIDIA CUDA Fast Fourier Transform library (cuFFT) that computes each individual FFTs up to 10 times faster [9] (see also [13]).", "startOffset": 166, "endOffset": 169}, {"referenceID": 12, "context": "We can get additional speed up by taking advantage of the NVIDIA CUDA Fast Fourier Transform library (cuFFT) that computes each individual FFTs up to 10 times faster [9] (see also [13]).", "startOffset": 180, "endOffset": 184}, {"referenceID": 2, "context": "As part of this work, we created a Caffe [3] fork1 that uses a multi-thread GPU implementation of OaA for efficient convolutions.", "startOffset": 41, "endOffset": 44}, {"referenceID": 10, "context": "[11] when the test image is much larger than the training images, and uses a sliding window approach across a pyramid of scales for simultaneous detection (localization) and classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "In this experiment we use each method of convolutions to train the CNN LeNet-5 [6] architecture using the MNIST [7] dataset.", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "In this experiment we use each method of convolutions to train the CNN LeNet-5 [6] architecture using the MNIST [7] dataset.", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "This is because the FFT software [2] is optimal for Fourier transforms with power of 2 sides along each dimension.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": ", cuFFT [9] to achieve maximum performance.", "startOffset": 8, "endOffset": 11}], "year": 2016, "abstractText": "Convolutional neural networks (CNNs) are currently state-of-the-art for various classification tasks, but are computationally expensive. Propagating through the convolutional layers is very slow, as each kernel in each layer must sequentially calculate many dot products for a single forward and backward propagation which equates to O(N2n2) per kernel per layer where the inputs are N\u00d7N arrays and the kernels are n\u00d7 n arrays. Convolution can be efficiently performed as a Hadamard product in the frequency domain. The bottleneck is the transformation which has a cost of O(N2 log2 N) using the fast Fourier transform (FFT). However, the increase in efficiency is less significant when N n as is the case in CNNs. We mitigate this by using the \u201coverlap-and-add\u201d technique reducing the computational complexity to O(N2 log2 n) per kernel. This method increases the algorithm\u2019s efficiency in both the forward and backward propagation, reducing the training and testing time for CNNs. Our empirical results show our method reduces computational time by a factor of up to 16.3 times the traditional convolution implementation for a 8 \u00d7 8 kernel and a 224 \u00d7 224 image.", "creator": "LaTeX with hyperref package"}}}