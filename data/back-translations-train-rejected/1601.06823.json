{"id": "1601.06823", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2016", "title": "Survey on the attention based RNN model and its applications in computer vision", "abstract": "The recurrent neural networks (RNN) can be used to solve the sequence to sequence problem, where both the input and the output have sequential structures. Usually there are some implicit relations between the structures. However, it is hard for the common RNN model to fully explore the relations between the sequences. In this survey, we introduce some attention based RNN models which can focus on different parts of the input for each output item, in order to explore and take advantage of the implicit relations between the input and the output items. The different attention mechanisms are described in detail. We then introduce some applications in computer vision which apply the attention based RNN models. The superiority of the attention based RNN model is shown by the experimental results. At last some future research directions are given.", "histories": [["v1", "Mon, 25 Jan 2016 21:54:02 GMT  (1267kb,D)", "http://arxiv.org/abs/1601.06823v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["feng wang", "david m j tax"], "accepted": false, "id": "1601.06823"}, "pdf": {"name": "1601.06823.pdf", "metadata": {"source": "CRF", "title": "Survey on the attention based RNN model and its applications in computer vision", "authors": ["Feng Wang"], "emails": ["f.wang-6@student.tudelft.nl", "D.M.J.Tax@tudelft.nl"], "sections": [{"heading": "1 Introduction 3", "text": "What does \"attention\" mean?................................. 3 1,2 What are the applications of the attention-based model in computer vision? 4 1,3 What is the attention-based RNN model?....................... 51.3.1 Recurring Neural Network (RNN)...................................... 5 1.3.2 The RNN for sequence-to-sequence problems......................... 5. 1.3.3 Attention-based RNN model................. 81.4. What are the attention mechanisms introduced in this survey?........... 11.."}, {"heading": "2 The attention based RNN model 11", "text": "......................................................................................................................................................................................................................................................................................................"}, {"heading": "3 Applications of the attention based RNN model in computer vision 27", "text": "3.1 Overview........................................................................................................................."}, {"heading": "4 Conclusion and future research 34", "text": "References 37AbstractRecurring Neural Networks (RNN) can be used to solve the sequence problem where both the input and the output have sequential structures. Normally, there are some implicit relationships between the structures. However, for the traditional RNN model, it is difficult to fully explore the relationships between the sequences. In this survey, we present some attention-based RNN models that can focus on different parts of the input for each output item in order to explore and exploit the implicit relationships between the input and the output items. We describe in detail the different attention mechanisms, and then present some applications in computer vision that apply the attention-based RNN model. The superiority of the attention-based RNN model is demonstrated by the experimental results. Finally, some future research guidelines are given."}, {"heading": "1. Introduction", "text": "What does \"attention\" mean? What are the applications of the attention-based model in the field of computer vision? What is the attention-based RNN model? What are the attention mechanisms introduced in this survey? What are the advantages of using the attention-based RNN model? These are the questions we want to answer in this section."}, {"heading": "1.1 What does \u201dattention\u201d mean?", "text": "In psychology, which is constrained by processing bottlenecks, people tend to focus selectively on a portion of the information while ignoring other perceptible information. [3] The mechanism mentioned above is usually called attention. In human visual processing, for example, the human eye has the ability to receive a large visual field, but usually only a small part is fixed on it. This is because different areas of the retina have different processing abilities, which are normally called sharpness, and only a small area of the retina, Fovea, has the greatest sharpness. In order to allocate limited visual processing resources, one must first select a certain part of the visual field and then focus on it. For example, when a person reads, the words to be read at a particular time are usually visited and processed. As a result, there are two main aspects of attention: \u2022 Decisive part of input must be determined, on which one must focus, the abstract definition is introduced by psychology, although the attention is very intuitive."}, {"heading": "1.2 What are the applications of the attention based model in computer vision?", "text": "As mentioned above, the attention mechanism plays an important role in human visual processing. Some researchers also bring attention to the area of computer stimulus. As in human perception, a computer vision system should also focus on the important part of the input image, rather than giving equal weights to all pixels. However, a simple and widely used method is to extract local image characteristics from the input. Normally, the local image characteristics can be dots, lines, corners or small image fields, and then some measurements are taken from the patches and converted into descriptors. Obviously, the distribution of local characteristics in a natural image is not uniform, causing the system to give different weights to different parts of the input image and satisfy the definition of alertness. Saliency detection is another typical example directly motivated by human perception. As mentioned above, people have the ability to quickly recognize prominent objects / regions and then visit them by shifting the line of vision."}, {"heading": "1.3 What is the attention based RNN model?", "text": "Note that attention is only a mechanism or methodology, so there is no strict definition of what it is in mathematics. For example, the local image features, the detection of prominence, the sliding window methods introduced above, or some recently proposed object detection methods such as [19] all use the attention mechanism in various mathematical forms. On the other hand, as shown later, the RNN is a specific type of neural network with a specific mathematical description. The attention-based RNN models in this survey refer in particular to the RNN models designed for the sequence of problems with the attention mechanism. Furthermore, all systems in this survey are feasible throughout, whereby the parameters for both the attention module and the common RNN model should be learned simultaneously."}, {"heading": "1.3.1 Recurrent neural network (RNN)", "text": "The biggest problem for CNN, however, is that it only accepts a fixed-length input vector and there is a fixed-length output vector, so it cannot handle the data with rich structures, especially the sequences. That's why RNN is interesting, which can not only operate via sequences of input vectors, but can also generate sequences of output vectors. Figure 2 shows an abstract structure of an RNN unit. The blue rectangles are the input vectors in a sequence, and the yellow rectangles are the output vectors in a sequence. By keeping a hidden state (green rectangles in Figure 2), the RNN is able to process the sequence data. The dotted arrow indicates that sometimes the output vectors do not need to be generated."}, {"heading": "1.3.2 The RNN for sequence to sequence problem", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.3.2.1. Sequence to sequence problem", "text": "As mentioned above, the RNN unit contains a hidden state that enables it to process variable-size sequential data. In this survey, we focus on the RNN models for sequence problems. The sequence for sequence problems is defined in such a way that it maps the input sequence to the output sequence, which is a very general definition. Here, the input sequence does not strictly refer to a sequence of items, otherwise for different sequence problems, it might exist in different forms. For example: \u2022 For the task of machine translation, one usually has to translate some sentences in the source language into the target language. In this case, the input sequence is a sentence in the natural language, with each item in the sequence being the word in the sentence. The order of items in the sequence is crucial. \u2022 In the video classification task, a video clip should usually be assigned a description. In this case, the input sequence is an object."}, {"heading": "1.3.2.2. The structure of the RNN model for sequence to sequence problem", "text": "Under the condition that the lengths of the input and output sequences can vary, it is not possible for a common RNN to directly construct the corresponding relationships between the elements in the input and output sequences without additional information. Thus, it is natural to divide the entire system into two parts: the encoder and the decoder [10, 47]. The encoder encodes all (or part of) the useful information of the input data into an intermediate code, and the decoder deals with generating the output sequence into two parts. The encoder encodes all (or part of) the input data into an intermediate code."}, {"heading": "1.3.3 Attention based RNN model", "text": "For the sequencers, it is important that they are able to play by the rules and that they are able to play by the rules."}, {"heading": "1.4 What are the attention mechanisms introduced in this survey?", "text": "In this study, we present four types of attention mechanisms, which are: item-wise soft attention, item-wise hard attention, location-wise hard attention and location-wise soft attention. Here, we give only a brief and high-level presentation of them, and leave the mathematical descriptions in the next chapter. For item-wise and location-wise, as mentioned above, the forms of the input sequence are different for different tasks. However, the item-wise mechanism requires that the input contains explicit items, or you must add an additional pre-processing step to generate a sequence of items from the input sequence. However, the location-wise mechanism is designed for the input, which is a single feature map. The term \"location\" is used because this type of input is an image, and if it is treated as a sequence of objects, all objects can be shown by their places.As described above, the item-wise method works on an item-level."}, {"heading": "1.5 What are the advantages of using attention based RNN model?", "text": "Here we give some general advantages of the attention-based RNN model. The detailed comparison between the attention-based RNN model and the common RNN model will be illustrated later. Advantages: \u2022 As the name suggests, the attention-based RNN model is able to learn to assign weights to different parts of the input, rather than treating all input elements equally, which allows the inherent relationships between the elements in the input and output sequence to be established. This is not only a way to increase the performance of some tasks, but it is also a powerful visualization tool compared to the common RNN model. \u2022 The hard attention model does not need to process all elements in the input sequence, but sometimes only processes the interested ones, so it is very useful for some tasks with only partially observable environments, such as playing, which normally cannot be handled by the common RNN model. \u2022 Not limited to the area of the computer, the Sequence-based NN model is also suitable for all types of natural attention-related problems [For example, Sequence-based N33]."}, {"heading": "1.6 Organization of this survey", "text": "In Section 2, we describe the general mathematical form of the attention-based RNN model, in particular four types of attention mechanisms: item-wise soft attention, item-wise hard attention, location-wise hard attention, and location-wise soft attention. Next, we present some typical applications of the attention-based RNN model in the field of computer vision in Section 3. Finally, we summarize the findings of the attention-based RNN model and discuss some potential challenges and future research directions."}, {"heading": "2. The attention based RNN model", "text": "In this section, we will first give a brief introduction to the neural network, followed by an abstract mathematical description of the RNN. Then, four attention mechanisms of the RNN model will be analyzed in detail."}, {"heading": "2.1 Neural network", "text": "A neural network is a data processing and computing model that consists of several neurons that are connected to each other; the basic component of all neural networks is the neuron, whose name refers to its inspiration from the human neuron cell. Ignoring the biological implication, a neuron in the neural network accepts one or more inputs, weighs the inputs and adds them up, and finally returns output by passing the sum through an activation function. Leave the input to the neuron i = < i1, i2,. in > the corresponding weights w = < w1, w2,.., wn > and the output can be o. A visual example of a neuron is shown in Figure 5, where f is the activation function. And we have: o = f (n procedure k = 1 (wkik) + w0) (1), where w0 is the bias parameter."}, {"heading": "2.2 A general RNN unit", "text": "The \"recurring\" input sequence in the RNN indicates that it performs the same operations for each element in the input sequence, while retaining a memory of the processed items for future operations. Let the input sequence be I = (i1, i2,.., iT). At each step t, the operation performed by a generic RNN unit can be described as follows: [o-t ht] = \u03c6W (it, ht \u2212 1) (4), where o-t is the predicted output vector at the time t and ht is the hidden state at the time. \u043eW is a neural network parameterized by W that takes the th input element (it), and the previous hidden state \u2212 1 is the predicted output vector at the time. \u043eW is a neural network that is parameterized by W that takes the th input element (it), and the previous hidden state \u2212 1 is the general input at the time. Here, the memory is clear that the previous state worked to."}, {"heading": "2.3 The model for sequence to sequence problem", "text": "As shown in Figure 3, the model for the sequence problem can generally be divided into two parts: the encoder and the decoder, both of which are neural networks. Let the input sequence be X and the output sequence be Y. As mentioned above, the input sequence for some tasks does not consist of explicit items for which we use X to represent the input sequence, otherwise X = (x1, x2,..., xT). Also, this overview only takes into account the output sequence that contains explicit items, i.e. Y = (y1, y2,..., yT \u2032) is the output sequence. The length of the input and output sequences need not be the same."}, {"heading": "2.3.1 Encoder", "text": "As mentioned above, the core task of an encoder is to encode all (or part) of the input elements into an intermediate code that is decrypted by the decoder. In the usual RNN for sequence-to-sequence problems, the model does not attempt to determine the corresponding relationships between the elements in the input and output sequence, and normally all elements in the input sequence are compressed into a single intermediate code. c is therefore an arbitrary neural network for the encoder, but its type usually depends on the input data. For example, if input X is represented as a single neural network parameterized by Wenc, c is usually a neural network without repetition. Here, the encoder can be any neural network, but its type usually depends on the input data. If input X is treated as a single characteristic diagram, a neural network without repetition is normally used as the encoder for CNN."}, {"heading": "2.3.2 Decoder", "text": "If the length of the output sequence is greater than 1, the decoder is recursive in most cases, because the decoder must have at least the knowledge of what it has already predicted in order to prevent a repetition of the prediction. In particular, in the RNN model with the attention mechanism introduced later, the weights of the input elements are mapped with the guidance of past predictions, so that in this case the decoder should be able to store some historical information. Consequently, in this overview we consider only those cases in which the decoder is an RNN. As mentioned above, in an ordinary RNN, the decoder accepts the intermediate code c as input and generates at each step j the predicted output y-j and the hidden state hj after [y-j hj] = Wdec (c, y-j-1, hj-1) (6), in which the decoder uses the recursive neural network to be mapped by Wdecoder output parameters of the previous T and y-y-1, and the outputs y-y-1."}, {"heading": "2.3.3 Learning", "text": "Like many other monitored learning problems, the monitored sequence is optimized to the sequence problem by maximizing the log probability. With the input sequence X, the ground truth output sequence Y and the predicted output sequence Y, the log probability for the j-point yj in the output sequence Y isLj (X, Y, \u03b8) = log p (yj | X, y1, y2,....) = log p (yj | y, ollyj) (7) is represented, where \u03b8 = [Wenc, Wdec] represents all learnable parameters in the model and p () calculates the probability of yj based on y-j. For example, in a classification problem where yj indicates the index of the label, p (yj, yj | y-j) = soft-max (y-j) yj."}, {"heading": "2.4 Attention based RNN model", "text": "In this section we provide a comprehensive mathematical description of the four attention mechanisms. As mentioned above, the Attention Module helps the encoder to calculate a better intermediate c for the decoder. Therefore, the decoder part of this section is identical to the one shown in Section 2.3.2, but the encoder network \u03c6Wenc may not always be identical to Equation (5)."}, {"heading": "2.4.1 Item-wise soft attention", "text": "The item-wise soft attention requires the input sequence X = contains some explicit items x1, x2,.., xT. Instead of extracting just one code c from the input X, the encoder of the item-wise soft attention RNN model extracts a set of codes C from X: C = {c1, c2,.., cT \u2032} (9), where the size of C does not have to be the same as X, which means that you can use multiple input items to calculate a single code or extract multiple codes from an input item. For simplicity, let's just set T \u2032 \u2032 \u2032 \u2032 s (T), where the size of C does not have to be identical (1, 2,.., T), where the encoder is Wenc's neural attention. Note here that the encoder is different from the encoder in Equation (5) because the encoder in the item-wise attention model is explained."}, {"heading": "2.4.2 Item-wise hard attention", "text": "Item-wise hard attention is very similar to item-wise soft attention. It has yet to calculate the weights for each code, as shown in Equation (9) to Equation (13). As mentioned above, \u03b1jt can be interpreted as the probability of code ct being relevant to output yj. Instead of a linear combination of all codes in C, itemwise hard attention stochastically selects a code based on its probabilities. In detail, an indicator lj is generated from a categorical distribution in decryption step j to specify which code to select: lj \u0445 C (T, {\u03b1jt} Tt = 1) (15), where C () is a categorical distribution parameterized by the probabilities of the codes ({\u03b1jt} Tt = 1) and lj in this case functions as an index: c = clj (16) If the size of C is 2, the above categorical distribution turns into an equation (T = 15) in Bernoui."}, {"heading": "2.4.3 Location-wise hard attention", "text": "As mentioned above, for some types of input X, such as an image, it is not trivial to extract the individual regions directly from them. So, the localized hard attention model is developed that accepts the entire feature card X as input, while we focus here only on two newer mechanisms proposed in [39] and [4], because the attention models in this work are more general and can be trained end-to-end. Since the localized hard attention model selects only one subregion of the input X at the decoding step, it makes sense not to correspond the selected region with the output item. There are two potential solutions to this problem: 1. Pick only one input vector at each decoding. 2. Make a few glances for each prediction that does not correspond in each region."}, {"heading": "2.4.4 Location-wise soft attention", "text": "As already mentioned, localized soft attention is transformed into Xout, and it is used to calculate the part of the QQ, and at each previous decoding step, a transformed version of the input tag card is generated to calculate the intermediate code. It is first called the spatial transformation network (STN) in [25]. Instead of the RNN model, the STN in [25] is first applied to a CNN model, but it can easily be transferred to an RNN model with tiny modifications. To clarify the explanations, we use Xin to represent the input of the STN (where X = Xin normally), and Xout to represent the output of the STN model. Figure 8 provides a visual example of how the STN model is embedded in the overall framework, and how it works."}, {"heading": "2.5 Learning of the attention based RNN models", "text": "As mentioned in the introduction, the difference between the soft and hard attention mechanisms in optimization is that the module of soft attention is differentiable in terms of its parameters, so the standard gradient increase / decent can be used for optimization. However, for hard attention, the model is not differentiable and the techniques from amplification learning are applied for optimization."}, {"heading": "2.5.1 Soft attention", "text": "For the item-wise soft attention mechanism (Equation (29), Equation (30) and the bilinear interpolation (34), if the total attention in Equation (12) is differentiable with respect to its inputs, the total attention is differentiable with respect to Watts, as well as the entire RNN model with respect to \u03b8 = [Wenc, Wdec, Watt]. Hence, the same sum of the log probability in Equation (8) is used as an objective function for learning and the gradient ascent is used to maximize objectivity. The location-wise soft attention module (STN) is also differentiable with respect to its parameters, if the Location Network \u03c6Wloc, the transformation and the sampling kernel k are carefully selected to ensure that their gradients can be defined with respect to their inputs. For example, when using an affable attention locale (30), the STN can be defined with respect to their inputs."}, {"heading": "2.5.2 Hard attention", "text": "As shown in Section 2.4.2 and Section 2.4.3, the hard attention mechanism (log) is stochastically selected an item or a subregion from the input. In this case, the gradients in relation to the selected item / region are zero because they are discrete; the zero gradients cannot be used to maximize the sum of log probabilities by the standard gradient ascent. Generally, this is a problem of forming a neural network with discrete values / units, and here we have just introduced the method used in [39, 4, 57] which uses the techniques of amplifying learning. Instead, the learning methods for item-wise and location-wise hard attention are essentially the same, but the item-wise hard attention implicitly sets the number of looks as 1. In this section, we simply make the number of looks as M.Instead of the raw log likelihood in the equation (7), which is a new L-probability that is an objective."}, {"heading": "2.6 Summary", "text": "In this section, we first give a brief introduction to the neural network, CNN and the RNN. Then we discuss the common RNN model for sequence-to-sequence problem. And the detailed descriptions of four types of attention mechanisms are given: item-wise soft attention, item-wise hard attention, location-wise hard attention, and location-wise soft attention, followed by optimization of attention-based RNN models."}, {"heading": "3. Applications of the attention based RNN model in computer vision", "text": "In this section, we will first give a brief overview of the differences when using different attention-based RNN models, and then introduce two applications in computer vision that apply the attention-based RNN model."}, {"heading": "3.1 Overview", "text": "As mentioned above, the item-wise attention model requires the input sequence to consist of explicit items, which is not trivial for some types of input, such as image input. A simple solution is to extract some patches manually from the input images and treat them as a sequence of items. In the extraction method, you can either extract some fixed size fields from some predefined locations or apply other methods of object suggestion, such as [49]. \u2022 The location-wise attention model can directly accept as input a feature card that avoids the above-mentioned step of \"Itextraction.\" And, compared to human perception, location-wise attention is more attractive because when a person sees an image, he will not manually divide the image into some patches and calculate the weights for them before he recognizes the objects in the image, he focuses directly on an object and its surroundings."}, {"heading": "3.2 Image classification and object detection", "text": "In fact, it is the case that most people who live and work in the USA, also live and work in Germany, have the same rights as people who live in the USA and have the same rights. In fact, it is the case that most people in the USA are not able to have the same rights as people in the USA. In the USA, it is the case that people in the United States of America are not able to have the same rights. In the USA, it is the case that people in the United States of America have the same rights as in the USA. In the USA, it is the case that people have the same rights to the same rights as in the USA. In the USA, it is the case that the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights and the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights and the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights to the same rights and the same rights to the same rights to the same rights to the same rights"}, {"heading": "3.3 Image caption", "text": "Captioning is a very challenging problem: by specifying an input image, the above problem uses a natural language system to describe the content of the image. A classic way of solving this problem is by dividing the problem into some subproblems, such as object recognition, object-word alignment, sentence generation by template, etc., and solving them independently of each other. Obviously, this process will make some sacrifices in terms of performance. With the development and success of the recursive network in the field of machine translation, the caption problem can also be treated as a machine translation problem, i.e. the image can also be considered as language, and the system simply translates it into another language (natural language, such as English). The RNN-based caption system has recently been proposed in an area [51] (Neural Image Caption, NIC) that fits perfectly with our encoder decoder decoder RNN system, without the attention mechanism of the general attention mechanism of the NNN structure being shown in the IC's Attention Mechanism."}, {"heading": "4. Conclusion and future research", "text": "In this study, we discuss attention, which is based on a slow learning process, and on the other hand, attention is not focused on improving attention. \u2022 Attention is an intuitive approach, applying different weight classes to different types of input problems that are widespread. \u2022 We show how common RNN works and provide detailed descriptions of four different attention mechanisms that are also embedded in the RNN model. \u2022 The item-wise attention based model requires that the input sequence contains explicit terms, such as image input, an additional step to extract terms, and their advantages and disadvantages are also analysed. \u2022 The item-wise attention based model requires that the input sequence contain explicit terms."}], "references": [{"title": "Salient region detection and segmentation", "author": ["Radhakrishna Achanta", "Francisco Estrada", "Patricia Wils", "Sabine S\u00fcsstrunk"], "venue": "In Computer Vision Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Cognitive psychology and its implications", "author": ["John R Anderson"], "venue": "WH Freeman/Times Books/Henry Holt & Co,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Learning wake-sleep recurrent attention models", "author": ["Jimmy Ba", "Ruslan R Salakhutdinov", "Roger B Grosse", "Brendan J Frey"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Symbiotic segmentation and part localization for fine-grained categorization", "author": ["Yuning Chai", "Victor S. Lempitsky", "Andrew Zisserman"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches. Syntax, Semantics and Structure in Statistical Translation, page", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio"], "venue": "Multimedia, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Histograms of oriented gradients for human detection", "author": ["Navneet Dalal", "Bill Triggs"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Fei-Fei Li"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Misha Denil", "Loris Bazzani", "Hugo Larochelle", "Nando de Freitas"], "venue": "Neural Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Language models for image captioning: The quirks and what works", "author": ["Jacob Devlin", "Hao Cheng", "Hao Fang", "Saurabh Gupta", "Li Deng", "Xiaodong He", "Geoffrey Zweig", "Margaret Mitchell"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Trevor Darrell", "Kate Saenko"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest N. Iandola", "Rupesh K. Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C. Platt", "C. Lawrence Zitnick", "Geoffrey Zweig"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Fine-grained categorization by alignments", "author": ["Efstratios Gavves", "Basura Fernando", "Cees GM Snoek", "Arnold WM Smeulders", "Tinne Tuytelaars"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "An active search strategy for efficient object class detection", "author": ["Abel Gonzalez-Garcia", "Alexander Vezhnevets", "Vittorio Ferrari"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "A model of saliency-based visual attention for rapid scene analysis", "author": ["Laurent Itti", "Christof Koch", "Ernst Niebur"], "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Spatial transformer networks", "author": ["Max Jaderberg", "Karen Simonyan", "Andrew Zisserman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Aligning where to see and what to tell: image caption with region-based attention and scene factorization", "author": ["Junqi Jin", "Kun Fu", "Runpeng Cui", "Fei Sha", "Changshui Zhang"], "venue": "arXiv preprint arXiv:1506.06272,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Novel dataset for fine-grained image categorization: Stanford dogs", "author": ["Aditya Khosla", "Nityananda Jayadevaprakash", "Bangpeng Yao", "Fei-Fei Li"], "venue": "In Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Hugo Larochelle", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out", "author": ["Chin-Yew Lin"], "venue": "Proceedings of the ACL-04 workshop,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Microsoft COCO: common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C. Lawrence Zitnick"], "venue": "In Computer Vision - ECCV 2014 - 13th European Conference,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Wang Ling", "Chris Dyer", "Alan W. Black", "Isabel Trancoso"], "venue": "In NAACL HLT", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Not all contexts are created equal: Better word representations with variable attention", "author": ["Wang Ling", "Yulia Tsvetkov", "Silvio Amir", "Ramon Fermandez", "Chris Dyer", "Alan W. Black", "Isabel Trancoso", "Chu-Cheng Lin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Learning to detect a salient object", "author": ["Tie Liu", "Zejian Yuan", "Jian Sun", "Jingdong Wang", "Nanning Zheng", "Xiaoou Tang", "Heung-Yeung Shum"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter"], "venue": "In Proceedings of AAAI,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "In NIPS Deep Learning Workshop", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2002}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Attention for fine-grained categorization", "author": ["Pierre Sermanet", "Andrea Frome", "Esteban Real"], "venue": "arXiv preprint arXiv:1412.7054,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Convolutional LSTM networks for subcellular localization of proteins", "author": ["S\u00f8ren Kaae S\u00f8nderby", "Casper Kaae S\u00f8nderby", "Henrik Nielsen", "Ole Winther"], "venue": "In Algorithms for Computational Biology - Second International Conference, AlCoB 2015, Mexico City,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Selective search for object recognition", "author": ["Jasper R.R. Uijlings", "Koen E.A. van de Sande", "Theo Gevers", "Arnold W.M. Smeulders"], "venue": "International Journal of Computer Vision,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2013}, {"title": "Cider: Consensusbased image description evaluation", "author": ["Ramakrishna Vedantam", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2015}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["Paul Viola", "Michael Jones"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2001}, {"title": "The optimal reward baseline for gradientbased reinforcement learning", "author": ["Lex Weaver", "Nigel Tao"], "venue": "Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2001}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2015}, {"title": "Unsupervised template learning for fine-grained object recognition", "author": ["Shulin Yang", "Liefeng Bo", "Jue Wang", "Linda G. Shapiro"], "venue": "In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2012}, {"title": "Attentionnet: Aggregating weak directions for accurate object detection", "author": ["Donggeun Yoo", "Sunggyun Park", "Joon-Young Lee", "Anthony S Paek", "In So Kweon"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "The above mechanism is usually called attention [3].", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "[2, 24, 35].", "startOffset": 0, "endOffset": 11}, {"referenceID": 22, "context": "[2, 24, 35].", "startOffset": 0, "endOffset": 11}, {"referenceID": 32, "context": "[2, 24, 35].", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "The sliding window paradigm [11, 52] is another model which matches the essence of attention.", "startOffset": 28, "endOffset": 36}, {"referenceID": 49, "context": "The sliding window paradigm [11, 52] is another model which matches the essence of attention.", "startOffset": 28, "endOffset": 36}, {"referenceID": 17, "context": "For example, the local image features, saliency detection, sliding window methods introduced above, or some recently proposed object detection methods like [19], all employ the attention mechanism in different mathematical forms.", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "So it is natural to divide the whole system into two parts: the encoder, and the decoder [10, 47].", "startOffset": 89, "endOffset": 97}, {"referenceID": 44, "context": "So it is natural to divide the whole system into two parts: the encoder, and the decoder [10, 47].", "startOffset": 89, "endOffset": 97}, {"referenceID": 4, "context": ", machine translation [6, 37], machine comprehension [22], sentence summarization [43], word representation [33, 34].", "startOffset": 22, "endOffset": 29}, {"referenceID": 34, "context": ", machine translation [6, 37], machine comprehension [22], sentence summarization [43], word representation [33, 34].", "startOffset": 22, "endOffset": 29}, {"referenceID": 20, "context": ", machine translation [6, 37], machine comprehension [22], sentence summarization [43], word representation [33, 34].", "startOffset": 53, "endOffset": 57}, {"referenceID": 40, "context": ", machine translation [6, 37], machine comprehension [22], sentence summarization [43], word representation [33, 34].", "startOffset": 82, "endOffset": 86}, {"referenceID": 30, "context": ", machine translation [6, 37], machine comprehension [22], sentence summarization [43], word representation [33, 34].", "startOffset": 108, "endOffset": 116}, {"referenceID": 31, "context": ", machine translation [6, 37], machine comprehension [22], sentence summarization [43], word representation [33, 34].", "startOffset": 108, "endOffset": 116}, {"referenceID": 43, "context": "\u2013 Bioinformatics [46].", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": "\u2013 Speech recognition [8, 36].", "startOffset": 21, "endOffset": 28}, {"referenceID": 33, "context": "\u2013 Speech recognition [8, 36].", "startOffset": 21, "endOffset": 28}, {"referenceID": 35, "context": "\u2013 Game play [38].", "startOffset": 12, "endOffset": 16}, {"referenceID": 37, "context": ", sigmoid, tanh, ReLU [40], PReLU [21].", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": ", sigmoid, tanh, ReLU [40], PReLU [21].", "startOffset": 34, "endOffset": 38}, {"referenceID": 41, "context": "For more details about the neural network, one can refer to [44].", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "The convolutional neural network (CNN) is a specific type of neural network with structures designed for image inputs [30], which usually consists of multiple convolutional layers followed by a few fully-connected layers.", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": ", ImageNet [12]) has good generalization power/ability and can be easily transferred to other tasks.", "startOffset": 11, "endOffset": 15}, {"referenceID": 41, "context": "For more details about CNN, one can refer to [44].", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Now two widely used structures are LSTM (long-short term memory) [23] and GRU (gated recurrent units) [9].", "startOffset": 65, "endOffset": 69}, {"referenceID": 7, "context": "Now two widely used structures are LSTM (long-short term memory) [23] and GRU (gated recurrent units) [9].", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "Here we only introduce the first proposed item-wise soft attention model in [6] for natural language processing.", "startOffset": 76, "endOffset": 79}, {"referenceID": 11, "context": "This kind of location-wise hard attention is analyzed in many previous works [13, 28], while here we only", "startOffset": 77, "endOffset": 85}, {"referenceID": 26, "context": "This kind of location-wise hard attention is analyzed in many previous works [13, 28], while here we only", "startOffset": 77, "endOffset": 85}, {"referenceID": 36, "context": "focus on two recent proposed mechanisms in [39] and [4], because the attention models in those works are more general and can be trained end-to-end.", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "focus on two recent proposed mechanisms in [39] and [4], because the attention models in those works are more general and can be trained end-to-end.", "startOffset": 52, "endOffset": 55}, {"referenceID": 54, "context": "One can also let the attention network generate these parameters as the generation of lm j in Equation (18) [59].", "startOffset": 108, "endOffset": 112}, {"referenceID": 23, "context": "It is firstly proposed in [25] called the spatial transformation network (STN).", "startOffset": 26, "endOffset": 30}, {"referenceID": 23, "context": "Instead of the RNN model, the STN in [25] is initially applied on a CNN model, but it can be easily transferred to a RNN model with tiny modifications.", "startOffset": 37, "endOffset": 41}, {"referenceID": 23, "context": "The figure is taken from [25].", "startOffset": 25, "endOffset": 29}, {"referenceID": 23, "context": "Since the original STN in [25] is applied on a CNN, there is no hidden state.", "startOffset": 26, "endOffset": 30}, {"referenceID": 23, "context": "As a result, in [25] the location network only takes Xin as input, and Equation (25) changes to Aj = \u03c6Wloc(Xin).", "startOffset": 16, "endOffset": 20}, {"referenceID": 36, "context": "In general, this is a problem of training a neural network with discrete values/units, and here we just introduced the method applied in [39, 4, 57] which employs the techniques from the reinforcement learning.", "startOffset": 137, "endOffset": 148}, {"referenceID": 2, "context": "In general, this is a problem of training a neural network with discrete values/units, and here we just introduced the method applied in [39, 4, 57] which employs the techniques from the reinforcement learning.", "startOffset": 137, "endOffset": 148}, {"referenceID": 52, "context": "In general, this is a problem of training a neural network with discrete values/units, and here we just introduced the method applied in [39, 4, 57] which employs the techniques from the reinforcement learning.", "startOffset": 137, "endOffset": 148}, {"referenceID": 51, "context": "As shown above, lj is generated from a distribution (Equation (15), Equation (17), or Equation (18)), which indicates that p(lj |xl) and \u2202 log p(lj |x) \u2202\u03b8 can be estimated by a Monte Carlo sampling as demonstrated in [56]: \u2202Lj \u2202\u03b8 \u2248 1 M M \u2211", "startOffset": 217, "endOffset": 221}, {"referenceID": 51, "context": "The whole learning process described above for the hard attention is equivalent to the REINFORCE learning rule in [56], and from the reinforcement learning perspective, after each step, the model can get a reward from the environment.", "startOffset": 114, "endOffset": 118}, {"referenceID": 36, "context": "Now as shown in [39], although Equation (47) is an unbiased estimation of the real gradient of Equation (44), it may have high variance because of the unbounded Rj .", "startOffset": 16, "endOffset": 20}, {"referenceID": 2, "context": "where b can be calculated in different ways which are discussed in [4, 5, 39, 53, 57].", "startOffset": 67, "endOffset": 85}, {"referenceID": 3, "context": "where b can be calculated in different ways which are discussed in [4, 5, 39, 53, 57].", "startOffset": 67, "endOffset": 85}, {"referenceID": 36, "context": "where b can be calculated in different ways which are discussed in [4, 5, 39, 53, 57].", "startOffset": 67, "endOffset": 85}, {"referenceID": 50, "context": "where b can be calculated in different ways which are discussed in [4, 5, 39, 53, 57].", "startOffset": 67, "endOffset": 85}, {"referenceID": 52, "context": "where b can be calculated in different ways which are discussed in [4, 5, 39, 53, 57].", "startOffset": 67, "endOffset": 85}, {"referenceID": 46, "context": "For the extraction method, one can either extract some fixed size patches from some pre-defined locations, or apply other object proposal methods, like [49].", "startOffset": 152, "endOffset": 156}, {"referenceID": 36, "context": "As a result, [39] and [4] propose the RNN based image classification models with the location-wise hard attention mechanism.", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "As a result, [39] and [4] propose the RNN based image classification models with the location-wise hard attention mechanism.", "startOffset": 22, "endOffset": 25}, {"referenceID": 36, "context": "So the models in [39] and [4] can not only make prediction of the image label, but also localize the position of the object.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "So the models in [39] and [4] can not only make prediction of the image label, but also localize the position of the object.", "startOffset": 26, "endOffset": 29}, {"referenceID": 46, "context": "If one wants to apply a convolution network to solve the object detection problem, he has to use a separate model to propose the potential locations of the objects, like [49], which is expensive.", "startOffset": 170, "endOffset": 174}, {"referenceID": 36, "context": "In this section, we will briefly introduce and analyze the models proposed in [39], [4], and their extensions.", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "In this section, we will briefly introduce and analyze the models proposed in [39], [4], and their extensions.", "startOffset": 84, "endOffset": 87}, {"referenceID": 36, "context": "A brief structure of the RNN model in [39] is shown in Figure 12.", "startOffset": 38, "endOffset": 42}, {"referenceID": 36, "context": "The experiments in [39] compare the performances of the proposed model with some nonrecurrent neural networks, and the results show the superiority of the attention based RNN model to the non-recurrent neural networks with similar number of parameters, especially on the datasets with noise.", "startOffset": 19, "endOffset": 23}, {"referenceID": 36, "context": "1 in [39].", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "[4] proposes an extended version of model as shown in Figure 13 by firstly making the network deeper, and secondly using a context vector to obtain a better first glimpse.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "The encoder (enc) in [4] is deeper, i.", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "The same context vector of the whole image is not fed into the decoder network r(1), because [4] observes that if so, the predicted", "startOffset": 93, "endOffset": 96}, {"referenceID": 36, "context": "Figure 12: The model proposed in [39].", "startOffset": 33, "endOffset": 37}, {"referenceID": 2, "context": "The figure is taken from [4], and some modifications are made to fit the model in [39].", "startOffset": 25, "endOffset": 28}, {"referenceID": 36, "context": "The figure is taken from [4], and some modifications are made to fit the model in [39].", "startOffset": 82, "endOffset": 86}, {"referenceID": 36, "context": "Model Test error [39] 9% [4] without the context vector 7% [4] 5%", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "Model Test error [39] 9% [4] without the context vector 7% [4] 5%", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "Model Test error [39] 9% [4] without the context vector 7% [4] 5%", "startOffset": 59, "endOffset": 62}, {"referenceID": 36, "context": "The MNIST pairs dataset randomly puts two digits into a 100\u00d7100 image with some additional noise in the background [39] (Figure 14b).", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "It is clear that the performance of [4] is better, and the context vector indeed makes some improvements by suggesting a more accurate first glimpse.", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "The model in [4] are also tested on multi-digit street view house number (SVHN) [41] sequence recognition task, and the results show that with the same level of error rates, the number of parameters to be learned, and the training time of the attention based RNN model are much less than the state-of-the-art CNN methods.", "startOffset": 13, "endOffset": 16}, {"referenceID": 38, "context": "The model in [4] are also tested on multi-digit street view house number (SVHN) [41] sequence recognition task, and the results show that with the same level of error rates, the number of parameters to be learned, and the training time of the attention based RNN model are much less than the state-of-the-art CNN methods.", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "Figure 13: The model proposed in [4].", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "The figure is taken from [4], and some modifications are made.", "startOffset": 25, "endOffset": 28}, {"referenceID": 36, "context": "(b) Two examples taken from the MNIST pairs dataset [39].", "startOffset": 52, "endOffset": 56}, {"referenceID": 42, "context": "[45] further extends the model in [4] with only a few modifications and applies it to a real-world image classification task: Stanford Dogs fine-grained categorization task [27], in which the images have larger clutter, occlusion, and variations in pose.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[45] further extends the model in [4] with only a few modifications and applies it to a real-world image classification task: Stanford Dogs fine-grained categorization task [27], in which the images have larger clutter, occlusion, and variations in pose.", "startOffset": 34, "endOffset": 37}, {"referenceID": 25, "context": "[45] further extends the model in [4] with only a few modifications and applies it to a real-world image classification task: Stanford Dogs fine-grained categorization task [27], in which the images have larger clutter, occlusion, and variations in pose.", "startOffset": 173, "endOffset": 177}, {"referenceID": 42, "context": "The biggest change made in [45] is that a part of the GoogLeNet [48] is used as the encoder, which is a pre-trained CNN on ImageNet dataset.", "startOffset": 27, "endOffset": 31}, {"referenceID": 45, "context": "The biggest change made in [45] is that a part of the GoogLeNet [48] is used as the encoder, which is a pre-trained CNN on ImageNet dataset.", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "Model Mean accuracy (MA) [7] [58]* 0.", "startOffset": 25, "endOffset": 28}, {"referenceID": 53, "context": "Model Mean accuracy (MA) [7] [58]* 0.", "startOffset": 29, "endOffset": 33}, {"referenceID": 5, "context": "38 [7]* 0.", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "46 [18]* 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 42, "context": "50 GoogLeNet 96\u00d796 (the encoder in [45]) 0.", "startOffset": 35, "endOffset": 39}, {"referenceID": 42, "context": "42 RNN with location-wise hard attention [45] 0.", "startOffset": 41, "endOffset": 45}, {"referenceID": 48, "context": "The RNN based image caption system is recently proposed in [51] (Neural Image Caption, NIC) which perfectly fits our encoder-decoder RNN framework without the attention mechanism.", "startOffset": 59, "endOffset": 63}, {"referenceID": 52, "context": "[57] adds the item-wise attention mechanisms into the NIC system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[57] implements both the item-wise soft and the item-wise hard attention", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "The figure is taken from [51].", "startOffset": 25, "endOffset": 29}, {"referenceID": 52, "context": "By using the attention mechanism, [57] reports improvements compared to the common RNN models without attention mechanism including NIC.", "startOffset": 34, "endOffset": 38}, {"referenceID": 52, "context": "However, both NIC and [57] have participated in the MS COCO Captioning Challenge 20155, and the competition gives opposite results.", "startOffset": 22, "endOffset": 26}, {"referenceID": 29, "context": "MS COCO Captioning Challenge 2015 is an image caption competition running on one of the biggest image caption datasets: Microsoft COCO [32].", "startOffset": 135, "endOffset": 139}, {"referenceID": 39, "context": "The performance is evaluated by human beings as well as some commonly used evaluation metrics (BLEU [42], Meteor [14], ROUGE-L [31] and CIDEr-D [50]).", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "The performance is evaluated by human beings as well as some commonly used evaluation metrics (BLEU [42], Meteor [14], ROUGE-L [31] and CIDEr-D [50]).", "startOffset": 113, "endOffset": 117}, {"referenceID": 28, "context": "The performance is evaluated by human beings as well as some commonly used evaluation metrics (BLEU [42], Meteor [14], ROUGE-L [31] and CIDEr-D [50]).", "startOffset": 127, "endOffset": 131}, {"referenceID": 47, "context": "The performance is evaluated by human beings as well as some commonly used evaluation metrics (BLEU [42], Meteor [14], ROUGE-L [31] and CIDEr-D [50]).", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "Model Human Other evaluation metrics M1 M2 BLEU-1 BLEU-2 BLEU-3 BLEU-4 Meteor ROUGE-L CIDEr-D Human 1 1 6 12 12 13 3 11 6 Google NIC 2 3 2 2 2 2 1 1 1 MSR [17] 3 2 5 5 5 5 4 5 4 [57] 4 5 9 9 9 9 6 8 9 MSR Captivator [15] 5 4 1 1 1 1 2 2 2 Berkeley LRCN [16] 6 6 10 7 8 8 7 6 8", "startOffset": 155, "endOffset": 159}, {"referenceID": 52, "context": "Model Human Other evaluation metrics M1 M2 BLEU-1 BLEU-2 BLEU-3 BLEU-4 Meteor ROUGE-L CIDEr-D Human 1 1 6 12 12 13 3 11 6 Google NIC 2 3 2 2 2 2 1 1 1 MSR [17] 3 2 5 5 5 5 4 5 4 [57] 4 5 9 9 9 9 6 8 9 MSR Captivator [15] 5 4 1 1 1 1 2 2 2 Berkeley LRCN [16] 6 6 10 7 8 8 7 6 8", "startOffset": 178, "endOffset": 182}, {"referenceID": 13, "context": "Model Human Other evaluation metrics M1 M2 BLEU-1 BLEU-2 BLEU-3 BLEU-4 Meteor ROUGE-L CIDEr-D Human 1 1 6 12 12 13 3 11 6 Google NIC 2 3 2 2 2 2 1 1 1 MSR [17] 3 2 5 5 5 5 4 5 4 [57] 4 5 9 9 9 9 6 8 9 MSR Captivator [15] 5 4 1 1 1 1 2 2 2 Berkeley LRCN [16] 6 6 10 7 8 8 7 6 8", "startOffset": 216, "endOffset": 220}, {"referenceID": 14, "context": "Model Human Other evaluation metrics M1 M2 BLEU-1 BLEU-2 BLEU-3 BLEU-4 Meteor ROUGE-L CIDEr-D Human 1 1 6 12 12 13 3 11 6 Google NIC 2 3 2 2 2 2 1 1 1 MSR [17] 3 2 5 5 5 5 4 5 4 [57] 4 5 9 9 9 9 6 8 9 MSR Captivator [15] 5 4 1 1 1 1 2 2 2 Berkeley LRCN [16] 6 6 10 7 8 8 7 6 8", "startOffset": 253, "endOffset": 257}, {"referenceID": 52, "context": "are 17 teams participating the competition in total, and here we list the rankings of some top teams in Table 3 including [57].", "startOffset": 122, "endOffset": 126}, {"referenceID": 52, "context": "Except for the team \u201dHuman\u201d and the system proposed by [57], all other teams shown in Table 3 use models without the attention mechanism.", "startOffset": 55, "endOffset": 59}, {"referenceID": 52, "context": "The model in [57] uses fixed-size feature maps (14\u00d714) to construct the code set C, and each feature map corresponds to a fixed-size patch of the input image.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "[26] claims that this design may harm the performance because some \u201dmeaningful scenes\u201d may only occupy a small part of the image patch or cannot be cover by a single patch, where the meaningful scene indicates the objects/scenes correspond to the word is about to be predicted.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] makes an improvement by firstly extract many object proposals [49] from an input image, which potentially contain the meaningful scenes, as the input sequence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[26] makes an improvement by firstly extract many object proposals [49] from an input image, which potentially contain the meaningful scenes, as the input sequence.", "startOffset": 67, "endOffset": 71}, {"referenceID": 24, "context": "However, [26] neither directly compares its performance to [57], nor attends the MS COCO Captioning Challenge 2015.", "startOffset": 9, "endOffset": 13}, {"referenceID": 52, "context": "However, [26] neither directly compares its performance to [57], nor attends the MS COCO Captioning Challenge 2015.", "startOffset": 59, "endOffset": 63}, {"referenceID": 24, "context": "Still, the problem proposed in [26] is very interesting and cannot be ignored: when the input of a RNN model is an", "startOffset": 31, "endOffset": 35}, {"referenceID": 52, "context": "The figure is taken from [57].", "startOffset": 25, "endOffset": 29}, {"referenceID": 3, "context": "Can the learning method be improved? [5] gives some insights but it is still an open question.", "startOffset": 37, "endOffset": 40}, {"referenceID": 18, "context": "Another direction is to further generalize an RNN and make it have a similar structure as the modern computer architecture [54, 20], e.", "startOffset": 123, "endOffset": 131}], "year": 2016, "abstractText": "3", "creator": "LaTeX with hyperref package"}}}