{"id": "1601.06931", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2016", "title": "Fisher Motion Descriptor for Multiview Gait Recognition", "abstract": "The goal of this paper is to identify individuals by analyzing their gait. Instead of using binary silhouettes as input data (as done in many previous works) we propose and evaluate the use of motion descriptors based on densely sampled short-term trajectories. We take advantage of state-of-the-art people detectors to define custom spatial configurations of the descriptors around the target person, obtaining a rich representation of the gait motion. The local motion features (described by the Divergence-Curl-Shear descriptor) extracted on the different spatial areas of the person are combined into a single high-level gait descriptor by using the Fisher Vector encoding. The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on `CASIA' dataset (parts B and C), `TUM GAID' dataset, `CMU MoBo' dataset and the recent `AVA Multiview Gait' dataset. The results show that this new approach achieves state-of-the-art results in the problem of gait recognition, allowing to recognize walking people from diverse viewpoints on single and multiple camera setups, wearing different clothes, carrying bags, walking at diverse speeds and not limited to straight walking paths.", "histories": [["v1", "Tue, 26 Jan 2016 09:05:26 GMT  (13202kb,D)", "http://arxiv.org/abs/1601.06931v1", "This paper extends with new experiments the one published at ICPR'2014"]], "COMMENTS": "This paper extends with new experiments the one published at ICPR'2014", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["f m castro", "m j mar\\'in-jim\\'enez", "n guil", "r mu\\~noz-salinas"], "accepted": false, "id": "1601.06931"}, "pdf": {"name": "1601.06931.pdf", "metadata": {"source": "CRF", "title": "Fisher Motion Descriptor for Multiview Gait Recognition", "authors": ["F.M. Castro", "M.J. Ma\u0155\u0131n-Jim\u00e9nezb", "N. Guil", "R. Mu\u00f1oz-Salinas"], "emails": ["fcastro@uma.es", "mjmarin@uco.es", "nguil@uma.es"], "sections": [{"heading": null, "text": "Instead of using binary silhouettes as input data (as has been the case in many previous work), we propose the use of motion descriptors and evaluate them using tightly sampled short-term trajectories. We use state-of-the-art person detectors to define tailor-made spatial configurations of the descriptors around the target, resulting in a rich representation of gait motion. Local motion characteristics (described by the divergence curl shear descriptor [1]), which are extracted in the different spatial areas of the person, are combined into a single high-level gait descriptor using Fisher vector coding [2]. The proposed approach with Pyramidal Fisher Motion coding is experimentally validated using \"CASIA\" datasets (Parts B and C), \"TUM GAID\" dataset [4], \"CMU Modato Motion,\" these multiple viewfinders of the Aneuter A5 view types."}, {"heading": "1. Introduction", "text": "The term gait refers to the way each person walks. In fact, people are well able to recognize people at a distance by their gait [7], which provides a good (non-invasive) method of identifying people without requiring their cooperation, as opposed to other biometric approaches such as iris or fingerprint analysis. Some potential applications are access controls in specialized areas (e.g. military bases or government facilities) or intelligent video surveillance (e.g. bank offices or subway stations), where it is crucial to identify potentially dangerous people without their cooperation. Although great efforts have been made in recent years [8], it is still far from solvent. Popular approaches to gait recognition require the compilation of binary silhouettes of people [9], usually through the application of some background segmentation techniques. However, this is a clear limitation in the presence of dynamic backgrounds and / or non-static cameras where noisy segmentations are achieved."}, {"heading": "1.1. Related work", "text": "Many research papers have been published in recent years to solve the problem of human gait recognition using various data sources such as inertial sensors [12, 13], foot pressure [14], infrared images [15], or traditional images. For example, in [8] we can find an overview of this problem, which summarizes some of the most popular approaches, some of which use explicit geometric models of human bodies, while others use only image characteristics. A sequence of binary silhouettes of the body is used as input data in many papers. In this sense, the most popular silhouette-based gait descriptor is the so-called Gait Enery Image (GEI). The key idea is to compile a temporal averaging of the binary silhouette of the target subject. Liu et al. [16] To improve gait recognition performance, we project the compilation of HOG descriptors from popular gait descriptors (GI)."}, {"heading": "2. Proposed framework", "text": "In this section, we present our proposed framework for solving the problem of gait recognition. Fig. 2 summarizes the pipeline of our approach. First, we calculate local motion descriptors from traces of tightly sampled points throughout the scene (Fig. 2.1). Since we do not assume a static background, we operate a person detector to remove unrelated point paths (Fig. 2.2). Furthermore, we spatially divide the person regions to aggregate the local motion descriptors into mid-level descriptors (Fig. 2.3). Finally, a discriminatory classifier is used to identify the subjects (Fig. 2.4)."}, {"heading": "2.1. Motion-based features", "text": "The first step of our pipeline is the calculation of densely sampled trajectories. These trajectories are calculated according to the approach of Wang et al. [10] First, the dense optical flux F = (ut, vt) is calculated on an adense grid [28] (i.e. step size of 5 pixels and over 8 scales), then each point pt = (xt, yt) on frame t is tracked to the next frame by median filtering as follows: pt + 1 = (xt + 1, yt + 1) = (xt, yt) + (M \u0445 F) | (x \u0445 t, y \u0441t) (1), where M is the core of the median filtering and (x, y, t) is the rounded position of pt. To minimize the drift effect, tracking is limited to the L frames. We use L = 15 as in [1]. As a post-processing step, noisy and uninformative trajectories are used."}, {"heading": "2.2. People detection and tracking", "text": "We follow a tracking-by-detection strategy as in [29]: we recognize people with the BB = BB-Tracks detection frame that we have not yet recognized. [30] And then we apply the clique partitioning algorithm of Ferrari et al. [31] to group detections in tracks (i.e. 3 \u00d7 16 bins histograms in RGB space and distance). To get the color histogram of a track, we calculate the average of all histograms that compose a track. Finally, we compare the similarity of tracks by distance. This is especially useful when the detector measures the person for a period of time."}, {"heading": "2.3. Pyramidal Fisher Motion Descriptor", "text": "Fisher Motion. As described above, our low-level characteristics are based on motion characteristics extracted from personal local trajectories = = 18. To create a personal gait descriptor, we must summarize the local characteristics. We propose the use of Fisher vectors (FV), where each Gaussian encoding corresponds to a visual word [2]. FV, which can be considered an extension of the bag of words (BOW), is represented by the number of occurrences of each visual word, in FV an image is described by a gradient vector computed from a generative probability model. The dimensionality of FV is 2ND, where N is the number of Gaussians in the GMM, and D is the dimensionality of the local motion descriptors."}, {"heading": "2.4. Classification", "text": "The final stage of our pipeline is to train a discriminatory classifier to distinguish between different human gaits. As this is a multi-class problem, we train P-binary Support Vector Machines (SVM) [35] (as many as different people) in a one-on-one strategy. Although the \u03c72 kernel is a popular choice for BOW-based descriptors, a linear kernel for FV is typically sufficient due to the rich feature representation it provides."}, {"heading": "2.5. Implementation details", "text": "For personal identification, we use the code published by the authors of [30]. For the calculation of local motion characteristics, we use the code published by the authors of [1]. Fisher-Vector encoding and classification is based on the code contained in the library VLFeat2."}, {"heading": "3. Overview of the experiments", "text": "To validate our approach, we are conducting various experiments on four sets of data: \"AVA Multi-View Dataset,\" \"CMU MoBo Dataset,\" \"CASIA B and C Datasets\" and \"TUM GAID Dataset.\" With these experiments, we are trying to answer questions such as: a) Is the combination of path-based features with FV a valid approach to gait recognition?; b) Can we learn different camera perspectives in a single classifier?; c) Can we improve the detection rate by spatially dividing the human body region?; d) What effect does the use of PCA-based dimension reduction have on detection performance?; e) What impact does the sequence length have on detection performance?; f) Is it necessary to use the DCS descriptor as a whole or can we use only some of its components?; and g) can the proposed model build on unrestricted tracks?"}, {"heading": "4. Experimental results on AVAMVG", "text": "The first dataset in which we perform our experiments is the \"AVA Multi-View Dataset for Gait Recognition\" (AVAMVG) [6]. In AVAMVG, 20 subjects perform 10 raceways in an indoor space. Each raceway is recorded by 6 color cameras placed around a room that the subjects traverse during the performance. Fig. 4 shows the scenario from the six camera perspectives available. Note that depending on the angle of view and trajectory performed, people appear on different scales and even show partially closed body parts. In particular, the third and sixth camera points shown in Fig. 4 are more likely to show partially visible bodies than the other four cameras most of the time. Therefore, in our experiments we will use only four cameras (i.e. Cam00, Cam01, Cam03, Cam04) without any loss of universality."}, {"heading": "4.1. Experimental setup", "text": "It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. \"(...) It is.\" (...) It is. \"(...) It is.\" (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...). It is. (...) It is. (...) It is. (...). It is. (...) It is. (...). (...) It is. (...) It is. (...). It is. It is. (...). (...) It is. (...) It is. It is. (...). It is. It is. (...). It is. It is. (...). It is. (...). It is. It is. It is. (...). (... It is. (...). It is. It is. (). It is. It is. (). (...). It is. It is. It is. It is. (...). (). It is. ()."}, {"heading": "4.2. Results", "text": "This year, the time has come for a considerable effort to be made in the first half of the year."}, {"heading": "5. Experimental results on MoBo", "text": "The second data set in which we carry out our experiments is the \"CMU Motion of Body\" (MoBo) database [5]. MoBo contains video sequences of 25 subjects who perform four different running patterns on a treadmill: slow walking, fast walking, inclined walking and walking with a ball. They were recorded from six camera perspectives. Fig. 5 shows six video images from the data set. The video resolution is 480 x 640 pixels."}, {"heading": "5.1. Experimental setup", "text": "We are conducting experiments similar to the above to further evaluate our proposed gait recognition framework. Note that unlike AVAMVG datasets, in which people actually move around the room, there is no actual displacement of the body in MoBo datasets while people walk on a treadmill. Therefore, we will only use four of the cameras associated with the body as a whole. In addition, there are a number of videos in which people do not move their arms freely while holding a ball, removing the movement pattern associated with the arm swing."}, {"heading": "5.2. Results", "text": "In this context, it should be noted that this project is a project, which is primarily a project."}, {"heading": "6. Experimental results on CASIA", "text": "The third dataset in which we conduct our experiments is the \"CASIA Gait Dataset\" [41], Part B (CASIAB) [3] and C (CASIA-C) [42]. In CASIA-B, 124 subjects perform walking paths in an indoor environment. The action is recorded from 11 angles. Three situations are considered: normal gait (nm), wearing a coat (cl) and carrying a bag (bg). The first camera point in the dataset is 0o and the last 180o, the middle ones are separated by 18o. Some examples are in the top row of Fig. 6. In CASIA-C, 153 subjects perform walking paths in an outdoor environment at night. The action is captured from a single vantage point using an infrared camera, taking into account four situations: normal gait (fn), slow gait (carrying) and a bag (carrying)."}, {"heading": "6.1. Experimental setup", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "6.2. Results", "text": "This year, it is only a matter of time before we reach an agreement."}, {"heading": "7. Experimental results on TUM GAID", "text": "The fourth data set in which we conduct our experiments is \"TUM Gait from Audio, Image and Depth (GAID) Database.\" [4] In TUM GAID 305 subjects perform two walkways in an indoor space, the first track is performed from left to right and the second from right to left. Therefore, both sides of the subjects are recorded. Two recording sessions were conducted, one in January, where the subjects wear heavy jackets and mostly winter boots, and the second in April, the subjects were wearing different clothing. Some examples can be seen in the Fig. 7. The top row shows three subjects recorded in the first session, each in a different state of walking, wearing a backpack and wearing protective shoes. The bottom row shows the same three people who are recorded in the second session under the same conditions as in the first session."}, {"heading": "7.1. Experimental setup", "text": "We carry out the experiments suggested by the authors of the dataset under [4]. Note that we carry out the experiments only in the RGB video stream and depth and audio streams for future improvements to our algorithm.In our experiments, we use the training and validation sets combined as in [4] to build the dictionary. The test set is used for training and testing the person-specific classifiers.In order to increase the number of training samples, we generate their mirror sequences, doubling the amount of samples available during the learning.Experiment A: Identification through different running conditions In this experiment, we use the first four sequences of normal walking (N1, N2, N3 N4) for training and the sequences N5, N6, B2, S1, S2 of normal walking, S2 of normal walking, S2 of normal running shoes for each test."}, {"heading": "7.2. Results", "text": "The results of the study show that the majority of them are people who are able to move, and that they are able to move."}, {"heading": "8. Final Discussion", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "8.1. Speed of the proposed system", "text": "To get an idea of the average speed of our system, we split up the time processing of the different phases it consists of. We have conducted these experiments on a state-of-the-art desktop computer with a 3.47 GHz CPU and 24 GB RAM. Most of the non-parallel code is written in Matlab, with some pieces of code written in C + +. The average time needed in seconds to process a video sequence of 50 images from CASIAB (320 x 240 pixels) is as follows: a) Calculation of dense tracks, 8.95; b) Person recognition, 54.2; c) Person tracking plus filtering of tracklets, 0.62; d) PFM calculation, 0.23; and e) SVM classification, 0.02. This gives a total of about 64 seconds for this type of 50-frame video sequence. It is clear that the computational bottleneck is located on the personal identification module."}, {"heading": "9. Conclusions", "text": "We have presented a new approach to detecting the human gait in video sequences. Our method builds a motion-based representation of the human gait by combining tightly sampled local features and Fisher vectors: the Pyramid Fishers Motion.The results show that PFM can achieve a high detection rate on a multi-camera system based on the evaluated data sets: AVAMVG, CMU MoBo and CASIA (relying on B and C) and on a single camera system such as TUM GAID. In the case of AVAMVG, perfect identification of individuals is achieved by combining information from different cameras and following the subjects a straight path. Furthermore, our pipeline shows good behaviour in unrestricted ways, as demonstrated by the experimental results - the model is trained on straight tracks and tested paths."}, {"heading": "Acknowledgments", "text": "This work was partially funded by the research projects TIN2012-32952 and BROCA, both funded by FEDER and the Spanish Ministry of Science and Technology; and by the project TIC-1692 (Junta de Andaluc'\u0131a). We also thank David Lo'pez for his help in creating the AVAMVG dataset. Parts of the research in this paper use the CASIA Gait Database, provided by the Institute of Automation, Chinese Academy of Sciences.7The HOG-based person detector, which is available in the OpenCV library, can be run for 60 pennies on our computers."}], "references": [{"title": "Better exploiting motion for better action recognition", "author": ["M. Jain", "H. Jegou", "P. Bouthemy"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving the fisher kernel for large-scale image classification", "author": ["F. Perronnin", "J. S\u00e1nchez", "T. Mensink"], "venue": "in: Proceedings of the European Conference on Computer Vision (ECCV)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "A framework for evaluating the effect of view angle", "author": ["S. Yu", "D. Tan", "T. Tan"], "venue": "clothing and carrying condition on gait recognition, in: Proceedings of the International Conference on Pattern Recognition, Vol. 4", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "The TUM Gait from Audio", "author": ["M. Hofmann", "J. Geiger", "S. Bachmann", "B. Schuller", "G. Rigoll"], "venue": "Image and Depth (GAID) database: Multimodal recognition of subjects and traits, Journal of Visual Communication and Image Representation 25 (1) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "The CMU Motion of Body (MoBo) Database", "author": ["R. Gross", "J. Shi"], "venue": "Tech. Rep. CMU-RI-TR-01-18, Robotics Institute ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "The AVA multi-view dataset for gait recognition", "author": ["D. L\u00f3pez-Fern\u00e1ndez", "F. Madrid-Cuevas", "A. Carmona-Poyato", "M. Ma\u0155\u0131n-Jim\u00e9nez", "R. Mu\u00f1oz Salinas"], "venue": "in: Activity Monitoring by Multiple Distributed Sensing, Lecture Notes in Computer Science", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognizing friends by their walk: Gait perception without familiarity cues", "author": ["J.E. Cutting", "L.T. Kozlowski"], "venue": "Bulletin of the psychonomic society 9 (5) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1977}, {"title": "A survey on visual surveillance of object motion and behaviors", "author": ["W. Hu", "T. Tan", "L. Wang", "S. Maybank"], "venue": "Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on 34 (3) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Individual recognition using gait energy image", "author": ["J. Han", "B. Bhanu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 28 (2) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Action Recognition by Dense Trajectories", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C.-L. Liu"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Video Google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "in: Proceedings of the International Conference on Computer Vision (ICCV), Vol. 2", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Similar gait action recognition using an inertial sensor", "author": ["T.T. Ngo", "Y. Makihara", "H. Nagahara", "Y. Mukaigawa", "Y. Yagi"], "venue": "Pattern Recognition 48 (4) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "The largest inertial sensor-based gait database and performance evaluation of gait-based personal authentication", "author": ["T.T. Ngo", "Y. Makihara", "H. Nagahara", "Y. Mukaigawa", "Y. Yagi"], "venue": "Pattern Recognition 47 (1) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A cascade fusion scheme for gait and cumulative foot pressure image recognition", "author": ["S. Zheng", "K. Huang", "T. Tan", "D. Tao"], "venue": "Pattern Recognition 45 (10) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Infrared gait recognition based on wavelet transform and support vector machine", "author": ["Z. Xue", "D. Ming", "W. Song", "B. Wan", "S. Jin"], "venue": "Pattern Recognition 43 (8) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiple HOG templates for gait recognition", "author": ["Y. Liu", "J. Zhang", "C. Wang", "L. Wang"], "venue": "in: Proceedings of the International Conference on Pattern Recognition, IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploring the effects of video length on gait recognition", "author": ["R. Martin-Felez", "J. Ortells", "R. Mollineda"], "venue": "in: Proceedings of the International Conference on Pattern Recognition", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Gait recognition by ranking", "author": ["R. Mart\u0301\u0131n-F\u00e9lez", "T. Xiang"], "venue": "in: Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Uncooperative gait recognition by learning to rank, Pattern Recognition", "author": ["R. Mart\u0301\u0131n-F\u00e9lez", "T. Xiang"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Video from nearly still: an application to low frame-rate gait recognition", "author": ["N. Akae", "A. Mansur", "Y. Makihara", "Y. Yagi"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Enhanced gabor feature based classification using a regularized locally tensor discriminant model for multiview gait recognition", "author": ["H. Hu"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on 23 (7) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiview gait recognition based on patch distribution features and uncorrelated multilinear sparse local discriminant canonical correlation analysis", "author": ["H. Hu"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on 24 (4) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Human gait recognition via sparse discriminant projection learning", "author": ["Z. Lai", "Y. Xu", "Z. Jin", "D. Zhang"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on 24 (10) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Identification of people walking along curved trajectories", "author": ["Y. Iwashita", "K. Ogawara", "R. Kurazume"], "venue": "Pattern Recognition Letters 48 (0) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Fisher kernels on visual vocabularies for image categorization", "author": ["F. Perronnin", "C. Dance"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Fisher tensor decomposition for unconstrained gait recognition", "author": ["W. Gong", "M. Sapienza", "F. Cuzzolin"], "venue": "in: Proc. of Tensor Methods for Machine Learning, Workshop of the European Conference of Machine Learning", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Pyramidal Fisher Motion for multiview gait recognition", "author": ["F.M. Castro", "M. Ma\u0155\u0131n-Jim\u00e9nez", "R. Medina-Carnicer"], "venue": "in: Proceedings of the International Conference on Pattern Recognition", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Two-frame motion estimation based on polynomial expansion", "author": ["G. Farneb\u00e4ck"], "venue": "in: Proc. of Scandinavian Conf. on Image Analysis, Vol. 2749", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "2D articulated human pose estimation and retrieval in (almost) unconstrained still images", "author": ["M. Eichner", "M.J. Ma\u0155\u0131n-Jim\u00e9nez", "A. Zisserman", "V. Ferrari"], "venue": "International Journal of Computer Vision 99 (2) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time affine region tracking and coplanar grouping", "author": ["V. Ferrari", "T. Tuytelaars", "L. Van Gool"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Detecting people looking at each other in videos", "author": ["M. Marin-Jimenez", "A. Zisserman", "M. Eichner", "V. Ferrari"], "venue": "International Journal of Computer Vision 106 (3) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Human Detection and Action Recognition in Video Sequences - Human Character Recognition in TV-Style Movies", "author": ["A. Kl\u00e4ser"], "venue": "Master thesis, Bonn-Rhein-Sieg University of Applied Sciences ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "N", "author": ["M. Ma\u0155\u0131n-Jim\u00e9nez"], "venue": "P\u00e9rez de la Blanca, M. Mendoza, Human action recognition from simple feature pooling, Pattern Analysis and Applications 17 (1) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "F", "author": ["E. Osuna", "R. Freund"], "venue": "Girosi, Support Vector Machines: training and applications., Tech. Rep. AI-Memo 1602, MIT ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1997}, {"title": "Silhouette-based human identification from body shape and gait", "author": ["R. Collins", "R. Gross", "J. Shi"], "venue": "in: Proceedings of the International Conference on Automatic Face and Gesture Recognition", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2002}, {"title": "Appearance-based gait recognition using independent component analysis", "author": ["J. Liang", "Y. Chen", "H. Hu", "H. Zhao"], "venue": "in: Proc. Int. Conf. on Advances in Natural Computation", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Silhouette-based gait recognition using procrustes shape analysis and elliptic fourier descriptors", "author": ["S.D. Choudhury", "T. Tjahjadi"], "venue": "Pattern Recognition 45 (9) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Gait recognition based on improved dynamic bayesian networks", "author": ["C. Chen", "J. Liang", "X. Zhu"], "venue": "Pattern Recognition 44 (4) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Gait probability image: An information-theoretic model of gait representation", "author": ["C.P. Lee", "A.W. Tan", "S.C. Tan"], "venue": "Journal of Visual Communication and Image Representation 25 (6) ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient night gait recognition based on template matching", "author": ["D. Tan", "K. Huang", "S. Yu", "T. Tan"], "venue": "in: Proceedings of the International Conference on Pattern Recognition", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Active energy image plus 2DLPP for gait recognition", "author": ["E. Zhang", "Y. Zhao", "W. Xiong"], "venue": "Signal Processing 90 (7) ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Incremental learning for video-based gait recognition with LBP flow", "author": ["M. Hu", "Y. Wang", "Z. Zhang", "D. Zhang", "J. Little"], "venue": "Cybernetics, IEEE Transactions on 43 (1) ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Human identification using temporal information preserving gait template", "author": ["C. Wang", "J. Zhang", "L. Wang", "J. Pu", "X. Yuan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11) ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust view-invariant multiscale gait recognition", "author": ["S.D. Choudhury", "T. Tjahjadi"], "venue": "Pattern Recognition 48 (3) ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Silhouette-based gait recognition via deterministic learning", "author": ["W. Zeng", "C. Wang", "F. Yang"], "venue": "Pattern Recognition 47 (11) ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "The humanid gait challenge problem: Data sets", "author": ["S. Sarkar", "P.J. Phillips", "Z. Liu", "I.R. Vega", "P. Grother", "K.W. Bowyer"], "venue": "performance, and analysis, IEEE Transactions on Pattern Analysis and Machine Intelligence 27 (2) ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2005}, {"title": "Gait curves for human recognition", "author": ["A. DeCann", "A. Ross"], "venue": "backpack detection, and silhouette correction in a nighttime environment, in: SPIE conference on Biometric Technology for Human Identification", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2010}, {"title": "Walker recognition without gait cycle estimation", "author": ["D. Tan", "S. Yu", "K. Huang", "T. Tan"], "venue": "in: S.-W. Lee, S. Li (Eds.), Advances in Biometrics, Vol. 4642 of Lecture Notes in Computer Science", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "Orthogonal diagonal projections for gait recognition", "author": ["D. Tan", "K. Huang", "S. Yu", "T. Tan"], "venue": "in: Proceedings of the IEEE International Conference on Image Processing, Vol. 1", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}, {"title": "Gait recognition using wavelet packet silhouette representation and transductive support vector machines", "author": ["F. Dadashi", "B. Araabi", "H. Soltanian-Zadeh"], "venue": "in: Image and Signal Processing, 2009. CISP \u201909. 2nd International Congress on", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "Uniprojective features for gait recognition", "author": ["D. Tan", "K. Huang", "S. Yu", "T. Tan"], "venue": "in: Advances in Biometrics, Vol. 4642 of Lecture Notes in Computer Science", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2007}, {"title": "Recognizing night walkers based on one pseudoshape representation of gait", "author": ["D. Tan", "K. Huang", "S. Yu", "T. Tan"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic gait recognition using weighted binary pattern on video", "author": ["W. Kusakunniran", "Q. Wu", "H. Li", "J. Zhang"], "venue": "in: Advanced Video and Signal Based Surveillance. AVSS \u201909. Sixth IEEE International Conference on", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "Speed-invariant gait recognition based on procrustes shape analysis using higher-order shape configuration", "author": ["W. Kusakunniran", "Q. Wu", "J. Zhang", "H. Li"], "venue": "in: Proceedings of the IEEE International Conference on Image Processing", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2011}, {"title": "Gait recognition across various walking speeds using higher order shape configuration based on a differential composition model", "author": ["W. Kusakunniran", "Q. Wu", "J. Zhang", "H. Li"], "venue": "Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on 42 (6) ", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2012}, {"title": "A robust speed-invariant gait recognition system for walker and runner identification", "author": ["Y. Guan", "C. Li"], "venue": "in: Biometrics (ICB), 2013 International Conference on", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "An improved adaptive background mixture model for real-time tracking with shadow detection", "author": ["P. KaewTraKulPong", "R. Bowden"], "venue": "in: Video-Based Surveillance Systems, Springer", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2002}, {"title": "Dynamic distance-based shape features for gait recognition", "author": ["T. Whytock", "A. Belyaev", "N. Robertson"], "venue": "Journal of Mathematical Imaging and Vision 50 (3) ", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The local motion features (described by the Divergence-Curl-Shear descriptor [1]) extracted on the different spatial areas of the person are combined into a single high-level gait descriptor by using the Fisher Vector encoding [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "The local motion features (described by the Divergence-Curl-Shear descriptor [1]) extracted on the different spatial areas of the person are combined into a single high-level gait descriptor by using the Fisher Vector encoding [2].", "startOffset": 227, "endOffset": 230}, {"referenceID": 2, "context": "The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on \u2018CASIA\u2019 dataset [3] (parts B and C), \u2018TUM GAID\u2019 dataset [4], \u2018CMU MoBo\u2019 dataset [5] and the recent \u2018AVA Multiview Gait\u2019 dataset [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 3, "context": "The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on \u2018CASIA\u2019 dataset [3] (parts B and C), \u2018TUM GAID\u2019 dataset [4], \u2018CMU MoBo\u2019 dataset [5] and the recent \u2018AVA Multiview Gait\u2019 dataset [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 4, "context": "The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on \u2018CASIA\u2019 dataset [3] (parts B and C), \u2018TUM GAID\u2019 dataset [4], \u2018CMU MoBo\u2019 dataset [5] and the recent \u2018AVA Multiview Gait\u2019 dataset [6].", "startOffset": 166, "endOffset": 169}, {"referenceID": 5, "context": "The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on \u2018CASIA\u2019 dataset [3] (parts B and C), \u2018TUM GAID\u2019 dataset [4], \u2018CMU MoBo\u2019 dataset [5] and the recent \u2018AVA Multiview Gait\u2019 dataset [6].", "startOffset": 214, "endOffset": 217}, {"referenceID": 6, "context": "Actually, humans are good recognizing people at a distance thanks to their gait [7], what provides a good (non invasive) way to identify people without requiring their cooperation, in contrast to other biometric approaches as iris or fingerprint analysis.", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "Although great effort has been put into this problem in recent years [8], it is still far from solved.", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "Popular approaches for gait recognition require the computation of the binary silhouettes of people [9], usually, by applying some background segmentation technique.", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "These kind of descriptors have become recently popular in the field of human action recognition [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "Bag of Words [11]).", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "In this paper we introduce a new gait descriptor, coined Pyramidal Fisher Vector, that combines the potential of recent human action recognition descriptors with the rich representation provided by Fisher Vectors encoding [2].", "startOffset": 222, "endOffset": 225}, {"referenceID": 11, "context": "Many research papers have been published in recent years tackling the problem of human gait recognition using different sources of data like inertial sensors [12, 13], foot pressure [14], infrared images [15] or the traditional images.", "startOffset": 158, "endOffset": 166}, {"referenceID": 12, "context": "Many research papers have been published in recent years tackling the problem of human gait recognition using different sources of data like inertial sensors [12, 13], foot pressure [14], infrared images [15] or the traditional images.", "startOffset": 158, "endOffset": 166}, {"referenceID": 13, "context": "Many research papers have been published in recent years tackling the problem of human gait recognition using different sources of data like inertial sensors [12, 13], foot pressure [14], infrared images [15] or the traditional images.", "startOffset": 182, "endOffset": 186}, {"referenceID": 14, "context": "Many research papers have been published in recent years tackling the problem of human gait recognition using different sources of data like inertial sensors [12, 13], foot pressure [14], infrared images [15] or the traditional images.", "startOffset": 204, "endOffset": 208}, {"referenceID": 7, "context": "For example, in [8] we can find a survey on this problem summarizing some of the most popular approaches.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "In this sense, the most popular silhouette-based gait descriptor is the called Gait Enery Image (GEI) [9].", "startOffset": 102, "endOffset": 105}, {"referenceID": 15, "context": "[16], to improve the gait recognition performance, propose the computation of HOG descriptors from popular gait descriptors as the GEI and the Chrono-Gait Image (CGI).", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In [17], the authors try to find the minimum number of gait cycles needed to carry out a successful recognition by using the GEI descriptor.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Martin-Felez and Xiang [18] [19], using GEI as the basic gait descriptor, propose a new ranking model for gait recognition.", "startOffset": 23, "endOffset": 27}, {"referenceID": 18, "context": "Martin-Felez and Xiang [18] [19], using GEI as the basic gait descriptor, propose a new ranking model for gait recognition.", "startOffset": 28, "endOffset": 32}, {"referenceID": 19, "context": "In [20], Akae et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Hu proposes in [21] the use of a regularized local tensor discriminant analysis method with the Enhanced Gabor representation of the GEI.", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "In addition, the same author defines in [22] a method to identify camera viewpoints at test time from patch distribution features.", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": "[23] proposed a novel discriminant subspace learning method (Sparse Bilinear Discriminant Analysis) that extends methods based on matrix-representation discriminant analysis to sparse", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] explicitly focus on curved trajectories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] is a key reference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Then, they describe the video sequence by using the Bag of Words (BOW) model [11].", "startOffset": 77, "endOffset": 81}, {"referenceID": 24, "context": "In parallel, Perronnin and Dance [25] introduced a new way of histogram-based encoding for sets of local descriptors for image categorization: the Fisher Vector (FV) encoding.", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "[26] is similar to ours in the sense that they propose a method that uses dense local spatio-temporal features and a Fisher-based representation rearranged as tensors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "A conference version of this paper was presented in [27].", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Firstly, dense optical flow F = (ut, vt) is computed [28] on a", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "We use L = 15 as in [1].", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "[1], which is computed as follows: \uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3 div(pt) = \u2202u(pt)", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "As described in [1], the divergence is related to axial motion, expansion and scaling effects, whereas the curl is related to rotation in the image plane.", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "Then, those kinematic features are combined in pairs as in [1] to get the final motion descriptors.", "startOffset": 59, "endOffset": 62}, {"referenceID": 28, "context": "We follow a tracking-by-detection strategy as in [29]: we detect people with the detection framework of Felzenszwalb et al.", "startOffset": 49, "endOffset": 53}, {"referenceID": 29, "context": "[31] to group detections into tracks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "full body) and an upper-body detector [32].", "startOffset": 38, "endOffset": 42}, {"referenceID": 31, "context": "Inspired by the work of Kl\u00e4ser [33], after running both detectors (Fig.", "startOffset": 31, "endOffset": 35}, {"referenceID": 31, "context": "Therefore, we define a procedure similar to the Kl\u00e4ser\u2019s one [33] to combine UB and FB detections.", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "We propose here the use of Fisher Vectors (FV) encoding [2].", "startOffset": 56, "endOffset": 59}, {"referenceID": 10, "context": "The FV, that can be seen as an extension of the Bag of Words (BOW) representation [11], builds on top of a Gaussian Mixture Model (GMM), where each Gaussian corresponds to a visual word.", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "N}, we can represent V by the following gradient vector [25]:", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "Following the proposal of [2], to compare two videos V and W , a natural kernel on these gradients is the Fisher Kernel: K(V,W ) = G\u03bb(V ) TF\u22121 \u03bb G\u03bb(W ), where F\u03bb is the Fisher Information Matrix.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "As stated in [2], the capability of description of the FV can be improved by applying it a signed square-root followed by L2 normalization.", "startOffset": 13, "endOffset": 16}, {"referenceID": 32, "context": "We borrow from [34] the idea of building a pyramidal representation of the gait motion.", "startOffset": 15, "endOffset": 19}, {"referenceID": 33, "context": "Since, this is a multiclass problem, we train P binary Support Vector Machines (SVM) [35] (as many as different people) in a one-vs-all strategy.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "For computing the local motion features, we use the code published by the authors of [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "The first dataset where we perform our experiments is the \u201cAVA Multi-View Dataset for Gait Recognition\u201d (AVAMVG) [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 10, "context": "We use the popular Bag of Words approach (BOW) [11] as baseline, which is compared to our approach.", "startOffset": 47, "endOffset": 51}, {"referenceID": 47, "context": "We sample dictionary sizes in the interval [500, 4000] for BOW, and in the interval [50, 200] for PFM.", "startOffset": 84, "endOffset": 93}, {"referenceID": 14, "context": "2, each row corresponds to a different number of frames in the range [15, 50].", "startOffset": 69, "endOffset": 77}, {"referenceID": 47, "context": "2, each row corresponds to a different number of frames in the range [15, 50].", "startOffset": 69, "endOffset": 77}, {"referenceID": 4, "context": "The second dataset where we carry out our experiments is the \u201cCMU Motion of Body\u201d (MoBo) database [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "number of frames in the range [5, 40].", "startOffset": 30, "endOffset": 37}, {"referenceID": 38, "context": "number of frames in the range [5, 40].", "startOffset": 30, "endOffset": 37}, {"referenceID": 34, "context": "To put our results in context, the authors of [36] report the results of training on slow walk and testing on both fast walk and ball, obtaining 92% and 96% of accuracy, respectively.", "startOffset": 46, "endOffset": 50}, {"referenceID": 35, "context": "We can also compare with the results published in [37], where in one case they train on slow walk and test on ball, obtaining 91.", "startOffset": 50, "endOffset": 54}, {"referenceID": 36, "context": "Comparing with the results reported in [38], we outperform the cases of training on slow walk and testing on ball, and training on fast walk and testing on slow walk, obtaining 100% and 92% of accuracy, respectively.", "startOffset": 39, "endOffset": 43}, {"referenceID": 37, "context": "In [39], the authors only perform the experiments in cases of training on slow walk and testing on fast walk and training on fast walk and testing on slow walk, obtaining 100% and 92% respectively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 38, "context": "In the recent paper [40], the reported results on this dataset only use the combination of training on fast walk and testing on slow walk, achieving", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "The third dataset where we perform our experiments is \u201cCASIA Gait Dataset\u201d [41], parts B (CASIAB) [3] and C (CASIA-C) [42].", "startOffset": 98, "endOffset": 101}, {"referenceID": 39, "context": "The third dataset where we perform our experiments is \u201cCASIA Gait Dataset\u201d [41], parts B (CASIAB) [3] and C (CASIA-C) [42].", "startOffset": 118, "endOffset": 122}, {"referenceID": 40, "context": "AEI+2DLPP [43] 124 3 3-nm 2-bg-cl 98.", "startOffset": 10, "endOffset": 14}, {"referenceID": 2, "context": "GEI [3] 124 4 2 97.", "startOffset": 4, "endOffset": 7}, {"referenceID": 41, "context": "8 iHMM [44] 84 5 1 94.", "startOffset": 7, "endOffset": 11}, {"referenceID": 42, "context": "7 CGI [45] 124 1 1 88.", "startOffset": 6, "endOffset": 10}, {"referenceID": 43, "context": "3 VI-MGR [46] 124 4 2 100 89.", "startOffset": 9, "endOffset": 13}, {"referenceID": 44, "context": "SDL [47] 124 3 3-nm 2-bg-cl 98.", "startOffset": 4, "endOffset": 8}, {"referenceID": 56, "context": "We use the implementation of [59] included in Matlab.", "startOffset": 29, "endOffset": 33}, {"referenceID": 55, "context": "Note that, for example, row RSM [58] used three sequences per subject during training, instead of the two we use.", "startOffset": 32, "endOffset": 36}, {"referenceID": 41, "context": "IF+iHMM [44] 84 5 1 98.", "startOffset": 8, "endOffset": 12}, {"referenceID": 41, "context": "GEI+PCA+LDA [44] [48] 84 5 1 96.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "GEI+PCA+LDA [44] [48] 84 5 1 96.", "startOffset": 17, "endOffset": 21}, {"referenceID": 46, "context": "Method #subjects fn fs fq fb Gait Curves [49] 153 91.", "startOffset": 41, "endOffset": 45}, {"referenceID": 47, "context": "5 NDDP [50] 153 98.", "startOffset": 7, "endOffset": 11}, {"referenceID": 48, "context": "0 ODP [51] 153 98.", "startOffset": 6, "endOffset": 10}, {"referenceID": 49, "context": "0 WPSR [52] 153 93.", "startOffset": 7, "endOffset": 11}, {"referenceID": 39, "context": "0 HTI [42] 46 94.", "startOffset": 6, "endOffset": 10}, {"referenceID": 50, "context": "0 HDP [53] 153 98.", "startOffset": 6, "endOffset": 10}, {"referenceID": 40, "context": "0 AEI [43] 153 88.", "startOffset": 6, "endOffset": 10}, {"referenceID": 51, "context": "7 Pseudoshape [54] 153 98.", "startOffset": 14, "endOffset": 18}, {"referenceID": 52, "context": "7 WBP [55] 153 99.", "startOffset": 6, "endOffset": 10}, {"referenceID": 53, "context": "1 HSC [56] 50 98.", "startOffset": 6, "endOffset": 10}, {"referenceID": 54, "context": "0 DCM [57] 120 97.", "startOffset": 6, "endOffset": 10}, {"referenceID": 55, "context": "0 RSM [58] 153 100 99.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "The fourth dataset where we perform our experiments is \u201cTUM Gait from Audio, Image and Depth (GAID) database\u201d [4].", "startOffset": 110, "endOffset": 113}, {"referenceID": 55, "context": "17 have been imported from the publication [58].", "startOffset": 43, "endOffset": 47}, {"referenceID": 3, "context": "In [4], Hofmann et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "We run here the experiments proposed by the authors of the dataset at [4].", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "In our experiments, we use the training and validation sets combined as in [4] for building the dictionary.", "startOffset": 75, "endOffset": 78}, {"referenceID": 44, "context": "SDL [47] - - - 96.", "startOffset": 4, "endOffset": 8}, {"referenceID": 3, "context": "GEI [4] 99.", "startOffset": 4, "endOffset": 7}, {"referenceID": 57, "context": "SEIM [60] 99.", "startOffset": 5, "endOffset": 9}, {"referenceID": 57, "context": "GVI [60] 99.", "startOffset": 4, "endOffset": 8}, {"referenceID": 57, "context": "SVIM [60] 98.", "startOffset": 5, "endOffset": 9}, {"referenceID": 55, "context": "RSM [58] 100.", "startOffset": 4, "endOffset": 8}, {"referenceID": 1, "context": "In addition, the FV-based formulation surpasses the BOW-based one, as stated by other authors in the problem of image categorization [2].", "startOffset": 133, "endOffset": 136}, {"referenceID": 26, "context": "Actually, the results reported in this work on curved trajectories improve on the ones published in the conference version [27], thanks to the new stage that links broken tracks of persons (see Sec.", "startOffset": 123, "endOffset": 127}, {"referenceID": 0, "context": "in [1].", "startOffset": 3, "endOffset": 6}], "year": 2016, "abstractText": "The goal of this paper is to identify individuals by analyzing their gait. Instead of using binary silhouettes as input data (as done in many previous works) we propose and evaluate the use of motion descriptors based on densely sampled short-term trajectories. We take advantage of state-of-the-art people detectors to define custom spatial configurations of the descriptors around the target person, obtaining a rich representation of the gait motion. The local motion features (described by the Divergence-Curl-Shear descriptor [1]) extracted on the different spatial areas of the person are combined into a single high-level gait descriptor by using the Fisher Vector encoding [2]. The proposed approach, coined Pyramidal Fisher Motion, is experimentally validated on \u2018CASIA\u2019 dataset [3] (parts B and C), \u2018TUM GAID\u2019 dataset [4], \u2018CMU MoBo\u2019 dataset [5] and the recent \u2018AVA Multiview Gait\u2019 dataset [6]. The results show that this new approach achieves state-of-the-art results in the problem of gait recognition, allowing to recognize walking people from diverse viewpoints on single and multiple camera setups, wearing different clothes, carrying bags, walking at diverse speeds and not limited to straight walking paths.", "creator": "LaTeX with hyperref package"}}}