{"id": "1601.07213", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2016", "title": "Unifying Adversarial Training Algorithms with Flexible Deep Data Gradient Regularization", "abstract": "We present DataGrad, a general back-propagation style training procedure for deep neural architectures that uses regularization of a deep Jacobian-based penalty. It can be viewed as a deep extension of the layerwise contractive auto-encoder penalty. More importantly, it unifies previous proposals for adversarial training of deep neural nets -- this list includes directly modifying the gradient, training on a mix of original and adversarial examples, using contractive penalties, and approximately optimizing constrained adversarial objective functions. In an experiment using a Deep Sparse Rectifier Network, we find that the deep Jacobian regularization of DataGrad (which also has L1 and L2 flavors of regularization) outperforms traditional L1 and L2 regularization both on the original dataset as well as on adversarial examples.", "histories": [["v1", "Tue, 26 Jan 2016 22:41:13 GMT  (69kb,D)", "http://arxiv.org/abs/1601.07213v1", null], ["v2", "Tue, 9 Feb 2016 20:40:13 GMT  (70kb,D)", "http://arxiv.org/abs/1601.07213v2", null], ["v3", "Fri, 29 Jul 2016 15:36:19 GMT  (92kb,D)", "http://arxiv.org/abs/1601.07213v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["alexander g ororbia ii", "c lee giles", "daniel kifer"], "accepted": false, "id": "1601.07213"}, "pdf": {"name": "1601.07213.pdf", "metadata": {"source": "CRF", "title": "Unifying Adversarial Training Algorithms with Flexible Deep Data Gradient Regularization", "authors": ["Alexander G. Ororbia II", "Daniel Kifer", "C. Lee Giles"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "However, it has recently been shown that deep architectures are sensitive to certain types of input disturbances, which can range from imperceptible to quite noticeable (even semi-random) disturbances, as in Nguyen et al. (2014). Examples that contain this type of disturbance are referred to as \"contradictory examples\" (Szegedy et al., 2013) and can cause a trained network to self-consciously miscalculate its input. While there are a variety of ways to generate contradictory samples, the fastest and most effective approaches in current literature are based on the idea of using back propagation to acquire the loss in relation to an input image (i.e. the Jacobian) and adding a small multiple of the Jacobian to the image."}, {"heading": "2 The DataGrad Framework", "text": "Considering a number of loss functions L0, L1,.., Lm and regulators R1,.., Rm, we consider the following loss function: LDG (t, d, \u0432) = \u03bb0 L0 (t, d, \u0432) + \u03bb1R1 (J L1 (t, d, \u0432) + \u00b7 \u00b7 \u00b7 + \u03bbmRm (J Lm (t, d, \u0432)), where d = (d1, d2,... dk) is a data example, t is its corresponding label / target, and \u0432 = {W1, W2,..., WK} represents the parameters of a neural K-layer mesh. We use J Li to denote the Jacobian of Li (the slope of Li in relation to d). Considering its position 0, ill1, ill1,., illm are the weight coefficients of terms in the DataGrad loss function. We refer to the entire data set as (d, D)."}, {"heading": "2.1 The Derivation", "text": "The first updated term of equation # # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 # 160 [160 # 160 # 160] The first updated term of equation 1, \u2202 \u2202 w (') [L0 (t, d, \u0445)] is however provided by the standard backpropagation. For the remaining Terme, since the Jacobian of loss also depends on the current weights, we see that \u2202 w (t, d, \u0445) [Rr (J Lr)] = \u2202 Rr (..) jamp; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp; amp # 160; amp; amp; amp; amp; amp # 160; amp; amp; amp; amp; amp; amp; 160; amp; amp; amp # 160; amp; amp # 160; amp; amp; amp; amp; 160; amp; amp; amp; amp; 160; amp; amp; amp # 160; amp; amp; amp # 160; amp; amp; amp; amp; amp; 160; amp; amp; amp # 160; amp; amp; amp # 160; amp; amp # 160; amp; amp # 160; amp; amp # 160; amp; amp # 160; amp # 160; amp # 160; amp # 160 # 160; amp # 160 # 160 # 160"}, {"heading": "2.2 The High-level View: Putting it All Together", "text": "At a high level, the loss Lr and the regulator Rr together serve to define an opposing sound vector y and the opposing example d = d + \u03c6y (where \u03c6 is a small constant), as explained in the previous section. Different decisions by Lr and Rr lead to different types of counter-examples. If, for example, we use Rr as a penalty L1, the resulting counter-example is the same as the method of the fast gradient sign Goodfellow et al. (2014). If we put together the components of our algorithm of finite differences, the stochastic gradient descending equation results: w'ij \u00b2 w'ij \u00b2 0 (t, d \u00b2)."}, {"heading": "2.3 Related Work", "text": "Since the recent discovery of adversarial examples (Szegedy et al., 2013), a variety of remedies have been proposed to make neural architectures robust for this problem. A simple solution is to simply add adversarial examples during each training round of the stochastic course. (Szegedy et al., 2013) This is exactly what Equation 4 specifies, so that post-hoc solution can be justified as the regulatory implementation of the data gradients. Subsequent work (Goodfellow et al., 2014) introduced the objective function of the objective course (t, d,) + (1 \u2212 \u03b1) L (t, d), where d) the adversarial implementation of the input d-based method would be necessary to establish the derived method with respect to tow (') ij (') ij, which is, is, is."}, {"heading": "3 Experimental Results", "text": "In this setup, DataGrad only needs 2 forward gears and 2 backpasses to perform a weight update (as opposed to 1 forward and 1 reverse in reverse propagation).We implemented several deep, economical rectifier architectures (Glorot et al., 2011), among the various schemes described above, this procedure is explicitly described in algorithms. We have several deep, economical rectifier architectures (Glorot et al., 2011)."}], "references": [{"title": "Greedy Layer-wise Training of Deep Networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Bengio et al\\.,? 2007", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Exact computation of the hessian matrix for the multi-layer perceptron", "author": ["C.M. Bishop"], "venue": "Neural Computation, 4(4):494\u2013501.", "citeRegEx": "Bishop,? 1992", "shortCiteRegEx": "Bishop", "year": 1992}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proc. 14th International Conference on Artificial Intelligence and Statistics, volume 15, pages 315\u2013323.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "http://arxiv.org/abs/1412.6572.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S. Gu", "L. Rigazio"], "venue": "http://arxiv.org/abs/1412.5068.", "citeRegEx": "Gu and Rigazio,? 2014", "shortCiteRegEx": "Gu and Rigazio", "year": 2014}, {"title": "Learning with a strong adversary", "author": ["R. Huang", "B. Xu", "D. Schuurmans", "C. Szepesvari"], "venue": "arXiv:1511.03034 [cs]. http://arxiv.org/abs/1511.03034.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["T. Miyato", "Maeda", "S.-i.", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": "http://arxiv.org/abs/1507. 00677.", "citeRegEx": "Miyato et al\\.,? 2015", "shortCiteRegEx": "Miyato et al\\.", "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "http://arxiv.org/ abs/1412.1897.", "citeRegEx": "Nguyen et al\\.,? 2014", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Improving back-propagation by adding an adversarial gradient", "author": ["A. Nkland"], "venue": "arXiv:1510.04189 [cs, stat]. http://arxiv.org/abs/1510.04189.", "citeRegEx": "Nkland,? 2015", "shortCiteRegEx": "Nkland", "year": 2015}, {"title": "Online learning of deep hybrid architectures for semi-supervised categorization", "author": ["A.G. Ororbia II", "D. Reitter", "J. Wu", "C.L. Giles"], "venue": "Machine Learning and Knowledge Discovery in Databases (Proceedings, ECML PKDD 2015), volume 9284 of Lecture Notes in Computer Science, pages 516\u2013532. Springer, Porto, Portugal.", "citeRegEx": "II et al\\.,? 2015", "shortCiteRegEx": "II et al\\.", "year": 2015}, {"title": "Fast exact multiplication by the hessian", "author": ["B.A. Pearlmutter"], "venue": "Neural Computa-", "citeRegEx": "Pearlmutter,? 1994", "shortCiteRegEx": "Pearlmutter", "year": 1994}, {"title": "Intriguing properties of neural networks. http://arxiv.org", "author": ["R. gus"], "venue": null, "citeRegEx": "gus,? \\Q2013\\E", "shortCiteRegEx": "gus", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "However, recently, it has been shown that deep architectures are sensitive to certain kinds of pertubations of the input, which can range from being barely perceptible to quite noticeable (even semi-random noise), as in Nguyen et al. (2014). Samples containing this type of noise are called \u201cadversarial examples\u201d (Szegedy et al.", "startOffset": 220, "endOffset": 241}, {"referenceID": 3, "context": "Earlier work suggested adding a regularization penalty on the deep Jacobian (Goodfellow et al., 2014; Gu and Rigazio, 2014), but had difficulty in computing the derivative (with respect to the weights) of the Jacobian, which is necessary for gradient-descent based algorithms.", "startOffset": 76, "endOffset": 123}, {"referenceID": 4, "context": "Earlier work suggested adding a regularization penalty on the deep Jacobian (Goodfellow et al., 2014; Gu and Rigazio, 2014), but had difficulty in computing the derivative (with respect to the weights) of the Jacobian, which is necessary for gradient-descent based algorithms.", "startOffset": 76, "endOffset": 123}, {"referenceID": 4, "context": "Instead, they utilized approximations such as a shallow layerwise Jacobian penalty (Gu and Rigazio, 2014) that is also used for regularizing contractive auto-encoders (Gu and Rigazio, 2014).", "startOffset": 83, "endOffset": 105}, {"referenceID": 4, "context": "Instead, they utilized approximations such as a shallow layerwise Jacobian penalty (Gu and Rigazio, 2014) that is also used for regularizing contractive auto-encoders (Gu and Rigazio, 2014).", "startOffset": 167, "endOffset": 189}, {"referenceID": 6, "context": "In particular, it helps explain some of the newer approaches to adversarial training (Miyato et al., 2015; Huang et al., 2015).", "startOffset": 85, "endOffset": 126}, {"referenceID": 5, "context": "In particular, it helps explain some of the newer approaches to adversarial training (Miyato et al., 2015; Huang et al., 2015).", "startOffset": 85, "endOffset": 126}, {"referenceID": 1, "context": "Since exact computation of the Hessian is slow (Bishop, 1992), we would expect that the computation of this matrix of partial derivatives would also be slow.", "startOffset": 47, "endOffset": 61}, {"referenceID": 10, "context": "However, it turns out that we do not need to compute the full matrix \u2013 we only need this matrix times a vector, and hence we can use ideas reminiscent of fast Hessian multiplication algorithms (Pearlmutter, 1994).", "startOffset": 193, "endOffset": 212}, {"referenceID": 3, "context": "For example, setting Rr to be the L1 penalty, the resulting adversarial example is the same as the fast gradient sign method of Goodfellow et al. (2014). Putting together the components of our finite differences algorithm, the stochastic gradient descent update equation becomes:", "startOffset": 128, "endOffset": 153}, {"referenceID": 3, "context": "Subsequent work (Goodfellow et al., 2014) introduced the objective function \u2211 d \u03b1L(t,d,\u0398) + (1 \u2212 \u03b1)L(t, d\u0302,\u0398), where d\u0302 is the adversarial version of input d.", "startOffset": 16, "endOffset": 41}, {"referenceID": 3, "context": "Subsequent work (Goodfellow et al., 2014) introduced the objective function \u2211 d \u03b1L(t,d,\u0398) + (1 \u2212 \u03b1)L(t, d\u0302,\u0398), where d\u0302 is the adversarial version of input d. A gradient-based method would need to compute the derivative with respect tow ij , which is \u03b1 L(t,d,\u0398) \u2202w (`) ij + (1\u2212\u03b1) L(t,d\u0302,\u0398) \u2202w (`) ij + (1\u2212\u03b1) L(t,d\u0302,\u0398) \u2202d\u0302 \u00b7 d d\u0302 dw (`) ij , since the construction of d\u0302 depends on w ij . Their work approximates the optimization by ignoring the third term, as it is difficult to compute. This approximation then results in an updated equation having the form of Equation 4, and hence actually optimizes the DataGrad objective. Nkland (2015) present a variant where the deep network is trained using back-propagation only on adversarial examples (rather than a mix of adversarial and original examples).", "startOffset": 17, "endOffset": 641}, {"referenceID": 5, "context": "Both Huang et al. (2015) and Miyato et al.", "startOffset": 5, "endOffset": 25}, {"referenceID": 5, "context": "Both Huang et al. (2015) and Miyato et al. (2015) propose to optimize constrained objective functions that can be put in the form min\u0398 \u2211 d maxg(r)\u2264c f(t,d, r,\u0398), where r represents adversarial noise and the constraint g(r) \u2264 c puts a bound on the size of the noise.", "startOffset": 5, "endOffset": 50}, {"referenceID": 4, "context": "Since the derivative of the constrained optimum is difficult to compute, Huang et al. (2015) and Miyato et al.", "startOffset": 73, "endOffset": 93}, {"referenceID": 4, "context": "Since the derivative of the constrained optimum is difficult to compute, Huang et al. (2015) and Miyato et al. (2015) opt to approximate/simplify the derivative making the second term disappear (as it would in the unconstrained case).", "startOffset": 73, "endOffset": 118}, {"referenceID": 4, "context": "Finally, Gu and Rigazio (2014) penalizes the Frobenius norm of the deep Jacobian.", "startOffset": 9, "endOffset": 31}, {"referenceID": 4, "context": "Finally, Gu and Rigazio (2014) penalizes the Frobenius norm of the deep Jacobian. However, they do this with a shallow layer-wise approximation. Specifically, they note that shallow contractive auto-encoders optimize the same objective for shallow (1-layer) networks and that the gradient of the Jacobian can be computed analytically in those cases Gu and Rigazio (2014). Thus, Gu and Rigazio (2014) applies this penalty layer by layer (hence it is a penalty on the derivative of each layer with respect to its immediate inputs) and uses this penalty as an approximation to regularizing the deep Jacobian.", "startOffset": 9, "endOffset": 371}, {"referenceID": 4, "context": "Finally, Gu and Rigazio (2014) penalizes the Frobenius norm of the deep Jacobian. However, they do this with a shallow layer-wise approximation. Specifically, they note that shallow contractive auto-encoders optimize the same objective for shallow (1-layer) networks and that the gradient of the Jacobian can be computed analytically in those cases Gu and Rigazio (2014). Thus, Gu and Rigazio (2014) applies this penalty layer by layer (hence it is a penalty on the derivative of each layer with respect to its immediate inputs) and uses this penalty as an approximation to regularizing the deep Jacobian.", "startOffset": 9, "endOffset": 400}, {"referenceID": 4, "context": "Finally, Gu and Rigazio (2014) penalizes the Frobenius norm of the deep Jacobian. However, they do this with a shallow layer-wise approximation. Specifically, they note that shallow contractive auto-encoders optimize the same objective for shallow (1-layer) networks and that the gradient of the Jacobian can be computed analytically in those cases Gu and Rigazio (2014). Thus, Gu and Rigazio (2014) applies this penalty layer by layer (hence it is a penalty on the derivative of each layer with respect to its immediate inputs) and uses this penalty as an approximation to regularizing the deep Jacobian. Since DataGrad does regularize the deep Jacobian, the work of Gu and Rigazio (2014) can also be viewed as an approximation to DataGrad.", "startOffset": 9, "endOffset": 690}, {"referenceID": 2, "context": "We implemented several deep sparse rectifier architectures (Glorot et al., 2011) (3 layers), under the various schemes described above, on the permutation-invariant 1This procedure is explicitly described in Algorithm 1 found in Appendix .", "startOffset": 59, "endOffset": 80}, {"referenceID": 0, "context": "Since DataGrad is effectively a \u201cdeep\u201d data-driven penalty, it may be used in tandem with most training objective functions (whether supervised, unsupervised Bengio et al. (2007), or hybrid Ororbia II et al.", "startOffset": 158, "endOffset": 179}, {"referenceID": 0, "context": "Since DataGrad is effectively a \u201cdeep\u201d data-driven penalty, it may be used in tandem with most training objective functions (whether supervised, unsupervised Bengio et al. (2007), or hybrid Ororbia II et al. (2015)).", "startOffset": 158, "endOffset": 215}], "year": 2017, "abstractText": "We present DataGrad, a general back-propagation style training procedure for deep neural architectures that uses regularization of a deep Jacobian-based penalty. It can be viewed as a deep extension of the layerwise contractive auto-encoder penalty. More importantly, it unifies previous proposals for adversarial training of deep neural nets \u2013 this list includes directly modifying the gradient, training on a mix of original and adversarial examples, using contractive penalties, and approximately optimizing constrained adversarial objective functions. In an experiment using a Deep Sparse Rectifier Network, we find that the deep Jacobian regularization of DataGrad (which also has L1 and L2 flavors of regularization) outperforms traditional L1 and L2 regularization both on the original dataset as well as on adversarial examples.", "creator": "LaTeX with hyperref package"}}}