{"id": "1601.07215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2016", "title": "Recurrent Neural Network Postfilters for Statistical Parametric Speech Synthesis", "abstract": "In the last two years, there have been numerous papers that have looked into using Deep Neural Networks to replace the acoustic model in traditional statistical parametric speech synthesis. However, far less attention has been paid to approaches like DNN-based postfiltering where DNNs work in conjunction with traditional acoustic models. In this paper, we investigate the use of Recurrent Neural Networks as a potential postfilter for synthesis. We explore the possibility of replacing existing postfilters, as well as highlight the ease with which arbitrary new features can be added as input to the postfilter. We also tried a novel approach of jointly training the Classification And Regression Tree and the postfilter, rather than the traditional approach of training them independently.", "histories": [["v1", "Tue, 26 Jan 2016 22:53:45 GMT  (70kb,D)", "http://arxiv.org/abs/1601.07215v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["prasanna kumar muthukumar", "alan w black"], "accepted": false, "id": "1601.07215"}, "pdf": {"name": "1601.07215.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORK POSTFILTERS FOR STATISTICAL PARAMETRIC SPEECH SYNTHESIS", "authors": ["Prasanna Kumar Muthukumar", "Alan W Black"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Recurrent Neural Network, Postfilter, Statistical Parametric Speech Synthesis"}, {"heading": "1. INTRODUCTION", "text": "Deep Neural Networks have had a huge impact on automatic speech recognition in recent years, so there has been a flurry of papers over the last two years on the use of deep neural networks for speech synthesis [3, 4, 5]. Nevertheless, it would be difficult to argue that deep neural networks have had the same success in synthesis as in ASR. DNN-influenced improvements in synthesis have been mostly relatively moderate, which becomes pretty obvious when looking at the Blizzard Challenge submissions [6] over the last three years. Few of the systems submitted use deep neural networks in any part of the pipeline, and those using DNNs seem to have no advantage over conventionally well-trained systems. Even in cases where the improvements look promising, the techniques had to rely on the use of much larger datasets than are normally required for synthesis."}, {"heading": "2. RELATION TO PRIOR WORK", "text": "The idea of using a postfilter to fix errors in the output of CARTs is not very new. Techniques such as Maximum Likelihood Parameter Generation (MLPG) [8] and Global Variance [9] have become standard, and even newer ideas such as the use of modulation spectra [10] have begun to move into the mainstream. These techniques offer a significant improvement in quality, but suffer from the disadvantage that the post filter for each feature that is used must be derived analytically. MLPG, for example, deals exclusively with the means, deltas, and delta deltas for each state."}, {"heading": "3. RECURRENT NEURAL NETWORKS", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "4. ADDING LEXICAL FEATURES", "text": "In all previous experiments, the RNN was only used with the same input functions that MLPG usually uses, but this does not use the full power of the RNN. Any function can be fed as input to the RNN to learn from it. This is the advantage that an RNN uses traditional post-filtering methods such as MLPG or Modulation Spectrum.We added all of Festival's lexical features as additional input features for the RNN, in addition to CART's predictions of f0, MCEP statics and deltas, and intonation. The standard deviations as well as the means of CART predictions were used, which resulted in 776 input features for the English language voices and 1076 features for the Hindi ones (the Hindi phoneset for synthesis is slightly larger).The output features were the same 25-dimensional MCEPs from the previous series of experiments."}, {"heading": "5. JOINT TRAINING OF THE CART AND THE POSTFILTER", "text": "The traditional way this technique works, we have to mathematically represent that we have to solve the problems by building the classification and regression trees that predict the parameters of the voice signal and then apply the postfilters to the output of the CART. The disadvantage is that the CART is unnecessarily agnostic to the existence of the postfilter. In an ideal system, the CART and the postfilter systems should not necessarily work well together, as each has its own set of idiosyncracies in handling data. MCEPs might not be the best representation that connects the two. In an ideal system, the CART and the postfilter systems should jointly agree on a representation of the data. That representation should be easy for the CART to learn as well as reasonably for the postfilter. One way to achieve this goal is to use a fairly new technique called Method of Auxiliary (MAC). To understand this technique, we have to mathematically represent."}, {"heading": "6. DISCUSSION", "text": "The massive improvements we are getting in the absence of other post-filters such as MLPG indicate that RNNs are definitely a viable option in the future, especially due to the ease with which random new features can be added. However, the combination of MLPG and RNNNs is a little less convincing, which could mean that RNNs have learned to do much the same thing as MLPG. Or it could mean that MLPG is not really suitable for use on RNN outputs. We believe that the answer could actually be a combination of both. Ultimately, the right solution could be to find an algorithm similar to MAC that links different post-filters together for common training. Future research in this direction could lead to insightful new results."}, {"heading": "7. ACKNOWLEDGEMENTS", "text": "Almost all of the experiments reported in this article were conducted on a Tesla K40 provided by an Nvidia Hardware Grant, or on g2.8xlarge EC2 instances funded by an Amazon AWS Research Grant. Without these, it would have been impossible to conduct all of the experiments described above in a reasonable amount of time."}, {"heading": "8. REFERENCES", "text": "[1] Heiga Zen Ling, Li Deng, and Alan W Black, \"Statistical parametric speech synthesis,\" Speech Communication, vol. 51, no. 11, pp. 1039-1064, 2009. [2] John Dines, Junichi Yamagishi, and Simon King, \"Measuring the gap between HMM-based ASR and TTS,\" Selected Topics in Signal Processing, IEEE Journal of, vol. 4, no. 6, pp. 1046-1058, 2010. [3] Heiga Zen, Andrew Senior, and Mike Schuster, \"Statistical parametric speech synthesis using deep neural networks,\" in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on."}], "references": [{"title": "Statistical parametric speech synthesis", "author": ["Heiga Zen", "Keiichi Tokuda", "Alan W Black"], "venue": "Speech Communication, vol. 51, no. 11, pp. 1039\u20131064, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Measuring the gap between HMM-based ASR and TTS", "author": ["John Dines", "Junichi Yamagishi", "Simon King"], "venue": "Selected Topics in Signal Processing, IEEE Journal of, vol. 4, no. 6, pp. 1046\u20131058, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistical parametric speech synthesis using deep neural networks", "author": ["Heiga Zen", "Andrew Senior", "Mike Schuster"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 7962\u20137966.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling spectral envelopes using restricted boltzmann machines and deep belief networks for statistical parametric speech synthesis", "author": ["Zhen-Hua Ling", "Li Deng", "Dong Yu"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 10, pp. 2129\u20132139, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "TTS synthesis with bidirectional LSTM based recurrent neural networks", "author": ["Yuchen Fan", "Yao Qian", "Fenglong Xie", "Frank K Soong"], "venue": "Proc. Interspeech, 2014, pp. 1964\u20131968.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Unit selection in a concatenative speech synthesis system using a large speech database", "author": ["Andrew J Hunt", "Alan W Black"], "venue": "Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on. IEEE, 1996, vol. 1, pp. 373\u2013376.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Speech parameter generation algorithms for HMMbased speech synthesis", "author": ["Keiichi Tokuda", "Takayoshi Yoshimura", "Takashi Masuko", "Takao Kobayashi", "Tadashi Kitamura"], "venue": "Acoustics, Speech, and Signal Processing, 2000. ICASSP\u201900. Proceedings. 2000 IEEE International Conference on. IEEE, 2000, vol. 3, pp. 1315\u20131318.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "A speech parameter generation algorithm considering global variance for HMM-based speech synthesis", "author": ["Tomoki Toda", "Keiichi Tokuda"], "venue": "IEICE TRANSAC- TIONS on Information and Systems, vol. 90, no. 5, pp. 816\u2013824, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "A postfilter to modify the modulation spectrum in HMM-based speech synthesis", "author": ["Shinnosuke Takamichi", "Tomoki Toda", "Graham Neubig", "Sakriani Sakti", "Shigenari Nakamura"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 290\u2013294.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "DNN-based stochastic postfilter for HMM-based speech synthesis", "author": ["Ling-Hui Chen", "Tuomo Raitio", "Cassia Valentini- Botinhao", "Junichi Yamagishi", "Zhen-Hua Ling"], "venue": "Proc. Interspeech, Singapore, Singapore, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A deep generative architecture for postfiltering in statistical parametric speech synthesis", "author": ["Ling-Hui Chen", "Tuomo Raitio", "Cassia Valentini- Botinhao", "Zhen-Hua Ling", "Junichi Yamagishi"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 23, no. 11, pp. 2003\u20132014, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Training recurrent neural networks, Ph.D", "author": ["Ilya Sutskever"], "venue": "thesis, University of Toronto,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "CLUSTERGEN: a statistical parametric synthesizer using trajectory modeling", "author": ["Alan W Black"], "venue": "INTER- SPEECH, 2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Mel-cepstral distance measure for objective speech quality assessment", "author": ["Robert F Kubichek"], "venue": "Communications, Computers and Signal Processing, 1993., IEEE Pacific Rim Conference on. IEEE, 1993, vol. 1, pp. 125\u2013 128.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "The CMU Arctic speech databases", "author": ["John Kominek", "Alan W Black"], "venue": "Fifth ISCA Workshop on Speech Synthesis, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "BigLearn, NIPS Workshop, 2011, number EPFL-CONF-192376.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1990}, {"title": "Synthesizer voice quality of new languages calibrated with mean mel cepstral distortion", "author": ["John Kominek", "Tanja Schultz", "Alan W Black"], "venue": "Spoken Languages Technologies for Under-Resourced Languages, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributed optimization of deeply nested systems", "author": ["Miguel A Carreira-Perpin\u00e1n", "Weiran Wang"], "venue": "arXiv preprint arXiv:1212.5921, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Statistical Parametric Speech Synthesis[1] has a tradition of borrowing ideas from the speech recognition community[2], and so there has been a flurry of papers in the last two years on using deep neural networks for speech synthesis[3, 4, 5].", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "Statistical Parametric Speech Synthesis[1] has a tradition of borrowing ideas from the speech recognition community[2], and so there has been a flurry of papers in the last two years on using deep neural networks for speech synthesis[3, 4, 5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "Statistical Parametric Speech Synthesis[1] has a tradition of borrowing ideas from the speech recognition community[2], and so there has been a flurry of papers in the last two years on using deep neural networks for speech synthesis[3, 4, 5].", "startOffset": 233, "endOffset": 242}, {"referenceID": 3, "context": "Statistical Parametric Speech Synthesis[1] has a tradition of borrowing ideas from the speech recognition community[2], and so there has been a flurry of papers in the last two years on using deep neural networks for speech synthesis[3, 4, 5].", "startOffset": 233, "endOffset": 242}, {"referenceID": 4, "context": "Statistical Parametric Speech Synthesis[1] has a tradition of borrowing ideas from the speech recognition community[2], and so there has been a flurry of papers in the last two years on using deep neural networks for speech synthesis[3, 4, 5].", "startOffset": 233, "endOffset": 242}, {"referenceID": 5, "context": "The end result of this is that Statistical Parametric Synthesis ends up having to lose the advantage it has over traditional unit-selection systems[7] in terms of the amount of data needed to build a reasonable system.", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "Techniques such as Maximum Likelihood Parameter Generation (MLPG)[8] and Global variance[9] have become standard, and even the newer ideas like the use of Modulation Spectrum[10] have started moving into the mainstream.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "Techniques such as Maximum Likelihood Parameter Generation (MLPG)[8] and Global variance[9] have become standard, and even the newer ideas like the use of Modulation Spectrum[10] have started moving into the mainstream.", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "Techniques such as Maximum Likelihood Parameter Generation (MLPG)[8] and Global variance[9] have become standard, and even the newer ideas like the use of Modulation Spectrum[10] have started moving into the mainstream.", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "There has been prior work in using DNN based postfilters for parametric synthesis in [11] and [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "There has been prior work in using DNN based postfilters for parametric synthesis in [11] and [12].", "startOffset": 94, "endOffset": 98}, {"referenceID": 4, "context": "RNNs have been used before for synthesis in [5], but as a replacement for the existing acoustic model and not as a ar X iv :1 60 1.", "startOffset": 44, "endOffset": 47}, {"referenceID": 11, "context": "A more theoretically sound approach to handle the interdependencies between consecutive frames is to use a Recurrent Neural Network[13].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "5 of [14].", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "The particular synthesizer we use is the Clustergen system[15].", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "Mel Cepstral Distortion[16] is used as the objective metric.", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "RMS and SLT are voices from the CMU Arctic speech databases[17], about an hour of speech each.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "The RNN was implemented using the Torch7 toolkit[18].", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "To train the RNN, we used the ADAGRAD[19] algorithm along with a two-step BackPropagation Through Time[20].", "startOffset": 37, "endOffset": 41}, {"referenceID": 18, "context": "To train the RNN, we used the ADAGRAD[19] algorithm along with a two-step BackPropagation Through Time[20].", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "[21] reports that an Table 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "One way of achieving this goal is to use a fairly new technique called Method of Auxiliary Coordinates (MAC)[22].", "startOffset": 108, "endOffset": 112}], "year": 2016, "abstractText": "In the last two years, there have been numerous papers that have looked into using Deep Neural Networks to replace the acoustic model in traditional statistical parametric speech synthesis. However, far less attention has been paid to approaches like DNN-based postfiltering where DNNs work in conjunction with traditional acoustic models. In this paper, we investigate the use of Recurrent Neural Networks as a potential postfilter for synthesis. We explore the possibility of replacing existing postfilters, as well as highlight the ease with which arbitrary new features can be added as input to the postfilter. We also tried a novel approach of jointly training the Classification And Regression Tree and the postfilter, rather than the traditional approach of training them independently.", "creator": "LaTeX with hyperref package"}}}