{"id": "1601.07996", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2016", "title": "Feature Selection: A Data Perspective", "abstract": "Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing high-dimensional data for data mining and machine learning problems. The objectives of feature selection include: building simpler and more comprehensible models, improving data mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities of feature selection algorithms. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the big data age, we revisit feature selection research from a data perspective, and review representative feature selection algorithms for generic data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for generic data, we generally categorize them into four groups: similarity based, information theoretical based, sparse learning based and statistical based methods. Finally, to facilitate and promote the research in this community, we also present a open-source feature selection repository that consists of most of the popular feature selection algorithms (", "histories": [["v1", "Fri, 29 Jan 2016 08:32:10 GMT  (1780kb)", "http://arxiv.org/abs/1601.07996v1", null], ["v2", "Fri, 26 Feb 2016 00:29:42 GMT  (1780kb)", "http://arxiv.org/abs/1601.07996v2", null], ["v3", "Fri, 4 Mar 2016 22:24:56 GMT  (1780kb)", "http://arxiv.org/abs/1601.07996v3", null], ["v4", "Mon, 26 Sep 2016 10:32:12 GMT  (1780kb)", "http://arxiv.org/abs/1601.07996v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jundong li", "kewei cheng", "suhang wang", "fred morstatter", "robert p trevino", "jiliang tang", "huan liu"], "accepted": false, "id": "1601.07996"}, "pdf": {"name": "1601.07996.pdf", "metadata": {"source": "CRF", "title": "Feature Selection: A Data Perspective", "authors": ["Jundong Li", "Kewei Cheng", "Suhang Wang", "Robert P. Trevino", "Jiliang Tang", "Huan Liu"], "emails": ["jundongl@asu.edu", "kcheng18@asu.edu", "swang187@asu.edu", "fmorstat@asu.edu", "rptrevin@asu.edu", "jlt@yahoo-inc.com", "huanliu@asu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 1.07 996v 1Keywords: Feature Selection"}, {"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "1.1 Traditional Categorizations of Feature Selection Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1.1 Label Perspective", "text": "Once label information is available, selection algorithms can generally be classified as supervised, unattended, and semi-supervised. Supervised Feature Selection Supervised Feature Selection is usually designed for classification or regression problems. It aims to select a subset of characteristics that are able to differentiate samples from different classes. With the existence of class labels, the relevance of characteristics is usually assessed through their correlation with class labels. A general framework of supervised feature selection is illustrated in Figure (3). The training phase of classification greatly depends on the selection of characteristics. After dividing the data into training and test groups, classifiers are selected based on a subset of characteristics selected through supervised feature selection."}, {"heading": "1.1.2 Search Strategy Perspective", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country, in which it is a city and in which it is a country, in which it is a country and in which it is a country."}, {"heading": "1.2 Feature Selection Algorithms from A Data Perspective", "text": "This year it is more than ever before."}, {"heading": "1.2.1 Organization of the Survey", "text": "We present this survey in five parts, and the topics covered are listed as follows: 1. Feature Selection with Generic Data (Section 2) (a) Similarity based Feature Selection Methods (b) Information Theoretical based Feature Selection Methods (c) Sparse Learning based Feature Selection Methods (d) Statistical based Feature Selection Methods2. Feature Selection with Structure Features (Section 3) (a) Feature Selection Algorithms with Group Structure Features1. http: / / scikit-learn.org / stable / 2. http: / / www.numpy.org / 3. http: / / www.scipy.org / (b) Feature Selection Algorithms with Tree Structure Features1. http: / / scikit-learn.org / stable / 2. http: / / www.numpy.org / 3. http: / / / www.scipy.org / (b) Feature Selection Algorithms with Linked Data (c) Feature Multi Selection Algorithms with Data Selection with Graph Selection (Feature Selection) (3)."}, {"heading": "2. Feature Selection on Generic Data", "text": "Over the past two decades, hundreds of character selection algorithms have been proposed. In this section, we group traditional character selection algorithms into four categories: similarity algorithms, information theory, little learning-based, and statistical methods according to the techniques they use during the character selection process. In the following subsections, we will briefly check each category with some representative algorithms. We will follow the matrix settings in Table 1. We will use bold uppercase letters for matrices (e.g. A), bold lowercase letters for vectors (e.g. a), calligraphic fonts for sets (e.g. F). We will follow the matrix settings in Matlab to follow the \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 A \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 series of matrix A as A (i,:), j-th column of A as A (:), j-th column of A as A (: j), (i, j) -te input of A as A (i, j) input of A \u00b7 \u00b7 \u00b7 \u00b7 x A as A (i,:), j-th row of matrix A as A (i,:), j-th column of A as A (:), j-th column of A as A (:), j-th column of A as A (:), j-th column of A as A (:), j-th column of A as A (:), j-th column of A as A (i, j), j-th column of A = A = 1 = 1, v = 1, v = 1 = 1, v = 1, v = 1, and A = 1 (v = 1 v = 1), and A = (v = 1 v = 1), and A = (v = (v = 1), and A = 1 of A = 1 = (v = 1). v = (v = 1)"}, {"heading": "2.1 Similarity based Methods", "text": "Different feature selection algorithms use different types of criteria to define the relevance of characteristics such as distance, separability, information, correlation, dependence, and reconstruction errors. Among them, there is a family of methods that evaluate the importance of characteristics by their ability to preserve data similarity. We refer to these methods as similarity-based feature selection methods. In supervised feature selection, data similarity can be derived from identification information; while in unattended feature selection methods, most methods use different distance metrics to obtain data similarity. In the case of a data set X-Rn-d with n instances and d-characteristics, the paired similarity between substances in an affinity matrix S-Rn-n can be selected. The affinity matrix S is symmetrical and its (i, j) -th entries show the similarity between the i instance xi j and the utility of the function S-Rn-n-n-n is the problem of the larger Si, the most important of which is the Sj-Si."}, {"heading": "2.1.1 Laplacian Score (He et al., 2005) (Unsupervised)", "text": "Laplacian Score is an uncontrolled selection algorithm that selects properties that best preserve the diverse structure of the data. It consists of three phases: First, it constructs a neighboring diagram G with n nodes where the i-th node corresponds to xi. Second, when the nodes i and j are connected, the entry in the affinity matrix Sij S (i, j) = e \u2212 | xi \u2212 xj | 2t, where t is a constant, otherwise S (i, j) = D (i, i) = f = 1 S (i, j) and the laplacian matrix L is L = D \u2212 S (Chung, 1997)."}, {"heading": "2.1.2 SPEC (Zhao and Liu, 2007) (Unsupervised and Supervised)", "text": "SPEC is an extension of the Laplacian Score, which is used for both monitored and unmonitored scenarios. (Example: In the unmonitored scenario, the data similarity is measured by the RBF core function: S (i, j) = E property (6), where nl is the number of data samples in class l. (Next, the construction of the affinity matrix S \u2212 x is used, the diagonal matrix D is defined as D (i, j) = 1 S (i, j) and the normalized laplactic matrix Lnorm (D \u2212 f)."}, {"heading": "2.1.3 Fisher Score (Duda et al., 2012) (Supervised)", "text": "Suppose the class names of n samples y = {y1, y2,..., yn} come from c classes, Fisher Score selects the characteristics so that the characteristic values of the samples within the same class are small, while the characteristic values of the samples from different classes are large. Fisher Score of each characteristic is evaluated as follows: fisher score (fi) = \u2211 c \u2212 j = 1 nj (\u00b5i, j \u2212 \u00b5i) 2 x x x x, where nj, \u00b5i, j and \u03c3 2 i, j is the number of samples in class j, mean value of the sample fi, variance value of the sample fi for samples in class j, variance value of the sample in class j, (9) where nj, \u00b5i, j and \u03c3 2 i, j is the number of samples in class j."}, {"heading": "2.1.4 Trace Ratio Criterion (Nie et al., 2008) (Supervised)", "text": "Recently, the trace ratio criterion for all features in F was proposed as follows: trace ratio: F ratio: F ratio: F ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio:: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio:: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio:: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio:: W ratio: W ratio: W ratio: W ratio: W ratio: W ratio"}, {"heading": "2.1.5 ReliefF (Robnik-S\u030cikonja and Kononenko, 2003) (Supervised)", "text": "Relief and its multi-class variant ReliefF are monitored filter algorithms that select q = q values to separate instances from different classes. Suppose that l-data instances are randomly selected from all n-instances, then the feature score of fi in Relief is defined as follows: Relief score (fi) = 12l-Score j = 1d (X (j, i) -X (NM (j) -Score of fi (j), i)))), (24) where NM (j) and NH (j) indicate the next data instances to xj (j, i) -Score of xj (n) -Score of H (X, i) -Score of H (j) and NH (j) -Score of H (j) indicates the next data score of xj (j)."}, {"heading": "2.2 Information Theoretical based Methods", "text": "The large family of existing selection algorithms is reduced to a specific case of the unified frame (we mainly present different heuristic filter criteria for measuring the significance of features).Since the relevance of a feature is usually measured by its correlation with class indicators, most algorithms in this family can be performed in a superordinate manner. Furthermore, most information theory concepts can only be applied to discrete variables. Therefore, feature selection algorithms in this family can only work with discrete data. For numerical characteristic values, some data discretization techniques (Dougherty et al, 1995; Kotsiantis and Kanellopoulos, 2006), two decades of research on information theoretical criteria can be unified within a conditional framework, and most algorithms can be reduced to a specific case of the unified frame."}, {"heading": "2.2.1 Mutual Information Maximization (or Information Gain) (Lewis, 1992)", "text": "Mutual information maximization (MIM) (also known as information gain) measures the importance of a feature based on its correlation with the class name. It assumes that if a feature has a strong correlation with the class name, it can help achieve good classification performance. Mutual information score for a new, unselected feature Xk is: JMIM (Xk) = I (Xk; Y). (37) It can be observed that in MIM, the score of the characteristics is individually evaluated, which is independent of other characteristics. Therefore, in MIM, only the feature correlation is taken into account, while the feature correlation is completely ignored. After obtaining the MIM corpus for all non-selected characteristics, we select the feature with the highest corpus of characteristics and add it to the selected feature set. The process repeats until the desired number of selected characteristics is reached. XY \u00b7 II \u00b7 It can also be observed that MIM is a combination Xeark = Xannon."}, {"heading": "2.2.2 Mutual Information Feature Selection (Battiti, 1994)", "text": "One limitation of the MIM feature criterion is that it assumes that features are independent from each other. However, in reality, good features should not only be strongly correlated with class names, but should also not be highly correlated with each other. In other words, the correlation between features should be minimized; the Mutual Information Feature Selection (MIFS) criterion takes into account both feature relevance and feature redundancy during the feature selection phase. (39) In MIFS, the feature relevance of the new feature is evaluated by the first term I (Xk; Y), while the second term attempts to punish the feature that has high reciprocal information with the currently selected features. (Xk; Xj), the feature relevance of the new feature is minimized by the first term I (Xk; Y)."}, {"heading": "2.2.3 Minimum Redundancy Maximum Relevance (Peng et al., 2005)", "text": "Unlike MIFS, which sets \u03b2 empirically to one (Peng et al., 2005), a criterion of minimum redundancy of maximum relevance (MRMR) was proposed to set the value of \u03b2 to the opposite of the number of selected characteristics: JMRMR (Xk) = I (Xk; Y) \u2212 1 | S | \u2211 Xj-SI (Xk; Xj). (41) With more selected characteristics, the effect of feature redundancy is gradually reduced. Intuition is that it becomes more difficult for new characteristics to be redundant to the characteristics that were already present in S. In (Brown et al., 2012), there is another interpretation that the paired independence between characteristics becomes stronger the more characteristics are added to S, possibly due to noise information in the data. The MRMR criterion is strongly linked to the conditional probability maximization framework: JCj-S (XK); Xj-S-S (XK); and Xj-Y-I (XK)."}, {"heading": "2.2.4 Conditional Infomax Feature Extraction (Lin and Tang, 2006)", "text": "MIFS and MRMR consider both the relevance of features and the redundancy of features at the same time. However, some studies by Lin and Tang (2006); El Akadi et al. (2008); Guo and Nixon (2009) show that, as opposed to minimizing redundancy of features, conditional redundancy between unselected features and already selected features should be maximized with class names. In other words, as long as feature redundancy with class names is stronger than redundancy within a feature, feature selection will have a negative effect. A typical feature selection under this argument is the conditional Infomax Feature Extraction (CIFE) criterion, where the feature evaluation for a new, unselected feature Xk: JCIFE (Xk) = I (Xk; Y) \u2212 and feature selection (Xj, SI) \u2212 Xj (Xj, Xj, Xj, Xj, Xj)."}, {"heading": "2.2.5 Joint Mutual Information (Yang and Moody, 1999)", "text": "An alternative criterion, Joint Mutual Information (Yang and Moody, 1999; Meyer et al., 2008), has been proposed to increase supplementary information to be shared between new, unselected features and selected features, taking into account class names. The feature criterion is listed as follows: JJMI (Xk) = DL (Xk, Xj; Y). (45) The basic idea of JMI is that we should include new features that complement existing features, taking into account class names. In contrast to previously mentioned approaches that can be directly represented by the linear combination of Shannon information terms, JMI cannot be directly reduced to the conditionality maximization framework. In (Brown et al., 2012), the authors show that the JMI criterion can be easily manipulated as: JJMI (Xk) = I (Xk); Y \u2212 1 | S (SJ) and SXY (S1) (SXY)."}, {"heading": "2.2.6 Conditional Mutual Information Maximization (Fleuret, 2004)", "text": "Next, we will show some other algorithms that can only be reduced to a non-linear combination of Shannon information dates. Among them, conditional mutual information maximization (CMIM) (Vidal-Naquet and Ullman, 2003; Fleuret, 2004) is a criterion that iteratively selects characteristics that maximize mutual information with the class names previously given to the selected characteristics. In other words, CMIM does not select the characteristic that is similar to previous ones, although its predictable performance for the class names is strong. Mathematically, the characteristic criterion for each new, unselected characteristic Xk can be formulated as follows: JCMIM (Xk) = min Xj-S [I (Xk; Y | Xj-M). (47) Note that the value of Xk (Xk; Y | Xj) mik."}, {"heading": "2.2.7 Informative Fragments (Vidal-Naquet and Ullman, 2003)", "text": "In (Vidal-Naquet and Ullman, 2003), the authors propose a feature criterion called Informative Fragments (IG). The feature criteria of each new, unselected feature are: JIF (Xk) = min Xj-S [I (XjXk; Y) \u2212 I (Xj; Y)]. (50) The intuition behind Informative Fragments is that the addition of the new feature Xk should maximize the value of conditional mutual information between Xk and existing features in S versus the mutual information between Xj and Y. An interesting phenomenon of informative fragments is that with the chain rule I (XkXj; Y) = I (Xj; Y) + I (Xk; Y | Xj) Informative Fragments have the equivalent form of CMIM, so it can also be reduced to the framework of conditional probability maximization."}, {"heading": "2.2.8 Interaction Capping (Jakulin, 2005)", "text": "Interaction capping is a characteristic criterion similar to CMIM in Equation (48), but instead of restricting the term I (Xj; Xk) -I (Xj; Xk | Y) as non-negative: JCMIM (Xk) = I (Xk; Y) \u2212 \u2211 Xj Smax [0, I (Xj; Xk) \u2212 I (Xj; Xk | Y)]. (51) Obviously, it is a special case of non-linear combination of Shannon information concepts by setting the function g (.) to max [0, I (Xj; Xk) \u2212 I (Xj; Xk | Y)]."}, {"heading": "2.2.9 Double Input Symmetrical Relevance (Meyer and Bontempi, 2006)", "text": "Another class of information theory-based methods such as Double Input Symmetrical Relevance (DISR) (Meyer and Bontempi, 2006) uses normalization techniques to normalize mutual information (Guyon et al., 2008): JDISR (Xk) = \u2211 Xj-SI (XjXk; Y) H (XjXkY). (52) It is easy to confirm that DISR is a nonlinear combination of Shannon information concepts and can be reduced to the framework of conditional probability maximization."}, {"heading": "2.2.10 Fast Correlation Based Filter (Yu and Liu, 2003)", "text": "There are other information theory methods for selecting characteristics that cannot simply be reduced to the uniform conditional probability maximization framework. At this point, we present an algorithm called Fast Correlation Based Filter (FCBF) (Yu and Liu, 2003), which is a filtering method that simultaneously exploits the correlation between characteristics and characteristics. The algorithm works as follows: (1) At a predefined threshold, a subset of characteristics S is selected that is strongly correlated with the class designation S, where SU is the symmetric uncertainty that SU expresses between a set of characteristics XS and the class designation Y: SU (XS, Y) = 2 I (XS; Y) H (XS) + H (Y) (Y). (53) A particular characteristic Xk is referred to as the dominant iff SU (Xk, Y), and the characteristics Xs (XS) and there are no characteristics Xj (k) (Xk = Y)."}, {"heading": "2.3 Sparse Learning based Methods", "text": "To address this problem, embedded methods embed the selection phase of the features in the construction of the learning algorithm, where these two phases complement each other; the selected features are suitable for this learning algorithm, which can be used for further analysis; there are three main types of embedded feature selection methods: the first type of embedded methods are pruning methods; at the very beginning, they use the entire set of features to train a learning model and then attempt to remove some features by setting the coefficients of features to zero while maintaining model performance; an example of methods for eliminating recursive features based on the support of a vector machine (SVM) (Guyon et al., 2002); and the second type of embedded methods includes a built-in feature selection mechanism such as ID3 (Quinlan, 1986) and Clan (Clan, 1993), which aim to minimize learning capability."}, {"heading": "2.3.1 Feature Selection with \u21131-norm Regularizer (Supervised) (Tibshirani,", "text": "It is the first one we consider, binary classification (yi is either 0 or 1) or regression problem with only one regression target. Without loss of generality, we can only consider linear classification or linear regression, but it can easily be extended to non-linear problems. To achieve feature selection, classification or regression target is added as a linear combination of data instances X such as SVM (Cortes and Vapnik, 1995) and logistical regression (Hosmer Jr and Lemeshow, 2004). To achieve feature selection, classification or regression model is applied to the classification or regression model. A major advantage of regulation is (Tibshirani, 1996). Hastie et al is that it forces some feature coefficients to become smaller and in some cases exactly zero."}, {"heading": "2.3.3 Efficient and Robust Feature Selection (Supervised) (Nie et al., 2010)", "text": "In (Nie et al., 2010), the authors propose an efficient and robust method of feature selection (REFS) by applying a common \"2.1 standard minimization\" for both loss function and regulation, arguing that the \"2 standard-based loss function\" is sensitive to loud data, while the \"2.1 standard-based loss function\" is more resistant to noise, because the \"2.1 standard-based loss function\" has a rotationally invariant property (Ding et al., 2006). Consistent with the 2.1 standard-regulated feature selection model, the \"2.1 standard loss function\" is added a \"regulator\" to achieve group economy. REFS's objective function is formulated as follows: min W | XW \u2212 Y | | 2.1 + \u03b1 | 2.1, (63) The objective function of RES, however, is not conformable in 2010 (both terms are non-conformable)."}, {"heading": "2.3.4 Multi-Cluster Feature Selection (Unsupervised) (Cai et al., 2010)", "text": "Most existing sparse learning approaches consist of three steps (1), and the selection phase is then derived from the sparse coefficients of characteristics. However, because the data collected is costly and time-consuming to obtain the unmonitored sparse learning characteristics, the selection of characteristics has received increasing attention in recent years (Cai et al., 2010; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015). Multi-cluster feature selection (MCFS) (Cai et al., 2010) is one of the first unmonitored feature selection algorithms to use sparse learning techniques to guide the feature selection process, MCFS proposes to select characteristics that can cover the multicluster-like structure of data in which spectral analysis is used to measure the correlation between different characteristics. MCS consists of three steps (1)."}, {"heading": "2.3.5 \u21132,1-norm Regularized Discriminative Feature Selection (Unsupervised) (Yang et al., 2011)", "text": "An alternative way is to use the discriminatory information encrypted in the data to select the most discriminatory characteristics by evaluating both the discriminatory information and the attribute correlations (Fukunaga, 2013). First, we briefly introduce discriminatory information coming from s classes and there are ni-instances in i-th class. Y-1) n-s denotes the class label for n-th classes. Y-n-s denotes the class label matrix so that Y (i, j) s is in j-th class, otherwise Y."}, {"heading": "2.3.6 Feature Selection Using Nonnegative Spectral Analysis (Unsupervised) (Li et al., 2012)", "text": "In addition to UDFS, there are several other ways to use discriminatory information for unattended feature selection. (NDFS) is an algorithm that performs spectral clustering and feature selection simultaneously in a common framework to select a subset of discriminatory features. (NDFS assumes that pseudo-class indicators can be achieved through spectral cluster techniques.) Unlike most existing spectral cluster techniques, NDFS does not imply negative and orthogonal constraints during the spectral cluster phase. The argument is that with these constraints, the learned pseudo-class labels are closer to real cluster class labels acting as regression constraints to guide the feature selection phase."}, {"heading": "2.3.7 Feature selection via joint embedding learning and sparse regression (Unsupervised) (Hou et al., 2011)", "text": "The selection of features via the joint embedding of learning and sparse regression (JELSR) (Hou et al., 2011) is an unattended feature selection similar to NDFS. Both embed the learning process of the pseudo-class in the sparse regression for the feature selection, the difference being that NDFS uses Diagram Laplacian to learn the pseudo-class labeling, while JELSR uses local linear embedding. Furthermore, NDFS imposes both orthogonal and non-negative constraints on the pseudo-class labeling, while JELSR only considers the orthogonal constraints. In the first step, JELSR builds a k-next neighbor chart. In the second step, NDFS does not use explicit affinity matrix S such as MCFS and NDFS, JELSR uses the local linear embedding (Sauerl and Roerum 2000)."}, {"heading": "2.4 Statistical based Methods", "text": "Another category of feature selection algorithms is based on different statistical metrics; we group them as statistical methods in this survey. Because they are based on some statistical metrics rather than learning algorithms to evaluate the relevance of features, most of them are filter-based methods. In addition, most statistical algorithms analyze features individually, so feature redundancies are inevitably ignored during the selection phase. In this category, we introduce some representative feature selection algorithms. Note that the vast majority of algorithms in this category work with discrete data, and numerical datasets must first perform discretization."}, {"heading": "2.4.1 Low Variance (Pedregosa et al., 2011) (Unsupervised)", "text": "Low variance is a simple attribute selection algorithm that eliminates the attribute whose variance is below a certain threshold. Suppose the dataset consists only of Boolean attributes, i.e. the attribute values are either 0 and 1, since the Boolean attributes are Bernoulli random variables, its variance value can be calculated as follows: Variance value (fi) = p (1 \u2212 p), (80) where p denotes the percentage of instances that assume the attribute value of 1. After the variance of the attributes has been determined, the attribute can be truncated directly with a variance value below a predefined threshold."}, {"heading": "2.4.2 T-score (Davis and Sampson, 1986) (Supervised)", "text": "The t-score is used for binary classification problems. Suppose that for each character fi there are the mean characteristic values for the instances of the first and second class, \u03c31 and \u03c32 are the corresponding standard deviation values, n1 and n2 the number of instances of these two classes. Then, the t-score for the character fi can be calculated as follows: t-score (fi) = | \u00b51 \u2212 \u00b52 | \u221a \u03c321 n1 + \u03c322 n2. (81) The basic idea of the t-score is to assess whether the mean value of two classes can be statistically distinguished by calculating the ratio between the mean difference and the variance of two classes. Normally, the higher the t-score, the more important the attribute."}, {"heading": "2.4.3 F-score (Wright, 1965) (Supervised)", "text": "t-score can only be used for binary classification tasks, so it has some limitations. f -score can deal with the multi-class situation by testing whether a feature is capable of separating samples from different classes well. Considering both class variance and class variance, the f -score can be calculated as follows: f score (fi) = \u2211 j nj c \u2212 1 (\u00b5j \u2212 \u00b5) 21 n \u2212 c \u0445 j (nj \u2212 1) \u03c32j. (82) Feature fi, nj, \u00b5, \u00b5j denote the number of instances from class j, the mean feature value, the mean feature value in class j, the standard deviation of the feature value in class j. Similar to t-score, the higher the t-score, the more important the feature."}, {"heading": "2.4.4 Chi-Square Score (Liu and Setiono, 1995) (Supervised)", "text": "The chi-square score of this attribute uses the independence test to determine whether the attribute is independent of the class designation. Given a particular attribute with different attribute values, the chi-square score of this attribute can be calculated as follows: chi-square score (fi) = r-score = 1c-score = 1 (njs \u2212 \u00b5js) 2 \u00b5js, (83) where njs is the number of instances with the j-th attribute value fi. Furthermore, a higher chi-square score indicates that the attribute is relatively more important."}, {"heading": "2.4.5 Gini Index (Gini, 1912) (Supervised)", "text": "Gini index is a statistical measure to quantify whether the attribute is capable of separating instances from different classes. In view of a attribute fi with different attribute values, W should denote for the j-th attribute value the set of instances with attribute value smaller or equal to the j-th attribute value, let W denote the set of instances with attribute value greater than the j-th attribute value. In other words, the j-th attribute value can split the dataset into W and W, then the Gini index value for the attribute fi is given as follows: Gini index value (fi) = min W (p (W) (1 \u2212 c \u2211 s = 1p (Cs | W) 2) + p (W) (1 \u2212 c \u0445 s = 1p (Cs | W) 2))))), (84) where Cs indicates that the class name is p (.) (In contrast to s (the Cp), the probability of the class is given by the W)."}, {"heading": "2.4.6 CFS (Hall and Smith, 1999) (Supervised)", "text": "The basic idea of CFS is to use a correlation based on heuristics to evaluate the value of the attribute subset F. CFS value (F) = krcf \u221a k + k (k \u2212 1) rff, (85) where the CFS value indicates the heuristic \"value\" of the attribute subset F with k characteristics. rcf is the mean attribute-class correlation and rff is the average attribute-attribute intercorrelation. In Equation (85) the counter indicates the predictive power of the attribute subset, while the denominator shows how much attribute redundancy the attribute set has. The basic idea of CFS is that a good attribute subset should have a strong correlation with class designations and are weakly intercorrelated. In order to find the attribute-class correlation and attribute-attribute correlation, CFS uses a fetal subset to estimate the optimum attribute uncertainty between the respective attribute 1992 (Vettersociation) and the global attribute strategy."}, {"heading": "3. Feature Selection with Structure Features", "text": "Existing methods for selecting traits for generic data are based on the strong assumption that traits are independent of each other while completely ignoring the intrinsic structures between traits. For example, these trait selection methods can select the same subset of traits even though the traits are reshuffled (Ye and Liu, 2012). However, in many real-world applications, traits also have different types of structures, such as spatial or temporal smoothness, disjunction groups, overlap groups, trees and diagrams (Tibshirani et al., 2005; Jenatton et al., 2011; Yuan et al., 2011; Huang et al., 2011; Zhou et al., 2012). With the existence of these intrinsic trait structures, feature selection algorithms that integrate knowledge of the structures of traits can help select more relevant traits and thus improve some post-learning tasks, such as classification and clustering. A motivating example is bioinformatics."}, {"heading": "3.1 Feature Selection with Group Feature Structures", "text": "In many real-world applications, features have group structures; one of the most common examples is that in multifactor analysis of variance (ANOVA), each factor is associated with several groups and can be expressed through a number of dummy features (Yuan and Lin, 2006); some other examples include different frequency bands, which are represented as groups in signal processing (McAuley et al., 2005); and genes with similar functionalities, which act as groups in bioinformatics (Ma et al., 2007). Therefore, when selecting features, it is more attractive to explicitly consider the group structure among features."}, {"heading": "3.1.1 Group Lasso (Supervised) (Yuan and Lin, 2006)", "text": "Lasso (Yuan and Lin, 2006; Bach, 2008; Jacob et al., 2009; Meier et al., 2008), which derives feature coefficients from some groups to be exactly zero, is a solution to this problem. In other words, a group of characteristics is selected as a whole or not selected. The difference between lasso and group lasso is shown by the toy example in Figure (9) that a total of 25 characteristics from 4 different groups G = {w1, G2, G3, G4} (each column denotes a feature) and there is no overlap between these groups. Looking at the explicit feature structure, we can divide the feature coefficients for these 25 characteristics into 4 parts G = {w1; w3; w4}, where wi denotes the feature coefficients for the characteristics of the group Gi. Lasso completely ignores the group structures between the characteristics Gi."}, {"heading": "3.1.3 Overlapping Sparse Group Lasso (Supervised) (Jacob et al., 2009)", "text": "One motivating example is the use of biologically significant gene / protein groups mentioned in (Ye and Liu, 2012). Different groups of genes can overlap, i.e., one protein / gene can belong to several groups. In this scenario group Lasso and Sparse Group Lasso are not applicable. The general overlap of Sparse Group lasso-regularization resembles the regularization concept of Sparse Group lasso: minimal loss (w; X, y) + \u03b1 | | 1 + (1 \u2212 \u03b1) g = 1hi | | wGi | 2. (89) The only difference in overlapping of Sparse Group lasso consists in that different characteristic groups Gi can have an overlapping, i.e. that there are at least two groups Gi and Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Gj Ge and Gj Gj G"}, {"heading": "3.2 Feature Selection with Tree Feature Structures", "text": "In addition to group structures, characteristics can also have other types of structures, such as tree structures. For example, in image processing such as facial images, different pixels (characteristics) can be represented as a tree, with the root node displaying the entire face, its child nodes being the various organs, and each individual pixel being considered a leaf node. In other words, these pixels enjoy a spatial localization structure. Another motivating example is that genes / proteins can form certain hierarchical tree structures (Liu and Ye, 2010). Recently, it was proposed to adopt the feature selection for characteristics that can be represented in an index tree (Kim and Xing, 2010; Liu and Ye, 2010; Jenatton et al., 2010)."}, {"heading": "3.2.1 Tree-guided Group Lasso (Supervised) (Liu and Ye, 2010)", "text": "In tree-led group Lasso, the structure can be represented through the characteristics as a tree with leaf nodes as characteristics. Each internal node in the tree denotes a group of characteristics so that the internal node is considered the root of a subtree and the group of characteristics is considered as leaf nodes. Each internal node in the tree is associated with a weight representing the height of its subtree, or how closely correlated the characteristics in that subtree are. We follow the definition of (Liu and Ye, 2010) to define tree-led group Lasso, for an index tree G with a depth of d, Gi = Gi1, Gi2,... Gini} denotes the total set of nodes (characteristics) in the i-th level (the root node is defined as level 0), and ni denotes the number of nodes in the i-th level, i-th level."}, {"heading": "3.3 Feature Selection with Graph Feature Structures", "text": "In many real-world applications, traits can have strong dependencies. For example, if we consider each word as a trait, we have synonyms and antonyms relationships between different words (Fellbaum, 1998). Furthermore, many biological studies show that genes tend to work in groups according to their biological functions, and that there are strong dependencies between some genes. Since traits in these cases have some dependencies, we can model the traits using an undirected graph in which nodes represent traits and edges between traits. Recent studies have shown that learning performance can be improved if we explicitly take into account dependency information between traits (Sandler et al., 2009; Kim and Xing, 2009; Yang et al., 2012). If there are some dependencies between traits, we can use an undirected graph G (N, E) to encode these dependencies."}, {"heading": "3.3.1 Laplacian Lasso (Supervised) (Ye and Liu, 2012)", "text": "Since characteristics have graph structures when two nodes (features) Ni and Nj are connected by an edge in G (N, E), the characteristics fi and fj are more likely to be selected together, and they should have similar coefficients of characteristics. One way to achieve this goal is by graph lasso - adding a graph regularizer based on lasso. The formulation is as follows: Minimum loss (w; X, y) + \u03b1 | | 1 + (1 \u2212 \u03b1) \u2211 i, jA (i, j) (wi \u2212 wj) 2, (92), where the first regularization term \u03b1 | | 1 is the same as the regularization term as lasso, while the second term forces that when a pair of characteristics exhibits strong dependencies, i.e. large A (i, j) (i, j), their coefficients of characteristics in the matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix."}, {"heading": "3.3.2 GFLasso (Supervised) (Kim and Xing, 2009)", "text": "In Equation (93), the characteristic structures of a chart are represented by an unsigned graph, and it promotes characteristics associated with similar coefficients of characteristics. However, in many cases, characteristics can also be negatively correlated. In this case, the characteristic graph G (N, E) is represented by a signed graph, with positive and negative edges. Recently, it has been proposed to explicitly take into account both positive and negative correlations of characteristics, with the objective function of the GFLasso being formulated as follows: minimal loss (w; X, y) + \u03b1 | | | 1 + (1 \u2212 \u03b1) \u2211 i, jA (i, j) (wi \u2212 sign (ri, j) wj) 2, (94), where ri, j indicates the correlation between two characteristics fi and fj. If two characteristics correlate positively, we have A (i, j) = 1 and ri, j > 0, and the penalty term necessarily correlate the characteristics (ri, j) wj, where we can have a positive coreliant on the other side &lt."}, {"heading": "3.3.3 GOSCAR (Supervised) (Yang et al., 2012)", "text": "In order to address the limitations of GFLasso (Yang et al., 2012), a GOSCAR algorithm proposed a formulation that makes the paired coefficients equivalent if they are connected via the feature diagram G (N, E). (95) In the above formulation, the formulation of GOSCAR is defined as: min wloss (w; X, y) + \u03b1 | 1 + (1 \u2212 \u03b1) \u2211 i, jA (i, j) max (wj | wj |). (95) In the above formulation, the \"1-standard regularization\" is used for the selection of features, while the paired norm term punishes large coefficients. The paired norm term can be composed as: max (wi | wj |)."}, {"heading": "4. Feature Selection with Heterogeneous Data", "text": "Traditional feature selection algorithms can only work with generic data from a single data source based on the data-independent and identically distributed (i.e. distributed) assumption, but heterogeneous data is becoming increasingly common in the age of big data. In the medical field, for example, high-dimensional gene traits are often associated with different types of clinical traits. As the data from each source can be noisy, partial or redundant, selecting relevant sources and sharing them for effective feature selection is another difficult issue. Another example is in social media platforms, where instances of high-dimensional traits are often linked together to integrate feature selection information. In this section, we review current feature selection algorithms for heterogeneous data from three aspects: (1) feature selection for linked data; (2) feature selection for multi-source data; and (3) feature selection data for multi-source data."}, {"heading": "4.1 Feature Selection Algorithms with Linked Data", "text": "Linked data has become ubiquitous in real-world applications such as Twitter4 (tweets linked by hyperlinks), Facebook social networks (people connected by friendships), and biological networks (protein interaction networks). Because linked data is related by different types of links, it differs from traditional attribute value data (or \"flat\" data). Figure (12) illustrates a toy example of linked data and its two representations. Figure (12 (a) shows 8 linked instances (u1 to u8), while Figure (12 (b) is a conventional representation of attribute value data, so that each row corresponds to an instance and each column corresponds to a feature."}, {"heading": "4.1.1 Feature Selection on Networks (Supervised) (Gu and Han, 2011)", "text": "In the United States, it is as if it is a way of linking content information and class information, and linking content information based on linking content information and class information. (Suppose the linking of content information to linking content matrix to n instances, and each instance is associated with a d-dimensional feature vector; Y-Rn \"c\" designates the class names in such a way that Y (i, k) is the class name for the i-th instance, Y \"(i, k) = 0 otherwise linking the adjacency matrix to all n-connected instances. The network can either be steered and not steered. With these notations, LapRLS can be a linear classified W.\""}, {"heading": "4.1.3 Unsupervised Feature Selection for Linked Data (Unsupervised) (Tang and Liu, 2012b, 2014b)", "text": "The task is to select a subjective feature from all relevant features. (LUFS) is an unattended feature selection box for linked data and essentially LUFS investigates how to take advantage of link information for unattended features. (LUFS) In general, feature selection aims to select a subset of relevant features with some limitations, while in the monitored feature selection some alternative criteria play the role of links that need to be exploited. (The problem is formulated as follows: Given the linked instances {x1, x2}, their feature information and link information, a feature matrix X and an adjacency matrix A can be displayed."}, {"heading": "4.2 Feature Selection Algorithms with Multi-Source Data", "text": "In recent decades, many selection algorithms have been proposed for features that have proven effective in handling high-dimensional data, but most of them are designed for a single data source. In many data mining and machine learning tasks, we may have multiple data sources for the same set of data instances. For example, recent advances in bioinformatics show the existence of some non-coding RNA species in addition to the widely used messenger RNA, these non-coding RNA species operate across a variety of biological processes. The availability of multiple data sources makes it possible to solve some intractable problems with a single source, as the multi-faceted representations of data can help depict some intrinsic patterns hidden in a single data source. The task of selecting features from multiple sources is formulated as follows: Given m data sources representing the same set of n instances, and their matrix representations X1, Rn \u00d7 d1, Xd2, Rd2, Rd2,... (where all features are selected from)."}, {"heading": "4.2.1 Multi-Source Feature Selection via Geometry-Dependent Covariance Analysis (Unsupervised) (Zhao and Liu, 2008)", "text": "In order to integrate information from multiple sources, the authors propose in (Zhao and Liu, 2008) an intuitive method to learn a global geometric pattern from all sources that reflects the intrinsic relationships between instances (Lanckriet et al., 2004), introducing a concept of geometry-dependent covariance that allows the use of global geometric pattern in covariance analysis to select characteristics. In the face of multiple local geometric patterns in an affinity matrix Si, in which i denotes the i-th data source, a global pattern can be obtained by combining all affinity matrices linearly as S =.m i = 1 \u03b1iSi, in which \u03b1i controls the affinity matrix of the i-th source. With the global geometric pattern derived from multiple data sources, a geometry-dependent sample covariance matrix for the target source Xi can be set as follows: C = S1\u2032 S1n \u2212 S1 \u2212 1 \u2212 1."}, {"heading": "4.3 Feature Selection Algorithms with Multi-View Data", "text": "These feature spaces are, of course, dependent and also high-dimensional, suggesting that feature selection is necessary to prepare these sources for effective data mining tasks such as multiview clustering. Since the multi-view feature selection aims to select features from different feature spaces simultaneously by using their relationships, they are, of course, different from the multi-source feature selection. The difference between multi-source feature selection and multi-view feature selection is illustrated in Figure (15). For parent multi-view feature selection, the most common approach is to Sparse Group Lasso et al., 2010; Peng et al., 2010) In this subsection we review some representative algorithms for unsupervised multi-view feature selection."}, {"heading": "4.3.2 Unsupervised Feature Selection for Multi-View Data (Unsupervised) (Tang et al., 2013)", "text": "AUMFS (Feng et al., 2013) learns a feature weight matrix for all characteristics from different angles in order to approximate pseudo-class labelling. In (Tang et al., 2013), the authors propose a novel, unattended feature selection method called Multi-view Feature Selection (MVFS). Similar to AUMFS, MVFS also uses spectral clustering with the affinity matrix from different angles to learn pseudo-class labelling. However, the optimization problem of MVFS can be formulated as follows: min tr (F'm, i = 1\u03bbiLiF) + m, i = 1\u03b2 (L XiWi \u2212 F, 2,1 + H, Wi, 2,1).t. F = I\u03bbi, and all characteristics are considered from different angles in order to adapt pseudo-class labelling."}, {"heading": "4.3.3 Multi-view Clustering and Feature Learning via Structured Sparsity (Wang et al., 2013a)", "text": "In some cases, it is possible that features from a particular viewpoint may contain more discriminatory information than features from other viewpoints for different viewpoints. According to (Wang et al., 2013a), a novel feature selection algorithm is proposed in (Wang et al., 2013a) that is more useful in image processing than other types of features in determining stop signs. To address this problem in multiple feature selection, a new feature selection algorithm with a common group of features is proposed in (Wang et al., 2013a): 1 standard and 2.1 standard regulation. Crucially, the feature weight matrix W1,..., Wm from different viewpoints, the group features: 1 standard is defined as a \u03b1 W standard."}, {"heading": "5. Feature Selection with Streaming Data", "text": "The methods presented in the previous sections assume that all data instances and features are known in advance. However, in many real-world applications, it is not the case that we are more likely to be confronted with dynamic data streams and feature streams. At worst, the size of the data or features is unknown or even infinite, so it is not practical to wait until all data instances or features are available to perform feature selection. When streaming data, a motivating example is that when the problem of online spam email detection occurs, new emails keep arriving, it is not easy to apply batch mode feature selection methods to select relevant features in a timely manner. In an orthogonal setting, the feature selection for streaming features also has practical significance. For example, Twitter produces more than 320 million tweets a day and a large amount of slang words (features) are generated continuously. These slang words immediately attract the attention of users and become short in time."}, {"heading": "5.1 Feature Selection Algorithms with Feature Streams", "text": "For the problem of feature selection for streaming features, the number of instances is considered constant, while the candidate features arrive individually; the task is to select a subset of relevant features in good time from all the features seen so far. Instead of searching for the entire feature pace, which is expensive, Streaming Feature Selection (SFS) processes a new feature upon arrival; a general framework of streaming feature selection is shown in Figure (16). At each step, a typical SFS algorithm determines whether the last arrived feature should be accepted; if the feature is added to the selected feature set, it then determines whether some existing features should be discarded from the selected feature set. The process repeats until no new features appear. Different algorithms have different implementations in the first step, which checks newly arrived features; the second step, which checks existing features, is optional for some algorithms."}, {"heading": "5.1.1 Grafting Algorithm (Supervised) (Perkins and Theiler, 2003)", "text": "The first attempt to perform the selection of streaming features is credited (Perkins and Theiler, 2003) for proposing a streaming feature selection framework based on a framework parameterized by a characteristic weight vector w for regulating the pitch gradient (Perkins et al., 2003). Grafting is a general technique that can handle a variety of models parameterized by a characteristic weight vector w subject to regulation of the 1 standard, such as Lasso: min wloss (w; X, y) + \u03b1 | | 1. (111) In general, grafting can work with the following objective function, where the features one arrive at a certain point in time, the central challenge being to efficiently update the parameter w when new features are constantly coming. The basic idea of the graft feature selection algorithm is based on the observation of a new feature (the incorporation of a new model into the Eq)."}, {"heading": "5.1.2 Alpha-investing Algorithm (Supervised) (Zhou et al., 2005b)", "text": "Alpha-investing (Zhou et al., 2005b) is an adaptive complexity penalty method that dynamically changes the threshold of error reduction required to accept a new feature. It is motivated by the desire to control the false detection rate (FDR) of newly arrived features so that a small proportion of false features do not greatly affect model accuracy. The detailed algorithm works as follows: \u2022 Initialize w0 = 0 (probability of false positives), i = 0 (index of features), selected features in the model SF = \u2205 \u2022 Step 1: Get a new feature fi \u2022 Step 2: Set \u03b1i = wi / (2i) \u2022 Step 3: wi + 1 = wi \u2212 \u03b1i, SF = Streaming value, if p value (fi, SF), p value wi + 1 = wi + \u03b1 value is invested when the new feature fi \u2022 Step 2: Set a new feature fi + (step fi, SF = wi), SF value (SF) < SF + wi = \u03b1 (wi fi) (wi), SF wi-fi = (wi)."}, {"heading": "5.1.3 Online Streaming Feature Selection Algorithm", "text": "(Wu et al., 2010, 2013) Unlike transplants and alpha investments, the authors examined the streaming feature selection problem from an information theory perspective using the concept of the Markov blanket (Wu et al., 2010, 2013). According to their definition, the entire feature set consists of four types of features: irrelevant features, redundant features, weakly relevant but not redundant features, and highly relevant features. However, features have arrived dynamically in a streaming mode, it is difficult to find all strongly relevant and non-redundant features. The proposed method, OSFS, is able to capture these non-redundant and highly relevant features. (1) Online relevance analysis, and (2) online redundancy analysis is difficult to find all strongly relevant and non-redundant features."}, {"heading": "5.1.4 Streaming Feature Selection with Group Structures", "text": "(Supervised) (Wang et al., 2013b; Li et al., 2013) Prior mentioned streaming feature selection algorithms evaluate new features individually. However, streaming features also cannot process group structures and current group feature selection algorithms such as Group Lasso online. Therefore, the authors in (Wang et al., 2013b, 2015) propose a streaming feature selection algorithm (OGFS) that consists of two parts: online intra group selection and online intergroup selection. In the online group selection phase, OGFS uses spectral feature selection techniques for streaming features in a particular group to assess whether the newly arrived feature increases the ratio between class distances and cash substances or whether it is a significant feature with discriminatory force. If the inclusion of this new feature increases this ratio or is statistically significant as a new subject."}, {"heading": "5.1.5 Unsupervised Streaming Feature Selection in Social Media (Unsupervised) (Li et al., 2015)", "text": "In social media, however, it is easy to collect huge amounts of unlabeled data while it is time and labor consuming to obtain labels. To deal with large-scale unlabeled data on social media, the authors proposed in (Li et al., 2015) a USFS algorithm to study uncontrolled streaming functionality. USFS \"key idea is to use source information such as link information to allow for an uncontrolled selection of streaming functionality. Figure 2 shows the workflow of the proposed USFS framework. USFS first uncovers hidden social factors from linking information through a mixed membership block model (Airoldi et al., 2009). Suppose that the number of instances is n and each instance is associated with a k-dimensional latent factor."}, {"heading": "5.2 Feature Selection Algorithms with Data Streams", "text": "In this subsection, we will examine the problem of feature selection with data streams, which is considered a double problem of streaming feature selection. Most existing feature selection algorithms assume that all data instances are available before they perform feature selection. However, such assumptions are not always correct in real-world applications that data instances are dynamically generated and arrive sequentially, so it is necessary and urgent to find some solutions to deal with high-dimensionality sequential data."}, {"heading": "5.2.1 Online Feature Selection (Supervised) (Wang et al., 2014)", "text": "In (Wang et al., 2014) an online feature selection algorithm (OFS) for binary classification is proposed. Let {x1, x2,..., xt...} and {y1, y2,..., yt...} denote a sequence of instances of input data and input class names, each instance xi + Rd being in a d-dimensional space and class name yi [\u2212 1, + 1}. The task of the OFS is to learn a linear classifier w (t) Rd that can be used to classify each instance xi by a linear function character (w (t) xi). To achieve the purpose of the feature selection, it requires that the linear classifier w (t) has so many elements in most B-dissimilar elements that it w (t), 0 \u2264 B. It indicates that for most B-characteristics no new functions are used for the classification xi (w)."}, {"heading": "5.2.2 Unsupervised Feature Selection on Data Streams (Unsupervised) (Huang et al., 2015)", "text": "OFS assumes that the class names of continuously generated data streams are available. However, in many real-world applications it is not the case that information is costly to obtain. To select a subset of relevant characteristics in a timely manner when unlabeled data is continuously generated, authors in (Huang et al., 2015) propose a novel, unattended feature selection method (FSDS) that is capable of selecting the characteristic coefficients in a timely manner with only one pass of data and utilizing limited storage capabilities.The basic idea of the FSDS is to use matrix sketches to efficiently maintain a slight approximation of the currently observed data and then apply regulated regression to obtain the characteristic coefficients that can be further used to preserve the importance of the characteristics.The authors empirically demonstrate that when some orthogonal conditions are met, the ridge regression can replace the laser selection that is efficient for characteristic selection."}, {"heading": "6. Performance Evaluation", "text": "In this section, we discuss the evaluation of feature selection algorithms, focusing on feature selection algorithms for generic data. First, we present the developed feature selection repository, then we present the algorithms to be evaluated, and some publicly available benchmark data sets that we collect. Finally, we present some widely used evaluation metrics and present the empirical experimental results."}, {"heading": "6.1 Feature Selection Repository", "text": "First, we present our efforts in developing a feature selection repository - scikit-feast. The purpose of this feature selection repository is to collect some widely used feature selection algorithms that have been developed in feature selection research to serve as a platform for facilitating their application, comparison, and joint study. The feature selection repository effectively helps researchers achieve more reliable evaluation in the process of developing new feature selection algorithms. We are developing the open source feature selection repository scikit-feast through one of the most popular programming languages - python. It includes more than 40 popular feature selection algorithms, including most of the traditional feature selection algorithms mentioned in this survey, and some structural and streaming feature selection algorithms - python. It includes more than 40 popular feature selection algorithms, including several traditional feature selection algorithms and some structural and streaming feature selection algorithms - python. We also maintain this feature selection kit based algorithm on a Learlearn / learn package of two scientific sources mentioned in this survey."}, {"heading": "6.2 Algorithms", "text": "We will provide detailed information on how these algorithms are evaluated, including data sets, evaluation criteria and experimental setup. Selected features of discrete selection algorithms are listed as follows: (3) weight constraints or partial selection; (4) characteristics of each algorithm: numerical or categorical; (4) the first two elements have already been mentioned; and the third category categorizes these algorithms based on sparse learning, based on statistical fundamentals; (3) results: weight constraint or partial selection; (4) features: numerical or categorized; (4) features are considered as those based on results."}, {"heading": "6.3 Datasets", "text": "To test different algorithms, we collect 25 publicly available benchmark data sets to evaluate the performance of feature selection algorithms. We list the detailed information of each data set in Table 2. We carefully select data sets from different categories, such as text data, image data, biological data, and some others. The characteristics in these data sets are either numerical or categorical values. We also present the number of attributes, the number of instances, and the number of classes for each data set. The heterogeneity of the data is important to show the strength and weakness of algorithms in different applications."}, {"heading": "6.4 Evaluation Metrics", "text": "Next, we introduce the commonly used method of evaluating the performance of feature selection algorithms. We have different yardsticks for monitored and unmonitored methods. Different assessment strategies are used for algorithms of different output types: 1. If it is a method of attribute weighting that returns the attribute value for each attribute, then we use all the selected features to perform the evaluation. Supervised Methods: To test the performance of the verified attribute selection algorithms, the evaluation framework introduced in Figure (3) is used. The entire dataset is usually divided into two parts - the training set T and the test set U. Feature selection algorithms are first applied to the training set T to obtain a subset of relevant features F."}, {"heading": "6.5 Experimental Results", "text": "The experimental results are shown in http: / / featureselection.asu.edu / scikit-feast / datasets.php) and for each dataset we list all applicable attribute selection algorithms along with its evaluation either in terms of classification or cluster task."}, {"heading": "7. Open Problems and Challenges", "text": "Over the last two decades, there has been tremendous work in developing feature-select algorithms for both theoretical analysis and real-world applications. However, we believe that more work can be done in this community. Here are some challenges and concerns that we need to mention and discuss."}, {"heading": "7.1 Scalability", "text": "In many scientific and business applications, data is usually measured in terabytes (1TB = 1012 bytes). Normally, data sets in the order of terabytes cannot be loaded directly into memory, limiting the usability of most feature selection algorithms. Currently, there are some attempts to use distributed programming frames such as MapReduce and MPI to perform a parallel feature selection for very large data sets (Singh et al., 2009; Zhao et al., 2013; Yamada et al., 2014). Furthermore, most proposed feature selection algorithms require time complexity proportional to O (d2) or even O (d3), with d being the feature selection."}, {"heading": "7.2 Stability", "text": "While the stability of these algorithms is also an important consideration in the development of new trait selection algorithms, a motivating example from the field of bioinformatics shows that domain experts would like to see the same set or similar sets of genes (traits) selected each time they receive new samples in the small amount of perturbations. Otherwise, domain experts would not trust these algorithms if they received completely different sets of traits with small data perturbations. Given their importance in practical application, the stability of trait selection algorithms has received increasing attention in the community (Kalousis et al., 2007; Er and Yu, 2010). It is observed that many known trait selection algorithms suffer from the low stability problem after the small data perturbations have been introduced in education."}, {"heading": "7.3 Model Selection", "text": "For most feature selection algorithms, especially in trait weighting methods, we need to specify the number of traits selected, but it is often not known which is the optimal number of selected traits. If the number of selected traits is too high, it can increase the risk of including some noisy, redundant and irrelevant traits that can compromise learning performance. On the other hand, it is not good to include too few selected traits, as some relevant traits can be eliminated. In practice, we usually use a heuristic method to search the number of selected traits and select the number that has the best classification or cluster performance, but the whole process is computationally expensive. It is still an open and difficult problem to determine the optimal number of selected traits. In addition to the optimal number of selected traits, we also need to specify the number of clusters or pseudo-classes for unmonitored trait selection algorithms."}, {"heading": "8. Conclusion", "text": "In the meantime, it is a hot research topic with practical relevance in many areas such as statistics, pattern recognition, machine learning, and data mining (including web, text, image, and microarrays).The goals of feature selection are: to build simpler and more comprehensible models, improve the performance of data mining, and assist in the preparation, purification, and understanding of data. In recent years, we have witnessed the development of hundreds of new feature selection methods.This survey article aims to provide a comprehensive overview of recent advances in feature selection. We first introduce basic concepts of feature selection, and emphasize the importance of using feature selection algorithms to solve practical problems."}], "references": [{"title": "Mixed membership stochastic blockmodels", "author": ["Edoardo M Airoldi", "David M Blei", "Stephen E Fienberg", "Eric P Xing"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Airoldi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Airoldi et al\\.", "year": 2009}, {"title": "The effect of the characteristics of the dataset on the selection stability", "author": ["Salem Alelyani", "Huan Liu", "Lei Wang"], "venue": "In Tools with Artificial Intelligence (ICTAI),", "citeRegEx": "Alelyani et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Alelyani et al\\.", "year": 2011}, {"title": "Feature selection for clustering: A review", "author": ["Salem Alelyani", "Jiliang Tang", "Huan Liu"], "venue": "Data Clustering: Algorithms and Applications,", "citeRegEx": "Alelyani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alelyani et al\\.", "year": 2013}, {"title": "Consistency of the group lasso and multiple kernel learning", "author": ["Francis R Bach"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bach.,? \\Q2008\\E", "shortCiteRegEx": "Bach.", "year": 2008}, {"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["Roberto Battiti"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Battiti.,? \\Q1994\\E", "shortCiteRegEx": "Battiti.", "year": 1994}, {"title": "An improved multitask learning approach with applications in medical diagnosis", "author": ["Jinbo Bi", "Tao Xiong", "Shipeng Yu", "Murat Dundar", "R Bharat Rao"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Bi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bi et al\\.", "year": 2008}, {"title": "Active learning for networked data", "author": ["Mustafa Bilgic", "Lilyana Mihalkova", "Lise Getoor"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "Bilgic et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bilgic et al\\.", "year": 2010}, {"title": "Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with oscar", "author": ["Howard D Bondell", "Brian J Reich"], "venue": null, "citeRegEx": "Bondell and Reich.,? \\Q2008\\E", "shortCiteRegEx": "Bondell and Reich.", "year": 2008}, {"title": "Conditional likelihood maximisation: a unifying framework for information theoretic feature selection", "author": ["Gavin Brown", "Adam Pocock", "Ming-Jie Zhao", "Mikel Luj\u00e1n"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Brown et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Brown et al\\.", "year": 2012}, {"title": "Unsupervised feature selection for multi-cluster data", "author": ["Deng Cai", "Chiyuan Zhang", "Xiaofei He"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Cai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2010}, {"title": "The dantzig selector: statistical estimation when p is much larger than n", "author": ["Emmanuel Candes", "Terence Tao"], "venue": "The Annals of Statistics,", "citeRegEx": "Candes and Tao.,? \\Q2007\\E", "shortCiteRegEx": "Candes and Tao.", "year": 2007}, {"title": "Spectral k-way ratio-cut partitioning and clustering", "author": ["Pak K Chan", "Martine DF Schlag", "Jason Y Zien"], "venue": "Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on,", "citeRegEx": "Chan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Chan et al\\.", "year": 1994}, {"title": "A survey on feature selection methods", "author": ["Girish Chandrashekar", "Ferat Sahin"], "venue": "Computers & Electrical Engineering,", "citeRegEx": "Chandrashekar and Sahin.,? \\Q2014\\E", "shortCiteRegEx": "Chandrashekar and Sahin.", "year": 2014}, {"title": "Spectral graph theory, volume 92", "author": ["Fan RK Chung"], "venue": "American Mathematical Soc.,", "citeRegEx": "Chung.,? \\Q1997\\E", "shortCiteRegEx": "Chung.", "year": 1997}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "Cover and Thomas.,? \\Q2012\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 2012}, {"title": "A direct formulation for sparse pca using semidefinite programming", "author": ["Alexandre d\u2019Aspremont", "Laurent El Ghaoui", "Michael I Jordan", "Gert RG Lanckriet"], "venue": "SIAM review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Statistics and data analysis in geology, volume 646", "author": ["John C Davis", "Robert J Sampson"], "venue": null, "citeRegEx": "Davis and Sampson.,? \\Q1986\\E", "shortCiteRegEx": "Davis and Sampson.", "year": 1986}, {"title": "Feature selection using multiple streams", "author": ["Paramveer S Dhillon", "Dean P Foster", "Lyle H Ungar"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Dhillon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2010}, {"title": "1-pca: rotational invariant l 1-norm principal component analysis for robust subspace factorization", "author": ["Chris Ding", "Ding Zhou", "Xiaofeng He", "Hongyuan Zha. R"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Ding et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2006}, {"title": "Supervised and unsupervised discretization of continuous features", "author": ["James Dougherty", "Ron Kohavi", "Mehran Sahami"], "venue": "In Machine learning: proceedings of the twelfth international conference,", "citeRegEx": "Dougherty et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dougherty et al\\.", "year": 1995}, {"title": "Unsupervised feature selection with adaptive structure learning", "author": ["Liang Du", "Yi-Dong Shen"], "venue": "arXiv preprint arXiv:1504.00736,", "citeRegEx": "Du and Shen.,? \\Q2015\\E", "shortCiteRegEx": "Du and Shen.", "year": 2015}, {"title": "Pattern classification", "author": ["Richard O Duda", "Peter E Hart", "David G Stork"], "venue": null, "citeRegEx": "Duda et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duda et al\\.", "year": 2012}, {"title": "Least angle regression", "author": ["Bradley Efron", "Trevor Hastie", "Iain Johnstone", "Robert Tibshirani"], "venue": "The Annals of statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "A powerful feature selection approach based on mutual information", "author": ["Ali El Akadi", "Abdeljalil El Ouardighi", "Driss Aboutajdine"], "venue": "International Journal of Computer Science and Network Security,", "citeRegEx": "Akadi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Akadi et al\\.", "year": 2008}, {"title": "Multi-task feature learning", "author": ["A Evgeniou", "Massimiliano Pontil"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Evgeniou and Pontil.,? \\Q2007\\E", "shortCiteRegEx": "Evgeniou and Pontil.", "year": 2007}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["Jianqing Fan", "Runze Li"], "venue": "Journal of the American statistical Association,", "citeRegEx": "Fan and Li.,? \\Q2001\\E", "shortCiteRegEx": "Fan and Li.", "year": 2001}, {"title": "Ultrahigh dimensional feature selection: beyond the linear model", "author": ["Jianqing Fan", "Richard Samworth", "Yichao Wu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2009}, {"title": "An efficient greedy method for unsupervised feature selection", "author": ["Ahmed K Farahat", "Ali Ghodsi", "Mohamed S Kamel"], "venue": "In Data Mining (ICDM),", "citeRegEx": "Farahat et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farahat et al\\.", "year": 2011}, {"title": "Adaptive unsupervised multiview feature selection for visual concept recognition", "author": ["Yinfu Feng", "Jun Xiao", "Yueting Zhuang", "Xiaoming Liu"], "venue": "In Computer Vision\u2013ACCV", "citeRegEx": "Feng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2012}, {"title": "Fast binary feature selection with conditional mutual information", "author": ["Fran\u00e7ois Fleuret"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fleuret.,? \\Q2004\\E", "shortCiteRegEx": "Fleuret.", "year": 2004}, {"title": "A note on the group lasso and a sparse group lasso", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": "arXiv preprint arXiv:1001.0736,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "Introduction to statistical pattern recognition", "author": ["Keinosuke Fukunaga"], "venue": "Academic press,", "citeRegEx": "Fukunaga.,? \\Q2013\\E", "shortCiteRegEx": "Fukunaga.", "year": 2013}, {"title": "Variability and mutability, contribution to the study of statistical distribution and relaitons", "author": ["CW Gini"], "venue": "Studi Economico-Giuricici della R,", "citeRegEx": "Gini.,? \\Q1912\\E", "shortCiteRegEx": "Gini.", "year": 1912}, {"title": "Genetic algorithms in search, optimization, and machine learning", "author": ["David E Golberg"], "venue": "Addion wesley,", "citeRegEx": "Golberg.,? \\Q1989\\E", "shortCiteRegEx": "Golberg.", "year": 1989}, {"title": "Towards feature selection in network", "author": ["Quanquan Gu", "Jiawei Han"], "venue": "In Proceedings of the 20th ACM international conference on Information and knowledge management,", "citeRegEx": "Gu and Han.,? \\Q2011\\E", "shortCiteRegEx": "Gu and Han.", "year": 2011}, {"title": "Generalized fisher score for feature selection", "author": ["Quanquan Gu", "Zhenhui Li", "Jiawei Han"], "venue": "In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Gu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2011}, {"title": "Gait feature subset selection by mutual information. Systems, Man and Cybernetics, Part A: Systems and Humans", "author": ["Baofeng Guo", "Mark S Nixon"], "venue": "IEEE Transactions on,", "citeRegEx": "Guo and Nixon.,? \\Q2009\\E", "shortCiteRegEx": "Guo and Nixon.", "year": 2009}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Guyon and Elisseeff.,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff.", "year": 2003}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["Isabelle Guyon", "Jason Weston", "Stephen Barnhill", "Vladimir Vapnik"], "venue": "Machine learning,", "citeRegEx": "Guyon et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Guyon et al\\.", "year": 2002}, {"title": "Feature extraction: foundations and applications, volume", "author": ["Isabelle Guyon", "Steve Gunn", "Masoud Nikravesh", "Lofti A Zadeh"], "venue": null, "citeRegEx": "Guyon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Guyon et al\\.", "year": 2008}, {"title": "Feature selection for machine learning: Comparing a correlation-based filter approach to the wrapper", "author": ["Mark A Hall", "Lloyd A Smith"], "venue": "In FLAIRS conference,", "citeRegEx": "Hall and Smith.,? \\Q1999\\E", "shortCiteRegEx": "Hall and Smith.", "year": 1999}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["David R Hardoon", "Sandor Szedmak", "John Shawe-Taylor"], "venue": "Neural computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "The elements of statistical learning: data mining, inference and prediction", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman", "James Franklin"], "venue": "The Mathematical Intelligencer,", "citeRegEx": "Hastie et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2005}, {"title": "Statistical Learning with Sparsity: The Lasso and Generalizations", "author": ["Trevor Hastie", "Robert Tibshirani", "Martin Wainwright"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2015}, {"title": "Laplacian score for feature selection", "author": ["Xiaofei He", "Deng Cai", "Partha Niyogi"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "He et al\\.,? \\Q2005\\E", "shortCiteRegEx": "He et al\\.", "year": 2005}, {"title": "Stable feature selection for biomarker discovery", "author": ["Zengyou He", "Weichuan Yu"], "venue": "Computational biology and chemistry,", "citeRegEx": "He and Yu.,? \\Q2010\\E", "shortCiteRegEx": "He and Yu.", "year": 2010}, {"title": "Applied logistic regression", "author": ["David W Hosmer Jr.", "Stanley Lemeshow"], "venue": null, "citeRegEx": "Jr and Lemeshow.,? \\Q2004\\E", "shortCiteRegEx": "Jr and Lemeshow.", "year": 2004}, {"title": "Feature selection via joint embedding learning and sparse regression", "author": ["Chenping Hou", "Feiping Nie", "Dongyun Yi", "Yi Wu"], "venue": "In IJCAI Proceedings-International Joint Conference on Artificial Intelligence,", "citeRegEx": "Hou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hou et al\\.", "year": 2011}, {"title": "Actnet: Active learning for networked texts in microblogging", "author": ["Xia Hu", "Jiliang Tang", "Huiji Gao", "Huan Liu"], "venue": "In SDM,", "citeRegEx": "Hu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2013}, {"title": "Unsupervised feature selection on data streams", "author": ["Hao Huang", "Shinjae Yoo", "S Kasiviswanathan"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Learning with structured sparsity", "author": ["Junzhou Huang", "Tong Zhang", "Dimitris Metaxas"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Huang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2011}, {"title": "Group lasso with overlap and graph lasso", "author": ["Laurent Jacob", "Guillaume Obozinski", "Jean-Philippe Vert"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Jacob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2009}, {"title": "Machine learning based on attribute interactions", "author": ["Aleks Jakulin"], "venue": "PhD thesis, Univerza v Ljubljani,", "citeRegEx": "Jakulin.,? \\Q2005\\E", "shortCiteRegEx": "Jakulin.", "year": 2005}, {"title": "Dasso: connections between the dantzig selector and lasso", "author": ["Gareth M James", "Peter Radchenko", "Jinchi Lv"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "James et al\\.,? \\Q2009\\E", "shortCiteRegEx": "James et al\\.", "year": 2009}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["Rodolphe Jenatton", "Julien Mairal", "Francis R Bach", "Guillaume R Obozinski"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Jenatton et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2010}, {"title": "Structured variable selection with sparsity-inducing norms", "author": ["Rodolphe Jenatton", "Jean-Yves Audibert", "Francis Bach"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Jenatton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2011}, {"title": "Principal component analysis", "author": ["Ian Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "Jolliffe.,? \\Q2002\\E", "shortCiteRegEx": "Jolliffe.", "year": 2002}, {"title": "Stability of feature selection algorithms: a study on high-dimensional spaces", "author": ["Alexandros Kalousis", "Julien Prados", "Melanie Hilario"], "venue": "Knowledge and information systems,", "citeRegEx": "Kalousis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kalousis et al\\.", "year": 2007}, {"title": "Statistical estimation of correlated genome associations to a quantitative trait network", "author": ["Seyoung Kim", "Eric P Xing"], "venue": "PLoS Genet,", "citeRegEx": "Kim and Xing.,? \\Q2009\\E", "shortCiteRegEx": "Kim and Xing.", "year": 2009}, {"title": "Tree-guided group lasso for multi-task regression with structured sparsity", "author": ["Seyoung Kim", "Eric P Xing"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Kim and Xing.,? \\Q2010\\E", "shortCiteRegEx": "Kim and Xing.", "year": 2010}, {"title": "The feature selection problem: Traditional methods and a new algorithm", "author": ["Kenji Kira", "Larry A Rendell"], "venue": "In AAAI,", "citeRegEx": "Kira and Rendell.,? \\Q1992\\E", "shortCiteRegEx": "Kira and Rendell.", "year": 1992}, {"title": "A practical approach to feature selection", "author": ["Kenji Kira", "Larry A Rendell"], "venue": "In Proceedings of the ninth international workshop on Machine learning,", "citeRegEx": "Kira and Rendell.,? \\Q1992\\E", "shortCiteRegEx": "Kira and Rendell.", "year": 1992}, {"title": "Wrappers for feature subset selection", "author": ["Ron Kohavi", "George H John"], "venue": "Artificial intelligence,", "citeRegEx": "Kohavi and John.,? \\Q1997\\E", "shortCiteRegEx": "Kohavi and John.", "year": 1997}, {"title": "Toward optimal feature selection", "author": ["Daphne Koller", "Mehran Sahami"], "venue": "In In 13th International Conference on Machine Learning,", "citeRegEx": "Koller and Sahami.,? \\Q1995\\E", "shortCiteRegEx": "Koller and Sahami.", "year": 1995}, {"title": "Discretization techniques: A recent survey", "author": ["Sotiris Kotsiantis", "Dimitris Kanellopoulos"], "venue": "GESTS International Transactions on Computer Science and Engineering,", "citeRegEx": "Kotsiantis and Kanellopoulos.,? \\Q2006\\E", "shortCiteRegEx": "Kotsiantis and Kanellopoulos.", "year": 2006}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Gert RG Lanckriet", "Nello Cristianini", "Peter Bartlett", "Laurent El Ghaoui", "Michael I Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Feature selection and feature extraction for text categorization", "author": ["David D Lewis"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "Lewis.,? \\Q1992\\E", "shortCiteRegEx": "Lewis.", "year": 1992}, {"title": "Group feature selection with streaming features", "author": ["Haiguang Li", "Xindong Wu", "Zhao Li", "Wei Ding"], "venue": "In Data Mining (ICDM),", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Unsupervised streaming feature selection in social media", "author": ["Jundong Li", "Xia Hu", "Jiliang Tang", "Huan Liu"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Unsupervised feature selection using nonnegative spectral analysis", "author": ["Zechao Li", "Yi Yang", "Jing Liu", "Xiaofang Zhou", "Hanqing Lu"], "venue": "In AAAI,", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Simple and deterministic matrix sketching", "author": ["Edo Liberty"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Liberty.,? \\Q2013\\E", "shortCiteRegEx": "Liberty.", "year": 2013}, {"title": "Conditional infomax learning: an integrated framework for feature extraction and fusion", "author": ["Dahua Lin", "Xiaoou Tang"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Lin and Tang.,? \\Q2006\\E", "shortCiteRegEx": "Lin and Tang.", "year": 2006}, {"title": "Chi2: Feature selection and discretization of numeric attributes", "author": ["Huan Liu", "Rudy Setiono"], "venue": "In tai,", "citeRegEx": "Liu and Setiono.,? \\Q1995\\E", "shortCiteRegEx": "Liu and Setiono.", "year": 1995}, {"title": "SLEP: Sparse Learning with Efficient Projections", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Arizona State University,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Moreau-yosida regularization for grouped tree structure learning", "author": ["Jun Liu", "Jieping Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Liu and Ye.,? \\Q2010\\E", "shortCiteRegEx": "Liu and Ye.", "year": 2010}, {"title": "Multi-task feature learning via efficient l 2, 1-norm minimization", "author": ["Jun Liu", "Shuiwang Ji", "Jieping Ye"], "venue": "In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Global and local structure preservation for feature selection", "author": ["Xinwang Liu", "Lei Wang", "Jian Zhang", "Jianping Yin", "Huan Liu"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Sparse logistic regression with lp penalty for biomarker identification", "author": ["Zhenqiu Liu", "Feng Jiang", "Guoliang Tian", "Suna Wang", "Fumiaki Sato", "Stephen J Meltzer", "Ming Tan"], "venue": "Statistical Applications in Genetics and Molecular Biology,", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Spectral clustering for multi-type relational data", "author": ["Bo Long", "Zhongfei Mark Zhang", "Xiaoyun Wu", "Philip S Yu"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Long et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Long et al\\.", "year": 2006}, {"title": "A probabilistic framework for relational clustering", "author": ["Bo Long", "Zhongfei Mark Zhang", "Philip S Yu"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Long et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Long et al\\.", "year": 2007}, {"title": "Supervised group lasso with applications to microarray data analysis", "author": ["Shuangge Ma", "Xiao Song", "Jian Huang"], "venue": "BMC bioinformatics,", "citeRegEx": "Ma et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2007}, {"title": "Classification in networked data: A toolkit and a univariate case study", "author": ["Sofus A Macskassy", "Foster Provost"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Macskassy and Provost.,? \\Q2007\\E", "shortCiteRegEx": "Macskassy and Provost.", "year": 2007}, {"title": "Network studies of social influence", "author": ["Peter V Marsden", "Noah E Friedkin"], "venue": "Sociological Methods & Research,", "citeRegEx": "Marsden and Friedkin.,? \\Q1993\\E", "shortCiteRegEx": "Marsden and Friedkin.", "year": 1993}, {"title": "Convex principal feature selection", "author": ["Mahdokht Masaeli", "Yan Yan", "Ying Cui", "Glenn Fung", "Jennifer G Dy"], "venue": "In SDM,", "citeRegEx": "Masaeli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Masaeli et al\\.", "year": 2010}, {"title": "Subband correlation and robust speech recognition", "author": ["James McAuley", "Ji Ming", "Darryl Stewart", "Philip Hanna"], "venue": "Speech and Audio Processing, IEEE Transactions on,", "citeRegEx": "McAuley et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2005}, {"title": "Birds of a feather: Homophily in social networks", "author": ["Miller McPherson", "Lynn Smith-Lovin", "James M Cook"], "venue": "Annual review of sociology,", "citeRegEx": "McPherson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McPherson et al\\.", "year": 2001}, {"title": "The group lasso for logistic regression", "author": ["Lukas Meier", "Sara Van De Geer", "Peter B\u00fchlmann"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Meier et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Meier et al\\.", "year": 2008}, {"title": "On the use of variable complementarity for feature selection in cancer classification", "author": ["Patrick E Meyer", "Gianluca Bontempi"], "venue": "In Applications of Evolutionary Computing,", "citeRegEx": "Meyer and Bontempi.,? \\Q2006\\E", "shortCiteRegEx": "Meyer and Bontempi.", "year": 2006}, {"title": "Information-theoretic feature selection in microarray data using variable complementarity", "author": ["Patrick Emmanuel Meyer", "Colas Schretter", "Gianluca Bontempi"], "venue": "Selected Topics in Signal Processing, IEEE Journal of,", "citeRegEx": "Meyer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Meyer et al\\.", "year": 2008}, {"title": "Unsupervised feature selection using feature similarity", "author": ["Pabitra Mitra", "CA Murthy", "Sankar K. Pal"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Mitra et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Mitra et al\\.", "year": 2002}, {"title": "Manifestation of emerging specialties in journal literature: A growth model of papers, references, exemplars, bibliographic coupling, cocitation, and clustering coefficient distribution", "author": ["Steven A Morris"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Morris.,? \\Q2005\\E", "shortCiteRegEx": "Morris.", "year": 2005}, {"title": "A branch and bound algorithm for feature subset selection", "author": ["Patrenahalli M Narendra", "Keinosuke Fukunaga"], "venue": "Computers, IEEE Transactions on,", "citeRegEx": "Narendra and Fukunaga.,? \\Q1977\\E", "shortCiteRegEx": "Narendra and Fukunaga.", "year": 1977}, {"title": "Introductory lectures on convex optimization, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}, {"title": "Finding and evaluating community structure in networks", "author": ["Mark EJ Newman", "Michelle Girvan"], "venue": "Physical review E,", "citeRegEx": "Newman and Girvan.,? \\Q2004\\E", "shortCiteRegEx": "Newman and Girvan.", "year": 2004}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y Ng", "Michael I Jordan", "Yair Weiss"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Ng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2002}, {"title": "Trace ratio criterion for feature selection", "author": ["Feiping Nie", "Shiming Xiang", "Yangqing Jia", "Changshui Zhang", "Shuicheng Yan"], "venue": "In AAAI,", "citeRegEx": "Nie et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2008}, {"title": "Efficient and robust feature selection via joint 2, 1-norms minimization", "author": ["Feiping Nie", "Heng Huang", "Xiao Cai", "Chris H Ding"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Nie et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2010}, {"title": "Joint covariate selection for grouped classification", "author": ["Guillaume Obozinski", "Ben Taskar", "Michael Jordan"], "venue": "Technical report,", "citeRegEx": "Obozinski et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Obozinski et al\\.", "year": 2007}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Peng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer", "author": ["Jie Peng", "Ji Zhu", "Anna Bergamaschi", "Wonshik Han", "Dong-Young Noh", "Jonathan R Pollack", "Pei Wang"], "venue": "The annals of applied statistics,", "citeRegEx": "Peng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2010}, {"title": "Online feature selection using grafting", "author": ["Simon Perkins", "James Theiler"], "venue": "In ICML, pages 592\u2013599,", "citeRegEx": "Perkins and Theiler.,? \\Q2003\\E", "shortCiteRegEx": "Perkins and Theiler.", "year": 2003}, {"title": "Grafting: Fast, incremental feature selection by gradient descent in function space", "author": ["Simon Perkins", "Kevin Lacker", "James Theiler"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Perkins et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Perkins et al\\.", "year": 2003}, {"title": "Robust unsupervised feature selection", "author": ["Mingjie Qian", "Chengxiang Zhai"], "venue": "In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,", "citeRegEx": "Qian and Zhai.,? \\Q2013\\E", "shortCiteRegEx": "Qian and Zhai.", "year": 2013}, {"title": "Induction of decision trees", "author": ["J. Ross Quinlan"], "venue": "Machine learning,", "citeRegEx": "Quinlan.,? \\Q1986\\E", "shortCiteRegEx": "Quinlan.", "year": 1986}, {"title": "5: programs for machine learning", "author": ["J Ross Quinlan. C"], "venue": null, "citeRegEx": "C4.,? \\Q1993\\E", "shortCiteRegEx": "C4.", "year": 1993}, {"title": "Theoretical and empirical analysis of relieff and rrelieff", "author": ["Marko Robnik-\u0160ikonja", "Igor Kononenko"], "venue": "Machine learning,", "citeRegEx": "Robnik.\u0160ikonja and Kononenko.,? \\Q2003\\E", "shortCiteRegEx": "Robnik.\u0160ikonja and Kononenko.", "year": 2003}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T Roweis", "Lawrence K Saul"], "venue": null, "citeRegEx": "Roweis and Saul.,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul.", "year": 2000}, {"title": "Regularized learning with networks of features", "author": ["Ted Sandler", "John Blitzer", "Partha P Talukdar", "Lyle H Ungar"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sandler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sandler et al\\.", "year": 2009}, {"title": "Mullert. Fisher discriminant analysis with kernels", "author": ["Bernhard Scholkopft", "Klaus-Robert"], "venue": "Neural networks for signal processing IX,", "citeRegEx": "Scholkopft and Klaus.Robert,? \\Q1999\\E", "shortCiteRegEx": "Scholkopft and Klaus.Robert", "year": 1999}, {"title": "Regression approaches for microarray data analysis", "author": ["Mark R Segal", "Kam D Dahlquist", "Bruce R Conklin"], "venue": "Journal of Computational Biology,", "citeRegEx": "Segal et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Segal et al\\.", "year": 2003}, {"title": "Collective classification in network data", "author": ["Prithviraj Sen", "Galileo Namata", "Mustafa Bilgic", "Lise Getoor", "Brian Galligher", "Tina Eliassi-Rad"], "venue": "AI magazine,", "citeRegEx": "Sen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sen et al\\.", "year": 2008}, {"title": "A mathematical theory of communication", "author": ["Claude Elwood Shannon"], "venue": "ACM SIGMOBILE Mobile Computing and Communications Review,", "citeRegEx": "Shannon.,? \\Q2001\\E", "shortCiteRegEx": "Shannon.", "year": 2001}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Shi and Malik.,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik.", "year": 2000}, {"title": "Parallel large scale feature selection for logistic regression", "author": ["Sameer Singh", "Jeremy Kubica", "Scott Larsen", "Daria Sorokina"], "venue": "In SDM,", "citeRegEx": "Singh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2009}, {"title": "Local fisher discriminant analysis for supervised dimensionality reduction", "author": ["Masashi Sugiyama"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Sugiyama.,? \\Q2006\\E", "shortCiteRegEx": "Sugiyama.", "year": 2006}, {"title": "Towards ultrahigh dimensional feature selection for big data", "author": ["Mingkui Tan", "Ivor W Tsang", "Li Wang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Tan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2014}, {"title": "Feature selection with linked data in social media", "author": ["Jiliang Tang", "Huan Liu"], "venue": "In SDM, pages 118\u2013128", "citeRegEx": "Tang and Liu.,? \\Q2012\\E", "shortCiteRegEx": "Tang and Liu.", "year": 2012}, {"title": "Unsupervised feature selection for linked social media data", "author": ["Jiliang Tang", "Huan Liu"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Tang and Liu.,? \\Q2012\\E", "shortCiteRegEx": "Tang and Liu.", "year": 2012}, {"title": "Feature selection for social media data", "author": ["Jiliang Tang", "Huan Liu"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),", "citeRegEx": "Tang and Liu.,? \\Q2014\\E", "shortCiteRegEx": "Tang and Liu.", "year": 2014}, {"title": "An unsupervised feature selection framework for social media data", "author": ["Jiliang Tang", "Huan Liu"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "Tang and Liu.,? \\Q2014\\E", "shortCiteRegEx": "Tang and Liu.", "year": 2014}, {"title": "Unsupervised feature selection for multiview data in social media", "author": ["Jiliang Tang", "Xia Hu", "Huiji Gao", "Huan Liu"], "venue": "In SDM,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "Feature selection for classification: A review", "author": ["Jiliang Tang", "Salem Alelyani", "Huan Liu"], "venue": "Data Classification: Algorithms and Applications,", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B Tenenbaum", "Vin De Silva", "John C Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Estimating the number of clusters in a data set via the gap statistic", "author": ["Robert Tibshirani", "Guenther Walther", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Tibshirani et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2001}, {"title": "Sparsity and smoothness via the fused lasso", "author": ["Robert Tibshirani", "Michael Saunders", "Saharon Rosset", "Ji Zhu", "Keith Knight"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Tibshirani et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2005}, {"title": "Numerical recipes: example book (C)", "author": ["William T Vetterling", "Saul A Teukolsky", "William H Press"], "venue": "Press Syndicate of the University of Cambridge,", "citeRegEx": "Vetterling et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Vetterling et al\\.", "year": 1992}, {"title": "Object recognition with informative features and linear classification", "author": ["Michel Vidal-Naquet", "Shimon Ullman"], "venue": "In ICCV,", "citeRegEx": "Vidal.Naquet and Ullman.,? \\Q2003\\E", "shortCiteRegEx": "Vidal.Naquet and Ullman.", "year": 2003}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike Von Luxburg"], "venue": "Statistics and computing,", "citeRegEx": "Luxburg.,? \\Q2007\\E", "shortCiteRegEx": "Luxburg.", "year": 2007}, {"title": "Multi-view clustering and feature learning via structured sparsity", "author": ["Hua Wang", "Feiping Nie", "Heng Huang"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Online feature selection and its applications", "author": ["Jialei Wang", "Peilin Zhao", "Steven CH Hoi", "Rong Jin"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Online group feature selection", "author": ["Jing Wang", "Zhong-Qiu Zhao", "Xuegang Hu", "Yiu-Ming Cheung", "Meng Wang", "Xindong Wu"], "venue": "In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Online feature selection with group structure analysis", "author": ["Jing Wang", "Meng Wang", "Peipei Li", "Luoqi Liu", "Zhongqiu Zhao", "Xuegang Hu", "Xindong Wu"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "The interpretation of population structure by f-statistics with special regard to systems of mating", "author": ["Sewall Wright"], "venue": "Evolution, pages 395\u2013420,", "citeRegEx": "Wright.,? \\Q1965\\E", "shortCiteRegEx": "Wright.", "year": 1965}, {"title": "Online streaming feature selection", "author": ["Xindong Wu", "Kui Yu", "Hao Wang", "Wei Ding"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Online feature selection with streaming features", "author": ["Xindong Wu", "Kui Yu", "Wei Ding", "Hao Wang", "Xingquan Zhu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Wu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2013}, {"title": "N3lars: Minimum redundancy maximum relevance feature selection for large and high-dimensional data", "author": ["Makoto Yamada", "Avishek Saha", "Hua Ouyang", "Dawei Yin", "Yi Chang"], "venue": "arXiv preprint arXiv:1411.2331,", "citeRegEx": "Yamada et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2014}, {"title": "Data visualization and feature selection: New algorithms for nongaussian data", "author": ["Howard Hua Yang", "John E Moody"], "venue": "In NIPS,", "citeRegEx": "Yang and Moody.,? \\Q1999\\E", "shortCiteRegEx": "Yang and Moody.", "year": 1999}, {"title": "Feature grouping and selection over an undirected graph", "author": ["Sen Yang", "Lei Yuan", "Ying-Cheng Lai", "Xiaotong Shen", "Peter Wonka", "Jieping Ye"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "Image clustering using local discriminant models and global integration", "author": ["Yi Yang", "Dong Xu", "Feiping Nie", "Shuicheng Yan", "Yueting Zhuang"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "Yang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "1-norm regularized discriminative feature selection for unsupervised learning", "author": ["Yi Yang", "Heng Tao Shen", "Zhigang Ma", "Zi Huang", "Xiaofang Zhou"], "venue": "In IJCAI ProceedingsInternational Joint Conference on Artificial Intelligence,", "citeRegEx": "Yang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "Sparse methods for biomedical data", "author": ["Jieping Ye", "Jun Liu"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Ye and Liu.,? \\Q2012\\E", "shortCiteRegEx": "Ye and Liu.", "year": 2012}, {"title": "Feature selection for high-dimensional data: A fast correlation-based filter solution", "author": ["Lei Yu", "Huan Liu"], "venue": "In ICML,", "citeRegEx": "Yu and Liu.,? \\Q2003\\E", "shortCiteRegEx": "Yu and Liu.", "year": 2003}, {"title": "Multiclass spectral clustering", "author": ["Stella X Yu", "Jianbo Shi"], "venue": "In Computer Vision,", "citeRegEx": "Yu and Shi.,? \\Q2003\\E", "shortCiteRegEx": "Yu and Shi.", "year": 2003}, {"title": "Efficient methods for overlapping group lasso", "author": ["Lei Yuan", "Jun Liu", "Jieping Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yuan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2011}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["Ming Yuan", "Yi Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Yuan and Lin.,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin.", "year": 2006}, {"title": "Flexible latent variable models for multi-task learning", "author": ["Jian Zhang", "Zoubin Ghahramani", "Yiming Yang"], "venue": "Machine Learning,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Towards mining trapezoidal data streams", "author": ["Qin Zhang", "Peng Zhang", "Guodong Long", "Wei Ding", "Chengqi Zhang", "Xindong Wu"], "venue": "In Data Mining (ICDM),", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "On model selection consistency of lasso", "author": ["Peng Zhao", "Bin Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Zhao and Yu.,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu.", "year": 2006}, {"title": "The composite absolute penalties family for grouped and hierarchical variable selection", "author": ["Peng Zhao", "Guilherme Rocha", "Bin Yu"], "venue": "The Annals of Statistics,", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}, {"title": "Spectral feature selection for supervised and unsupervised learning", "author": ["Zheng Zhao", "Huan Liu"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Zhao and Liu.,? \\Q2007\\E", "shortCiteRegEx": "Zhao and Liu.", "year": 2007}, {"title": "Multi-source feature selection via geometry-dependent covariance analysis", "author": ["Zheng Zhao", "Huan Liu"], "venue": "In FSDM,", "citeRegEx": "Zhao and Liu.,? \\Q2008\\E", "shortCiteRegEx": "Zhao and Liu.", "year": 2008}, {"title": "Massively parallel feature selection: an approach based on variance preservation", "author": ["Zheng Zhao", "Ruiwen Zhang", "James Cox", "David Duling", "Warren Sarle"], "venue": "Machine learning,", "citeRegEx": "Zhao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}, {"title": "Learning from labeled and unlabeled data on a directed graph", "author": ["Dengyong Zhou", "Jiayuan Huang", "Bernhard Sch\u00f6lkopf"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}, {"title": "Modeling disease progression via fused sparse group lasso", "author": ["Jiayu Zhou", "Jun Liu", "Vaibhav A Narayan", "Jieping Ye"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}, {"title": "Streaming feature selection using alpha-investing", "author": ["Jing Zhou", "Dean Foster", "Robert Stine", "Lyle Ungar"], "venue": "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}, {"title": "The adaptive lasso and its oracle properties", "author": ["Hui Zou"], "venue": "Journal of the American statistical association,", "citeRegEx": "Zou.,? \\Q2006\\E", "shortCiteRegEx": "Zou.", "year": 2006}, {"title": "Regularization and variable selection via the elastic net", "author": ["Hui Zou", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Zou and Hastie.,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie.", "year": 2005}], "referenceMentions": [{"referenceID": 42, "context": "When applying data mining and machine learning algorithms on high dimensional data, a critical issue is known as curse of dimensionality (Hastie et al., 2005).", "startOffset": 137, "endOffset": 158}, {"referenceID": 56, "context": "Examples of feature extraction methods include Principle Component Analysis (PCA) (Jolliffe, 2002), Linear Discriminant Analysis (LDA) (Scholkopft and Mullert, 1999), Canonical Correlation Analysis (CCA) (Hardoon et al.", "startOffset": 82, "endOffset": 98}, {"referenceID": 41, "context": "Examples of feature extraction methods include Principle Component Analysis (PCA) (Jolliffe, 2002), Linear Discriminant Analysis (LDA) (Scholkopft and Mullert, 1999), Canonical Correlation Analysis (CCA) (Hardoon et al., 2004), Singular Value Decomposition (Golub and Van Loan, 2012), ISOMAP (Tenenbaum et al.", "startOffset": 204, "endOffset": 226}, {"referenceID": 123, "context": ", 2004), Singular Value Decomposition (Golub and Van Loan, 2012), ISOMAP (Tenenbaum et al., 2000) and Locally Linear Embedding (LLE) (Roweis and Saul, 2000).", "startOffset": 73, "endOffset": 97}, {"referenceID": 107, "context": ", 2000) and Locally Linear Embedding (LLE) (Roweis and Saul, 2000).", "startOffset": 43, "endOffset": 66}, {"referenceID": 124, "context": "Lasso (Tibshirani, 1996), Information Gain (Cover and Thomas, 2012), Relief (Kira and Rendell, 1992a), MRMR (Peng et al.", "startOffset": 6, "endOffset": 24}, {"referenceID": 14, "context": "Lasso (Tibshirani, 1996), Information Gain (Cover and Thomas, 2012), Relief (Kira and Rendell, 1992a), MRMR (Peng et al.", "startOffset": 43, "endOffset": 67}, {"referenceID": 99, "context": "Lasso (Tibshirani, 1996), Information Gain (Cover and Thomas, 2012), Relief (Kira and Rendell, 1992a), MRMR (Peng et al., 2005), Fisher Score (Duda et al.", "startOffset": 108, "endOffset": 127}, {"referenceID": 21, "context": ", 2005), Fisher Score (Duda et al., 2012), Laplacian Score (He et al.", "startOffset": 22, "endOffset": 41}, {"referenceID": 44, "context": ", 2012), Laplacian Score (He et al., 2005), and SPEC (Zhao and Liu, 2007) are some of the well known feature selection techniques.", "startOffset": 25, "endOffset": 42}, {"referenceID": 151, "context": ", 2005), and SPEC (Zhao and Liu, 2007) are some of the well known feature selection techniques.", "startOffset": 18, "endOffset": 38}, {"referenceID": 37, "context": "Therefore, many different search strategies such as sequential search (Guyon and Elisseeff, 2003), hill-climbing search, bestfirst search (Kohavi and John, 1997), branch-and-bound search (Narendra and Fukunaga, 1977), genetic algorithms (Golberg, 1989) are proposed to yield a local optimum learning performance.", "startOffset": 70, "endOffset": 97}, {"referenceID": 62, "context": "Therefore, many different search strategies such as sequential search (Guyon and Elisseeff, 2003), hill-climbing search, bestfirst search (Kohavi and John, 1997), branch-and-bound search (Narendra and Fukunaga, 1977), genetic algorithms (Golberg, 1989) are proposed to yield a local optimum learning performance.", "startOffset": 138, "endOffset": 161}, {"referenceID": 91, "context": "Therefore, many different search strategies such as sequential search (Guyon and Elisseeff, 2003), hill-climbing search, bestfirst search (Kohavi and John, 1997), branch-and-bound search (Narendra and Fukunaga, 1977), genetic algorithms (Golberg, 1989) are proposed to yield a local optimum learning performance.", "startOffset": 187, "endOffset": 216}, {"referenceID": 33, "context": "Therefore, many different search strategies such as sequential search (Guyon and Elisseeff, 2003), hill-climbing search, bestfirst search (Kohavi and John, 1997), branch-and-bound search (Narendra and Fukunaga, 1977), genetic algorithms (Golberg, 1989) are proposed to yield a local optimum learning performance.", "startOffset": 237, "endOffset": 252}, {"referenceID": 106, "context": "Some representative criteria include feature discriminative ability to separate samples (Kira and Rendell, 1992b; Robnik-\u0160ikonja and Kononenko, 2003), feature correlation (Koller and Sahami, 1995; Guyon and Elisseeff, 2003), mutual information (Yu and Liu, 2003; Peng et al.", "startOffset": 88, "endOffset": 149}, {"referenceID": 63, "context": "Some representative criteria include feature discriminative ability to separate samples (Kira and Rendell, 1992b; Robnik-\u0160ikonja and Kononenko, 2003), feature correlation (Koller and Sahami, 1995; Guyon and Elisseeff, 2003), mutual information (Yu and Liu, 2003; Peng et al.", "startOffset": 171, "endOffset": 223}, {"referenceID": 37, "context": "Some representative criteria include feature discriminative ability to separate samples (Kira and Rendell, 1992b; Robnik-\u0160ikonja and Kononenko, 2003), feature correlation (Koller and Sahami, 1995; Guyon and Elisseeff, 2003), mutual information (Yu and Liu, 2003; Peng et al.", "startOffset": 171, "endOffset": 223}, {"referenceID": 143, "context": "Some representative criteria include feature discriminative ability to separate samples (Kira and Rendell, 1992b; Robnik-\u0160ikonja and Kononenko, 2003), feature correlation (Koller and Sahami, 1995; Guyon and Elisseeff, 2003), mutual information (Yu and Liu, 2003; Peng et al., 2005), feature ability to preserve data manifold structure (He et al.", "startOffset": 244, "endOffset": 281}, {"referenceID": 99, "context": "Some representative criteria include feature discriminative ability to separate samples (Kira and Rendell, 1992b; Robnik-\u0160ikonja and Kononenko, 2003), feature correlation (Koller and Sahami, 1995; Guyon and Elisseeff, 2003), mutual information (Yu and Liu, 2003; Peng et al., 2005), feature ability to preserve data manifold structure (He et al.", "startOffset": 244, "endOffset": 281}, {"referenceID": 44, "context": ", 2005), feature ability to preserve data manifold structure (He et al., 2005; Gu et al., 2011; Zhao and Liu, 2007), and feature ability to reconstruct the original data (Masaeli et al.", "startOffset": 61, "endOffset": 115}, {"referenceID": 35, "context": ", 2005), feature ability to preserve data manifold structure (He et al., 2005; Gu et al., 2011; Zhao and Liu, 2007), and feature ability to reconstruct the original data (Masaeli et al.", "startOffset": 61, "endOffset": 115}, {"referenceID": 151, "context": ", 2005), feature ability to preserve data manifold structure (He et al., 2005; Gu et al., 2011; Zhao and Liu, 2007), and feature ability to reconstruct the original data (Masaeli et al.", "startOffset": 61, "endOffset": 115}, {"referenceID": 83, "context": ", 2011; Zhao and Liu, 2007), and feature ability to reconstruct the original data (Masaeli et al., 2010; Farahat et al., 2011).", "startOffset": 82, "endOffset": 126}, {"referenceID": 27, "context": ", 2011; Zhao and Liu, 2007), and feature ability to reconstruct the original data (Masaeli et al., 2010; Farahat et al., 2011).", "startOffset": 82, "endOffset": 126}, {"referenceID": 37, "context": "Currently, there exist a number of feature selection surveys (Guyon and Elisseeff, 2003; Alelyani et al., 2013; Chandrashekar and Sahin, 2014; Tang et al., 2014).", "startOffset": 61, "endOffset": 161}, {"referenceID": 2, "context": "Currently, there exist a number of feature selection surveys (Guyon and Elisseeff, 2003; Alelyani et al., 2013; Chandrashekar and Sahin, 2014; Tang et al., 2014).", "startOffset": 61, "endOffset": 161}, {"referenceID": 12, "context": "Currently, there exist a number of feature selection surveys (Guyon and Elisseeff, 2003; Alelyani et al., 2013; Chandrashekar and Sahin, 2014; Tang et al., 2014).", "startOffset": 61, "endOffset": 161}, {"referenceID": 122, "context": "Currently, there exist a number of feature selection surveys (Guyon and Elisseeff, 2003; Alelyani et al., 2013; Chandrashekar and Sahin, 2014; Tang et al., 2014).", "startOffset": 61, "endOffset": 161}, {"referenceID": 44, "context": "1 Laplacian Score (He et al., 2005) (Unsupervised) Laplacian Score is an unsupervised feature selection algorithm which selects features that can best preserve the data manifold structure.", "startOffset": 18, "endOffset": 35}, {"referenceID": 13, "context": "The diagonal matrix D is defined as D(i, i) = \u2211n j=1 S(i, j) and the laplacian matrix L is L = D \u2212 S (Chung, 1997).", "startOffset": 101, "endOffset": 114}, {"referenceID": 151, "context": "2 SPEC (Zhao and Liu, 2007) (Unsupervised and Supervised) SPEC is an extension of Laplacian Score that work for both supervised and unsupervised scenarios.", "startOffset": 7, "endOffset": 27}, {"referenceID": 13, "context": "Afterwards the construction of affinity matrix S, the diagonal matrix D is defined as D(i, i) = \u2211n j=1 S(i, j) and the normalized laplacian matrix Lnorm is Lnorm = D \u2212 1 2 (D \u2212 S)D 12 (Chung, 1997).", "startOffset": 184, "endOffset": 197}, {"referenceID": 21, "context": "3 Fisher Score (Duda et al., 2012) (Supervised) Fisher Score is a supervised feature selection algorithm.", "startOffset": 15, "endOffset": 34}, {"referenceID": 44, "context": "According to (He et al., 2005), Fisher Score can be considered as a special case of Laplacian Score as long as the affinity matrix is as follows:", "startOffset": 13, "endOffset": 30}, {"referenceID": 35, "context": "To tackle this issue, a Generalized Fisher Score method Gu et al. (2011) is proposed to jointly select features.", "startOffset": 56, "endOffset": 73}, {"referenceID": 95, "context": "4 Trace Ratio Criterion (Nie et al., 2008) (Supervised) Recently, the trace ratio criterion has been proposed to directly select the global optimal feature subset based on the corresponding score, which is computed in a trace ratio norm.", "startOffset": 24, "endOffset": 42}, {"referenceID": 106, "context": "5 ReliefF (Robnik-\u0160ikonja and Kononenko, 2003) (Supervised) Relief and its multi-class variant ReliefF are supervised filter algorithms that select features to separate instances from different classes.", "startOffset": 10, "endOffset": 46}, {"referenceID": 151, "context": "Then according to (Zhao and Liu, 2007), the criterion of ReliefF is equivalent to the following with above assumptions:", "startOffset": 18, "endOffset": 38}, {"referenceID": 21, "context": "As indicated in (Duda et al., 2012), many handdesigned information theoretic criteria are proposed to maximize feature relevance and minimize feature redundancy.", "startOffset": 16, "endOffset": 35}, {"referenceID": 19, "context": "For numeric feature values, some data discretization techniques (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006) are required.", "startOffset": 64, "endOffset": 124}, {"referenceID": 64, "context": "For numeric feature values, some data discretization techniques (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006) are required.", "startOffset": 64, "endOffset": 124}, {"referenceID": 8, "context": "Two decades of research on information theoretic criteria can be unified in a conditional likelihood maximization framework, and most algorithms can be reduced to be a specific case of the unified framework (Brown et al., 2012) .", "startOffset": 207, "endOffset": 227}, {"referenceID": 112, "context": "The concept of information gain (Shannon, 2001) between X and Y is used to measure their dependency with entropy and conditional entropy.", "startOffset": 32, "endOffset": 47}, {"referenceID": 66, "context": "1 Mutual Information Maximization (or Information Gain) (Lewis, 1992) Mutual Information Maximization (MIM)(also known as Information Gain) measures the importance of a feature by its correlation with the class label.", "startOffset": 56, "endOffset": 69}, {"referenceID": 4, "context": "2 Mutual Information Feature Selection (Battiti, 1994) A limitation of MIM feature selection criterion is that it assumes that features are independent of each other.", "startOffset": 39, "endOffset": 54}, {"referenceID": 99, "context": "3 Minimum Redundancy Maximum Relevance (Peng et al., 2005) Unlike MIFS that empirically sets \u03b2 to be one, (Peng et al.", "startOffset": 39, "endOffset": 58}, {"referenceID": 99, "context": ", 2005) Unlike MIFS that empirically sets \u03b2 to be one, (Peng et al., 2005) proposed a Minimum Redundancy Maximum Relevance (MRMR) criterion to set the value of \u03b2 the reverse of the number of selected features: JMRMR(Xk) = I(Xk;Y )\u2212 1 |S| \u2211", "startOffset": 55, "endOffset": 74}, {"referenceID": 71, "context": "4 Conditional Infomax Feature Extraction (Lin and Tang, 2006) MIFS and MRMR consider both feature relevance and feature redundancy at the same time.", "startOffset": 41, "endOffset": 61}, {"referenceID": 69, "context": "4 Conditional Infomax Feature Extraction (Lin and Tang, 2006) MIFS and MRMR consider both feature relevance and feature redundancy at the same time. However, some studies Lin and Tang (2006); El Akadi et al.", "startOffset": 42, "endOffset": 191}, {"referenceID": 23, "context": "However, some studies Lin and Tang (2006); El Akadi et al. (2008); Guo and Nixon (2009) show that in contrast to minimize the feature redundancy, the conditional redundancy between unselected features and already selected features given class labels should be maximized.", "startOffset": 46, "endOffset": 66}, {"referenceID": 23, "context": "However, some studies Lin and Tang (2006); El Akadi et al. (2008); Guo and Nixon (2009) show that in contrast to minimize the feature redundancy, the conditional redundancy between unselected features and already selected features given class labels should be maximized.", "startOffset": 46, "endOffset": 88}, {"referenceID": 138, "context": "5 Joint Mutual Information (Yang and Moody, 1999) MIFS and MRMR reduce feature redundancy in the feature selection process.", "startOffset": 27, "endOffset": 49}, {"referenceID": 138, "context": "An alternative criterion, Joint Mutual Information (Yang and Moody, 1999; Meyer et al., 2008) was proposed to increase complementary information that are shared between new unselected feature and selected features given the class labels.", "startOffset": 51, "endOffset": 93}, {"referenceID": 88, "context": "An alternative criterion, Joint Mutual Information (Yang and Moody, 1999; Meyer et al., 2008) was proposed to increase complementary information that are shared between new unselected feature and selected features given the class labels.", "startOffset": 51, "endOffset": 93}, {"referenceID": 8, "context": "In (Brown et al., 2012), the authors demonstrate that with simple manipulations, the JMI criterion can be re-written as: JJMI(Xk) = I(Xk;Y )\u2212 1 |S| \u2211", "startOffset": 3, "endOffset": 23}, {"referenceID": 29, "context": "6 Conditional Mutual Information Maximization (Fleuret, 2004) Previously mentioned information theoretic feature selection criterion can be reduced to a linear combination of Shannon information terms.", "startOffset": 46, "endOffset": 61}, {"referenceID": 128, "context": "Among them, Conditional Mutual Information Maximization (CMIM) (Vidal-Naquet and Ullman, 2003; Fleuret, 2004) is a criterion which iteratively selects features which maximize the mutual information with the class labels given the selected features so far.", "startOffset": 63, "endOffset": 109}, {"referenceID": 29, "context": "Among them, Conditional Mutual Information Maximization (CMIM) (Vidal-Naquet and Ullman, 2003; Fleuret, 2004) is a criterion which iteratively selects features which maximize the mutual information with the class labels given the selected features so far.", "startOffset": 63, "endOffset": 109}, {"referenceID": 128, "context": "7 Informative Fragments (Vidal-Naquet and Ullman, 2003) In (Vidal-Naquet and Ullman, 2003), the authors propose a feature selection criterion called Informative Fragments (IG).", "startOffset": 24, "endOffset": 55}, {"referenceID": 128, "context": "7 Informative Fragments (Vidal-Naquet and Ullman, 2003) In (Vidal-Naquet and Ullman, 2003), the authors propose a feature selection criterion called Informative Fragments (IG).", "startOffset": 59, "endOffset": 90}, {"referenceID": 52, "context": "8 Interaction Capping (Jakulin, 2005) Interaction Capping is a similar feature selection criterion as CMIM in Eq.", "startOffset": 22, "endOffset": 37}, {"referenceID": 87, "context": "9 Double Input Symmetrical Relevance (Meyer and Bontempi, 2006) Another class of information theoretical based methods such as Double Input Symmetrical Relevance (DISR) (Meyer and Bontempi, 2006) exploits normalization techniques to normalize mutual information (Guyon et al.", "startOffset": 37, "endOffset": 63}, {"referenceID": 87, "context": "9 Double Input Symmetrical Relevance (Meyer and Bontempi, 2006) Another class of information theoretical based methods such as Double Input Symmetrical Relevance (DISR) (Meyer and Bontempi, 2006) exploits normalization techniques to normalize mutual information (Guyon et al.", "startOffset": 169, "endOffset": 195}, {"referenceID": 39, "context": "9 Double Input Symmetrical Relevance (Meyer and Bontempi, 2006) Another class of information theoretical based methods such as Double Input Symmetrical Relevance (DISR) (Meyer and Bontempi, 2006) exploits normalization techniques to normalize mutual information (Guyon et al., 2008):", "startOffset": 262, "endOffset": 282}, {"referenceID": 143, "context": "10 Fast Correlation Based Filter (Yu and Liu, 2003) There are other information theoretical based feature selection methods that can not be simply be reduced to the unified conditional likelihood maximization framework.", "startOffset": 33, "endOffset": 51}, {"referenceID": 143, "context": "Here, we introduce one algorithm named Fast Correlation Based Filter (FCBF) (Yu and Liu, 2003).", "startOffset": 76, "endOffset": 94}, {"referenceID": 38, "context": "At the very beginning, they use the whole set of features to train a learning model and then attempt to remove some features by setting the feature coefficients to zero, while maintaining the model performance, one example method in this category is recursive feature elimination methods using support vector machine (SVM) (Guyon et al., 2002).", "startOffset": 323, "endOffset": 343}, {"referenceID": 104, "context": "The second type of embedded methods contain a built-in feature selection mechanism such as ID3 (Quinlan, 1986) and C4.", "startOffset": 95, "endOffset": 110}, {"referenceID": 124, "context": "1 Feature Selection with l1-norm Regularizer (Supervised) (Tibshirani, 1996; Hastie et al., 2015) First, we consider the binary classification (yi is either 0 or 1) or regression problem with only one regression target.", "startOffset": 58, "endOffset": 97}, {"referenceID": 43, "context": "1 Feature Selection with l1-norm Regularizer (Supervised) (Tibshirani, 1996; Hastie et al., 2015) First, we consider the binary classification (yi is either 0 or 1) or regression problem with only one regression target.", "startOffset": 58, "endOffset": 97}, {"referenceID": 124, "context": "One main advantage of l1-norm regularization (Lasso) (Tibshirani, 1996; Hastie et al., 2015) is that it forces some feature coefficients to to become smaller and, in some cases, exactly zero.", "startOffset": 53, "endOffset": 92}, {"referenceID": 43, "context": "One main advantage of l1-norm regularization (Lasso) (Tibshirani, 1996; Hastie et al., 2015) is that it forces some feature coefficients to to become smaller and, in some cases, exactly zero.", "startOffset": 53, "endOffset": 92}, {"referenceID": 124, "context": "\u2022 Lasso Regularization (Tibshirani, 1996): Lasso is short for least absolute shrinkage and selection operator, it is based on the l1-norm regularization term on the feature coefficient w: penalty(w) = ||w||1 = d", "startOffset": 23, "endOffset": 41}, {"referenceID": 157, "context": "\u2022 Adaptive Lasso Regularization (Zou, 2006): The Lasso variable selection phase is consistent if it satisfies non-trivial solutions.", "startOffset": 32, "endOffset": 43}, {"referenceID": 149, "context": "However, this condition is difficult to satisfy in some scenarios (Zhao and Yu, 2006).", "startOffset": 66, "endOffset": 85}, {"referenceID": 25, "context": "Another critical issue of Lasso is that the lasso shrinkage produces biased estimates for the large coefficients, and thus it could be suboptimal in terms of estimation risk (Fan and Li, 2001).", "startOffset": 174, "endOffset": 192}, {"referenceID": 157, "context": "In (Zou, 2006), the authors show that the adaptive lasso enjoys the oracle properties and can be solved by the same efficient algorithm for solving the Lasso.", "startOffset": 3, "endOffset": 14}, {"referenceID": 158, "context": "\u2022 Elastic Net Regularization (Zou and Hastie, 2005): In Lasso, the number of selected features is usually bounded by the number of data instances, which is unrealistic in many applications.", "startOffset": 29, "endOffset": 51}, {"referenceID": 89, "context": "image processing and natural language processing (Mitra et al., 2002; Segal et al., 2003; Liu et al., 2007), it is common that features may have some strong correlations with each other.", "startOffset": 49, "endOffset": 107}, {"referenceID": 110, "context": "image processing and natural language processing (Mitra et al., 2002; Segal et al., 2003; Liu et al., 2007), it is common that features may have some strong correlations with each other.", "startOffset": 49, "endOffset": 107}, {"referenceID": 77, "context": "image processing and natural language processing (Mitra et al., 2002; Segal et al., 2003; Liu et al., 2007), it is common that features may have some strong correlations with each other.", "startOffset": 49, "endOffset": 107}, {"referenceID": 158, "context": "To handle features with high correlations, Elastic Net regularization (Zou and Hastie, 2005) is proposed as:", "startOffset": 70, "endOffset": 92}, {"referenceID": 10, "context": "In (Candes and Tao, 2007; James et al., 2009), the authors show that the errors of Dantzig selector is up to a logarithmic factor log(d) (d is the feature dimensionality).", "startOffset": 3, "endOffset": 45}, {"referenceID": 53, "context": "In (Candes and Tao, 2007; James et al., 2009), the authors show that the errors of Dantzig selector is up to a logarithmic factor log(d) (d is the feature dimensionality).", "startOffset": 3, "endOffset": 45}, {"referenceID": 53, "context": "Strong theoretical results in (James et al., 2009) show that LASSO and Dantzig selector are closely related.", "startOffset": 30, "endOffset": 50}, {"referenceID": 97, "context": "This problem can be solved by the l2,1-norm regularization which is widely applied in many applications (Obozinski et al., 2007; Evgeniou and Pontil, 2007; Bi et al., 2008; Zhang et al., 2008).", "startOffset": 104, "endOffset": 192}, {"referenceID": 24, "context": "This problem can be solved by the l2,1-norm regularization which is widely applied in many applications (Obozinski et al., 2007; Evgeniou and Pontil, 2007; Bi et al., 2008; Zhang et al., 2008).", "startOffset": 104, "endOffset": 192}, {"referenceID": 5, "context": "This problem can be solved by the l2,1-norm regularization which is widely applied in many applications (Obozinski et al., 2007; Evgeniou and Pontil, 2007; Bi et al., 2008; Zhang et al., 2008).", "startOffset": 104, "endOffset": 192}, {"referenceID": 147, "context": "This problem can be solved by the l2,1-norm regularization which is widely applied in many applications (Obozinski et al., 2007; Evgeniou and Pontil, 2007; Bi et al., 2008; Zhang et al., 2008).", "startOffset": 104, "endOffset": 192}, {"referenceID": 146, "context": "The l2,1-norm regularization has strong connections with group lasso (Yuan and Lin, 2006) which will be explained later.", "startOffset": 69, "endOffset": 89}, {"referenceID": 96, "context": "3 Efficient and Robust Feature Selection (Supervised) (Nie et al., 2010) In (Nie et al.", "startOffset": 54, "endOffset": 72}, {"referenceID": 96, "context": ", 2010) In (Nie et al., 2010), the authors propose an efficient and robust feature selection (REFS) method by employing a joint l2,1-norm minimization on both the loss function and the regularization.", "startOffset": 11, "endOffset": 29}, {"referenceID": 18, "context": "The reason is that l2,1norm loss function has a rotational invariant property (Ding et al., 2006).", "startOffset": 78, "endOffset": 97}, {"referenceID": 96, "context": "In (Nie et al., 2010), an efficient algorithm is proposed to solve this optimization problem with strict convergence analysis.", "startOffset": 3, "endOffset": 21}, {"referenceID": 9, "context": "4 Multi-Cluster Feature Selection (Unsupervised) (Cai et al., 2010) Most of existing sparse learning based approaches build a learning model with the supervision of class labels.", "startOffset": 49, "endOffset": 67}, {"referenceID": 9, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 141, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 47, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 69, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 103, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 76, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 20, "context": "However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years (Cai et al., 2010; Yang et al., 2011; Hou et al., 2011; Li et al., 2012; Qian and Zhai, 2013; Liu et al., 2014; Du and Shen, 2015).", "startOffset": 171, "endOffset": 301}, {"referenceID": 9, "context": "Multi-Cluster Feature Selection (MCFS) (Cai et al., 2010) is one of the first unsupervised feature selection algorithm using sparse learning techniques.", "startOffset": 39, "endOffset": 57}, {"referenceID": 11, "context": "In the first step, spectral clustering (Chan et al., 1994; Ng et al., 2002) is applied on the dataset to detect the cluster structure of the data.", "startOffset": 39, "endOffset": 75}, {"referenceID": 94, "context": "In the first step, spectral clustering (Chan et al., 1994; Ng et al., 2002) is applied on the dataset to detect the cluster structure of the data.", "startOffset": 39, "endOffset": 75}, {"referenceID": 141, "context": "5 l2,1-norm Regularized Discriminative Feature Selection (Unsupervised) (Yang et al., 2011) To perform unsupervised feature selection, one widely accepted criterion is to select features that best preserve the manifold structure of the data (He et al.", "startOffset": 72, "endOffset": 91}, {"referenceID": 44, "context": ", 2011) To perform unsupervised feature selection, one widely accepted criterion is to select features that best preserve the manifold structure of the data (He et al., 2005; Zhao and Liu, 2007; Cai et al., 2010).", "startOffset": 157, "endOffset": 212}, {"referenceID": 151, "context": ", 2011) To perform unsupervised feature selection, one widely accepted criterion is to select features that best preserve the manifold structure of the data (He et al., 2005; Zhao and Liu, 2007; Cai et al., 2010).", "startOffset": 157, "endOffset": 212}, {"referenceID": 9, "context": ", 2011) To perform unsupervised feature selection, one widely accepted criterion is to select features that best preserve the manifold structure of the data (He et al., 2005; Zhao and Liu, 2007; Cai et al., 2010).", "startOffset": 157, "endOffset": 212}, {"referenceID": 31, "context": "An alternative way is to exploit the discriminative information encoded in the data that has been proven to be effective in many learning tasks (Fukunaga, 2013).", "startOffset": 144, "endOffset": 160}, {"referenceID": 141, "context": "In (Yang et al., 2011), the authors propose a new unsupervised feature selection algorithm", "startOffset": 3, "endOffset": 22}, {"referenceID": 141, "context": "Instead of using global discriminative information, authors in (Yang et al., 2011) propose to utilize the local discriminative information (Sugiyama, 2006; Yang et al.", "startOffset": 63, "endOffset": 82}, {"referenceID": 115, "context": ", 2011) propose to utilize the local discriminative information (Sugiyama, 2006; Yang et al., 2010) to select discriminative features.", "startOffset": 64, "endOffset": 99}, {"referenceID": 140, "context": ", 2011) propose to utilize the local discriminative information (Sugiyama, 2006; Yang et al., 2010) to select discriminative features.", "startOffset": 64, "endOffset": 99}, {"referenceID": 140, "context": "Following the definition of global discriminative information (Yang et al., 2010; Fukunaga, 2013), the local discriminative score for each instance xi is computed as: DSi = tr[(S (i) t + \u03bbId) S (i) b ] = tr[WXP(i)X\u0303i \u2032 (X\u0303iX\u0303i \u2032 + \u03bbId) X\u0303iP \u2032 (i)XW], (70)", "startOffset": 62, "endOffset": 97}, {"referenceID": 31, "context": "Following the definition of global discriminative information (Yang et al., 2010; Fukunaga, 2013), the local discriminative score for each instance xi is computed as: DSi = tr[(S (i) t + \u03bbId) S (i) b ] = tr[WXP(i)X\u0303i \u2032 (X\u0303iX\u0303i \u2032 + \u03bbId) X\u0303iP \u2032 (i)XW], (70)", "startOffset": 62, "endOffset": 97}, {"referenceID": 69, "context": "6 Feature Selection Using Nonnegative Spectral Analysis (Unsupervised) (Li et al., 2012) In addition to UDFS, there are some other ways to exploit discriminative information for unsupervised feature selection.", "startOffset": 71, "endOffset": 88}, {"referenceID": 47, "context": "Nonnegative Discriminative Feature Selection (NDFS) (Hou et al., 2011) is an algorithm which performs spectral clustering and feature selection simultaneously in a joint framework to select a subset of discriminative features.", "startOffset": 52, "endOffset": 70}, {"referenceID": 113, "context": "NDFS adopts a strategy to learn the weight cluster matrix such that the local geometric structure of the data can be well preserved (Shi and Malik, 2000; Yu and Shi, 2003).", "startOffset": 132, "endOffset": 171}, {"referenceID": 144, "context": "NDFS adopts a strategy to learn the weight cluster matrix such that the local geometric structure of the data can be well preserved (Shi and Malik, 2000; Yu and Shi, 2003).", "startOffset": 132, "endOffset": 171}, {"referenceID": 47, "context": "7 Feature selection via joint embedding learning and sparse regression (Unsupervised) (Hou et al., 2011) Feature selection via joint embedding learning and sparse regression (JELSR) (Hou et al.", "startOffset": 86, "endOffset": 104}, {"referenceID": 47, "context": ", 2011) Feature selection via joint embedding learning and sparse regression (JELSR) (Hou et al., 2011) is an unsupervised feature selection that is similar to NDFS.", "startOffset": 85, "endOffset": 103}, {"referenceID": 107, "context": "In the second step, instead of using some explicit affinity matrix S like MCFS and NDFS, JELSR takes advantage of the local linear embedding (Roweis and Saul, 2000) to learn the local approximation matrix, i.", "startOffset": 141, "endOffset": 164}, {"referenceID": 98, "context": "1 Low Variance (Pedregosa et al., 2011) (Unsupervised) Low Variance is a simple feature selection algorithm which eliminates the feature whose variance is below some threshold.", "startOffset": 15, "endOffset": 39}, {"referenceID": 16, "context": "2 T-score (Davis and Sampson, 1986) (Supervised) t-score is used for binary classification problems.", "startOffset": 10, "endOffset": 35}, {"referenceID": 134, "context": "3 F-score (Wright, 1965) (Supervised) t-score can only be applied for binary classification task, therefore it has some limitations.", "startOffset": 10, "endOffset": 24}, {"referenceID": 72, "context": "4 Chi-Square Score (Liu and Setiono, 1995) (Supervised) Chi-square score utilizes the test of independence to assess whether the feature is independent of the class label.", "startOffset": 19, "endOffset": 42}, {"referenceID": 32, "context": "5 Gini Index (Gini, 1912) (Supervised) Gini index is a statistical measure to quantify if the feature is able to separate instances from different classes.", "startOffset": 13, "endOffset": 25}, {"referenceID": 40, "context": "6 CFS (Hall and Smith, 1999) (Supervised) The basic idea of CFS is to use a correlation based heuristic to evaluate the worth of feature subset F : CFS score(F) = krcf", "startOffset": 6, "endOffset": 28}, {"referenceID": 127, "context": "In order to get the feature-class correlation and feature-feature correlation, CFS uses symmetrical uncertainty (Vetterling et al., 1992) to estimate the degree of associations between two attributes.", "startOffset": 112, "endOffset": 137}, {"referenceID": 142, "context": "For example, these feature selection methods may select the same subset of features even though the features are reshuffled (Ye and Liu, 2012).", "startOffset": 124, "endOffset": 142}, {"referenceID": 146, "context": "One of the most common example is that in multifactor analysis-of-variance (ANOVA), each factor is associated with several groups and can be expressed by a set of dummy features (Yuan and Lin, 2006).", "startOffset": 178, "endOffset": 198}, {"referenceID": 84, "context": "Some other examples include different frequency bands represented as groups in signal processing (McAuley et al., 2005) and genes with similar functionalities acting as groups in bioinformatics (Ma et al.", "startOffset": 97, "endOffset": 119}, {"referenceID": 80, "context": ", 2005) and genes with similar functionalities acting as groups in bioinformatics (Ma et al., 2007).", "startOffset": 82, "endOffset": 99}, {"referenceID": 146, "context": "1 Group Lasso (Supervised) (Yuan and Lin, 2006) Group Lasso (Yuan and Lin, 2006; Bach, 2008; Jacob et al.", "startOffset": 27, "endOffset": 47}, {"referenceID": 146, "context": "1 Group Lasso (Supervised) (Yuan and Lin, 2006) Group Lasso (Yuan and Lin, 2006; Bach, 2008; Jacob et al., 2009; Meier et al., 2008), which derives feature coefficients from some groups to be exact zero, is a solution to this problem.", "startOffset": 60, "endOffset": 132}, {"referenceID": 3, "context": "1 Group Lasso (Supervised) (Yuan and Lin, 2006) Group Lasso (Yuan and Lin, 2006; Bach, 2008; Jacob et al., 2009; Meier et al., 2008), which derives feature coefficients from some groups to be exact zero, is a solution to this problem.", "startOffset": 60, "endOffset": 132}, {"referenceID": 51, "context": "1 Group Lasso (Supervised) (Yuan and Lin, 2006) Group Lasso (Yuan and Lin, 2006; Bach, 2008; Jacob et al., 2009; Meier et al., 2008), which derives feature coefficients from some groups to be exact zero, is a solution to this problem.", "startOffset": 60, "endOffset": 132}, {"referenceID": 86, "context": "1 Group Lasso (Supervised) (Yuan and Lin, 2006) Group Lasso (Yuan and Lin, 2006; Bach, 2008; Jacob et al., 2009; Meier et al., 2008), which derives feature coefficients from some groups to be exact zero, is a solution to this problem.", "startOffset": 60, "endOffset": 132}, {"referenceID": 30, "context": "2 Sparse Group Lasso (Supervised) (Friedman et al., 2010; Peng et al., 2010) Once Group Lasso selects a group, all the features in the group will be selected.", "startOffset": 34, "endOffset": 76}, {"referenceID": 100, "context": "2 Sparse Group Lasso (Supervised) (Friedman et al., 2010; Peng et al., 2010) Once Group Lasso selects a group, all the features in the group will be selected.", "startOffset": 34, "endOffset": 76}, {"referenceID": 30, "context": "Sparse Group Lasso (Friedman et al., 2010) takes advantage of both Lasso and Group Lasso, and it produces a solution with simultaneous intra-group and inter-group sparsity.", "startOffset": 19, "endOffset": 42}, {"referenceID": 51, "context": "3 Overlapping Sparse Group Lasso (Supervised) (Jacob et al., 2009) Above methods consider the disjoint group structures among features.", "startOffset": 46, "endOffset": 66}, {"referenceID": 51, "context": "However, groups may overlap with each other in some applications (Jacob et al., 2009; Jenatton et al., 2011; Zhao et al., 2009).", "startOffset": 65, "endOffset": 127}, {"referenceID": 55, "context": "However, groups may overlap with each other in some applications (Jacob et al., 2009; Jenatton et al., 2011; Zhao et al., 2009).", "startOffset": 65, "endOffset": 127}, {"referenceID": 150, "context": "However, groups may overlap with each other in some applications (Jacob et al., 2009; Jenatton et al., 2011; Zhao et al., 2009).", "startOffset": 65, "endOffset": 127}, {"referenceID": 142, "context": "One motivating example is the usage of biologically meaningful gene/protein groups mentioned in (Ye and Liu, 2012).", "startOffset": 96, "endOffset": 114}, {"referenceID": 74, "context": "Another motivating example is that genes/proteins may form certain hierarchical tree structures (Liu and Ye, 2010).", "startOffset": 96, "endOffset": 114}, {"referenceID": 59, "context": "Recently, Tree-guided Group Lasso is proposed to handle the feature selection for features that can be represented in an index tree (Kim and Xing, 2010; Liu and Ye, 2010; Jenatton et al., 2010).", "startOffset": 132, "endOffset": 193}, {"referenceID": 74, "context": "Recently, Tree-guided Group Lasso is proposed to handle the feature selection for features that can be represented in an index tree (Kim and Xing, 2010; Liu and Ye, 2010; Jenatton et al., 2010).", "startOffset": 132, "endOffset": 193}, {"referenceID": 54, "context": "Recently, Tree-guided Group Lasso is proposed to handle the feature selection for features that can be represented in an index tree (Kim and Xing, 2010; Liu and Ye, 2010; Jenatton et al., 2010).", "startOffset": 132, "endOffset": 193}, {"referenceID": 74, "context": "1 Tree-guided Group Lasso (Supervised) (Liu and Ye, 2010) In Tree-guided Group Lasso, the structure over the features can be represented as a tree with leaf nodes as features.", "startOffset": 39, "endOffset": 57}, {"referenceID": 74, "context": "We follow the definition from (Liu and Ye, 2010) to define Tree-guided Group Lasso, for an index tree G with a depth of d, Gi = {G1, G2, .", "startOffset": 30, "endOffset": 48}, {"referenceID": 108, "context": "Recent studies have shown that the learning performance can be improved if we explicitly take into account the dependency information among features (Sandler et al., 2009; Kim and Xing, 2009; Yang et al., 2012).", "startOffset": 149, "endOffset": 210}, {"referenceID": 58, "context": "Recent studies have shown that the learning performance can be improved if we explicitly take into account the dependency information among features (Sandler et al., 2009; Kim and Xing, 2009; Yang et al., 2012).", "startOffset": 149, "endOffset": 210}, {"referenceID": 139, "context": "Recent studies have shown that the learning performance can be improved if we explicitly take into account the dependency information among features (Sandler et al., 2009; Kim and Xing, 2009; Yang et al., 2012).", "startOffset": 149, "endOffset": 210}, {"referenceID": 142, "context": "1 Laplacian Lasso (Supervised) (Ye and Liu, 2012) Since features exhibit graph structures, when two nodes (features) Ni and Nj are connected by an edge in G(N,E), the features fi and fj are more likely to be selected together, and they should have similar feature coefficients.", "startOffset": 31, "endOffset": 49}, {"referenceID": 158, "context": "(93) reduces to the elastic net penalty (Zou and Hastie, 2005).", "startOffset": 40, "endOffset": 62}, {"referenceID": 22, "context": "Since the graph regularization term w\u2032Lw is convex and differentiable, existing efficient algorithms like LARS (Efron et al., 2004) and proximal gradient descent methods (Liu and Ye, 2009) can be directly applied.", "startOffset": 111, "endOffset": 131}, {"referenceID": 58, "context": "2 GFLasso (Supervised) (Kim and Xing, 2009) In Eq.", "startOffset": 23, "endOffset": 43}, {"referenceID": 139, "context": "3 GOSCAR (Supervised) (Yang et al., 2012) To address the limitations of GFLasso, (Yang et al.", "startOffset": 22, "endOffset": 41}, {"referenceID": 139, "context": ", 2012) To address the limitations of GFLasso, (Yang et al., 2012) proposed a GOSCAR algorithm by putting a l\u221e-norm regularization to enforce the pairwise feature coefficients to be equal if they are connected over the feature graph G(N,E).", "startOffset": 47, "endOffset": 66}, {"referenceID": 7, "context": "The formulation of the GOSCAR is more general than the OSCAR algorithm which is proposed in (Bondell and Reich, 2008).", "startOffset": 92, "endOffset": 117}, {"referenceID": 139, "context": "Therefore, a new formulation with non-convex grouping penalty is also proposed in (Yang et al., 2012):", "startOffset": 82, "endOffset": 101}, {"referenceID": 81, "context": "Many linked data related learning tasks are proposed such as collective classification (Macskassy and Provost, 2007; Sen et al., 2008), relational learning (Long et al.", "startOffset": 87, "endOffset": 134}, {"referenceID": 111, "context": "Many linked data related learning tasks are proposed such as collective classification (Macskassy and Provost, 2007; Sen et al., 2008), relational learning (Long et al.", "startOffset": 87, "endOffset": 134}, {"referenceID": 6, "context": ", 2006, 2007), and active learning (Bilgic et al., 2010; Hu et al., 2013), but the task of feature selection is not well studied due to some of its unique challenges: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information.", "startOffset": 35, "endOffset": 73}, {"referenceID": 48, "context": ", 2006, 2007), and active learning (Bilgic et al., 2010; Hu et al., 2013), but the task of feature selection is not well studied due to some of its unique challenges: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information.", "startOffset": 35, "endOffset": 73}, {"referenceID": 34, "context": "1 Feature Selection on Networks (Supervised) (Gu and Han, 2011) In (Gu and Han, 2011), authors propose a supervised feature selection algorithm (FSNet) based on Laplacian Regularized Least Squares (LapRLS).", "startOffset": 45, "endOffset": 63}, {"referenceID": 34, "context": "1 Feature Selection on Networks (Supervised) (Gu and Han, 2011) In (Gu and Han, 2011), authors propose a supervised feature selection algorithm (FSNet) based on Laplacian Regularized Least Squares (LapRLS).", "startOffset": 67, "endOffset": 85}, {"referenceID": 92, "context": "(99) can be solved by proximal gradient descent methods (Nesterov, 2004).", "startOffset": 56, "endOffset": 72}, {"referenceID": 90, "context": "As illustrated in Figure (14), LinkedFS extracts four basic types of relations as hypotheses: (a) CoPost - a user u2 can have multiple posts (p3, p4 and p5) and these posts are more similar than those randomly selected; (b) CoFollowing - two users u1 and u3 follow a user u4, its counterpart in citation analysis is bibliographic coupling (Morris, 2005), and their posts are more likely to be of similar topics; (c) CoFollowed - two users u2 and u4 are followed by a third user u1, similar to co-citation relation (Morris, 2005) in citation analysis, and their posts are more likely to be similar to each other; and (d) Following - a user u1 follows another user u2, and their posts (e.", "startOffset": 339, "endOffset": 353}, {"referenceID": 90, "context": "As illustrated in Figure (14), LinkedFS extracts four basic types of relations as hypotheses: (a) CoPost - a user u2 can have multiple posts (p3, p4 and p5) and these posts are more similar than those randomly selected; (b) CoFollowing - two users u1 and u3 follow a user u4, its counterpart in citation analysis is bibliographic coupling (Morris, 2005), and their posts are more likely to be of similar topics; (c) CoFollowed - two users u2 and u4 are followed by a third user u1, similar to co-citation relation (Morris, 2005) in citation analysis, and their posts are more likely to be similar to each other; and (d) Following - a user u1 follows another user u2, and their posts (e.", "startOffset": 514, "endOffset": 528}, {"referenceID": 85, "context": "These four hypotheses are supported by social correlation theories such as homophily (McPherson et al., 2001) and social influence (Marsden and Friedkin, 1993) in explaining the existence of similarity as what these relations suggest.", "startOffset": 85, "endOffset": 109}, {"referenceID": 82, "context": ", 2001) and social influence (Marsden and Friedkin, 1993) in explaining the existence of similarity as what these relations suggest.", "startOffset": 29, "endOffset": 57}, {"referenceID": 93, "context": "Particularly, it uses modularity maximization (Newman and Girvan, 2004) to extract hidden factor matrix H.", "startOffset": 46, "endOffset": 71}, {"referenceID": 152, "context": "1 Multi-Source Feature Selection via Geometry-Dependent Covariance Analysis (Unsupervised) (Zhao and Liu, 2008) To integrate information from multiple sources, authors in (Zhao and Liu, 2008) propose an intuitive way to learn a global geometric pattern from all sources that reflects the intrinsic relationships among instances (Lanckriet et al.", "startOffset": 91, "endOffset": 111}, {"referenceID": 152, "context": "1 Multi-Source Feature Selection via Geometry-Dependent Covariance Analysis (Unsupervised) (Zhao and Liu, 2008) To integrate information from multiple sources, authors in (Zhao and Liu, 2008) propose an intuitive way to learn a global geometric pattern from all sources that reflects the intrinsic relationships among instances (Lanckriet et al.", "startOffset": 171, "endOffset": 191}, {"referenceID": 65, "context": "1 Multi-Source Feature Selection via Geometry-Dependent Covariance Analysis (Unsupervised) (Zhao and Liu, 2008) To integrate information from multiple sources, authors in (Zhao and Liu, 2008) propose an intuitive way to learn a global geometric pattern from all sources that reflects the intrinsic relationships among instances (Lanckriet et al., 2004).", "startOffset": 328, "endOffset": 352}, {"referenceID": 44, "context": "The basic idea of the first method is similar to Laplacian score (He et al., 2005) and SPEC (Zhao and Liu, 2007) mentioned above, hence one limitation is that feature redundancy is not considered since it measures features individually.", "startOffset": 65, "endOffset": 82}, {"referenceID": 151, "context": ", 2005) and SPEC (Zhao and Liu, 2007) mentioned above, hence one limitation is that feature redundancy is not considered since it measures features individually.", "startOffset": 17, "endOffset": 37}, {"referenceID": 30, "context": "For supervised multi-view feature selection, the most common approach is Sparse Group Lasso (Friedman et al., 2010; Peng et al., 2010).", "startOffset": 92, "endOffset": 134}, {"referenceID": 100, "context": "For supervised multi-view feature selection, the most common approach is Sparse Group Lasso (Friedman et al., 2010; Peng et al., 2010).", "startOffset": 92, "endOffset": 134}, {"referenceID": 121, "context": "2 Unsupervised Feature Selection for Multi-View Data (Unsupervised) (Tang et al., 2013) AUMFS (Feng et al.", "startOffset": 68, "endOffset": 87}, {"referenceID": 121, "context": "In (Tang et al., 2013), the authors propose a novel unsupervised feature selection method called Multi-view Feature Selection (MVFS).", "startOffset": 3, "endOffset": 22}, {"referenceID": 30, "context": "It can be observed that the basic idea is very similar to sparse group lasso (Friedman et al., 2010; Peng et al., 2010) which also requires intra-group sparsity and inter-group sparsity.", "startOffset": 77, "endOffset": 119}, {"referenceID": 100, "context": "It can be observed that the basic idea is very similar to sparse group lasso (Friedman et al., 2010; Peng et al., 2010) which also requires intra-group sparsity and inter-group sparsity.", "startOffset": 77, "endOffset": 119}, {"referenceID": 148, "context": "Recently, there exists some work trying to combine these two dual problems together, the problem is referred as feature selection on Trapezoidal data streams (Zhang et al., 2015).", "startOffset": 158, "endOffset": 178}, {"referenceID": 101, "context": "1 Grafting Algorithm (Supervised) (Perkins and Theiler, 2003) The first attempt to perform streaming feature selection is credited to (Perkins and Theiler, 2003).", "startOffset": 34, "endOffset": 61}, {"referenceID": 101, "context": "1 Grafting Algorithm (Supervised) (Perkins and Theiler, 2003) The first attempt to perform streaming feature selection is credited to (Perkins and Theiler, 2003).", "startOffset": 134, "endOffset": 161}, {"referenceID": 102, "context": "They proposed a streaming feature selection framework based on stagewise gradient descent regularized risk framework (Perkins et al., 2003).", "startOffset": 117, "endOffset": 139}, {"referenceID": 17, "context": "In (Dhillon et al., 2010), authors extended the alpha-investing algorithm and proposed a proposed a multiple streamwise feature selection algorithm to the case where there are multiple feature streams.", "startOffset": 3, "endOffset": 25}, {"referenceID": 67, "context": "4 Streaming Feature Selection with Group Structures (Supervised) (Wang et al., 2013b; Li et al., 2013) Previous mentioned streaming feature selection algorithms evaluate new features individually.", "startOffset": 65, "endOffset": 102}, {"referenceID": 67, "context": "In addition to OGFS, a similar algorithm is proposed in (Li et al., 2013).", "startOffset": 56, "endOffset": 73}, {"referenceID": 68, "context": "5 Unsupervised Streaming Feature Selection in Social Media (Unsupervised) (Li et al., 2015) Vast majority of streaming feature selection methods are supervised which utilize label information to guide feature selection process.", "startOffset": 74, "endOffset": 91}, {"referenceID": 68, "context": "To deal with large-scale unlabeled data in social media, authors in (Li et al., 2015) proposed an USFS algorithm to study unsupervised streaming feature selection.", "startOffset": 68, "endOffset": 85}, {"referenceID": 0, "context": "USFS first uncovers hidden social factors from link information by mixed membership stochastic blockmodel (Airoldi et al., 2009).", "startOffset": 106, "endOffset": 128}, {"referenceID": 131, "context": "1 Online Feature Selection (Supervised) (Wang et al., 2014) In (Wang et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 131, "context": ", 2014) In (Wang et al., 2014), an online feature selection algorithm (OFS) for binary classification is proposed.", "startOffset": 11, "endOffset": 30}, {"referenceID": 49, "context": "2 Unsupervised Feature Selection on Data Streams (Unsupervised) (Huang et al., 2015) OFS assumes that the class labels of continuously generated data streams are available.", "startOffset": 64, "endOffset": 84}, {"referenceID": 49, "context": "To timely select a subset of relevant features when unlabeled data are continuously being generated, authors in (Huang et al., 2015) propose a novel unsupervised feature selection method (FSDS) that is able to perform feature selection timely with only one pass of the data and utilize limited storage.", "startOffset": 112, "endOffset": 132}, {"referenceID": 70, "context": "Therefore, FSDS utilizes the matrix sketching strategy from (Liberty, 2013) to maintain a low-rank approximation of X(t).", "startOffset": 60, "endOffset": 75}, {"referenceID": 114, "context": "Currently, there are some attempts to use distributed programming frameworks such as MapReduce and MPI to perform parallel feature selection for very large scale datasets (Singh et al., 2009; Zhao et al., 2013; Yamada et al., 2014).", "startOffset": 171, "endOffset": 231}, {"referenceID": 153, "context": "Currently, there are some attempts to use distributed programming frameworks such as MapReduce and MPI to perform parallel feature selection for very large scale datasets (Singh et al., 2009; Zhao et al., 2013; Yamada et al., 2014).", "startOffset": 171, "endOffset": 231}, {"referenceID": 137, "context": "Currently, there are some attempts to use distributed programming frameworks such as MapReduce and MPI to perform parallel feature selection for very large scale datasets (Singh et al., 2009; Zhao et al., 2013; Yamada et al., 2014).", "startOffset": 171, "endOffset": 231}, {"referenceID": 26, "context": "In this case, well-designed feature selection algorithms in linear or sub-linear running time is more preferred (Fan et al., 2009; Tan et al., 2014).", "startOffset": 112, "endOffset": 148}, {"referenceID": 116, "context": "In this case, well-designed feature selection algorithms in linear or sub-linear running time is more preferred (Fan et al., 2009; Tan et al., 2014).", "startOffset": 112, "endOffset": 148}, {"referenceID": 57, "context": "Considering its importance in practical usage, stability of feature selection algorithms has received increasing attention in the community (Kalousis et al., 2007; He and Yu, 2010).", "startOffset": 140, "endOffset": 180}, {"referenceID": 45, "context": "Considering its importance in practical usage, stability of feature selection algorithms has received increasing attention in the community (Kalousis et al., 2007; He and Yu, 2010).", "startOffset": 140, "endOffset": 180}, {"referenceID": 1, "context": "It is also found in (Alelyani et al., 2011) that the underlying characteristics of data may greatly affect the stability of feature selection algorithms and the stability issue may also be data dependent.", "startOffset": 20, "endOffset": 43}, {"referenceID": 125, "context": "For instance, in (Tibshirani et al., 2001), a principled way to estimate the number of suitable clusters in a dataset is proposed.", "startOffset": 17, "endOffset": 42}], "year": 2016, "abstractText": "Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing high-dimensional data for data mining and machine learning problems. The objectives of feature selection include: building simpler and more comprehensible models, improving data mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities of feature selection algorithms. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the big data age, we revisit feature selection research from a data perspective, and review representative feature selection algorithms for generic data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for generic data, we generally categorize them into four groups: similarity based, information theoretical based, sparse learning based and statistical based methods. Finally, to facilitate and promote the research in this community, we also present a open-source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/scikit-feast/). At the end of this survey, we also have a discussion about some open problems and challenges that need to be paid more attention in future research.", "creator": "LaTeX with hyperref package"}}}