{"id": "1601.08188", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2016", "title": "Lipreading with Long Short-Term Memory", "abstract": "Lipreading, i.e. speech recognition from visual-only recordings of a speaker's face, can be achieved with a processing pipeline based solely on neural networks, yielding significantly better accuracy than conventional methods. Feed-forward and recurrent neural network layers (namely Long Short-Term Memory; LSTM) are stacked to form a single structure which is trained by back-propagating error gradients through all the layers. The performance of such a stacked network was experimentally evaluated and compared to a standard Support Vector Machine classifier using conventional computer vision features (Eigenlips and Histograms of Oriented Gradients). The evaluation was performed on data from 19 speakers of the publicly available GRID corpus. With 51 different words to classify, we report a best word accuracy on held-out evaluation speakers of 79.6% using the end-to-end neural network-based solution (11.6% improvement over the best feature-based solution evaluated).", "histories": [["v1", "Fri, 29 Jan 2016 16:48:07 GMT  (122kb,D)", "http://arxiv.org/abs/1601.08188v1", "Accepted for publication at ICASSP 2016"]], "COMMENTS": "Accepted for publication at ICASSP 2016", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["michael wand", "jan koutn\\'ik", "j\\\"urgen schmidhuber"], "accepted": false, "id": "1601.08188"}, "pdf": {"name": "1601.08188.pdf", "metadata": {"source": "CRF", "title": "LIPREADING WITH LONG SHORT-TERM MEMORY", "authors": ["Michael Wand", "Jan Koutn\u0131\u0301k", "J\u00fcrgen Schmidhuber"], "emails": [], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "4.1. Baseline Feature Extraction and Classification", "text": "The NN-based lipreader was compared to an SVM classifier using conventional features, i.e. self-lips [15] used in [17] as the basic feature, and histograms of oriented gradients (HOG) [33] as a more complex feature that performed well in preliminary experiments. Eigenlip features are generated from raw frames by calculating the PCA decomposition on the training data and then transforming all images by multiplying with the PCA matrix, retaining only a certain number of dimensions ordered by maximum variance. HOG was originally a feature for object recognition [33]; it splits the image window into small spatial regions (cells) and accumulates a local 1-D histogram of gradients or edge orientations above the pixels in each cell. Histogram entations are standardized over larger spatial ranges."}, {"heading": "4.2. Neural Network Lipreader", "text": "Neural networks (NNs) consist of processing units (neurons) connected to each other by traceable weighted connections, typically organized in layers that can be distinguished by their connectivity: (1) feedback NNs transmit the input signal to the output neurons without allowing cyclic calculations; (2) recursive NNs wire the connections in a cycle that forms a temporal memory. Also, in the monitored case, NNs are typically formed by gradient descent, which is realized by erroneous backpropagation through the layers, followed by an adjustment of weights. In the case of recursive NNs, the error spreads along the timeline, also referred to as backpropagation over time, by transforming the recursive connections into a feedback-forward structure as deep as the length of the sequence. Such deep networks cause the gradient [35 to disappear]."}, {"heading": "5.1. Experimental Setup", "text": "In a series of experiments with loudspeakers 1-9, the optimal configuration of feature extraction for the SVM experiments was determined, as well as the best neural network structure. The best PCA cutoff for eigenlip properties was 100 components, the best HOG cell size was 8. The best SVM detection results were obtained with a linear kernel at a sequence feature vector length of 6 frames. Increasing the fea ture vector length did not improve accuracy, almost certainly because among the short video sequences containing letters, the most errors occur (see subsection 5.2), 6 frames already cover all available information."}, {"heading": "5.2. Results", "text": "The LSTM lipreader proves to be very self-explanatory, the LSTM lipreader proves to be very self-explanatory, the LSTM lipreader proves to be very self-explanatory, the LSTM lipreader proves to be very self-explanatory, the LSTM lipreader improves the accuracy by 11.6% compared to the best conventional solution (HOG + SVM), the Figure 2 shows a typical confusion on speaker 7, where the classification was performed with the LSTM lipreader, the lines show reference words with names that have hypotheses, the confusion on letters is much higher than on longer words (lower), for this speaker and the configuration of the letters the accuracy is on the letters."}], "references": [{"title": "Hearing Lips and Seeing Voices", "author": ["H. McGurk", "J. MacDonald"], "venue": "Nature, vol. 264, no. 5588, pp. 746 \u2013 748, 1976.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1976}, {"title": "Automatic Lipreading to Enhance Speech Recognition (Speech Reading)", "author": ["E.D. Petajan"], "venue": "Ph.D. dissertation, University of Illinois at Urbana-Champaign, 1984.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1984}, {"title": "Lipreading from Color Video", "author": ["G.I. Chiou", "J.-N. Hwang"], "venue": "IEEE Transactions on Image Processing, vol. 6, no. 8, pp. 1192 \u2013 1195, 1997.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Comparing Visual Features for Lipreading", "author": ["Y. Lan", "R. Harvey", "B.-J. Theobald", "E.-J. Ong", "R. Bowden"], "venue": "Proc. of the Int. Conference on Auditory-Visual Speech Processing, 2009, pp. 102 \u2013 106.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Recent Developments in Automated Lip-reading", "author": ["R. Bowden", "S. Cox", "R. Harvey", "Y. Lan", "E.-J. Ong", "G. Owen", "B.-J. Theobald"], "venue": "Proc. SPIE, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Active Appearance Models", "author": ["T.F. Cootes", "G.J. Edwards", "C.J. Taylor"], "venue": "IEEE Trans. on Pattern Anal. and Machine Intel., vol. 23, no. 6, pp. 681 \u2013 685, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Lipreading With Local Spatiotemporal Descriptors", "author": ["G. Zhao", "M. Barnard", "M. Pietik\u00e4inen"], "venue": "IEEE Trans.s on Multimedia, vol. 11, no. 7, pp. 1254 \u2013 1265, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Continuous-Speech Phone Recognition from Ultrasound and Optical Images of the Tongue and Lips", "author": ["T. Hueber", "G. Chollet", "B. Denby", "G. Dreyfus", "M. Stone"], "venue": "Proc. Interspeech, 2007, pp. 658\u2013661.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Lipreading Approach for Isolated Digits Recognition Under Whisper and Neutral Speech", "author": ["F. Tao", "C. Busso"], "venue": "Proc. Interspeech, 2014, pp. 1154 \u2013 1158.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Connectionist Speech Recognition", "author": ["H. Bourlard", "N. Morgan"], "venue": "A Hybrid Approach. Kluwer Academic Publishers,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Proc. Magazine, vol. 29, no. 6, pp. 82 \u2013 97, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Hybrid Speech Recognition with Deep Bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A. Mohamed"], "venue": "Proc. ASRU, 2013, pp. 273 \u2013 278.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech Recognition with Deep Recurrent Neural Networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "Proc. ICASSP, 2013, pp. 6645 \u2013 6649.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comp., vol. 9, pp. 1735 \u2013 1780, 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Eigenlips\u2019 for Robust Speech Recognition", "author": ["C. Bregler", "Y. Konig"], "venue": "Proc. ICASSP, 1994, pp. 669 \u2013 672.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1994}, {"title": "Lipreading using Convolutional Neural Network", "author": ["K. Noda", "Y. Yamaguchi", "K. Nakadai", "H.G. Okuno", "T. Ogata"], "venue": "Proc. Interspeech, 2014, pp. 1149 \u2013 1153.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Silent Speech Interfaces", "author": ["B. Denby", "T. Schultz", "K. Honda", "T. Hueber", "J. Gilbert"], "venue": "Speech Communication, vol. 52, no. 4, pp. 270 \u2013 287, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Speech Synthesis from Real Time Ultrasound Images of the Tongue", "author": ["B. Denby", "M. Stone"], "venue": "Proc. ICASSP, 2004, pp. I\u2013685 \u2013 I\u2013688.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Eigentongue Feature Extraction for an Ultrasound-based Silent Speech Interface", "author": ["T. Hueber", "G. Aversano", "G. Chollet", "B. Denby", "G. Dreyfus", "Y. Oussar", "P. Roussel", "M. Stone"], "venue": "Proc. ICASSP, 2007, pp. I\u2013 1245 \u2013 I\u20131248.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Development of a Silent Speech Interface Driven by Ultrasound and Optical Images of the Tongue and Lips", "author": ["T. Hueber", "E.-L. Benaroya", "G. Chollet", "B. Denby", "G. Dreyfus", "M. Stone"], "venue": "Speech Communication, vol. 52, pp. 288 \u2013 300, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "A Speech Prosthesis Employing a Speech Synthesizer \u2013 Vowel Discrimination from Perioral Muscle Activities and Vowel Production", "author": ["N. Sugie", "K. Tsunoda"], "venue": "IEEE Transactions on Biomedical Engineering, vol. 32, no. 7, pp. 485 \u2013 490, 1985.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1985}, {"title": "Use of Myoelectric Signals to Recognize Speech", "author": ["M.S. Morse", "S.H. Day", "B. Trull", "H. Morse"], "venue": "Proc. 11th Annual Conference of the IEEE Engineering in Medicine and Biology Society, 1989, pp. 1793 \u2013 1794.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1989}, {"title": "Modeling Coarticulation in Large Vocabulary EMG-based Speech Recognition", "author": ["T. Schultz", "M. Wand"], "venue": "Speech Comm., vol. 52, no. 4, pp. 341 \u2013 353, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Tackling Speaking Mode Varieties in EMG-based Speech Recognition", "author": ["M. Wand", "M. Janke", "T. Schultz"], "venue": "IEEE Transaction on Biomedical Engineering, vol. 61, no. 10, pp. 2515 \u2013 2526, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Development of a (Silent) Speech Recognition System for Patients Following Laryngectomy", "author": ["M.J. Fagan", "S.R. Ell", "J.M. Gilbert", "E. Sarrazin", "P.M. Chapman"], "venue": "Medical Engineering and Physics, vol. 30, pp. 419 \u2013 425, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proc. ICML, 2006, pp. 369 \u2013 376.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards End-To-End Speech Recognition with Recurrent Neural Networks", "author": ["A. Graves", "N. Jaitly"], "venue": "Proc. ICML, 2014, pp. 1764 \u2013 1772.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "DeepSpeech: Scaling up endto-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng"], "venue": "arXiv: 1412: 5567v1, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Backpropagation Applied to Handwritten Zip Code Recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackeland"], "venue": "Neural Computation, vol. 1, pp. 541 \u2013 551, 1989.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1989}, {"title": "A Committee of Neural Networks for Traffic Sign Classification", "author": ["D. Ciresan", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "Proc. IJCNN, 2011, pp. 1918 \u2013 1921.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "An Audio-Visual Corpus for Speech Perception and Automatic Speech Recognition", "author": ["M. Cooke", "J. Barker", "S. Cunningham", "X. Shao"], "venue": "Journal of the Acoustical Soc. of America, vol. 120, no. 5, pp. 2421 \u2013 2424, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Histograms of Oriented Gradients for Human Detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Proc. CVPR, vol. 1, 2005, pp. 886 \u2013 893.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "VLFeat: An Open and Portable Library of Computer Vision Algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "http: //www.vlfeat.org/, 2008.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "A Field Guide to Dynamical Recurrent Neural Networks", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "IEEE Press,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural Computation, vol. 12, no. 10, pp. 2451\u20132471, 2000.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2000}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u0131\u0301k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "CoRR, vol. abs/1503.04069, 2015. [Online]. Available: http://arxiv.org/abs/1503.04069", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Viseme Definitions Comparison for Visual-only Speech Recognition", "author": ["L. Cappelletta", "N. Harte"], "venue": "Proc. EUSIPCO, 2011, pp. 2109 \u2013 2113.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "It is well-known that humans understand speech not only by listening, but also by taking visual cues into account [1].", "startOffset": 114, "endOffset": 117}, {"referenceID": 1, "context": "Consequently, research on making lipreading available to electronic speech recognition and processing systems has been of interest for some decades, with pioneering work done by Petajan [2].", "startOffset": 186, "endOffset": 189}, {"referenceID": 2, "context": "His PhD thesis proposed to use lipreading to augment conventional automatic speech recognition (ASR), yet later researchers started to perform purely visual speech recognition [3], which is also the goal of this study.", "startOffset": 176, "endOffset": 179}, {"referenceID": 3, "context": "[4, 5], use a lip tracking system as a first stage, followed by versatile image features such as Active Appearance Models [6] or", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[4, 5], use a lip tracking system as a first stage, followed by versatile image features such as Active Appearance Models [6] or", "startOffset": 0, "endOffset": 6}, {"referenceID": 5, "context": "[4, 5], use a lip tracking system as a first stage, followed by versatile image features such as Active Appearance Models [6] or", "startOffset": 122, "endOffset": 125}, {"referenceID": 6, "context": "Local Binary Patterns [7].", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "[7], or Hidden Markov Models (HMMs), e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4, 5, 8, 9].", "startOffset": 0, "endOffset": 12}, {"referenceID": 4, "context": "[4, 5, 8, 9].", "startOffset": 0, "endOffset": 12}, {"referenceID": 7, "context": "[4, 5, 8, 9].", "startOffset": 0, "endOffset": 12}, {"referenceID": 8, "context": "[4, 5, 8, 9].", "startOffset": 0, "endOffset": 12}, {"referenceID": 9, "context": "Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10\u201312], more recently replacing the entire processing chain [13].", "startOffset": 148, "endOffset": 155}, {"referenceID": 10, "context": "Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10\u201312], more recently replacing the entire processing chain [13].", "startOffset": 148, "endOffset": 155}, {"referenceID": 11, "context": "Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10\u201312], more recently replacing the entire processing chain [13].", "startOffset": 148, "endOffset": 155}, {"referenceID": 12, "context": "Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10\u201312], more recently replacing the entire processing chain [13].", "startOffset": 209, "endOffset": 213}, {"referenceID": 13, "context": "For the latter, the Long Short Term Memory (LSTM; [14]) architecture is typically used.", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "to [5, 7].", "startOffset": 3, "endOffset": 9}, {"referenceID": 6, "context": "to [5, 7].", "startOffset": 3, "endOffset": 9}, {"referenceID": 1, "context": "Lipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17].", "startOffset": 98, "endOffset": 105}, {"referenceID": 14, "context": "Lipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17].", "startOffset": 98, "endOffset": 105}, {"referenceID": 2, "context": "Lipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17].", "startOffset": 155, "endOffset": 166}, {"referenceID": 15, "context": "Lipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17].", "startOffset": 155, "endOffset": 166}, {"referenceID": 16, "context": "The latter gives rise to a Silent Speech interface, which is defined as a system \u201cenabling speech communication to take place when an audible acoustic signal is unavailable\u201d [18].", "startOffset": 174, "endOffset": 178}, {"referenceID": 16, "context": "laryngectomees, whose voice box (larynx) has been removed) to communicate, as well as enabling confidential and undisturbing communication in public places [18].", "startOffset": 156, "endOffset": 160}, {"referenceID": 4, "context": "automatic speech extraction from surveillance videos and its interpretation for forensic purposes [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 17, "context": "Lipreading has been augmented with ultrasound images of the tongue and vocal tract [19\u201321].", "startOffset": 83, "endOffset": 90}, {"referenceID": 18, "context": "Lipreading has been augmented with ultrasound images of the tongue and vocal tract [19\u201321].", "startOffset": 83, "endOffset": 90}, {"referenceID": 19, "context": "Lipreading has been augmented with ultrasound images of the tongue and vocal tract [19\u201321].", "startOffset": 83, "endOffset": 90}, {"referenceID": 20, "context": "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22\u201325] or (electro-)magnetic articulography [26].", "startOffset": 130, "endOffset": 137}, {"referenceID": 21, "context": "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22\u201325] or (electro-)magnetic articulography [26].", "startOffset": 130, "endOffset": 137}, {"referenceID": 22, "context": "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22\u201325] or (electro-)magnetic articulography [26].", "startOffset": 130, "endOffset": 137}, {"referenceID": 23, "context": "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22\u201325] or (electro-)magnetic articulography [26].", "startOffset": 130, "endOffset": 137}, {"referenceID": 24, "context": "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22\u201325] or (electro-)magnetic articulography [26].", "startOffset": 175, "endOffset": 179}, {"referenceID": 9, "context": "NNs have been used in speech recognition as feature extractors in HMM-based speech recognizers [10, 11].", "startOffset": 95, "endOffset": 103}, {"referenceID": 10, "context": "NNs have been used in speech recognition as feature extractors in HMM-based speech recognizers [10, 11].", "startOffset": 95, "endOffset": 103}, {"referenceID": 25, "context": "An end-to-end neural network system [27,28] finally outperformed HMM-based systems and achieved the best performance (16% error) on the large Switchboard Hub5\u201900 ar X iv :1 60 1.", "startOffset": 36, "endOffset": 43}, {"referenceID": 26, "context": "An end-to-end neural network system [27,28] finally outperformed HMM-based systems and achieved the best performance (16% error) on the large Switchboard Hub5\u201900 ar X iv :1 60 1.", "startOffset": 36, "endOffset": 43}, {"referenceID": 27, "context": "speech recognition benchmark [29].", "startOffset": 29, "endOffset": 33}, {"referenceID": 28, "context": "Since then, Convolutional NNs (CNNs) trained by gradient descent [30] dominate, e.", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "[31], the area of image recognition, as well as related tasks like object detection and segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "The first CNN application in lipreading [17] uses the CNN as a preprocessor for an HMM-based sequence classifier.", "startOffset": 40, "endOffset": 44}, {"referenceID": 30, "context": "Our experiments were performed using the GRID audiovisual corpus [32]1, consisting of video and audio recordings of 34 speakers saying 1000 sentences each.", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "gridcorpus image height along the middle column) and \u03c3 = 500 pixels, and rescaled into [0, 1] interval.", "startOffset": 87, "endOffset": 93}, {"referenceID": 0, "context": "all pixel values were remapped to [0, 1] interval), and all the values in the complete dataset were standardized.", "startOffset": 34, "endOffset": 40}, {"referenceID": 14, "context": "The NN-based lipreader was compared to a baseline SVM classifier using conventional features, namely Eigenlips [15], which were used as a baseline feature in [17], and Histograms of Oriented Gradients (HOG) [33] as a more complex feature which yielded good performance in preliminary experiments.", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "The NN-based lipreader was compared to a baseline SVM classifier using conventional features, namely Eigenlips [15], which were used as a baseline feature in [17], and Histograms of Oriented Gradients (HOG) [33] as a more complex feature which yielded good performance in preliminary experiments.", "startOffset": 158, "endOffset": 162}, {"referenceID": 31, "context": "The NN-based lipreader was compared to a baseline SVM classifier using conventional features, namely Eigenlips [15], which were used as a baseline feature in [17], and Histograms of Oriented Gradients (HOG) [33] as a more complex feature which yielded good performance in preliminary experiments.", "startOffset": 207, "endOffset": 211}, {"referenceID": 31, "context": "HOG is originally a feature extractor for object recognition [33]; it divides the image window into small spatial regions (cells) and accumulates a local 1-D histogram of gradient directions or edge orientations over the pixels in each cell.", "startOffset": 61, "endOffset": 65}, {"referenceID": 32, "context": "The HOG features were obtained using the VLFeat library [34].", "startOffset": 56, "endOffset": 60}, {"referenceID": 33, "context": "Such deep networks cause the gradient to explode or vanish [35], which can be fixed by replacing a single recurrent NN unit by a LSTM cell that avoids the problem by linear recurrent connection of a cell inside the LSTM unit [14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "Such deep networks cause the gradient to explode or vanish [35], which can be fixed by replacing a single recurrent NN unit by a LSTM cell that avoids the problem by linear recurrent connection of a cell inside the LSTM unit [14].", "startOffset": 225, "endOffset": 229}, {"referenceID": 34, "context": "The information flow through the LSTM cell is regulated by input, output and forget [36] gates using multiplicative connections.", "startOffset": 84, "endOffset": 88}, {"referenceID": 35, "context": "See [37] for detailed LSTM description, analysis and setup.", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "[7] report that second-degree polynomial SVM kernels perform the best, however their SVM classifier treats the sequential information differently.", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "[38] and the references therein).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "We note that [17] report phone accuracy on a corpus which consists of whole words: here the context plays a great role in improving recognition.", "startOffset": 13, "endOffset": 17}], "year": 2016, "abstractText": "Lipreading, i.e. speech recognition from visual-only recordings of a speaker\u2019s face, can be achieved with a processing pipeline based solely on neural networks, yielding significantly better accuracy than conventional methods. Feedforward and recurrent neural network layers (namely Long Short-Term Memory; LSTM) are stacked to form a single structure which is trained by back-propagating error gradients through all the layers. The performance of such a stacked network was experimentally evaluated and compared to a standard Support Vector Machine classifier using conventional computer vision features (Eigenlips and Histograms of Oriented Gradients). The evaluation was performed on data from 19 speakers of the publicly available GRID corpus. With 51 different words to classify, we report a best word accuracy on held-out evaluation speakers of 79.6% using the end-toend neural network-based solution (11.6% improvement over the best feature-based solution evaluated).", "creator": "LaTeX with hyperref package"}}}