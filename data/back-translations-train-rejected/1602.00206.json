{"id": "1602.00206", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2016", "title": "Unsupervised Deep Hashing for Large-scale Visual Search", "abstract": "Learning based hashing plays a pivotal role in large-scale visual search. However, most existing hashing algorithms tend to learn shallow models that do not seek representative binary codes. In this paper, we propose a novel hashing approach based on unsupervised deep learning to hierarchically transform features into hash codes. Within the heterogeneous deep hashing framework, the autoencoder layers with specific constraints are considered to model the nonlinear mapping between features and binary codes. Then, a Restricted Boltzmann Machine (RBM) layer with constraints is utilized to reduce the dimension in the hamming space. Extensive experiments on the problem of visual search demonstrate the competitiveness of our proposed approach compared to state-of-the-art.", "histories": [["v1", "Sun, 31 Jan 2016 06:36:47 GMT  (108kb)", "http://arxiv.org/abs/1602.00206v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["zhaoqiang xia", "xiaoyi feng", "jinye peng", "abdenour hadid"], "accepted": false, "id": "1602.00206"}, "pdf": {"name": "1602.00206.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zhaoqiang Xia", "Xiaoyi Feng", "Jinye Peng", "Abdenour Hadid"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 2.00 206v 1 [cs.C V] 31 Jan 20Index Terms - Learning based hashing, Unsupervised learning, Deep learning, Autoencoder, RBM"}, {"heading": "1. INTRODUCTION", "text": "In the age of big data, large-scale visual search is essential for accessing and processing a huge amount of images, and this is of great importance in many areas of computer vision. Compared to tree-based approaches, hash-based methods use multiple hash functions to project image functions into binary codes and are better suited for large-scale visual searches based on compact representations [1, 2]. Thus, hash-based approaches to dealing with high-dimensional data become attractive. Moreover, existing hash approaches can be divided into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9]."}, {"heading": "2. RELATED WORK", "text": "Learning-based hashing. Depending on whether the semantic information is used or not, learning-based hashing can be divided into three categories: unattended hashing, semi-supervised hashing, and supervised hashing. Unsupervised hashing approaches do not use semantic information (such as tags), while supervised hashing approaches learn hash functions with semantic information. Semi-supervised approaches model the data with labeled and unlabeled data. For the first category, spectral hashing (SH) [6], ITerative quantization (ITQ) [7], and K-Means hashing (KMH) use various objective functions with restrictions on bi-narization loss or / and / or binary bit variance. For the second category, semi-supervised hashing (SSH) hashing (SSH) by Wang et al."}, {"heading": "3. DEEP HASHING", "text": "Fig. 1 illustrates the framework of our proposed deep hare method. The framework contains two heterogeneous layers: (1) several deep autoencoder layers; (2) an RBM layer. With a feature vector x = (x1, x2,..., xd) T, the deep hash framework can transform the input vector into a binary vector b = (b1, b2,..., bk) T using k \u0445 d."}, {"heading": "3.1. SAE Layers", "text": "Let's assume that there are L layers in our deep encoder layers, and the hash function in the Lth layer is H l (ul) = = tanh (W lvl \u2212 1 + bl). ulls represents the input vector in the Lth layer as V1 (v1, v2, vq) T. \u2212 s To learn several layers in the Lth layer formation, it has been proposed to minimize reconstruction errors. As shown in Figure 2, the deep learn vector layer (i.e. SAE) can be split into several layers."}, {"heading": "3.2. RBM Layer", "text": "Since the variables in the RBM layer are binary, the drawing function is used to transform the output vector of the SAE layers in such a way that each input unit of the RBM layer can be evaluated as {0, 1}. Suppose that the visible layer and the hidden layer are each designated as v and h, whereas a, b and W are the bias and weights of the visible layer and the hidden layer. The energy of the RBM model is defined as E (v, h) = \u2212 aTv \u2212 b T h \u2212 hTWv and the common probability of (v, h) is P (v, h) = 1 Z e \u2212 E (v, h).The optimization problem of a conventional RBM layer is defined as E (v, h)."}, {"heading": "4. EXPERIMENTAL ANALYSIS", "text": "To evaluate our proposed method, we conducted extensive experiments on two data sets: CIFAR-101 and MIRFLICKR25K2. The CIFAR-10 data sets consist of 60,000 32 \u00d7 32 color images in 10 classes with 50,000 educational images and 10,000 test images. The MIRFLICKR-25K data sets contain 25,000 color images in 26 classes, in which 20,000 training images and 5,000 test images are randomly selected. In addition, the cascade-like 512-D GIST [22] and 512-D features (BoF) are used to display images."}, {"heading": "5. CONCLUSION", "text": "We proposed a heterogeneous deep learning architecture for learning hash functions. With two limitations for balanced and uncorrelated binary codes, we learned the parameters of SAE and RBM layers. Experimental results and comprehensive comparative analysis of the problem of large-scale image search evaluated the effectiveness of our proposed approach, which surpassed state-of-the-art uncontrolled methods."}, {"heading": "6. REFERENCES", "text": "[1] L. Pauleve, H. Je \u0301 gou, and L. Amsaleg Processing, \"Locality sensitive hashing: A comparison of hash function types and querying mechanisms,\" Pattern Recognition Letters, vol. 31, no. 11, pp. 1348-1358, 2010. [2] J. Wang, S. Kumar, and S. F. Chang, \"Semi-supervised hashing for large-scale search,\" IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 34, no. 12, pp. 2393-2406, 2012. [3] A. Gionis, P. Indyk, and R. Motwani, \"Similarity search in high dimensions via hashing,\" in Proceedings of the 25th International Conference on Very Large Data Bases, 1999, pp. 518-529. [4] M. Slaney and M. Casey, \"Locality-sensitive hashing for finding nearest neighbors,\" IEE Procashing. \""}], "references": [{"title": "Locality sensitive hashing: A comparison of hash function types and querying mechanisms", "author": ["L. Paulev\u00e9", "H. J\u00e9gou", "L. Amsaleg"], "venue": "Pattern Recognition Letters, vol. 31, no. 11, pp. 1348\u20131358, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Semi-supervised hashing for large-scale search", "author": ["J. Wang", "S. Kumar", "S.F. Chang"], "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 34, no. 12, pp. 2393\u20132406, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "Proceedings of the 25th International Conference on Very Large Data Bases, 1999, pp. 518\u2013529.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Locality-sensitive hashing for finding nearest neighbors", "author": ["M. Slaney", "M. Casey"], "venue": "IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 128\u2013131, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Kernelized locality-sensitive hashing for scalable image search", "author": ["B. Kulis", "K. Grauman"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2009, pp. 2130 \u2013 2137.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, vol. 282, no. 3, pp. 1753\u20131760, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 817 \u2013 824.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "K-means hashing: An affinity-preserving quantization method for learning binary compact codes", "author": ["K. He", "F. Wen", "J. Sun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 2938\u20132945.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Semisupervised nonlinear hashing using bootstrap sequential projection learning", "author": ["C. Wu", "J. Zhu", "D. Cai", "C. Chen", "J. Bu"], "venue": "IEEE Transactions on Knowledge & Data Engineering, vol. 25, no. 6, pp. 1380\u20131393, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Supervised hashing with kernels", "author": ["S.F. Chang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2074\u20132081.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Ldahash: Improved matching with smaller descriptors", "author": ["P. Fua", "M. Bronstein", "A.C. Bronstein", "Strecha"], "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 34, no. 1, pp. 66\u201378, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Attribute discovery via predictable discriminative binary codes", "author": ["M. Rastegari", "A. Farhadi", "D. Forsyth"], "venue": "Lecture Notes in Computer Science on ECCV, vol. 7577, no. 1, pp. 876\u2013889, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning", "author": ["Y. Lecun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, pp. 436\u201344, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1090\u20131098.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "Proceedings of the 25th international conference on Machine learning, 2008, pp. 1096\u20131103.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep hashing for compact binary codes learning", "author": ["V.E. Liong", "J. Lu", "G. Wang", "P. Moulin", "J. Zhou"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 2475\u20132483.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised hashing for image retrieval via image representation learning", "author": ["R. Xia", "Y. Pan", "H. Lai", "C. Liu", "S. Yan"], "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014, pp. 2156\u20132162.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning of binary hash codes for fast image retrieval", "author": ["K. Lin", "H.-F. Yang", "J.-H. Hsiao", "C.-S. Chen"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2015, pp. 27\u201335.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Simultaneous feature learning and hash coding with deep neural networks", "author": ["H. Lai", "Y. Pan", "Y. Liu", "S. Yan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3270\u20133278.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Training restricted boltzmann machines: An introduction", "author": ["A. Fischer", "C. Igel"], "venue": "Pattern Recognition, vol. 47, no. 1, pp. 25\u201339, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "International Journal of Computer Vision, vol. 42, no. 3, pp. 145\u2013175, 2001.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Efficient visual search of videos cast as text retrieval", "author": ["J. Sivic", "A. Zisserman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 4, pp. 591\u2013606, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Compared to tree based approaches, hashing based methods utilize several hash functions to project image features into binary codes and are more suitable for large-scale visual search due to compact representations [1, 2].", "startOffset": 215, "endOffset": 221}, {"referenceID": 1, "context": "Compared to tree based approaches, hashing based methods utilize several hash functions to project image features into binary codes and are more suitable for large-scale visual search due to compact representations [1, 2].", "startOffset": 215, "endOffset": 221}, {"referenceID": 2, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 110, "endOffset": 119}, {"referenceID": 3, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 110, "endOffset": 119}, {"referenceID": 4, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 110, "endOffset": 119}, {"referenceID": 1, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 147, "endOffset": 162}, {"referenceID": 5, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 147, "endOffset": 162}, {"referenceID": 6, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 147, "endOffset": 162}, {"referenceID": 7, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 147, "endOffset": 162}, {"referenceID": 8, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 147, "endOffset": 162}, {"referenceID": 9, "context": "by adding kernelization [10], it is still challenging to select an appropriate kernel function for specific data.", "startOffset": 24, "endOffset": 28}, {"referenceID": 5, "context": "For the first category, the Spectral Hashing (SH) [6], ITerative Quantization (ITQ) [7] and K-Means Hashing (KMH) [8] used different objective functions with constraints of bi-", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "For the first category, the Spectral Hashing (SH) [6], ITerative Quantization (ITQ) [7] and K-Means Hashing (KMH) [8] used different objective functions with constraints of bi-", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "For the first category, the Spectral Hashing (SH) [6], ITerative Quantization (ITQ) [7] and K-Means Hashing (KMH) [8] used different objective functions with constraints of bi-", "startOffset": 114, "endOffset": 117}, {"referenceID": 1, "context": "[2] constructed an objective function minimizing binarization loss of labeled data and maximizing the variance of unlabeled data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The approach was later extended by employing nonlinear hash functions [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 10, "context": "For the third category, Linear Discriminant Analysis (LDA) [11] and multiple linearSVMs [12] were used as hash functions and trained with large margin criterion.", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "For the third category, Linear Discriminant Analysis (LDA) [11] and multiple linearSVMs [12] were used as hash functions and trained with large margin criterion.", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "Recently, several deep learning algorithms have been proposed in machine learning and applied to visual object detection and recognition, image classification, face verification and many other research problems [13].", "startOffset": 211, "endOffset": 215}, {"referenceID": 13, "context": "Since several foundational deep learning frameworks, such as Convolutional Neural Networks (CNN) [14], Stacked AutoEncoders (SAE) [15] and Deep Belief Network (DBN) [16], have been presented, numerous deep learning approaches are developed based on these frameworks.", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "Since several foundational deep learning frameworks, such as Convolutional Neural Networks (CNN) [14], Stacked AutoEncoders (SAE) [15] and Deep Belief Network (DBN) [16], have been presented, numerous deep learning approaches are developed based on these frameworks.", "startOffset": 130, "endOffset": 134}, {"referenceID": 15, "context": "Since several foundational deep learning frameworks, such as Convolutional Neural Networks (CNN) [14], Stacked AutoEncoders (SAE) [15] and Deep Belief Network (DBN) [16], have been presented, numerous deep learning approaches are developed based on these frameworks.", "startOffset": 165, "endOffset": 169}, {"referenceID": 16, "context": "[17] presented a framework minimizing a global quantization loss function with two constraints to learn binary codes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In [18, 19, 20], the convolutional neural networks were utilized to extract visual features and a hashing layer was combined to learn binary codes through supervised learning.", "startOffset": 3, "endOffset": 15}, {"referenceID": 18, "context": "In [18, 19, 20], the convolutional neural networks were utilized to extract visual features and a hashing layer was combined to learn binary codes through supervised learning.", "startOffset": 3, "endOffset": 15}, {"referenceID": 19, "context": "In [18, 19, 20], the convolutional neural networks were utilized to extract visual features and a hashing layer was combined to learn binary codes through supervised learning.", "startOffset": 3, "endOffset": 15}, {"referenceID": 14, "context": "To learn multiple-layers autoencoder, layer-by-layer training has been proposed [15] to minimize the reconstruction error.", "startOffset": 80, "endOffset": 84}, {"referenceID": 5, "context": "Besides preserving the similarity in the projected space by minimizing the reconstruction error, the representative hash codes should be balanced and uncorrelated [6].", "startOffset": 163, "endOffset": 166}, {"referenceID": 20, "context": "We utilize the Contrastive Divergence (CD) algorithm [21] to seek the numerical solution of the problem (Eq.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "Algorithm 1 The Heterogeneous Deep Hashing (HetDH) 1: Initialization: Set up \u03bb, \u03bc, \u03b2, \u03b1, L; Set up iteration times T and convergence errors \u01eb1, \u01eb2; Randomly initialize elements of W, b, a in [0, 1]; Split the training set into M epochs, each having N images.", "startOffset": 191, "endOffset": 197}, {"referenceID": 21, "context": "Moreover, the cascaded 512-D GIST [22] and 512-D Bag-of-Features (BoF) [23] are used for image representation.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "Moreover, the cascaded 512-D GIST [22] and 512-D Bag-of-Features (BoF) [23] are used for image representation.", "startOffset": 71, "endOffset": 75}, {"referenceID": 4, "context": "For comparative analysis, the KLSH [5], SH [6], ITQ [7] and KMH [8] algorithms3 are considered and used as baseline methods.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "For comparative analysis, the KLSH [5], SH [6], ITQ [7] and KMH [8] algorithms3 are considered and used as baseline methods.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "For comparative analysis, the KLSH [5], SH [6], ITQ [7] and KMH [8] algorithms3 are considered and used as baseline methods.", "startOffset": 52, "endOffset": 55}, {"referenceID": 7, "context": "For comparative analysis, the KLSH [5], SH [6], ITQ [7] and KMH [8] algorithms3 are considered and used as baseline methods.", "startOffset": 64, "endOffset": 67}], "year": 2016, "abstractText": "Learning based hashing plays a pivotal role in large-scale visual search. However, most existing hashing algorithms tend to learn shallow models that do not seek representative binary codes. In this paper, we propose a novel hashing approach based on unsupervised deep learning to hierarchically transform features into hash codes. Within the heterogeneous deep hashing framework, the autoencoder layers with specific constraints are considered to model the nonlinear mapping between features and binary codes. Then, a Restricted Boltzmann Machine (RBM) layer with constraints is utilized to reduce the dimension in the hamming space. Extensive experiments on the problem of visual search demonstrate the competitiveness of our proposed approach compared to stateof-the-art.", "creator": "LaTeX with hyperref package"}}}