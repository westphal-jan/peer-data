{"id": "1602.00367", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2016", "title": "Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers", "abstract": "Document classification tasks were primarily tackled at word level. Recent research that works with character-level inputs shows several benefits over word-level approaches such as natural incorporation of morphemes and better handling of rare words. We propose a neural network architecture that utilizes both convolution and recurrent layers to efficiently encode character inputs. We validate the proposed model on eight large scale document classification tasks and compare with character-level convolution-only models. It achieves comparable performances with much less parameters.", "histories": [["v1", "Mon, 1 Feb 2016 02:53:41 GMT  (86kb,D)", "http://arxiv.org/abs/1602.00367v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yijun xiao", "kyunghyun cho"], "accepted": false, "id": "1602.00367"}, "pdf": {"name": "1602.00367.pdf", "metadata": {"source": "CRF", "title": "Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers", "authors": ["Yijun Xiao", "Kyunghyun Cho"], "emails": ["ryjxiao@nyu.edu", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "In this year it has reached the point where it is a pure \"yes,\" a \"yes,\" a \"yes,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"no,\" a \"yes,\" a \"no,\" a \"no,\" a \"\" no, \"a\" a \"\" no, \"a\" \"a\" \"\" \"\", \"a\" \"\" \"\" \"\" \",\" a \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "2 Basic Building Blocks: Neural Network Layers", "text": "In this section, we describe four basic levels of a neural network, which will later be used to form a single network for classifying a document."}, {"heading": "2.1 Embedding Layer", "text": "As already mentioned, each document is presented as a sequence of uniform vectors. A uniform vector of the i-th symbol in a vocabulary is a binary vector whose elements are all zeros, except for the i-th element, which is set to one. Therefore, each document is a sequence of uniform vectors T (x1, x2,.., xT). An embedding layer projects each of the uniform vectors into a d-dimensional continuous vector space Rd. This is done simply by multiplying the uniform vector from the left by a weight matrix W-Rd-V-V-V, where | V-V is the number of unique symbols in a vocabulary: et = Wx.After the embedding layer, the input sequence of uniform vectors becomes a sequence of dense, reality-weighted vectors (e1, e2, eT)."}, {"heading": "2.2 Convolutional Layer", "text": "A wave layer consists of two stages. In the first stage, a series of d'filters of the receptive field quantity r, F'Rd '\u00b7 r, is plotted on the input sequence: ft = \u03c6 (F [et \u2212 (r \u2212 (r \u2212 2) + 1;..; et;.., et + (r / 2)]), where \u03c6 is a nonlinear activation function like tanh or a rectifier. This happens for each step of the input sequence, resulting in a sequence F = (f1, f2,.., fT). The resulting sequence F is max-pooled with the size r \u2032: f't = max (f't \u2212 1) \u00b7 r '+ 1,.., ft \u00b7 r \u2032), where max applies to each element of the vectors, resulting in a sequence F' = (f '1, f '2,.., f'T / r \u2032)."}, {"heading": "2.3 Recurrent Layer", "text": "A recursive layer consists of a recursive function f, which takes an input vector and the previous hidden state as input and returns the new hidden state: ht = f (xt, ht \u2212 1), where xt-Rd is a one-off step from the input sequence (x1, x2,.., xT). h0 \"Rd\" is often initialized as a recursive vector. \"Recursive function\" However, the most naive recursive function suffers from the problem of disappearing gradients (Wxxt + Uhht \u2212 1), where Wx \"Rd\" \u00b7 d \"and Uh\" Rd \"\u00b7 d\" are the weight matrices. However, this naive recursive function suffers from the problem of disappearing gradients (Bengio et al., 1994; Hochreiter et al., 2001). More recently, it has been common to use a more complicated function to control the flow of information to prevent disappearing gradients."}, {"heading": "2.4 Classification Layer", "text": "A classification layer is essentially a logistic regression classifier. For a fixed-dimensional input from the bottom layer, it is affinetransformed by the classification layer, followed by a Softmax activation function (Bridle, 1990) to calculate the predictive probabilities for all categories, using p (y = k | X) = exp (w > k x + bk) \u2211 Kk \u00b2 = 1 exp (w > k \u2032 x + bk \u2032), with wk's and bk's being the weight and bias vectors. We assume that there are K-categories. It is worth noting that this classification layer needs a fixed-dimensional vector as input, while the recursive layer or the revolutionary layer is a variable length sequence of vectors (the length determined by the input sequence), which can be resolved by simple max-pooling vectors (both recurrent and recurrent) over the last cursive (as well as in 2014)."}, {"heading": "3 Character-Level Convolutional-Recurrent Network", "text": "In this section, we propose a mix of revolutionary and recurring networks for classifying documents at the character level."}, {"heading": "3.1 Motivation", "text": "A basic motivation for using the revolutionary layer is that it learns to extract superior, abstract (local) translation-invariant features from the input sequence, in this case the document. Despite this advantage, we noted that it takes many layers of convolution to capture long-term dependencies, due to the locality of convolution and merging (see Sec. 2.2). This becomes more difficult the longer the input sequence grows, and in the case of character-based modeling, it is common for a document to be a sequence sequence of hundreds or thousands of characters. Ultimately, this leads to the need for a very deep network with many revolutionary layers. Unlike the revolutionary layer, the recursive layer of Sec. 2.3 is able to capture long-term dependencies, especially when there is only one layer to play."}, {"heading": "3.2 Model Description", "text": "The proposed model, which we refer to as a convolution-recursive network (ConvRec), begins with a uniform sequence input X = (x1, x2,.., xT), which is converted into a sequence of dense, real-weighted vectors E = (e1, e2,.., eT), using the embedding layer from Sec. 2.1. We apply several revolutionary layers (Sec. 2.2) to E to obtain a shorter sequence of feature vectors: F = (f1, f2,.., fT), which is then fed into a bidirectional recursive layer (Sec. 2.3), resulting in two sequencesHforward = (\u2212 \u2192 h 1, \u2212 h 2,.)."}, {"heading": "3.3 Related Work", "text": "In fact, it is a reactionary act that is in a position to mobilize the reactionary forces that are present in the reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, asasasasasasasasasasactic, reactionary, reactionary, asasasasasasasasasasasactic, asasasactionary, asasasactionary, asasasasactionary, asasasactionary, asasasasasactionary, asasasasasasasasactionary, reactionary, asasasasasactionary, asasasasasasasasasasactionary, asasasasasasasactic, asasasasactionary, asasasasasasasasactic, asasasasasasasasactic, asasasasasactic, asasasasasactic, asasasactic, asasasasasasasasactic, asasasactic, asasasasactic, asasasasasactic, asasasasasasasactic, asasasasasactic, asasasasasactic, asactic, asasasasasactic, asasasasasactic, asasasactic, asasasasasasasasasasactic, asactic, asactic, asasasactic, asasasasasasactic, asasasasactic, asasasasasactic, asasasasasasasasasasactititititititititititititititititititititititititititititititititititititititititititititititititititititititi"}, {"heading": "4 Experiment Settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Task Description", "text": "We validate the proposed model based on eight large-scale document classification tasks (Zhang et al., 2015), ranging in size from 200,000 to 4,000,000 documents, including mood analysis (Yelp ratings, Amazon ratings), ontology classification (DBPedia), question type classification (Yahoo! Answers), and news categorization (AG News, Sogou News).Data sets A summary of statistics for each data set is listed in Table 1. In each class, there are the same number of examples for both training and test sets. For example, the DBPedia data set includes 40,000 training sessions and 5,000 test examples per class. For more detailed information on the structure of data sets see (Zhang et al., 2015)."}, {"heading": "4.2 Model Settings", "text": "Vocabulary V for our experiments consists of 96 characters, including all uppercase and lowercase letters, numbers, common punctuation marks and spaces. The embedding size d is 8.As described in paragraph 3.1, we believe that adding recurring layers can effectively reduce the number of layers needed to capture long-term dependencies. Therefore, we are looking at models with two to five layers for each data set. According to the notations in sec.2.2, each layer has d \u2032 = 128 filters. For AG and Yahoo! responses, we are also experimenting larger models with 1,024 filters in the layers of the deflection. The receptive field size r is either five or three depending on the depth. The maximum pool size r \u00b2 is set to 2. The reflected linear units (ReLUs, Glorot et al., 2011) are used as activation functions in the deflectors."}, {"heading": "4.3 Training and Validation", "text": "The validation quantity is the same as the corresponding test quantity and is balanced in each class.The models are calculated by minimizing the following regularized negative log probability or cross entropy loss.X's and Y's are document character sequences and the corresponding observed class assignments in the training set D. w is the capture of model weights. The weight loss is calculated with \u03bb = 5 x 10 \u2212 4, l = \u2212 x, y \u00b2 D Log (p (y | X)) + \u03bb 2 x 2We train our models with AdaDelta (Lines, 2012) with \u03c1 = 0.95, = 10 \u2212 5 and a batch size of 128. Examples are added to the longest sequence in each batch and the masks to help identify the padded region."}, {"heading": "5 Results and Analysis", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "6 Conclusion", "text": "The proposed model is capable of encoding documents from the character level by capturing subword formations.We validated the proposed model using eight large-scale document classification tasks; the model achieved comparable results with much less revolutionary layers than pure convolution; we also discussed several aspects that affect the performance of the model. Generally, the proposed model performs better when the number of classes is large, the training size is small, and when the number of revolutionary layers is set to two or three. The proposed model is a general coding architecture that is not limited to classification tasks or natural language input. Thus, it (Chen et al., 2015; Visin et al., 2015) combined folding and recurring layers to tackle image segmentation tasks; (Sainath et al., 2015) applied a similar model to speech recognition. It will be interesting to see how future research layers that may be lost are transferred to other layers of information."}, {"heading": "Acknowledgments", "text": "This work is part of the DS-GA 1010-001 Independent Study in Data Science course at the Center for Data Science at New York University."}], "references": [{"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A Smith"], "venue": "arXiv preprint arXiv:1508.00657", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["John S Bridle"], "venue": "In Neurocomputing,", "citeRegEx": "Bridle.,? \\Q1990\\E", "shortCiteRegEx": "Bridle.", "year": 1990}, {"title": "LSTM networks for sentiment analysis", "author": ["Carrier", "Cho2014] Pierre Luc Carrier", "Kyunghyun Cho"], "venue": "Deep Learning Tutorials", "citeRegEx": "Carrier et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Carrier et al\\.", "year": 2014}, {"title": "Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform", "author": ["Jonathan T. Barron", "George Papandreou", "Kevin Murphy", "Alan L. Yuille"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers et al.2000] Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statis-", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["Yoshua Bengio", "Paolo Frasconi", "Jfirgen Schmidhuber"], "venue": "long-term dependencies,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Ilya Sutskever", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews. arXiv preprint arXiv:1412.5335", "author": ["Marc\u2019Aurelio Ranzato", "Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Mesnil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2014}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["Sainath et al.2015] T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Sainath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts Potts"], "venue": "In EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "From feedforward to recurrent lstm neural networks for language modeling", "author": ["Hermann Ney", "Ralf Schluter"], "venue": null, "citeRegEx": "Sundermeyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2015}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Tang et al.2015] Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Reseg: A recurrent neural network for object segmentation. CoRR, abs/1511.07053", "author": ["Kyle Kastner", "Aaron C. Courville", "Yoshua Bengio", "Matteo Matteucci", "KyungHyun Cho"], "venue": null, "citeRegEx": "Visin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Visin et al\\.", "year": 2015}, {"title": "Backpropagation through time: what does it do and how to do it", "author": ["P. Werbos"], "venue": "In Proceedings of IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advanced in Neural Information Processing Systems (NIPS 2015),", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "More recently, it has become more common to use a deep neural network, which jointly performs feature extraction and classification, for document classification (Kim, 2014; Mesnil et al., 2014; Socher et al., 2013; Carrier and Cho, 2014).", "startOffset": 161, "endOffset": 237}, {"referenceID": 14, "context": "More recently, it has become more common to use a deep neural network, which jointly performs feature extraction and classification, for document classification (Kim, 2014; Mesnil et al., 2014; Socher et al., 2013; Carrier and Cho, 2014).", "startOffset": 161, "endOffset": 237}, {"referenceID": 16, "context": "More recently, it has become more common to use a deep neural network, which jointly performs feature extraction and classification, for document classification (Kim, 2014; Mesnil et al., 2014; Socher et al., 2013; Carrier and Cho, 2014).", "startOffset": 161, "endOffset": 237}, {"referenceID": 10, "context": "(Kim et al., 2015) and Ling et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 13, "context": "(Ling et al., 2015) proposed to use a character sequence as an alternative to the word-level one-hot vector.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "A similar idea was applied to dependency parsing in (Ballesteros et al., 2015).", "startOffset": 52, "endOffset": 78}, {"referenceID": 23, "context": "(Zhang et al., 2015).", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "The character-level convolutional net in (Zhang et al., 2015) is composed of many layers of convolution and max-pooling, similarly to the convolutional network in computer vision (see, e.", "startOffset": 41, "endOffset": 61}, {"referenceID": 12, "context": ", (Krizhevsky et al., 2012).", "startOffset": 2, "endOffset": 27}, {"referenceID": 23, "context": "As the receptive field of each convolutional layer is often small (7 or 3 in (Zhang et al., 2015),) the network must have many layers in order to capture long-term dependencies in an input sentence.", "startOffset": 77, "endOffset": 97}, {"referenceID": 23, "context": "(Zhang et al., 2015) used a very deep convolutional network with six convolutional layers followed by two fully-connected layers.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": ", (Cho et al., 2014; Sundermeyer et al., 2015)) and from the fact that the recurrent network can efficiently capture long-term dependencies even with a single layer.", "startOffset": 2, "endOffset": 46}, {"referenceID": 18, "context": ", (Cho et al., 2014; Sundermeyer et al., 2015)) and from the fact that the recurrent network can efficiently capture long-term dependencies even with a single layer.", "startOffset": 2, "endOffset": 46}, {"referenceID": 5, "context": "Because the recurrent layer, consisting of either gated recurrent units (GRU, (Cho et al., 2014) or long short-term memory units (LSTM, (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 78, "endOffset": 96}, {"referenceID": 6, "context": ", 2014) or long short-term memory units (LSTM, (Hochreiter and Schmidhuber, 1997; Gers et al., 2000), can efficiently capture long-term dependencies, the proposed network only needs a very small number of convolutional layers.", "startOffset": 47, "endOffset": 100}, {"referenceID": 23, "context": "We empirically validate the proposed model, to which we refer as a convolution-recurrent network, on the eight large-scale document classification tasks from (Zhang et al., 2015).", "startOffset": 158, "endOffset": 178}, {"referenceID": 23, "context": "We mainly compare the proposed model against the convolutional network in (Zhang et al., 2015) and show that it is indeed possible to use a much smaller model to achieve the same level of classification performance when a recurrent layer is put on top of the convolutional layers.", "startOffset": 74, "endOffset": 94}, {"referenceID": 1, "context": "This naive recursive function however is known to suffer from the problem of vanishing gradient (Bengio et al., 1994; Hochreiter et al., 2001).", "startOffset": 96, "endOffset": 142}, {"referenceID": 9, "context": "This naive recursive function however is known to suffer from the problem of vanishing gradient (Bengio et al., 1994; Hochreiter et al., 2001).", "startOffset": 96, "endOffset": 142}, {"referenceID": 6, "context": "Long short-term memory (LSTM) unit from (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) is a representative example.", "startOffset": 40, "endOffset": 93}, {"referenceID": 2, "context": "Given a fixed-dimensional input from the lower layer, the classification layer affinetransforms it followed by a softmax activation function (Bridle, 1990) to compute the predictive probabilities for all the categories.", "startOffset": 141, "endOffset": 155}, {"referenceID": 11, "context": "This can be addressed by either simply max-pooling the vectors (Kim, 2014) over the time dimension (for both convolutional and recurrent layers), taking the last hidden state (for recurrent layers) or taking the last hidden states of the forward and reverse recurrent networks (for bidirectional recurrent layers.", "startOffset": 63, "endOffset": 74}, {"referenceID": 11, "context": "Convolutional network for document classification The convolutional networks for document classification, proposed earlier in (Kim, 2014; Zhang et al., 2015) and illustrated in Fig.", "startOffset": 126, "endOffset": 157}, {"referenceID": 23, "context": "Convolutional network for document classification The convolutional networks for document classification, proposed earlier in (Kim, 2014; Zhang et al., 2015) and illustrated in Fig.", "startOffset": 126, "endOffset": 157}, {"referenceID": 19, "context": "Hybrid model: Conv-GRNN Perhaps the most related work is the convolution-gated recurrent neural net (Conv-GRNN) from (Tang et al., 2015).", "startOffset": 117, "endOffset": 136}, {"referenceID": 19, "context": "These are orthogonal to each other, and it is possible to plug in the proposed ConvRec as a sentence feature extraction module in the Conv-GRNN from (Tang et al., 2015).", "startOffset": 149, "endOffset": 168}, {"referenceID": 16, "context": ", (Socher et al., 2013).", "startOffset": 2, "endOffset": 23}, {"referenceID": 23, "context": "For more detailed information on the data set construction process, see (Zhang et al., 2015).", "startOffset": 72, "endOffset": 92}, {"referenceID": 7, "context": "Rectified linear units (ReLUs, (Glorot et al., 2011)) are used as activation functions in the convolutional layers.", "startOffset": 31, "endOffset": 52}, {"referenceID": 17, "context": "Dropout (Srivastava et al., 2014) is an effective way to regularize deep neural networks.", "startOffset": 8, "endOffset": 33}, {"referenceID": 22, "context": "We train our models using AdaDelta (Zeiler, 2012) with \u03c1 = 0.", "startOffset": 35, "endOffset": 49}, {"referenceID": 23, "context": "Our Model (Zhang et al., 2015) Data set # Ex.", "startOffset": 10, "endOffset": 30}, {"referenceID": 21, "context": "The gradient of the cost function is computed with backpropagation through time (BPTT, (Werbos, 1990)).", "startOffset": 87, "endOffset": 101}, {"referenceID": 23, "context": "We compare to the best character-level convolutional model without data augmentation from (Zhang et al., 2015) on each data set.", "startOffset": 90, "endOffset": 110}, {"referenceID": 4, "context": "For example, (Chen et al., 2015; Visin et al., 2015) combined convolution and recurrent layers to tackle image segmentation tasks; (Sainath et al.", "startOffset": 13, "endOffset": 52}, {"referenceID": 20, "context": "For example, (Chen et al., 2015; Visin et al., 2015) combined convolution and recurrent layers to tackle image segmentation tasks; (Sainath et al.", "startOffset": 13, "endOffset": 52}, {"referenceID": 15, "context": ", 2015) combined convolution and recurrent layers to tackle image segmentation tasks; (Sainath et al., 2015) applied a similar model to do speech recognition.", "startOffset": 86, "endOffset": 108}], "year": 2016, "abstractText": "Document classification tasks were primarily tackled at word level. Recent research that works with character-level inputs shows several benefits over word-level approaches such as natural incorporation of morphemes and better handling of rare words. We propose a neural network architecture that utilizes both convolution and recurrent layers to efficiently encode character inputs. We validate the proposed model on eight large scale document classification tasks and compare with character-level convolution-only models. It achieves comparable performances with much less parameters.", "creator": "LaTeX with hyperref package"}}}