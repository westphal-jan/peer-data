{"id": "1602.00426", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2016", "title": "An Iterative Deep Learning Framework for Unsupervised Discovery of Speech Features and Linguistic Units with Applications on Spoken Term Detection", "abstract": "In this work we aim to discover high quality speech features and linguistic units directly from unlabeled speech data in a zero resource scenario. The results are evaluated using the metrics and corpora proposed in the Zero Resource Speech Challenge organized at Interspeech 2015. A Multi-layered Acoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics fof the given corpus and the language behind, thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target Deep Neural Network (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration. We call this iterative deep learning framework the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which generates both high quality speech features for the Track 1 of the Challenge and acoustic tokens for the Track 2 of the Challenge. In addition, we performed extra experiments on the same corpora on the application of query-by-example spoken term detection. The experimental results showed the iterative deep learning framework of MAT-DNN improved the detection performance due to better underlying speech features and acoustic tokens.", "histories": [["v1", "Mon, 1 Feb 2016 08:37:56 GMT  (633kb,D)", "http://arxiv.org/abs/1602.00426v1", "arXiv admin note: text overlap witharXiv:1506.02327"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1506.02327", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["cheng-tao chung", "cheng-yu tsai", "hsiang-hung lu", "chia-hsiang liu", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1602.00426"}, "pdf": {"name": "1602.00426.pdf", "metadata": {"source": "CRF", "title": "AN ITERATIVE DEEP LEARNING FRAMEWORK FOR UNSUPERVISED DISCOVERY OF SPEECH FEATURES AND LINGUISTIC UNITS WITH APPLICATIONS ON SPOKEN TERM DETECTION", "authors": ["Cheng-Tao Chung", "Cheng-Yu Tsai", "Hsiang-Hung Lu", "Chia-Hsiang Liu", "Hung-yi Lee", "Lin-shan Lee"], "emails": [], "sections": [{"heading": null, "text": "Index terms - zero resource, unattended learning, dnn, hmm"}, {"heading": "1. INTRODUCTION", "text": "In the era of big data, huge amounts of raw speech data are easy to obtain, but annotated speech data remains hard to obtain, leading to an increased importance of zero-resource applications where advertised data is not required, such as retrieving spoken terms. The goal of the Zero Resource Speech Challenge is to inspire the development of language technologies under the extreme situation where an entire language needs to be learned from scratch. [2] 3, 4, 5] In this paper, we are developing new approaches to the unattended discovery of language traits and language units."}, {"heading": "2. PROPOSED APPROACH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Overview of the proposed framework", "text": "The framework of the approach is shown in Fig1. In the left part, the multi-layered acoustic tokenizers (MAT) produce many sets of acoustic tokens with uncontrolled HMMs, each describing some aspects of the given corpus, which are specified by two hyperparameters describing the HMM configurations. Each set of acoustic tokens for each configuration is obtained by iterative optimization of the token models and token labels on the given acoustic corpus. Several pairs of hyperparameters have been selected to produce multi-layered token labels for the given corpus, which are used as training targets of the Multitarget Deep Neural Network (MDNN) on the right part of Fig.1, so that the knowledge executed by different token sets at different levels is an active bottleneck."}, {"heading": "2.2. Multi-layered Acoustic Tokenizer(MAT)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1. Unsupervised Token Discovery for Each layer of MAT", "text": "The aim is to obtain in a completely unsupervised manner several sets of acoustic signs, each defined by the hyperparameters \u0432 = (m, n). It is easy to find acoustic signs from the corpus for a selected hyperparameter set = (m, n), which determines the configuration of the HMM (number of states per model and number of unique models) [10, 11, 12, 13, 14]. This can be achieved by first finding an initial designation set \u03c90 on the basis of a set of accepted signs for all characteristics in body X as in (1) [13]. Subsequently, in each iteration step the HMM parameters can be obtained with the previous designation set 1 as in (2) and the new designation set \u0441\u043e\u0441\u043e\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u043e\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u043e\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u043e\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441"}, {"heading": "2.2.2. Granularity Space of Multi-layered Acoustic Token Sets", "text": "The process described above can be performed with different HMM configurations, each characterized by two hyperparameters: the number of states m in each acoustic sign HMM and the total number of unique acoustic signs n during initialization. The state designations of a signal can be considered as temporal segmentation, so that the total number of different acoustic signs represents the phonetic granularity, resulting in a two-dimensional representation of temporal and phonetic granularities as shown in Fig.2. The points in this two-dimensional space in Fig.2 correspond to acoustic sign configurations with different granularities that combine complex knowledge of the corpus and language with two-dimensional parameters."}, {"heading": "2.3. Mutual Reinforcement(MR) of Multi-layered Tokens", "text": "Since all layers of acoustic characters obtained in the above MAT are learned unchecked, they may not be very precise, but we have many layers, each for a specific pair of hyperparameters \u0443 = (m, n), so that they can amplify each other (MR). This is explained here and shown in Fig.3, including the merger of token boundaries and the re-initialization of LDA-based token marks as shown in Fig.3 (a)."}, {"heading": "2.3.1. Token Boundary Fusion", "text": "Fig.3 (b) shows the token limit when a portion of an expression is segmented into acoustic tokens at different levels with different pairs of hyperparameters, usually = (m, n). We define a boundary function bm, n (j) at each level, where the possible boundary between each pair of two adjacent frames within the utterance j is the time index for such possible boundaries. At each level bm, n (j) = 1, if boundary j is a token limit and 0 otherwise. All these boundary functions bm, n (j) for all different levels are then weighted and averaged to give a common boundary function B (j). The weights take into account the fact that smaller m or shorter HMMs produce more boundaries, so that these boundaries should weigh less. The peaks of B (j) are then selected based on the second derivatives and some filtering and delay processes."}, {"heading": "2.3.2. LDA-based Token Label Re-initialization", "text": "As shown in Fig.3 (c), each new segment obtained above usually consists of a sequence of acoustic signs at each level based on the signs defined at that level. We now consider all signs at all different levels to be different words, so we have a vocabulary of MN \u2211 i = 1 ni words, i.e. there are ni words at the i level and there are altogether MN layers. Thus, a new segment is considered here as a document (pouch) consisting of words (tokens) collected from all different layers. Latent dirichlet allocation [15] (LDA) is prepared for theme modeling, and then each document (new segment) is labeled with the most likely topic. As in LDA a topic is characterized by word distribution, here a token distribution across different layers can also represent a certain acoustic characteristic or acoustic symbol."}, {"heading": "2.4. The Multi-target DNN (MDNN)", "text": "As shown in the right part of Fig.1, the sequence of token marking from one level (with hyperparameters \u043d = (m, n) is a valid target for a supervised frame-by-frame training, albeit unattended. In the first work here, we simply take the token label and not the HMM state as the training target. As shown in Fig.1, there are multi-layered token labels with different hyperparameters \u0445 = (m, n) for each utterance offered by MAT on the left, so that we look at all multi-layered token labels together by learning the parameters for a single DNN with a uniformly weighted cross-entropy object at the output level. As a result, the bottleneck feature (BNF) extracted from this DNN automatically merges all knowledge of the corpus and language from the different sets of acoustic tokens."}, {"heading": "2.5. The Iterative Learning Framework for MAT-DNN", "text": "Once the BNFs are extracted from the MDNN in iteration 1, they can be used as input to the MAT to the left of Fig.1 to replace the original acoustic characteristics. MAT then generates updated sets of multi-layer token labels, and these updated sets of multi-layer token labels can be used as updated MDNN training objectives. MDNN input characteristics can also be updated by concatenating the original acoustic characteristics with the newly extracted BNFs as concatenated characteristics, and this process can be repeated. The concatenated feature used as input of the MDNN can be linked by concatenating unattended characteristics that can be supplemented with other approaches such as the Deep Boltzmann Machine [16] (DBM) as downstream characteristics, using Long-Short Memory Recurrent Neural Network [17] (LSTM-RNN)."}, {"heading": "2.6. Spoken Term Detection", "text": "Let us specify {pr, r = 1, 2, 3,.., n} the n acoustic characters in the series of \u0432 = (m, n). First, we construct a distance matrix S of size n \u00b7 n offline for each set of characters. (4) The KL divergence KL (i, j) between two character HMMs in (4) is defined as the symmetrical KL divergence between states based on the variable approximation [19] of the states. In the online phase, for each query q (i, j) entered, we run the following query q and each document (utterance) d in the archive for each query query."}, {"heading": "3. EXPERIMENTAL SETUP", "text": "The MAT-DNN made it possible to carry the MAT-DNN in the following way. We set m = 3, 5, 7, 9 states per token HMM and n = 50, 100, 300, 500 characters in the MAT, the total of 16 layers (m = 11.13) in the MAT and in the MDN, which we used in the 39 dimensions of the MCS coefficients (MFCC) with the energy, the delta and the double delta as starting point for the input into the MDN and the MDN-N."}, {"heading": "4. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Feature Quality in Metrics of Track 1", "text": "The evaluation was based on the ABX discrimination test [28], including improvements to loudspeakers and loudspeakers, and the distortions achieved by performing dynamic time warping on feature sequences of predefined phone pairs were used as distance metrics for the ABX discrimination test. Results in error percentages (the lower the better) are listed in Table 1.Rows (1) and (13), which are the official base line of MFCC features and the official overhead line of telephone posteriorgrams provided by the challenge. Row (3) is used for the DBM posteriorgrams, which are used as the strong unattended base of the MFCC features. The results in rows (4), (5) and (6) have been used to train all systems in this work. Row (3) is used for the DBM posteriorgrams extracted from the MFCC series."}, {"heading": "4.2. Quality of the Discovered Units in Metrics of Track 2", "text": "This year we have it in the hand in which we are, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand."}, {"heading": "4.3. Unsupervised Spoken Term Detection", "text": "The mentioned reactionary tendencies in the reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionMnlpnlrsrteeaF)."}, {"heading": "5. CONCLUSION", "text": "In this paper, we propose an iterative deep-learning framework, MATDNN, to discover high-quality features and multi-layered acoustic token sets in a completely unattended manner. These features and tokens are evaluated by the metrics and corpora defined in the Zero Resource Speech Challenge in Interspeech 2015. We merge the information from different token sets into speech recognition experiments and get good initial results. We hope these results will serve as good references for future research."}, {"heading": "6. ACKNOWLEDGMENT", "text": "We thank Yuan-ming Liou, Yen-Chen Wu and Yen-Ju Lu for providing the DBM posterior programs and i-vectors used in the MAT-DNN of this work."}, {"heading": "7. REFERENCES", "text": "In recent years, it has become clear that the individual countries of the world are not only one country, but also a country in which people are able to integrate and integrate themselves, in which they are able to integrate. [1] In the countries of the EU in which they live and work, it is such that it is a country in which people are able to integrate themselves. [3] In the countries of the world in which they live, it is such as in the USA, in Europe, in Europe, in Europe and in Europe, in the USA and in Europe, in Europe and in Europe, in Europe and in Europe, in Europe and in Europe, in Europe and in Europe, in Europe and in Europe, in Europe and in Europe, in Europe, in Europe and in Europe, in Europe, in Europe and in Europe, in Europe and in Europe, in Europe and in Europe, in Europe and in Europe, in Europe and in the USA, in Europe and in Europe, in Europe and in the USA, in Europe and in Europe, in the USA and in Europe, in Europe and in Europe, in the USA and in the USA, in Europe and in Europe, in the USA and in Europe, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the countries of the countries in the countries of the countries of the countries of the world in the countries of the world in which people are able to integrate themselves. [3]"}], "references": [{"title": "Deep Neural Networks For Acoustic Modeling In Speech Recognition: The Shared Views Of Four Research Groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A Nonparametric Bayesian Approach To Acoustic Model Discovery", "author": ["Chia-ying Lee", "James Glass"], "venue": "Proceedings Of The 50th Annual Meeting Of The Association For Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 40\u201349.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised Training Of An HMM-based Self-Organizing Unit Recognizer With Applications To Topic Classification And Keyword Discovery", "author": ["Man-hung Siu", "Herbert Gish", "Arthur Chan", "William Belfield", "Steve Lowe"], "venue": "Computer Speech & Language, vol. 28, no. 1, pp. 210\u2013223, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised Neural Network Based Feature Extraction Using Weak Top-Down Constraints", "author": ["Herman Kamper", "Micha Elsner", "Aren Jansen", "Sharon Goldwater"], "venue": ".", "citeRegEx": "4", "shortCiteRegEx": null, "year": 0}, {"title": "Segmental Acoustic Indexing For Zero Resource Keyword Search", "author": ["Keith Levin", "Aren Jansen", "Benjamin Van Durme"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised Spoken Term Detection With Spoken Queries By Multi-level Acoustic Patterns With Varying Model Granularity", "author": ["Cheng-Tao Chung", "Chun-an Chan", "Lin-shan Lee"], "venue": "Acoustics, Speech And Signal Processing (ICASSP), 2014 IEEE International Conference On. IEEE, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Enhancing Automatically Discovered Multi-level Acoustic PatternsConsidering Context Consistency with Applications in Spoken Term Detection", "author": ["Cheng-Tao Chung", "Wei-Ning Hsu", "Cheng-Yi Lee", "Lin- Shan Lee"], "venue": "Acoustics, Speech And Signal Processing (ICASSP), 2015 IEEE International Conference On. IEEE, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigating The Learning Effect Of Multilingual Bottle-Neck Features For ASR", "author": ["Ngoc Thang Vu", "Jochen Weiner", "Tanja Schultz"], "venue": "Fifteenth Annual Conference Of The International Speech Communication Association, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "The Language-Independent Bottleneck Features", "author": ["Karel Vesely", "Martin Karafi\u00e1t", "Frantisek Grezl", "Marcel Janda", "Ekaterina Egorova"], "venue": "Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE, 2012, pp. 336\u2013341.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards Unsupervised Training Of Speaker Independent Acoustic Models", "author": ["Aren Jansen", "Kenneth Church"], "venue": "IN- TERSPEECH, 2011, pp. 1693\u20131692.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised Training Of An Hmm-based Speech Recognizer For Topic Classification", "author": ["Herbert Gish", "Man-hung Siu", "Arthur Chan", "William Belfield"], "venue": "INTERSPEECH, 2009, pp. 1935\u20131938.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Improved Topic Classification And Keyword Discovery Using An Hmm-based Speech Recognizer Trained Without Supervision", "author": ["Man-Hung Siu", "Herbert Gish", "Arthur Chan", "William Belfield"], "venue": "INTERSPEECH, 2010, pp. 2838\u2013 2841.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised Discovery Of Linguistic Structure Including Twolevel Acoustic Patterns Using Three Cascaded Stages Of Iterative Optimization", "author": ["Cheng-Tao Chung", "Chun-an Chan", "Lin-shan Lee"], "venue": "Acoustics, Speech And Signal Processing (ICASSP), 2013 IEEE International Conference On. IEEE, 2013, pp. 8081\u20138085.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised Models For Morpheme Segmentation And Morphology Learning", "author": ["Mathias Creutz", "Krista Lagus"], "venue": "ACM Transactions on Speech and Language Processing (TSLP), vol. 4, no. 1, pp. 3, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "the Journal of machine Learning research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Deep Boltzmann Machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton"], "venue": "International Conference On Artificial Intelligence And Statistics, 2009, pp. 448\u2013455.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "I-vector Based Speaker Recognition On Short Utterances", "author": ["Ahilan Kanagasundaram", "Robbie Vogt", "David B Dean", "Sridha Sridharan", "Michael W Mason"], "venue": "Proceedings Of The 12th Annual Conference Of The International Speech Communication Association. International Speech Communication Association (ISCA), 2011, pp. 2341\u20132344.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximating The Kullback Leibler Divergence Between Gaussian Mixture Models", "author": ["John R Hershey", "Peder A Olsen"], "venue": "Acoustics, Speech And Signal Processing, 2007. ICASSP 2007. IEEE International Conference On. IEEE, 2007, vol. 4, pp. IV\u2013317.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "zrst", "author": ["Cheng-Tao Chung"], "venue": "https://github.com/C2Tao/zrst, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "SRILM-an Extensible Language Modeling Toolkit", "author": ["Andreas Stolcke"], "venue": "INTERSPEECH, 2002.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "MALLET: A Machine Learning For Language Toolkit", "author": ["Andrew K McCallum"], "venue": "2002.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "The Kaldi Speech Recognition Toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely"], "venue": "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. Dec. 2011, IEEE Signal Processing Society, IEEE Catalog No.: CFP11SRW-USB.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "libdnn", "author": ["Po-Wei Chou"], "venue": "https://github.com/botonchou/libdnn, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Buckeye Corpus Of Conversational Speech (2nd Release)", "author": ["Mark A Pitt", "Laura Dilley", "Keith Johnson", "Scott Kiesling", "William Raymond", "Elizabeth Hume", "Eric Fosler-Lussier"], "venue": "Columbus, OH: Department of Psychology, Ohio State University, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Evaluating Speech Features With The Minimal-Pair ABX Task: Analysis Of The Classical MFC/PLP Pipeline", "author": ["Thomas Schatz", "Vijayaditya Peddinti", "Francis Bach", "Aren Jansen", "Hynek Hermansky", "Emmanuel Dupoux"], "venue": "INTERSPEECH 2013: 14th Annual Conference Of The International Speech Communication Association, 2013, pp. 1\u20135.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Bridging The Gap Between Speech Technology And Natural Language Processing: An Evaluation Toolbox For Term Discovery Systems", "author": ["Bogdan Ludusan", "Maarten Versteegh", "Aren Jansen", "Guillaume Gravier", "Xuan-Nga Cao", "Mark Johnson", "Emmanuel Dupoux"], "venue": "Language Resources And Evaluation Conference, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient Spoken Term Discovery Using Randomized Algorithms", "author": ["Aren Jansen", "Benjamin Van Durme"], "venue": "Automatic Speech Recognition And Understanding (ASRU), 2011 IEEE Workshop On. IEEE, 2011, pp. 401\u2013406.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "With the dominant paradigm of automatic speech recognition (ASR) technologies being supervised learning [1], speech technologies under the zero resource scenario is a relatively less explored topic.", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "The goal of the Zero Resource Speech Challenge organized in Interspeech 2015 is to inspire the development of speech technologies under the extreme situation where a whole language has to be learned from scratch [2, 3, 4, 5].", "startOffset": 212, "endOffset": 224}, {"referenceID": 2, "context": "The goal of the Zero Resource Speech Challenge organized in Interspeech 2015 is to inspire the development of speech technologies under the extreme situation where a whole language has to be learned from scratch [2, 3, 4, 5].", "startOffset": 212, "endOffset": 224}, {"referenceID": 3, "context": "The goal of the Zero Resource Speech Challenge organized in Interspeech 2015 is to inspire the development of speech technologies under the extreme situation where a whole language has to be learned from scratch [2, 3, 4, 5].", "startOffset": 212, "endOffset": 224}, {"referenceID": 4, "context": "The goal of the Zero Resource Speech Challenge organized in Interspeech 2015 is to inspire the development of speech technologies under the extreme situation where a whole language has to be learned from scratch [2, 3, 4, 5].", "startOffset": 212, "endOffset": 224}, {"referenceID": 5, "context": "The different layer of the tokens carry complementary knowledge about the corpus and the language behind [6], thus can be further mutually reinforced [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "The different layer of the tokens carry complementary knowledge about the corpus and the language behind [6], thus can be further mutually reinforced [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "The multi-layered token labels generated by the MAT are then used as the training targets of a Multi-target Deep Neural Network [8] (MDNN) to learn the framewise bottleneck features [9] (BNFs).", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "The multi-layered token labels generated by the MAT are then used as the training targets of a Multi-target Deep Neural Network [8] (MDNN) to learn the framewise bottleneck features [9] (BNFs).", "startOffset": 182, "endOffset": 185}, {"referenceID": 9, "context": "It is straightforward to discover acoustic tokens from the corpus for a chosen hyperparameter set \u03c8 = (m,n) that determines the HMM configuration (number of states per model m and number of distinct models n) [10, 11, 12, 13, 14].", "startOffset": 209, "endOffset": 229}, {"referenceID": 10, "context": "It is straightforward to discover acoustic tokens from the corpus for a chosen hyperparameter set \u03c8 = (m,n) that determines the HMM configuration (number of states per model m and number of distinct models n) [10, 11, 12, 13, 14].", "startOffset": 209, "endOffset": 229}, {"referenceID": 11, "context": "It is straightforward to discover acoustic tokens from the corpus for a chosen hyperparameter set \u03c8 = (m,n) that determines the HMM configuration (number of states per model m and number of distinct models n) [10, 11, 12, 13, 14].", "startOffset": 209, "endOffset": 229}, {"referenceID": 12, "context": "It is straightforward to discover acoustic tokens from the corpus for a chosen hyperparameter set \u03c8 = (m,n) that determines the HMM configuration (number of states per model m and number of distinct models n) [10, 11, 12, 13, 14].", "startOffset": 209, "endOffset": 229}, {"referenceID": 13, "context": "It is straightforward to discover acoustic tokens from the corpus for a chosen hyperparameter set \u03c8 = (m,n) that determines the HMM configuration (number of states per model m and number of distinct models n) [10, 11, 12, 13, 14].", "startOffset": 209, "endOffset": 229}, {"referenceID": 12, "context": "This can be achieved by first finding an initial label set \u03c90 based on a set of assumed tokens for all features in the corpus X as in (1) [13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 14, "context": "Latent Dirichlet Allocation [15] (LDA) is preformed for topic modeling, and then each document (new segment) is labeled with the most probable topic.", "startOffset": 28, "endOffset": 32}, {"referenceID": 15, "context": "The concatenated feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained with other approaches such as the Deep Boltzmann Machine [16] (DBM) posteriorgrams, Long-Short Term Memory Recurrent Neural Network [17] (LSTM-RNN) autoencoder bottleneck features, and i-vectors [18] trained on MFCC.", "startOffset": 185, "endOffset": 189}, {"referenceID": 16, "context": "The concatenated feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained with other approaches such as the Deep Boltzmann Machine [16] (DBM) posteriorgrams, Long-Short Term Memory Recurrent Neural Network [17] (LSTM-RNN) autoencoder bottleneck features, and i-vectors [18] trained on MFCC.", "startOffset": 260, "endOffset": 264}, {"referenceID": 17, "context": "The concatenated feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained with other approaches such as the Deep Boltzmann Machine [16] (DBM) posteriorgrams, Long-Short Term Memory Recurrent Neural Network [17] (LSTM-RNN) autoencoder bottleneck features, and i-vectors [18] trained on MFCC.", "startOffset": 323, "endOffset": 327}, {"referenceID": 18, "context": "The KL-divergence KL(i, j) between two token HMMs in (4) is defined as the symmetric KL-divergence between the states based on the variational approximation [19] summed over the states.", "startOffset": 157, "endOffset": 161}, {"referenceID": 19, "context": "The MAT was trained using the zrst [20], a python wrapper for the HTK toolkit [21] and srilm [22] that we developed for training unsupervised HMMs with varying model granularity.", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "The MAT was trained using the zrst [20], a python wrapper for the HTK toolkit [21] and srilm [22] that we developed for training unsupervised HMMs with varying model granularity.", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "The LDA model we used in the Mutual Reinforcement was trained by MALLET [23].", "startOffset": 72, "endOffset": 76}, {"referenceID": 22, "context": "The i-vectors were extracted using Kaldi [24].", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "The DBM posteriorgram was extracted using libdnn [25].", "startOffset": 49, "endOffset": 53}, {"referenceID": 24, "context": "The MDNN was trained using Caffe [26].", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "The two corpora used in the Challenge were used here: the Buckeye corpus [27] in English and the NCHLT Xitsonga Speech corpus in Tsonga.", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "The evaluation was based on the ABX discriminability test [28] including across-speaker and within-speaker tests.", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "Track 2 of the Challenge defined a total of 7 evaluation metrics for 3 tasks [29] describing different aspects of the quality of the linguistic unit discovered from the corpora: coverage, Normalized Edit Distance(NED) and matching F-score for the matching task; grouping F-score and type F-score for clustering task; token F-score, boundary F-score for the parsing task.", "startOffset": 77, "endOffset": 81}, {"referenceID": 28, "context": "We selected three typical example token sets (A)(B)(C) out of the many proposed here, and compared them with the JHU baseline [30] in Table 2 including Precision (P), Recall (R) and F-scores (F).", "startOffset": 126, "endOffset": 130}], "year": 2016, "abstractText": "In this work we aim to discover high quality speech features and linguistic units directly from unlabeled speech data in a zero resource scenario. The results are evaluated using the metrics and corpora proposed in the Zero Resource Speech Challenge organized at Interspeech 2015. A Multi-layered Acoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics fof the given corpus and the language behind, thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target Deep Neural Network (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration. We call this iterative deep learning framework the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which generates both high quality speech features for the Track 1 of the Challenge and acoustic tokens for the Track 2 of the Challenge. In addition, we performed extra experiments on the same corpora on the application of query-by-example spoken term detection. The experimental results showed the iterative deep learning framework of MAT-DNN improved the detection performance due to better underlying speech features and acoustic tokens.", "creator": "LaTeX with hyperref package"}}}