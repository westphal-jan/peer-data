{"id": "1602.00554", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2016", "title": "Graph-based Predictable Feature Analysis", "abstract": "We propose a new method for the unsupervised extraction of predictable features from high-dimensional time-series, where high predictability is understood very generically as low variance in the distribution of the next data point given the current one. We show how this objective can be understood in terms of graph embedding as well as how it corresponds to the information-theoretic measure of excess entropy in special cases. Experimentally, we compare the approach to two other algorithms for the extraction of predictable features, namely ForeCA and PFA, and show how it is able to outperform them in certain settings.", "histories": [["v1", "Mon, 1 Feb 2016 15:11:48 GMT  (239kb)", "https://arxiv.org/abs/1602.00554v1", null], ["v2", "Thu, 11 May 2017 12:41:25 GMT  (286kb)", "http://arxiv.org/abs/1602.00554v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bj\\\"orn weghenkel", "asja fischer", "laurenz wiskott"], "accepted": false, "id": "1602.00554"}, "pdf": {"name": "1602.00554.pdf", "metadata": {"source": "CRF", "title": "Graph-based Predictable Feature Analysis", "authors": ["Bj\u00f6rn Weghenkel", "Asja Fischer", "Laurenz Wiskott"], "emails": ["bjoern.weghenkel@rub.de", "fischer@iro.umontreal.ca", "laurenz.wiskott@rub.de"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.00 554v 2 [cs.L G"}, {"heading": "1 Introduction", "text": "In such a scenario, the ability to predict the future is a necessary condition for being useful in any way."}, {"heading": "2 Graph-based Predictable Feature Analysis", "text": "Given is a time series xt-RN, t = 1,.., S, as training data, which is assumed to be generated by a stationary stochastic process (Xt) t of order p. The goal of the GPFA is to find a low-dimensional attribute space for this process by means of an orthogonal transformation A-RN \u00b7 M, which leads to projected random variables Y t = A T Xt with low average variance, which specifies the state of the p previous time steps. To simplify the notation, we use X (p) t to denote the concatenation (X T t,..., X T \u2212 p + 1) T of the p predecessor of Xt + 1. The corresponding state values are vectors in RN \u00b7 p and are denoted by x (p) t."}, {"heading": "2.1 Measuring predictability", "text": "We understand the predictability of the learned attribute space based on the variance of the projected random variable Y t in this space: the lower its average variance given its past p-step, the higher the predictability. We measure this based on the expected covariance matrix Y t + 1, which Y (p) t specifies, and minimize it with respect to its trace, i.e., we minimize the sum of variances in all major directions. (1) Formally, we are looking for the projection matrix A, which leads to a projected stochastic process (Y t) t with minimal trace (E Y (p) t [cov (Y t + 1 | Y (p) t)]. (1) Formally, we call this \"minimization of variance\" in the following. If we make the generally reasonable assumption that p (Y t + 1 | Y (p) t = y (p) t) t is process-related, that the variance of the next Gaussausse order is not sufficient (1)."}, {"heading": "2.2 Estimating predictability", "text": "In practice, the expected value in (1) can be estimated by sampling a time series y1,.., yS from the process (Y t). However, the empirical estimate for the covariance matrices cov (Y t + 1 | Y (p) t = y (p) t), with y (p) t, is not directly available because there could be only one sample of Y (p) t + 1 with the previous state value y (p) t. Therefore, we calculate a k-nearest neighbourhood (kNN) estimate instead. Intuitively, the sample size is increased by also taking into account the k-points that are most similar (e.g. in terms of Euclidean distance) to y (p) t, provided that a distribution p (Y + 1 | Y) t = dynamics (p) t = dynamics (p) that are most similar."}, {"heading": "2.3 Simplifying predictability", "text": "Finding the transformation A that leads to the most predictable (Y t) t in the sense of (1) is made difficult by the fact that predictability can only be assessed after A. The circular nature of this optimization problem motivates the iterated algorithm described in Section 2.8. As a helpful intermediate step, we define a weaker measure of predictability that is based on the input Xt instead of the characteristics Y t and has a closed solution, namely minimization trace (E X (p) t [cov (Y t) + 1 | X (p) t))))) via its k-nearest neighbor estimate (< cov ({yi + 1 | i \u00b2 K (p) t) > t via K (p)))))). (3) Analogous to K (p) t (p) t, the set K (p) t, contains the indices of the closest neighbors of x (t) and p t \u2212 t."}, {"heading": "2.4 Predictability as graph", "text": "Instead of directly optimizing the goal (3), we formulate it in such a way that it can be interpreted as embedding an undirected chart (< 1) (< 1). (< 1) (< 1) (< 1) (< 1) (< 1) (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"K\") + (\"(\" K \").\" (\"(\"). \"(\" (\") (\") (\"(\"). \"(\" (\") (\"). \"(\" (\") (\") (. \"(\") (. \"(\") (\") (.\" (. \") () (.\" (\") (.\") (. \"() (\") (. \"(.\") () (. \"(.\") (. \"(.\") (. \") (.\" (. \") (.\") (.) (. \"(.) (.\" (.) (.) (.) (. \"(.) (.\" (.) (.) (. \"(.) (.\") (.) (. \"(.) (.\" (. \") (.) (.) (.\" (.) (.) (.) (.) (. \"(.\") (. \") (.\" (. \") (.) (.\") (. \"(.\") (. \"(.) (.\" (. \") (.\") (. \") (.\" (. \"(.\") (. \") (.) (.\" (. \") (.\") (.) (. \"(.\") (. \") ("}, {"heading": "2.5 Graph embedding", "text": "To find the orthogonal transformation A = (a1, a2,.., aM), which minimizes (4), have the training data in X = (x1, x2,.., xS) and D = RS \u00b7 S concatenate as a diagonal matrix, where Dii = \u2211 j Wij is the sum of the edge weights connected to the node xi. Next, let L: = D \u2212 W be the chart Laplacian. Then, the minimization of (4) can be found as a minimization of 12S \u2211 i, j = 1Wij \u0445ATxi \u2212 ATxj \u04452 = 12S \u0445 i, j = 1Wij trace ((((A Txi \u2212 ATxj) (ATxj) T) = Trace (S \u0445 i = 1ATxiDiix T i \u2212 S \u0445i, j \u0432i = 1ATxiWijx T \u0432i) = Trace (ATxi \u2212 ATxj) T) = (X \u2212 ATTj) T = (XTA \u2212 ALT) = (XTA = XTA) (XTA) (XTA) = (XT)."}, {"heading": "2.6 Additional heuristics", "text": "The following three heuristics were found to be useful for improving results in practice. < < < < < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p"}, {"heading": "2.7 Algorithm", "text": "Lines starting with (1) and (2) indicate the steps for GPFA (1) and GPFA (2), respectively. 1. Calculate neighborhoods for each x (p) t, t = p,..., S, calculate the index set K (p) t of k nearest neighbors (plus t itself). 2. Construct diagrams (in the future), the connection matrix W to zero. \u2212 For each t = p,. \u2212, S \u2212 1, add edges, either according to (1) Wi + 1, j + 1 x (p), j + 1 x (p) or (2) Wi + 1. Wallet, i + j (1)."}, {"heading": "2.8 Iterated GPFA", "text": "As shown in Section 2.4 above, the core algorithm produces characteristics (Y t) t with a low track (E X (p) t [cov (Y t + 1 | X (p) t)]). In many cases, these characteristics can already be predictable in themselves, i.e. they have a low track (EY (p) t [cov (Y t + 1 | Y (p) t)]). However, there are cases in which the results of both targets may differ significantly (see Figure 2 for an example of such a case). Also, the k-nearest neighbor estimates of the covariances are becoming increasingly unreliable in higher dimensional spaces."}, {"heading": "2.9 Relationship to predictive information", "text": "In this section we will discuss under which conditions the target of GPFA corresponds to the extraction of characteristics with maximum predictive information (Y) = H (Y) = H (Y) = H (Y) = H (Y) = H (Y) = H (Y) = H (Y) = H (Y) = H (Y) + 1) \u2212 H (Y) \u2212 H (Y) t of order (p) t, (10) where H (Y) + 1 = E [\u2212 log p (Y)]]] is entropy andH (Y) + 1 (Y) t of property Y (p) t of property Y (Y) t."}, {"heading": "2.10 Time complexity", "text": "In the following section, we derive the asymptotic time complexity of the GPFA as a function of the number of training samples S, the input dimensions N, the process order p, the output dimensions M, the number of iterations R, and the neighborhood size k.2.10.1 k-next-neighbor searchThe first arithmetically expensive step of the GPFA is the k-next-neighbor search. If we naively assume a brute force approach, it can be realized in O (NpS). This search is repeated for each of the S data points and for each of the R iterations (in N \u00b7 p dimensions for the first iteration and in M \u00b7 p for all others). Therefore, in the worst case, the k-next-neighbor search has a time complexity of O (NpS2 + RMpS2). Of course, there are more efficient approaches for the k-next-neighbor search."}, {"heading": "2.10.2 Matrix multiplications", "text": "The second costly step consists of the matrix multiplications in (8) to calculate the projected graph Laplacian. When multiplying two dense matrices of size l \u00b7 m and m \u00b7 n, we assume the computational costs of O (lmn). If the first matrix is sparse, where L is the number of non-zero elements, we assume O (Ln), resulting in a complexity of O (N2S + LN) for the left side of (8). With GPFA (1), a maximum of L = 2k2S non-elements (according to the edges added to the graph, which are not all unique), with GPFA (2) a maximum of L = 2kS. The right side of (8) can then be ignored, because the complexity of O (N2S + SN) is completely dominated by the left side."}, {"heading": "2.10.3 Eigenvalue decomposition", "text": "For the solution of the eigenvalue problem (8) R-times we assume an additional time complexity of O (RN3). This is also a conservative assumption, since only the first M eigenvectors have to be calculated."}, {"heading": "2.10.4 Overall time complexity", "text": "Taken together, the GPFA has a time complexity of O (NpS2 + RMpS2 + RN2S + RLN + RN3) with L = k2S for GPFA (1) and L = kS for GPFA (2). Relative to the individual variables, this means: O (S2), O (N3), O (M), O (p), O (R) and O (k2) or O (k) for GPFA (1) or GPFA (2), respectively."}, {"heading": "3 Related methods", "text": "In this section, we briefly summarize the algorithms that are most closely related to GPFA, namely SFA, ForeCA, and PFA."}, {"heading": "3.1 SFA", "text": "Although SFA was originally developed to model aspects of the visual cortex, it has also been successfully applied to various technical problems (see [7] for a brief overview), such as state-of-the-art aging [9]. It is one of the few DR algorithms that takes into account the temporal structure of the data. Especially, slow-varying signals can be considered a special case of predictable characteristics [6]. It is also possible to reformulate the slowness principle implemented by SFA with respect to graphical embedding, for example to include marking information in the optimization problem [8]."}, {"heading": "3.2 ForeCA", "text": "In the case of ForeCA [10], (Xt) t is assumed to be a second-order stationary process, and the aim of the algorithm is to find an extraction vector that makes the projected signals Yt = a T Xt as predictable as possible, i.e. has low entropy in its power spectrum. Like SFA, ForeCA has the advantage of being completely free of models and parameters. For the formal definition of the prediction function, we first consider the autovariance function of the signal such (l) = E (Yt \u2212 \u00b5Y) E (Yt \u2212 l \u2212 \u00b5Y), where \u00b5Y is the mean value and the associated autocorrelation function \u03c1Y (l) = \u03b3Y (l) / Y (0). The spectral density of the process can then be calculated as the Fourier transform of the autocorrelation function, i.e., asfY density as a projection."}, {"heading": "3.3 PFA", "text": "The motivation behind PFA is to find an orthogonal transformation < RN \u00b7 M and coefficient matrices Bi-RM \u00b7 M, with i = 1. p, so that the linear, auto-regressive prediction error of order p, < CHATxt \u2212 p-i = 1BiA Txt \u2212 i-2 > t is minimized. However, this is a difficult problem to optimize because the optimal values of A and Bi depend on each other. Therefore, the solution is addressed via a related but simpler optimization problem: Let's not: = (x T \u2212 1,.., x T \u2212 p) T-RN \u00b7 p be a vector containing the p-step history of xt. Let other W-RN \u00b7 N \u00b7 N \u00b7 p contain the coefficients that minimize the error of predicting XT from their own history, i.e. < CHT \u2212 prediction of XT > 2."}, {"heading": "4 Experiments", "text": "We performed experiments 4 with different sets of data to compare GPFA with SFA, ForeCA and PFA. As a starting point, we compared the characteristics extracted from all algorithms with characteristics generated by projection into an arbitrary (i.e. randomly selected) M-dimensional subspace of the N-dimensional vector space of the data. In all experiments, the training set was first whitened and then the same white value transformation was applied to the test set. After the training, the learned projection was used to extract the most predictable M-dimensional signal from the test set with each of the algorithms. The extracted signals were evaluated in terms of empirical predictability (2). The neighborhood size used for this evaluation is hereinafter called q to distinguish them from the neighborhood size k used during the training of GPFA. As there is no natural choice for the different evaluation functions, which effectively result from different q, we have randomly chosen each of the results q = 10."}, {"heading": "4.1 Toy example (\u201cpredictable noise\u201d)", "text": "The dataset contains a certain type of predictable signals, which are difficult for most algorithms to identify. In addition, the example lends itself to getting an impression of runtime constants of the various algorithms resulting from the large O notation in Section 2.10.First, a two-dimensional signal text = (t-t-1) (13) 4GPFA and experiments were implemented in Python 2.7. Code and datasets are published upon acceptance. 5Note that while the algorithms themselves do not depend on any random effects, the generation of the dataset does. Half of the variance in this sequence can be predicted if xt-1 is known (i.e., p = 1), making the noise partially predictable. This two-dimensional signal was augmented with N-2 additional dimensions of normally distributed noise to produce the full dataset."}, {"heading": "4.2 Auditory data", "text": "In the second series of experiments, we focused on short-term Fourier transformations (STFTs) of audio files. Three public audio files (a silent movie, a soundtrack), an ambient sound from a bar, and an ambient sound from a forest) were actually sampled to 22kHz mono. STFTs were computed using the stft Python library and represented the remaining complex values as two real values. For each repetition of the experiment, Strain = 10000 consecutive frames were randomly selected and Stest = 5000 different frames were selected as a test set. PCA was calculated for each training session to obtain 99% of the variance."}, {"heading": "4.3 Visual data", "text": "A third experiment was performed on a visual dataset. We modified the simulator from the Mario AI challenge [14] to return raw visual input in grayscale without text markup, scaled the raw input from 320 x 240 to 160 x 120 dimensions, and then the final data points were taken from a small window of 20 x 20 = 400 pixels at a point where much of the game dynamics occurred (see Figure 8 for an example). As with auditory datasets, for each experiment, Strain = 10000 consecutive training sessions and Stest = 5000 non-overlapping test frames were randomly selected and PCA applied to both, maintaining 99% of the variance. Finally, M predictable components were extracted from each of the algorithms and evaluated for predictability (2). The parameters p and k were re-selected from a range of candidate values to achieve the best results (see Figure 9a-9). Two things are clearly shown in the results from the Mario AI challenge [14]."}, {"heading": "5 Discussion and Future work", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able, in which they are able, in which they are able, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in fact, in which they, in fact, in which they, in fact, are able to put themselves, in a different world, in which they are able to live, in which they are able to live, in which they are able to live, in which they, in which they are able to live, in which they are able to"}, {"heading": "6 Conclusion", "text": "We presented Graph-based Predictable Feature Analysis (GPFA), a new algorithm for unattended learning of predictable features from high-dimensional time series. We proposed to use the variance of the conditional distribution of the next point in time, taking into account the previous points in time, to quantify the predictability of the learned representations and showed how this quantity is related to the information-theoretical measurement of predictive information. As shown, finding the projection that minimizes the proposed predictability size can be reformulated as a problem of graph embedding. Experimentally, GPFA produced very competitive results, especially in auditory STFT datasets, making it a promising candidate for any problem of Dimensionality Reduction (DR) where the data is inherently embedded in time."}], "references": [{"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Neural Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Predictive information", "author": ["William Bialek", "Naftali Tishby"], "venue": "e-print arXiv:cond-mat/9902341,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Predictability, complexity, and learning", "author": ["William Bialek", "Ilya Nemenman", "Naftali Tishby"], "venue": "Neural Computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Spectral regression: A unified approach for sparse subspace learning", "author": ["Deng Cai", "Xiaofei He", "Jiawei Han"], "venue": "In Seventh IEEE International Conference on Data Mining (ICDM", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Non parametric time series analysis and prediction: Uniform almost sure convergence of the window and k-nn autoregression estimates", "author": ["G\u00e9rard Collomb"], "venue": "Statistics: A Journal of Theoretical and Applied Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1985}, {"title": "Predictive coding and the slowness principle: An information-theoretic approach", "author": ["Felix Creutzig", "Henning Sprekeler"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Slow feature analysis: Perspectives for technical applications of a versatile learning algorithm", "author": ["Alberto N. Escalante-B", "Laurenz Wiskott"], "venue": "Ku\u0308nstliche Intelligenz [Artificial Intelligence],", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "How to solve classification and regression problems on high-dimensional data with a supervised extension of slow feature analysis", "author": ["Alberto N. Escalante-B", "Laurenz Wiskott"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Improved graphbased SFA: Information preservation complements the slowness principle", "author": ["Alberto N. Escalante-B", "Laurenz Wiskott"], "venue": "e-print arXiv:1601.03945,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Forecastable component analysis", "author": ["Georg Goerg"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML 2013),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Principal component analysis on non-gaussian dependent data", "author": ["Fang Han", "Han Liu"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML 2013),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Locality preserving projections", "author": ["Xiaofei He", "Partha Niyogi"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Learning state representations with robotic priors", "author": ["Rico Jonschkowski", "Oliver Brock"], "venue": "Autonomous Robots,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "The Mario AI benchmark and competitions", "author": ["Sergey Karakovskiy", "Julian Togelius"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Predictive representations of state", "author": ["Michael L. Littman", "Richard S. Sutton", "Satinder Singh"], "venue": "In Advances in Neural Information Processing Systems (NIPS 2001),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Using predictive representations to improve generalization in reinforcement learning", "author": ["Eddie J. Rafols", "Mark B. Ring", "Richard S. Sutton", "Brian Tanner"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI 2005),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Predictable feature analysis", "author": ["Stefan Richthofer", "Laurenz Wiskott"], "venue": "e-print arXiv:1311.2503,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T. Roweis", "Lawrence K. Saul"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Computational mechanics: Pattern and prediction, structure and simplicity", "author": ["Cosma Rohilla Shalizi", "James P. Crutchfield"], "venue": "Journal of Satistical Physics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Predictive projections", "author": ["Nathan Sprague"], "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "On the relation of slow feature analysis and Laplacian eigenmaps", "author": ["Henning Sprekeler"], "venue": "Neural Computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Information-theoretic approach to interactive learning", "author": ["Susanne Still"], "venue": "Europhysics Letters,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B. Tenenbaum", "Vin de Silva", "John C. Langford"], "venue": "Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "The information bottleneck method", "author": ["Naftali Tishby", "Fernando C. Pereira", "William Bialek"], "venue": "e-print arXiv:physics/0004057,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike von Luxburg"], "venue": "Statistics and Computing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Slow feature analysis: Unsupervised learning of invariances", "author": ["Laurenz Wiskott", "Terrence Sejnowski"], "venue": "Neural Computation,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2002}, {"title": "Graph embedding and extensions: A general framework for dimensionality reduction", "author": ["Shuicheng Yan", "Dong Xu", "Benyu Zhang", "Hong-Jiang Zhang", "Qiang Yang", "Stephen Lin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": "The recently proposed forecastable component analysis (ForeCA) [10] is based on the idea that predictable signals can be recognized by their low entropy in the power spectrum while white noise in contrast would result in a power spectrum with maximal entropy.", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "Predictable feature analysis (PFA) [17] focuses on signals that are well predictable through autoregressive processes.", "startOffset": 35, "endOffset": 39}, {"referenceID": 25, "context": "Another DR approach that was not designed to extract predictable features but explicitly takes into account the temporal structure of the data is slow feature analysis (SFA) [26].", "startOffset": 174, "endOffset": 178}, {"referenceID": 5, "context": "Still, the resulting slow features can be seen as a special case of predictable features [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 19, "context": "For reinforcement learning settings, predictive projections [20] and robotic priors [13] learn mappings where actions applied to similar states result in similar successor states.", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "For reinforcement learning settings, predictive projections [20] and robotic priors [13] learn mappings where actions applied to similar states result in similar successor states.", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "Also, there are recent variants of PCA that at least allow for weak statistical dependence between samples [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 2, "context": "The proposed measure has the advantage of being very generic, of making only few assumptions about the data at hand, and of being easy to link to the information-theoretic quantity of predictive information [3], that is, the mutual information between past and future.", "startOffset": 207, "endOffset": 210}, {"referenceID": 26, "context": "Through its formulation in terms of a graph embedding problem, it can be straightforwardly combined with many other, mainly geometrically motivated objectives that have been formulated in the graph embedding framework [27]\u2014like Isomap [23], Locally Linear Embedding [LLE, 18], Laplacian Eigenmaps [1], and Locality Preserving Projections [LPP, 12].", "startOffset": 218, "endOffset": 222}, {"referenceID": 22, "context": "Through its formulation in terms of a graph embedding problem, it can be straightforwardly combined with many other, mainly geometrically motivated objectives that have been formulated in the graph embedding framework [27]\u2014like Isomap [23], Locally Linear Embedding [LLE, 18], Laplacian Eigenmaps [1], and Locality Preserving Projections [LPP, 12].", "startOffset": 235, "endOffset": 239}, {"referenceID": 0, "context": "Through its formulation in terms of a graph embedding problem, it can be straightforwardly combined with many other, mainly geometrically motivated objectives that have been formulated in the graph embedding framework [27]\u2014like Isomap [23], Locally Linear Embedding [LLE, 18], Laplacian Eigenmaps [1], and Locality Preserving Projections [LPP, 12].", "startOffset": 297, "endOffset": 300}, {"referenceID": 3, "context": "Moreover, GPFA could make use of potential speed-ups like spectral regression [4], include additional label information in its graph like in [8], or could be applied to non-vectorial data like text.", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "Moreover, GPFA could make use of potential speed-ups like spectral regression [4], include additional label information in its graph like in [8], or could be applied to non-vectorial data like text.", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "to auto-regressive time series as well [5].", "startOffset": 39, "endOffset": 42}, {"referenceID": 11, "context": "(7) See [12] for the analogous derivation of the one-dimensional case that was largely adopted here as well as for a kernelized version of the graph embedding.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "Normalized graph embedding First, in the context of graph embedding, the minimization of aXLXa described in the section above is often solved subject to the additional constraint aXDXa = 1 (see for instance [12, 25]).", "startOffset": 207, "endOffset": 215}, {"referenceID": 24, "context": "Normalized graph embedding First, in the context of graph embedding, the minimization of aXLXa described in the section above is often solved subject to the additional constraint aXDXa = 1 (see for instance [12, 25]).", "startOffset": 207, "endOffset": 215}, {"referenceID": 18, "context": "Conceptually this is related to the idea of causal states [19], where all (discrete) states that share the same conditional distribution over possible futures are mapped to the same causal state (also see [22] for a closely related formulation in interactive settings).", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "Conceptually this is related to the idea of causal states [19], where all (discrete) states that share the same conditional distribution over possible futures are mapped to the same causal state (also see [22] for a closely related formulation in interactive settings).", "startOffset": 205, "endOffset": 209}, {"referenceID": 1, "context": ", [2] and [19]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 18, "context": ", [2] and [19]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "1 SFA Although SFA originally has been developed to model aspects of the visual cortex, it has been successfully applied to different problems in technical domains as well (see [7] for a short overview), like, for example, state-of-the art age-estimation [9].", "startOffset": 177, "endOffset": 180}, {"referenceID": 8, "context": "1 SFA Although SFA originally has been developed to model aspects of the visual cortex, it has been successfully applied to different problems in technical domains as well (see [7] for a short overview), like, for example, state-of-the art age-estimation [9].", "startOffset": 255, "endOffset": 258}, {"referenceID": 5, "context": "In particular, slowly varying signals can be seen as a special case of predictable features [6].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "It is also possible to reformulate the slowness principle implemented by SFA in terms of graph embedding, for instance to incorporate label information into the optimization problem [8].", "startOffset": 182, "endOffset": 185}, {"referenceID": 9, "context": "2 ForeCA In case of ForeCA [10], (Xt)t is assumed to be a stationary second-order process and the goal of the algorithm is finding an extraction vector a such that the projected signals Yt = a T Xt are as forecastable as possible, that is, having a low entropy in their power spectrum.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "For details about ForeCA see [10].", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "For further details about PFA see [17].", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "We modified the simulator from the Mario AI challenge [14] to return raw visual input in gray-scale without text labels.", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "This strategy is usually applied to SFA, often in combination with hierarchical stacking of SFA nodes which further increases the non-linearities while at the same time regularizing spatially (on visual data) [7].", "startOffset": 209, "endOffset": 212}, {"referenceID": 26, "context": "We already mentioned above that kernel versions of graph embedding are readily available [27, 4].", "startOffset": 89, "endOffset": 96}, {"referenceID": 3, "context": "We already mentioned above that kernel versions of graph embedding are readily available [27, 4].", "startOffset": 89, "endOffset": 96}, {"referenceID": 20, "context": "This step is repeated\u2014each time with the original graph\u2014 resulting in an embedding for the original graph that is increasingly non-linear with every repetition (see [21] for details).", "startOffset": 165, "endOffset": 169}, {"referenceID": 23, "context": "Another information-theoretic concept relevant in this context (besides predictive information) is that of information bottlenecks [24].", "startOffset": 131, "endOffset": 135}, {"referenceID": 5, "context": "In fact, SFA has been shown to implement a special case of such a past-future information bottleneck for Gaussian variables [6].", "startOffset": 124, "endOffset": 127}, {"referenceID": 14, "context": "That\u2019s why state representations encoding the agent\u2019s future generalize better and allow for more efficient learning of policies than state representations that encode the agent\u2019s past [15, 16].", "startOffset": 185, "endOffset": 193}, {"referenceID": 15, "context": "That\u2019s why state representations encoding the agent\u2019s future generalize better and allow for more efficient learning of policies than state representations that encode the agent\u2019s past [15, 16].", "startOffset": 185, "endOffset": 193}], "year": 2017, "abstractText": "We propose graph-based predictable feature analysis (GPFA), a new method for unsupervised learning of predictable features from high-dimensional time series, where high predictability is understood very generically as low variance in the distribution of the next data point given the previous ones. We show how this measure of predictability can be understood in terms of graph embedding as well as how it relates to the informationtheoretic measure of predictive information in special cases. We confirm the effectiveness of GPFA on different datasets, comparing it to three existing algorithms with similar objectives\u2014namely slow feature analysis, forecastable component analysis, and predictable feature analysis\u2014to which GPFA shows very competitive results.", "creator": "LaTeX with hyperref package"}}}