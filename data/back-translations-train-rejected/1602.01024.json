{"id": "1602.01024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2016", "title": "On Deep Multi-View Representation Learning: Objectives and Optimization", "abstract": "We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for learning while only one view is available for downstream tasks. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a batch-style correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them empirically on image, speech, and text tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE). We also explore a stochastic optimization procedure for minibatch correlation-based objectives and discuss the time/performance trade-offs for kernel-based and neural network-based implementations.", "histories": [["v1", "Tue, 2 Feb 2016 17:51:43 GMT  (1335kb)", "http://arxiv.org/abs/1602.01024v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weiran wang", "raman arora", "karen livescu", "jeff bilmes"], "accepted": false, "id": "1602.01024"}, "pdf": {"name": "1602.01024.pdf", "metadata": {"source": "CRF", "title": "On Deep Multi-View Representation Learning: Objectives and Optimization", "authors": ["Weiran Wang", "Karen Livescu"], "emails": ["WEIRANWANG@TTIC.EDU", "ARORA@CS.JHU.EDU", "KLIVESCU@TTIC.EDU", "BILMES@EE.WASHINGTON.EDU"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.01 02"}, {"heading": "1. Introduction", "text": "In many cases, we have access to multiple \"views\" of data during the training period, while only one view is available during the test period, or for a downstream task. Views can be several measurement modalities, such as simultaneous recording of audio + video (Kidron et al., 2005; Chaudhuri et al., 2009), audio + articulation (Arora and Livescu, 2013; Wang et al., 2015a), images + text (Hardoon et al., 2004; Hodosh et al., 2013; Yan and Mikolajczyk, 2015), or parallel texts in two languages (Vinokourov et al., 2003; Haghighi et al., 2008; Faruqui et al., 2014; Faruqui et al., 2014; Lu et al., 2015), but other information may be extracted from the same source."}, {"heading": "2. DNN-based multi-view feature learning", "text": "In this section, we discuss several existing and new learning approaches with multiple views based on deep, forward-looking neural networks, including their objective functions and optimization procedures. Schematic diagrams summarizing the methods are presented in Fig. 1. Notation. In the multi-view learning scenario, we have access to paired observations from two views, designated (x1, y1),..., (xN, yN), where N is the sample size and xi-RDx and yi-R Dy is for i = 1,.., N. We also designate the data matrices for each view X = [x1,.., xN] and Y = [y1,.., yN]. We use bold letters, e.g. f, to denote multidimensional mappings implemented by DNNs. A DNN f of depth Kf is implemented, f of the form Kf."}, {"heading": "2.1 Split autoencoders (SplitAE)", "text": "Ngiam et al. (2011) propose to extract common representations by learning to reconstruct both views from the one point of view available at the test date. In this approach, the feature extraction network f is divided, while the reconstruction networks p and q are separated for each view. We call this model a split autoencoder (SplitAE), which is schematically represented in Figure 1 (a). The aim of this model is 2. Distortions can be introduced by adding to the input.the sum of reconstruction errors for the two views.The intuition for this model is that the common representation can be extracted from a single view and can be used to reconstruct all views.3 The autoencode loss of the incicent (f (xi)) -q (f (f (xi))) -2. The intuition for this model is that the common representation can be extracted from a single view and used to reconstruct all views."}, {"heading": "2.2 Deep canonical correlation analysis (DCCA)", "text": "This year, he has found himself in the position in which he is able to retaliate, in the position in which he is able to retaliate, in the position in which he is able to retaliate, in the position in which he is able to be in, in the position in which he is able to be in, in the position in which he is able to be in, in the position in which he is able to be in, in the position in which he is able to unite."}, {"heading": "2.3 Deep canonically correlated autoencoders (DCCAE)", "text": "Inspired by both CCA and reconstruction-based objectives, we propose a new model consisting of two autoencoders that optimizes the combination of the canonical correlation between the learned \"bottleneck\" representations and the reconstruction errors of the autoencoders. In other words, we optimize the following objectives in Wf, Wg, Wp, Wq, U, V \u2212 1 N tr (U f (X) g (Y) V) + NN i = 1 xi \u2212 p (f (xi))."}, {"heading": "2.4 Correlated autoencoders (CorrAE)", "text": "In the next approach, we remove the uncorrelation limitations of the DCCAE target and leave only the sum of the scalar correlations between pairs of learned dimensions and the reconstruction error term to test how important the original CCA constraints are. We call this model correlated autocoders (CorrAE), which are also represented by Figure 1 (c). Its target can be asmin Wf, Wg, Wp, Wq, U, V \u2212 1 N tr (U f (X) g (Y) V) + \u03bbNN, i = 1, xi \u2212 p (f (xi), 2 + 2, yi \u2212 q (yi)), 2 (3) s.t. u i f (X) ui = v i g (Y) i (Y) vi = N, 1 \u2264 L.E, where there is a trade-off parameter."}, {"heading": "2.5 Minimum-distance autoencoders (DistAE)", "text": "The CCA target can be seen as minimizing the distance between the learned projections of the two views while satisfying the white constraints on the projections (Hardoon et al., 2004).The constraints make it difficult to optimize the CCA-based targets as explained above.This observation motivates us to consider additional targets that are resolved in sums via training examples, while maintaining the intuition of the CCA target as a reconstruction error between two mappings.Here we consider two variants that we call minimum distance autoencoders (DistAE).The first variant DistAE-1 optimizes the following target: min Wf, Wg, WQ1NN."}, {"heading": "3. Related work", "text": "Here, we focus on related work on multiview feature learning using neural networks and CCA's kernel extension."}, {"heading": "3.1 Neural network feature extraction using CCA-like objectives", "text": "There are several approaches to depicting people associated with a goal similar to CCA. Assuming that the two views have a common cause (e.g. depth), the 1-dimensional output of each network can be considered an \"internal learning signal.\""}, {"heading": "3.2 Kernel CCA", "text": "It is obvious (1) reduced to linear CCA if f and g are the identity mappings. (1) A popular nonlinear approach is the way in which KCCA (KCCA) and g (Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Hardoon et al., 2004; KCCA, 2001; KCCA, 2001; KCCA, 2001; KCCA, 2001; KCCA, 2001; KCCA, 2001; KCCA, 2001; KCCA, 2001; KCCA, 2001; KCCA, 2001; KCCA, 2001; KCCA, 2001; KCCA, 2001; KCCA, 2001; KCCA, 2001; KCCA, 2001."}, {"heading": "3.3 Other related models", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "4. Experiments", "text": "We will first demonstrate the proposed algorithms and related work on several multidimensional learning tasks (Sections 4.1-4.3). In our setting, the second view is not available during the test period, so we will try to learn a functional transformation of the first / primary view that gathers useful information from the second view with a paired training set of two views. Then, in Section 4.4, we will examine the stochastic optimization process for the DCCA target (1). We will focus on several downstream tasks, including noisy digital image classification, speech recognition, and semantic similarity of word pairs. In these tasks, we will compare the following methods in the multidimensional learning environment: \u2022 DNN-based models, including SplitAE, CorrAE, DCCA, DCCAE, and DistAE. \u2022 Linear CCA (CCA), which correspond to the internal similarities of DCCA, with only a linear network without hidden layers for both views."}, {"heading": "4.1 Noisy MNIST digits", "text": "In this task, we generate two-dimensional data using the MNIST dataset (LeCun et al., 1998), which consists of 28 x 28 grayscale images, with 60K / 10K images for training / examination. We then create a more sophisticated version of the dataset as follows (see fig. 2 for examples). We first rescale the values of the individual graphics to [0, 1] (by splitting the original values into [0, 255] by 255). We then randomly rotate the images from different angles, which are uniformly sampled from [\u2212 \u03c0 / 4], and the resulting images are used as input. For each view 1 image, we randomly select an image of the same identity (0-9) from the original dataset, we add independent random noise sampled from [0, 1] to each pixel, and the corresponding pixel end values to [0, 1]."}, {"heading": "4.2 Acoustic-articulatory data for speech recognition", "text": "Next, we will experiment with the Wisconsin X-Ray Microbeam (XRMB) Corpus (Westbury, 1994) of simultaneously recorded voice and articulation measurements of 47 American speakers. Similarly, we have followed the setup of Arora and Livescu (2013) and use the learned features for phonetic recognition when tested on audio alone (Arora and Livescu, 2013), the inputs to multiview functions are acoustic features (39D features consisting of the frequency coefficients of MFCCs) and their first and second derivatives (horizontal / vertical displacement of 8 pellets)."}, {"heading": "4.3 Multilingual data for word embeddings", "text": "We follow the setup of Faruqui and Dyer (2014) and use 640-dimensional monolingual word vectors as input, which are analyzed via latent semantic analysis on the WMT 2011 monolingual. We evaluate learned characteristics on two groups of tasks and use the same 36K English-German word pairs for multiview learning. The learned mappings are applied to the original English word embeddings (180K words) and the projections are used for evaluation. We evaluate learned characteristics on two groups of tasks. The first group consists of the four word similarity tasks of Faruqui and Dyer: WS-353 and the two splits WS-REL and WS-REL, RG-65, MC-30, and MTurk-287."}, {"heading": "4.4 Empirical analysis of DCCA optimization", "text": "It is a question of whether and in what form, how and in what manner and in what manner the people in the USA, in Europe, in Europe, in Europe, in the world, in the world, in the USA, in Europe, in the world, in the USA, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the"}, {"heading": "5. Conclusion", "text": "We have found that CCA-based models outperform autoencoder-based models (SplitAE) and models based on squaring the circle. We have examined these goals in the context of DNNs, but we expect the same trends to be applied to other network architectures, such as conventional (LeCun et al., 1998) and recursive (Elman, 1990) networks, and that is a direction for future work."}, {"heading": "Appendix A. Analysis of stochastic optimization for DCCA", "text": "Our SGD-like optimization in relation to the individual network levels (see SGD-3) works as follows: \"We have only a limited number of training places.\" (\"We.\") (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\" (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\" (. \"We.\"). (\"We.\" (. \"We.\"). (\"We.\" We. \"(.\" We. \"). (.\" We. \"We.\" (. \"We.\"). (. \"We.\" (. \"We.\"). (\"We.\"). (\"We. (.\"). (\"We. (.\" We. \").\" (\"). (\" We. (. \"We.\"). (. (\"We.\"). (\"). (\" We. (\"We.\"). (\"We. (.\"). (\"We.\"). (. (\"We. (\"). (\"We.\"). (\"We. (.\"). (\"We.\"). (. (.). (\"We.\"). (\"We. (.\"). (\"). (. (\" We. (.). (\"We.). (\" We. \"). (. (.). (\" We. \"We. (. (.). (. (.). (.). (.). (. (\" We. (.). (\"We. (.).). (. (\" We.). (. (\"We.). (. (.). (. (."}, {"heading": "Acknowledgment", "text": "The authors thank Louis Goldstein for the phonetic alignment of the data used in the recognition experiment, Manaal Faruqui, Chris Dyer, Ang Lu, Mohit Bansal, and Kevin Gimpel for sharing resources for the multilingual embedding experiments, Nati Srebro for their contributions to the stochastic optimization of DCCA, and Geoff Hinton for the helpful discussion of the CCA goal."}], "references": [{"title": "A kernel method for canonical correlation analysis", "author": ["Shotaro Akaho"], "venue": "In Proceedings of the International Meeting of the Psychometric Society (IMPS2001). Springer-Verlag,", "citeRegEx": "Akaho.,? \\Q2001\\E", "shortCiteRegEx": "Akaho.", "year": 2001}, {"title": "Deep canonical correlation analysis", "author": ["Galen Andrew", "Raman Arora", "Jeff Bilmes", "Karen Livescu"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Andrew et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Kernel CCA for multi-view learning of acoustic features using articulatory measurements", "author": ["Raman Arora", "Karen Livescu"], "venue": "In Symposium on Machine Learning in Speech and Language Processing (MLSLP),", "citeRegEx": "Arora and Livescu.,? \\Q2012\\E", "shortCiteRegEx": "Arora and Livescu.", "year": 2012}, {"title": "Multi-view CCA-based acoustic features for phonetic recognition across speakers and domains", "author": ["Raman Arora", "Karen Livescu"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201913),", "citeRegEx": "Arora and Livescu.,? \\Q2013\\E", "shortCiteRegEx": "Arora and Livescu.", "year": 2013}, {"title": "Multi-view learning with supervision for transformed bottleneck features", "author": ["Raman Arora", "Karen Livescu"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201914),", "citeRegEx": "Arora and Livescu.,? \\Q2014\\E", "shortCiteRegEx": "Arora and Livescu.", "year": 2014}, {"title": "Kernel independent component analysis", "author": ["Francis R. Bach", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bach and Jordan.,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2002}, {"title": "A probabilistic interpretation of canonical correlation analysis", "author": ["Francis R. Bach", "Michael I. Jordan"], "venue": "Technical Report 688,", "citeRegEx": "Bach and Jordan.,? \\Q2005\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2005}, {"title": "Learning a Mahalanobis metric from equivalence constraints", "author": ["Aharon Bar-Hillel", "Tomer Hertz", "Noam Shental", "Daphna Weinshall"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bar.Hillel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bar.Hillel et al\\.", "year": 2005}, {"title": "Self-organizing neural network that discovers surfaces in random-dot stereograms", "author": ["Suzanna Becker", "Geoffrey E. Hinton"], "venue": "Nature, 355:161\u2013163,", "citeRegEx": "Becker and Hinton.,? \\Q1992\\E", "shortCiteRegEx": "Becker and Hinton.", "year": 1992}, {"title": "Multi-view clustering", "author": ["Steffen Bickel", "Tobias Scheffer"], "venue": "In Proc. of the 4th IEEE Int. Conf. Data Mining (ICDM\u201904),", "citeRegEx": "Bickel and Scheffer.,? \\Q2004\\E", "shortCiteRegEx": "Bickel and Scheffer.", "year": 2004}, {"title": "On the regularization of canonical correlation analysis", "author": ["Tijl De Bie", "Bart De Moor"], "venue": "Int. Sympos. ICA and BSS,", "citeRegEx": "Bie and Moor.,? \\Q2003\\E", "shortCiteRegEx": "Bie and Moor.", "year": 2003}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Blacoe and Lapata.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "Correlational spectral clustering", "author": ["Mathew B. Blaschko", "Christoph H. Lampert"], "venue": "In Proc. of the 2008 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Blaschko and Lampert.,? \\Q2008\\E", "shortCiteRegEx": "Blaschko and Lampert.", "year": 2008}, {"title": "Canonical correlation: A tutorial", "author": ["Magnus Borga"], "venue": "Available at http://www.imt.liu.se/ magnus/cca/tutorial/tutorial.pdf,", "citeRegEx": "Borga.,? \\Q2001\\E", "shortCiteRegEx": "Borga.", "year": 2001}, {"title": "The tradeoffs of large scale learning", "author": ["Leon Bottou", "Olivier Bousquet"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bottou and Bousquet.,? \\Q2008\\E", "shortCiteRegEx": "Bottou and Bousquet.", "year": 2008}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Sarath Chandar", "Stanislas Lauly", "Hugo Larochelle", "Mitesh M. Khapra", "Balaraman Ravindran", "Vikas Raykar", "Amrita Saha"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chandar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "Correlational neural networks", "author": ["Sarath Chandar", "Mitesh M. Khapra", "Hugo Larochelle", "Balaraman Ravindran"], "venue": "[cs.CL], April", "citeRegEx": "Chandar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2015}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Trans. Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["Kamalika Chaudhuri", "Sham M. Kakade", "Karen Livescu", "Karthik Sridharan"], "venue": "In Proc. of the 26th Int. Conf. Machine Learning", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Information bottleneck for Gaussian variables", "author": ["Gal Chechik", "Amir Globerson", "Naftali Tishby", "Yair Weiss"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chechik et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2005}, {"title": "Information-theoretic metric learning", "author": ["Jason V. Davis", "Brian Kulis", "Prateek Jain", "Suvrit Sra", "Inderjit S. Dhillon"], "venue": "In Proc. of the 24th Int. Conf. Machine Learning", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Multi-view learning of word embeddings via CCA", "author": ["Paramveer Dhillon", "Dean Foster", "Lyle Ungar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer"], "venue": "In Proc. EACL,", "citeRegEx": "Faruqui and Dyer.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Multi-view dimensionality reduction via canonical correlation analysis", "author": ["Dean P. Foster", "Rie Johnson", "Sham M. Kakade", "Tong Zhang"], "venue": "Technical report,", "citeRegEx": "Foster et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2009}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S. Roweis"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Globerson and Roweis.,? \\Q2006\\E", "shortCiteRegEx": "Globerson and Roweis.", "year": 2006}, {"title": "Euclidean embedding of co-occurrence data", "author": ["Amir Globerson", "Gal Chechik", "Fernando Pereira", "Naftali Tishby"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Globerson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2007}, {"title": "Neighbourhood components analysis", "author": ["Jacob Goldberger", "Sam Roweis", "Geoff Hinton", "Ruslan Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Goldberger et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2005}, {"title": "Linear Algebra for Signal Processing, volume 69 of The IMA Volumes in Mathematics and its Applications, chapter The Canonical Correlations of Matrix Pairs and their Numerical Computation, pages 27\u201349", "author": ["Gene H. Golub", "Hongyuan Zha"], "venue": null, "citeRegEx": "Golub and Zha.,? \\Q1995\\E", "shortCiteRegEx": "Golub and Zha.", "year": 1995}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["Aria Haghighi", "Percy Liang", "Taylor Berg-Kirkpatrick", "Dan Klein"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Haghighi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["David R. Hardoon", "Sandor Szedmak", "John Shawe-Taylor"], "venue": "Neural Computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Multilingual distributed representations without word alignment", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proc. of the 2nd Int. Conf. Learning Representations (ICLR", "citeRegEx": "Hermann and Blunsom.,? \\Q2014\\E", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "Tandem connectionist feature extraction for conventional HMM systems", "author": ["Hynek Hermansky", "Daniel P.W. Ellis", "Sangita Sharma"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc", "citeRegEx": "Hermansky et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hermansky et al\\.", "year": 2000}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Learning distance metrics with contextual constraints for image retrieval", "author": ["Steven C.H. Hoi", "Wei Liu", "Michael R. Lyu", "Wei-Ying Ma"], "venue": "In Proc. of the 2006 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Hoi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hoi et al\\.", "year": 2006}, {"title": "Relations between two sets of variates", "author": ["Harold Hotelling"], "venue": "Biometrika, 28(3/4):321\u2013377,", "citeRegEx": "Hotelling.,? \\Q1936\\E", "shortCiteRegEx": "Hotelling.", "year": 1936}, {"title": "Nonlinear canonical correlation analysis by neural networks", "author": ["W.W. Hsieh"], "venue": "Neural Networks,", "citeRegEx": "Hsieh.,? \\Q2000\\E", "shortCiteRegEx": "Hsieh.", "year": 2000}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In Proc. of the 22nd Int. Conf. Information and Knowledge Management", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Kernel methods match deep neural networks on TIMIT: Scalable learning in high-dimensional random Fourier spaces", "author": ["Po-Sen Huang", "Haim Avron", "Tara Sainath", "Vikas Sindhwani", "Bhuvana Ramabhadran"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201914),", "citeRegEx": "Huang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["Sham M. Kakade", "Dean P. Foster"], "venue": "In Proc. of the 20th Annual Conference on Learning Theory (COLT\u201907),", "citeRegEx": "Kakade and Foster.,? \\Q2007\\E", "shortCiteRegEx": "Kakade and Foster.", "year": 2007}, {"title": "Pixels that sound", "author": ["Einat Kidron", "Yoav Y. Schechner", "Michael Elad"], "venue": "In Proc. of the 2005 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Kidron et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kidron et al\\.", "year": 2005}, {"title": "Learning semantics with deep belief network for cross-language information retrieval", "author": ["Jungi Kim", "Jinseok Nam", "Iryna Gurevych"], "venue": "In Proceedings of COLING 2012: Posters,", "citeRegEx": "Kim et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2012}, {"title": "A neural implementation of canonical correlation analysis", "author": ["Pei Ling Lai", "Colin Fyfe"], "venue": "Neural Networks,", "citeRegEx": "Lai and Fyfe.,? \\Q1999\\E", "shortCiteRegEx": "Lai and Fyfe.", "year": 1999}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["Pei Ling Lai", "Colin Fyfe"], "venue": "Int. J. Neural Syst.,", "citeRegEx": "Lai and Fyfe.,? \\Q2000\\E", "shortCiteRegEx": "Lai and Fyfe.", "year": 2000}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Randomized nonlinear component analysis", "author": ["David Lopez-Paz", "Suvrit Sra", "Alex Smola", "Zoubin Ghahramani", "Bernhard Schoelkopf"], "venue": "In Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "Lopez.Paz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2014}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proc. NAACL,", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Large scale canonical correlation analysis with iterative least squares", "author": ["Yichao Lu", "Dean P. Foster"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lu and Foster.,? \\Q2014\\E", "shortCiteRegEx": "Lu and Foster.", "year": 2014}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Zhuang Ma", "Yichao Lu", "Dean Foster"], "venue": "In Proc. of the 32nd Int. Conf. Machine Learning (ICML", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Introduction to Information Retrieval", "author": ["Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Correlated random features for fast semi-supervised learning", "author": ["Brian McWilliams", "David Balduzzi", "Joachim Buhmann"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "McWilliams et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McWilliams et al\\.", "year": 2013}, {"title": "Nonlinear feature extraction using generalized canonical correlation analysis", "author": ["Thomas Melzer", "Michael Reiter", "Horst Bischof"], "venue": "In Proc. of the 11th Int. Conf. Artificial Neural Networks", "citeRegEx": "Melzer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Melzer et al\\.", "year": 2001}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell and Lapata.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Algorithms for the assignment and transportation problems", "author": ["James Munkres"], "venue": "J. SIAM,", "citeRegEx": "Munkres.,? \\Q1957\\E", "shortCiteRegEx": "Munkres.", "year": 1957}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2002}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Ng"], "venue": "In Proc. of the 28th Int. Conf. Machine Learning (ICML", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T. Roweis", "Lawrence K. Saul"], "venue": null, "citeRegEx": "Roweis and Saul.,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul.", "year": 2000}, {"title": "Code available at http://www.cs.ubc.ca/ \u0303schmidtm/Software/minFunc.html", "author": ["Mark Schmidt"], "venue": "minFunc,", "citeRegEx": "Schmidt.,? \\Q2012\\E", "shortCiteRegEx": "Schmidt.", "year": 2012}, {"title": "Learning a distance metric from relative comparisons", "author": ["Matthew Schultz", "Thorsten Joachims"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Schultz and Joachims.,? \\Q2004\\E", "shortCiteRegEx": "Schultz and Joachims.", "year": 2004}, {"title": "Adjustment learning and relevant component analysis", "author": ["Noam Shental", "Tomer Hertz", "Daphna Weinshall", "Misha Pavel"], "venue": "In Proc. 7th European Conf. Computer Vision (ECCV\u201902),", "citeRegEx": "Shental et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Shental et al\\.", "year": 2002}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["Richard Socher", "Fei-Fei Li"], "venue": "In Proc. of the 2010 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Socher and Li.,? \\Q2010\\E", "shortCiteRegEx": "Socher and Li.", "year": 2010}, {"title": "Improved multimodal deep learning with variation of information", "author": ["Kihyuk Sohn", "Wenling Shang", "Honglak Lee"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sohn et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2014}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava and Salakhutdinov.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava and Salakhutdinov.", "year": 2014}, {"title": "The information bottleneck method", "author": ["Naftali Tishby", "Fernando Pereira", "William Bialek"], "venue": "In Proc. 37th Annual Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "Tishby et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Tishby et al\\.", "year": 1999}, {"title": "User-friendly tools for random matrices: An introduction", "author": ["J.A. Tropp"], "venue": "NIPS Tutorials,", "citeRegEx": "Tropp.,? \\Q2012\\E", "shortCiteRegEx": "Tropp.", "year": 2012}, {"title": "Distance metric learning with kernels", "author": ["Ivor W. Tsang", "James T. Kwok"], "venue": "In Proc. of the 13th Int. Conf. Artificial Neural Networks", "citeRegEx": "Tsang and Kwok.,? \\Q2003\\E", "shortCiteRegEx": "Tsang and Kwok.", "year": 2003}, {"title": "Visualizing data using t-SNE", "author": ["Laurens J.P. van der Maaten", "Geoffrey E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Inferring a semantic representation of text via cross-language correlation analysis", "author": ["Alexei Vinokourov", "Nello Cristianini", "John Shawe-Taylor"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Vinokourov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Vinokourov et al\\.", "year": 2003}, {"title": "Partial-Hessian strategies for fast learning of nonlinear embeddings", "author": ["Max Vladymyrov", "Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "In Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "Vladymyrov and Carreira.Perpi\u00f1\u00e1n.,? \\Q2012\\E", "shortCiteRegEx": "Vladymyrov and Carreira.Perpi\u00f1\u00e1n.", "year": 2012}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201915),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "On deep multi-view representation learning", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes"], "venue": "In Proc. of the 32nd Int. Conf. Machine Learning (ICML 2015),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Stochastic optimization for deep cca via nonlinear orthogonal iterations", "author": ["Weiran Wang", "Raman Arora", "Nati Srebro", "Karen Livescu"], "venue": "In 53nd Annual Allerton Conference on Communication, Control and Computing,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q. Weinberger", "Lawrence K. Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Weinberger and Saul.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul.", "year": 2009}, {"title": "X-Ray Microbeam Speech Production Database User\u2019s Handbook Version 1.0", "author": ["John R. Westbury"], "venue": "Waisman Center on Mental Retardation & Human Development,", "citeRegEx": "Westbury.,? \\Q1994\\E", "shortCiteRegEx": "Westbury.", "year": 1994}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["Christopher K.I. Williams", "Matthias Seeger"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Williams and Seeger.,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger.", "year": 2001}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["Eric Xing", "Andrew Ng", "Michael Jordan", "Stuart Russell"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Xing et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2003}, {"title": "Document clustering based on non-negative matrix factorization", "author": ["Wei Xu", "Xin Liu", "Yihong Gong"], "venue": "In Proc. of the 26th ACM Conf. Research and Development in Information Retrieval (SIGIR", "citeRegEx": "Xu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2003}, {"title": "Deep correlation for matching images and text", "author": ["Fei Yan", "Krystian Mikolajczyk"], "venue": "In Proc. of the 2015 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Yan and Mikolajczyk.,? \\Q2015\\E", "shortCiteRegEx": "Yan and Mikolajczyk.", "year": 2015}, {"title": "Nystr\u00f6m method vs random Fourier features: A theoretical and empirical comparison", "author": ["Tianbao Yang", "Yu-Feng Li", "Mehrdad Mahdavi", "Rong Jin", "Zhi-Hua Zhou"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 42, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video (Kidron et al., 2005; Chaudhuri et al., 2009), audio + articulation (Arora and Livescu, 2013; Wang et al.", "startOffset": 96, "endOffset": 141}, {"referenceID": 18, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video (Kidron et al., 2005; Chaudhuri et al., 2009), audio + articulation (Arora and Livescu, 2013; Wang et al.", "startOffset": 96, "endOffset": 141}, {"referenceID": 3, "context": ", 2009), audio + articulation (Arora and Livescu, 2013; Wang et al., 2015a), images + text (Hardoon et al.", "startOffset": 30, "endOffset": 75}, {"referenceID": 30, "context": ", 2015a), images + text (Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al.", "startOffset": 24, "endOffset": 115}, {"referenceID": 63, "context": ", 2015a), images + text (Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al.", "startOffset": 24, "endOffset": 115}, {"referenceID": 35, "context": ", 2015a), images + text (Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al.", "startOffset": 24, "endOffset": 115}, {"referenceID": 81, "context": ", 2015a), images + text (Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al.", "startOffset": 24, "endOffset": 115}, {"referenceID": 71, "context": ", 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al., 2003; Haghighi et al., 2008; Chandar et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015), but may also be different information extracted from the same source, such as words + context (Pennington et al.", "startOffset": 70, "endOffset": 181}, {"referenceID": 29, "context": ", 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al., 2003; Haghighi et al., 2008; Chandar et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015), but may also be different information extracted from the same source, such as words + context (Pennington et al.", "startOffset": 70, "endOffset": 181}, {"referenceID": 15, "context": ", 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al., 2003; Haghighi et al., 2008; Chandar et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015), but may also be different information extracted from the same source, such as words + context (Pennington et al.", "startOffset": 70, "endOffset": 181}, {"referenceID": 23, "context": ", 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al., 2003; Haghighi et al., 2008; Chandar et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015), but may also be different information extracted from the same source, such as words + context (Pennington et al.", "startOffset": 70, "endOffset": 181}, {"referenceID": 48, "context": ", 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al., 2003; Haghighi et al., 2008; Chandar et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015), but may also be different information extracted from the same source, such as words + context (Pennington et al.", "startOffset": 70, "endOffset": 181}, {"referenceID": 58, "context": ", 2015), but may also be different information extracted from the same source, such as words + context (Pennington et al., 2014) or document text + text of 1", "startOffset": 103, "endOffset": 128}, {"referenceID": 9, "context": "inbound hyperlinks (Bickel and Scheffer, 2004).", "startOffset": 19, "endOffset": 46}, {"referenceID": 41, "context": "Under certain assumptions, theoretical results exist showing the advantages of multi-view techniques for downstream tasks (Kakade and Foster, 2007; Foster et al., 2009; Chaudhuri et al., 2009).", "startOffset": 122, "endOffset": 192}, {"referenceID": 24, "context": "Under certain assumptions, theoretical results exist showing the advantages of multi-view techniques for downstream tasks (Kakade and Foster, 2007; Foster et al., 2009; Chaudhuri et al., 2009).", "startOffset": 122, "endOffset": 192}, {"referenceID": 18, "context": "Under certain assumptions, theoretical results exist showing the advantages of multi-view techniques for downstream tasks (Kakade and Foster, 2007; Foster et al., 2009; Chaudhuri et al., 2009).", "startOffset": 122, "endOffset": 192}, {"referenceID": 71, "context": "Experimentally, prior work has shown the benefit of multi-view methods on tasks such as retrieval (Vinokourov et al., 2003; Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013), clustering (Blaschko and Lampert, 2008; Chaudhuri et al.", "startOffset": 98, "endOffset": 187}, {"referenceID": 30, "context": "Experimentally, prior work has shown the benefit of multi-view methods on tasks such as retrieval (Vinokourov et al., 2003; Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013), clustering (Blaschko and Lampert, 2008; Chaudhuri et al.", "startOffset": 98, "endOffset": 187}, {"referenceID": 63, "context": "Experimentally, prior work has shown the benefit of multi-view methods on tasks such as retrieval (Vinokourov et al., 2003; Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013), clustering (Blaschko and Lampert, 2008; Chaudhuri et al.", "startOffset": 98, "endOffset": 187}, {"referenceID": 35, "context": "Experimentally, prior work has shown the benefit of multi-view methods on tasks such as retrieval (Vinokourov et al., 2003; Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013), clustering (Blaschko and Lampert, 2008; Chaudhuri et al.", "startOffset": 98, "endOffset": 187}, {"referenceID": 12, "context": ", 2013), clustering (Blaschko and Lampert, 2008; Chaudhuri et al., 2009), and classification/recognition (Dhillon et al.", "startOffset": 20, "endOffset": 72}, {"referenceID": 18, "context": ", 2013), clustering (Blaschko and Lampert, 2008; Chaudhuri et al., 2009), and classification/recognition (Dhillon et al.", "startOffset": 20, "endOffset": 72}, {"referenceID": 21, "context": ", 2009), and classification/recognition (Dhillon et al., 2011; Arora and Livescu, 2013; Ngiam et al., 2011).", "startOffset": 40, "endOffset": 107}, {"referenceID": 3, "context": ", 2009), and classification/recognition (Dhillon et al., 2011; Arora and Livescu, 2013; Ngiam et al., 2011).", "startOffset": 40, "endOffset": 107}, {"referenceID": 57, "context": ", 2009), and classification/recognition (Dhillon et al., 2011; Arora and Livescu, 2013; Ngiam et al., 2011).", "startOffset": 40, "endOffset": 107}, {"referenceID": 33, "context": "One type of objective is based on deep autoencoders (Hinton and Salakhutdinov, 2006), where the objective is to learn a compact representation that best reconstructs the inputs.", "startOffset": 52, "endOffset": 84}, {"referenceID": 57, "context": "This approach has been shown to be effective for speech and vision tasks (Ngiam et al., 2011).", "startOffset": 73, "endOffset": 93}, {"referenceID": 13, "context": "The CCA objective has been studied extensively and has a number of useful properties and interpretations (Borga, 2001; Bach and Jordan, 2002, 2005; Chechik et al., 2005), and the optimal linear projection mappings can be obtained by solving an eigenvalue system of a matrix whose dimensions equal the input dimensionalities.", "startOffset": 105, "endOffset": 169}, {"referenceID": 19, "context": "The CCA objective has been studied extensively and has a number of useful properties and interpretations (Borga, 2001; Bach and Jordan, 2002, 2005; Chechik et al., 2005), and the optimal linear projection mappings can be obtained by solving an eigenvalue system of a matrix whose dimensions equal the input dimensionalities.", "startOffset": 105, "endOffset": 169}, {"referenceID": 45, "context": "To overcome the limiting power of linear projections, a nonlinear extension\u2013kernel canonical correlation analysis (KCCA)\u2013has also been proposed (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004).", "startOffset": 144, "endOffset": 243}, {"referenceID": 0, "context": "To overcome the limiting power of linear projections, a nonlinear extension\u2013kernel canonical correlation analysis (KCCA)\u2013has also been proposed (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004).", "startOffset": 144, "endOffset": 243}, {"referenceID": 53, "context": "To overcome the limiting power of linear projections, a nonlinear extension\u2013kernel canonical correlation analysis (KCCA)\u2013has also been proposed (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004).", "startOffset": 144, "endOffset": 243}, {"referenceID": 5, "context": "To overcome the limiting power of linear projections, a nonlinear extension\u2013kernel canonical correlation analysis (KCCA)\u2013has also been proposed (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004).", "startOffset": 144, "endOffset": 243}, {"referenceID": 30, "context": "To overcome the limiting power of linear projections, a nonlinear extension\u2013kernel canonical correlation analysis (KCCA)\u2013has also been proposed (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004).", "startOffset": 144, "endOffset": 243}, {"referenceID": 71, "context": "CCA and KCCA have long been the workhorse for multi-view feature learning and dimensionality reduction (Vinokourov et al., 2003; Kakade and Foster, 2007; Socher and Li, 2010; Dhillon et al., 2011).", "startOffset": 103, "endOffset": 196}, {"referenceID": 41, "context": "CCA and KCCA have long been the workhorse for multi-view feature learning and dimensionality reduction (Vinokourov et al., 2003; Kakade and Foster, 2007; Socher and Li, 2010; Dhillon et al., 2011).", "startOffset": 103, "endOffset": 196}, {"referenceID": 63, "context": "CCA and KCCA have long been the workhorse for multi-view feature learning and dimensionality reduction (Vinokourov et al., 2003; Kakade and Foster, 2007; Socher and Li, 2010; Dhillon et al., 2011).", "startOffset": 103, "endOffset": 196}, {"referenceID": 21, "context": "CCA and KCCA have long been the workhorse for multi-view feature learning and dimensionality reduction (Vinokourov et al., 2003; Kakade and Foster, 2007; Socher and Li, 2010; Dhillon et al., 2011).", "startOffset": 103, "endOffset": 196}, {"referenceID": 44, "context": "Several alternative nonlinear CCA-like approaches based on neural networks have also been proposed (Lai and Fyfe, 1999; Hsieh, 2000), but the full DNN extension of CCA, termed deep CCA (DCCA, Andrew et al.", "startOffset": 99, "endOffset": 132}, {"referenceID": 38, "context": "Several alternative nonlinear CCA-like approaches based on neural networks have also been proposed (Lai and Fyfe, 1999; Hsieh, 2000), but the full DNN extension of CCA, termed deep CCA (DCCA, Andrew et al.", "startOffset": 99, "endOffset": 132}, {"referenceID": 57, "context": "1 Split autoencoders (SplitAE) Ngiam et al. (2011) propose to extract shared representations by learning to reconstruct both views from the one view that is available at test time.", "startOffset": 31, "endOffset": 51}, {"referenceID": 1, "context": "2 Deep canonical correlation analysis (DCCA) Andrew et al. (2013) propose a DNN extension of CCA termed deep CCA (DCCA; see Fig.", "startOffset": 45, "endOffset": 66}, {"referenceID": 10, "context": ",vL] are the CCA directions that project the DNN outputs and (rx, ry) > 0 are regularization parameters added to the diagonal of the sample autocovariance matrices (Bie and Moor, 2003; Hardoon et al., 2004).", "startOffset": 164, "endOffset": 206}, {"referenceID": 30, "context": ",vL] are the CCA directions that project the DNN outputs and (rx, ry) > 0 are regularization parameters added to the diagonal of the sample autocovariance matrices (Bie and Moor, 2003; Hardoon et al., 2004).", "startOffset": 164, "endOffset": 206}, {"referenceID": 1, "context": "While this is in principle not needed, it is crucial for practical algorithmic implementations such as ours, and matches the original formulation of DCCA (Andrew et al., 2013).", "startOffset": 154, "endOffset": 175}, {"referenceID": 1, "context": ", 2015a, with the gradient formulas given as in Andrew et al., 2013). Intuitively, this approach works because a large minibatch 3. The authors also propose a bimodal deep autoencoder combining DNN transformed features from both views; this model is more natural for the multimodal fusion setting, where both views are available at test time, but can also be used in our multi-view setting (see Ngiam et al., 2011). Empirically, however, Ngiam et al. (2011) report that SplitAE tends to work better in the multi-view setting than bimodal autoencoders.", "startOffset": 48, "endOffset": 458}, {"referenceID": 13, "context": "Interpretations CCA maximizes the mutual information between the projected views for certain distributions (Borga, 2001), while training an autoencoder to minimize reconstruction error amounts to maximizing a lower bound on the mutual information between inputs and learned features (Vincent et al.", "startOffset": 107, "endOffset": 120}, {"referenceID": 70, "context": "Interpretations CCA maximizes the mutual information between the projected views for certain distributions (Borga, 2001), while training an autoencoder to minimize reconstruction error amounts to maximizing a lower bound on the mutual information between inputs and learned features (Vincent et al., 2010).", "startOffset": 283, "endOffset": 305}, {"referenceID": 30, "context": "5 Minimum-distance autoencoders (DistAE) The CCA objective can be seen as minimizing the distance between the learned projections of the two views, while satisfying the whitening constraints for the projections (Hardoon et al., 2004).", "startOffset": 211, "endOffset": 233}, {"referenceID": 65, "context": "There has also been work on multi-view feature learning using deep Boltzmann machines (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014).", "startOffset": 86, "endOffset": 141}, {"referenceID": 64, "context": "There has also been work on multi-view feature learning using deep Boltzmann machines (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014).", "startOffset": 86, "endOffset": 141}, {"referenceID": 8, "context": ", depth is the common cause for adjacent patches of images), Becker and Hinton (1992) propose to maximize a sample-based estimate of mutual information between outputs of neural networks for the two views.", "startOffset": 61, "endOffset": 86}, {"referenceID": 8, "context": ", depth is the common cause for adjacent patches of images), Becker and Hinton (1992) propose to maximize a sample-based estimate of mutual information between outputs of neural networks for the two views. In this work, the 1-dimensional output of each network can be considered to be \u201cinternal teaching signals\u201d for the other. It is less straightforward to extend their sample-based estimator of mutual information to higher dimensions, while the CCA objective is always closely related to maximal mutual information between the views (under the joint multivariate Gaussian distributions of the inputs, see Borga, 2001). Lai and Fyfe (1999) propose to optimize the correlation (rather than canonical correlation) between the outputs of networks for each view, subject to scale constraints on each output dimension.", "startOffset": 61, "endOffset": 642}, {"referenceID": 8, "context": ", depth is the common cause for adjacent patches of images), Becker and Hinton (1992) propose to maximize a sample-based estimate of mutual information between outputs of neural networks for the two views. In this work, the 1-dimensional output of each network can be considered to be \u201cinternal teaching signals\u201d for the other. It is less straightforward to extend their sample-based estimator of mutual information to higher dimensions, while the CCA objective is always closely related to maximal mutual information between the views (under the joint multivariate Gaussian distributions of the inputs, see Borga, 2001). Lai and Fyfe (1999) propose to optimize the correlation (rather than canonical correlation) between the outputs of networks for each view, subject to scale constraints on each output dimension. Instead of directly solving this constrained formulation, the authors apply Lagrangian relaxation and solve the resulting unconstrained objective using SGD. Note, however, that their objective is different from that of CCA, as there are no constraints that the learned dimensions within each view be uncorrelated. Hsieh (2000) proposes a neural network-based model involving three modules: one module for extracting a pair of maximally correlated one-dimensional features for the two views; and a second and third module for reconstructing the original inputs of the two views from the learned features.", "startOffset": 61, "endOffset": 1143}, {"referenceID": 8, "context": ", depth is the common cause for adjacent patches of images), Becker and Hinton (1992) propose to maximize a sample-based estimate of mutual information between outputs of neural networks for the two views. In this work, the 1-dimensional output of each network can be considered to be \u201cinternal teaching signals\u201d for the other. It is less straightforward to extend their sample-based estimator of mutual information to higher dimensions, while the CCA objective is always closely related to maximal mutual information between the views (under the joint multivariate Gaussian distributions of the inputs, see Borga, 2001). Lai and Fyfe (1999) propose to optimize the correlation (rather than canonical correlation) between the outputs of networks for each view, subject to scale constraints on each output dimension. Instead of directly solving this constrained formulation, the authors apply Lagrangian relaxation and solve the resulting unconstrained objective using SGD. Note, however, that their objective is different from that of CCA, as there are no constraints that the learned dimensions within each view be uncorrelated. Hsieh (2000) proposes a neural network-based model involving three modules: one module for extracting a pair of maximally correlated one-dimensional features for the two views; and a second and third module for reconstructing the original inputs of the two views from the learned features. In this model, the feature dimensions can be learned one after another, each learned using as input the reconstruction residual from previous dimensions. This approach is intuitively similar to CorrAE and DCCA, but the three modules are each trained separately, so there is no unified objective. Kim et al. (2012) propose an algorithm that first uses deep belief networks and the autoencoder objective to extract features for two languages independently, and then applies linear CCA to the learned features (activations at the bottleneck layer of the autoencoders) to learn the final representation.", "startOffset": 61, "endOffset": 1734}, {"referenceID": 0, "context": "One popular nonlinear approach is kernel CCA (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004), corresponding to choosing f(x) and g(y) to be feature maps induced by positive definite kernels kx(\u00b7, \u00b7) and ky(\u00b7, \u00b7), respectively (e.", "startOffset": 45, "endOffset": 150}, {"referenceID": 53, "context": "One popular nonlinear approach is kernel CCA (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004), corresponding to choosing f(x) and g(y) to be feature maps induced by positive definite kernels kx(\u00b7, \u00b7) and ky(\u00b7, \u00b7), respectively (e.", "startOffset": 45, "endOffset": 150}, {"referenceID": 5, "context": "One popular nonlinear approach is kernel CCA (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004), corresponding to choosing f(x) and g(y) to be feature maps induced by positive definite kernels kx(\u00b7, \u00b7) and ky(\u00b7, \u00b7), respectively (e.", "startOffset": 45, "endOffset": 150}, {"referenceID": 30, "context": "One popular nonlinear approach is kernel CCA (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004), corresponding to choosing f(x) and g(y) to be feature maps induced by positive definite kernels kx(\u00b7, \u00b7) and ky(\u00b7, \u00b7), respectively (e.", "startOffset": 45, "endOffset": 150}, {"referenceID": 47, "context": "Two widely used approximation techniques are random Fourier features (Lopez-Paz et al., 2014) and the Nystr\u00f6m approximation (Williams and Seeger, 2001).", "startOffset": 69, "endOffset": 93}, {"referenceID": 78, "context": ", 2014) and the Nystr\u00f6m approximation (Williams and Seeger, 2001).", "startOffset": 38, "endOffset": 65}, {"referenceID": 5, "context": "Other approximation techniques such as incomplete Cholesky decomposition (Bach and Jordan, 2002), partial Gram-Schmidt (Hardoon et al.", "startOffset": 73, "endOffset": 96}, {"referenceID": 30, "context": "Other approximation techniques such as incomplete Cholesky decomposition (Bach and Jordan, 2002), partial Gram-Schmidt (Hardoon et al., 2004), incremental SVD (Arora and Livescu, 2012) have also been proposed and applied to KCCA.", "startOffset": 119, "endOffset": 141}, {"referenceID": 2, "context": ", 2004), incremental SVD (Arora and Livescu, 2012) have also been proposed and applied to KCCA.", "startOffset": 25, "endOffset": 50}, {"referenceID": 49, "context": "Although recently iterative algorithms have been introduced for very large CCA problems (Lu and Foster, 2014), they are aimed at sparse matrices and do not have a natural out-of-sample extension.", "startOffset": 88, "endOffset": 109}, {"referenceID": 79, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 62, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 7, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 68, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 61, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 27, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 36, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 25, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 20, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 76, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 60, "context": "In this sense the CCA setting is more similar to that of Shental et al. (2002) and Bar-Hillel et al.", "startOffset": 57, "endOffset": 79}, {"referenceID": 7, "context": "(2002) and Bar-Hillel et al. (2005), which use only side information regarding groups of similar samples (\u201cchunklets\u201d) for single-view data.", "startOffset": 11, "endOffset": 36}, {"referenceID": 7, "context": "(2002) and Bar-Hillel et al. (2005), which use only side information regarding groups of similar samples (\u201cchunklets\u201d) for single-view data. For multi-view data, Globerson et al. (2007) propose an algorithm for learning Euclidean embeddings by defining a joint or conditional distribution of the views based on Euclidean distance 9", "startOffset": 11, "endOffset": 186}, {"referenceID": 31, "context": "Recently, there has been increasing interest in learning (multi-view) representations using contrastive losses which aim to enforce that distances between dissimilar pairs are larger than distances between similar pairs by some margin (Hermann and Blunsom, 2014; Huang et al., 2013).", "startOffset": 235, "endOffset": 282}, {"referenceID": 39, "context": "Recently, there has been increasing interest in learning (multi-view) representations using contrastive losses which aim to enforce that distances between dissimilar pairs are larger than distances between similar pairs by some margin (Hermann and Blunsom, 2014; Huang et al., 2013).", "startOffset": 235, "endOffset": 282}, {"referenceID": 66, "context": "Finally, CCA has a connection with the information bottleneck method (Tishby et al., 1999).", "startOffset": 69, "endOffset": 90}, {"referenceID": 19, "context": "Indeed, in the case of Gaussian variables, the information bottleneck method finds the same subspace as CCA (Chechik et al., 2005).", "startOffset": 108, "endOffset": 130}, {"referenceID": 47, "context": "The first implementation, denoted FKCCA, uses random Fourier features (Lopez-Paz et al., 2014) and the second implementation, denoted NKCCA, uses the Nystr\u00f6m approximation (Williams and Seeger, 2001).", "startOffset": 70, "endOffset": 94}, {"referenceID": 78, "context": ", 2014) and the second implementation, denoted NKCCA, uses the Nystr\u00f6m approximation (Williams and Seeger, 2001).", "startOffset": 85, "endOffset": 112}, {"referenceID": 82, "context": "2, in both FKCCA/NKCCA, we transform the original inputs to an M -dimensional feature space where the inner products between samples approximate the kernel similarities (Yang et al., 2012).", "startOffset": 169, "endOffset": 188}, {"referenceID": 46, "context": "1 Noisy MNIST digits In this task, we generate two-view data using the MNIST dataset (LeCun et al., 1998), which consists of 28 \u00d7 28 grayscale digit images, with 60K/10K images for training/testing.", "startOffset": 85, "endOffset": 105}, {"referenceID": 56, "context": "We use spectral clustering (Ng et al., 2002) so as to account for possibly non-convex cluster shapes.", "startOffset": 27, "endOffset": 44}, {"referenceID": 80, "context": "We measure clustering performance with two criteria, clustering accuracy (AC) and normalized mutual information (NMI) (Xu et al., 2003; Manning et al., 2008).", "startOffset": 118, "endOffset": 157}, {"referenceID": 51, "context": "We measure clustering performance with two criteria, clustering accuracy (AC) and normalized mutual information (NMI) (Xu et al., 2003; Manning et al., 2008).", "startOffset": 118, "endOffset": 157}, {"referenceID": 55, "context": "a linear assignment problem using the Hungarian algorithm (Munkres, 1957).", "startOffset": 58, "endOffset": 73}, {"referenceID": 33, "context": "The networks (f ,p) are pre-trained in a layerwise manner using restricted Boltzmann machines (Hinton and Salakhutdinov, 2006) and similarly for (g,q) with inputs from the corresponding view.", "startOffset": 94, "endOffset": 126}, {"referenceID": 17, "context": "We train one-versus-one linear SVMs (Chang and Lin, 2011) on the projected training set (now using the ground truth labels), and test on the projected test set, while using the projected tuning set for selecting the SVM hyperparameter (the penalty parameter for hinge loss).", "startOffset": 36, "endOffset": 57}, {"referenceID": 45, "context": "For KCCAs, we fix both rx and ry at a small positive value of 10\u22124 (as suggested by Lopez-Paz et al. (2014), FKCCA is robust to rx, ry), and do grid search for the Gaussian kernel width over {2, 3, 4, 5, 6, 8, 10, 15, 20} for view 1 and {2.", "startOffset": 84, "endOffset": 108}, {"referenceID": 72, "context": "We run the t-SNE implementation of Vladymyrov and Carreira-Perpi\u00f1\u00e1n (2012) on projected test images for 300 iterations with a perplexity of 20.", "startOffset": 35, "endOffset": 75}, {"referenceID": 77, "context": "2 Acoustic-articulatory data for speech recognition We next experiment with the Wisconsin X-Ray Microbeam (XRMB) corpus (Westbury, 1994) of simultaneously recorded speech and articulatory measurements from 47 American English speakers.", "startOffset": 120, "endOffset": 136}, {"referenceID": 3, "context": "Multi-view feature learning via CCA/KCCA/DCCA has previously been shown to improve phonetic recognition performance when tested on audio alone (Arora and Livescu, 2013; Wang et al., 2015a).", "startOffset": 143, "endOffset": 188}, {"referenceID": 2, "context": "Multi-view feature learning via CCA/KCCA/DCCA has previously been shown to improve phonetic recognition performance when tested on audio alone (Arora and Livescu, 2013; Wang et al., 2015a). We follow the setup of Arora and Livescu (2013) and use the learned features for speakerindependent phonetic recognition.", "startOffset": 144, "endOffset": 238}, {"referenceID": 2, "context": "Multi-view feature learning via CCA/KCCA/DCCA has previously been shown to improve phonetic recognition performance when tested on audio alone (Arora and Livescu, 2013; Wang et al., 2015a). We follow the setup of Arora and Livescu (2013) and use the learned features for speakerindependent phonetic recognition. Similarly to Arora and Livescu (2013), the inputs to multi-view feature learning are acoustic features (39D features consisting of mel frequency cepstral coefficients (MFCCs) and their first and second derivatives) and articulatory features (horizontal/vertical displacement of 8 pellets attached to different parts of the vocal tract) concatenated over a 7-frame window around each frame, giving 273D acoustic inputs and 112D articulatory inputs for each view.", "startOffset": 144, "endOffset": 350}, {"referenceID": 32, "context": "All of the learned feature types are used in a tandem approach (Hermansky et al., 2000), i.", "startOffset": 63, "endOffset": 87}, {"referenceID": 2, "context": "The recognizer has one 3-state left-to-right HMM per phone, using the same language model as in Arora and Livescu (2013). For each fold, we select the best hyperparameters based on recognition accuracy on the tuning speakers, and use the corresponding learned model for the test speakers.", "startOffset": 96, "endOffset": 121}, {"referenceID": 2, "context": "The recognizer has one 3-state left-to-right HMM per phone, using the same language model as in Arora and Livescu (2013). For each fold, we select the best hyperparameters based on recognition accuracy on the tuning speakers, and use the corresponding learned model for the test speakers. As before, models based on neural networks are trained via SGD with the optimization parameters tuned by grid search. Here we do not use pre-training for weight initialization. A small weight decay parameter of 5\u00d7 10\u22124 is used for all layers. For each algorithm, the feature dimensionality L is tuned over {30, 50, 70}. For DNN-based models, we use hidden layers of 1500 ReLUs. For DCCA, we vary the network depths (up to 3 nonlinear hidden layers) of each view. In the best DCCA architecture, f has 3 ReLU layers of 1500 units followed by a linear output layer while g has only a linear output layer. For CorrAE/DistAE/DCCAE, we use the same architecture of DCCA for the encoders, and we set the decoders to have symmetric architectures to the encoders. For this domain, we find that the best choice of architecture for the encoders/decoders for View 2 is linear while for View 1 it is typically three layers deep. For SplitAE, the encoder f is similarly deep and the View 1 decoder p has the symmetric architecture, while its View 2 decoder q was set to linear to match the best choice for the other methods. We fix (rx, ry) to small values as before. The trade-off parameter \u03bb is tuned for each algorithm by grid search. For FKCCA, we find it important to use a large number of random features M to get a competitive result, consistent with the findings of Huang et al. (2014) when using random Fourier features for speech data.", "startOffset": 96, "endOffset": 1669}, {"referenceID": 23, "context": "We follow the setup of Faruqui and Dyer (2014) and use as inputs 640-dimensional monolingual word vectors trained via latent semantic analysis on the WMT 2011 monolingual news corpora and use the same 36K English-German word pairs for multi-view learning.", "startOffset": 23, "endOffset": 47}, {"referenceID": 23, "context": "We follow the setup of Faruqui and Dyer (2014) and use as inputs 640-dimensional monolingual word vectors trained via latent semantic analysis on the WMT 2011 monolingual news corpora and use the same 36K English-German word pairs for multi-view learning. The learned mappings are applied to the original English word embeddings (180K words) and the projections are used for evaluation. We evaluate learned features on two groups of tasks. The first group consists of the four word similarity tasks from Faruqui and Dyer (2014): WS-353 and the two splits WS-SIM and WS-REL, RG-65, MC-30, and MTurk-287.", "startOffset": 23, "endOffset": 528}, {"referenceID": 23, "context": "We follow the setup of Faruqui and Dyer (2014) and use as inputs 640-dimensional monolingual word vectors trained via latent semantic analysis on the WMT 2011 monolingual news corpora and use the same 36K English-German word pairs for multi-view learning. The learned mappings are applied to the original English word embeddings (180K words) and the projections are used for evaluation. We evaluate learned features on two groups of tasks. The first group consists of the four word similarity tasks from Faruqui and Dyer (2014): WS-353 and the two splits WS-SIM and WS-REL, RG-65, MC-30, and MTurk-287. The second group of tasks uses the adjective-noun (AN) and verbobject (VN) subsets from the bigram similarity dataset of Mitchell and Lapata (2010), and tuning and test splits (of size 649/1972) for each subset (we exclude the noun-noun subset as we find that the NN human annotations often reflect \u201ctopical\u201d rather than \u201cfunctional\u201d similarity).", "startOffset": 23, "endOffset": 751}, {"referenceID": 11, "context": "of the bigram, as done in prior work (Blacoe and Lapata, 2012).", "startOffset": 37, "endOffset": 62}, {"referenceID": 14, "context": "It is clear that for small minibatches (100, 200), the objective quickly plateaus at a low value, whereas for large enough minibatch size, there is always a steep increase at the beginning, which is a known advantage of stochastic firstorder algorithms (Bottou and Bousquet, 2008), and a wide range of learning rate/momentum give very similar results.", "startOffset": 253, "endOffset": 280}, {"referenceID": 28, "context": "(2015c) have proposed a nonlinear orthogonal iterations (NOI) algorithm for the DCCA objective which extends the alternating least squares procedure (Golub and Zha, 1995; Lu and Foster, 2014) and its stochastic version (Ma et al.", "startOffset": 149, "endOffset": 191}, {"referenceID": 49, "context": "(2015c) have proposed a nonlinear orthogonal iterations (NOI) algorithm for the DCCA objective which extends the alternating least squares procedure (Golub and Zha, 1995; Lu and Foster, 2014) and its stochastic version (Ma et al.", "startOffset": 149, "endOffset": 191}, {"referenceID": 50, "context": "(2015c) have proposed a nonlinear orthogonal iterations (NOI) algorithm for the DCCA objective which extends the alternating least squares procedure (Golub and Zha, 1995; Lu and Foster, 2014) and its stochastic version (Ma et al., 2015) for CCA.", "startOffset": 219, "endOffset": 236}, {"referenceID": 13, "context": "It is clear that for small minibatches (100, 200), the objective quickly plateaus at a low value, whereas for large enough minibatch size, there is always a steep increase at the beginning, which is a known advantage of stochastic firstorder algorithms (Bottou and Bousquet, 2008), and a wide range of learning rate/momentum give very similar results. The reason for such behavior is that the stochastic estimate of the DCCA objective becomes more accurate as minibatch size n increases. We provide theoretical analysis of the error between the true objective and its stochastic estimate in Appendix A. Recently, Wang et al. (2015c) have proposed a nonlinear orthogonal iterations (NOI) algorithm for the DCCA objective which extends the alternating least squares procedure (Golub and Zha, 1995; Lu and Foster, 2014) and its stochastic version (Ma et al.", "startOffset": 254, "endOffset": 633}, {"referenceID": 13, "context": "It is clear that for small minibatches (100, 200), the objective quickly plateaus at a low value, whereas for large enough minibatch size, there is always a steep increase at the beginning, which is a known advantage of stochastic firstorder algorithms (Bottou and Bousquet, 2008), and a wide range of learning rate/momentum give very similar results. The reason for such behavior is that the stochastic estimate of the DCCA objective becomes more accurate as minibatch size n increases. We provide theoretical analysis of the error between the true objective and its stochastic estimate in Appendix A. Recently, Wang et al. (2015c) have proposed a nonlinear orthogonal iterations (NOI) algorithm for the DCCA objective which extends the alternating least squares procedure (Golub and Zha, 1995; Lu and Foster, 2014) and its stochastic version (Ma et al., 2015) for CCA. Each iteration of the NOI algorithm adaptively estimates the covariance matrices of the projections of each view, whitens the projections of a minibatch using the estimated covariance matrices, and takes a gradient step over DNN weight parameters of the nonlinear least squares problems of regressing each view\u2019s input against the whitened projection of the other view for the minibatch. The advantage of NOI is that it performs well with smaller minibatch sizes and thus reduces memory consumption. Wang et al. (2015c) have shown that NOI can achieve the same objective value as STO using smaller minibatches.", "startOffset": 254, "endOffset": 1391}, {"referenceID": 1, "context": "Our split of the data is the same as the one used by Andrew et al. (2013). We note that Lopez-Paz et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 1, "context": "Our split of the data is the same as the one used by Andrew et al. (2013). We note that Lopez-Paz et al. (2014) used the same speaker in their experiments, but they randomly shuffled all 50K frames before creating the splits.", "startOffset": 53, "endOffset": 112}, {"referenceID": 1, "context": "Our split of the data is the same as the one used by Andrew et al. (2013). We note that Lopez-Paz et al. (2014) used the same speaker in their experiments, but they randomly shuffled all 50K frames before creating the splits. We suspect that DCCA (as well as their own algorithm) were under-tuned in their experiments. We ran experiments on a randomly shuffled dataset with careful tuning of kernel widths for FKCCA/NKCCA (using rank M = 6000) and obtained canonical correlations of 99.2/105.6/107.6 for FKCCA/NKCCA/DCCA, which are better than those reported in Lopez-Paz et al. (2014). 20", "startOffset": 53, "endOffset": 586}, {"referenceID": 60, "context": "We use the L-BFGS implementation of Schmidt (2012), which includes a good line-search procedure.", "startOffset": 36, "endOffset": 51}, {"referenceID": 46, "context": "We have studied these objectives in the context of DNNs, but we expect that the same trends should apply also to other network architectures such as convolutional (LeCun et al., 1998) and recurrent (Elman, 1990; Hochreiter and Schmidhuber, 1997) networks, and this is one direction for future work.", "startOffset": 163, "endOffset": 183}, {"referenceID": 22, "context": ", 1998) and recurrent (Elman, 1990; Hochreiter and Schmidhuber, 1997) networks, and this is one direction for future work.", "startOffset": 22, "endOffset": 69}, {"referenceID": 34, "context": ", 1998) and recurrent (Elman, 1990; Hochreiter and Schmidhuber, 1997) networks, and this is one direction for future work.", "startOffset": 22, "endOffset": 69}, {"referenceID": 18, "context": "CCA is expected to perform well for clustering and classification when the two views are uncorrelated given the class label (Chaudhuri et al., 2009).", "startOffset": 124, "endOffset": 148}, {"referenceID": 41, "context": "The usefulness of CCA in such settings has previously been analyzed theoretically (Kakade and Foster, 2007; McWilliams et al., 2013) and has begun to be explored experimentally (Arora and Livescu, 2014).", "startOffset": 82, "endOffset": 132}, {"referenceID": 52, "context": "The usefulness of CCA in such settings has previously been analyzed theoretically (Kakade and Foster, 2007; McWilliams et al., 2013) and has begun to be explored experimentally (Arora and Livescu, 2014).", "startOffset": 82, "endOffset": 132}, {"referenceID": 4, "context": ", 2013) and has begun to be explored experimentally (Arora and Livescu, 2014).", "startOffset": 52, "endOffset": 77}, {"referenceID": 13, "context": "Our analysis is based on the following formulation of the linear CCA solution (Borga, 2001).", "startOffset": 78, "endOffset": 91}], "year": 2016, "abstractText": "We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for learning while only one view is available for downstream tasks. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a batch-style correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them empirically on image, speech, and text tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE). We also explore a stochastic optimization procedure for minibatch correlation-based objectives and discuss the time/performance trade-offs for kernel-based and neural network-based implementations.", "creator": "LaTeX with hyperref package"}}}