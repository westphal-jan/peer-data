{"id": "1602.01208", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2016", "title": "Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences", "abstract": "In this paper, we propose a novel unsupervised learning method for the lexical acquisition of words related to places visited by robots, from human continuous speech signals. We address the problem of learning novel words by a robot that has no prior knowledge of these words except for a primitive acoustic model. Further, we propose a method that allows a robot to effectively use the learned words and their meanings for self-localization tasks. The proposed method is nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates the generative model for self-localization and the unsupervised word segmentation in uttered sentences via latent variables related to the spatial concept. We implemented the proposed method SpCoA on SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile robot in a real environment. Further, we conducted experiments for evaluating the performance of SpCoA. The experimental results showed that SpCoA enabled the robot to acquire the names of places from speech sentences. They also revealed that the robot could effectively utilize the acquired spatial concepts and reduce the uncertainty in self-localization.", "histories": [["v1", "Wed, 3 Feb 2016 06:56:51 GMT  (2922kb,D)", "https://arxiv.org/abs/1602.01208v1", "Draft submitted to IEEE Transactions on Autonomous Mental Development (TAMD)"], ["v2", "Wed, 16 Mar 2016 12:17:46 GMT  (1447kb,D)", "http://arxiv.org/abs/1602.01208v2", "Draft submitted to IEEE Transactions on Autonomous Mental Development (TAMD)"], ["v3", "Sat, 7 May 2016 11:59:51 GMT  (1453kb,D)", "http://arxiv.org/abs/1602.01208v3", "This paper was accepted in the IEEE Transactions on Cognitive and Developmental Systems. (04-May-2016)"]], "COMMENTS": "Draft submitted to IEEE Transactions on Autonomous Mental Development (TAMD)", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.RO", "authors": ["akira taniguchi", "tadahiro taniguchi", "tetsunari inamura"], "accepted": false, "id": "1602.01208"}, "pdf": {"name": "1602.01208.pdf", "metadata": {"source": "CRF", "title": "Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences", "authors": ["Akira Taniguchi", "Tadahiro Taniguchi", "Tetsunari Inamura"], "emails": ["a.taniguchi@em.ci.ritsumei.ac.jp;", "taniguchi@em.ci.ritsumei.ac.jp).", "inamura@nii.ac.jp)."], "sections": [{"heading": null, "text": "In this sense, it is important that people are able to identify themselves and to understand what they are doing. (...) It is important that people are able to identify themselves. (...) It is important that people are able to identify themselves. (...) It is important that people are able to identify themselves. (...) \"It is important that people are able to identify themselves.\" (...) \"It is important that people are able to identify themselves.\" (...) \"It is important.\" (...). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). \"It is important.\" (. \"(.).\" (.). \"(.).\" It is important. \"(.\" (.). \"(.).\" (.). \"(.).\" (.).). (. (.). (.). (.). (. (.). (.). (.). (.).). (. (. (.).). (. (.). (.). (. (.).). (.). (.). (. (.).).). (. (.). (.).). (.). (. ().). (.). (). (.). (It is.). (.). (.). (). (.). (). (.).). (It is. (.). (.). (). (.).). (It is important.). (It is important.). (). (.). (). ().). (It is. ().). (). (It is. ()."}, {"heading": "II. RELATED WORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Lexical acquisition", "text": "Most studies on lexical acquisition typically focus on lexical acquisition of words not related to objects [1], [3] - [11]. However, Roy et al. proposed a computer model that would allow a robot to learn the names of objects from an object image and spontaneous speech directed at children [1]. Their results showed that the model performed language segmentation, word discovery and visual categorization. Iwahashi et al. reported that a robot correctly understands the situation and acquires the relationship between object behavior and sentences [3]. Qu & Chai focused on the connection between language and the moment and the use of domain knowledge in lexical acquisition [7], [8] that a robot correctly understands the situation and acquires the relationship between object behavior and sentences."}, {"heading": "B. Simultaneous learning of places and vocabulary", "text": "The following studies have lexical acquisition related to places. However, these studies could not use the language skills learned in other estimates such as the self-localization of a robot.Taguchi et al. proposed a method for the unattended learning of phoneme sequences and relationships between words and objects without prior linguistic knowledge of phonemes [2], [15]. Further, they proposed a method for the simultaneous categorization of self-position coordinates and lexical learning [16]. These experimental results showed that it was possible to learn the name of a place from utterances in some cases and the output of words that were not used for learning. Milford et al. suggested RatSLAM inspired by the biological knowledge of a pose cell of rodents [17]. Milford et al."}, {"heading": "III. SPATIAL CONCEPT ACQUISITION", "text": "We propose a non-parametric Bayesian method for spatial concepts (SpCoA) that integrates a non-parametric morphological analyzer for the lattice [23], i.e. latticelm1, a spatial cluster method, and Monte Carlo localization (MCL) [24]."}, {"heading": "A. Generative model", "text": "In our study, we define a position as a specific coordinate or local point in the environment, and the position distribution as the spatial area of the environment. Furthermore, we define a spatial concept as the names of places and the position distributions corresponding to these names. The model developed for the capture of spatial concepts is a probabilistic generative model that integrates a selective distribution with the simultaneous clustering of places and words. Table I shows each variable of the graphical model. The number of words in a sentence is referred to as Bt. The generative model of the proposed method is defined as an equation (1-10)."}, {"heading": "B. Overview of the proposed method SpCoA", "text": "We assume that a robot performs self-localization by using control data and sensor data at any time. The procedure for learning spatial concepts is as follows: 1) A speech robot will teach a robot the names of places as shown in Fig. 1 (b). Each time the robot arrives at a place that was a specific learning destination, the speech robot will say a sentence, including the name of the current place. 2) The robot will perform the speech recognition from the pronounced speech signal data. Thus, the speech recognition system The self-localization procedure that uses spatial concepts is as follows: 1) The words of the learned spatial concept will be entered into the dictionary of the speech recognition system. 2) When a robot receives a speech signal, the speech recognition will be performed. Then, a word sequence is obtained as the 1-best speech recognition result. 3) The robot modifies the self-localization of words obtained by speech recognition."}, {"heading": "C. Learning of spatial concept", "text": "The teaching data are a set of sentences expressed for all teaching periods. The number of teaching periods is called To = {t1, t2,. tN}, and the number of teaching records is called N. The model parameters are called \"W,\" \"p,\" \"p,\" \"p,\" \"p,\" \"p,\" \"p,\" \"p,\" \"p,\" \"p,\" \"\" p, \"\" \"p,\" \"\" p, \"\" \"p,\" \"\" \"p.\" The initial values of the model can be arbitrarily determined in accordance with a condition. Furthermore, the number of teaching records is called N. The model parameters are called \"p\" W, \"\" \"p,\" \"p,\" \"\" p, \"\" \"p,\" \"\" \"\" p, \"\" p, \"\" p, \"\" \"\" p, \"\" \"\" \"p,\" \"\" \"\" \"\" p, \"\" \"\" \"\" \"\" \"\" \"\" \"\"., \".\"., \".\". \".\". \""}, {"heading": "D. Self-localization of after learning spatial concepts", "text": "A robot that acquires spatial concepts can use spatial concepts for self-localization = > The estimated model parameters that include both functions are: p (x0: t | z1: t, u1: t, O1: t, B: t), p (zt | xt), p (xt | 1, ut) p (x0: t | z1: t, u1: t, O1: t \u2212 1, B: p). (23) When the robot hears the name of a place spoken by the proponent, in addition to the probability of the sensor model of MCL, the probability of xt in relation to a speech recognition set is calculated."}, {"heading": "IV. EXPERIMENT I", "text": "In this experiment, we validate the evidence for the proposed method (SpCoA) in an environment simulated on the simulator platform SIGVerse2 [31], which allows the simulation of social interactions. Speech recognition is performed using the Japanese Continuous Speech Recognition System Julius3 [32], [33]. The set of 43 Japanese phonemes, which is adopted by the Acoustical Society of Japan (ASJ)'s speech database committee, is Julius [32]. The representation of these phonemes is also adopted in this study. Julius system uses a dictionary containing 115 Japanese syllables. The microphone attached to the robot is SHURE's PG27-USB. Furthermore, an unsupervised morphological analyzer, a latticelm 0.4, is implemented [23].In the experiment, we compare the following three types of word segmentation methods."}, {"heading": "A. Learning of spatial concepts", "text": "The experimental environment is shown in Fig. 3. A mobile robot can move by learning forward, backward, right rotation or left rotation on a two-dimensional plane. In this experiment, the robot can use an approximately correct map of the environment under consideration. The robot has a ranking of the proposed distribution on a map of the environment under consideration. The colors of the groups of points are determined randomly. Each balloon shows the index number for each position distribution. 5. Result of the multinomial distributions of the names of the places W (top); multinomial distributions of the index (bottom): All words obtained during the experiment are fixed or fixed."}, {"heading": "B. Phoneme recognition accuracy of uttered sentences", "text": "1) Conditions: We compared the performance of three types of word segmentation methods for all pronounced sentences considered. It was difficult to separate the ambiguous syllable recognition from the unattended word segmentation. Therefore, in this experiment, we considered the positions of a delimiter as a single letter. We calculated the match rate of a phoneme sequence of a recognition result for each pronounced sentence and the correct phoneme sequence of the teaching data, which was segmented accordingly into Japanese morphemes, using MeCab4, a standard Japanese morphological analyzer commonly used for processing natural language. The match rate of the phoneme sequence was calculated using the phoneme precision rate (PAR) as follows: PAR = 1 \u2212 S + D + I N. (25) The numerator of the equation (25) is calculated by using the Levensht a distance between the correct phonemon sequence and the phonon sequence."}, {"heading": "C. Estimation accuracy of spatial concepts", "text": "1) Conditions: We compared the matching rate with the estimated results of the Index Ct of the spatial concepts of each teaching expression and the classification results of the correct response given by man. Evaluation of this experiment used the Adjusted Edge Index (ARI) [34]. ARI is a measure of the degree of similarity between two cluster results. Furthermore, we compared the proposed method with a method of word grouping without location information to investigate the effect of lexical capture using location information. In particular, a method of word grouping without location information used the Dirichlet Process Mixture (DPM) of the Unigram Model of an SBP representation. The parameters that corresponded to those of the proposed method were the same as the parameters of the proposed method and were sampled using Gibbs sampling.2) Results: Figure 6 shows the results of the average of the ARI values of 10 attempts to learn by Gibbs Sampling."}, {"heading": "D. Accuracy of acquired phoneme sequences representing the names of places", "text": "1) Conditions: We evaluated whether the place names for the eligible teaching locations were properly learned. This experiment is based on the requirement of the best phoneme sequence Ot, which best represents the self-position of a robot. The robot moves near each teaching location. The probability of a word Ot, best if the self-position of the robot is given, p (Ot, best | xt), can be determined by using Equation (24). The word with the highest probability was selected. We compared the PAR with the correct phoneme sequence and a selected place name. Since \"kiqchiN\" and \"daidokoro\" were taught for the same place, the word whose PAR was the higher probability was accepted. 2) Results: Fig. 7 shows the results of PAR for the word that was considered a place name. SpCoA (Latticelm), the proposed method using the results of the unattended word sequence Bollar, was considered as the best of the word sequence (0,001), the results of PAR (0,001), and the results of PAR (0,001)."}, {"heading": "E. Self-localization that utilizes acquired spatial concepts", "text": "1) Conditions: In this experiment we validate that the robot can efficiently use the acquired spatial concepts (we compare the estimation accuracy of the localization for the proposed method (SpCoA MCL) and the conventional MCL. When a robot arrives at the learning destination, the speaker pronounces the sentence that contains the name of the location again for the robot. The number of particles is M = 1000, and the initial particles are evenly distributed in the considered environment. The robot performs a control peration for each time step. The estimation error in the localization is evaluated as follows: While we perform the localization, we record the estimation error (equation (26) at the level of the ground."}, {"heading": "V. EXPERIMENT II", "text": "In this experiment, the effectiveness of the proposed method was tested using an autonomous mobile robot TurtleBot 25 in a real-world environment. Figure 9 shows the TurtleBot 2 used in the experiments. Mapping and self-localisation are carried out by the robot operating system (ROS). The speech recognition system, the microphone and the unattended morphological analyser were the same as described in Section IV."}, {"heading": "A. Learning of spatial concepts in the real environment", "text": "In this experiment, self-localization was performed using a map generated by SLAM. Initial particles are defined by the true starting position of the robot, but the map generated in the real environment and the names of the teaching locations are shown in the figure. 10) Results: The number of teaching locations learned on the map shows the position distributions learned on the map. Table V shows the five best elements of the multinomic distributions of the location WCt and the multinomic distribution of the position distribution indices."}, {"heading": "B. Modification of localization by the acquired spatial concepts", "text": "1) Conditions: In this experiment, we verified the modification results of the self-localization by using spatial concepts in the global self-localization. In this experiment, we used the learning results of the spatial concepts presented in Section V-A. The experimental procedures are shown below. Initial particles were evenly distributed throughout the ground. In this experiment, the robot begins to move from some distance to the destination. When the robot reached the destination, the expressor spoke the sentence containing the name of the place for the robot. After receiving the language information, the robot modified the self-localization based on the acquired spatial concepts. The number of particles was the same as in Section V-A.2) Results: Figure 12 shows the results of the self-localization before (the upper part of the figure) and then (the lower part of the figure) the expression of the particles in three places. The particle states are indicated by red arrows. The moving position of the robot is indicated by a green line (the green line being indicated)."}, {"heading": "VI. CONCLUSION AND FUTURE WORK", "text": "In this paper, we discussed spatial concept capture, lexical capture in connection with places and self-localization with acquired spatial concepts. We proposed non-parametric Bayesian spatial concept capture method SpCoA, which integrates latticelm [23], a spatial cluster method, and MCL. We conducted experiments to evaluate the performance of SpCoA in a simulation and in a real environment. SpCoA showed good results in all experiments. In experiments of learning spatial concepts, the robot was able to form spatial concepts for the places of learning targets from human continuous speech signals both in the space of the simulation environment and in the entire floor of the real environment. Furthermore, the unattended word segmentation method latticelm was able to reduce variability and errors in the recognition of phonemes in all expressions. SpCoA achieved more accurate lexical capture by performing word segmentation."}], "references": [{"title": "Learning words from sights and sounds: A computational model,", "author": ["D. Roy", "A. Pentland"], "venue": "Cognitive science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Learning lexicons from spoken utterances based on statistical model selection,", "author": ["R. Taguchi", "N. Iwahashi", "T. Nose", "K. Funakoshi", "M. Nakano"], "venue": "in Annual Conference of the International Speech Communication Association (INTERSPEECH),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Language acquisition through a human\u2013robot interface by combining speech, visual, and behavioral information,", "author": ["N. Iwahashi"], "venue": "Information Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Robots that learn to converse: Developmental approach to situated language processing,", "author": ["N. Iwahashi", "R. Taguchi", "K. Sugiura", "K. Funakoshi", "M. Nakano"], "venue": "Proceedings of International Symposium on Speech and Language Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Probabilistic grounding of situated speech using plan recognition and reference resolution,", "author": ["P. Gorniak", "D. Roy"], "venue": "Proceedings of the 7th international conference on Multimodal interfaces. ACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Incorporating temporal and semantic information with eye gaze for automatic word acquisition in multimodal conversational systems,", "author": ["S. Qu", "J.Y. Chai"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Multimodal language acquisition based on motor learning and interaction,\u201d in From Motor Learning to Interaction Learning in Robots", "author": ["J. H\u00f6rnstein", "L. Gustavsson", "J. Santos-Victor", "F. Lacerda"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Online learning of concepts and words using multimodal LDA and hierarchical Pitman-Yor Language Model,", "author": ["T. Araki", "T. Nakamura", "T. Nagai", "S. Nagasaka", "T. Taniguchi", "N. Iwahashi"], "venue": "in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "The mathematics of statistical machine translation: Parameter estimation,", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "R.L. Mercer"], "venue": "Computational linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1993}, {"title": "Multimodal categorization by hierarchical Dirichlet process,", "author": ["T. Nakamura", "T. Nagai", "N. Iwahashi"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling,", "author": ["D. Mochihashi", "T. Yamada", "N. Ueda"], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Learning physically grounded lexicons from spoken utterances,", "author": ["R. Taguchi", "N. Iwahashi", "K. Funakoshi", "M. Nakano", "T. Nose", "T. Nitta"], "venue": "Human Machine Interaction\u2013Getting Closer,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Learning place-names from spoken utterances and localization results by mobile robot,", "author": ["R. Taguchi", "Y. Yamada", "K. Hattori", "T. Umezaki", "M. Hoguro", "N. Iwahashi", "K. Funakoshi", "M. Nakano"], "venue": "in Annual Conference of the International Speech Communication Association (INTERSPEECH),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "RatSLAM: a hippocampal model for simultaneous localization and mapping,", "author": ["M. Milford", "G. Wyeth", "D. Prasser"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Learning spatial concepts from RatSLAM representations,", "author": ["M. Milford", "R. Schulz", "D. Prasser", "G. Wyeth", "J. Wiles"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Lingodroids: socially grounding place names in privately grounded cognitive maps,", "author": ["R. Schulz", "G. Wyeth", "J. Wiles"], "venue": "Adaptive Behavior,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Communication between Lingodroids with different cognitive capabilities,", "author": ["S. Heath", "D. Ball", "R. Schulz", "J. Wiles"], "venue": "IEEE International Conference on Robotics and Automation (ICRA). IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Are we there yet? grounding temporal concepts in shared journeys,", "author": ["R. Schulz", "G. Wyeth", "J. Wiles"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Grounded spatial symbols for task planning based on experience,", "author": ["K. Welke", "P. Kaiser", "A. Kozlov", "N. Adermann", "T. Asfour", "M. Lewis", "M. Steedman"], "venue": "in 13th International Conference on Humanoid Robots (Humanoids). IEEE/RAS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Bayesian learning of a language model from continuous speech,", "author": ["G. Neubig", "M. Mimura", "T. Kawahara"], "venue": "IEICE TRANSACTIONS on Information and Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Monte carlo localization for mobile robots,", "author": ["F. Dellaert", "D. Fox", "W. Burgard", "S. Thrun"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "A constructive definition of Dirichlet priors,", "author": ["J. Sethuraman"], "venue": "Statistica Sinica,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1994}, {"title": "A sticky HDP-HMM with application to speaker diarization,", "author": ["E.B. Fox", "E.B. Sudderth", "M.I. Jordan", "A.S. Willsky"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "FastSLAM: A factored solution to the simultaneous localization and mapping problem,", "author": ["M. Montemerlo", "S. Thrun", "D. Koller", "B. Wegbreit"], "venue": "Proceedings of the AAAI National Conference on Artificial Intelligence. American Association for Artificial Intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "An efficient FastSLAM algorithm for generating maps of large-scale cyclic environments from raw laser range measurements,", "author": ["D. Hahnel", "W. Burgard", "D. Fox", "S. Thrun"], "venue": "in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}, {"title": "Computational aspects of sequential Monte Carlo filter and smoother,", "author": ["G. Kitagawa"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Simulator platform that enables social interaction simulation \u2013SIGVerse: SocioIntelliGenesis simulator\u2013,", "author": ["T. Inamura", "T. Shibata", "H. Sena", "T. Hashimoto", "N. Kawai", "T. Miyashita", "Y. Sakurai", "M. Shimizu", "M. Otake", "K. Hosoda"], "venue": "IEEE/SICE International Symposium on System Integration,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Sharable software repository for Japanese large vocabulary continuous speech recognition,", "author": ["T. Kawahara", "T. Kobayashi", "K. Takeda", "N. Minematsu", "K. Itou", "M. Yamamoto", "A. Yamada", "T. Utsuro", "K. Shikano"], "venue": "Fifth International Conference on Spoken Language Processing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Julius\u2014an open source realtime large vocabulary recognition engine,", "author": ["A. Lee", "T. Kawahara", "K. Shikano"], "venue": "European Conference on Speech Communication and Technology (EUROSPEECH),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "Arabie, \u201cComparing partitions,", "author": ["P.L. Hubert"], "venue": "Journal of classification,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1985}, {"title": "Multimodal concept and word learning using phoneme sequences with errors,", "author": ["T. Nakamura", "T. Araki", "T. Nagai", "S. Nagasaka", "T. Taniguchi", "N. Iwahashi"], "venue": "in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Mutual learning of an object concept and language model based on MLDA and NPYLM,", "author": ["T. Nakamura", "T. Nagai", "K. Funakoshi", "S. Nagasaka", "T. Taniguchi", "N. Iwahashi"], "venue": "in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Iterative Bayesian Word Segmentation for Unsupervised Vocabulary Discovery from Phoneme Lattices,", "author": ["J. Heymann", "O. Walter", "R. Haeb-Umbach", "B. Raj"], "venue": "in 39th International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "These studies differ from speech recognition studies based on a large vocabulary and natural language processing studies based on lexical, syntactic, and semantic knowledge [1], [2].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "These studies differ from speech recognition studies based on a large vocabulary and natural language processing studies based on lexical, syntactic, and semantic knowledge [1], [2].", "startOffset": 178, "endOffset": 181}, {"referenceID": 0, "context": "Most studies on lexical acquisition typically focus on lexicons about objects [1], [3]\u2013[11].", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "Most studies on lexical acquisition typically focus on lexicons about objects [1], [3]\u2013[11].", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "Most studies on lexical acquisition typically focus on lexicons about objects [1], [3]\u2013[11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "proposed a computational model that enables a robot to learn the names of objects from an object image and spontaneous infant-directed speech [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "reported that a robot properly understands the situation and acquires the relationship of object behaviors and sentences [3]\u2013[5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "reported that a robot properly understands the situation and acquires the relationship of object behaviors and sentences [3]\u2013[5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "Qu & Chai focused on the conjunction between speech and eye gaze and the use of domain knowledge in lexical acquisition [7], [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "Qu & Chai\u2019s method based on the IBM translation model [12] estimates the word-entity association probability.", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "The method proposed in [10] is a categorization method based on multimodal latent Dirichlet allocation (MLDA) that enables the acquisition of object concepts from multimodal information, such as visual, auditory, and haptic information [13].", "startOffset": 236, "endOffset": 240}, {"referenceID": 10, "context": "addressed the development of a method combining unsupervised word segmentation from uttered sentences by a nested Pitman-Yor language model (NPYLM) [14] and the learning of object concepts by MLDA [11].", "startOffset": 148, "endOffset": 152}, {"referenceID": 7, "context": "addressed the development of a method combining unsupervised word segmentation from uttered sentences by a nested Pitman-Yor language model (NPYLM) [14] and the learning of object concepts by MLDA [11].", "startOffset": 197, "endOffset": 201}, {"referenceID": 1, "context": "proposed a method for the unsupervised learning of phoneme sequences and relationships between words and objects from various user utterances without any prior linguistic knowledge other than an acoustic model of phonemes [2], [15].", "startOffset": 222, "endOffset": 225}, {"referenceID": 11, "context": "proposed a method for the unsupervised learning of phoneme sequences and relationships between words and objects from various user utterances without any prior linguistic knowledge other than an acoustic model of phonemes [2], [15].", "startOffset": 227, "endOffset": 231}, {"referenceID": 12, "context": "Further, they proposed a method for the simultaneous categorization of self-position coordinates and lexical learning [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 13, "context": "proposed RatSLAM inspired by the biological knowledge of a pose cell of the hippocampus of rodents [17].", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "proposed a method that enables a robot to acquire spatial concepts by using RatSLAM [18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "Further, Lingodroids, mobile robots that learn a language through robot-to-robot communication, have been studied [19]\u2013[21].", "startOffset": 114, "endOffset": 118}, {"referenceID": 17, "context": "Further, Lingodroids, mobile robots that learn a language through robot-to-robot communication, have been studied [19]\u2013[21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "In [21], the researchers showed that it was possible to learn temporal concepts in a manner analogous to the acquisition of spatial concepts.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "proposed a method that acquires spatial representation by the integration of the representation of the continuous state space on the sensorimotor level and the discrete symbolic entities used in high-level reasoning [22].", "startOffset": 216, "endOffset": 220}, {"referenceID": 19, "context": "We propose nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates a nonparametric morphological analyzer for the lattice [23], i.", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": ", latticelm1, a spatial clustering method, and Monte Carlo localization (MCL) [24].", "startOffset": 78, "endOffset": 82}, {"referenceID": 21, "context": "The prior distribution configured by using the stick breaking process (SBP) [25] is denoted as GEM(\u00b7), the multinomial distribution as Mult(\u00b7), the Dirichlet distribution as Dir(\u00b7), the inverse\u2013Wishart distribution as IW(\u00b7), and the multivariate Gaussian (normal) distribution as N (\u00b7).", "startOffset": 76, "endOffset": 80}, {"referenceID": 19, "context": "1latticelm is the name of the tool that [23] is implemented and is treated as the name of the method in this study.", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": ", a weaklimit approximation [26].", "startOffset": 28, "endOffset": 32}, {"referenceID": 19, "context": "We use an unsupervised word segmentation method latticelm that can directly segment words from the lattices of the speech recognition results of the uttered sentences [23].", "startOffset": 167, "endOffset": 171}, {"referenceID": 10, "context": "Unsupervised word segmentation using the lattices of syllable recognition is expected to be able to reduce the variability and errors in phonemes as compared to NPYLM [14], i.", "startOffset": 167, "endOffset": 171}, {"referenceID": 20, "context": "The self-localization method adopts MCL [24], a method that is generally used as the localization of mobile robots for simultaneous localization and mapping (SLAM) [27].", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": "assume that a robot generates an environment map by using MCL-based SLAM such as FastSLAM [28], [29] in advance, and then, performs localization by using the generated map.", "startOffset": 90, "endOffset": 94}, {"referenceID": 24, "context": "assume that a robot generates an environment map by using MCL-based SLAM such as FastSLAM [28], [29] in advance, and then, performs localization by using the generated map.", "startOffset": 96, "endOffset": 100}, {"referenceID": 25, "context": "Self-positions x0:T are sampled by using a Monte Carlo fixed-lag smoother [30] in the learning phase.", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "4: x0:t\u223cMonte Carlo smoother(x0:t\u22121, u1:t, z1:t) [30] 5: if the speech signal is observed then", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "12: OTo,B \u223c latticelm(L) [23] 13: // Gibbs sampling 14: Initialize parameters iTo , CTo , \u0398 = {W,\u03bc,\u03a3, \u03c6l, \u03c0} 15: for j = 1 to iteration number do", "startOffset": 25, "endOffset": 29}, {"referenceID": 26, "context": "In this experiment, we validate the evidence of the proposed method (SpCoA) in an environment simulated on the simulator platform SIGVerse2 [31], which enables the simulation of social interactions.", "startOffset": 140, "endOffset": 144}, {"referenceID": 27, "context": "The speech recognition is performed using the Japanese continuous speech recognition system Julius3 [32], [33].", "startOffset": 100, "endOffset": 104}, {"referenceID": 28, "context": "The speech recognition is performed using the Japanese continuous speech recognition system Julius3 [32], [33].", "startOffset": 106, "endOffset": 110}, {"referenceID": 27, "context": "The set of 43 Japanese phonemes defined by Acoustical Society of Japan (ASJ)\u2019s speech database committee is adopted by Julius [32].", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "4, is implemented [23].", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "In this case, latticelm [23] is almost equivalent to NPYLM [14].", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "In this case, latticelm [23] is almost equivalent to NPYLM [14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "The evaluation of this experiment used the adjusted Rand index (ARI) [34].", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "We proposed nonparametric Bayesian spatial concept acquisition method SpCoA that integrates latticelm [23], a spatial clustering method, and MCL.", "startOffset": 102, "endOffset": 106}, {"referenceID": 30, "context": "on the basis of the integration of the learning of object concepts with a language model [35], [36].", "startOffset": 89, "endOffset": 93}, {"referenceID": 31, "context": "on the basis of the integration of the learning of object concepts with a language model [35], [36].", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "proposed a method that alternately and repeatedly updates phoneme recognition results and the language model by using unsupervised word segmentation [37].", "startOffset": 149, "endOffset": 153}], "year": 2016, "abstractText": "In this paper, we propose a novel unsupervised learning method for the lexical acquisition of words related to places visited by robots, from human continuous speech signals. We address the problem of learning novel words by a robot that has no prior knowledge of these words except for a primitive acoustic model. Further, we propose a method that allows a robot to effectively use the learned words and their meanings for self-localization tasks. The proposed method is nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates the generative model for self-localization and the unsupervised word segmentation in uttered sentences via latent variables related to the spatial concept. We implemented the proposed method SpCoA on SIGVerse, which is a simulation environment, and TurtleBot 2, which is a mobile robot in a real environment. Further, we conducted experiments for evaluating the performance of SpCoA. The experimental results showed that SpCoA enabled the robot to acquire the names of places from speech sentences. They also revealed that the robot could effectively utilize the acquired spatial concepts and reduce the uncertainty in self-localization.", "creator": "LaTeX with hyperref package"}}}