{"id": "1602.01576", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2016", "title": "A Factorized Recurrent Neural Network based architecture for medium to large vocabulary Language Modelling", "abstract": "Statistical language models are central to many applications that use semantics. Recurrent Neural Networks (RNN) are known to produce state of the art results for language modelling, outperforming their traditional n-gram counterparts in many cases. To generate a probability distribution across a vocabulary, these models require a softmax output layer that linearly increases in size with the size of the vocabulary. Large vocabularies need a commensurately large softmax layer and training them on typical laptops/PCs requires significant time and machine resources. In this paper we present a new technique for implementing RNN based large vocabulary language models that substantially speeds up computation while optimally using the limited memory resources. Our technique, while building on the notion of factorizing the output layer by having multiple output layers, improves on the earlier work by substantially optimizing on the individual output layer size and also eliminating the need for a multistep prediction process.", "histories": [["v1", "Thu, 4 Feb 2016 07:53:11 GMT  (536kb)", "http://arxiv.org/abs/1602.01576v1", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["anantharaman palacode narayana iyer"], "accepted": false, "id": "1602.01576"}, "pdf": {"name": "1602.01576.pdf", "metadata": {"source": "CRF", "title": "A Factorized Recurrent Neural Network based architecture for medium to large vocabulary Language Modelling", "authors": ["Anantharaman Palacode", "Narayana Iyer"], "emails": ["ananth@jnresearch.com"], "sections": [{"heading": null, "text": "This year, it is time to put yourself in a position to take the lead."}, {"heading": "A. Rationale", "text": "A full Softmax classifier for speech modeling generates a probability distribution across all words in the output vocabulary, given the context words. However, for most real-world applications, this could be an overkill, since not every word in the vocabulary can have a reasonable probability of following every other word. We suggest an architecture that uses this observation, that is, every word w in the educational corpus can only follow a limited number of words in the vocabulary V, which is often a much smaller subset of V. This list, which we can also call the \"sequence list,\" is given by the bigram distribution of the word. Mentioning B (w) is the list of bigram keys of the word w, we observe: | B (w); < < | V |. This is shown in Figure (2) for the Brown corpus and Figure (3) for our custom vocabulary that has text on mobile device product ratings."}, {"heading": "B. Architecture", "text": "This year, there is less than a year to go before an agreement can be reached."}, {"heading": "C. Output Layer Assignment", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to move, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in fact, in which they, in fact, are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move"}, {"heading": "D. Computational Complexity Analysis", "text": "This year, it will be able to fix and fix the mentioned bugs."}], "references": [{"title": "A survey on the application of recurrent neural networks to statistical language modeling", "author": ["Wim De Mulder", "Steven Bethard", "Marie-Francine Moens"], "venue": "Computer Speech & Language", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Recurrent neural network based language", "author": ["T Mikolov", "Martin Karafiat", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur"], "venue": "Proc. INTERSPEECH", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Extensions of Recurrent neural network based language model, Acoustics, Speech and Signal Processing", "author": ["T Mikolov", "S Kombrink", "L Burget", "JH \u010cernock\u00fd", "S Khudanpur"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Distributed Representations of Words and Phrases and their compositionality, Advances in neural information processing", "author": ["T Mikolov"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "\u010cernock\u00fd. Strategies For Training Large Scale Neural Network Language Models, Automatic Speech Recognition and Understanding (ASRU)", "author": ["T Mikolov", "A Deoras", "D Povey", "J L Burget"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton"], "venue": "Advances in neural information processing systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Hierarchical Probabilistic Neural Network Language Model, AISTATS", "author": ["F Morin", "Y. Bengio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Accelerating recurrent neural network training via two stage classes andparallelization", "author": ["Z. Huang", "G. Zweig", "M. Levit", "B. Dumoulin", "B. Oguz", "S. Chang"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R Collobert", "J Weston"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R Collobert", "J Weston", "L Bottou", "M Karlen", "K Kavukcuoglu", "P Kuksa"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Learning internal representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1986}, {"title": "Finding structure in time", "author": ["J. Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Factored neural language models", "author": ["A. Alexandrescu", "K. Kirchhoff"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Recurrent Neural Network (RNN) based architectures, with their ability to support an arbitrarily sized context have been reported to outperform most of the state of the art n-gram systems [1].", "startOffset": 188, "endOffset": 191}, {"referenceID": 1, "context": "report a 50% reduction in perplexity using a mixture of RNNs compared to the state of the art traditional models [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "Some of the approaches are aimed at modelling very large corpora with hundreds of millions of word tokens [5].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "The recent advances such as the class based factorization approaches [3][14] demonstrate the feasibility of improving the training speed without significant degradation in performance compared to full softmax.", "startOffset": 69, "endOffset": 72}, {"referenceID": 13, "context": "The recent advances such as the class based factorization approaches [3][14] demonstrate the feasibility of improving the training speed without significant degradation in performance compared to full softmax.", "startOffset": 72, "endOffset": 76}, {"referenceID": 6, "context": "Morin and Bengio [7] first introduced the hierarchical softmax technique, that was improved upon subsequently in the work reported in [4][6].", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "Morin and Bengio [7] first introduced the hierarchical softmax technique, that was improved upon subsequently in the work reported in [4][6].", "startOffset": 134, "endOffset": 137}, {"referenceID": 5, "context": "Morin and Bengio [7] first introduced the hierarchical softmax technique, that was improved upon subsequently in the work reported in [4][6].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "[8] that is also based on an RNN architecture for language modelling, uses a two-level hierarchy of classes where words are binned in to classes by word frequency and classes are categorized in to super classes according to the class frequency.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The network also learns the input word representation simultaneously with the language model [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "However the fixed size context, a large softmax size and the simultaneous learning of an accurate word representation are major deficiencies [2].", "startOffset": 141, "endOffset": 144}, {"referenceID": 9, "context": "Semantic Role Labeling (SRL), Language Models and Semantically Related Words [10][11].", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "Semantic Role Labeling (SRL), Language Models and Semantically Related Words [10][11].", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "In their paper [2] Mikolov et al.", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "reported a language model based on a simple recurrent neural network or Elman network architecture [13], which is easy to implement and train.", "startOffset": 99, "endOffset": 103}, {"referenceID": 1, "context": "In order to improve the efficiencies, the model described in [2] was extended in to a RNN with the single large output layer factorized in to multiple smaller layers with the addition of a class layer.", "startOffset": 61, "endOffset": 64}, {"referenceID": 11, "context": "Concretely, the time complexity of a training step for the full (without factorization) softmax layer is proportional to: O = (1 + H) \u00d7 H \u00d7 \u03c4 + H \u00d7 V (1) where: H is the number of hidden units of the RNN, \u03c4 denotes the time steps through which we backpropagate, as per the back propagation through time (BPTT) algorithm [12] and V is the", "startOffset": 320, "endOffset": 324}, {"referenceID": 1, "context": "The basis for complexity analysis for our technique is similar to the class based factorization described in [2] and characterized in equation (2), with a major difference.", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "The training time reported in [2] was about 6 hours for this corpus and BLAS library [16] was used for speed up.", "startOffset": 30, "endOffset": 33}], "year": 2016, "abstractText": "Statistical language models are central to many applications that use semantics. Recurrent Neural Networks (RNN) are known to produce state of the art results for language modelling, outperforming their traditional n-gram counterparts in many cases. To generate a probability distribution across a vocabulary, these models require a softmax output layer that linearly increases in size with the size of the vocabulary. Large vocabularies need a commensurately large softmax layer and training them on typical laptops/PCs requires significant time and machine resources. In this paper we present a new technique for implementing RNN based large vocabulary language models that substantially speeds up computation while optimally using the limited memory resources. Our technique, while building on the notion of factorizing the output layer by having multiple output layers, improves on the earlier work by substantially optimizing on the individual output layer size and also eliminating the need for a multistep prediction process. Keywords-Recurrent Neural Networks; Language Models; hierarchical softmax; class based prediction", "creator": "Microsoft\u00ae Word 2013"}}}