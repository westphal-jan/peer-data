{"id": "1602.01580", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2016", "title": "Long-term Planning by Short-term Prediction", "abstract": "We consider planning problems, that often arise in autonomous driving applications, in which an agent should decide on immediate actions so as to optimize a long term objective. For example, when a car tries to merge in a roundabout it should decide on an immediate acceleration/braking command, while the long term effect of the command is the success/failure of the merge. Such problems are characterized by continuous state and action spaces, and by interaction with multiple agents, whose behavior can be adversarial. We argue that dual versions of the MDP framework (that depend on the value function and the $Q$ function) are problematic for autonomous driving applications due to the non Markovian of the natural state space representation, and due to the continuous state and action spaces. We propose to tackle the planning task by decomposing the problem into two phases: First, we apply supervised learning for predicting the near future based on the present. We require that the predictor will be differentiable with respect to the representation of the present. Second, we model a full trajectory of the agent using a recurrent neural network, where unexplained factors are modeled as (additive) input nodes. This allows us to solve the long-term planning problem using supervised learning techniques and direct optimization over the recurrent neural network. Our approach enables us to learn robust policies by incorporating adversarial elements to the environment.", "histories": [["v1", "Thu, 4 Feb 2016 08:06:59 GMT  (27kb,D)", "http://arxiv.org/abs/1602.01580v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shai shalev-shwartz", "nir ben-zrihem", "aviad cohen", "amnon shashua"], "accepted": false, "id": "1602.01580"}, "pdf": {"name": "1602.01580.pdf", "metadata": {"source": "CRF", "title": "Long-term Planning by Short-term Prediction", "authors": ["Shai Shalev-Shwartz", "Nir Ben-Zrihem", "Aviad Cohen", "Amnon Shashua"], "emails": [], "sections": [{"heading": null, "text": "For example, if a car tries to merge in a roundabout, it should opt for an immediate acceleration / braking command, while the long-term effect of the command is the success / failure of the merger. Such problems are characterized by continuous state and action spaces, as well as interaction with multiple actors whose behavior can be counterproductive. We argue that dual versions of the MDP framework (which depend on the value function and Q function) are problematic for autonomous driving applications, due to the non-Markovian representation of the natural state and continuous state and action spaces. We propose to address the planning task by dividing the problem into two phases: First, we apply supervised learning to predict the near future based on the present. We demand that the predictor be differentiable in terms of the representation of the state and action spaces. Second, we model a complete trajectory of the actor using a neural network to solve the unexplained factor in the immediate approach."}, {"heading": "1 Introduction", "text": "Two of the most important elements of autonomous driving systems are awareness and planning. As an example, we consider a simple vehicle control system (\"we should find a compact representation of the current state of the environment\"), while planning is concerned with deciding what measures to take to optimize future goals. [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of machine learning in the planning phase, machine learning approaches to planning in the framework of Reinforcement Learning (RL) are examined - see [4, 22, 23] for a comprehensive review of enhancing learning in robotics. Typically, machine learning is carried out in a sequence of successive rounds. In the round, planners (a.k.a. the agent) observe a state, st-S, which represents the agent as well as the environment."}, {"heading": "2 Planning by Prediction", "text": "We assume that the probability that we will apply our simulators to T-steps while carrying out the individual steps on the basis of R-actions is relatively high. (We assume that the probability that we will apply our simulators to T-steps while we apply the distribution mechanisms on the basis of R-actions. (We assume that the distribution mechanisms will be applied on the basis of R-actions.) We assume that the probability that the distribution mechanisms will be applied on the basis of R-actions is very high. (We assume that the distribution mechanisms will not work on the basis of R-actions.) We assume that the distribution mechanisms will be applied on the basis of R-actions. (We assume that the probability that the distribution mechanisms will be applied on the basis of R-actions.) We assume that the distribution mechanisms will be applied on the basis of R-actions."}, {"heading": "2.1 Robustness to Adversarial Environment", "text": "Since our model does not impose probable assumptions about the state of the Earth, we can consider environments in which the Earth is chosen in an adversarial manner. Of course, we must make some limitations on autonomous driving, otherwise the adversary may make the planning problem impossible. A natural limitation is to demand that the choice of the Earth be limited in an adversarial way by a constant. Robustness against the hostile environment is quite useful in applications of autonomous driving. We describe a real aspect of the hostile environment in Section 3.Here we show that choosing the Earth in a contradictory way could even speed up the learning process, as it can focus the learner on the robust optimal politics. Let's consider the following simple game: The state is st-R, the action is at the highest level, and the immediate loss function is at the lowest level."}, {"heading": "3 Example Applications", "text": "The aim of this section is to demonstrate some aspects of our approach using two toy examples: adaptive cruise control (ACC) and entering a roundabout."}, {"heading": "3.1 The ACC Problem", "text": "In the ACC problem, a host vehicle tries to maintain a reasonable distance of 1.5 seconds from a target vehicle while driving as smoothly as possible. We provide a simple model for this problem as follows: The state space is R3 and the action space is R. The first coordinate of the state is the speed of the target vehicle, the second coordinate is the speed of the guest vehicle, and the last coordinate is the distance between host and target (namely the position of the guest vehicle minus the position of the target vehicle in the road curve).The action to be taken by the host is acceleration and is called an. We designate the time difference between successive laps (in the experiment we set a distance of \u2212 \u2212 \u2212 \u2212 \u2212 0.1 seconds). \u2212 Denote st = (v target t, v host t, xt) and denote by a target t the (unknown) acceleration of the target. The full dynamics of the system can be described by x: vconsttt = [v target t \u2212 1 targets t + a target t, xt + 1]."}, {"heading": "3.2 Merging into a Roundabout", "text": "An episode begins when the agent approaches the bottom entrance of the roundabout, and ends when the agent reaches the second exit of the roundabout, or after a fixed number of steps. A successful episode is first measured by keeping a safe distance from all other vehicles in the roundabout at all times. Second, the agent should finish the route as quickly as possible. Third, he should adhere to an \"aggressive\" acceleration policy that accelerates when the host tries to merge in front of it. Probably 1 \u2212 p, the target vehicle is modeled by a \"defensive\" behavior, with the probability p, a target vehicle is modeled by an \"aggressive\" acceleration policy that accelerates when the host tries to merge it. Probably 1 \u2212 p, the target vehicle is modeled by a \"defensive\" vehicle that accelerates and merges the host."}, {"heading": "4 Discussion", "text": "Our approach is based on dividing the near future into a predictable and an unpredictable part. We have demonstrated the effectiveness of the learning procedure for two simple tasks: adaptive cruise control and roundabout control. The described technique can be adapted to learning driving strategies in other scenarios, such as lane change decisions, motorway exit and exit, negotiation of right of way at junctions, yielding for pedestrians, and complicated planning in urban scenarios."}], "references": [{"title": "Reinforcement learning with long short-term memory", "author": ["Bram Bakker"], "venue": "In NIPS, pages 1475\u20131482,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Dynamic programming and lagrange multipliers", "author": ["Richard Bellman"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1956}, {"title": "Introduction to the mathematical theory of control processes, volume", "author": ["Richard Bellman"], "venue": "IMA,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1971}, {"title": "Dynamic programming and optimal control, volume 1", "author": ["Dimitri P Bertsekas"], "venue": "Athena Scientific Belmont, MA,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "R-max\u2013a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Iterative solution of games by fictitious play", "author": ["George W Brown"], "venue": "Activity analysis of production and allocation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1951}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "A simple adaptive procedure leading to correlated", "author": ["S. HART", "A. MAS-COLELL"], "venue": "equilibrium. Econometrica,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Nash q-learning for general-sum stochastic games", "author": ["Junling Hu", "Michael P Wellman"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Reinforcement learning in robotics: A survey", "author": ["Jens Kober", "J Andrew Bagnell", "Jan Peters"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Theory and application of reward shaping in reinforcement learning", "author": ["Adam Daniel Laud"], "venue": "PhD thesis, University of Illinois at Urbana-Champaign,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["Michael L Littman"], "venue": "In Proceedings of the eleventh international conference on machine learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y Ng", "Daishi Harada", "Stuart Russell"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Reinforcement Learning with Recurrent Neural Network", "author": ["Anton Maximilian Sch\u00e4fer"], "venue": "PhD thesis, Universitat Osnabruck,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Reinforcement learning in markovian and non-markovian environments", "author": ["J\u00fcrgen Schmidhuber"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1991}, {"title": "If multi-agent learning is the answer, what is the question", "author": ["Yoav Shoham", "Rob Powers", "Trond Grenager"], "venue": "Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Deterministic policy gradient algorithms", "author": ["David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin Riedmiller"], "venue": "In ICML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Algorithms for reinforcement learning", "author": ["Csaba Szepesv\u00e1ri"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Learning to play the game of chess", "author": ["S. Thrun"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1995}, {"title": "Adversarial reinforcement learning", "author": ["William Uther", "Manuela Veloso"], "venue": "Technical report,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}, {"title": "A survey of solution techniques for the partially observed markov decision process", "author": ["Chelsea C White III"], "venue": "Annals of Operations Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1991}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) \u2014 see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.", "startOffset": 125, "endOffset": 140}, {"referenceID": 9, "context": "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) \u2014 see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.", "startOffset": 125, "endOffset": 140}, {"referenceID": 21, "context": "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) \u2014 see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.", "startOffset": 125, "endOffset": 140}, {"referenceID": 22, "context": "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) \u2014 see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.", "startOffset": 125, "endOffset": 140}, {"referenceID": 11, "context": "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) \u2014 see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.", "startOffset": 168, "endOffset": 172}, {"referenceID": 1, "context": "Most of the algorithms rely in some way or another on the mathematically elegant model of a Markov Decision Process (MDP), pioneered by the work of Bellman [2, 3].", "startOffset": 156, "endOffset": 162}, {"referenceID": 2, "context": "Most of the algorithms rely in some way or another on the mathematically elegant model of a Markov Decision Process (MDP), pioneered by the work of Bellman [2, 3].", "startOffset": 156, "endOffset": 162}, {"referenceID": 11, "context": "First, as noted in [12], usually in robotics, we may only be able to find some approximate notion of a Markovian behaving state.", "startOffset": 19, "endOffset": 23}, {"referenceID": 25, "context": "One possible solution to this problem is to use partially observed MDPs [27], in which we still assume that there is a Markovian state, but we only get to see an observation that is distributed according to the hidden state.", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "For example, the minimax-Q learning [14] or the Nash-Q learning [9].", "startOffset": 36, "endOffset": 40}, {"referenceID": 8, "context": "For example, the minimax-Q learning [14] or the Nash-Q learning [9].", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "Other approaches to Stochastic Games are explicit modeling of the other players, that goes back to Brown\u2019s fictitious play [6], and vanishing regret learning algorithms [8, 7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "Other approaches to Stochastic Games are explicit modeling of the other players, that goes back to Brown\u2019s fictitious play [6], and vanishing regret learning algorithms [8, 7].", "startOffset": 169, "endOffset": 175}, {"referenceID": 6, "context": "Other approaches to Stochastic Games are explicit modeling of the other players, that goes back to Brown\u2019s fictitious play [6], and vanishing regret learning algorithms [8, 7].", "startOffset": 169, "endOffset": 175}, {"referenceID": 24, "context": "See also [25, 24, 11, 5].", "startOffset": 9, "endOffset": 24}, {"referenceID": 23, "context": "See also [25, 24, 11, 5].", "startOffset": 9, "endOffset": 24}, {"referenceID": 10, "context": "See also [25, 24, 11, 5].", "startOffset": 9, "endOffset": 24}, {"referenceID": 4, "context": "See also [25, 24, 11, 5].", "startOffset": 9, "endOffset": 24}, {"referenceID": 19, "context": "As noted in [20], learning in multi-agent setting is inherently more complex than in the single agent setting.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "For example, the deep-Q-networks (DQN) learning algorithm of [16] has been successful at playing Atari games.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "[21]), but they again rely on approximating the Q function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Indeed, it was observed that value based methods rarely work out-ofthe-box in robotic applications [12], and that the best performing methods rely on a lot of prior knowledge and reward shaping [13, 17].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "Indeed, it was observed that value based methods rarely work out-ofthe-box in robotic applications [12], and that the best performing methods rely on a lot of prior knowledge and reward shaping [13, 17].", "startOffset": 194, "endOffset": 202}, {"referenceID": 16, "context": "Indeed, it was observed that value based methods rarely work out-ofthe-box in robotic applications [12], and that the best performing methods rely on a lot of prior knowledge and reward shaping [13, 17].", "startOffset": 194, "endOffset": 202}, {"referenceID": 18, "context": "A radically different approach has been introduced by Schmidhuber [19], who tackled the RL problem using a recurrent neural network (RNN).", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "Following [19], there have been several additional algorithms that rely on RNNs for RL problems.", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "For example, Backer [1] proposed to tackle the RL problem using recurrent networks with the LSTM architecture.", "startOffset": 20, "endOffset": 23}, {"referenceID": 17, "context": "Sch\u00e4fer [18] used RNN to model the dynamics of partially observed MDPs.", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "Most notably is the REINFORCE framework of Williams [28].", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "It has been recently successful for visual attention [15, 29].", "startOffset": 53, "endOffset": 61}, {"referenceID": 27, "context": "It has been recently successful for visual attention [15, 29].", "startOffset": 53, "endOffset": 61}, {"referenceID": 18, "context": "As already noted by [19], the ability of REINFORCE to estimate the derivative of stochastic units can be straightforwardly combined within the RNN framework.", "startOffset": 20, "endOffset": 24}, {"referenceID": 18, "context": "As noted in [19], explicitly expressing the dynamic of the system in a transparent way enables to incorporate prior knowledge more easily.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "st = ([st\u22121[0] + \u03c4a target t\u22121 ]+, [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+)", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "st = ([st\u22121[0] + \u03c4a target t\u22121 ]+, [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+)", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "st = ([st\u22121[0] + \u03c4a target t\u22121 ]+, [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+)", "startOffset": 81, "endOffset": 84}, {"referenceID": 0, "context": "= (st\u22121[0], [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+) } {{ } DNNN (st\u22121,at) +([st\u22121[0] + \u03c4a target t\u22121 ]+ \u2212 st\u22121[0], 0, 0) } {{ } \u03bdt", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "= (st\u22121[0], [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+) } {{ } DNNN (st\u22121,at) +([st\u22121[0] + \u03c4a target t\u22121 ]+ \u2212 st\u22121[0], 0, 0) } {{ } \u03bdt", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "= (st\u22121[0], [st\u22121[1] + \u03c4at\u22121]+, [st\u22121[2] + \u03c4(st\u22121[0]\u2212 st\u22121[1])]+) } {{ } DNNN (st\u22121,at) +([st\u22121[0] + \u03c4a target t\u22121 ]+ \u2212 st\u22121[0], 0, 0) } {{ } \u03bdt", "startOffset": 58, "endOffset": 61}], "year": 2016, "abstractText": "We consider planning problems, that often arise in autonomous driving applications, in which an agent should decide on immediate actions so as to optimize a long term objective. For example, when a car tries to merge in a roundabout it should decide on an immediate acceleration/braking command, while the long term effect of the command is the success/failure of the merge. Such problems are characterized by continuous state and action spaces, and by interaction with multiple agents, whose behavior can be adversarial. We argue that dual versions of the MDP framework (that depend on the value function and the Q function) are problematic for autonomous driving applications due to the non Markovian of the natural state space representation, and due to the continuous state and action spaces. We propose to tackle the planning task by decomposing the problem into two phases: First, we apply supervised learning for predicting the near future based on the present. We require that the predictor will be differentiable with respect to the representation of the present. Second, we model a full trajectory of the agent using a recurrent neural network, where unexplained factors are modeled as (additive) input nodes. This allows us to solve the long-term planning problem using supervised learning techniques and direct optimization over the recurrent neural network. Our approach enables us to learn robust policies by incorporating adversarial elements to the environment.", "creator": "LaTeX with hyperref package"}}}