{"id": "1602.01711", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2016", "title": "The Great Time Series Classification Bake Off: An Experimental Evaluation of Recently Proposed Algorithms. Extended Version", "abstract": "In the last five years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only 9 of these algorithms are significantly more accurate than both benchmarks and that one classifier, the Collective of Transformation Ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more rigorous testing of new algorithms in the future.", "histories": [["v1", "Thu, 4 Feb 2016 15:24:22 GMT  (54kb,D)", "http://arxiv.org/abs/1602.01711v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anthony bagnall", "aaron bostrom", "james large", "jason lines"], "accepted": false, "id": "1602.01711"}, "pdf": {"name": "1602.01711.pdf", "metadata": {"source": "CRF", "title": "The Great Time Series Classification Bake Off: An Experimental Evaluation of Recently Proposed Algorithms. Extended Version", "authors": ["Anthony Bagnall", "Aaron Bostrom"], "emails": ["ajb@uea.ac.uk", "a.bostrom@uea.ac.uk", "j.large@uea.ac.uk", "j.lines@uea.ac.uk"], "sections": [{"heading": "1. INTRODUCTION", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2. CLASSIFICATION ALGORITHMS", "text": "A case / instance is a pair {x, y} with m observations x1,.., xm (the time series) and a discrete class variable y with possible values. A list of n cases with associated class names is T = < X, y > = < (x1, y1),..., (xn, yn) >. A classifier is a function or mapping from the space of possible inputs to a probability distribution via the class variables values. The vast majority of time series research in the field of data mining focuses on alternative distance measurements that can be used for clustering, query and classification. For TSC, these distance measurements are classified almost excessively with a single neighbor (1-NN). The standard benchmark distance measurements are Euclidean distance (ED) and dynamic time distortion (DTW)."}, {"heading": "2.1 Time Domain Distance Based Classifiers", "text": "In 2008 Ding et al. [12] evaluated 8 different distance measurements on 38 datasets and found none significantly better than the DTW. Since then, three further elastic measures have been proposed."}, {"heading": "Weighted DTW (WDTW) [19]", "text": "Jeong et al. describe WDTW [19], which adds a multiplicative weight penalty based on the delay distance between the points in the delay path. It favors a reduced deformation and is a gentle alternative to the cutoff point approach of using a delay window. In creating the distance matrix M, a weight penalty w | i \u2212 j | is applied for a delay distance of | i \u2212 j | so that Mi, j = w | i \u2212 j | (ai \u2212 bj) 2.A logistic weight function is used so that a deformation of a point imposes a weight of w (a) = wmax1 + e \u2212 g \u00b7 (a \u2212 m / 2), where wmax is an upper limit for the weight (set to 1), m is the row length and g is a parameter that controls the penalty for large deformations. The greater g, the greater the penalty for deformation."}, {"heading": "Time Warp Edit (TWE) [25]", "text": "Marteau proposes the TWE distance [25] \u2212 \u2212 \u2212 21 \u2212 ai > i > j > i \u00b7 j \u00b7 j for j \u00b7 i \u00b7 j \u00b7 j for j \u00b7 j (11). It allows a distortion in the timeline and combines the processing distance with Lp \u2212 norms. The distortion is controlled by a stiffness parameter, a stiffness parameter. Stiffness forces a multiplicative penalty on the distance between the matching points similar to WDTW. \u2212 2: D (1, 1) \u2190 0 3: D (2, 1) \u2190 a12 4: D (1, 2): > (1) 5: i 2: 2 to 20 \u00b7 m + 1 matrix initialized to zero. \u2212 2: D (1, 1) \u2190 0 3: D (2, 1) \u2190 a12 4: D (1, 2): > (1) 5: i 2 to 20 \u00b7 j (1 \u2212 j), 1: 1 \u2212 j (1 \u2212 j) to 1 \u2212 i (1 \u2212 D)."}, {"heading": "Move-Split-Merge (MSM) [32]", "text": "Stefan et al. [32] present MSM Distance (Algorithm 2), a metric that is conceptually similar to other distance-based approaches, where similarity is calculated by using a series of operations to transform a given series into a target series. Moving is equivalent to a replacement operation in which one value is replaced by another. Columns and merges differ from other approaches in that they attempt to add context to inserts and deletions. Splitting inserts an identical copy of a value immediately after itself, and the merge process is used to delete a value if it directly follows an identical value. Algorithm 2 MSM (a, b) Parameters: Penalty value c 1: Let D be a m \u00b7 m matrix initialized to zero. 2: D (1, 1) Respect distance | a1 \u2212 b1 | 3: for i \u2190 2 to do 4: C + (1), C: 1 \u2212 D \u2212 i \u2212 i (i \u2212 1)."}, {"heading": "2.2 Differential Distance Based Classifiers", "text": "There is a group of algorithms based on the differences in the first order of the series, a \u2032 i = ai \u2212 b DC (1) DC (2) DC (1) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "2.3 Dictionary Based Classifiers", "text": "Dictionary-based approaches approach approximate and reduce the dimensionality of rows by converting them into representative words and then based on comparing the word distribution. The core process of dictionary approaches is to build words by passing a sliding window with the length w over each row, approximating each window to generate l-values, and then discrediting these values by assigning each one a symbol from an alphabet of size \u03b1."}, {"heading": "Bag of Patterns (BOP) [23]", "text": "BOP is a dictionary classifier based on the Symbolic Aggregate Approximation (SAX) method for converting series into strings [22]. SAX reduces the dimension of a series by Piecewise Aggregate Approximation (PAA) [8], then discredits the (normalized) series into containers formed of equal probability ranges of normal distribution.BOP works by applying SAX to each window to form a word. If successive windows produce identical words, only the first of this sequence is recorded, which is included to avoid overcounting trivial matches. The distribution of words over a row forms a counting histogramm.To classify new samples, the same transformation is applied to the new series and its closest neighbor within the training matrix. BOP sets the three parameters by cross-validation. Classification of new samples is done by a 1-Nclimatic distance between histograms."}, {"heading": "Symbolic Aggregate Approximation - Vector Space Model (SAXVSM) [31]", "text": "The main differences between BOP and SAXVSM are that SAXVSM forms word distributions via classes and weights them according to the frequency / frequency of the word (tf \u00b7 idf). For SAXVSM, the term frequency algorithm is 6 buildClassifierBOP (a list of n cases of length m, T = {X, y}) parameters: the word length l, the alphabet size \u03b1 and the window length w 1: Let H have a list of n histograms < h1,., hn > 2: for i cases of length m, T = {X, y}. Parameters: the word length l, the alphabet size \u03b1 and the window length w 1: Ltf."}, {"heading": "Bag of SFA Symbols (BOSS) [30]", "text": "BOSS also uses windows to form words about series, but there are several major differences from BOP > q q cases. Primarily, BOSS uses a truncated Discrete Fourier Transform (DFT) instead of a PAA in each window. Another difference is that the truncated series is discredited by a technique called Multiple Coefficient Binning (MCB), rather than using fixed intervals. MCB finds the disconnection points to be a pre-processing step by estimating the distribution of the Fourier coefficients by segmenting the series, running a DFT, and then determining breakpoints for each coefficient so that each bin contains the same number of elements. BOSS then includes stages similar to BOP; it uses windows for word distribution through the application of DFT and discrediting by MCB. A tailored distance function is used for classifying the nearest neighbors."}, {"heading": "Published Results for Dictionary Based Classifiers", "text": "BOP and SAXVSM were evaluated on the basis of the 20 and 19 UCR problems, respectively. All algorithms used the standard division of train and test. BOSS presents results on an extended list of 58 data sets from a number of sources, DTWF uses 47 UCR data. On the 19 data sets they all have in common, BOP is the algorithm 9 buildClassifierDTWF (A list of n cases of length m, T = {X, y}) parameters: the SVM order, SAX word length l, alphabet size \u03b1 and window length w, DTW window width r 1: Leave Z a list of n cases of length m, z1.., zninitialized to zero. 2: for i \u2190 1 to n do 3: for j word length l, alphabet size \u03b1 and window length w: DTW window width r 1: Leave Z a list of n cases of length 2n + al, z1, z1."}, {"heading": "2.4 Shapelet Based Classifiers", "text": "Shapelets are time sequences that discriminate against class affiliation and allow for the identification of phase-independent local similarities between series within the same class. Ye and Keogh's original Shapelets algorithm [33] used a shapelet as the division criterion for a decision tree. Recently, there have been three advances in the use of shapelets."}, {"heading": "Fast Shapelets (FS) [27]", "text": "Rakthanmanon and Keogh [27] propose an extension of the decision tree Shapelet approach [33, 26], which accelerates the Shapelet discovery. Instead of a complete enumerative search at each node, the Shapelets algorithm is discredited and approaches the Shapelets. Specifically, a dictionary of SAX words is first created for each possible Shapelet length. The dimensionality of the SAX dictionary is reduced by masking randomly selected letters (random projection). Multiple random projections are performed, and a frequency histogram is created for each class. A score for each SAX word can be calculated based on how well these frequency tables distinguish between classes. The k-best SAX words are then mapped back to the original Shapelets (random projection), which are evaluated on the information modulus of 10 SAX-D, with SAX-D-D-D-values:"}, {"heading": "Shapelet Transform (ST) [18, 6]", "text": "Hills et al. [18] propose a shapelet transformation that separates the shapelet discovery from the classifier by finding the top k shapelets in a single run (as opposed to the decision tree that searches for the best shapelet on each node), using shapelets to transform the data, with each attribute in the new dataset representing the distance of a row to one of the shapelets. We use the latest version of this transformation [6], which balances the number of shapelets per class and evaluates each shapelet according to how well it distinguishes only one class. The transformation described in algorithm 11 generates a new dataset. Following [2, 6] we construct a classifier from this dataset that uses a weighted set of standard classifiers and evaluates each shapelet according to how well it distinguishes only one class. We refer to Nearest Neighbour with Forest Machines (Naik Neighbour Case Machines), which validates binelve 11."}, {"heading": "Learned Shapelets (LS) [16]", "text": "Grabocka et al. [16] describe a shapelet discovery algorithm that uses a heuristic shapelet search method instead of enumeration. LS finds k-shapelets that, unlike the alternatives, are not limited to subseries in the training data. K-shapelets are initialized by a k-mean clustering of candidates from the training data. The objective function of the optimization process is a logistic loss function (with regression term) L based on a logistic regression model for each class. Together, the algorithm learns the weights for the regression W, and the shapelets S in a two-step iterative process to produce a final logistic regression model. Algorithm 12 learnShapelets (A list of n cases of length m, T = {X, y}) Parameters: Number of shapelets K, minimum length Lmin, regression size W."}, {"heading": "Published Results for Shapelet Based Classifiers", "text": "We can reproduce results that are not significantly different from FS and ST. Published results for FS are significantly inferior to those for LS and ST (see Figure 2). There is no significant difference between the published results of LS and ST. We can reproduce the output of the published code for LS, but are unable to reproduce the actual published results. LS's author believes that the difference is caused by the fact that we have not taken into account the adaptive learning rate adjustment implemented by Adagrad. We are working with him to include this improvement."}, {"heading": "2.5 Interval Based Classifiers", "text": "A family of algorithms derives characteristics from intervals in each series. For a series of lengths m, there are m (m \u2212 1) / 2 possible intervals that are connected. The two key decisions to use this approach are firstly, how to deal with the enormous increase in the dimension of the feature space, and secondly, what actually has to do with each interval. Rodriguez et al. [29] were the first to take this approach and address the first problem by using only intervals of lengths that correspond to two powers, and secondly, by calculating binary characteristics over each interval based on thresholds for the interval mean and the standard deviation. Subsequently, a support vector engine is trained on this transformed feature set. This algorithm was a precursor of three recently proposed interval-based classifiers that we implemented."}, {"heading": "Time Series Forest (TSF) [11]", "text": "Deng et al. [11] overcome the problem of the huge interval space by applying a random forest approach, using summary statistics (mean, standard deviation and slope) of each interval as characteristics. Each member of the ensemble receives \u221a m intervals. A classification tree that has two tailor-made characteristics is defined. Firstly, instead of evaluating all possible column points to find the best information gain, a fixed number of assessment points is predefined. We assume that it is expedient to speed up the classifier as it eliminates the need to sort cases by each attribute value. Secondly, a sophisticated slit criterion is introduced to choose between characteristics with equal information gain. \u2212 This is defined as the distance between the slit edge and the nearest case. The intuition behind the idea is that if two splits have the same drip gain from the next case, the closest one is the split gain."}, {"heading": "Time Series Bag of Features (TSBF) [5]", "text": "The first stage involves generating a subseries classification problem. The second stage forms class estimates for each subseries. The third stage constructs a set of characteristics for each original instance of these probabilities. It can informally be summarized as the following example. Level 1: Generate a subseries classification based on the pocket of the feature representation. 1. Select w subseries start and end points (line 7). These are the same for each of the complete series, repeat the following steps for each of the w subseries in the series, take v equivalent algorithms 14 buildClassifierTSBF (A list of n cases of length m, T = {X} parameters: the length factor z, the minimum interval duration and the number of number of individual subseries."}, {"heading": "Learned Pattern Similarity (LPS) [4]", "text": "It is also based on intervals, but the main difference is that sub-series become attributes, not cases. Like TSBF, the structure of the final model is initially an internal prediction model. However, LPS creates an internal prediction model instead of a classification model. The internal model is designed to detect correlations between sub-series, and in this sense, an approximation to an automatic prediction function is possible. LPS selects random sub-series in the original data are linked to form a new attribute. The internal model selects a random attribute as the reaction variable then constructs a regression tree. A collection of these regression trees are edited to form a new set of instances based on the number of sub-series, each based on the number of sub-series."}, {"heading": "Published Results for Interval Based Classifiers", "text": "TSF and TSBF were evaluated based on the initial 46 UCR problems, LPS based on an extended set of 75 data sets first used in [24] using the standard single train / test splits. Figure 3 shows the ranking of published results for the problem sets they have in common. Although TSBF has the highest average ranking, there is no significant difference between the classifiers at the 5% level. Paired comparisons do not show a significant difference between the threads. All three algorithms are stochastical, and our implementations are not identical, so there will inevitably be discrepancies between our results and those of the original software. Our implementation of TSF has higher accuracy on 21 of the 44 data sets, worse on 23. The mean difference in accuracy is less than 1%. There is no significant difference in means (at the 5% level) with a ranking test or a binomical test."}, {"heading": "2.6 Ensemble Classifiers", "text": "TSF, TSBF, and BOSS are ensembles based on the same core classifier. Other approaches, such as the ST ensemble described in Section 2.4, use different classification components. Two other recently proposed heterogeneous TSC ensembles are the following."}, {"heading": "Elastic Ensemble (EE) [24]", "text": "Lines and Bagnall [24] show that none of the individual components of the EE significantly exceeds the DTWCV. However, we show that by combining the predictions of 1-NN classifiers built with these distance measurements and using a matching scheme that significantly exceeds the DTWCV according to the accuracy set by cross validation, the 11 classifiers in the EE are 1-NN with euclidean distances (ED), fully dynamic time delay (DTW), DTW with the window size set by cross validation (DTWCV), derived DTW with full windows and windows by cross validation (DTW and DTWCV), weighted DTW (WTW) and Dvative DWW (DWW)."}, {"heading": "2.7 Summary", "text": "For example, TSBF is an interval and ensemble-based approach, and LPS is based on autocorrelation. Table 1 shows the approach of breaking down algorithm verses. There are many other approaches that have been suggested that we have not considered due to time constraints and non-compliance with our inclusion criteria. Two worth noting are Silva et al. \"s Recurrence Plot Compression Distance (RPCD) [9] and Fulcher and Jones\" feature-based linear classifier (FBL) [13]. RPCD involves transforming each series into a two-dimensional recurrence plot, which then measures similarity based on the size of the MPEG1 encoding of the concatenation of the resulting images. We were unable to find a working Java-based MPEG1 encoder, and the technique does not seem to work with the MPEG4 encoders."}, {"heading": "3. DATA AND EXPERIMENTAL DESIGN", "text": "The mentioned the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "4. RESULTS", "text": "For reasons of space, we do not present complete data, but an analysis of our results. All our results and tables to derive the diagrams are available under [1]."}, {"heading": "4.1 Benchmark Classifiers", "text": "The most obvious starting point for any classification problem is the use of a standard classifier that treats each series with a vector (i.e., no explicit use of any autocorellation structures is made). Some of the features that complicate TSC problems include some cases, long series (large number of attributes), many of which are redundant or correlated. These are problems that have been well studied in machine learning and classifiers to compensate for them. TSC features that refute traditional classifiers include discriminatory features in the autocorrelation function, phase independence within a class, and embedded discriminatory sub-series. However, not all problems will have this feature, and benchmarking against standard classifiers can provide insight into the problem characteristics."}, {"heading": "4.2 Comparison Against Benchmark Classifiers", "text": "Table 4 summarizes the pairwise results of the 19 classifiers against DTW and RotF. Nine classifiers are significantly better than both benchmarks: COTE; ST; BOSS; EE; DTWF; TSF; TSBF; LPS; and MSM. BOP, SAXVSM, and FS are all significantly worse than both benchmarks. This reflects the published FS results, but is worse than expected for BOP and SAXVSM."}, {"heading": "4.3 Comparison of All TSC Algorithms", "text": "Figure 5 shows the crucial difference between the nine classifiers, which are significantly better than both benchmarks. The above conclusion from this graph is that COTE is significantly better than the others. EE and ST are components of COTE, so this result shows the benefits of combining classifiers in alternative attribute spaces. The second characteristic is the good performance of BOSS and to a lesser extent of DTWF. We discuss these results in detail below."}, {"heading": "4.4 Results by Algorithm Type", "text": "This year it is more than ever in the history of the city we are in. It is only a matter of time before we will go in search, until we will go in search."}, {"heading": "4.5 Results by Problem Type", "text": "Sample sizes are small, so we must be careful to draw too many conclusions. However, this table shows how the evaluation can provide insights into problem areas. For example, shapelets are best for 4 out of 6 ElectricDevice problems and 3 out of 6 ECG data sets, but overall only 26% of problems. This makes sense in terms of applications, as the profile of power consumption and ECG irregularity will be a subset of the entire and largely phase-independent set. Vector classifiers are best for 43% of spectrograph data sets. COTE is the best algorithm for over 40% of outline problems, suggesting that there are a number of features that help classify these problems, and no single representation is likely to be sufficient."}, {"heading": "5. CONCLUSIONS", "text": "eeisrrVnllrtee\u00fcgr rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "Acknowledgment", "text": "This work is supported by the UK Engineering and Physical Sciences Research Council (EPSRC) [grant number EP / M015087 / 1] and the experiments were carried out in the High Performance Computing Cluster supported by the Research and Specialist Computing Support Service of the University of East Anglia. We would like to thank Leo Earl for his help and forbearance with our excessive computing requirements."}, {"heading": "6. REFERENCES", "text": "[1] A. Bagnall and J. Tuv. A bag-of-features to classification time series. http: / / timeseriesclassification.com. [2] A. Bagnall, J. Lines, J. Hills, and A. Bostrom. J. Schmidt-series classification with COTE: the collective of transformation-based ensembles. IEEE Transactions on Knowledge and Data Engineering, 27: 2522-2535, 2015. [3] G. Batista, E. Keogh, O. Tataw, and V. deSouza. CID: an efficient complexity-invariant distance measure for time series. Data Mining and Knowledge Discovery, 28 (3): 634-669, 2014. M. Baydogan and G. Runger. Time series representation and similarity based on local autopatterns. Data Mining and Knowledge Discovery, Discovery, 2015. [5] M. Baydogan, G. Runger."}], "references": [{"title": "Time-series classification with COTE: The collective of transformation-based ensembles", "author": ["A. Bagnall", "J. Lines", "J. Hills", "A. Bostrom"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "CID: an efficient complexity-invariant distance measure for time series", "author": ["G. Batista", "E. Keogh", "O. Tataw", "V. deSouza"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Time series representation and similarity based on local autopatterns", "author": ["M. Baydogan", "G. Runger"], "venue": "Data Mining and Knowledge Discovery, online first,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "A bag-of-features framework to classify time series", "author": ["M. Baydogan", "G. Runger", "E. Tuv"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Binary shapelet transform for multiclass time series classification", "author": ["A. Bostrom", "A. Bagnall"], "venue": "In Proc.17th DaWaK,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Locally adaptive dimensionality reduction for indexing large time series databases", "author": ["K. Chakrabarti", "E. Keogh", "S. Mehrotra", "M. Pazzani"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Time series classification using compression distance of recurrence plots", "author": ["G. Batista D. Silva", "V. de Souza"], "venue": "In Proc. 13th IEEE ICDM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Dem\u0161ar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "A time series forest for classification and feature extraction", "author": ["H. Deng", "G. Runger", "E. Tuv", "M. Vladimir"], "venue": "Information Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Querying and mining of time series data: Experimental comparison of representations and distance measures", "author": ["H. Ding", "G. Trajcevski", "P. Scheuermann", "X. Wang", "E. Keogh"], "venue": "In Proc. 34th VLDB,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Highly comparative feature-based time-series classification", "author": ["B. Fulcher", "N. Jones"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Using derivatives in time series classification", "author": ["T. G\u00f3recki", "M. Luczak"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Non-isometric transforms in time series classification using DTW", "author": ["T. G\u00f3recki", "M. Luczak"], "venue": "Knowledge-Based Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Learning time-series shapelets", "author": ["J. Grabocka", "N. Schilling", "M. Wistuba", "L. Schmidt-Thieme"], "venue": "In Proc. 20th SIGKDD,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "The WEKA data mining software: An update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I. Witten"], "venue": "SIGKDD Explorations,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Classification of time series by shapelet transformation", "author": ["J. Hills", "J. Lines", "E. Baranauskas", "J. Mapp", "A. Bagnall"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Weighted dynamic time warping for time series classification", "author": ["Y. Jeong", "M. Jeong", "O. Omitaomu"], "venue": "Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Using dynamic time warping distances as features for improved time series classification", "author": ["R. Kate"], "venue": "Data Mining and Knowledge Discovery, online first,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Experiencing SAX: a novel symbolic representation of time series", "author": ["J. Lin", "E. Keogh", "W. Li", "S. Lonardi"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Rotation-invariant similarity in time series using bag-of-patterns representation", "author": ["J. Lin", "R. Khade", "Y. Li"], "venue": "Journal of Intelligent Information Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Time series classification with ensembles of elastic distance measures", "author": ["J. Lines", "A. Bagnall"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Time warp edit distance with stiffness adjustment for time series matching", "author": ["P. Marteau"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Logical-shapelets: An expressive primitive for time series classification", "author": ["A. Mueen", "E. Keogh", "N. Young"], "venue": "In Proc. 17th SIGKDD,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Fast-shapelets: A fast algorithm for discovering robust time series shapelets", "author": ["T. Rakthanmanon", "E. Keogh"], "venue": "In Proc. 13th SDM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Three myths about dynamic time warping data mining", "author": ["C. Ratanamahatana", "E. Keogh"], "venue": "In Proc. 5th SDM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "Support vector machines of interval-based features for time series classification", "author": ["J. Rod\u0155\u0131guez", "C. Alonso", "J. Maestro"], "venue": "Knowledge-Based Systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "The BOSS is concerned with time series classification in the presence of noise", "author": ["P. Sch\u00e4fer"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "SAX-VSM: interpretable time series classification using sax and vector space model", "author": ["P. Senin", "S. Malinchik"], "venue": "In Proc. 13th IEEE ICDM,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "The Move-Split-Merge metric for time series", "author": ["A. Stefan", "V. Athitsos", "G. Das"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Time series shapelets: a novel technique that allows accurate, interpretable and fast classification", "author": ["L. Ye", "E. Keogh"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "To address these problems we have implemented 20 different TSC algorithms in Java, integrated with the WEKA toolkit [17].", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": "Secondly, of those 8 significantly better than both benchmarks, by far the best classifier is COTE [2], an algorithm we proposed.", "startOffset": 99, "endOffset": 102}, {"referenceID": 24, "context": "It has been shown that setting r through cross validation to maximize training accuracy, as proposed in [28], significantly increases accuracy [24].", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "It has been shown that setting r through cross validation to maximize training accuracy, as proposed in [28], significantly increases accuracy [24].", "startOffset": 143, "endOffset": 147}, {"referenceID": 9, "context": "[12] evaluated 8 different distance measures on 38 data sets and found none significantly better than DTW.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Weighted DTW (WDTW) [19]", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "describe WDTW [19], which adds a multiplicative weight penalty based on the warping distance between points in the warping path.", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "Time Warp Edit (TWE) [25]", "startOffset": 21, "endOffset": 25}, {"referenceID": 21, "context": "Marteau propose the TWE distance [25], an elastic distance metric that includes characteristics from both LCSS and DTW.", "startOffset": 33, "endOffset": 37}, {"referenceID": 28, "context": "Move-Split-Merge (MSM) [32]", "startOffset": 23, "endOffset": 27}, {"referenceID": 28, "context": "[32] present MSM distance (Algorithm 2), a metric that is conceptually similar to other edit distance-", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In [24] it was shown that there is no significant difference between 1-NN with DTW and with WDTW, TWE or MSM on a set of 72 problems using a single train test split.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "Various methods that have used just the differences have been described [19], but the most successful approaches combine distance in the time domain and the difference domain.", "startOffset": 72, "endOffset": 76}, {"referenceID": 1, "context": "Complexity Invariant distance (CID) [3]", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "[3] describe a means of weighting a distance measure to compensate for differences in the complexity in the two series being compared.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Derivative DTW (DDDTW ) [14] Algorithm 3 CID(a,b)", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "G\u00f3recki and Luczak [14] describe an approach for using a weighted combination of raw series and first-order differences for NN classification with either the Euclidean distance or full-window DTW.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "An optimisation to reduce the search space of possible parameter values is proposed in [14].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "Derivative Transform Distance (DTDC) [15]", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "G\u00f3recki and Luczak proposed an extension of DDDTW that uses DTW in conjunction with transforms and derivatives [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "Bag of Patterns (BOP) [23]", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "BOP is a dictionary classifier built on the Symbolic Aggregate Approximation (SAX) method for converting series to strings [22].", "startOffset": 123, "endOffset": 127}, {"referenceID": 5, "context": "SAX reduces the dimension of a series through Piecewise Aggregate Approximation (PAA) [8], then discretises the (normalised) series into bins formed from equal probability areas of the Normal distribution.", "startOffset": 86, "endOffset": 89}, {"referenceID": 27, "context": "Symbolic Aggregate Approximation - Vector Space Model (SAXVSM) [31]", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "Bag of SFA Symbols (BOSS) [30]", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": "DTW Features (DTWF ) [20]", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "Kate [20] proposes a feature generation scheme that combines DTW distances to training cases and SAX histograms.", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "The original shapelets algorithm by Ye and Keogh [33] uses a shapelet as the splitting criterion for a decision tree.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Fast Shapelets (FS) [27]", "startOffset": 20, "endOffset": 24}, {"referenceID": 23, "context": "Rakthanmanon and Keogh [27] propose an extension of the decision tree shapelet approach [33, 26] that speeds up shapelet discovery.", "startOffset": 23, "endOffset": 27}, {"referenceID": 29, "context": "Rakthanmanon and Keogh [27] propose an extension of the decision tree shapelet approach [33, 26] that speeds up shapelet discovery.", "startOffset": 88, "endOffset": 96}, {"referenceID": 22, "context": "Rakthanmanon and Keogh [27] propose an extension of the decision tree shapelet approach [33, 26] that speeds up shapelet discovery.", "startOffset": 88, "endOffset": 96}, {"referenceID": 29, "context": "The k-best SAX words are selected then mapped back to the original shapelets, which are assessed using information gain in a way identical to that used in [33].", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "Shapelet Transform (ST) [18, 6]", "startOffset": 24, "endOffset": 31}, {"referenceID": 4, "context": "Shapelet Transform (ST) [18, 6]", "startOffset": 24, "endOffset": 31}, {"referenceID": 15, "context": "[18] propose a shapelet transformation that separates the shapelet discovery from the classifier by finding the top k shapelets on a single run (in contrast to the decision tree, which searches for the best shapelet at each node).", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "We use the most recent version of this transform [6] that balances the number of shapelets per class and evaluates each shapelet on how well it discriminates just one class.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "Following [2, 6] we construct a classifier from this dataset using a weighted ensemble of standard classifiers.", "startOffset": 10, "endOffset": 16}, {"referenceID": 4, "context": "Following [2, 6] we construct a classifier from this dataset using a weighted ensemble of standard classifiers.", "startOffset": 10, "endOffset": 16}, {"referenceID": 13, "context": "Learned Shapelets (LS) [16]", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "[16] describe a shapelet discovery algorithm that adopts a heuristic gradient descent shapelet search procedure rather than enumeration.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[29] were the first to adopt this approach and address the first issue by using only intervals of lengths equal to powers of two and the second by calculating binary features over each intervals based on threshold rules on the interval mean and standard deviation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Time Series Forest (TSF) [11]", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "[11] overcome the problem of the huge interval feature space by employing a random forest approach, using summary statistics (mean, standard deviation and slope) of each interval as features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Time Series Bag of Features (TSBF) [5]", "startOffset": 35, "endOffset": 38}, {"referenceID": 2, "context": "Learned Pattern Similarity (LPS) [4]", "startOffset": 33, "endOffset": 36}, {"referenceID": 20, "context": "TSF and TSBF were evaluated on the original 46 UCR problems, LPS on an extended set of 75 data sets first used in [24] using the standard single train/test splits.", "startOffset": 114, "endOffset": 118}, {"referenceID": 20, "context": "archive have been normalised, whereas many of the data proposed in [24] are not normalised.", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "Elastic Ensemble (EE) [24]", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "Lines and Bagnall [24] show that none of the individual components of EE significantly outperforms DTWCV.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "The 11 classifiers in EE are 1-NN with Euclidean distance (ED), full dynamic time warping (DTW), DTW with window size set through cross validation (DTWCV), derivative DTW with full window and window set through cross validation (DDTW and DDTWCV), weighted DTW (WDTW) and derivative weighted DTW (WDDTW) [19], longest common subsequence (LCSS), Edit Distance with Real Penalty (ERP), Time Warp Edit (TWE) distance [25], and the Move-SplitMerge (MSM) distance metric [32].", "startOffset": 303, "endOffset": 307}, {"referenceID": 21, "context": "The 11 classifiers in EE are 1-NN with Euclidean distance (ED), full dynamic time warping (DTW), DTW with window size set through cross validation (DTWCV), derivative DTW with full window and window set through cross validation (DDTW and DDTWCV), weighted DTW (WDTW) and derivative weighted DTW (WDDTW) [19], longest common subsequence (LCSS), Edit Distance with Real Penalty (ERP), Time Warp Edit (TWE) distance [25], and the Move-SplitMerge (MSM) distance metric [32].", "startOffset": 413, "endOffset": 417}, {"referenceID": 28, "context": "The 11 classifiers in EE are 1-NN with Euclidean distance (ED), full dynamic time warping (DTW), DTW with window size set through cross validation (DTWCV), derivative DTW with full window and window set through cross validation (DDTW and DDTWCV), weighted DTW (WDTW) and derivative weighted DTW (WDDTW) [19], longest common subsequence (LCSS), Edit Distance with Real Penalty (ERP), Time Warp Edit (TWE) distance [25], and the Move-SplitMerge (MSM) distance metric [32].", "startOffset": 465, "endOffset": 469}, {"referenceID": 0, "context": "Collective of Transformation Ensembles (COTE) [2]", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "We use the classifier called flat-COTE in [2].", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "\u2019s Recurrence Plot Compression Distance (RPCD) [9] and Fulcher and Jones\u2019s feature-based linear classifier (FBL) [13].", "startOffset": 47, "endOffset": 50}, {"referenceID": 10, "context": "\u2019s Recurrence Plot Compression Distance (RPCD) [9] and Fulcher and Jones\u2019s feature-based linear classifier (FBL) [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "is further worth noting that COTE produces significantly better results than both RPCD and FBL [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "We follow the basic methodology described in [10] when testing for significant difference between classifiers.", "startOffset": 45, "endOffset": 49}, {"referenceID": 24, "context": "A more useful benchmark is 1-NN dynamic time warping with a warping window set through cross validation (DTW) [28].", "startOffset": 110, "endOffset": 114}, {"referenceID": 20, "context": "This conclusion contradicts the results in [24] which found no difference between all the elastic algorithms and DTW.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "We made no attempts to speed up the distance measures, and of all the measures used in [24], MSM and TWE were by far the slowest.", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "The changes proposed in [6] have not only made it much faster, but have also increased accuracy.", "startOffset": 24, "endOffset": 27}], "year": 2016, "abstractText": "In the last five years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only 9 of these algorithms are significantly more accurate than both benchmarks and that one classifier, the Collective of Transformation Ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more rigorous testing of new algorithms in the future.", "creator": "LaTeX with hyperref package"}}}