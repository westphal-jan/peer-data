{"id": "1602.01895", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Generate Image Descriptions based on Deep RNN and Memory Cells for Images Features", "abstract": "Generating natural language descriptions for images is a challenging task. The traditional way is to use the convolutional neural network (CNN) to extract image features, followed by recurrent neural network (RNN) to generate sentences. In this paper, we present a new model that added memory cells to gate the feeding of image features to the deep neural network. The intuition is enabling our model to memorize how much information from images should be fed at each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed that our model outperforms other state-of-the-art models with higher BLEU scores.", "histories": [["v1", "Fri, 5 Feb 2016 00:17:18 GMT  (140kb,D)", "http://arxiv.org/abs/1602.01895v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["shijian tang", "song han"], "accepted": false, "id": "1602.01895"}, "pdf": {"name": "1602.01895.pdf", "metadata": {"source": "CRF", "title": "Generate Image Descriptions based on Deep RNN and Memory Cells for Images Features", "authors": ["Shijian Tang", "Song Han"], "emails": ["sjtang@stanford.edu", "songhan@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "However, the task is to generate sentences or phrases in order to summarize and describe the contents represented in images. By means of this technique, the machines are able to imitate the behavior of people who are able to grasp the semantic meaning in images. The task is to fill in the templates based on the images. These approaches are very limited to the ability of models to generate sentence descriptions. Other approaches transfer this task into a multimodal embedding problem. This work by Farhadi et al. (2011), Jia et al. (2011), Ordonez et al."}, {"heading": "2 The Architecture of Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Image Features Representation", "text": "CNN has proven to be a powerful tool for extracting image characteristics and has been widely used in image classification (Krizhevsky et al. (2012), object recognition (Girshick et al. (2014)) and other tasks. In this essay, we select the deep and powerful VGGNet to extract image characteristics. Specifically, each raw image is fed as input into VGGNet. After forward propagation, the last fully connected layer will output a 4096 dimension vector as an image characteristic for each image."}, {"heading": "2.2 Sentence Representation", "text": "The sentence can be represented as a sequence of a single word. The time step t is defined as the index of the t th word in the sentence (+ 1 layer) to represent the position of each word. Suppose the sentence contains T words, the time step of the first word is t = 1, the second word is t = 2, and for the last word is t = T. For each sentence we add a special START symbol at the first step to indicate the beginning of the sentence, as well as the END symbol at the last step as the end of each sentence. The single word is represented as a vector. Some pre-trained word vector models have been developed such as word2vec by Mikolov et al. (2013) and Glove by Pennington et al. (2014) However, in this model we trained the word vectors of scratch instead of the pre-trained model, because in general the re-trained word vector is more powerful for specific task.The standard RNN model can be expressed as h ()."}, {"heading": "2.3 Memory Cells for Image Features", "text": "The value of the gate depends on the state of the hidden layers in the previous time step.h1 (t) = f (W sx (t) + W hhN (t \u2212 1) + g (t).W iCNN (I) is the image characteristics extracted from CNN. W i has the dimension of H by 4096, which puts the image characteristics in the same space of the hidden layers of RNN. g (t) is the output of the gate, and we are the element-by-element multiplication. W g transfers the value of the last hidden layer into the previous time step (hN \u2212 1) into the same space of the hidden layers of RNN. g (t) is the output of the gate, and we are the elephant multiplication. W g transfers the value of the last hidden layer into the previous time step."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Dataset", "text": "We experimented with the data sets Flickr8K and Flickr30K introduced in Hodosh et al. (2013). Each image in these data sets is described by 5 independent sets. Therefore, for each image we can create 5 samples with one educational pair each. We have 8000 and 31000 images for Flickr8K and Flickr30K respectively. Each data set has been divided into development data with 1000 images, test data with 1000 images and the remaining images as training data."}, {"heading": "3.2 Training", "text": "Stochastic Gradient Descent (SGD) with a minibatch size of 100 pairs of frames was used during the training. To accelerate the convergence of the model, RMSprop Annealing Policy Hinton et al. (2012) was chosen as the activation function, in which the step size of each parameter is scaled by the window offset norm of its gradient. To overcome the problem of the disappearing gradient, ReLU was chosen as the activation function. In addition, we used the elemental clip gradient tricks, in which we cut the gradient to 5. To regulate the model, we add the standard of weights to the loss function L2, and as suggested by Zaremba et al. (2014), we used a failure ratio of 0.5 to all layers except the hidden layers. As Equation 4 shows, a model with a large N indicates deeper hidden layers, resulting in a large capacity."}, {"heading": "3.3 Generate Image Description", "text": "The sentence description for each image in the test data set is generated by feeding the image characteristics into the trained model with a START token. At any time, we can directly select the word that corresponds to the most likely word in the vocabulary as the output word, which is also the input word for the next time step. Following this method, we can generate a sentence repetitively until we reach the END token. (2014) To evaluate the performance, we use the BLEU rating as the yardstick that is widely used in the papers to focus on this topic (Carpathy (2015), Vinyals et al. (2014), Mao et al. (2014), The BLEU rating will evaluate the similarity of the sentences generated with the principles of truth. Table 1 and Table 2 show the BLEU rating for several models. As shown on Table 1 and Table 2, our model surpasses the results from Karpathie (2015) Mao al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al"}, {"heading": "4 Conclusion", "text": "In this paper, we developed a new model for image description generation. The image characteristics extracted from VGGNet are fed into each time step of a multi-layer deep RNN, with the image characteristic vector element by element multiplied by a memory vector determined by the state of the hidden layer in the previous time step. Experiments with Flickr8K and Flickr30K data sets show that this model achieves higher performance in the BLEU score. Our model also benefits from its low complexity and ease of training. As an extension of this work, we will train our model on a larger data set such as MSCOCO and increase the number of hidden layers in each time step to further improve the performance of our model. We will also try to adopt other CNNs such as GoogleNet to extract image characteristics."}], "references": [{"title": "From image annotation to image description", "author": ["Gupta", "Mannem2012] Ankush Gupta", "Prashanth Mannem"], "venue": null, "citeRegEx": "Gupta et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2012}, {"title": "Alexander Berg", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi"], "venue": "and Tamara Berg.", "citeRegEx": "Kulkarni et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Image description using visual dependency representations", "author": ["Desmond Elliott", "Frank Keller"], "venue": null, "citeRegEx": "Elliott et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2013}, {"title": "Julia Hockenmaier", "author": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian"], "venue": "and David Forsyth.", "citeRegEx": "Farhadi et al. 2011", "shortCiteRegEx": null, "year": 2010}, {"title": "Mathieu Salzmann", "author": ["Yangqing Jia"], "venue": "and Trevor Darrell.", "citeRegEx": "Jia et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Andrew Y", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning"], "venue": "Ng.", "citeRegEx": "Socher et al. 2011", "shortCiteRegEx": null, "year": 2014}, {"title": "and Tamara L", "author": ["Vicente Ordonez", "Girish Kulkarni"], "venue": "Berg.", "citeRegEx": "Ordonez et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": null, "citeRegEx": "Karpathy and Fei.Fei.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Samy Bengio", "author": ["Oriol Vinyals", "Alexander Toshev"], "venue": "and Dumitru Erhan.", "citeRegEx": "Vinyals et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "and Alan L", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang"], "venue": "Yuille.", "citeRegEx": "Mao et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Peter Young", "author": ["Micah Hodosh"], "venue": "and Julia Hockenmaier.", "citeRegEx": "Hodosh et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Todd Ward", "author": ["Kishore Papineni", "Salim Roukos"], "venue": "and Wei-Jing Zhu", "citeRegEx": "Papineni et al. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Ilya Sutskever", "author": ["Alex Krizhevsky"], "venue": "and Geoffrey Hinton.", "citeRegEx": "Krizhevsky et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Trevor Darrell", "author": ["Ross Girshick", "Jeff Donahue"], "venue": "and Jitendra Malik", "citeRegEx": "Girshick et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Greg Corrado", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen"], "venue": "and Jeffrey Dean", "citeRegEx": "Mikolov et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Christopher D", "author": ["Jeffrey Pennington", "Richard Socher"], "venue": "Manning", "citeRegEx": "Pennington et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Kyunghyun Cho", "author": ["Razvan Pascanu", "Caglar Gulcehre"], "venue": "and Yoshua Bengio", "citeRegEx": "Pascanu et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Nitish Srivastava", "author": ["Geoffrey Hinton"], "venue": "and Kevin Swersky.", "citeRegEx": "Hinton et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Ilya Sutskever", "author": ["Wojciech Zaremba"], "venue": "Oriol Vinyals", "citeRegEx": "Zaremba et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Vincent Vanhoucke", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan"], "venue": "Andrew Rabinovich", "citeRegEx": "Szegedy et al. 2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "Generating natural language descriptions for images is a challenging task. The traditional way is to use the convolutional neural network (CNN) to extract image features, followed by recurrent neural network (RNN) to generate sentences. In this paper, we present a new model that added memory cells to gate the feeding of image features to the deep neural network. The intuition is enabling our model to memorize how much information from images should be fed at each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed that our model outperforms other state-of-the-art models with higher BLEU scores.", "creator": "LaTeX with hyperref package"}}}