{"id": "1602.01910", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Fast Multiplier Methods to Optimize Non-exhaustive, Overlapping Clustering", "abstract": "Clustering is one of the most fundamental and important tasks in data mining. Traditional clustering algorithms, such as K-means, assign every data point to exactly one cluster. However, in real-world datasets, the clusters may overlap with each other. Furthermore, often, there are outliers that should not belong to any cluster. We recently proposed the NEO-K-Means (Non-Exhaustive, Overlapping K-Means) objective as a way to address both issues in an integrated fashion. Optimizing this discrete objective is NP-hard, and even though there is a convex relaxation of the objective, straightforward convex optimization approaches are too expensive for large datasets. A practical alternative is to use a low-rank factorization of the solution matrix in the convex formulation. The resulting optimization problem is non-convex, and we can locally optimize the objective function using an augmented Lagrangian method. In this paper, we consider two fast multiplier methods to accelerate the convergence of an augmented Lagrangian scheme: a proximal method of multipliers and an alternating direction method of multipliers (ADMM). For the proximal augmented Lagrangian or proximal method of multipliers, we show a convergence result for the non-convex case with bound-constrained subproblems. These methods are up to 13 times faster---with no change in quality---compared with a standard augmented Lagrangian method on problems with over 10,000 variables and bring runtimes down from over an hour to around 5 minutes.", "histories": [["v1", "Fri, 5 Feb 2016 02:08:57 GMT  (147kb,D)", "http://arxiv.org/abs/1602.01910v1", "9 pages. 2 figures"]], "COMMENTS": "9 pages. 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yangyang hou", "joyce jiyoung whang", "david f gleich", "inderjit s dhillon"], "accepted": false, "id": "1602.01910"}, "pdf": {"name": "1602.01910.pdf", "metadata": {"source": "CRF", "title": "Fast Multiplier Methods to Optimize Non-exhaustive, Overlapping Clustering", "authors": ["Yangyang Hou", "Joyce Jiyoung Whang", "David F. Gleich", "Inderjit S. Dhillon"], "emails": ["dgleich}@purdue.edu", "jjwhang@skku.edu", "inderjit@cs.utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 The NEO-K-Means Objective", "text": "The objective of non-exhaustive, overlapping cluster formation is to find a set of contiguous clusters, so that clusters can overlap each other and outliers can not be assigned to any cluster. That is, if a set of data points X = {x1, x2,... xn}, we find a set of clusters C1, C2,..., Ck, so that C1, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C3, C3, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C3, C3, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C1, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2, C2,"}, {"heading": "3 The Augmented Lagrangian Method for the NEO-K-Means Low-Rank SDP", "text": "The advanced Lagrange technique is an iterative process in which each iteration is performed by minimizing an advanced Lagrange problem, which includes an up-to-date estimate of the Lagrange multipliers for the constraints and a square penalty term that forces the feasibility of the solution. We present it here because we will rely heavily on the notation for our later outcomes. Let \u03bb = [\u03b1 1; \u03bb2; \u03bb3] find the Lagrange multipliers for the three scalar constraints (a), (c), (d), let us find the corresponding Lagrange multipliers (\u03bb2; \u03bb3] the f multipliers for the three scalar constraints (a), (c), (e)."}, {"heading": "4 Fast Multiplier Methods for the NEO-K-Means Low-Rank SDP", "text": "The interest in proximal point methods and alternative methods for convex and almost convex targets in machine learning is growing again due to their rapid convergence rate. At this point we propose two variants of the classic extended Lagrange approach to the problem (2.2), which can use some of these techniques for improved speed."}, {"heading": "4.1 Proximal Augmented Lagrangian (PALM).", "text": "The Lagrangian Proximal Method differs from the classical augmented Lagrangian Method only in an additional proximal regularization term for primary updates, which can be considered as a kind of simultaneous primary-dual-point step, helping to regulate the partial problems solved in each step. This idea leads to the following iterations: xk + 1 = argmin x = argmin x = argmin x = argmin x-DLA (x; p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, and regularization, proximal, and regularization, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal, proximal"}, {"heading": "5 Convergence Analysis of the Proximal Augmented Lagrangian", "text": "For these cases, local convergence is the best method we can achieve. From Pennanen [19] we know that the proximal method of multipliers is locally convergent for a general class of problems with sufficient assumptions. We will show that our proximal method of multiplier algorithms can be handled in (2,2) ways, and we extend their analysis to our case. Because we imitate the analysis from Pennanen for a specific new problem, we have decided to explicitly imitate the original language to highlight the changes in the derivative."}, {"heading": "6 Simplified Alternating Direction Method of Multipliers", "text": "One disadvantage of the two proposed methods is that they involve the use of the L-BFGS-B method to solve the bound non-convex targets in the sub-steps. It is a complex routine with a high runtime itself. In this section, we are interested to see if there are simplified ADMM variants that could further improve runtime by avoiding this non-convex solver. For example, this corresponds to imprecise ADMM (which allows imprecise, alternating minimization solutions, e.g. one proximal gradient step per block). In the ADMM method from Section 4.2, we know that updating the block variables f, g, s, and r is simple so that we can get globally optimal solutions. The only hard part is updating Y, which is non-convex."}, {"heading": "7 Experimental Results", "text": "In this section, we will demonstrate the efficiency of our proposed methods on real-world problems. Our primary goal is to compare our two new methods, PALM and ADMM, in terms of optimization capability with the classic extended Lagrange method (ALM) (2.2), all of which are implemented in MATLAB and use the L-BFGS-B routine written in Fortran [6] to solve the limited nonlinear partial problems."}, {"heading": "7.1 Convergence Analysis on the Karate Club", "text": "We use the Zachary Karate Club Network [25], which is a small social network between 34 members of a karate club.Figure 1 shows (a) the infinity standard of the constraint Vector and (b) the NEO-K Means shows low-level SDP functional values defined in (2,2) over time. We set the feasibility tolerance for L-BFGS-B to less than 10 \u2212 3. Both of our methods, PALM and ADMM, achieve a faster convergence than ALM both in terms of the feasibility of the solution as well as the objective functional value, mainly because the partial problems for L-BFGS-B are solved more quickly. To show that the common variants of ADMM do not accelerate the convergence in our problem, we also compare them with the simplified method of the alternative direction of the multipliers (Section 6, which is referred to by SADM as the SADM problem), the solution of SADM-MADM is significantly lower."}, {"heading": "7.2 Data Clustering on Real-world Datasets.", "text": "Next, we will compare the three methods (ALM, PALM and ADMM) on larger datasets. We will use three different datasets from [1]. The SCENE datasets [4] contain 2,407 scenes represented as feature vectors; the YEAST datasets [9] consist of 2,417 genes in which the traits are based on micro-array expression data and phylogenetic profiles; the MUSIC datasets [23] contain a series of 593 different songs. There are known soil truth clusters on these datasets (we use as number of soil truth clusters); k = 6 for MUSIC and SCENE = 14 for YEAST. The aim of this comparison is to show that PALM and ADMM performance is equivalent to the ALM method while we run much faster. We will also compare against iterative NEOK algorithms as references."}, {"heading": "8 Discussion", "text": "Overall, the result from the previous section shows that both the PALM and ADMM methods are faster than the ALM methods with essentially unchanged quality, so we can easily recommend them instead of ALM for the optimization of these low-level objectives. However, there is still a significant gap between the performance of the simple iterative algorithm and the optimization methods we propose here. However, the optimization methods avoid the worst-case behavior of the iterative method and lead to more robust and reliable results, as illustrated in the YEAST dataset and in other experiments from [11]. With respect to the cluster problem, we are trying to identify a convergence guarantee for the ADMM method in this non-convex case, which would put the fastest method we have for optimization on solid theoretical ground. In view of the cluster problem, we are investigating the integrity properties of the SDP relaxation itself, [our goal here is to show the result of relaxation itself]."}], "references": [{"title": "Relax", "author": ["P. Awasthi", "A.S. Bandeira", "M. Charikar", "R. Krishnaswamy", "S. Villar", "R. Ward"], "venue": "no need to round: integrality of clustering formulations. In ITCS, pages 191\u2013200", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Model-based overlapping clustering", "author": ["A. Banerjee", "C. Krumpelman", "J. Ghosh", "S. Basu", "R.J. Mooney"], "venue": "KDD, pages 532\u2013537", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning multi-label scene classification", "author": ["M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown"], "venue": "Pattern Recognition, 37(9):1757 \u2013 1771", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A limited memory algorithm for bound constrained optimization", "author": ["R.H. Byrd", "P. Lu", "J. Nocedal", "C. Zhu"], "venue": "SIAM J. Sci. Comput., 16(5):1190\u20131208", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "The direct extension of admm for multi-block convex minimization problems is not necessarily convergent", "author": ["C. Chen", "B. He", "Y. Ye", "X. Yuan"], "venue": "Math. Prog., pages 1\u201323", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "An extended version of the k-means method for overlapping clustering", "author": ["G. Cleuziou"], "venue": "ICPR, pages 1\u20134", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "A kernel method for multilabelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "NIPS, pages 681\u2013687", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "A primal-dual regularized interior-point method for convex quadratic programs", "author": ["M. Friedlander", "D. Orban"], "venue": "Math. Prog. Comput., 4(1):71\u2013107", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonexhaustive", "author": ["Y. Hou", "J.J. Whang", "D.F. Gleich", "I.S. Dhillon"], "venue": "overlapping clustering via low-rank semidefinite programming. In KDD, pages 427\u2013436", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Carlos", "author": ["J. Humes"], "venue": "P. Silva, and B. Svaiter. Some inexact hybrid proximal augmented Lagrangian algorithms. Numerical Algorithms, 35(2-4):175\u2013184", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Inexact variants of the proximal point algorithm without monotonicity", "author": ["A.N. Iusem", "T. Pennanen", "B.F. Svaiter"], "venue": "SIAM J. Optimiz., 13(4):1080\u20131097", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Trans. Inf. Theory, 28(2):129\u2013137", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1982}, {"title": "Icdm workshops", "author": ["H. Lu", "Y. Hong", "W.N. Street", "F. Wang", "H. Tong"], "venue": "Overlapping clustering with sparseness constraints, pages 486\u2013494", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "On the convergence of alternating direction Lagrangian methods for nonconvex structured optimization problems", "author": ["S. Magn\u00fasson", "P.C. Weeraddana", "M.G. Rabbat", "C. Fischione"], "venue": "IEEE Trans. Control Netw. Syst.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Numerical Optimization", "author": ["J. Nocedal", "S.J. Wright"], "venue": "Springer, 2nd edition", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Proximal algorithms", "author": ["N. Parikh", "S. Boyd"], "venue": "Found. Trends Opt., 1(3):127\u2013239", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Local convergence of the proximal point algorithm and multiplier methods without monotonicity", "author": ["T. Pennanen"], "venue": "Math. Oper. Res., 27(1):170\u2013191", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Strongly regular generalized equations", "author": ["S.M. Robinson"], "venue": "Math. Oper. Res., 5(1):43\u201362", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1980}, {"title": "Augmented Lagrangians and applications of the proximal point algorithm in convex programming", "author": ["R.T. Rockafellar"], "venue": "Math. Oper. Res., 1(2):97\u2013116", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1976}, {"title": "Variational analysis", "author": ["R.T. Rockafellar", "R.J.-B. Wets"], "venue": "Springer", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-label classification of music into emotions", "author": ["K. Trohidis", "G. Tsoumakas", "G. Kalliris", "I.P. Vlahavas"], "venue": "ISMIR, pages 325\u2013330", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Nonexhaustive", "author": ["J.J. Whang", "I.S. Dhillon", "D.F. Gleich"], "venue": "overlapping k-means. In SDM, pages 936\u2013944", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "An information flow model for conflict and fission in small groups", "author": ["W.W. Zachary"], "venue": "J. Anthropol. Res., 33(4):452\u2013473", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1977}], "referenceMentions": [{"referenceID": 22, "context": "We recently proposed the NEO-K-Means (Non-Exhaustive, Overlapping K-Means) objective as a generalization of the k-means clustering objective that allows us to simultaneously identify overlapping clusters as well as outliers [24].", "startOffset": 224, "endOffset": 228}, {"referenceID": 22, "context": "There are currently two practical methods to optimize the non-convex NEO-K-Means objective for large problems: the iterative NEO-K-Means algorithm [24] that generalizes Lloyd\u2019s algorithm [14] and an augmented Lagrangian algorithm to optimize a non-convex, low-rank semidefinite programming (SDP) relaxation of the NEO-K-Means objective [11].", "startOffset": 147, "endOffset": 151}, {"referenceID": 12, "context": "There are currently two practical methods to optimize the non-convex NEO-K-Means objective for large problems: the iterative NEO-K-Means algorithm [24] that generalizes Lloyd\u2019s algorithm [14] and an augmented Lagrangian algorithm to optimize a non-convex, low-rank semidefinite programming (SDP) relaxation of the NEO-K-Means objective [11].", "startOffset": 187, "endOffset": 191}, {"referenceID": 9, "context": "There are currently two practical methods to optimize the non-convex NEO-K-Means objective for large problems: the iterative NEO-K-Means algorithm [24] that generalizes Lloyd\u2019s algorithm [14] and an augmented Lagrangian algorithm to optimize a non-convex, low-rank semidefinite programming (SDP) relaxation of the NEO-K-Means objective [11].", "startOffset": 336, "endOffset": 340}, {"referenceID": 9, "context": "In addition, the augmented Lagrangian method tends to achieve better F1 performance on identifying groundtruth clusters and produce better overlapping communities in real-world networks than the simple iterative algorithm [11].", "startOffset": 222, "endOffset": 226}, {"referenceID": 10, "context": ", [12]) or the proximal method of multipliers [21].", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": ", [12]) or the proximal method of multipliers [21].", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "There is an extensive literature on both strategies for convex optimization [5, 10, 21] and there are a variety of convergence theories in the non-convex case [16, 19, 13].", "startOffset": 76, "endOffset": 87}, {"referenceID": 8, "context": "There is an extensive literature on both strategies for convex optimization [5, 10, 21] and there are a variety of convergence theories in the non-convex case [16, 19, 13].", "startOffset": 76, "endOffset": 87}, {"referenceID": 19, "context": "There is an extensive literature on both strategies for convex optimization [5, 10, 21] and there are a variety of convergence theories in the non-convex case [16, 19, 13].", "startOffset": 76, "endOffset": 87}, {"referenceID": 14, "context": "There is an extensive literature on both strategies for convex optimization [5, 10, 21] and there are a variety of convergence theories in the non-convex case [16, 19, 13].", "startOffset": 159, "endOffset": 171}, {"referenceID": 17, "context": "There is an extensive literature on both strategies for convex optimization [5, 10, 21] and there are a variety of convergence theories in the non-convex case [16, 19, 13].", "startOffset": 159, "endOffset": 171}, {"referenceID": 11, "context": "There is an extensive literature on both strategies for convex optimization [5, 10, 21] and there are a variety of convergence theories in the non-convex case [16, 19, 13].", "startOffset": 159, "endOffset": 171}, {"referenceID": 17, "context": "Towards that end, we specialize a general convergence result about the proximal augmented Lagrangian or proximal method of multipliers due to Pennanen [19] to our algorithm.", "startOffset": 151, "endOffset": 155}, {"referenceID": 22, "context": "To find such clusters, we proposed the NEO-KMeans objective function in [24].", "startOffset": 72, "endOffset": 76}, {"referenceID": 22, "context": "We also found that optimizing a weighted and kernelized NEO-K-Means objective is equivalent to optimizing normalized cuts for overlapping community detection [24].", "startOffset": 158, "endOffset": 162}, {"referenceID": 22, "context": "Some guidelines about how to select \u03b1 and \u03b2 have been described in [24].", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "1), a simple iterative algorithm has also been proposed in [24].", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "However, the simple iterative algorithm tends to get stuck at a local optimum that can be far away from the global optimum, like the standard k-means algorithm [14].", "startOffset": 160, "endOffset": 164}, {"referenceID": 9, "context": "The following optimization problem is a non-convex relaxation of the NEO-K-Means problem that was developed in our previous work [11].", "startOffset": 129, "endOffset": 133}, {"referenceID": 9, "context": "2), the classical augmented Lagrangian method (ALM) has been used in [11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 4, "context": "To minimize the subproblem with respect to the variables Y , f , g, s, and r, we can use a limited-memory BFGS with bound constraints algorithm [6].", "startOffset": 144, "endOffset": 147}, {"referenceID": 9, "context": "In [11], it has been shown that this technique produces reasonable solutions for the NEO-K-Means objective.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "If we let \u03c4 = \u03c3, this special case is called proximal method of multipliers, first introduced in [21].", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "The proximal method of multipliers has better theoretical convergence guarantees for convex optimization problems (compared with the augmented Lagrangian) [21].", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "51 of [17]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "From Pennanen [19], we know that the proximal method of multipliers is locally convergent for a general class of problems with sufficient assumptions.", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "Thus, there is a high degree of textual overlap between the following results and [19].", "startOffset": 82, "endOffset": 86}, {"referenceID": 17, "context": "We choose adding the multipliers here in order to be consistent with the analysis in [19].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "A point (x\u0304, \u03bb\u0304) is said to satisfy the strong secondorder sufficient condition [20] for problem (5.", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "(Note that the theorem and proof are revisions and specializations of Theorem 19 from [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 18, "context": "1) [20], the strongly secondorder sufficient condition and the linear independence condition imply that the KKT system for (5.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Thus, Algorithm 1 is equivalent to Algorithm 3 in [19] (their general algorithm), and by Theorem 17 of [19], we have the local convergence result stated in Theorem 5.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "Thus, Algorithm 1 is equivalent to Algorithm 3 in [19] (their general algorithm), and by Theorem 17 of [19], we have the local convergence result stated in Theorem 5.", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "26 from [22] and the analogous derivation in the proof of Theorem 19 of [19].", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "26 from [22] and the analogous derivation in the proof of Theorem 19 of [19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "This was adjusted in [13], which showed local convergence for approximate solutions of (P ).", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "For instance, in [7], it has been shown that an ADMM method does not converge for a multi-block case even for a convex problem.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "All these three algorithms are implemented in MATLAB and use the L-BFGS-B routine [6] written in Fortran to solve the bound-constrained nonlinear subproblems.", "startOffset": 82, "endOffset": 85}, {"referenceID": 23, "context": "We use the Zachary\u2019s karate club network [25] which is a small social network among 34 members of a karate club.", "startOffset": 41, "endOffset": 45}, {"referenceID": 2, "context": "The SCENE dataset [4] contains 2,407 scenes represented as feature vectors; the YEAST dataset [9] consists of 2,417 genes where the features are based on micro-array expression data and phylogenetic profiles; the MUSIC dataset [23] contains a set of 593 different songs.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "The SCENE dataset [4] contains 2,407 scenes represented as feature vectors; the YEAST dataset [9] consists of 2,417 genes where the features are based on micro-array expression data and phylogenetic profiles; the MUSIC dataset [23] contains a set of 593 different songs.", "startOffset": 94, "endOffset": 97}, {"referenceID": 21, "context": "The SCENE dataset [4] contains 2,407 scenes represented as feature vectors; the YEAST dataset [9] consists of 2,417 genes where the features are based on micro-array expression data and phylogenetic profiles; the MUSIC dataset [23] contains a set of 593 different songs.", "startOffset": 227, "endOffset": 231}, {"referenceID": 9, "context": "We initialize ALM, PALM, and ADMM using the iterative NEO-K-Means algorithm as also used in [11].", "startOffset": 92, "endOffset": 96}, {"referenceID": 22, "context": "The parameters \u03b1 and \u03b2 in the NEO-K-Means are automatically estimated by the strategies proposed in [24].", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "We use the procedure from [11] to round the real-valued solutions to discrete assignments.", "startOffset": 26, "endOffset": 30}, {"referenceID": 1, "context": "We also compare the results with other stateof-the-art overlapping clustering methods, MOC [3], ESA [15], and OKM [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 13, "context": "We also compare the results with other stateof-the-art overlapping clustering methods, MOC [3], ESA [15], and OKM [8].", "startOffset": 100, "endOffset": 104}, {"referenceID": 6, "context": "We also compare the results with other stateof-the-art overlapping clustering methods, MOC [3], ESA [15], and OKM [8].", "startOffset": 114, "endOffset": 117}, {"referenceID": 9, "context": "However, the optimization procedures avoid the worst-case behavior of the iterative method and result in more robust and reliable results as illustrated on the YEAST dataset and in other experiments from [11].", "startOffset": 204, "endOffset": 208}, {"referenceID": 9, "context": "In terms of the clustering problem, we are exploring the integrality properties of the SDP relaxation itself [11].", "startOffset": 109, "endOffset": 113}, {"referenceID": 0, "context": "Our goal here is to show a result akin to that proved in [2] about integrality in relaxations of the k-means objective.", "startOffset": 57, "endOffset": 60}], "year": 2016, "abstractText": "Clustering is one of the most fundamental and important tasks in data mining. Traditional clustering algorithms, such as K-means, assign every data point to exactly one cluster. However, in real-world datasets, the clusters may overlap with each other. Furthermore, often, there are outliers that should not belong to any cluster. We recently proposed the NEO-K-Means (Non-Exhaustive, Overlapping K-Means) objective as a way to address both issues in an integrated fashion. Optimizing this discrete objective is NPhard, and even though there is a convex relaxation of the objective, straightforward convex optimization approaches are too expensive for large datasets. A practical alternative is to use a low-rank factorization of the solution matrix in the convex formulation. The resulting optimization problem is non-convex, and we can locally optimize the objective function using an augmented Lagrangian method. In this paper, we consider two fast multiplier methods to accelerate the convergence of an augmented Lagrangian scheme: a proximal method of multipliers and an alternating direction method of multipliers (ADMM). For the proximal augmented Lagrangian or proximal method of multipliers, we show a convergence result for the non-convex case with bound-constrained subproblems. These methods are up to 13 times faster\u2014with no change in quality\u2014compared with a standard augmented Lagrangian method on problems with over 10,000 variables and bring runtimes down from over an hour to around 5 minutes.", "creator": "LaTeX with hyperref package"}}}