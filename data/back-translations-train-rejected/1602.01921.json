{"id": "1602.01921", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Recognition of Visually Perceived Compositional Human Actions by Multiple Spatio-Temporal Scales Recurrent Neural Networks", "abstract": "The current paper proposes a novel dynamic neural network model, multiple spatio-temporal scales recurrent neural network (MSTRNN) used for categorization of complex human action pattern in video image. The MSTRNN has been developed by newly introducing recurrent connectivity to a prior-proposed model, multiple spatio-temporal scales neural network (MSTNN) [1] such that the model can learn to extract latent spatio-temporal structures more effectively by developing adequate recurrent contextual dynamics. The MSTRNN was evaluated by conducting a set of simulation experiments on learning to categorize human action visual patterns. The first experiment on categorizing a set of long-concatenated human movement patterns showed that MSTRNN outperforms MSTNN in the capability of learning to extract long-ranged correlation in video image. The second experiment on categorizing a set of object-directed actions showed that the MSTRNN can learn to extract structural relationship between actions and directed-objects. Our analysis on the characteristics of miscategorization in both cases of object-directed action and pantomime actions indicated that the model network developed the categorical memories by organizing relational structure among them. Development of such relational structure is considered to be beneficial for gaining generalization in categorization.", "histories": [["v1", "Fri, 5 Feb 2016 04:00:16 GMT  (627kb)", "http://arxiv.org/abs/1602.01921v1", "20 pages, 7 figures, 7 tables"], ["v2", "Wed, 5 Oct 2016 07:59:03 GMT  (3953kb,D)", "http://arxiv.org/abs/1602.01921v2", "18 pages, 7 figures, 6 tables"], ["v3", "Wed, 22 Feb 2017 16:33:49 GMT  (1882kb,D)", "http://arxiv.org/abs/1602.01921v3", "10 pages, 9 figures, 5 tables"]], "COMMENTS": "20 pages, 7 figures, 7 tables", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["haanvid lee", "minju jung", "jun tani"], "accepted": false, "id": "1602.01921"}, "pdf": {"name": "1602.01921.pdf", "metadata": {"source": "CRF", "title": "Characteristics of Visual Categorization of Long-Concatenated and Object-Directed Human Actions by a Multiple Spatio-Temporal Scales Recurrent Neural Network Model", "authors": ["Haanvid Lee", "Minju Jung", "Jun Tani"], "emails": ["tani1216jp@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is not that it is a purely visual matter, as it is in most cases. It is rather that it is a purely visual matter. It is not that it is a purely visual matter. Rather, it is that it is a purely visual matter. It is that it is a purely visual matter. It is that it is a purely visual matter. It is not that it is a purely visual matter. It is rather that it is a purely visual matter. It is not that it is a purely visual matter."}, {"heading": "2. Model", "text": "However, the Multiple Spatio-Temporal Recurrent Neural Network (MSTRNN) is a hierarchical neural network model that has the ability to extract spatio-temporal characteristics in the dynamic images. In the previous study, a dynamic neural network model was developed that refers to Multiple SpatioTemporal Neural Network (MSTNN) [1] and was developed for the purpose of automatically categorizing video image patterns based on learning. MSTNN was developed by combining two previous models of Convolutional Neural Network (CNN) [2] and Multiple Timescales Recurrent Neural Network (MTRNN) [14]. The basic idea of the MSTNN is that both spatial and temporal constraints imposed on neural activity vary according to layer. The model used characterizes maps of leaky integrorneural neurons with constants of time that exhibit the same Natio-constant values in the same layers."}, {"heading": "2.1 Model Architecture", "text": "In fact, it is the case that you will be able to go to a place where you can go to a place where you are able to move."}, {"heading": "2.2 Forward Dynamics", "text": "Not so long ago, it was only a matter of time before it would happen."}, {"heading": "2.3 Training", "text": "The comparison was made using the Kullback-Leibler divergence. The cost function for the formation of the MSTRNN model is shown in Equation 6. The error calculated for an input video is called E.E = \""}, {"heading": "3. Experiments", "text": "The current study examined the characteristics of the MSTRNN model by performing two types of experimental tasks to categorize human actions: The first experimental task examined the ability of MSTRNN to learn longer sequences of human motion patterns compared to the ability of MSTNN [1] in the same task; the second experimental task examined how the MSTRNN can learn to categorize a set of object-driven actions by generalization by performing various cognitive analyses."}, {"heading": "3.1 Learning to categorize longer sequences of movement patterns", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Task description", "text": "The experiment was conducted to compare the ability of learning to categorize a series of compositional long visual sequences between MSTRNN and MSTNN. To this end, a set of specimen video data was prepared by concatenating 3 different human action videos from the Weizmann dataset. These 3 human actions from the Weizmann dataset are, jump-in-place (JP), one-hand-wave (OH), two-hand-wave (TH) as shown in Figure 2. The dataset consists of 9 subjects, 27 categories, and a study of the action sequence for each category. And the superficial silhouettes of the Weizmann dataset were used by background subtraction. Silhouette frames were reduced to 48x54. For the structure of the MSTRNN and MSTNN models, those were used in the experiment."}, {"heading": "B. Experiment results", "text": "The simulation showed that the categorization accuracy of the MSTNN was almost twice as high as that of the MSTNN, as shown in Figure 3 (the results video is available at https: / / sites.google.com / site / haanvidlee / mstrnn-experiment-1-demo-video).The categorization accuracy of the MSTRNN was 83.5%, while that of MSTNN was only 43.2%.This result implies that the context units with recurring weights contributed to better categorization for far-reaching video images of compositional human action sequences."}, {"heading": "3.2 Learning to categorize object-directed actions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Description of the task", "text": "In the current task, the video dataset was prepared using 9 actions performed with 10 people. In the dataset, 15 object-oriented action categories were prepared as a whole, as shown in Figure 4 and Table 3. The video dataset was designed so that it can be preserved in some areas. We do not have the slightest connection between the dynamic sights and the directed objects. Videos were recorded for each subject with the appearance of three different objects. This setting was introduced for the purpose of later analysis."}, {"heading": "B. Experimental results (1) Leave-one-subject-out classification results of all subjects.", "text": "In fact, it is that one will be able to move to another world, in which one must move to another world, in which one is able to reorient oneself, in which one must move to another world, in which one must move to another world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world."}, {"heading": "4. Discussion and conclusion", "text": "In fact, we are in a position to go in search of a solution that enables us, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in the position we are in."}, {"heading": "Acknowledgement", "text": "This work was funded by the National Research Foundation of Korea (NRF), which is funded by the Korean Government (MSIP) (No 2014R1A2A2A2A01005491)."}], "references": [{"title": "Self-Organization of Spatio-Temporal Hierarchy via Learning of Dynamic Visual Image Patterns on Action Sequences", "author": ["M Jung", "J Hwang", "J. Tani"], "venue": "PloS one. 2015 Jul 6;10(7):e0131214", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J Deng", "W Dong", "R Socher", "LJ Li", "K Li", "L. Fei-Fei"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition;", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Going deeper with convolutions", "author": ["C Szegedy", "W Liu", "Y Jia", "P Sermanet", "S Reed", "D Anguelov", "D Erhan", "V Vanhoucke", "A. Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O Russakovsky", "J Deng", "H Su", "J Krause", "S Satheesh", "S Ma", "Z Huang", "A Karpathy", "A Khosla", "M Bernstein", "AC Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "3D convolutional neural networks for human action recognition", "author": ["S Ji", "W Xu", "M Yang", "K. Yu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild", "author": ["K Soomro", "AR Zamir", "M. Shah"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "HMDB: a large video database for human motion recognition", "author": ["Kuehne H", "H. Jhuang", "E. Garrote", "T. Poggio", "T. Serre"], "venue": "In Proceedings of the International Conference on Computer Vision (ICCV)", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Sequential Deep Learning for Human Action Recognition", "author": ["M Baccouche", "F Mamalet", "C Wolf", "C Garcia", "A. Baskurt"], "venue": "Human Behavior Understanding", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["FA Gers", "NN Schraudolph", "J. Schmidhuber"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Sequence to Sequence - Video to Text", "author": ["S Venugopalan", "M Rohrbach", "J Donahue", "R Mooney", "T Darrell", "K. Saenko"], "venue": "CoRR. 2015;abs/1505.00487", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Perceptual structures and distributed motor control: Bethesda, MD: American Physiological Society.", "author": ["Arbib MA"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1981}, {"title": "Self-Organization and Compositionality in Cognitive Brains: A Neurorobotics Study", "author": ["J. Tani"], "venue": "Proceedings of the IEEE", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model: A Humanoid Robot Experiment", "author": ["Y Yamashita", "J. Tani"], "venue": "PLoS Comput Biol", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Finding structure in time", "author": ["Elman JL"], "venue": "Cognitive Science", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "Attractor dynamics and parallelism in a connectionist sequential machine", "author": ["Jordan MI"], "venue": "Artificial neural networks: IEEE Press;", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["DC Ciresan", "U Meier", "J Masci", "LM Gambardella", "J. Schmidhuber"], "venue": "Proceedings of the Twenty-Second international joint conference on Artificial Intelligence - Volume Volume Two;", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Parallel distributed processing: MIT", "author": ["DE Rumelhart", "JL McClelland", "PR. Group"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1986}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N Srivastava", "G Hinton", "A Krizhevsky", "I Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1929}, {"title": "Error detecting and error correcting codes", "author": ["Hamming RW"], "venue": "Bell System Tech J. 1950;29:147--60", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1950}, {"title": "On a routing problem", "author": ["R. Bellman"], "venue": "Quarterly of Applied Mathematics", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1958}, {"title": "Analysis of a Complex of Statistical Variables into Principal Components", "author": ["H. Hotelling"], "venue": "Journal of Educational Psychology", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1933}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton GE", "Salakhutdinov RR"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A Karpathy", "G Toderici", "S Shetty", "T Leung", "R Sukthankar", "L. Fei-Fei"], "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The MSTRNN has been developed by newly introducing recurrent connectivity to a priorproposed model, multiple spatio-temporal scales neural network (MSTNN) [1] such that the model can learn to extract latent spatio-temporal structures more effectively by developing adequate recurrent contextual dynamics.", "startOffset": 155, "endOffset": 158}, {"referenceID": 1, "context": "Especially, a convolutional neural network (CNN) [2], which has been developed as inspired by the mammalian visual cortex for its spatial hierarchical processing of visual features, has shown remarkably better recognition performance for static natural visual images compared to conventional vision recognition schemes which used elaborately hand-coded visual features.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "Actually, recent CNN trained with 1 million of visual image in ImageNet [3] can classify hundreds of object image with error rate of 0.", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "0665 [4].", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "It is considered that the CNN\u2019s performance in this task is close to that of human [5].", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "A typical approach to the problem has been to use a 3D convolutional neural network (3D CNN) [6] in which dynamic visual image patterns can be recognized by simply transforming a sequence of 2-dimensional visual spatial patterns within a fixed temporal window into a large 3-dimensional pattern.", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "Actually, 3D CNN showed good performances on many challenging video recognition public datasets related to human action categories such as UCF-101 [7] and HMDB-51 [8] by extracting short-range temporal correlations in the temporal window.", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "Actually, 3D CNN showed good performances on many challenging video recognition public datasets related to human action categories such as UCF-101 [7] and HMDB-51 [8] by extracting short-range temporal correlations in the temporal window.", "startOffset": 163, "endOffset": 166}, {"referenceID": 8, "context": "[9] has proposed a two-stage model to maintain temporal information in the entire sequence by adding a long short-term memory (LSTM) network [10] as a second stage of the 3D CNN.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[9] has proposed a two-stage model to maintain temporal information in the entire sequence by adding a long short-term memory (LSTM) network [10] as a second stage of the 3D CNN.", "startOffset": 141, "endOffset": 145}, {"referenceID": 10, "context": "Similarly, Venugopalan and colleagues [11] proposed an architecture composed of a CNN for video processing and an LSTM concatenated downstream of the CNN for the generation of corresponding word sequences that was trained by using nearly a hundred thousand video clips with annotated descriptive sentences.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "Such spatio-temporal structures should be involved with compositionality in human action generation [12, 13].", "startOffset": 100, "endOffset": 108}, {"referenceID": 12, "context": "Such spatio-temporal structures should be involved with compositionality in human action generation [12, 13].", "startOffset": 100, "endOffset": 108}, {"referenceID": 11, "context": "The temporal compositionality can be accounted by the fact that most of goal-directed human actions are composed by sequential combinations of commonly used behavior primitives [12].", "startOffset": 177, "endOffset": 181}, {"referenceID": 0, "context": "In the prior study, a dynamic neural network model, referred to Multiple SpatioTemporal Neural Network (MSTNN) [1] was developed for the purpose of automatic categorization of video image patterns based on learning.", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "MSTNN was developed by combining two prior existing models of Convolutional Neural Network (CNN) [2] and Multiple Timescales Recurrent Neural Network (MTRNN) [14].", "startOffset": 97, "endOffset": 100}, {"referenceID": 13, "context": "MSTNN was developed by combining two prior existing models of Convolutional Neural Network (CNN) [2] and Multiple Timescales Recurrent Neural Network (MTRNN) [14].", "startOffset": 158, "endOffset": 162}, {"referenceID": 14, "context": "It has been shown that context units with recurrent connectivity play important roles in extracting latent temporal structures from exemplar temporal sequences [15, 16].", "startOffset": 160, "endOffset": 168}, {"referenceID": 15, "context": "It has been shown that context units with recurrent connectivity play important roles in extracting latent temporal structures from exemplar temporal sequences [15, 16].", "startOffset": 160, "endOffset": 168}, {"referenceID": 16, "context": "A context layer consists of feature units used in the MSTNN model, and our newly designed context units, and pooling units [17] if the layer does the pooling operation (see Figure 1 (b), (c)).", "startOffset": 123, "endOffset": 127}, {"referenceID": 16, "context": "This is because the functionality of the max-pooling units lies not in capturing temporal structures, but in capturing the translation invariant features of the images and reducing the feature size [17].", "startOffset": 198, "endOffset": 202}, {"referenceID": 0, "context": "Categorization in the model was performed in the delay response manner [1].", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "3 Training The training was conducted by a supervised manner using the delay response scheme [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 17, "context": "The error obtained by the Equation 6 for an input video was used for optimizing a set of learnable parameters by using back propagated through time (BPTT) [18].", "startOffset": 155, "endOffset": 159}, {"referenceID": 18, "context": "0005 and fullyconnected layer was trained with 50% dropout rate [19] to prevent the overfitting.", "startOffset": 64, "endOffset": 68}, {"referenceID": 0, "context": "The first experiment task examined the capability of MSTRNN in learning longer sequences of human movement patterns as compared to the capability of MSTNN [1] in the same task.", "startOffset": 155, "endOffset": 158}, {"referenceID": 19, "context": "First, distance between two joint categories is calculated by simply taking 2-dimensional bit hamming distance [20] for action category and object category.", "startOffset": 111, "endOffset": 115}, {"referenceID": 20, "context": "In this situation, we may consider Bellman-Ford [21, 22] algorithm that defines distance between nodes in graph structure in terms of the minimum path length between them.", "startOffset": 48, "endOffset": 56}, {"referenceID": 0, "context": "Discussion and conclusion A newly proposed dynamic neural network model, MSTRNN has been built by modifying the MSTNN [1] model to include leaky integrator units that have recurrent connections in each level.", "startOffset": 118, "endOffset": 121}, {"referenceID": 21, "context": "Although our preliminary study examined various trials using analytical schemes such as principle component analysis (PCA) [23] or auto-encoder [24] scheme for this purpose, any reportable results have not been obtained yet.", "startOffset": 123, "endOffset": 127}, {"referenceID": 22, "context": "Although our preliminary study examined various trials using analytical schemes such as principle component analysis (PCA) [23] or auto-encoder [24] scheme for this purpose, any reportable results have not been obtained yet.", "startOffset": 144, "endOffset": 148}, {"referenceID": 23, "context": "Also, method of flipping the images of the dataset horizontally by 50% could be used to increase the size of the dataset [25].", "startOffset": 121, "endOffset": 125}], "year": 2016, "abstractText": "The current paper proposes a novel dynamic neural network model, multiple spatio-temporal scales recurrent neural network (MSTRNN) used for categorization of complex human action pattern in video image. The MSTRNN has been developed by newly introducing recurrent connectivity to a priorproposed model, multiple spatio-temporal scales neural network (MSTNN) [1] such that the model can learn to extract latent spatio-temporal structures more effectively by developing adequate recurrent contextual dynamics. The MSTRNN was evaluated by conducting a set of simulation experiments on learning to categorize human action visual patterns. The first experiment on categorizing a set of long-concatenated human movement patterns showed that MSTRNN outperforms MSTNN in the capability of learning to extract long-ranged correlation in video image. The second experiment on categorizing a set of objectdirected actions showed that the MSTRNN can learn to extract structural relationship between actions and directed-objects. Our analysis on the characteristics of miscategorization in both cases of object-directed action and pantomime actions indicated that the model network developed the categorical memories by organizing relational structure among them. Development of such relational structure is considered to be beneficial for gaining generalization in categorization.", "creator": "Acrobat PDFMaker 11 for Word"}}}