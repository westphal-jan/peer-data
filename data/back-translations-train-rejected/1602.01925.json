{"id": "1602.01925", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Massively Multilingual Word Embeddings", "abstract": "We introduce new methods for estimating and evaluating embeddings of words from dozens of languages in a single shared embedding space. Our estimation methods, multiCluster and multiCCA, use dictionaries and monolingual data; they do not require parallel data. Our new evaluation method, multiQVEC+, is shown to correlate better than previous ones with two downstream tasks (text categorization and parsing). On this evaluation and others, our estimation methods outperform existing ones. We also describe a web portal for evaluation that will facilitate further research in this area, along with open-source releases of all our methods.", "histories": [["v1", "Fri, 5 Feb 2016 04:26:38 GMT  (25kb)", "http://arxiv.org/abs/1602.01925v1", null], ["v2", "Sat, 21 May 2016 08:08:21 GMT  (32kb)", "http://arxiv.org/abs/1602.01925v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["waleed ammar", "george mulcaire", "yulia tsvetkov", "guillaume lample", "chris dyer", "noah a smith"], "accepted": false, "id": "1602.01925"}, "pdf": {"name": "1602.01925.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Waleed Ammar", "George Mulcaire", "Yulia Tsvetkov", "Guillaume Lample Chris Dyer", "Noah A. Smith"], "emails": ["wammar@cs.cmu.edu,", "gmulc@uw.edu,", "ytsvetko@cs.cmu.edu", "glample@cs.cmu.edu,", "cdyer@cs.cmu.edu,", "nasmith@cs.washington.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.01 925v 1 [cs.C L] 5F eb"}, {"heading": "1 Introduction", "text": "Vector-space representations of words are widely used in statistical models of natural language. In addition to the improvements to the standard monolingual NLP tasks (Collobert and Weston, 2008), the common representation of words in different languages offers fascinating possibilities (Klementiev et al., 2012). A second chance arises from the translation of a word that can never be used in other languages. While the previous work has used craft characteristics that serve as the basis for the transfer of monolingual data (Zeman and Resnik, 2008), there is an exchange of models that can be used in one language."}, {"heading": "2 Estimating Multilingual Embeddings", "text": "Let L be a set of languages, and let Vm be the set of surface shapes (pairs of words) in m-L. Let V = m-L-Vm. Our goal is to estimate a partial embedding function. (i) Semantically similar words in the same language are nearby, (ii) the domain of the function covers as many words in V as possible. We use distribution similarity in a monolingual corpus Mm to model semantic similarity between words in the same language. (iii) The domain of the function covers as many words in V as possible. We use distribution similarity in a monolingual corpus Mm to model semantic similarity between words in the same language."}, {"heading": "2.1 Multilingual cluster (multiCluster) embeddings", "text": "In this approach, we divide the problem into two simpler sub-problems: E = Eembed \u043c Ecluster, where Ecluster: L \u00b7 V 7 \u2192 C deterministically maps words to multilingual clusters C, and Eembed: C \u2192 Rd assigns a vector to each cluster. To find clusters with translation-like words, we use a bilingual dictionary and then we create anchor points in the vector space to bridge languages in monolingual corpora from all languages in L to estimate an embedding for each cluster. By forcing words from different languages in a cluster to share the same embedding, we create anchor points in the vector space to bridge languages. Specifically, we define the clusters as the interconnected components in a graph in which nodes (language, surface shape) pairs and edges with translation entries in Dm, n. We assign the cluster arbitrary IDs and then replace each word in a monolingual corresponding model with an ID, n."}, {"heading": "2.2 Multilingual CCA (multiCCA) embeddings", "text": "\"We have chosen a bilingual embedding method based on a canonical correlation analysis (CCA) and have shown that the resulting embedding for English words exceeds the monolingual embedding in each language separately.\" Then they use monolingual embedding techniques to make monolingual embedding for each language independent (Em and En) to enter semantic embedding in each language separately. \"The bilingual embedding Dm, n, they use monolingual embedding for the monolingual embedding Em and En, which capture bilingual embedding in each language separately.\" The linear projections are of Tm \u2192 m and Tn \u2192 m, n and Tn \u2192 m, which we understand as monolingual embedding in the monolingual embedding Em and En."}, {"heading": "2.3 MultiSkip embeddings", "text": "Luong et al. (2015b) proposed a method for estimating a bilingual embedding using only parallel data; it extends the Skipgram model of Mikolov et al. (2013a). The Skipgram model defines a distribution over words u, which in a context window (size K) of a word v: p (u | v) = expEskipgram (m, v) Econtext (m, u) Vm expEskipgram (m, v) Econtext (m, u \u2032) In practice, this distribution can be estimated using an opposite estimation approximation (Gutmann and Hyva \ufffd rinen, 2012), while maximizing the probability of log: \"i-pos\" (Mm), \"k-words\" (m), \"i-1,1,..., K} log p (ui + k | ui), where pos-bilpus-bilpus-forming words are."}, {"heading": "2.4 Translation-invariant matrix factorization", "text": "Gardner et al. (2015) suggested that multilingual embeddings should be invariant. Let's consider a matrix X-R-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-XA-X"}, {"heading": "3 Evaluating Multilingual Embeddings", "text": "One of our most important contributions is streamlining the evaluation of multilingual embeddings. In addition to evaluating the objectives (i-iii) according to \u00a7 2, a good evaluation value should also (iv) have a good correlation with performance in downstream applications and (v) be computationally efficient. It is easy to evaluate the coverage (iii) by counting the number of words covered by an embedding function in a closed vocabulary. Intrinsic evaluation metrics are generally designed to be computationally efficient (v) but cannot achieve the objectives (i, ii, iv). In design, standard (monolingual) word similarity tasks (i) fulfil best, while cross-border word similarity tasks and word translation tasks (ii) fulfil best (ii). We propose another evaluation method (multiQVEC +) designed to assess objectives (i, ii) metallic."}, {"heading": "3.1 Word similarity", "text": "Word similarity datasets such as WS-353-SIM (Agirre et al., 2009) and MEN (Bruni et al., 2014) provide human judgments on semantic similarity. By ranking words by cosinal similarity and empirical similarity judgments, a rank correlation can be calculated that assesses how well the estimated vectors capture human intuitions about semantic kinship. Some previous work on bilingual and multilingual embeddings focused on monolingual word similarity in order to evaluate embeddings (e.g. Faruqui and Dyer., 2014) This approach is limited because it cannot measure the degree to which embeddings from different languages are similar (ii). In this paper, we report results on an English word similarity task, the Stanford RW dataset (Luong et al., 2013), as well as a combination of multiple multilingual word similarities (Colacho-Camachal et, 2015)."}, {"heading": "3.2 Word translation", "text": "The score for a word pair (l1, w1), (l2, w2), both covered by an embedded E, is 1 if Cosine (E (l1, w1), E (l1, w1), E (l2, w \u2032 2) and W \u00b2 2, where Gl2 is the set of words of the language l2 in the evaluation record and Cosine is the cosmic similarity function. Otherwise, the score for this word pair is 0. The total value is the average for all word pairs covered by the embedding function. This is a variant of the method used by Mikolov et al. (2013b) for the evaluation of bilingual embeddings."}, {"heading": "3.3 Correlation-based evaluation", "text": "It is a question of whether it is really about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way and a way it is about which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way and a way it is about which it is about which it is about which it is about a way in which it is about a way in which it is about a way"}, {"heading": "3.4 Extrinsic tasks", "text": "To assess the usefulness of word embedding for a downstream task, we use the embedding vector as a dense feature representation of each word in the input, deliberately removing all other features available for that word (e.g. prefixes, suffixes, part-of-speech).For each task, we train a model based on the aggregated training data available for multiple languages, and evaluate the aggregated evaluation data in the same set of languages. We apply this to multilingual document classification and multilingual dependency sparsing.For document classification, we follow Klementiev et al. (2012) when using the RCV corpus of wireel text and train a classifier that distinguishes between four topics. While most previous work that used this data only in a bilingual setup, simultaneously train the classifier on documents in seven languages, 4 and evaluate the development / test section of these languages."}, {"heading": "4 Evaluation Portal", "text": "To facilitate future research on multilingual word embedding, we developed a web portal6 so that researchers who develop new estimation methods can evaluate these using a series of evaluation tasks. The portal serves the following purposes: \u2022 Download the monolingual and bilingual data that we have used to estimate multilingual embedding in this paper \u2022 Download standard development / test datasets for each of the evaluation metrics to help researchers working in this area, to help, confidently-4Danish, German, English, Spanish, French, Italian and Swedish. 5http: / / hdl.handle.net / 11234 / LRT-1478 6 http: / / 128.2.220.95 / multilingual L L L, L L, L L, L, L, L, L, L, L, de, en, it, fr, sv Dependence of bg, cs, da, de, de, el, +, 220.95 / multilingual, L, L, multi, L, L, L, L, L, L, L 639-1 Code document classification da, de, de, de, de, de, en, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, L, c, c, c, c, c, c, c, c, c, c, c, c, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L,"}, {"heading": "5 Experiments", "text": "Our experiments should show two primary results: (i) how well the proposed intrinsic evaluation metrics correlate with downstream tasks using multilingual word vectors (\u00a7 5.1) and (ii) which estimation methods work best (\u00a7 5.2)."}, {"heading": "5.1 Correlations between intrinsic vs. extrinsic evaluation metrics", "text": "In this experiment, we look at four intrinsic evaluation metrics (multilingual word similarity, word translation, multiQVEC and multiQVEC +) and two extrinsic evaluation metrics (multilingual document classification and multilingual parsing).Data: All evaluation data sets we use are available for download on the evaluation portal. All other data is available for download. MWS353 data sets (Leviant and Range, 2015) For the word translation task, we use a subset of 647 translation pairs from Wiktionary into English, Italian and Danish, which are limited by the Reuters license and cannot be published again. All other data is available for download. MWS353 data sets (Leviant and Range, 2015) we use a subset of 647 translation pairs from Wiktionary into English, Italian and Danish."}, {"heading": "5.2 Evaluating multilingual estimation methods", "text": "This year, it has come to the point that it will only be a matter of time before it happens, until it does."}, {"heading": "6 Previous Work", "text": "We focused on methods for creating multilingual embeddings for many languages, but there is a rich body of literature on bilingual embeddings, including work on machine translation (Zou et al., 2013; Hermann and Blunsom, 2014; Cho et al., 2014; Luong et al., 2015b; Luong et al., 2015a, among others), 12 lingual dependency parsing (Guo et al., 2015; Guo et al., 2016) and on lingual document classification (Klementiev et al., 2012; Gouws et al., 2014; Kocisky prior et al., 2014). Word clusters are a related form of distribution representation; clusters have also been proposed for the creation of lingual distribution representations (Ta \ufffd ckstro \ufffd m et al., 2012)."}, {"heading": "7 Conclusion", "text": "We introduced two estimation methods for multilingual word embeddings, multiCCA and multiCluster, which require only bilingual dictionaries and monolingual corpora, and used them to train embeddings for 59 languages.12Hermann and Blunsom (2014) showed that the bicvm method can be extended to more than two languages, but the published software library only supports bilingual embeddings. We tried to follow the recommendation of the first author on https: / / github.com / karlmoritz / bicvm / issues / 4, but we were unable to reproduce their outcomes. mated by using our dictionary-based methods to exceed those estimated using other methods for two downstream tasks: multilingual dependency sparing and multilingual document classification. We also developed a new method to facilitate embeddings using bilingual, multilingual pre-evaluation methods to assign the nine."}, {"heading": "Acknowledgments", "text": "Waleed Ammar is supported by the Google Fellowship in Natural Language Processing. Part of this material is based on work supported by a subcontract with Raytheon BBN Technologies Corp. under DARPA Prime contract number HR0011-15-C0013. This work was partially supported by the National Science Foundation through the award IIS1526745. We thank Manaal Faruqui, Wang Ling, Kazuya Kawakami, Matt Gardner and Benjamin Wilson for their helpful comments."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Agirre et al.2009] Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa"], "venue": "In Proceedings of Human Language Technologies:", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "In Proc. of JAIR", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "A framework for the construction of monolingual and cross-lingual word similarity datasets", "author": ["Mohammad Taher Pilehvar", "Roberto Navigli"], "venue": "In Proc. of ACL", "citeRegEx": "Camacho.Collados et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Camacho.Collados et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proc. of EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proc. of ICML", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "A simple, fast, and effective reparameterization of IBM Model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proc. of NAACL", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith"], "venue": "In Proc. of ACL", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Dyer2014] Manaal Faruqui", "Chris Dyer"], "venue": "Proc. of EACL. Association for Computational Linguistics", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Translation invariant word embeddings", "author": ["Gardner et al.2015] Matt Gardner", "Kejun Huang", "Evangelos Papalexakis", "Xiao Fu", "Partha Talukdar", "Christos Faloutsos", "Nicholas Sidiropoulos", "Tom Mitchell"], "venue": "In Proc. of EMNLP", "citeRegEx": "Gardner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2015}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments. arXiv preprint arXiv:1410.2455", "author": ["Gouws et al.2014] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": null, "citeRegEx": "Gouws et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2014}, {"title": "Crosslingual dependency parsing based on distributed representations", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of ACL", "citeRegEx": "Guo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "A representation learning framework for multi-source transfer parsing", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of AAAI", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann", "Hyv\u00e4rinen2012] Michael U Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "In Proc. of JMLR", "citeRegEx": "Gutmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2012}, {"title": "Multilingual Models for Compositional Distributional Semantics", "author": ["Hermann", "Blunsom2014] Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proc. of ACL", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Ivan Titov", "Binod Bhattarai"], "venue": "In Proc. of COLING", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Learning bilingual word representations by marginalizing alignments", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "In arXiv preprint arXiv:1405.0947", "citeRegEx": "Kocisk\u1ef3 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kocisk\u1ef3 et al\\.", "year": 2014}, {"title": "Judgment language matters: Towards judgment language informed vector space modeling", "author": ["Leviant", "Reichart2015] Ira Leviant", "Roi Reichart"], "venue": "In arXiv preprint arXiv:1508.00106", "citeRegEx": "Leviant et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Leviant et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proc. of CoNLL, Sofia, Bulgaria", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proc. of ACL", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bilingual word representations with monolingual quality in mind", "author": ["Luong et al.2015b] Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "In Proc. of the 1st Workshop on Vector Space Modeling for Natural Language Processing", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Supersense tagging for danish", "author": ["Anders Johannsen", "Sussi Olsen", "Sanni Nimb", "Nicolai Hartvig Srensen", "Anna Braasch", "Anders Sgaard", "Bolette Sandford Pedersen"], "venue": "In Proc. of NODALIDA,", "citeRegEx": "Alonso et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alonso et al\\.", "year": 2015}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Slav Petrov", "Keith Hall"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "McDonald et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proc. of ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Quoc V. Le", "Ilya Sutskever"], "venue": "In arXiv preprint arXiv:1309.4168v1", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A semantic concordance", "author": ["Claudia Leacock", "Randee Tengi", "Ross T. Bunker"], "venue": "In Proc. of HLT,", "citeRegEx": "Miller et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1993}, {"title": "Building the italian", "author": ["Francesco Barsotti", "Marco Battista", "Nicoletta Calzolari", "Ornella Corazzari", "Alessandro Lenci", "Antonio Zampolli", "Francesca Fanciulli", "Maria Massetani", "Remo Raffaelli"], "venue": null, "citeRegEx": "Montemagni et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Montemagni et al\\.", "year": 2003}, {"title": "Intriguing properties of neural networks", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "In Proc. of ICLR", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proc. of NAACL,", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer"], "venue": "In Proc. of EMNLP", "citeRegEx": "Tsvetkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Cross-language parser adaptation between related languages", "author": ["Zeman", "Resnik2008] Daniel Zeman", "Philip Resnik"], "venue": "In Proc. of IJCNLP,", "citeRegEx": "Zeman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zeman et al\\.", "year": 2008}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y Zou", "Richard Socher", "Daniel M Cer", "Christopher D Manning"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "NLP tasks (Collobert and Weston, 2008), shared representation of words across languages offer intriguing possibilities (Klementiev et al., 2012).", "startOffset": 119, "endOffset": 144}, {"referenceID": 21, "context": "While previous work has used hand-engineered features that are cross-linguistically stable as the basis model transfer (Zeman and Resnik, 2008; McDonald et al., 2011), automatically learned embeddings offer the promise of better generalization at lower cost (Klementiev et al.", "startOffset": 119, "endOffset": 166}, {"referenceID": 14, "context": ", 2011), automatically learned embeddings offer the promise of better generalization at lower cost (Klementiev et al., 2012; Hermann and Blunsom, 2014; Guo et al., 2016).", "startOffset": 99, "endOffset": 169}, {"referenceID": 11, "context": ", 2011), automatically learned embeddings offer the promise of better generalization at lower cost (Klementiev et al., 2012; Hermann and Blunsom, 2014; Guo et al., 2016).", "startOffset": 99, "endOffset": 169}, {"referenceID": 8, "context": "(Gardner et al., 2015) and a variant of the multiSkip method (Guo et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": ", 2015) and a variant of the multiSkip method (Guo et al., 2016).", "startOffset": 46, "endOffset": 64}, {"referenceID": 5, "context": "To do this, we align the corpus using fast align (Dyer et al., 2013) in both directions.", "startOffset": 49, "endOffset": 68}, {"referenceID": 10, "context": "We developed the multiSkip method independently of Guo et al. (2016). 2.", "startOffset": 51, "endOffset": 69}, {"referenceID": 22, "context": "We can then apply any monolingual embedding estimator; here, we use the skipgram model from Mikolov et al. (2013a).", "startOffset": 92, "endOffset": 115}, {"referenceID": 17, "context": "To establish a bilingual embedding, with a parallel corpus Pm,n of source language m and target language n, Luong et al. (2015b) estimate conditional models of words in both source and target positions.", "startOffset": 108, "endOffset": 129}, {"referenceID": 8, "context": "Gardner et al. (2015) solves for a low-rank decomposition UV\u22a4 which both approximates X as well as its transformations A\u22a4X, XA and A\u22a4XA by defining the following objective:", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "MultiQVEC+ extends QVEC (Tsvetkov et al., 2015), a recently proposed monolingual evaluation method, addressing fundamental flaws and extending it to multiple languages.", "startOffset": 24, "endOffset": 47}, {"referenceID": 0, "context": "Word similarity datasets such as WS-353-SIM (Agirre et al., 2009) and MEN (Bruni et al.", "startOffset": 44, "endOffset": 65}, {"referenceID": 1, "context": ", 2009) and MEN (Bruni et al., 2014) provide human judgments of semantic similarity.", "startOffset": 16, "endOffset": 36}, {"referenceID": 17, "context": "For this paper, we report results on an English word similarity task, the Stanford RW dataset (Luong et al., 2013), as well as a combination of several cross-lingual word similarity datasets (Camacho-Collados et al.", "startOffset": 94, "endOffset": 114}, {"referenceID": 2, "context": ", 2013), as well as a combination of several cross-lingual word similarity datasets (Camacho-Collados et al., 2015).", "startOffset": 84, "endOffset": 115}, {"referenceID": 22, "context": "This is a variant of the method used by Mikolov et al. (2013b) to evaluate bilingual embeddings.", "startOffset": 40, "endOffset": 63}, {"referenceID": 28, "context": "Our method is a monolingual improvement and a multilingual extension of QVEC\u2014a recently proposed monolingual evaluation based on alignment of embeddings to a matrix of features extracted from a linguistic resource (Tsvetkov et al., 2015).", "startOffset": 214, "endOffset": 237}, {"referenceID": 26, "context": "First, it is not invariant to linear transformations of the embeddings\u2019 basis, whereas the bases in word embeddings are generally arbitrary (Szegedy et al., 2014).", "startOffset": 140, "endOffset": 162}, {"referenceID": 24, "context": "In this paper, instead of only constructing the linguistic matrix based on monolingual annotations, we use supersense tag annotations for English (Miller et al., 1993), Danish (Mart\u0131nez Alonso et al.", "startOffset": 146, "endOffset": 167}, {"referenceID": 25, "context": ", 2015) and Italian (Montemagni et al., 2003) to create extensions of", "startOffset": 20, "endOffset": 45}, {"referenceID": 14, "context": "For document classification, we follow Klementiev et al. (2012) in using the RCV corpus of newswire text, and train a classifier which differentiates between four topics.", "startOffset": 39, "endOffset": 64}, {"referenceID": 5, "context": "For dependency parsing, we train the stack-LSTM parser of Dyer et al. (2015) on a subset of the languages in the universal dependencies v1.", "startOffset": 58, "endOffset": 77}, {"referenceID": 10, "context": ", 2015a, inter alia),12 cross-lingual dependency parsing (Guo et al., 2015; Guo et al., 2016), and cross-lingual document classification (Klementiev et al.", "startOffset": 57, "endOffset": 93}, {"referenceID": 11, "context": ", 2015a, inter alia),12 cross-lingual dependency parsing (Guo et al., 2015; Guo et al., 2016), and cross-lingual document classification (Klementiev et al.", "startOffset": 57, "endOffset": 93}, {"referenceID": 14, "context": ", 2016), and cross-lingual document classification (Klementiev et al., 2012; Gouws et al., 2014; Kocisk\u1ef3 et al., 2014).", "startOffset": 51, "endOffset": 118}, {"referenceID": 9, "context": ", 2016), and cross-lingual document classification (Klementiev et al., 2012; Gouws et al., 2014; Kocisk\u1ef3 et al., 2014).", "startOffset": 51, "endOffset": 118}, {"referenceID": 15, "context": ", 2016), and cross-lingual document classification (Klementiev et al., 2012; Gouws et al., 2014; Kocisk\u1ef3 et al., 2014).", "startOffset": 51, "endOffset": 118}, {"referenceID": 27, "context": "Word clusters is a related form of distributional representation; in clustering, cross-lingual distributional representations were proposed as well (T\u00e4ckstr\u00f6m et al., 2012).", "startOffset": 148, "endOffset": 172}], "year": 2017, "abstractText": "We introduce new methods for estimating and evaluating embeddings of words from dozens of languages in a single shared embedding space. Our estimation methods, multiCluster and multiCCA, use dictionaries and monolingual data; they do not require parallel data. Our new evaluation method, multiQVEC+, is shown to correlate better than previous ones with two downstream tasks (text categorization and parsing). On this evaluation and others, our estimation methods outperform existing ones. We also describe a web portal for evaluation that will facilitate further research in this area, along with open-source releases of all our methods.", "creator": "LaTeX with hyperref package"}}}