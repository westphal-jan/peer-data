{"id": "1602.02070", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Compressive PCA for Low-Rank Matrices on Graphs", "abstract": "Randomized algorithms reduce the complexity of low-rank recovery methods only w.r.t dimension p of a big dataset $Y \\in \\Re^{p \\times n}$. However, the case of large n is cumbersome to tackle without sacrificing the recovery. The recently introduced Fast Robust PCA on Graphs (FRPCAG) approximates a recovery method for matrices which are low-rank on graphs constructed between their rows and columns. In this paper we provide a novel framework, Compressive PCA on Graphs (CPCA) for an approximate recovery of such data matrices from sampled measurements. We introduce a RIP condition for low-rank matrices on graphs which enables efficient sampling of the rows and columns to perform FRPCAG on the sampled matrix. Several efficient, parallel and parameter-free decoders are presented along with their theoretical analysis for the low-rank recovery and clustering applications of PCA. On a single core machine, CPCA gains a speed up of p/k over FRPCAG, where k &lt;&lt; p is the subspace dimension. Numerically, CPCA can efficiently cluster 70,000 MNIST digits in less than a minute and recover a low-rank matrix of size 10304 X 1000 in 15 secs, which is 6 and 100 times faster than FRPCAG and exact recovery.", "histories": [["v1", "Fri, 5 Feb 2016 15:51:34 GMT  (1418kb,D)", "https://arxiv.org/abs/1602.02070v1", null], ["v2", "Mon, 11 Apr 2016 10:51:25 GMT  (1418kb,D)", "http://arxiv.org/abs/1602.02070v2", null], ["v3", "Mon, 2 May 2016 13:49:40 GMT  (1426kb,D)", "http://arxiv.org/abs/1602.02070v3", null], ["v4", "Tue, 4 Oct 2016 08:35:35 GMT  (1879kb,D)", "http://arxiv.org/abs/1602.02070v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nauman shahid", "nathanael perraudin", "gilles puy", "pierre vandergheynst"], "accepted": false, "id": "1602.02070"}, "pdf": {"name": "1602.02070.pdf", "metadata": {"source": "CRF", "title": "Compressive PCA for Low-Rank Matrices on Graphs", "authors": ["Nauman Shahid", "Nathanael Perraudin", "Gilles Puy", "Pierre Vandergheynst"], "emails": ["pierre.vandergheynst}@epfl.ch,", "gilles.puy@gmail.com"], "sections": [{"heading": null, "text": "In fact, it is such that it is a matter of a way in which people are able to put themselves into the world, in which they are able to understand the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live."}, {"heading": "A. The Problem Statement", "text": "In this paper, we answer the following questions: 1) What would be an efficient and highly scalable recovery framework for data sets consisting of two low-ranged manifolds? 2) Alternatively, given a few randomly selected observations and features from a data matrix Y, < p \u00b7 n, is it possible to efficiently restore the full nonlinear low-ranged representation? We mostly limit ourselves to the above case 1, where a graphical predecessor is available or can be conveniently constructed for the full observation set of the application in question. A brief first treatment of the second case is section VII.C of this paper."}, {"heading": "B. Contributions", "text": "In fact, it is the case that most of us will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they"}, {"heading": "A. Graphs for Compressed data", "text": "To ensure the preservation of algebraic and spectral properties, it is possible to construct the compressed Laplacian L & # 8222; r & # 8220; and L & # 8222; c & # 8220; from the cron reduction of Lr & # 8220; and Lc & # 8211; Let us set the set of sampled nodes and the addition and specify L & # 8222; l & # 8220; (row, column) L & # 8222; r.t & # 8220; L & # 8220; c & # 8220; f & # 252; r the columns of the compressed matrix Y & # 8220;."}, {"heading": "B. FRPCAG on the Compressed Data", "text": "Once the results are available, the next step is restoring the low-grade matrix X-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K"}, {"heading": "V. DECODERS FOR LOW-RANK RECOVERY", "text": "The goal is to decode the low-rank matrix X < p \u00b7 n for the full Y. We assume that X = MrX + Mc + E, where E = MrX + E, the noise caused by (4).A. Ideal decoder A, is decoded directly to the original graph Lr and Lc, if you know the basis Pkr, Qkc contains the solution to the following optimization problem: min X = MrXMc \u2212 X = 2F s.t: (X) i = span (Pkr), (X >) j = span (Qkc), Qkc: (6) Theorem 2. Let Mr. and Mc be such that (2) Appendix \u2212 k."}, {"heading": "B. Alternate Decoder", "text": "Since the ideal decoder is mathematically expensive, we propose to decode X from X by using a convex and mathematically comprehensible problem that involves minimizing the graph-dirichlet energies. (8) Theorem 3. Let Lord and Mc be such that (2) holds true and (8) X can also be the solution to (8) with (8) errors, (8) with (8) errors, (8) with (8) errors, (8) with (8) errors, (8) with (8) errors, (8) with (8) errors, (8) with errors, (8) with errors, (8) with errors, (8) with errors, (8) with errors, (8) with errors, (8) with (8) and (8)."}, {"heading": "C. Approximate Decoder", "text": "The aim of this step-by-step approach is to observe the close relationship between alternative and approximate coordination."}, {"heading": "A. Clustering", "text": "This year, we will be able to put ourselves at the forefront to pave the way for the future."}, {"heading": "B. Low-rank recovery", "text": "To achieve the effectiveness of our model, we must conduct experiments with 1000 frames of 3 videos that are available online. All frames are vectorized and arranged in a data matrix of Y whose columns match frames. The first row corresponds to a frame from the shopping mall lobby, the second row from the food counter and the third row from the airport lobby. The second row shows the recovery of frames for an actual frame. The first row corresponds to a frame from the shopping mall lobby, the second row from the food counter and the third row from the airport lobby."}, {"heading": "C. Low-Rank Recovery from Random Projections", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "A. Proof of theorem 1", "text": "We start with the sample of the lines. Theorem 5 in [28] shows that for each line (0, 1), there is a probability of at least 1 \u2212 r, (1 \u2212 r), (2 \u2212 r), (2 \u2212 r), (1 \u2212 r), (2 \u2212 r), (1 \u2212 r), (1 \u2212 r), (2 \u2212 r), (2 \u2212 r), (2 \u2212 r), (2 \u2212 r), (2 \u2212 r), (27), (2 \u2212 c), (2 \u2212 c), (2 \u2212 r), (1 \u2212 r), (2 \u2212 r), (2 \u2212 r), (2 \u2212 c), (2 \u2212 r), (2 \u2212 r), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (5), (4), (4), (4), (4), (4), (4), (4), (4),"}, {"heading": "B. Proof of Theorem 2", "text": "Using the optimum condition we have for each Z-Rp \u00b7 n, Mr. X-Mc-X-C-F \u2264 Mr. ZMc-X-X-F. For Z = X we have Mr. X-Mc-X-F \u2264 Mr. X-Mc-X-X-F, which is Mr. X-Mc-MrX-Mc-E-E-F-E-E-E-F. As (2) applies, we have Mr. X-Mc-E-Mc-E-F-E-Mr (X-X-X) Mc-F-E-E-F-E-E-E-E-F."}, {"heading": "C. Proof of Theorem 3", "text": "Using the optimum condition we have for each Z-question < p \u00b7 n and the optimal solution X-question = X-question + E-question: \"How can we answer this question?\" (\"How can we solve it?\") (\"How can we solve it?\") (\"How can we solve it?\") (\"How can we solve it?\") (\"How can we solve it?\") (\"How can we solve it?\") (\"How can we solve it?\") (\"How can we solve it?\") (\"How can we solve it?\") (\"How can we solve it?\") (\"How can we solve it?\") (\"How can we solve it?\" (How can we solve it? \") (How can we solve it?) (How can we solve it?) (How can we solve it? (How can we solve it?) (How can we solve it?) (How can we solve it?) (How can we solve it?) (How can we solve it?"}, {"heading": "E. Proof of Theorem 4", "text": "We can write (12) and (13) as follows: min u1 \u00b7 \u00b7 up p + p + p + p + p + p + p > i Lcvi] (39) min v1 \u00b7 \u00b7 vn + p = 1 [2] m > c vi \u2212 v \u00b2 i \u00b2 s 22 + p > i Lcvi] (40) In this proof we only deal with the problem (39) and the restoration of U \u00b2 s. The proof for the problem (13) and the restoration of V \u00b2 s is identical, and the above two problems can be solved independently for each i. Theorem 3.2 of [28] gives us: \"u\" m \"i \u2212 u \u00b2 i \u00b2 i \u00b2 p \u00b2 s (1 \u2212 p \u00b2 s \u00b2 s) [2 + 1 \u00b2 r \u00b2 s \u00b2 s \u00b2 s \u00b2 s (F \u00b2 s \u00b2 s \u00b2 s)."}, {"heading": "F. Proof of Lemma 1", "text": "Let's divide L into submatrices as follows: L = [Laa Lab Lba Lbb] Now (17) we can write as: min Sa [Sa Sb] > [Laa Lab Lba Lbb] [Sa Sb] s.t: Sb = RFurther development we get: min Sa S > a LaaSa + S > a LabR + R > LbaSa + RLbbRusing: 2LabR + 2LaaSa = 0Sa = \u2212 L \u2212 1aaLabR"}, {"heading": "G. Other Approximate Decoders", "text": "Alternatively, if the full data matrix Y is available, we can further reduce the complexity by doing a graph sample for only one of the two subranges U or V. Approximate decoder 2: Suppose we do the upsampling only for U, then the approximate decoder 2 can be written as follows: min Utr (U > LrU) s.t: MrU = U. The solution for U is given by Gl. 18. Then we can write V as follows: V = Y > U \u03a3 (1).p However, we do not need to explicitly specify V here, but the lower-level X can be determined directly from U with the projection given below: X = U."}, {"heading": "H. Computational Complexities & Additional Results", "text": "We assume that K, k, p and n + k + K + K + K, that we are able to process the data. \"The construction of Graph Gr is contained only in FRPCAG and CPCA. (1) We assume that K, k and Chebyshev are able to use the Graph Gc. (2) We assume that K, k and Chebyshev are able to use the Graph Gc + K + K + K. (2) We assume that K, k, p and n + K + K + K + K, c and CPCA. (2) We assume that K, K and CPCA."}], "references": [{"title": "A variational approach to stable principal component pursuit", "author": ["A. Aravkin", "S. Becker", "V. Cevher", "P. Olsen"], "venue": "arXiv preprint arXiv:1406.1089,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "On the rate of convergence of the preconditioned conjugate gradient method", "author": ["O. Axelsson", "G. Lindskog"], "venue": "Numerische Mathematik, 48(5):499\u2013523,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1986}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, 2(1):183\u2013202,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural computation, 15(6):1373\u2013 1396,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "An improved approximation algorithm for the column subset selection problem", "author": ["C. Boutsidis", "M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 968\u2013977. Society for Industrial and Applied Mathematics,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["D. Cai", "X. He", "J. Han", "T.S. Huang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(8):1548\u20131560,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust principal component analysis", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Signal processing with compressive measurements", "author": ["M. Davenport", "P.T. Boufounos", "M.B. Wakin", "R.G. Baraniuk"], "venue": "Selected Topics in Signal Processing, IEEE Journal of,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Kron reduction of graphs with applications to electrical networks", "author": ["F. Dorfler", "F. Bullo"], "venue": "Circuits and Systems I: Regular Papers, IEEE Transactions on, 60(1):150\u2013163,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Using the triangle inequality to accelerate k-means", "author": ["C. Elkan"], "venue": "ICML, volume 3, pages 147\u2013153,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Laplacian sparse coding, hypergraph laplacian sparse coding, and applications", "author": ["S. Gao", "I.-H. Tsang", "L.-T. Chia"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(1):92\u2013104,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust pca with compressed data", "author": ["W. Ha", "R.F. Barber"], "venue": "Advances in Neural Information Processing Systems, pages 1927\u20131935,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.-G. Martinsson", "J.A. Tropp"], "venue": "SIAM review, 53(2):217\u2013288,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Graph-laplacian pca: Closed-form solution and robustness", "author": ["B. Jiang", "C. Ding", "J. Tang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 3492\u20133498. IEEE,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Low-rank matrix factorization with multiple hypergraph regularizers", "author": ["T. Jin", "J. Yu", "J. You", "K. Zeng", "C. Li", "Z. Yu"], "venue": "Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "How to learn a graph from smooth signals", "author": ["V. Kalofolias"], "venue": "th International Conference on Artificial Intelligence and Statistics AISTATS, Cadiz, Spain,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning the parts of objects by nonnegative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, 401(6755):788\u2013791,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Identifying outliers in large matrices via randomized adaptive compressive sampling", "author": ["X. Li", "J. Haupt"], "venue": "Signal Processing, IEEE Transactions on, 63(7):1792\u20131807,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["G. Liu", "Z. Lin", "S. Yan", "J. Sun", "Y. Yu", "Y. Ma"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(1):171\u2013184,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Graph-regularized low-rank representation for destriping of hyperspectral images", "author": ["X. Lu", "Y. Wang", "Y. Yuan"], "venue": "IEEE transactions on geoscience and remote sensing, 51(7):4009\u20134018,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable nearest neighbour algorithms for high dimensional data", "author": ["M. Muja", "D. Lowe"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D.G. Lowe"], "venue": "VISAPP (1), 2,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast randomized singular value thresholding for nuclear norm minimization", "author": ["T.-H. Oh", "Y. Matsushita", "Y.-W. Tai", "I.S. Kweon"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4484\u20134493,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Generalized laplacian precision matrix estimation for graph signal processing", "author": ["E. Pavez", "A. Ortega"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6350\u2013 6354. IEEE,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "GSPBOX: A toolbox for signal processing on graphs", "author": ["N. Perraudin", "J. Paratte", "D. Shuman", "V. Kalofolias", "P. Vandergheynst", "D.K. Hammond"], "venue": "ArXiv e-prints,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "UNLocBoX A matlab convex optimization toolbox using proximal splitting methods", "author": ["N. Perraudin", "D. Shuman", "G. Puy", "P. Vandergheynst"], "venue": "ArXiv e-prints, #feb#", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Stationary signal processing on graphs", "author": ["N. Perraudin", "P. Vandergheynst"], "venue": "ArXiv e-prints, #jan#", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Random sampling of bandlimited signals on graphs", "author": ["G. Puy", "N. Tremblay", "R. Gribonval", "P. Vandergheynst"], "venue": "arXiv preprint arXiv:1511.05118,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "High dimensional low rank plus sparse matrix decomposition", "author": ["M. Rahmani", "G. Atia"], "venue": "arXiv preprint arXiv:1502.00182,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Randomized robust subspace recovery for big data", "author": ["M. Rahmani", "G.K. Atia"], "venue": "Machine Learning for Signal Processing (MLSP), 2015 IEEE 25th International Workshop on, pages 1\u20136. IEEE,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Collaborative filtering with graph information: Consistency and scalable methods", "author": ["N. Rao", "H.-F. Yu", "P.K. Ravikumar", "I.S. Dhillon"], "venue": "Advances in Neural Information Processing Systems, pages 2107\u20132115,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u20132326,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "A fast all nearest neighbor algorithm for applications involving large point-clouds", "author": ["J. Sankaranarayanan", "H. Samet", "A. Varshney"], "venue": "Computers &amp; Graphics, 31(2):157\u2013174,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Network topology identification from spectral templates", "author": ["S. Segarra", "A.G. Marques", "G. Mateos", "A. Ribeiro"], "venue": "arXiv preprint arXiv:1604.02610,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust principal component analysis on graphs", "author": ["N. Shahid", "V. Kalofolias", "X. Bresson", "M. Bronstein", "P. Vandergheynst"], "venue": "arXiv preprint arXiv:1504.06151,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast robust pca on graphs", "author": ["N. Shahid", "N. Perraudin", "V. Kalofolias", "G. Puy", "P. Vandergheynst"], "venue": "IEEE Journal of Selected Topics in Signal Processing, 10(4):740\u2013756,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Graph dual regularization non-negative matrix factorization for co-clustering", "author": ["F. Shang", "L. Jiao", "F. Wang"], "venue": "Pattern Recognition, 45(6):2237\u2013 2250,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "Signal Processing Magazine, IEEE, 30(3):83\u201398,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Accelerated filtering on graphs using lanczos method", "author": ["A. Susnjara", "N. Perraudin", "D. Kressner", "P. Vandergheynst"], "venue": "arXiv preprint arXiv:1509.04537,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Matrix coherence and the nystrom method", "author": ["A. Talwalkar", "A. Rostamizadeh"], "venue": "arXiv preprint arXiv:1004.2008,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Low rank approximation with sparse integration of multiple manifolds for data representation", "author": ["L. Tao", "H.H. Ip", "Y. Wang", "X. Shu"], "venue": "Applied Intelligence, pages 1\u201317,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Compressive spectral clustering", "author": ["N. Tremblay", "G. Puy", "R. Gribonval", "P. Vandergheynst"], "venue": "arXiv preprint arXiv:1602.02018,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "On the conditioning of random subdictionaries", "author": ["J.A. Tropp"], "venue": "Applied and Computational Harmonic Analysis, 25(1):1\u201324,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Randomized algorithms for low-rank matrix factorizations: sharp performance bounds", "author": ["R. Witten", "E. Candes"], "venue": "Algorithmica, pages 1\u201318,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable sparse subspace clustering by orthogonal matching pursuit", "author": ["C. You", "D. Robinson", "R. Vidal"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, volume 1,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Low-rank matrix approximation with manifold regularization", "author": ["Z. Zhang", "K. Zhao"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(7):1717\u20131729,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "Robust Principal Component Analysis (RPCA) [7], a linear dimensionality reduction algorithm can be used to exactly describe a dataset lying on a single linear lowdimensional subspace.", "startOffset": 43, "endOffset": 46}, {"referenceID": 18, "context": "Low-rank Representation (LRR) [19], on the other hand can be used for data drawn from multiple linear subspaces.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "Many high dimensional datasets lie intrinsically on a smooth and very low-dimensional manifold that can be characterized by a graph G between the data samples [4].", "startOffset": 159, "endOffset": 162}, {"referenceID": 37, "context": "Then, the combinatorial Laplacian that characterizes the graph G is defined as L = D \u2212 W and its normalized form as Ln = D\u22121/2(D \u2212W )D\u22121/2 [38].", "startOffset": 139, "endOffset": 143}, {"referenceID": 34, "context": "Extensions of RPCA and LRR such as Robust PCA on Graphs (RPCAG) [35] and Graph Regularized LRR (GLRR) [20] propose to incorporate graph regularization as a method to recover non-linear low-rank structures.", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "Extensions of RPCA and LRR such as Robust PCA on Graphs (RPCAG) [35] and Graph Regularized LRR (GLRR) [20] propose to incorporate graph regularization as a method to recover non-linear low-rank structures.", "startOffset": 102, "endOffset": 106}, {"referenceID": 42, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 139, "endOffset": 143}, {"referenceID": 4, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 145, "endOffset": 148}, {"referenceID": 43, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 150, "endOffset": 154}, {"referenceID": 17, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 162, "endOffset": 166}, {"referenceID": 22, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 168, "endOffset": 172}, {"referenceID": 28, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 174, "endOffset": 178}, {"referenceID": 29, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 180, "endOffset": 184}, {"referenceID": 11, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 186, "endOffset": 190}, {"referenceID": 7, "context": "Randomized techniques come into play to deal with the scalability problem associated with very high dimensional data (the case of large p) [43], [5], [44], [18], [13], [23], [29], [30], [12] using the tools of compression [8].", "startOffset": 222, "endOffset": 225}, {"referenceID": 39, "context": "The case of large n can be tackled by using the sampling schemes accompanied with Nystrom method [40].", "startOffset": 97, "endOffset": 101}, {"referenceID": 44, "context": "Scalable extensions of LRR such as [45] exist but they focus only on the subspace clustering application.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "al [1]", "startOffset": 3, "endOffset": 6}, {"referenceID": 35, "context": "The recently introduced Fast Robust PCA on Graphs (FRPCAG) [36] approximates a recovery method for non-linear low-rank datasets, which are called Low-rank matrices on graphs.", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": "Inspired by the underlying stationarity assumption [27], the authors introduce a joint notion of lowrankness for the features and samples (rows and columns) of a data matrix.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 145, "endOffset": 149}, {"referenceID": 40, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 151, "endOffset": 155}, {"referenceID": 45, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 157, "endOffset": 161}, {"referenceID": 14, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 163, "endOffset": 167}, {"referenceID": 5, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 169, "endOffset": 172}, {"referenceID": 36, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 174, "endOffset": 178}, {"referenceID": 13, "context": "However, the clustering experiments had been widely adopted as a standard procedure to demonstrate the quality of the feature extraction methods [11], [41], [46], [15], [6], [37], [14].", "startOffset": 180, "endOffset": 184}, {"referenceID": 27, "context": "Our proposed framework is inspired by the recently introduced sampling of band-limited signals on graphs [28].", "startOffset": 105, "endOffset": 109}, {"referenceID": 27, "context": "While we borrow several concepts from here, our framework is significantly different from [28] in many contexts.", "startOffset": 90, "endOffset": 94}, {"referenceID": 27, "context": "We target the low-rank recovery of matrices, whereas [28] targets the recovery of band-limited signals / vectors.", "startOffset": 53, "endOffset": 57}, {"referenceID": 27, "context": "The design of a sampling scheme is the major focus of [28], while we just focus on the case of uniform sampling and instead focus on how much to sample jointly given the two graphs.", "startOffset": 54, "endOffset": 58}, {"referenceID": 27, "context": "Of course, our method can be extended directly for the other sampling schemes in [28].", "startOffset": 81, "endOffset": 85}, {"referenceID": 27, "context": "Unlike [28], we target two applications related to PCA: 1) low-rank recovery and 2) clustering.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "Thus, contrary to [28] our proposed decoders are designed for these applications.", "startOffset": 18, "endOffset": 22}, {"referenceID": 27, "context": "A major contribution of our work in contrast to [28] is the design of approximate decoders for low-rank recovery and clustering which significantly boost the speed of our framework for big datasets without compromising on the performance.", "startOffset": 48, "endOffset": 52}, {"referenceID": 35, "context": "Then, low-rank matrices on graphs can be defined as following and recovered by solving FRPCAG [36].", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "Throughout this work we use the approximate nearest neighbor algorithm (FLANN [21]) for graph construction whose complexity is O(np log(n)) for p n [33] (and it can be performed in parallel).", "startOffset": 78, "endOffset": 82}, {"referenceID": 32, "context": "Throughout this work we use the approximate nearest neighbor algorithm (FLANN [21]) for graph construction whose complexity is O(np log(n)) for p n [33] (and it can be performed in parallel).", "startOffset": 148, "endOffset": 152}, {"referenceID": 37, "context": "In the above equations Qkc\u2206 c i and P > kr \u2206j characterize the first kc and kr fourier modes [38] of the nodes i and j on the graphs Gc and Gr respectively.", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "need to be sampled from the graphs Gr and Gc such that the properties of the graphs are preserved [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 27, "context": "Theorem 1 is a direct extension of the RIP for k-bandlimited signals on one graph [28].", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "It is proved in [28] that \u03bdkc \u2265 \u221a kc and \u03bdkr \u2265 \u221a kr.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "One should resort to a more distribution aware sampling in such a case as presented in [28].", "startOffset": 87, "endOffset": 91}, {"referenceID": 27, "context": "A consequence of the result in [28] is that there always exist distributions that ensure that the RIP holds when sampling O(kr log(kr)) rows and O(kc log(kc)) columns only.", "startOffset": 31, "endOffset": 35}, {"referenceID": 27, "context": "The optimal sampling distribution for which this result holds is defined in [28] (see Section 2.", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "Furthermore, a fast algorithm to compute this distribution also exists (Section 4 of [28]).", "startOffset": 85, "endOffset": 89}, {"referenceID": 8, "context": "To ensure the preservation of algebraic and spectral properties one can construct the compressed Laplacians L\u0303r \u2208 <\u03c1r\u00d7\u03c1r and L\u0303c \u2208 <\u03c1c\u00d7\u03c1c from the Kron reduction of Lr and Lc [9].", "startOffset": 175, "endOffset": 178}, {"referenceID": 8, "context": "4 of [9] two nodes \u03b1, \u03b2 are not connected in L\u0303c if there is no path between them in Lc via \u03a9\u0304.", "startOffset": 5, "endOffset": 8}, {"referenceID": 27, "context": "Such schemes have been discussed in [28] and have not been addressed in this work.", "startOffset": 36, "endOffset": 40}, {"referenceID": 38, "context": "The only expensive operation above is the inverse of L(\u03a9\u0304, \u03a9\u0304) which can be performed with O(OlKn) cost using the Lancoz method [39], where Ol is the number of iterations for Lancoz approximation.", "startOffset": 128, "endOffset": 132}, {"referenceID": 35, "context": "The low-rank matrix X\u0303 = X\u0303\u2217+\u1ebc can be recovered by solving the FRPCAG problem as proposed in [36] and re-written below:", "startOffset": 93, "endOffset": 97}, {"referenceID": 35, "context": "From Theorem 1 in [36], the low-rank approximation error comprises the orthogonal projection of X\u0303\u2217 on the complement graph eigenvectors (  \u0304\u0303 Qkc ,  \u0304\u0303 Pkr ) and depends on the spectral gaps \u03bb\u0303kc/\u03bb\u0303kc+1, \u03bb\u0303kr/\u03bb\u0303kr+1 as following: \u2016X\u0303\u2217  \u0304\u0303 Qkc\u2016F + \u2016  \u0304\u0303 P> krX\u0303 \u2016F = \u2016\u1ebc\u2016F \u2264 1 \u03b3 \u03c6(E) + \u2016\u1ef8 \u2016F ( \u03bb\u0303kc", "startOffset": 18, "endOffset": 22}, {"referenceID": 30, "context": "(12) &(13) are feasible solutions of the joint non-convex, factorized, and graph regularized low-rank optimization problem like the one presented in [31].", "startOffset": 149, "endOffset": 153}, {"referenceID": 30, "context": "and B be the subspaces that we want to recover then we can re-write the problem studied in [31] as following:", "startOffset": 91, "endOffset": 95}, {"referenceID": 41, "context": "We refer to the Compressive Spectral Clustering (CSC) framework [42], where the authors solve a similar problem by arguing that each of the columns of C can be obtained by assuming that it lies close to the span(Qkc), where Qkc are the first kc Laplacian eigenvectors of the graph Gc.", "startOffset": 64, "endOffset": 68}, {"referenceID": 27, "context": "(30) in the proof of Theorem 1 and Theorem 5 in [28]).", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "2 in [28].", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "More specifically, one can refer to [3] for a detailed study on FISTA and [2] for PCG.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "More specifically, one can refer to [3] for a detailed study on FISTA and [2] for PCG.", "startOffset": 74, "endOffset": 77}, {"referenceID": 25, "context": "We perform two types of experiments corresponding to two applications of PCA 1) Data clustering and 2) Low-rank recovery using two open-source toolboxes: the UNLocBoX [26] and the GSPBox [25].", "startOffset": 167, "endOffset": 171}, {"referenceID": 24, "context": "We perform two types of experiments corresponding to two applications of PCA 1) Data clustering and 2) Low-rank recovery using two open-source toolboxes: the UNLocBoX [26] and the GSPBox [25].", "startOffset": 187, "endOffset": 191}, {"referenceID": 34, "context": "1) Experimental Setup: Datasets: We perform our clustering experiments on 5 benchmark databases (as in [35], [36]): CMU PIE, ORL, YALE, MNIST and USPS.", "startOffset": 103, "endOffset": 107}, {"referenceID": 35, "context": "1) Experimental Setup: Datasets: We perform our clustering experiments on 5 benchmark databases (as in [35], [36]): CMU PIE, ORL, YALE, MNIST and USPS.", "startOffset": 109, "endOffset": 113}, {"referenceID": 3, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 165, "endOffset": 168}, {"referenceID": 31, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 203, "endOffset": 207}, {"referenceID": 13, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 255, "endOffset": 259}, {"referenceID": 45, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 311, "endOffset": 315}, {"referenceID": 16, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 359, "endOffset": 363}, {"referenceID": 5, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 425, "endOffset": 428}, {"referenceID": 6, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 450, "endOffset": 453}, {"referenceID": 34, "context": "Comparison with other methods: We compare the clustering performance of CPCA with 11 other models including: 1) k-means on original data 2) Laplacian Eigenmaps (LE) [4] 3) Locally Linear Embedding (LLE) [32] 4) Standard PCA 5) Graph Laplacian PCA (GLPCA) [14] 6) Manifold Regularized Matrix Factorization (MMF) [46] 7) Non-negative Matrix Factorization (NMF) [17] 8) Graph Regularized Nonnegative Matrix Factorization (GNMF) [6] 9) Robust PCA (RPCA) [7] 10) Robust PCA on Graphs (RPCAG) [35] and", "startOffset": 487, "endOffset": 491}, {"referenceID": 35, "context": "11) Fast Robust PCA on Graphs (FRPCAG) [36].", "startOffset": 39, "endOffset": 43}, {"referenceID": 21, "context": "Gr, Gc are constructed using FLANN [22] as discussed in Section II.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": "The state-of-the-art results [16], [24], [34] do not provide any scalable solutions yet.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "The state-of-the-art results [16], [24], [34] do not provide any scalable solutions yet.", "startOffset": 35, "endOffset": 39}, {"referenceID": 33, "context": "The state-of-the-art results [16], [24], [34] do not provide any scalable solutions yet.", "startOffset": 41, "endOffset": 45}, {"referenceID": 35, "context": "The time reported here corresponds to steps 2 to 6 of Table I, Algorithm 1 of [36] for FRPCAG, [7] for RPCA and [35] for RPCAG, excluding the construction of graphs Gr, Gc.", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "The time reported here corresponds to steps 2 to 6 of Table I, Algorithm 1 of [36] for FRPCAG, [7] for RPCA and [35] for RPCAG, excluding the construction of graphs Gr, Gc.", "startOffset": 95, "endOffset": 98}, {"referenceID": 34, "context": "The time reported here corresponds to steps 2 to 6 of Table I, Algorithm 1 of [36] for FRPCAG, [7] for RPCA and [35] for RPCAG, excluding the construction of graphs Gr, Gc.", "startOffset": 112, "endOffset": 116}], "year": 2016, "abstractText": "We introduce a novel framework for an approximate recovery of data matrices which are low-rank on graphs, from sampled measurements. The rows and columns of such matrices belong to the span of the first few eigenvectors of the graphs constructed between their rows and columns. We leverage this property to recover the non-linear low-rank structures efficiently from sampled data measurements, with a low cost (linear in n). First, a Resrtricted Isometry Property (RIP) condition is introduced for efficient uniform sampling of the rows and columns of such matrices based on the cumulative coherence of graph eigenvectors. Secondly, a state-of-the-art fast low-rank recovery method is suggested for the sampled data. Finally, several efficient, parallel and parameter-free decoders are presented along with their theoretical analysis for decoding the low-rank and cluster indicators for the full data matrix. Thus, we overcome the computational limitations of the standard linear low-rank recovery methods for big datasets. Our method can also be seen as a major step towards efficient recovery of nonlinear low-rank structures. For a matrix of size n \u00d7 p, on a single core machine, our method gains a speed up of p/k over Robust Principal Component Analysis (RPCA), where k p is the subspace dimension. Numerically, we can recover a low-rank matrix of size 10304\u00d71000, 100 times faster than Robust PCA.", "creator": "LaTeX with hyperref package"}}}