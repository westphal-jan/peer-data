{"id": "1602.02123", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Sequence Classification with Neural Conditional Random Fields", "abstract": "The proliferation of sensor devices monitoring human activity generates voluminous amount of temporal sequences needing to be interpreted and categorized. Moreover, complex behavior detection requires the personalization of multi-sensor fusion algorithms. Conditional random fields (CRFs) are commonly used in structured prediction tasks such as part-of-speech tagging in natural language processing. Conditional probabilities guide the choice of each tag/label in the sequence conflating the structured prediction task with the sequence classification task where different models provide different categorization of the same sequence. The claim of this paper is that CRF models also provide discriminative models to distinguish between types of sequence regardless of the accuracy of the labels obtained if we calibrate the class membership estimate of the sequence. We introduce and compare different neural network based linear-chain CRFs and we present experiments on two complex sequence classification and structured prediction tasks to support this claim.", "histories": [["v1", "Fri, 5 Feb 2016 19:19:46 GMT  (564kb,D)", "http://arxiv.org/abs/1602.02123v1", "14th International Conference on Machine Learning and Applications (ICMLA) 2015"]], "COMMENTS": "14th International Conference on Machine Learning and Applications (ICMLA) 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["myriam abramson"], "accepted": false, "id": "1602.02123"}, "pdf": {"name": "1602.02123.pdf", "metadata": {"source": "CRF", "title": "Sequence Classification with Neural Conditional Random Fields", "authors": ["Myriam Abramson"], "emails": ["myriam.abramson@nrl.navy.mil"], "sections": [{"heading": null, "text": "Index Terms - Hybrid Learning Algorithms, Neurocrfs, Sequence ClassifiedI. INTRODUCTIONThe proliferation of sensor devices to monitor human activity generates an enormous amount of time sequences that need to be interpreted and categorized. In addition, the detection of complex behavior requires the personalization of multi-sensor fusion algorithms. For example, stress detection from physiological measurements may involve the fusion of multiple variables such as pupil dilation, heart rate, and skin temperature. Furthermore, it is the relative measurement to other states, rather than its absolute value, that indicates stress that requires the monitoring of sequences of states and their transitions. Conditional Random Fields (CRFs) are often used in structured prediction tasks such as part-of language processing. Conditional probabilities guide the choice of each tag / label in the sequence that the structured task sequence task could combine with the same sequence assignment class of different evaluation models, whereby different evaluation classes could be used."}, {"heading": "II. RELATED WORK", "text": "The problem is that the sequence class is able to form a sequence class in which the sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence"}, {"heading": "III. CONDITIONAL RANDOM FIELDS", "text": "We distinguish between weak-supervised CRFs, where the labels are learned through an auxiliary classification, and strong-supervised CRFs, where the labels are known without ambiguity. Below the derivation of the CRFs, we describe fundamental probability principles and limit our discussion to the classification of the linear chain CRFs for the sequence networks according to conditional independence."}, {"heading": "IV. METHODOLOGY", "text": "We address the problem of sequence learning with a potentially infinite set of labels by combining several models with each other. As in [3], we use the energy production of the output nodes, E (x, y), before comparing and contrasting the results of the different Neural Network Architectures. Viterbi Algorithm [7], [5] guides step-by-step predictions to non-linear feature functions in the case of Multilayer Perceptrons (MLPs), and we compare and contrast different Neural Network Architectures. Viterbi Algorithm [7] guides step-by-step predictions to maximize the choice of each individual label in relation to the entire sequence. P (y, x) is then defined as follows: P (y, x)."}, {"heading": "V. EMPIRICAL EVALUATION", "text": "All neural networks used 1000 SGD examples or less when convergence to zero errors occurred during the training; weights for all neural networks were initialized to small values drawn from a normal distribution with zero mean and standard deviation of 0.0015; the number of hidden nodes for the MLP-based architectures CRF-MLP and CRF-RNN was set to ni + no4, as in [14], where ni is the number of inputs and no is the number of outputs; no attempt was made to optimize the hyperparameters of the various learners; the sigmoid function was the activation function for the nodes in the hidden layer; and the derivation of the square loss function propagated the error at the output node; the propagated loss in structured perception was the difference between the predicted result and the actual result for each function that referred to outputs (0 / 5, with an average)."}, {"heading": "A. OCR Dataset", "text": "The OCR dataset [15] contains 52152 16x8 raster images of letters containing 55 different words with a length of 3 to 14. The number of examples per word varies from 71 to 151. Table I describes these characteristics of the dataset. One model is 5 Word Length # words # examples3 9 1283 5 4 568 6 768 7 5 695 8 750 9 8 1047 10 5 584 11 3 304 12 2 298 13 3 313 14 3 266Table I OCR DATASET CHARACTERISTICS Figure 6. Comparison results from neural network-based CRFs on the OCR dataset. Each dataset is the average result for a word over 5 iterations. Built for each word and tested against other words of the same length. At each iteration, the data for each word is randomly divided into 2 / 3 training sessions and 1 / 3 tests to rate the FRR and, compared to a sample, the significant CRF CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CR6-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF-CRF"}, {"heading": "B. Web Analytics", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "VI. CONCLUSION", "text": "In summary, we have argued that the discriminatory capabilities of CRFs in the structured prediction task do not necessarily translate to the task of sequence classification. To this end, we have compared and compared different architectures of neuroCRFs in two different tasks, the task of handwriting recognition OCR and the task of web analysis, using the same methodology. We have shown that CRFs as a structured prediction approach can also be applied to sequence classification by training multiple models for different types of sequences with potentially different labels and calibrating the results of each model (prior to softmax squashing) with a threshold determined by a linear model. While the structured perceptron overall performs better in the structured prediction task, the discriminatory power of MLP-based architectures, CRF-MLP and CRF-RNN transmits the sequence classification task as a superior task."}], "references": [{"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 694\u2013699, ACM, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "A brief survey on sequence classification", "author": ["Z. Xing", "J. Pei", "E. Keogh"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 12, no. 1, pp. 40\u201348, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural conditional random fields", "author": ["T. Do", "T. Arti"], "venue": "International Conference on Artificial Intelligence and Statistics, pp. 177\u2013184, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "CoRR, vol. abs/1308.0850, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pp. 1\u20138, Association for Computational Linguistics, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Discriminatively trained markov model for sequence classification", "author": ["O. Yakhnenko", "A. Silvescu", "V. Honavar"], "venue": "Data Mining, Fifth IEEE International Conference on, pp. 8\u2013pp, IEEE, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE, pp. 257\u2013 286, 1989.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1989}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "International Conference on Machine Learning ICML, 2001.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning temporal user profiles of web browsing behavior", "author": ["M. Abramson"], "venue": "6th ASE International Conference on Social Computing SocialCom, (Stanford, CA), May 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "An introduction to conditional random fields", "author": ["C. Sutton", "A. McCallum"], "venue": "arXiv preprint arXiv:1011.4088, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning users\u2019 interests by unobtrusively observing their normal behavior", "author": ["J. Goecks", "J. Shavlik"], "venue": "Proceedings of the 5th international conference on Intelligent user interfaces, pp. 129\u2013132, ACM, 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "A comparison of approaches to on-line handwritten character recognition", "author": ["R.H. Kassel"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Implicit authentication through learning user behavior", "author": ["E. Shi", "Y. Niu", "M. Jakobsson", "R. Chow"], "venue": "Information Security, pp. 99\u2013113, Springer, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "words, the score obtained by the CRF model representing a ranking of the sequence given the model has to correlate with the empirical class membership probability [1].", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "The problem that sequence classification addresses is introduced in [2], namely sequence classification is defined as learning the function mapping a sequence s to a class label l from a set of labels L.", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "Neural conditional random fields or neuroCRFs have been investigated in [3] using deep neural networks showing the influence of 2 layers vs.", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "Similar to HMMs, Long Short Term Memory (LSTM) recurrent neural networks (RNNs) [4] learn a sequence of labels from unsegmented data such as that found in handwriting or speech recognition.", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "In [5], perceptrons were integrated as discriminative learners", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "A distinction is made between discriminative and generative k-Markov models in the sequence classification of simple symbolic sequences [6].", "startOffset": 136, "endOffset": 139}, {"referenceID": 6, "context": "CRFs are a supervised method for structured prediction similar to HMMs [7] while relaxing the independence assumption of the observations and the Markov assumption [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "CRFs are a supervised method for structured prediction similar to HMMs [7] while relaxing the independence assumption of the observations and the Markov assumption [8].", "startOffset": 164, "endOffset": 167}, {"referenceID": 1, "context": "CRFs address the strong classification problem [2] of", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "an auxiliary classifier and strongly-supervised CRFs where the labels are known without ambiguity [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 9, "context": "spatial neighborhood properties of the graph as expected [10].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "There are two types of feature functions in representing a sequence [11]: (1) edge functions between two labels and (2) observation functions relating x and y.", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "The normalization constant Z is computationally intractable [11] but does not need to be computed when predicting the labels in the", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": "As in [3], we use the energy output of the output nodes, E(x, y), before squashing by the softmax function, as the score of", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "The Viterbi algorithm [7], [5] guides the step-by-step predictions to maximize the choice of each label y with respect to the entire sequence.", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "The Viterbi algorithm [7], [5] guides the step-by-step predictions to maximize the choice of each label y with respect to the entire sequence.", "startOffset": 27, "endOffset": 30}, {"referenceID": 11, "context": "2) Recurrent neural network [12] (CRF-RNN) trained with backpropagation where the activations of the hidden units at the previous time step are added to the inputs at the next time step in the sequence.", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "3) Structured perceptron [5] (CRF-PRCPT) as described above II.", "startOffset": 25, "endOffset": 28}, {"referenceID": 12, "context": "The number of hidden nodes for the MLP-based architectures, CRF-MLP and CRF-RNN, was set to ni+no 4 as in [14], where ni is the number of inputs and no is the number of outputs.", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "The OCR dataset [15] contains 52152 16x8 raster images of letters composing 55 distinct words of length ranging from 3 to 14.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "We extend previous work done in the context of Web browsing [9] to profile authentication from social media activities of Reddit users.", "startOffset": 60, "endOffset": 63}, {"referenceID": 14, "context": "As stated in [16], commonality is not a good discriminator.", "startOffset": 13, "endOffset": 17}], "year": 2016, "abstractText": "The proliferation of sensor devices monitoring human activity generates voluminous amount of temporal sequences needing to be interpreted and categorized. Moreover, complex behavior detection requires the personalization of multi-sensor fusion algorithms. Conditional random fields (CRFs) are commonly used in structured prediction tasks such as part-of-speech tagging in natural language processing. Conditional probabilities guide the choice of each tag/label in the sequence conflating the structured prediction task with the sequence classification task where different models provide different categorization of the same sequence. The claim of this paper is that CRF models also provide discriminative models to distinguish between types of sequence regardless of the accuracy of the labels obtained if we calibrate the class membership estimate of the sequence. We introduce and compare different neural network based linearchain CRFs and we present experiments on two complex sequence classification and structured prediction tasks to support this claim.", "creator": "LaTeX with hyperref package"}}}