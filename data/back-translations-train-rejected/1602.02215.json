{"id": "1602.02215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2016", "title": "Swivel: Improving Embeddings by Noticing What's Missing", "abstract": "We present Submatrix-wise Vector Embedding Learner (Swivel), a method for generating low-dimensional feature embeddings from a feature co-occurrence matrix. Swivel performs approximate factorization of the point-wise mutual information matrix via stochastic gradient descent. It uses a piecewise loss with special handling for unobserved co-occurrences, and thus makes use of all the information in the matrix. While this requires computation proportional to the size of the entire matrix, we make use of vectorized multiplication to process thousands of rows and columns at once to compute millions of predicted values. Furthermore, we partition the matrix into shards in order to parallelize the computation across many nodes. This approach results in more accurate embeddings than can be achieved with methods that consider only observed co-occurrences, and can scale to much larger corpora than can be handled with sampling methods.", "histories": [["v1", "Sat, 6 Feb 2016 04:39:41 GMT  (522kb,D)", "http://arxiv.org/abs/1602.02215v1", "9 pages, 4 figures"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["noam shazeer", "ryan doherty", "colin evans", "chris waterson"], "accepted": false, "id": "1602.02215"}, "pdf": {"name": "1602.02215.pdf", "metadata": {"source": "CRF", "title": "Swivel: Improving Embeddings by Noticing What\u2019s Missing", "authors": ["Noam Shazeer", "Ryan Doherty", "Colin Evans", "Chris Waterson"], "emails": ["NOAM@GOOGLE.COM", "PORTALFIRE@GOOGLE.COM", "COLINHEVANS@GOOGLE.COM", "WATERSON@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "Dense vector representations of words have proven useful for natural language tasks, such as determining semantic similarity, analysis, and translation. Recently, work by Mikolov et al. (2013a) and others has suggested an investigation into the construction of word vectors using stochastic lineage methods. Models tend to fall into one of two categories: matrix factorization or sliding sampling: Baroni et al. (2014) refers to these as \"counting\" and \"predicting methods,\" respectively. In this paper, we present the submatrix method Vector Embedding Learner (Swivel), a \"count-based\" method for generating low-dimensional embedding of features from a coexistence matrix. Panning uses stochastic lineage to perform a weighted safe matrix factorization that ultimately leads to embedding that reconstructs mutual information."}, {"heading": "2. Related Work", "text": "While there are a number of interesting approaches to creating word embedding, those by Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) are two relatively new approaches that can be evaluated in two ways. First, words with similar distribution contexts should be close to each other in order to enter the embedding space. Second, manipulation of the distribution context should lead directly to similar translations into the embedded space. \"Analogical\" Traversal of the vector space NS. \"ar Xiv: 160 2.02 215v 1 [cs.C L] 6F eb2 01Skipgram Negative Sampling."}, {"heading": "3. Swivel", "text": "Swivel is an attempt to have our cake and eat it. Like GloVe, it works on coexistence statistics rather than sampling; like SGNS, it takes advantage of the fact that many coexistences in the corpus are not observed. Like both, Swivel performs a weighted coexistence matrix between traits. Furthermore, Swivel is designed to work well in a distributed environment, e.g. Distbelief (Dean et al., 2012).At a high level, Swivel starts with an anm \u00d7 n coexistence matrix between m row and n column traits. Each row trait and column trait is assigned to a D-dimensional embedding vector. The vectors are grouped into blocks, each of which defines a submatrix. Training takes place by selecting a cullet (and thus its corresponding row block and column block)."}, {"heading": "3.1. Construction", "text": "To begin with, a m \u00b7 n co-occurrence matrix X is constructed, in which each cell XIJ in the matrix contains the observed frequency number of the row character i with the column character j. The limit number of each row character (xi \u0445 = \u2211 j xij) and each column character (x \u0445 j = \u2211 i xij) are calculated, as well as the total sum of all cells in the matrix, | D | = \u2211 i, j xij. As with other embedding methods, Swivel is agnostic for both the domain from which the characters are extracted and for the exact number of characters used. Furthermore, the \"feature vocabulary\" used for the rows does not necessarily have to be the same as that used for the column block. Rows are sorted in descending order of character frequency and are then summarized into k elements, with an efficient calculation based on the below being selected."}, {"heading": "3.2. Training", "text": "Prior to the training, the two-dimensional embedding is initialized with small random values. (2 W) Rm \u00b7 d is the matrix of embedding for the m-line characters (e.g. words), W) Rn \u00b7 d is the matrix of embedding for the n-column characters (e.g. word contexts). The training then proceeds iteratively as follows: A Shard Xij submatrix is randomly selected, along with the k-line embedding vectors Wi \"W from block i, and the k-column vectors W\" W \"W from column j.\" The matrix product WiW \"> j\" is calculated to produce the k2 predicted PMI values, which are then compared with the observed PMI values for Shard Xij.The error between the predicted and actual values is used to calculate the gradients: These are illustrated for each line and each column."}, {"heading": "3.3. Training Objective and Cost Function", "text": "\"It is not the way in which objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective, objective"}, {"heading": "4. Experiments", "text": "We have conducted several experiments to evaluate the embeddings provided by Swivel.Corpora. According to Pennington et al. (2014), we evaluated the 300-dimensional embeddings from an August 2015 to an August 2015. (2014), We have both focused on the different terms, and we have divided them into sentences. (2014), We have used the same vocabulary for all experiments, but the resulting training data contain 3.3 billion tokens over 89 million sentences. The most common terms have been used to produce the vocabulary, and the same vocabulary is used for all experiments. To ensure a careful comparison, we have embedded the publicly available words word2vec3 and GloVe4 programs as our baselines.word2vec has been configured to generate the embeddings with five negative examples."}, {"heading": "5. Discussion", "text": "The way in which they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they work in the way they"}, {"heading": "6. Conclusion", "text": "Swivel produces low-dimensional embedding of features from a coexistence matrix. It optimizes a goal very similar to that of SGNS and GloVe: The point product of a word embedding with context embedding should approximate the observed PMI of the two words in the corpus. In contrast to SGNS, Swivel's calculation requirements depend on the size of the coexistence matrix and not on the size of the corpus. This means that they can be applied to much larger companies. In contrast to GloVe, Swivel explicitly considers all coexistence information - including ignored occurrences - to produce embedding. In the case of unnoticed coexistence, a \"soft hinge loss\" prevents the PMI model from overestimating, resulting in demonstrably better embedding of rare features without sacrificing quality for common occurrences. Andrew McCalvel capitalizes on vectorized hardware and blocking this seam to avoid the high costs of overdrawing the structure."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnetbased approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Simlex999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "arXiv preprint arXiv:1408.3456,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Yifan Hu", "Yehuda Koren", "Chris Volinsky"], "venue": "In Data Mining,", "citeRegEx": "Hu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2008}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empirical Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A word at a time: computing word relatedness using temporal semantic analysis", "author": ["Kira Radinsky", "Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch"], "venue": "In Proceedings of the 20th international conference on World wide web,", "citeRegEx": "Radinsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Radinsky et al\\.", "year": 2011}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Using wiktionary for computing semantic relatedness", "author": ["Torsten Zesch", "Christof M\u00fcller", "Iryna Gurevych"], "venue": "In AAAI,", "citeRegEx": "Zesch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zesch et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 9, "context": "Recently, work by Mikolov et al. (2013a) and others has inspired an investigation into the construction of word vectors using stochastic gradient descent methods.", "startOffset": 18, "endOffset": 41}, {"referenceID": 9, "context": "Recently, work by Mikolov et al. (2013a) and others has inspired an investigation into the construction of word vectors using stochastic gradient descent methods. Models tend to fall into one of two categories: matrix factorization or sampling from a sliding window: Baroni et al. (2014) refers to these as \u201ccount\u201d and \u201cpredict\u201d methods, respectively.", "startOffset": 18, "endOffset": 288}, {"referenceID": 11, "context": ", 2013a) and GloVe (Pennington et al., 2014) are two relatively recent approaches that have received quite a bit of attention.", "startOffset": 19, "endOffset": 44}, {"referenceID": 9, "context": "The word2vec program released by Mikolov et al. (2013a) generates word embeddings by sliding a window over a large corpus of text.", "startOffset": 33, "endOffset": 56}, {"referenceID": 1, "context": ", distbelief (Dean et al., 2012).", "startOffset": 13, "endOffset": 32}, {"referenceID": 5, "context": "This latter similarity is reminiscent of weighted alternating least squares (Hu et al., 2008), which treats f(xij) as a confidence estimate that favors accurate estimation of certain parameters over uncertain ones.", "startOffset": 76, "endOffset": 93}, {"referenceID": 2, "context": "The gradient descent is dampened using Adagrad (Duchi et al., 2011), and the process repeats until the error no longer decreases appreciably.", "startOffset": 47, "endOffset": 67}, {"referenceID": 1, "context": "By storing the parameters in a central parameter server (Dean et al., 2012), it is possible to distribute training by processing several shards in parallel on different worker machines.", "startOffset": 56, "endOffset": 75}, {"referenceID": 13, "context": "We do this in a lock-free fashion (Recht et al., 2011) using Google\u2019s asynchronous stochastic gradient descent infrastructure distbelief (Dean et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 1, "context": ", 2011) using Google\u2019s asynchronous stochastic gradient descent infrastructure distbelief (Dean et al., 2012).", "startOffset": 90, "endOffset": 109}, {"referenceID": 11, "context": "Following Pennington et al. (2014), we pro-", "startOffset": 10, "endOffset": 35}, {"referenceID": 7, "context": "Performance of SGNS, GloVe, and Swivel vectors across different tasks with respect to methods tested by Levy et al. (2015),", "startOffset": 104, "endOffset": 123}, {"referenceID": 11, "context": "We experimented with adding word vector wi with its corresponding context vector w\u0303i (Pennington et al., 2014); however, best performance was achieved using the word vectorwi alone, as was originally reported by Mikolov et al.", "startOffset": 85, "endOffset": 110}, {"referenceID": 9, "context": ", 2014); however, best performance was achieved using the word vectorwi alone, as was originally reported by Mikolov et al. (2013a).", "startOffset": 109, "endOffset": 132}, {"referenceID": 11, "context": "(This was also noted by Pennington et al. (2014).)", "startOffset": 24, "endOffset": 49}, {"referenceID": 6, "context": "Following Levy and Goldberg\u2019s 2014 suggestion that SGNS factors a shifted PMI matrix, we experimented with shifting the objective PMI value by a small amount. Specifically, Levy and Goldberg (2014) suggest that the SGNS PMI objective is shifted by log k, where k is the number of negative samples drawn from the unigram distribution.", "startOffset": 10, "endOffset": 198}, {"referenceID": 3, "context": "For word similarity, we used WordSim353 (Finkelstein et al., 2001) partitioned into WordSim Similarity and WordSim Relatedness (Zesch et al.", "startOffset": 40, "endOffset": 66}, {"referenceID": 14, "context": ", 2001) partitioned into WordSim Similarity and WordSim Relatedness (Zesch et al., 2008; Agirre et al., 2009); Bruni et al.", "startOffset": 68, "endOffset": 109}, {"referenceID": 0, "context": ", 2001) partitioned into WordSim Similarity and WordSim Relatedness (Zesch et al., 2008; Agirre et al., 2009); Bruni et al.", "startOffset": 68, "endOffset": 109}, {"referenceID": 4, "context": "datasets that were used by Levy et al. (2015). For word similarity, we used WordSim353 (Finkelstein et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 7, "context": "As with Levy et al. (2015), we evaluated Swivel using the MSR and Google datasets (Mikolov et al.", "startOffset": 8, "endOffset": 27}], "year": 2016, "abstractText": "We present Submatrix-wise Vector Embedding Learner (Swivel), a method for generating lowdimensional feature embeddings from a feature co-occurrence matrix. Swivel performs approximate factorization of the point-wise mutual information matrix via stochastic gradient descent. It uses a piecewise loss with special handling for unobserved co-occurrences, and thus makes use of all the information in the matrix. While this requires computation proportional to the size of the entire matrix, we make use of vectorized multiplication to process thousands of rows and columns at once to compute millions of predicted values. Furthermore, we partition the matrix into shards in order to parallelize the computation across many nodes. This approach results in more accurate embeddings than can be achieved with methods that consider only observed cooccurrences, and can scale to much larger corpora than can be handled with sampling methods.", "creator": "LaTeX with hyperref package"}}}