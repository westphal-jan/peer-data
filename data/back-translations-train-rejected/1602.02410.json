{"id": "1602.02410", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2016", "title": "Exploring the Limits of Language Modeling", "abstract": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 24.2. We also release these models for the NLP and ML community to study and improve upon.", "histories": [["v1", "Sun, 7 Feb 2016 19:11:17 GMT  (76kb,D)", "http://arxiv.org/abs/1602.02410v1", null], ["v2", "Thu, 11 Feb 2016 23:01:48 GMT  (77kb,D)", "http://arxiv.org/abs/1602.02410v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rafal jozefowicz", "oriol vinyals", "mike schuster", "noam shazeer", "yonghui wu"], "accepted": false, "id": "1602.02410"}, "pdf": {"name": "1602.02410.pdf", "metadata": {"source": "META", "title": "Exploring the Limits of Language Modeling", "authors": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "emails": ["RAFALJ@GOOGLE.COM", "VINYALS@GOOGLE.COM", "SCHUSTER@GOOGLE.COM", "NOAM@GOOGLE.COM", "YONGHUI@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "In fact, models that are able to point to sentences that are grammatically correct but do not cause other tasks to play a key role in basic linguistic understandings such as answering questions, text translation, or summarizing texts have played a key role. LMs have played a key role in traditional NLP tasks such as speech recognition (Arisoy et al.), machine translation (Schwenk et al., 2012), and text summary (Rush et al., 2015)."}, {"heading": "2. Related Work", "text": "In this section, we describe previous work relevant to the approaches discussed in this paper. A more detailed discussion of language model research is included in Mikolov, 2012."}, {"heading": "2.1. Language Models", "text": "The goal of LM is to learn probability distribution through sequences of symbols that relate to a language. Much work has been done in both parametric (e.g., log-linear models) and non-parametric (e.g., count-based LMs) approaches. Number-based approaches (based on statistics from N programs) typically add smoothing that takes other parametric approaches into account (still possible) sequences and have been quite successful. To this extent, Kneser-Ney 5-gram models (Kneser & Ney, 1995) are relatively strong baselines that challenge other parametric approaches based on neural networks for large amounts of training data (Bengio et al, 2006). Most of our work is based on recursive neural networks (RNN, 2015) models that maintain long-term dependencies."}, {"heading": "2.2. Convolutional Embedding Models", "text": "The approach proposed in (Ling et al., 2015) builds word embedding using bidirectional LSTMs (Schuster & Paliwal, 1997; Graves & Schmidhuber, 2005) over the characters; the recurring networks process sequences of characters from both sides and their final state vectors; the resulting representation is then fed to a neural network, which achieves very good results on a sublinguistic tagging task.In (Kim et al., 2015) the word sequences from both sides and their final state vectors are linked, the resulting representation is then fed to a neural network.This model achieved very good results on a sublinguistic tagging task.In (Kim et al., 2015) the word sequences are processed by a 1-d CNN (Le Cun et al., 1990), with the sequence functions best represented for each TB level."}, {"heading": "2.3. Softmax Over Large Vocabularies", "text": "Assigning probability distributions across large vocabulary is a challenge in arithmetical terms. To model language, maximizing the log probability of a given word sequence leads to optimizing the transverse entropy between the target probability distribution (e.g. the target word we should predict) and our model predictions p. In general, predictions come from a linear layer followed by a soft max nonlinearity: p (w) = exp (zw); w \"word embedding\" for w.The biggest challenge when | V | is very large (in the order of one million in this essay) is the fact that the calculation of all internal products between h and all embedding Benno & Hieronymus (even if they exploit matrix) and the multiplication (in the order of one million in this essay), hypocalyps (scust), scust, and scust (2005), are the calculation of all internal products within the matrix itself and in the course of all interconnection with each other."}, {"heading": "3. Language Modeling Improvements", "text": "Recurring neural networks-based LMs use the chain rule to model joint probabilities using word sequences: p (w1,.., wN) = N \u0441i = 1 p (wi | w1,.., wi \u2212 1), where the context of all previous words is encoded with an LSTM and the probability of words uses a softmax (see Figure 1 (a)))."}, {"heading": "3.1. Relationship between Noise Contrastive Estimation and Importance Sampling", "text": "As discussed in Section 2.3, a large scale Softmax is necessary for the training of good LMs, because the vocabulary size = Y = =. A Hierarchical Softmax (Mnih & Hinton, 2009) uses a tree in which the probability distribution over words is broken down into a product of two probabilities for each word, which greatly reduces the training and consequence time, as only the path indicated by the hierarchy needs to be calculated and updated. Choosing a good hierarchy is important to achieve good results, and we have not explored this approach further, while sampling methods are good for our setups.Sampling approaches are only useful during training, as they suggest an approximation of the loss that is cheap to achieve a distributed setting - but the duration of normalization has yet to be calculated."}, {"heading": "3.2. CNN Softmax", "text": "Recent ef-forts on small language modeling have used CNN character embedding for input of embedding (Kim et al., 2015). Although this is not so simple, we propose an extension of this idea to also reduce the number of parameters of the Softmax layer. Recall from Section 2.3 that the Softmax computing a logit as zw = hT ew where h is a context vector and ew the word embedding. Instead of generating a matrix of | \u00d7 h | t | (whose lines correspond to ew) we produce ew with a CNN using the characters ofw = CNN (charsw) - we call this a CNN Softmax. We used the same network architecture to dynamically generate the Softmax word embedding without sharing the parameters with the input word embedding."}, {"heading": "3.3. Char LSTM Predictions", "text": "The CNN Softmax layer can handle arbitrary words and is much more efficient in terms of the number of parameters than the full Softmax matrix, but it is still considerably slow in assessing the confusion we need to calculate the partition function. A class of models that solve this problem more efficiently are character-level LSTMs (Sutskever et al., 2011; Graves, 2013), which make predictions from one character to another, thus allowing the calculation of probabilities over a much smaller vocabulary. On the other hand, these models are more difficult to learn and seem to be worse even on small tasks such as PTB (Graves, 2013). This is most likely due to the fact that the sequences become much longer on average, as the LSTM reads the input character by letter, rather than word by word. Therefore, we combine word and character-level models by feeding an LSTM vocabulary into a small LSTM state (see the target date)."}, {"heading": "4. Experiments", "text": "All experiments were carried out using the TensorFlow system (Abadi et al., 2015), with the exception of some older models used in the ensemble."}, {"heading": "4.1. Data Set", "text": "The experiments are conducted using the 1B Word Benchmark Dataset introduced by (Chelba et al., 2013), a publicly available measure of the progress of statistical language modeling. The dataset contains about 0.8B words with a vocabulary of 793471 words, including sentence boundary markers. All sentences are shuffled and duplicates removed. Words not included in vocabulary (OOV) are marked with a special UNK symbol (there are about 0.3% of such words)."}, {"heading": "4.2. Model Setup", "text": "The typical measure of reporting progress in language modeling is perplexity, which is the average probability per word log on the holdout record: e \u2212 1 N \u2211 i ln pwi. We follow the standard procedure and add up over all words (including the end-of-sentence symbol). We used the 1B Word benchmark record without pre-processing. Given the mixed sentences, they are entered into the network as a stack of independent word streams. Whenever a sentence ends, a new sentence begins without padding (maximizing occupancy per stack). In models that use characters as input or as target, each word is fed to the model as a sequence of character IDs of predetermined length (see Figure 1 (b). Words have been edited to include special start and end effects per stack and have been padded to reach the expected length. That is, if the maximum word length was 10, the word would be \"converted to digits.\""}, {"heading": "4.3. Model Architecture", "text": "We examined many variations of RNN LM architectures, including the dimensions of the embedding layers, the state, projection sizes, and the number of LSTM layers to be used. Extensive testing of all combinations would be extremely time-consuming for such a large dataset, but our results suggest that LSTMs with a projection layer (i.e. a bottleneck between hidden states such as in (Sak et al., 2014), trained with truncated BPTT (Williams & Peng, 1990) in 20 well-executed steps, with suspensions (Srivastava et al., 2014) before and after each LSTM layer. The distortions of the LSTM format were initialized to 1.0 (Jozefowicz et al., 2015). The size of the models is described in more detail in the following sections, and the selection of hyperparameters will be released after publication."}, {"heading": "4.4. Training Procedure", "text": "The models were practiced to convergence with an AdaGrad optimizer with a learning rate of 0.2. In all experiments, the RNNs were unrolled for 20 steps without ever resetting the LSTM states. We used a batch size of 128. We cut the gradients of the LSTM weights so that their norm is limited to 1.0. Using these hyperparameters, we found that large LSTMs were relatively easy to train. The same learning rate was used in almost all experiments. In some cases, we had to reduce them by an order of magnitude. Unless otherwise specified, the experiments were performed with 32 GPU workers and asynchronous gradient updates. Further details will be specified in full with the code upon release."}, {"heading": "5. Results and Analysis", "text": "In this section, we summarize the results of our experiments and conduct an in-depth analysis. Table 1 contains all the results for our models compared to previously published work. Table 2 shows past and our own work on model ensembles. We hope that our encouraging results, which improved the best confusion of a single model from 51.3 to 30.0 (with a significant reduction in model size) and set a new record of 24.2 ensembles, will enable rapid research and progress to advance Language Modeling. To this end, we will publish the model weights and recipes after publication."}, {"heading": "5.1. Size Matters", "text": "Therefore, the size of the LSTM layer is a very important factor influencing the results, as shown in Table 1. The best models are the largest we could build into a GPU memory. Our largest model was a 2-layer LSTM with 8192 + 1024 dimensional repeat state in each of the layers. Increasing the embedding and projection size also helps, but causes a large increase in the number of parameters that is less desirable. Finally, the formation of an RNN instead of an LSTM yields worse results (about 5 percent worse perplexity) for a comparable model size."}, {"heading": "5.2. Regularization Importance", "text": "As shown in Table 1, the use of dropouts improves results. To our surprise, even relatively small models (e.g. single-layer 2048-unit LSTM projected to 512-dimensional results) may miss out on the training set if trained long enough, ultimately leading to holdout degradation.The use of dropouts on non-recurring connections largely mitigates these problems. Although overadjustments still occur, there is no need for early cancellation. For models with 4096 units or less in the LSTM shift, we used 10% failure probability. For larger models, 25% were significantly better. Even with such regulation, confusion on the training set can be up to 6 points below the test value. In one experiment, we tried to use a smaller vocabulary from the 100,000 most common words and found that the difference between train and test is smaller - suggesting that too much capacity is given to rare words."}, {"heading": "5.3. Importance Sampling is Data Efficient", "text": "Table 3 shows the test perplexity of the NCE compared to the IS loss after some epochs of the 2048 unit LSTM with 512 projection. The IS target significantly improves the speed and overall performance of the model compared to the NCE."}, {"heading": "5.4. Word Embeddings vs Character CNN", "text": "Replacing the embedding layer with a parameterized neural network that processes the characters of a particular word, the model can consume any word and is not limited to a fixed vocabulary. This feature is useful for conversation or informal text records, as well as for morphologically rich languages. Our experiments show that the use of embedding at the character level is feasible and does not affect performance - in fact, our best single model uses embedding with CNN characters. An additional advantage is that the number of parameters of the input layer is reduced by a factor of 11 (although the training speed is slightly worse). As an inference, embedding can be pre-calculated so that there is no speed loss. Overall, the embedding of the best model is parameterized by 72M weights (from 820M weights).Table 4 shows some examples of nearest embedding for some words outside the word layer when using CNS."}, {"heading": "5.5. Smaller Models with CNN Softmax", "text": "Most of the parameters are in the linear plane before Softmax: 820M versus a total of 1,04B parameters. In one of the experiments, we froze the word LSTM after convergence and replaced the Softmax layer with the CNN Softmax subnetwork. Without fine tuning, this model could achieve 39.8 perplexity with only 293M weights (as shown in Table 1). As described in Section 3.2, embedding a word \"Correction\" reduces the gap between regular and CNN Softmax. In fact, we can replace the model size with perplexity by, for example, adding 100M weights (through a 128-dimensional bottleneck embedding), we achieve 35.8 perplexity (see Table 1).Unlike CNN Softmax, we have a model that replaces the softbase with a comparison layer that does not work on one plane (4M)."}, {"heading": "5.6. Training Speed", "text": "The smaller version of the LSTM model with 2048 units and 512 projections takes less than 10 hours to get below 45 perplexity, and after just 2 hours of training, the model beats the previous state of the art on this dataset. The best model takes about 5 days to reach 35 perplexity and 10 days to 32.5. Best results were achieved after 3 weeks of training. See Table 3 for more details."}, {"heading": "5.7. Ensembles", "text": "We have examined several of our best models and achieved 24.2 test perplexity (see Table 2 for more details and results), an improvement of over 40% over previous work. Interestingly, including the best N-gram model reduces perplexity by 1.2 points, although the model itself is quite weak (67.6 perplexity). Most previous work has either had to work with the best N-gram model (as their RNN used a limited vocabulary of a few thousand words) or use N-gram features as an additional input to the RNN. On the contrary, our results suggest that N-gram is of limited use and suggest that a carefully trained LSTM-LM is the most competitive model."}, {"heading": "5.8. LSTMs are best on the tail words", "text": "Figure 2 shows the difference in log probabilities between our best model (at 30.0 perplexity) and the KN-5. As shown in the chart, LSTM is better across all groups and performs significantly better on rare words than KN-5. This is encouraging as it seems to suggest that LSTM LMs could fare even better for languages or datasets where the number of rare words is greater than traditional N-gram models."}, {"heading": "5.9. Samples from the model", "text": "To evaluate the model qualitatively, we sampled many phrases: We discarded short and politically incorrect ones, but the sample shown below is otherwise \"raw\" (i.e. not hand-picked).The samples are of high quality - which is no surprise given the perplexity achieved - but there are occasional errors. Sentences generated by the ensemble (about 26 perplexity): < S > Given that in the last three years, more and more new technologies have come onto the market quickly, more and more companies are now having to confront the ever-changing and ever-changing environmental challenges online. < S > Check for updates on this breaking news. < S > About 800 people gathered from noon to 2 p.m. at Hever Castle on Long Beach, three to four times as many as in the burial certificate. < S > We are aware of the legal owner's written instructions not to mention Rosenberg's negative comments in any way if they are relevant, as stated in the documents, < S > eBay said in a statement."}, {"heading": "6. Discussion and Conclusions", "text": "The reduction in perplexity from 51.3 to 30.0 is due to several key components that we have examined in this paper. Therefore, a large, regulated LSTM with projection layers and an approximation to the true Softmax with importance tests performs much better than N-grams. Unlike previous work, we do not need to interpolate both the RNN LM and the N-gram, and the gains are marginal. By studying recent advances in model architectures (e.g. LSTMs), the use of lower-case CNNs, and by sharing our findings in this paper and related codes and models (which will be published after publication), we hope to stimulate research into large-scale language models, a problem that we consider crucial for language understanding."}, {"heading": "Acknowledgements", "text": "We thank Ciprian Chelba, Ilya Sutskever and the Google Brain team for their help and discussions and Koray Kavukcuoglu for his help with the manuscript."}], "references": [{"title": "TensorFlow: Large-scale machine learning", "author": ["Fernanda", "Vinyals", "Oriol", "Warden", "Pete", "Wattenberg", "Martin", "Wicke", "Yu", "Yuan", "Zheng", "Xiaoqiang"], "venue": "on heterogeneous systems,", "citeRegEx": "Fernanda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fernanda et al\\.", "year": 2015}, {"title": "Deep neural network language models", "author": ["Arisoy", "Ebru", "Sainath", "Tara N", "Kingsbury", "Brian", "Ramabhadran", "Bhuvana"], "venue": "In Proceedings of the NAACL-HLT", "citeRegEx": "Arisoy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arisoy et al\\.", "year": 2012}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Ballesteros", "Miguel", "Dyer", "Chris", "Smith", "Noah A"], "venue": "arXiv preprint arXiv:1508.00657,", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Bengio", "Yoshua", "Sen\u00e9cal", "Jean-S\u00e9bastien"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2008}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Bengio", "Yoshua", "Sen\u00e9cal", "Jean-S\u00e9bastien"], "venue": "In AISTATS,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Neural probabilistic language models", "author": ["Bengio", "Yoshua", "Schwenk", "Holger", "Sen\u00e9cal", "JeanS\u00e9bastien", "Morin", "Fr\u00e9deric", "Gauvain", "Jean-Luc"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Chelba", "Ciprian", "Mikolov", "Tomas", "Schuster", "Mike", "Ge", "Qi", "Brants", "Thorsten", "Koehn", "Phillipp", "Robinson", "Tony"], "venue": "arXiv preprint arXiv:1312.3005,", "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Sentence compression by deletion with lstms", "author": ["Filippova", "Katja", "Alfonseca", "Enrique", "Colmenares", "Carlos A", "Kaiser", "Lukasz", "Vinyals", "Oriol"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers", "Felix A", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Multilingual language processing from bytes", "author": ["Gillick", "Dan", "Brunk", "Cliff", "Vinyals", "Oriol", "Subramanya", "Amarnag"], "venue": "arXiv preprint arXiv:1512.00103,", "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Gutmann", "Michael", "Hyv\u00e4rinen", "Aapo"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Gutmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2010}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Document context language models", "author": ["Ji", "Yangfeng", "Cohn", "Trevor", "Kong", "Lingpeng", "Dyer", "Chris", "Eisenstein", "Jacob"], "venue": "arXiv preprint arXiv:1511.03962,", "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Jozefowicz", "Rafal", "Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Kim", "Yoon", "Jernite", "Yacine", "Sontag", "David", "Rush", "Alexander M"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Improved backingoff for m-gram language modeling", "author": ["Kneser", "Reinhard", "Ney", "Hermann"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Le Cun", "B Boser", "Denker", "John S", "D Henderson", "Howard", "Richard E", "W Hubbard", "Jackel", "Lawrence D"], "venue": "In Advances in neural information processing systems. Citeseer,", "citeRegEx": "Cun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Cun et al\\.", "year": 1990}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling", "Wang", "Lu\u0131\u0301s", "Tiago", "Marujo", "Astudillo", "Ram\u00f3n Fernandez", "Amir", "Silvio", "Dyer", "Chris", "Black", "Alan W", "Trancoso", "Isabel"], "venue": "arXiv preprint arXiv:1508.02096,", "citeRegEx": "Ling et al\\.,? \\Q2096\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2096}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong", "Minh-Thang", "Sutskever", "Ilya", "Le", "Quoc V", "Vinyals", "Oriol", "Zaremba", "Wojciech"], "venue": "arXiv preprint arXiv:1410.8206,", "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Statistical language models based on neural networks. Presentation at Google", "author": ["Mikolov", "Tom\u00e1\u0161"], "venue": "Mountain View,", "citeRegEx": "Mikolov and Tom\u00e1\u0161.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tom\u00e1\u0161.", "year": 2012}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Andriy", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Mnih", "Andriy", "Kavukcuoglu", "Koray"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In Aistats,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Rush", "Alexander M", "Chopra", "Sumit", "Weston", "Jason"], "venue": "arXiv preprint arXiv:1509.00685,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Sak", "Hasim", "Senior", "Andrew W", "Beaufays", "Fran\u00e7oise"], "venue": "In INTERSPEECH,", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Mike", "Paliwal", "Kuldip K"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Large, pruned or continuous space language models on a gpu for statistical machine translation", "author": ["Schwenk", "Holger", "Rousseau", "Anthony", "Attik", "Mohammed"], "venue": "In Proceedings of the NAACL-HLT", "citeRegEx": "Schwenk et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2012}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Serban", "Iulian Vlad", "Sordoni", "Alessandro", "Bengio", "Yoshua", "Courville", "Aaron C", "Pineau", "Joelle"], "venue": "CoRR, abs/1507.04808,", "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Sparse non-negative matrix language modeling for skipgrams", "author": ["Shazeer", "Noam", "Pelemans", "Joris", "Chelba", "Ciprian"], "venue": "Proceedings of Interspeech,", "citeRegEx": "Shazeer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shazeer et al\\.", "year": 2015}, {"title": "Improving neural networks with dropout", "author": ["Srivastava", "Nitish"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "Srivastava and Nitish.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava and Nitish.", "year": 2013}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Srivastava", "Nitish", "Mansimov", "Elman", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1502.04681,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["Srivastava", "Rupesh K", "Greff", "Klaus", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Efficient exact gradient update for training deep networks with very large sparse targets", "author": ["Vincent", "Pascal", "de Br\u00e9bisson", "Alexandre", "Bouthillier", "Xavier"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vincent et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["Vinyals", "Oriol", "Le", "Quoc"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Larger-context language modelling", "author": ["Wang", "Tian", "Cho", "Kyunghyun"], "venue": "arXiv preprint arXiv:1511.03729,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "An efficient gradientbased algorithm for on-line training of recurrent network trajectories", "author": ["Williams", "Ronald J", "Peng", "Jing"], "venue": "Neural computation,", "citeRegEx": "Williams et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1990}, {"title": "Scaling recurrent neural network language models", "author": ["Williams", "Will", "Prasad", "Niranjani", "Mrva", "David", "Ash", "Tom", "Robinson", "Tony"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Williams et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "LMs have played a key role in traditional NLP tasks such as speech recognition (Arisoy et al., 2012), machine translation (Schwenk et al.", "startOffset": 79, "endOffset": 100}, {"referenceID": 29, "context": "), or text summarization (Rush et al., 2015; Filippova et al., 2015).", "startOffset": 25, "endOffset": 68}, {"referenceID": 9, "context": "), or text summarization (Rush et al., 2015; Filippova et al., 2015).", "startOffset": 25, "endOffset": 68}, {"referenceID": 33, "context": "For example, when trained on movie subtitles (Serban et al., 2015; Vinyals & Le, 2015), these language models are able to generate basic answers to questions about object colors, facts about people, etc.", "startOffset": 45, "endOffset": 86}, {"referenceID": 39, "context": "Lastly, recently proposed sequence-to-sequence models employ conditional language models as their key component to solve diverse tasks like machine translation (Sutskever et al., 2014; Cho et al., 2014) or video generation (Srivastava et al.", "startOffset": 160, "endOffset": 202}, {"referenceID": 7, "context": "Lastly, recently proposed sequence-to-sequence models employ conditional language models as their key component to solve diverse tasks like machine translation (Sutskever et al., 2014; Cho et al., 2014) or video generation (Srivastava et al.", "startOffset": 160, "endOffset": 202}, {"referenceID": 6, "context": "Indeed, most recent work on large scale LM has shown that RNNs are great in combination with N-grams, as they may have different strengths that complement N-gram models, but worse when considered in isolation (Chelba et al., 2013; Williams et al., 2015; Ji et al., 2015a; Shazeer et al., 2015).", "startOffset": 209, "endOffset": 293}, {"referenceID": 44, "context": "Indeed, most recent work on large scale LM has shown that RNNs are great in combination with N-grams, as they may have different strengths that complement N-gram models, but worse when considered in isolation (Chelba et al., 2013; Williams et al., 2015; Ji et al., 2015a; Shazeer et al., 2015).", "startOffset": 209, "endOffset": 293}, {"referenceID": 34, "context": "Indeed, most recent work on large scale LM has shown that RNNs are great in combination with N-grams, as they may have different strengths that complement N-gram models, but worse when considered in isolation (Chelba et al., 2013; Williams et al., 2015; Ji et al., 2015a; Shazeer et al., 2015).", "startOffset": 209, "endOffset": 293}, {"referenceID": 24, "context": "We believe that, despite much work being devoted to small data sets like the Penn Tree Bank (PTB) (Marcus et al., 1993), research on larger tasks is very relevant as overfitting is not the main limitation in current language modeling, but is the main characteristic of the PTB task.", "startOffset": 98, "endOffset": 119}, {"referenceID": 6, "context": "We focused on a well known, large scale LM benchmark: the One Billion Word Benchmark data set (Chelba et al., 2013).", "startOffset": 94, "endOffset": 115}, {"referenceID": 8, "context": "Similar to Imagenet (Deng et al., 2009), which helped advance computer vision, we believe that releasing and working on large data sets and models with clear benchmarks will help advance Language Modeling.", "startOffset": 20, "endOffset": 39}, {"referenceID": 5, "context": "To this extent, Kneser-Ney smoothed 5-gram models (Kneser & Ney, 1995) are a fairly strong baseline which, for large amounts of training data, have challenged other parametric approaches based on Neural Networks (Bengio et al., 2006).", "startOffset": 212, "endOffset": 233}, {"referenceID": 10, "context": "To this extent, we used the Long-Short Term Memory model (Hochreiter & Schmidhuber, 1997) which uses a gating mechanism (Gers et al., 2000) to ensure proper propagation of information through many time steps.", "startOffset": 120, "endOffset": 139}, {"referenceID": 6, "context": "Much work has been done on small and large scale RNN-based LMs (Mikolov, 2012; Chelba et al., 2013; Zaremba et al., 2014; Williams et al., 2015; Ji et al., 2015a; Wang & Cho, 2015; Ji et al., 2015b).", "startOffset": 63, "endOffset": 198}, {"referenceID": 45, "context": "Much work has been done on small and large scale RNN-based LMs (Mikolov, 2012; Chelba et al., 2013; Zaremba et al., 2014; Williams et al., 2015; Ji et al., 2015a; Wang & Cho, 2015; Ji et al., 2015b).", "startOffset": 63, "endOffset": 198}, {"referenceID": 44, "context": "Much work has been done on small and large scale RNN-based LMs (Mikolov, 2012; Chelba et al., 2013; Zaremba et al., 2014; Williams et al., 2015; Ji et al., 2015a; Wang & Cho, 2015; Ji et al., 2015b).", "startOffset": 63, "endOffset": 198}, {"referenceID": 8, "context": "A clear example found in computer vision is Imagenet (Deng et al., 2009), which enabled learning complex vision models from large amounts of data (Krizhevsky et al.", "startOffset": 53, "endOffset": 72}, {"referenceID": 20, "context": ", 2009), which enabled learning complex vision models from large amounts of data (Krizhevsky et al., 2012).", "startOffset": 81, "endOffset": 106}, {"referenceID": 30, "context": "Despite the large number of parameters, we try to minimize computation as much as possible by adopting a strategy proposed in (Sak et al., 2014) of projecting a relatively big recurrent state space down so that the matrices involved remain relatively small, yet the model has large memory capacity.", "startOffset": 126, "endOffset": 144}, {"referenceID": 18, "context": "There is an increased interest in incorporating characterlevel inputs to build word embeddings for various NLP problems, including part-of-speech tagging, parsing and language modeling (Ling et al., 2015; Kim et al., 2015; Ballesteros et al., 2015).", "startOffset": 185, "endOffset": 248}, {"referenceID": 2, "context": "There is an increased interest in incorporating characterlevel inputs to build word embeddings for various NLP problems, including part-of-speech tagging, parsing and language modeling (Ling et al., 2015; Kim et al., 2015; Ballesteros et al., 2015).", "startOffset": 185, "endOffset": 248}, {"referenceID": 18, "context": "In (Kim et al., 2015), the words characters are processed by a 1-d CNN (Le Cun et al.", "startOffset": 3, "endOffset": 21}, {"referenceID": 4, "context": "Several approaches have been proposed to cope with the scaling issue: importance sampling (Bengio et al., 2003; Bengio & Sen\u00e9cal, 2008), Negative Contrast Estimation (NCE) (Gutmann & Hyv\u00e4rinen, 2010; Mnih & Kavukcuoglu, 2013), self normalizing partition functions (Vincent et al.", "startOffset": 90, "endOffset": 135}, {"referenceID": 40, "context": ", 2003; Bengio & Sen\u00e9cal, 2008), Negative Contrast Estimation (NCE) (Gutmann & Hyv\u00e4rinen, 2010; Mnih & Kavukcuoglu, 2013), self normalizing partition functions (Vincent et al., 2015) or Hierarchical Softmax (Morin & Bengio, 2005; Mnih & Hinton, 2009) \u2013 they all offer good solutions to this problem.", "startOffset": 160, "endOffset": 182}, {"referenceID": 18, "context": "Recent efforts on small scale language modeling have used CNN character embeddings for the input embeddings (Kim et al., 2015).", "startOffset": 108, "endOffset": 126}, {"referenceID": 23, "context": "This may be useful for other problems such as Machine Translation where handling out-of-vocabulary words is very important (Luong et al., 2014).", "startOffset": 123, "endOffset": 143}, {"referenceID": 11, "context": "This has shown to help when using byte-level input embeddings for named entity recognition (Gillick et al., 2015), and we hope it will enable similar gains when used to map onto words.", "startOffset": 91, "endOffset": 113}, {"referenceID": 38, "context": "A class of models that solve this problem more efficiently are character-level LSTMs (Sutskever et al., 2011; Graves, 2013).", "startOffset": 85, "endOffset": 123}, {"referenceID": 6, "context": "The experiments are performed on the 1B Word Benchmark data set introduced by (Chelba et al., 2013), which is a publicly available benchmark for measuring progress of statistical language modeling.", "startOffset": 78, "endOffset": 99}, {"referenceID": 30, "context": ", a bottleneck between hidden states as in (Sak et al., 2014)) trained with truncated BPTT (Williams & Peng, 1990) for 20 steps performed well.", "startOffset": 43, "endOffset": 61}, {"referenceID": 45, "context": "Following (Zaremba et al., 2014) we use dropout (Srivastava, 2013) before and after every LSTM layer.", "startOffset": 10, "endOffset": 32}, {"referenceID": 17, "context": "0 (Jozefowicz et al., 2015).", "startOffset": 2, "endOffset": 27}, {"referenceID": 18, "context": "For any model using character embedding CNNs, we closely follow the architecture from (Kim et al., 2015).", "startOffset": 86, "endOffset": 104}, {"referenceID": 8, "context": "We hope for future research to focus on reasonably sized datasets taking inspiration from recent advances seen in the computer vision community thanks to efforts such as Imagenet (Deng et al., 2009).", "startOffset": 179, "endOffset": 198}], "year": 2016, "abstractText": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 24.2. We also release these models for the NLP and ML community to study and improve upon.", "creator": "LaTeX with hyperref package"}}}