{"id": "1602.02543", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Homogeneity of Cluster Ensembles", "abstract": "The expectation and the mean of partitions generated by a cluster ensemble are not unique in general. This issue poses challenges in statistical inference and cluster stability. In this contribution, we state sufficient conditions for uniqueness of expectation and mean. The proposed conditions show that a unique mean is neither exceptional nor generic. To cope with this issue, we introduce homogeneity as a measure of how likely is a unique mean for a sample of partitions. We show that homogeneity is related to cluster stability. This result points to a possible conflict between cluster stability and diversity in consensus clustering. To assess homogeneity in a practical setting, we propose an efficient way to compute a lower bound of homogeneity. Empirical results using the k-means algorithm suggest that uniqueness of the mean partition is not exceptional for real-world data. Moreover, for samples of high homogeneity, uniqueness can be enforced by increasing the number of data points or by removing outlier partitions. In a broader context, this contribution can be placed as a further step towards a statistical theory of partitions.", "histories": [["v1", "Mon, 8 Feb 2016 12:28:57 GMT  (1486kb,D)", "http://arxiv.org/abs/1602.02543v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["brijnesh j jain"], "accepted": false, "id": "1602.02543"}, "pdf": {"name": "1602.02543.pdf", "metadata": {"source": "CRF", "title": "Homogeneity of Cluster Ensembles", "authors": ["Brijnesh J. Jain"], "emails": ["brijnesh.jain@gmail.com"], "sections": [{"heading": null, "text": "Homogeneity of the EnsemblesBrijnesh J. Jain Technical University Berlin, Germany E-mail: brijnesh.jain @ gmail.com The expectation and mean of the partitions generated by a cluster ensemble are generally not unique. This problem poses statistical inferences and cluster stability challenges. In this article, we provide sufficient conditions for the uniqueness of expectation and mean. The proposed conditions show that a unique mean is neither exceptional nor general. To address this problem, we present homogeneity as a measure of how likely a unique mean is for a sample of partitions. We show that homogeneity is related to cluster stability. This result points to a possible conflict between cluster stability and diversity in consensus clusters. To assess homogeneity in a practical environment, we propose an efficient method to calculate a lower limit of homogeneity. Empty results using the mean algorithm suggest that the uniformity is not exceptional."}, {"heading": "1. Introduction 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2. Partition Spaces 4", "text": "2.1. Partitions.............................................................................................................................."}, {"heading": "3. Homogeneity of a Sample 6", "text": "3.1. Fre'chet functions.........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4. Experiments 13", "text": "4.1. Experiments on synthetic data...................... 134.1.1. Results on G4 datasets....................... 14. 4.1.2. Results on U-shapes and Gaussians................ 184.2. Experiments on UCI datasets.............."}, {"heading": "5. Conclusion 21", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Preliminaries 22", "text": "A.1. Notations......................................................................................................................."}, {"heading": "B. Proofs 24", "text": "B.1. Evidence of theorem 3.1................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they have set themselves in order to play by the rules."}, {"heading": "2. Partition Spaces", "text": "To analyze partitions, we propose a geometric representation proposed in [13]. We first show that a partition can be considered as a point in a geometric space, the so-called orbit space. Orbit spaces are well researched, have a rich geometric structure and have a natural connection to Euclidean spaces [2, 12, 17]. Then, we give the orbit spaces P of partitions a distance function that is related to Euclidean metric \u043c, so that (P, \u03b4) becomes a geodetic metric space."}, {"heading": "2.1. Partitions", "text": "A partition X of Z with \"clusters C1,..., C\" is specified by a matrix X [0, 1] \"\u00b7 m so that XT1\" = 1 m, where 1 \"\" R \"and 1\" Rm are vectors of all ones. Lines xk: of the matrix X refer to the cluster Ck of partition X. The columns x: j of X refer to the data points zj. The elements xkj of the matrix X = (xkj) represent the degree of affiliation of all partitions zj to the cluster Ck. The constraint XT1 '= 1 m requires that the affiliation values x: j of the data point zj must sum up to 1 across all clusters. With P, we designate the set of all partitions with \"clusters above m data points.\" Since some clusters may be empty, the set P, \"m also contains partitions with less than\" P \"and the number of the matrix that we consider\" exactly \"m.\""}, {"heading": "2.2. Orbit Spaces", "text": "We define the representation space X of the set P = P ', m of the partitions byX = {X] \"[0, 1]\" \u00b7 m: XT1' = 1 '. \"Then we have a natural projection \u03c0: X \u2192 P, X 7 \u2192 X = \u03c0 (X), which sends matrices X to the partitions X that they represent. \u03c0 gives two properties: (1) Each partition can be represented by at least one matrix, and (2) a partition can have multiple matrix representations. Suppose that matrix X is a partition X-P. The subset of all matrices that X represents forms an equivalence class [X], which can be achieved by performing the rows of matrix X in all possible ways."}, {"heading": "2.3. Intrinsic Metric", "text": "The Euclidean norm for matrices X and X is defined by the formula \"X\" = \"Q\" = \"K = 1 m, J = 1 | xkj | 2 1 / 2.\" The Euclidean norm induces a distance on P of the formula \"P\": P \u00b7 P \u2192 R, (X, Y) 7 \u2192 min \"(X-Y: X-X, Y-Y\").Then the pair (P, \u03b4) is a geodesic metric space [13], theorem 2.1."}, {"heading": "3. Homogeneity of a Sample", "text": "Based on these conditions, we examine the probability of a unique mean division. To this end, we propose the homogeneity of a sample as a measure of how close a sample comes to a unique mean. We provide an efficient method for calculating a lower limit of homogeneity. Finally, this section on homogeneity refers to cluster stability and points to potentially conflicting approaches to cluster formation: stability and diversity in consensus clustering."}, {"heading": "3.1. Fre\u0301chet Functions", "text": "In this section, we link the consensus function of the average partition approach to the Fre \ufffd chet functions [8]. This link provides access to many results from statistics in non-Euclidean spaces [1]. Let (P, \u03b4) be a partition space equipped with the metric \u03b4 induced by the Euclidean norm. We assume that Q is a probability measure for P with SQ.1 support. Any partition M \u00b2 P that minimizes FQ is an expected partition. We say Q is homogeneous if the expected partition of Q is unique. Otherwise, Q is referred to as a minimum of FQ, but is generally not unique [13]. Any partition M \u00b2 P that minimizes FQ is an expected partition."}, {"heading": "3.2. Conditions of Uniqueness", "text": "In this section we show that the expected and mean division is unique if the partitions to be grasped are contained in a sufficiently small sphere.1The support of Q is the smallest closed subset SQ P, so that Q (SQ) = 1.The sphere B (Z, r) with center Z-P and radius r is a group of the formB (Z, r) = {X-P: \u03b4 (X, Z) \u2264 r}.We call the sphere B (Z, r) homogeneous if there is a bijective isometry: B (Z, r) \u2212 \u2192 B (Z, r), where B (Z, r) the sphere in euclidean space X is centered in the diagram Z-Z. The definition of homogeneous sphere is independent of the choice of representation, since two spheres in X are isometric at different centers, but identical radii r radii. The maximum homogeneity (max-hom) radius Z is the greatest expectation for the existence of Q = 3.0, where S1 is uniformity."}, {"heading": "3.3. Asymmetric Partitions", "text": "s show the subset of (\") permutation matrices without identity matrix. If the division matrix Z > 0, the division matrix Z = 0, the division matrix Z = 0, the division matrix Z = 0, the division matrix Z = 0, the division matrix Z = 0, the division matrix Z = 0, the division matrix Z = 0, the division matrix Z = 0, the division matrix Z = 0, the division matrix Z = 0."}, {"heading": "3.4. Homogeneity", "text": "In this section, we present homogeneity as a measure of how close a sample comes to a unique mean and has a lower limit that can be easily determined. Suppose Sn = (X1,.., Xn) is a sample of n partitions. By H (Sn) we mean the set of all homogeneous subsamples of Sn, which is the set of all subsamples of Sn with a unique middle partition. Obviously, H (Sn) is not empty, because subsamples consisting of a singleton are homogeneous. If Sn is homogeneous, then H (Sn) coincides with the power set of Sn. The homogeneity of a sample Sn is measured by H (Sn) = max {S | n: S \u00b2 H (Sn)}.Homogeneity measures how close a sample is to the homogeneity of Sn."}, {"heading": "3.5. Clustering Stability", "text": "This section links cluster instability with consensus clustering and outlines how homogeneity relates to cluster stability in a simplified environment. Finally, we briefly point to a potential conflict between cluster stability and diversity in consensus clustering."}, {"heading": "3.5.1. Clustering Instability", "text": "The choice of cluster number is a persistent problem of cluster selection. One way to \"select\" is based on the concept of cluster stability. The intuitive idea behind cluster stability is that a cluster algorithm should create similar partitions when it is repeatedly applied to slightly different datasets of the same underlying distribution. In this context, we assume that Sn, k = (X1,..., Xn) is a sample of n partitions Xi, m of (possibly different) datasets of size m with k clusters. Following [16], cluster stability model selection is a problem of minimizing the function, k = 1n2 n \u2211 i = 1 n \u0445j = 1 \u0445k (Xi, Xj) over all numbers k of clusters in such a way that 1 \u2264 kmin \u2264 kmax \u2264 m. An option for cluster stability selection is then as follows: \"= arg min k In, k. The function, In cluster measurement instability is called the distance between the average and the common instability of the cluster.\""}, {"heading": "3.5.2. Homogeneity vs. Stability", "text": "Intuitively, we expect the average pair distance Ik, m between partitions and the average distance Fn, k (Mk) to a middle partition to be correlated if the underlying distance function \u2206 k behaves well. If \u2206 k is a metric, we have (see Section B.5) Fn, k (Mk) \u2264 In, k, (1) where Mk is a middle or medoid partition of the sample Sn, k. These considerations suggest that the variation Fn, k (Mk) can serve as an alternative score function for model selection related to cluster instability. In k, we select the number \"of clusters according to the rule\" = arg min k Fn, k (Mk). To correlate homogeneity with cluster stability, we consider the (non-symmetric) distance k (Xi, Xj) = 1 \u2212 IAi (Xj).The Fre-chet function of Sn assumes cluster stability."}, {"heading": "3.5.3. Stability vs. Diversity", "text": "In consensus clusters, it is recommended to use different partitions to improve performance [5, 22, 23]. Following [5], diversity is measured in the same way as cluster stability, namely by the sum of the paired distances between the sample partitions. Although diversity corresponds to cluster instability, its application in consensus clusters does not contradict the goal of cluster stability per se. In order to do justice to cluster stability, the question is whether the resulting consensus partitions are stable. Since diversity corresponds to low homogeneity, it is unlikely that a sample of different partitions has a unique mean value. To fulfill cluster stability, the question under which conditions are two different mean partitions similar? To answer this question, we need the following result: Theory 3.6. LetM-P is a mean partition of the sample Sn = (X1,., Xn)."}, {"heading": "4. Experiments", "text": "The aim of this section is to assess the homogeneity of samples obtained by k-means applied to synthetic and real data."}, {"heading": "4.1. Experiments on Synthetic Data", "text": "We created the following dataset types in R2: 1. UD: Uniform Distribution2 = Uniform Distribution2 = Uniform Distribution2. G4: Four Gaussians3. G9: Nine Gaussians4. U2: Two U-Shapes5. U4: Four U-ShapesThe UD datasets consist of m data points drawn from the uniform distribution on the unit square. The G4 and G9 datasets consist of m data points drawn from four and nine Gaussian distributions respectively, resp., with identical covariance matrix \u03c32I and different mean vectors. The mean vectors of the G2 dataset are the four vertices of the unit square. The nine mean vectors of the G9 dataset are of the form (x, y) with x, y, y and vice versa."}, {"heading": "4.1.1. Results on G4 Datasets", "text": "The aim of the first series of experiments is to evaluate homogeneity as a function of the parameters k = 3.5. We assume that the cluster structure in the data is essentially determined by the k mean algorithm for an appropriate value of k. We consider the G4 data sets as unbalanced and contrast the results with those obtained on UD data sets. Unless otherwise stated, the default data types are considered as the default data types: (i) G4 generated with standard deviation of 0.05, (ii) G4 generated with standard deviation of 0.7, and (ii) UD generated with factor datasetyps.For each k data type {2, 10} and for all three types of datasets."}, {"heading": "4.1.2. Results on U-Shapes and Gaussians", "text": "The aim of the second series of experiments is to assess homogeneity as a function of the parameter k under the assumption that k-means is not able to detect a visible cluster structure in the data. To do this, we looked at U2 and U4 datasets and compared the results with those obtained on G4 and G9 datasets. Parameters for all datasets were \u03c3 = 0.1 and mc = 50, where mc is the number of data points of a single cluster. Figure 1 shows examples for all four datasets. Figure 6 shows the average \u03b1-homogeneity for all four datasets. As before, the general trend is that homogeneity decreases with increasing k and is interrupted only when the clearly visible cluster structure in the dataset can be detected."}, {"heading": "4.2. Experiments on UCI Datasets", "text": "The aim of this experiment is to investigate the probability of a unique middle partition for real data sets. To this end, we have used six data sets from the UCI Machine Learning Repository [15] in Table 1. For each data set and for each k mean, we applied 100 times and recorded the \u03b1 homogeneity. Figure 8 shows the \u03b1 homogeneities for each data set as a function of the k number. We observed that (i) the uniqueness of the middle partition is guaranteed for small k values, and (ii) the homogeneity decreases with increasing k. Exceptions to these general observations are the music and eye data sets. Except for the music data set, observation shows (i) that the uniqueness of the middle partition can be guaranteed for small k values, and (ii) the homogeneity of the real data sets decreases. This result indicates that uniquencies of practical relevance and no matter of exceptional cases are uniqueness (these are the large data sets)."}, {"heading": "5. Conclusion", "text": "We have shown that both the expected partition and the middle partition are unique when the support is contained in an open subset of a Max Hom ball. According to this condition, uniqueness is neither an exceptional nor a generic property. To address this problem, we proposed homogeneity as a measure of how close a sample is to a unique mean. Homogeneity is not limited to consensus clustering, but is also related to cluster stability, which in turn indicates the possibility that cluster stability and diversity in consensus cluster formation can be contradictory goals. Homogeneity can be efficiently limited from below by \u03b1 homogeneity, which applies the degree of asymmetry of a partition. \u03b1 homogeneity allows us to identify a partial sample of the largest subsamples of partitions that can be used to ensure a unique mean collision."}, {"heading": "A. Preliminaries", "text": "This section presents technical details useful to prove the results proposed in the main text. A.1. Notations We use the following notationsThe effect of the permutation P-X on the subset U-Xis the set defined by P-U = {PX: X-U.} A transposition is a permutation matrix P-X. A fundamental result of algebra is that each permutation matrix P-X is a matrix product P-Q1 \u00b7 \u00b7 Qt of transpositions Qi-q with a minimum number t > 0 of factors.A.2. Dirichlet Fundamental DomainsA subset F of X is a base set F-P-X if and only if F contains exactly one representation X from each orbit."}, {"heading": "DZ = {X \u2208 X : \u2016X \u2212Z\u2016 \u2264 \u2016X \u2212 PZ\u2016 for all P \u2208 \u03a0}", "text": "The next list of results includes some properties of the Dirichlet base domain: 1. There is a basic set of properties of the Dirichlet base domain: 1. There is a basic set of properties so that the D-Z base domain is a Dirichlet base domain with the representation Z of an asymmetric partition Z-P. 4. There is a basic set of properties so that the D-Z base domain is designed so that the D-Z base domain is DZ-DZ. 2. We have Z base domain with the representation Z. 3. Each item X-Z represents an asymmetric partition. 4. Supply that X, PX-Z base area for some P principles. Then X, PX-Z base area DZ. 5. PDZ = DPZ base area for all P bases. The proof follows [12], Prop. 3.13, but is adapted to the notation and terminology of this explanation."}, {"heading": "B. Proofs", "text": "B.1. Evidence of theory 3.1Parts 1-4 show the uniqueness of the expected partition and Part 5 shows the uniqueness of the mean partition.1. Both assertions apply trivially to asymmetric partitions Z, because SQ \"BZ\" = {Z. \"2. Let Z\" P \"be an asymmetric partition, making SQ\" BZ. \"We choose an arbitrary representation Z\" Z \"and an arbitrary cross section: P\" DZ. \"Let SZ\" (SQ) be the image of support SQ. \"Since BZ is a homogeneous partition, we have haveF\" (Y) = \"P\" (X, Y) 2dQ \"(X) 2dQ\" (Z) = SZ \".Z\" is the image of support SQ. \"\u2212 Z\" It is a homogeneous sphere. \""}], "references": [{"title": "Nonparametric Inference on Manifolds with Applications to Shape Spaces", "author": ["A. Bhattacharya", "R. Bhattacharya"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Introduction to Compact Transformation", "author": ["G.E. Bredon"], "venue": "Groups. Elsevier,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1972}, {"title": "A Combination Scheme for Fuzzy Clustering", "author": ["E. Dimitriadou", "A. Weingessel", "K. Hornik"], "venue": "Advances in Soft Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Weighted cluster ensembles: Methods and analysis", "author": ["C. Domeniconi", "M. Al-Razgan"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Cluster ensemble selection", "author": ["X.Z. Fern", "W. Lin"], "venue": "Statistical Analysis and Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Integrating microarray data by consensus clustering", "author": ["V. Filkov", "S. Skiena"], "venue": "International Journal on Artificial Intelligence Tools,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Ensemble clustering by means of clustering embedding in vector spaces", "author": ["L. Franek", "X. Jiang"], "venue": "Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Les \u00e9l\u00e9ments al\u00e9atoires de nature quelconque dans un espace distanci\u00e9", "author": ["M. Fr\u00e9chet"], "venue": "Annales de l\u2019institut Henri Poincare\u0301,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1948}, {"title": "A Survey: Clustering Ensembles Techniques", "author": ["R. Ghaemi", "N. Sulaiman", "H. Ibrahim", "N. Mustapha"], "venue": "Proceedings of World Academy of Science, Engineering and Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Clustering aggregation", "author": ["A. Gionis", "H. Mannila", "P. Tsaparas"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Data clustering: 50 years beyond K-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Geometry of Graph Edit Distance Spaces", "author": ["B.J. Jain"], "venue": "arXiv: 1505.08071,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Asymptotic Behavior of Mean Partitions in Consensus Clustering", "author": ["B.J. Jain"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Solving consensus and semi-supervised clustering problems using nonnegative matrix factorization", "author": ["T. Li", "C. Ding", "M.I. Jordan"], "venue": "IEEE International Conference on Data Mining,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "UCI Machine Learning Repository, [http://archive.ics.uci.edu/ml", "author": ["M. Lichman"], "venue": "University of California, School of Information and Computer Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Clustering stability: An overview", "author": ["U. von Luxburg"], "venue": "Now Publishers Inc.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Foundations of Hyperbolic Manifolds", "author": ["J.G. Ratcliffe"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Cluster Ensembles \u2013 A Knowledge Reuse Framework for Combining Multiple Partitions", "author": ["A. Strehl", "J. Ghosh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Analysis of consensus partition in cluster ensemble", "author": ["A.P. Topchy", "M.H. Law", "A.K. Jain", "A. Fred"], "venue": "IEEE International Conference on Data Mining,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Clustering ensembles: Models of consensus and weak partitions", "author": ["A.P. Topchy", "A.K. Jain", "W. Punch"], "venue": "IEEE Transactions in Pattern Analysis and Machine Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Weighted partition consensus via kernels", "author": ["S. Vega-Pons", "J. Correa-Morris", "J. Ruiz-Shulcloper"], "venue": "Pattern Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "A survey of clustering ensemble algorithms", "author": ["S. Vega-Pons", "J. Ruiz-Shulcloper"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Ensemble methods: foundations and algorithms", "author": ["Z.H. Zhou"], "venue": "CRC Press,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "Inspired by the success of classifier ensembles, consensus clustering has emerged as a research topic [9, 22].", "startOffset": 102, "endOffset": 109}, {"referenceID": 21, "context": "Inspired by the success of classifier ensembles, consensus clustering has emerged as a research topic [9, 22].", "startOffset": 102, "endOffset": 109}, {"referenceID": 2, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 3, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 5, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 6, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 9, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 13, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 17, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 19, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 20, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 12, "context": "Moreover, under reasonable conditions, uniqueness implies strong consistency and gives rise to different versions of the law of large numbers [13, 19].", "startOffset": 142, "endOffset": 150}, {"referenceID": 18, "context": "Moreover, under reasonable conditions, uniqueness implies strong consistency and gives rise to different versions of the law of large numbers [13, 19].", "startOffset": 142, "endOffset": 150}, {"referenceID": 10, "context": "This result points to potentially colliding approaches in clustering: standard clustering advocates stability [11, 16] and consensus clustering", "startOffset": 110, "endOffset": 118}, {"referenceID": 15, "context": "This result points to potentially colliding approaches in clustering: standard clustering advocates stability [11, 16] and consensus clustering", "startOffset": 110, "endOffset": 118}, {"referenceID": 4, "context": "advocates diversity [5, 22, 23], which will be briefly discussed.", "startOffset": 20, "endOffset": 31}, {"referenceID": 21, "context": "advocates diversity [5, 22, 23], which will be briefly discussed.", "startOffset": 20, "endOffset": 31}, {"referenceID": 22, "context": "advocates diversity [5, 22, 23], which will be briefly discussed.", "startOffset": 20, "endOffset": 31}, {"referenceID": 12, "context": "In a wider context, the results presented in this paper contribute towards a statistical theory of partitions [13].", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "To analyze partitions, we suggest a geometric representation proposed in [13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "Orbit spaces are well explored, possess a rich geometrical structure and have a natural connection to Euclidean spaces [2, 12, 17].", "startOffset": 119, "endOffset": 130}, {"referenceID": 11, "context": "Orbit spaces are well explored, possess a rich geometrical structure and have a natural connection to Euclidean spaces [2, 12, 17].", "startOffset": 119, "endOffset": 130}, {"referenceID": 16, "context": "Orbit spaces are well explored, possess a rich geometrical structure and have a natural connection to Euclidean spaces [2, 12, 17].", "startOffset": 119, "endOffset": 130}, {"referenceID": 0, "context": ", C` is specified by a matrix X \u2208 [0, 1]`\u00d7m such that X1` = 1m, where 1` \u2208 R and 1m \u2208 R are vectors of all ones.", "startOffset": 34, "endOffset": 40}, {"referenceID": 0, "context": "Orbit Spaces We define the representation space X of the set P = P`,m of partitions by X = { X \u2208 [0, 1]`\u00d7m : X1` = 1` } .", "startOffset": 97, "endOffset": 103}, {"referenceID": 12, "context": "Then the pair (P, \u03b4) is a geodesic metric space [13], Theorem 2.", "startOffset": 48, "endOffset": 52}, {"referenceID": 7, "context": "Fr\u00e9chet Functions In this section, we link the consensus function of the mean partition approach to Fr\u00e9chet functions [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "This link provides access to many results from Statistics in Non-Euclidean spaces [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 12, "context": "The minimum of FQ exists but but is not unique, in general [13].", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "As for expected Fr\u00e9chet functions FQ, the minimum of Fn exists but is not unique, in general [13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "Following [16], model selection in clustering is posed as the problem of minimizing the function", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "Diversity In consensus clustering it is recommended to use diverse partitions to improve the performance [5, 22, 23].", "startOffset": 105, "endOffset": 116}, {"referenceID": 21, "context": "Diversity In consensus clustering it is recommended to use diverse partitions to improve the performance [5, 22, 23].", "startOffset": 105, "endOffset": 116}, {"referenceID": 22, "context": "Diversity In consensus clustering it is recommended to use diverse partitions to improve the performance [5, 22, 23].", "startOffset": 105, "endOffset": 116}, {"referenceID": 4, "context": "Following [5], diversity is measured in the same way as cluster instability, namely by the sum of pairwise distances between sample partitions.", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "To comply with cluster stability, the question is under which conditions are two different mean partitions similar? To answer this question, we need the following result proved by [3]: Theorem 3.", "startOffset": 180, "endOffset": 183}, {"referenceID": 14, "context": "For this, we considered six datasets from the UCI Machine Learning Repository [15] listed in Table 1.", "startOffset": 78, "endOffset": 82}, {"referenceID": 16, "context": "[17], Theorem 6.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The proof follows [12], Prop.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "[17], Theorem 6.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Then X \u2208 \u2202DZ is a boundary point of DZ by [17], Theorem 6.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "for all 0 < \u03c1 \u2264 \u03b1Z/4 by [17], Theorem 13.", "startOffset": 24, "endOffset": 28}], "year": 2016, "abstractText": "The expectation and the mean of partitions generated by a cluster ensemble are not unique in general. This issue poses challenges in statistical inference and cluster stability. In this contribution, we state sufficient conditions for uniqueness of expectation and mean. The proposed conditions show that a unique mean is neither exceptional nor generic. To cope with this issue, we introduce homogeneity as a measure of how likely is a unique mean for a sample of partitions. We show that homogeneity is related to cluster stability. This result points to a possible conflict between cluster stability and diversity in consensus clustering. To assess homogeneity in a practical setting, we propose an efficient way to compute a lower bound of homogeneity. Empirical results using the k-means algorithm suggest that uniqueness of the mean partition is not exceptional for real-world data. Moreover, for samples of high homogeneity, uniqueness can be enforced by increasing the number of data points or by removing outlier partitions. In a broader context, this contribution can be placed as a further step towards a statistical theory of partitions.", "creator": "LaTeX with hyperref package"}}}