{"id": "1602.02617", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Adaptive imputation of missing values for incomplete pattern classification", "abstract": "In classification of incomplete pattern, the missing values can either play a crucial role in the class determination, or have only little influence (or eventually none) on the classification results according to the context. We propose a credal classification method for incomplete pattern with adaptive imputation of missing values based on belief function theory. At first, we try to classify the object (incomplete pattern) based only on the available attribute values. As underlying principle, we assume that the missing information is not crucial for the classification if a specific class for the object can be found using only the available information. In this case, the object is committed to this particular class. However, if the object cannot be classified without ambiguity, it means that the missing values play a main role for achieving an accurate classification. In this case, the missing values will be imputed based on the K-nearest neighbor (K-NN) and self-organizing map (SOM) techniques, and the edited pattern with the imputation is then classified. The (original or edited) pattern is respectively classified according to each training class, and the classification results represented by basic belief assignments are fused with proper combination rules for making the credal classification. The object is allowed to belong with different masses of belief to the specific classes and meta-classes (which are particular disjunctions of several single classes). The credal classification captures well the uncertainty and imprecision of classification, and reduces effectively the rate of misclassifications thanks to the introduction of meta-classes. The effectiveness of the proposed method with respect to other classical methods is demonstrated based on several experiments using artificial and real data sets.", "histories": [["v1", "Mon, 8 Feb 2016 15:52:08 GMT  (439kb,D)", "http://arxiv.org/abs/1602.02617v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["zhun-ga liu", "quan pan", "jean dezert", "arnaud martin"], "accepted": false, "id": "1602.02617"}, "pdf": {"name": "1602.02617.pdf", "metadata": {"source": "CRF", "title": "Adaptive imputation of missing values for incomplete pattern classification", "authors": ["Zhun-ga Liu", "Quan Pan", "Jean Dezert", "Arnaud Martin"], "emails": ["liuzhunga@nwpu.edu.cn", "jean.dezert@onera.fr", "Arnaud.Martin@univ-rennes1.fr"], "sections": [{"heading": null, "text": "This year it has come to the point where there is only one person who is able to retaliate, \"he said in an interview with the German Press Agency.\" We have never lost as much time as this year, \"he said."}, {"heading": "II. BACKGROUND KNOWLEDGE", "text": "Faith Function Theory (BFT) can well characterize uncertain and inaccurate information and is used in this work to classify patterns. SOM technology is used to find the optimized weight vectors that are used to represent the corresponding class, and this can reduce the computing burden in estimating the missing values based on the K-NN method."}, {"heading": "A. Basis of belief function theory", "text": "The belief functionality (BFT) proposed by Glenn Shafer is also known as the Dempster-Shafer theory (DST), or the mathematical theory of evidence [16] - [18]. Consider a distinction framework consisting of c exclusive and exhaustive hypotheses (classes) designated by [DP] (DP = {\u03c9i, i = 1, 2,., c). The power set of [S] rules used by [BFT] is the set of all subsets of \", including the empty set.\" For example, if the [S] elements of [S] -2, spectra of the spectra [S] -2, spectra, spectra, spectra [S] -2, spectra of the distribution of. \"In the classification problem, the singleton element (e.g..\" i \") represents a specific class. In this work, the disjunction (union) of classes (we originally proposed by Dropolster), although the metropolitan rule is preferred."}, {"heading": "B. Overview of Self-Organizing Map", "text": "The SOM then defines an image of the entrance space into a projected 2D space, and it is still able to obtain the topological properties of the entrance space with the help of an adjacent function. Thus, SOM is very useful for visualizing low-dimensional views of high-dimensional data through a non-linear projection process. The node at position (i, j), i = 1,.. M, j = 1,.., N corresponds to a weighting vector that is vascular vascular."}, {"heading": "III. CREDAL CLASSIFICATION OF INCOMPLETE PATTERN", "text": "Our new method consists of two main steps: In the first step, the object (incomplete pattern) is classified directly according to the known attribute values, and the missing values are ignored. If a specific classification result of a8 can be obtained, the classification procedure is performed, because the available attribute information is sufficient for the classification, but if the class of the object cannot be uniquely identified in the first step, this means that the information not available in the missing values is likely to be decisive for the classification. In this case, one must enter the second step of the method to classify the object with a proper imputation of missing values. In the classification procedure, the original or edited pattern is classified according to each class of training data. The global merging of these classification results, which can be regarded as multiple sources of evidence represented by BBA's, is then used for the credal classification of the object. Our new method for credential classification of incomplete classification based on incomplete patterns of adaptive theory is called the adaptive theory only."}, {"heading": "A. First step: Direct classification of incomplete pattern using the available data", "text": "We consider a set of test patterns (samples) X = {x1,.., xn}, based on a set of marked training patterns Y = {y1,.., ys}, beyond the scope of distinguishing features,.., \u03c9j}, in this work we focus on the classification of incomplete patterns in which some attribute values are missing. So we consider all test patterns (e.g. xi, i = 1,. n) with several missing values. The training record Y may also have incomplete patterns in some applications. However, if the incomplete patterns assume a very small amount of less than 5% in the training data collection, they can be ignored in the classification. If the incomplete patterns are large, the missing values usually have to be estimated first, and the classifier is trained using the edited (complete) patterns."}, {"heading": "B. Second step: Classification of incomplete pattern with imputation of missing values", "text": "1) Multiple Estimation of Missing Values: There are various methods for estimating missing attribute values. In particular, the K-NN imputation method generally provides good performance. However, the main disadvantage of the KNN method is its high computational load, since one has to calculate the distances of the object with all training samples. Inspired by [43], we propose to use the Self11Organized Map (SOM) technique to reduce the computational complexity. SOM can be applied to any class of training data, and then M \u00b7 N weight vectors are achieved using the optimization method. These optimized weight vectors allow the characterization of the entire class, and they are used to represent the corresponding data class. The number of weight vectors is usually small (e.g. 5 \u00d7 6)."}, {"heading": "IV. EXPERIMENTS", "text": "Three experiments with artificial and real data sets were used to measure the performance of this new CCAI method compared to the K-NN Imputation (KNNI) method (12), FCM Imputation (FCMI) method (13], [14], SOM Imputation (SOMI) [15] method and our previous credal classification PCC method [25]. SOM technique is also used in the second step of the CCAI method, but CCAI differs from the previous SOMI method, SOM is applied to the entire training data set, and the missing values are estimated exactly on the basis of an activation group consisting of the best match node (unit) of the input classes and their close neighbors. Then, the edited pattern can be classified with the imputation of missing values using a standard classification. Nevertheless, SOM is not included in the first step of the CCAI class, and the object is directly classified under disregard of the missing values."}, {"heading": "A. Experiment 1 (artificial data set)", "text": "In the first experiment, we show the interest of credal classification based on belief functions in relation to the traditional classification using probability frameworks (3ig classification). Unfortunately, a 3-class data set (3-class data set) is not available when the uniform distributions of the three 2-D uniform distribution patterns shown in Figure 2 are taken into account here. Each class has 200 training samples and 200 test samples, and there are 600 training samples and 600 test samples in total. Uniform distributions of the three classes are characterized by the following interval boundaries: x-label interval intervals y-intervals 1 (5, 65) (5, 25) \u03c92 (95, 155, 25) Spectrum 2 (50, 110) Spectrum 3 (50, 70) The values in the second dimension corresponding to the y-coordinates of the test samples are all missing. Thus, the test samples are classified according to the only value available in the first dimension of the spectrum 2 (50, 110) Spectrum 3 (50, 70) Spectrum 3 (50, 70) The values in the second dimension corresponding to the y-coordinates of the test samples are all missing."}, {"heading": "B. Experiment 2 (artificial data set)", "text": "In this second experiment, we evaluate the performance of the CCAI method using a 4D dataset that also includes three classes, \u03c92 and \u03c93. Artificial data are generated from three 4D Gaussian distributions characterized by the following mean vectors and covariance matrices (I refer to the 4-4 identity matrix): \u00b51 = (10, 50, 100) T, 1 = 10 \u00b7 I-2 = (30, 40, 50, 90) T, 2 = (20, 90, 130) T, 1 = 12 \u00b7 We used g-training samples, and g-test samples (for g = 500, and g = 1000) in each class. So there are a total of N = 3 x g-training samples and N = 3 x g-test samples."}, {"heading": "C. Experiment 3 (real data set)", "text": "This year it is so far that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "V. CONCLUSION", "text": "In the first step of the CCAI method, some objects (incomplete patterns) are directly classified, ignoring the missing values if the specific classification result can be achieved, which effectively reduces the computational complexity by avoiding the imputation of the missing values. However, if the information available is not sufficient to achieve a specific classification of the object in the first step, we estimate (recover) the missing values before entering the classification procedure in the second step. SOM and K-NN techniques are used to evaluate the estimation of missing attributes with a good compromise between the estimation accuracy and the computing load. The credal classification in this thesis allows the object to belong to different singleton classes and meta classes (i.e. disjunction of several classes).Once the object is assigned to a meta class (e.g. A-B), it is better distinguished if it cannot result in a good classification (i.e. the ulse of multiple classes and other classes)."}], "references": [{"title": "Pattern classification with missing data: a review", "author": ["P. Garcia-Laencina", "J. Sancho-Gomez", "A. Figueiras-Vidal"], "venue": "Neural Comput Appl. Vol. 19, pp. 263\u2013282", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistical Analysis with Missing Data", "author": ["R.J. Little", "D.B. Rubin"], "venue": "John Wiley & Sons, New York, 1987 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Pattern Classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "2 edition, Wiley-Interscience", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Supervised learning from incomplete data via an EM approach", "author": ["Z. Ghahramani", "M.I. Jordan"], "venue": "In: Cowan JD et al. (Eds) Adv. Neural Inf. Process., Morgan Kaufmann Publishers Inc., Vol. 6, pp. 120\u2013127", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Dealing with missing values in neural network-based diagnostic systems", "author": ["P.K. Sharpe", "R.J. Solly"], "venue": "Neural Comput Appl Vol. 3(2):73\u201377", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1986}, {"title": "Fuzzy C-means clustering of incomplete data", "author": ["R.J. Hathaway", "J.C. Bezdek"], "venue": "IEEE Trans. Syst Man Cybern:B Cybern, Vol.31(5): 735\u2013744", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Handling missing values in support vector machine classifiers", "author": ["K. Pelckmans", "J.D. Brabanter", "J.A.K. Suykens", "B.D. Moor"], "venue": "Neural Networks, Vol. 18, No. 5\u20136, pp. 684\u2013692", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Impact of imputation of missing values on classification error for discrete data", "author": ["A. Farhangfar", "L. Kurgan", "J. Dy"], "venue": "Pattern Recognition Vol. 41, pp. 3692\u20133705", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Imputing missing values: The effect on the accuracy of classification", "author": ["D.J. Mundfrom", "A. Whitcomb"], "venue": "Multiple Linear Regression Viewpoints. Vol. 25, No.1, pp. 13\u201319", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "A Study of K-Nearest Neighbour as an Imputation Method", "author": ["G. Batista", "M.C. Monard"], "venue": "Proc. of Second International Conference on Hybrid Intelligent Systems (IOS Press, v. 87), pp. 251\u2013260", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Towards missing data imputation: a study of fuzzy k-means clustering method, In: 4th international conference of rough sets and current trends in computing (RSCTC04), pp 573\u2013579", "author": ["D. Li", "J.Deogun", "W. Spaulding", "B. Shuart"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Self-organizing map for data imputation and correction in surveys", "author": ["F. Fessant", "S. Midenet"], "venue": "Neural Comput. Appl., Vol. 10, No. 4, pp. 300\u2013310", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "A mathematical theory of evidence", "author": ["G. Shafer"], "venue": "Princeton Univ. Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1976}, {"title": "The combination of evidence in the transferable belief model", "author": ["P. Smets"], "venue": "IEEE Trans. on Pattern Anal. and Mach. Intell.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1990}, {"title": "Support vector regression of membership functions and belief functions - Application for pattern recognition", "author": ["H. Laanaya", "A. Martin", "D. Aboutajdine", "A. Khenchaf"], "venue": "Information Fusion, Vol. 11, No. 4, pp. 338\u2013350", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "A k-nearest neighbor classification rule based on Dempster-Shafer Theory", "author": ["T. Den\u0153ux"], "venue": "IEEE Trans. Systems, Man and Cybernetics, Vol. 25, No. 5, pp. 804\u2013813", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "An evidence-theoretic k-NN rule with parameter optimization", "author": ["L.M. Zouhal", "T. Den\u0153ux"], "venue": "IEEE Trans. Systems, Man and Cybernetics - Part C, Vol. 28, No. 2, pp. 263\u2013271", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "A new belief-based K-nearest neighbor classification method", "author": ["Z.-g. Liu", "Q. Pan", "J. Dezert"], "venue": "Pattern Recognition,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Credal classification rule for uncertain data based on belief functions", "author": ["Z.-g. Liu", "Q. Pan", "J. Dezert", "G. Mercier"], "venue": "Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Classification using belief functions: relationship between case-based and model-based approaches", "author": ["T. Den\u0153ux", "P. Smets"], "venue": "IEEE Trans. on Systems, Man and Cybernetics, Part B: Vol. 36, No. 6, pp. 1395\u20131406", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "A neural network classifier based on Dempster-Shafer theory", "author": ["T. Den\u0153ux"], "venue": "IEEE Trans. on Systems, Man and Cybernetics A, Vol. 30, No. 2, pp. 131\u2013150", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Felix T", "author": ["X. Deng", "Y. Hu"], "venue": "S. Chan, S. Mahadevan, Y. Deng, Parameter estimation based on interval-valued belief structures, European Journal of Operational Research, Vol.241, No.2, pp.579\u2013582", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "ECM: An evidential version of the fuzzy c-means algorithm", "author": ["M.-H. Masson", "T. Den\u0153ux"], "venue": "Pattern Recognition, Vol. 41, No. 4, pp. 1384\u20131397", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Belief C-Means: An extension of fuzzy c-means algorithm in belief functions framework", "author": ["Z.-g. Liu", "J. Dezert", "G. Mercier", "Q. Pan"], "venue": "Pattern Recognition Letters,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Credal c-means clustering method based on belief functions", "author": ["Z.-g. Liu", "Q. Pan", "J. Dezert", "G. Mercier"], "venue": "Knowledge-based systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Q", "author": ["K. Zhou", "A. Martin"], "venue": "Pan, Z.-g. Liu, Median evidential c-means algorithm and its application to community detection, Knowledgebased systems,Vol. 74: 69\u201388", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Maximum likelihood estimation from uncertain data in the belief function framework", "author": ["T. Den\u0153ux"], "venue": "IEEE Transactions on Knowledge and Data Engineering, Vol. 25, No. 1, pp.119\u2013130", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Combination of sources of evidence with different discounting factors based on a new dissimilarity measure", "author": ["Z.-g. Liu", "J. Dezert", "Q. Pan", "G. Mercier"], "venue": "Decision Support Systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "A new decision-making method by incomplete preferences based on evidence distance", "author": ["S. Huang", "X. Su", "Y. Hu", "S. Mahadevan", "Y. Deng"], "venue": "Knowledge-Based Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "X", "author": ["X. Li", "J. Dezert", "F. Smarandache"], "venue": "Huang,Evidence supporting measure of similarity for reducing the complexity in information fusion, Information Science, Vol.181, No.10, pp. 1818\u20131835", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequential weighted combination for unreliable evidence based on evidence", "author": ["D.q. Han", "Y. Deng", "C.z Han"], "venue": "variance, Decision Support Systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "The Self-Organizing Map", "author": ["T. Kohonen"], "venue": "Proceedings of the IEEE, Vol.78, No.9, pp. 1464\u20131480", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1990}, {"title": "Representation and combination of uncertainty with belief functions and possibility measures", "author": ["D. Dubois", "H. Prade"], "venue": "Computational Intelligence, Vol. 4, No. 4, pp. 244\u2013264", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1988}, {"title": "On the validity of Dempster\u2019s rule of combination", "author": ["L.A. Zadeh"], "venue": "Memo M79/24, Univ. of California, Berkeley, USA", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1979}, {"title": "On the validity of Dempster\u2019s fusion rule and its interpretation as a generalization of Bayesian fusion", "author": ["J. Dezert", "A. Tchamova"], "venue": "rule, International Journal of Intelligent Systems,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Information Fusion Based on New Proportional Conflict Redistribution Rules", "author": ["F. Smarandache", "J. Dezert"], "venue": "Proceedings of Fusion 2005, Int. Conf. on Information Fusion, Philadelphia, PA, USA, July 25-29", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2005}, {"title": "On the estimation of mass functions using Self Organizing Maps", "author": ["I. Hammami", "J. Dezert", "G. Mercier", "A. Hamouda"], "venue": "Proc. of Belief 2014 Conf. Oxford, UK, Sept. 26\u201329", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Predictive inference: an introduction", "author": ["S. Geisser"], "venue": "New York, NY: Chapman and Hall", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1993}], "referenceMentions": [{"referenceID": 0, "context": "So it is crucial to develop efficient techniques to classify as best as possible the objects with missing attribute values (incomplete pattern), and the search for a solution of this problem remains an important research topic in the pattern classification field [1], [2].", "startOffset": 263, "endOffset": 266}, {"referenceID": 1, "context": "So it is crucial to develop efficient techniques to classify as best as possible the objects with missing attribute values (incomplete pattern), and the search for a solution of this problem remains an important research topic in the pattern classification field [1], [2].", "startOffset": 268, "endOffset": 271}, {"referenceID": 2, "context": "Some more details about pattern classification can be found in [3], [4].", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "Some more details about pattern classification can be found in [3], [4].", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "There have been many approaches developed for classifying the incomplete patterns [1], and they can be broadly grouped into four different types.", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "The second type is the modelbased techniques [5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "For instance, the expectation-maximization (EM) algorithm have been applied to many problems involving missing data for training Gaussian mixture models [5].", "startOffset": 153, "endOffset": 156}, {"referenceID": 5, "context": "The third type classifiers are designed to directly handle incomplete pattern without imputing the missing values, such as neural network ensemble methods [6], decision trees [7], fuzzy approaches [8] and support vector machine classifier [9].", "startOffset": 155, "endOffset": 158}, {"referenceID": 6, "context": "The third type classifiers are designed to directly handle incomplete pattern without imputing the missing values, such as neural network ensemble methods [6], decision trees [7], fuzzy approaches [8] and support vector machine classifier [9].", "startOffset": 175, "endOffset": 178}, {"referenceID": 7, "context": "The third type classifiers are designed to directly handle incomplete pattern without imputing the missing values, such as neural network ensemble methods [6], decision trees [7], fuzzy approaches [8] and support vector machine classifier [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 8, "context": "The third type classifiers are designed to directly handle incomplete pattern without imputing the missing values, such as neural network ensemble methods [6], decision trees [7], fuzzy approaches [8] and support vector machine classifier [9].", "startOffset": 239, "endOffset": 242}, {"referenceID": 9, "context": "The missing values are filled with proper estimations [10] at first, and then the edited patterns are classified using the normal classifier (for the complete pattern).", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "mean imputation [11], regress imputation [2], etc, or by machine learning methods, e.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "mean imputation [11], regress imputation [2], etc, or by machine learning methods, e.", "startOffset": 41, "endOffset": 44}, {"referenceID": 11, "context": "K-nearest neighbors imputation (KNNI) [12], Fuzzy c-means (FCM) imputation (FCMI) [13], [14], Self-organizing map imputation (SOMI) [15], etc.", "startOffset": 38, "endOffset": 42}, {"referenceID": 12, "context": "K-nearest neighbors imputation (KNNI) [12], Fuzzy c-means (FCM) imputation (FCMI) [13], [14], Self-organizing map imputation (SOMI) [15], etc.", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "K-nearest neighbors imputation (KNNI) [12], Fuzzy c-means (FCM) imputation (FCMI) [13], [14], Self-organizing map imputation (SOMI) [15], etc.", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "In FCMI, the missing values are imputed according to the clustering centers of FCM and taking into account the distances of the object to these centers [13], [14].", "startOffset": 158, "endOffset": 162}, {"referenceID": 13, "context": "In SOMI [15], the best match node (unit) of incomplete pattern can be found ignoring the missing values, and the imputation of the missing values", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "Belief function theory (BFT), also called Dempster-Shafer theory (DST) [16] and its extension [17], [18] offer a mathematical framework for modeling uncertainty and imprecise information [19].", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "Belief function theory (BFT), also called Dempster-Shafer theory (DST) [16] and its extension [17], [18] offer a mathematical framework for modeling uncertainty and imprecise information [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "BFT has already been applied successfully for object classification [20]\u2013[28], clustering [29]\u2013[33] and multi-source information fusion [34]\u2013[37], etc.", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "BFT has already been applied successfully for object classification [20]\u2013[28], clustering [29]\u2013[33] and multi-source information fusion [34]\u2013[37], etc.", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "BFT has already been applied successfully for object classification [20]\u2013[28], clustering [29]\u2013[33] and multi-source information fusion [34]\u2013[37], etc.", "startOffset": 90, "endOffset": 94}, {"referenceID": 28, "context": "BFT has already been applied successfully for object classification [20]\u2013[28], clustering [29]\u2013[33] and multi-source information fusion [34]\u2013[37], etc.", "startOffset": 95, "endOffset": 99}, {"referenceID": 29, "context": "BFT has already been applied successfully for object classification [20]\u2013[28], clustering [29]\u2013[33] and multi-source information fusion [34]\u2013[37], etc.", "startOffset": 136, "endOffset": 140}, {"referenceID": 32, "context": "BFT has already been applied successfully for object classification [20]\u2013[28], clustering [29]\u2013[33] and multi-source information fusion [34]\u2013[37], etc.", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "Some classifiers for the complete pattern based on DST have been developed by Den\u0153ux and his collaborators to come up with the evidential K-nearest neighbors (EK-NN) [21], evidential neural network (ENN) [27], etc.", "startOffset": 166, "endOffset": 170}, {"referenceID": 22, "context": "Some classifiers for the complete pattern based on DST have been developed by Den\u0153ux and his collaborators to come up with the evidential K-nearest neighbors (EK-NN) [21], evidential neural network (ENN) [27], etc.", "startOffset": 204, "endOffset": 208}, {"referenceID": 19, "context": "We have proposed credal classifiers [23], [24] for complete pattern considering all the possible meta-classes (i.", "startOffset": 36, "endOffset": 40}, {"referenceID": 20, "context": "We have proposed credal classifiers [23], [24] for complete pattern considering all the possible meta-classes (i.", "startOffset": 42, "endOffset": 46}, {"referenceID": 19, "context": "In [23], a belief-based K-nearest neighbor classifier (BK-NN) has been presented, and the credal classification of object is done according to the distances between the object and its K nearest neighbors as well as two given (acceptance and rejection) distance thresholds.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Thus, a simple credal classification rule (CCR) [24] has been further developed, and the belief value of object associated with different classes (i.", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "Moreover, when the training data is not available, we have also proposed several credal clustering methods [30]\u2013[32] in different cases.", "startOffset": 107, "endOffset": 111}, {"referenceID": 27, "context": "Moreover, when the training data is not available, we have also proposed several credal clustering methods [30]\u2013[32] in different cases.", "startOffset": 112, "endOffset": 116}, {"referenceID": 33, "context": "To reduce the computational burden, Self-Organizing Map (SOM) [38] is applied in each class, and the optimized weighting vectors are used to represent the corresponding class.", "startOffset": 62, "endOffset": 66}, {"referenceID": 14, "context": "If the object is directly classified using only the known values, Dempster-Shafer1 (DS) fusion rule [16] is applied because of the simplicity of this rule and also because the BBA\u2019s to fuse are usually in low conflict.", "startOffset": 100, "endOffset": 104}, {"referenceID": 34, "context": "Otherwise, a new fusion rule inspired by Dubois and Prade (DP) rule [39] is used to classify the edited pattern with proper imputation of its missing values.", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "The Belief Function Theory (BFT) introduced by Glenn Shafer is also known as Dempster-Shafer Theory (DST), or the Mathematical Theory of Evidence [16]\u2013[18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 15, "context": "The Belief Function Theory (BFT) introduced by Glenn Shafer is also known as Dempster-Shafer Theory (DST), or the Mathematical Theory of Evidence [16]\u2013[18].", "startOffset": 151, "endOffset": 155}, {"referenceID": 14, "context": "In this work, the disjunction (union) of several Although the rule has been proposed originally by Arthur Dempster, we prefer to call it Dempster-Shafer rule because it has been widely promoted by Shafer in [16].", "startOffset": 207, "endOffset": 211}, {"referenceID": 0, "context": ") from 2 to [0, 1] satisfying m(\u2205) = 0 and the normalization condition \u2211", "startOffset": 12, "endOffset": 18}, {"referenceID": 24, "context": "The credal classification (or partitioning) [29] is defined as n-tuple M = (m1, \u00b7 \u00b7 \u00b7 ,mn) of BBA\u2019s, where mi is the basic belief assignment of the object xi \u2208 X , i = 1, .", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "For combining multiple sources of evidence represented by a set of BBA\u2019s, the well-known Dempster\u2019s rule [16] is still widely used, even if its justification is an open debate and questionable in the community [40], [41].", "startOffset": 105, "endOffset": 109}, {"referenceID": 35, "context": "For combining multiple sources of evidence represented by a set of BBA\u2019s, the well-known Dempster\u2019s rule [16] is still widely used, even if its justification is an open debate and questionable in the community [40], [41].", "startOffset": 210, "endOffset": 214}, {"referenceID": 36, "context": "For combining multiple sources of evidence represented by a set of BBA\u2019s, the well-known Dempster\u2019s rule [16] is still widely used, even if its justification is an open debate and questionable in the community [40], [41].", "startOffset": 216, "endOffset": 220}, {"referenceID": 35, "context": "However, this redistribution can yield unreasonable results in the high conflicting cases [40], as well as in some special low conflicting cases as well [41].", "startOffset": 90, "endOffset": 94}, {"referenceID": 36, "context": "However, this redistribution can yield unreasonable results in the high conflicting cases [40], as well as in some special low conflicting cases as well [41].", "startOffset": 153, "endOffset": 157}, {"referenceID": 15, "context": "Among the possible alternatives of DS rule, we find Smets\u2019 conjunctive rule (used in his transferable belief model (TBM) [18]), Dubois-Prade (DP) rule [39], and more recently the more complex Proportional Conflict Redistributions (PCR) rules [42].", "startOffset": 121, "endOffset": 125}, {"referenceID": 34, "context": "Among the possible alternatives of DS rule, we find Smets\u2019 conjunctive rule (used in his transferable belief model (TBM) [18]), Dubois-Prade (DP) rule [39], and more recently the more complex Proportional Conflict Redistributions (PCR) rules [42].", "startOffset": 151, "endOffset": 155}, {"referenceID": 37, "context": "Among the possible alternatives of DS rule, we find Smets\u2019 conjunctive rule (used in his transferable belief model (TBM) [18]), Dubois-Prade (DP) rule [39], and more recently the more complex Proportional Conflict Redistributions (PCR) rules [42].", "startOffset": 242, "endOffset": 246}, {"referenceID": 33, "context": "Self-Organizing Map (SOM) (also called Kohonen map) [38] introduced by Teuvo Kohonen is a type of artificial neural network (ANN), and it is trained by unsupervised learning method.", "startOffset": 52, "endOffset": 56}, {"referenceID": 38, "context": "distance measure) [43].", "startOffset": 18, "endOffset": 22}, {"referenceID": 33, "context": "The detailed information about SOM can be found in [38].", "startOffset": 51, "endOffset": 55}, {"referenceID": 38, "context": "Inspired by [43], we propose to use the Self", "startOffset": 12, "endOffset": 16}, {"referenceID": 33, "context": "Organized Map (SOM) technique [38] to reduce the computational complexity.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": ")) are classically discounted [16] by \uf8f4\uf8f2\uf8f4\uf8f3 m\u0302 og i (\u03c9g) = \u03b1 \u03c9g i m og i (\u03c9g) m\u0302 og i (\u03a9) = 1\u2212 \u03b1 \u03c9g i + \u03b1 \u03c9g i m og i (\u03a9) (13)", "startOffset": 30, "endOffset": 34}, {"referenceID": 39, "context": "also apply the cross validation [44] (e.", "startOffset": 32, "endOffset": 36}, {"referenceID": 11, "context": "Three experiments with artificial and real data sets have been used to test the performance of this new CCAI method compared with the K-NN imputation (KNNI) method [12], FCM imputation (FCMI) method [13], [14], SOM imputation (SOMI) [15] method and our previous credal classification PCC method [25].", "startOffset": 164, "endOffset": 168}, {"referenceID": 12, "context": "Three experiments with artificial and real data sets have been used to test the performance of this new CCAI method compared with the K-NN imputation (KNNI) method [12], FCM imputation (FCMI) method [13], [14], SOM imputation (SOMI) [15] method and our previous credal classification PCC method [25].", "startOffset": 205, "endOffset": 209}, {"referenceID": 13, "context": "Three experiments with artificial and real data sets have been used to test the performance of this new CCAI method compared with the K-NN imputation (KNNI) method [12], FCM imputation (FCMI) method [13], [14], SOM imputation (SOMI) [15] method and our previous credal classification PCC method [25].", "startOffset": 233, "endOffset": 237}, {"referenceID": 22, "context": "The evidential neural network classifier (ENN) [27] is adopted in the sequel experiments to classify the edited pattern with the estimated values in PCC, KNNI and FCMI, since ENN produce generally good results in the classification6.", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "The evidential K-nearest neighbor (EK-NN) method [21] is also used to classify the edited pattern in Experiment 3 with real data for comparison.", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "The parameters of ENN and EK-NN can be automatically optimized as explained in [27] and [22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "The parameters of ENN and EK-NN can be automatically optimized as explained in [27] and [22].", "startOffset": 88, "endOffset": 92}, {"referenceID": 4, "context": "For KNNI, the values of K ranging from 5 to 20 neighbors have been tested, and the mean error rate with K \u2208 [5, 20] is given in Table I.", "startOffset": 108, "endOffset": 115}, {"referenceID": 16, "context": "For KNNI, the values of K ranging from 5 to 20 neighbors have been tested, and the mean error rate with K \u2208 [5, 20] is given in Table I.", "startOffset": 108, "endOffset": 115}], "year": 2016, "abstractText": "In classification of incomplete pattern, the missing values can either play a crucial role in the class determination, or have only little influence (or eventually none) on the classification results according to the context. We propose a credal classification method for incomplete pattern with adaptive imputation of missing values based on belief function theory. At first, we try to classify the object (incomplete pattern) based only on the available attribute values. As underlying principle, we assume that the missing information is not crucial for the classification if a specific class for the object can be found using only the available information. In this case, the object is committed to this particular class. However, if the object cannot be classified without ambiguity, it means that the missing values play a main role for achieving an accurate classification. In this case, the missing values will be imputed based on the K-nearest neighbor (K-NN) and self-organizing map (SOM) techniques, and the edited pattern with the imputation is then classified. The (original or edited) pattern is respectively classified according to each training class, and the classification results represented by basic belief assignments are fused with proper combination rules for making the credal classification. The object is allowed to belong with different masses of belief to the specific classes and meta-classes (which are particular disjunctions of several single classes). The credal classification captures well the uncertainty and imprecision of classification, and reduces effectively the rate of misclassifications thanks to the introduction of meta-classes. The effectiveness of the proposed method with respect to other classical methods is demonstrated based on several experiments using artificial and real data sets.", "creator": "LaTeX with hyperref package"}}}