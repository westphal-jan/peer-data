{"id": "1602.02672", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks", "abstract": "We propose deep distributed recurrent Q-networks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks. In these tasks, the agents are not given any pre-designed communication protocol. Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol. We present empirical results on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so. To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols. In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.", "histories": [["v1", "Mon, 8 Feb 2016 18:01:35 GMT  (3732kb,D)", "http://arxiv.org/abs/1602.02672v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["jakob n foerster", "yannis m assael", "nando de freitas", "shimon whiteson"], "accepted": false, "id": "1602.02672"}, "pdf": {"name": "1602.02672.pdf", "metadata": {"source": "META", "title": "Learning to Communicate to Solve Riddles  with Deep Distributed Recurrent Q-Networks", "authors": ["Jakob N. Foerster", "Yannis M. Assael", "Nando de Freitas", "Shimon Whiteson"], "emails": ["JAKOB.FOERSTER@CS.OX.AC.UK", "YANNIS.ASSAEL@CS.OX.AC.UK", "NANDODEFREITAS@GOOGLE.COM", "SHIMON.WHITESON@CS.OX.AC.UK"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that most of them will be able to put themselves at the centre of attention, and that they will be able to put themselves at the centre of attention. (...) In fact, it is the case that they are able to put themselves at the centre of attention. (...) In fact, it is the case that they are able to put themselves at the centre of attention. (...) In fact, it is the case that they are able to put themselves at the centre of attention. (...) In fact, it is the case that they are able to put themselves at the centre of attention. \"(...)"}, {"heading": "2. Background", "text": "In this section we briefly introduce DQN and its multi-agent and recurring extensions."}, {"heading": "2.1. Deep Q-Networks", "text": "In a single, fully observable, reinforced learning environment (Sutton & Barto, 1998), an agent observes at each discrete time step t his current state st + 1. His goal is to maximize expectations towards the discounted return, RtRt = rt + \u03b3rt + 1 + \u03b3 2rt + \u00b7 \u00b7 \u00b7, (1) which is a discount factor. Q function of a policy is: Q\u03c0 (s, a) = E [Rt | st = s, at = a]. (2) The optimal action value function Q (s, a) = max\u03c0 Q (s, a) is a discount factor."}, {"heading": "2.2. Independent DQN", "text": "DQN has been extended to cooperative multi-agent settings, in which each agent observes the global st, selects an individual agency and receives a team reward, rt, shared among all actors. Tampuu et al. (2015) address this environment with a framework that combines DQN with independent Q-Learning, applied to two-player pong, in which all actors learn their own Q-functions Qm (s, am; \u03b8mi) independently and at the same time. While independent Q-Learning can in principle lead to convergence problems (since one actor's learning does not make the environment appear stationary to other actors), it has a strong empirical track record (Shoham et al., 2007; Shoham & LeytonBrown, 2009; Zawadzki et al., 2014)."}, {"heading": "2.3. Deep Recurrent Q-Networks", "text": "Both DQN and independent DQN assume complete observability, i.e., the agent receives st as input. In contrast, st is hidden in partially observable environments and instead receives only an observation that is correlated with st but generally not unique. Hausknecht & Stone (2015) suggest the recurring Qnetwork (DRQN) architecture to address individual, partially observable settings. Instead of approximating Q (s, a) with an upstream network, they approach Q (o, a) with a relapsing neural network that can maintain an internal state and aggregated observations over time. This can be modelled by adding an additional input ht \u2212 1, representing the hidden state of the network, resulting in Q (ot, ht \u2212 1, a; \u03b8i)."}, {"heading": "2.4. Partially Observable Multi-Agent RL", "text": "In this work, we look at environments where there are both multiple actors and partial observability: each actor receives his own private omt at each step and maintains an internal state hmt. However, we assume that learning can take place in a centralized way, i.e., agents can share parameters etc. during learning, as long as the strategies they learn are conditioned only on the basis of their private history. In other words, we consider a centralized learning of decentralized policies. We are interested in such environments because only when several agents and partial observation exist side by side, agents have the incentive to communicate. As no communication protocol is a priori given, agents must first automatically develop and agree upon such a protocol. To our knowledge, no work on deep RL has considered such environments and no work has shown that deep RL communication protocols can be successfully learned."}, {"heading": "3. DDRQN", "text": "However, the simplest approach to deep RL in partially observable multi-agent settings is simply to combine Q = Q = all learned agents with independent Q-learning, in which case each agent represents the Q network, the Qm (omt, h m \u2212 1, am; \u03b8mi), in terms of the conditions on that agent's individual hidden state as well as observation. This approach, which we call the naive method, leads to poor results, as we propose in Section 5. Instead, we propose deeply distributed recurring Q networks (DDRQN), which make three key changes to the naive method. The first, final action involves providing each agent with its previous actions as input into the next step. As the agents employ stochastic strategies for exploration, they should generally perform their actions on their action observation histories, not just their observation histories. Feeding the last action as input allows the NN to observe action."}, {"heading": "4. Multi-Agent Riddles", "text": "In this section we describe the puzzles we use to evaluate DDRQN."}, {"heading": "4.1. Hats Riddle", "text": "The hat puzzle can be described as follows: \"An executioner lines up 100 prisoners one at a time and puts a red or blue hat on each prisoner's head. Each prisoner can see the hats of the people in front of him in the line - but not his own hat, nor that of anyone behind him. The executioner begins at the end (back) and asks the last prisoner about the color of his hat. He must answer\" red \"or\" blue. \"If he answers correctly, he is allowed to live. If he gives the wrong answer, he is killed immediately and silently. (While everyone hears the answer, no one knows whether an answer was correct.) The night before the deployment, the prisoners advise a strategy to help them. (Poundstone, 2012). Figure 1 illustrates this situation. An optimal strategy for all prisoners is to agree on a communication protocol in which the first prisoner says\" blue \"when the number of agents is equal and\" red. \"(Otherwise,\" all prisoners have a room in which they are)."}, {"heading": "4.2. Switch Riddle", "text": "In fact, it is not the case that one is able to observe the current state of the light bulb if one wants him to turn on the light bulb. He also has the ability to turn on the light bulb, and he believes that all the detainees have visited the interrogation room at a certain time."}, {"heading": "5. Experiments", "text": "In this section, we evaluate DDRQN on both multi-agent puzzles. In our experiments, prisoners select actions with a greedy policy of = 1 \u2212 0.5 1n for the hat puzzle and = 0.05 for the switch puzzle, for the latter the discount factor has been set to \u03b3 = 0.95, and the target nets update to \u03b1 \u2212 = 0.01 as described in Section 3, while in both cases the weights have been optimized with Adam (Kingma & Ba, 2014) with a learning rate of 1 \u00b7 10 \u2212 3. The proposed architectures use rectified linear units and LSTM cells. Further details of the network implementations will be published online in the Supplementary Material and in the Source Code."}, {"heading": "5.1. Hats Riddle", "text": "In fact, it is a purely reactionary project, which is a reactionary project, which is primarily a reactionary project."}, {"heading": "5.2. Switch Riddle", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "6. Related Work", "text": "There has been a wealth of work on multi-agent reinforcement learning with communication, e.g., (Tan, 1993; Melo et al., 2011; Panait & Luke, 2005; Zhang & Lesser, 2013; Maravall et al., 2013), but most of this work assumes a predefined communication protocol, with the exception of the work by Kasai et al. (2008), in which Q-learning tabular agents must learn the contents of a message in order to solve a predator-predator task. Their approach is similar to the Q-table benchmark used in Section 5.1. In contrast, DDRQN uses recursive neural networks, which provide memory-based communication and generalization via agents. Another example of open communication learning in a multi-agent task is given in (Giles & Jim, 2002)."}, {"heading": "7. Conclusions & Future Work", "text": "In order to communicate successfully, the actors in these tasks must first automatically develop and agree on their own communication protocol. We presented empirical results on two multi-agent learning problems based on well-known puzzles, demonstrating that DDRQN can successfully solve such tasks by discovering elegant communication protocols. In addition, we presented ablation experiments that confirm that each of the key components of the DDRQN architecture is critical to its success. Future work is needed to fully understand and improve the scalability of the DDRQN architecture for a large number of agents, e.g. for n > 4 in the switch puzzle. We also hope to further investigate the \"local minima\" structure of coordination and strategy underlying these puzzles. Another way to improve this is to expand DDRQN to leverage various multiagent adaptations of Q-Learning."}, {"heading": "8. Acknowledgements", "text": "This work was supported by the Oxford-Google DeepMind Graduate Scholarship and the EPSRC."}], "references": [{"title": "Data-efficient learning of feedback policies from image pixels using deep dynamical models", "author": ["Assael", "J.-A. M", "N. Wahlstr\u00f6m", "T.B. Sch\u00f6n", "Deisenroth", "M. p"], "venue": "arXiv preprint arXiv:1510.02173,", "citeRegEx": "Assael et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Assael et al\\.", "year": 2015}, {"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "In ICLR,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["M.G. Bellemare", "G. Ostrovski", "A. Guez", "P.S. Thomas", "R. Munos"], "venue": "In AAAI,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "An overview of recent progress in the study of distributed multi-agent coordination", "author": ["Y. Cao", "W. Yu", "W. Ren", "G. Chen"], "venue": "IEEE Transactions on Industrial Informatics,", "citeRegEx": "Cao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "Probabilistic approach to collaborative multi-robot localization", "author": ["D. Fox", "W. Burgard", "H. Kruppa", "S. Thrun"], "venue": "Autonomous Robots,", "citeRegEx": "Fox et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2000}, {"title": "A formal analysis and taxonomy of task allocation in multi-robot systems", "author": ["B.P. Gerkey", "M.J. Matari"], "venue": "International Journal of Robotics Research,", "citeRegEx": "Gerkey and Matari,? \\Q2004\\E", "shortCiteRegEx": "Gerkey and Matari", "year": 2004}, {"title": "Learning communication for multi-agent systems", "author": ["C.L. Giles", "K.C. Jim"], "venue": "In Innovative Concepts for AgentBased Systems,", "citeRegEx": "Giles and Jim,? \\Q2002\\E", "shortCiteRegEx": "Giles and Jim", "year": 2002}, {"title": "Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning", "author": ["X. Guo", "S. Singh", "H. Lee", "R.L. Lewis", "X. Wang"], "venue": "In NIPS,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Deep recurrent Qlearning for partially observable MDPs", "author": ["M. Hausknecht", "P. Stone"], "venue": "arXiv preprint arXiv:1507.06527,", "citeRegEx": "Hausknecht and Stone,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht and Stone", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Learning of communication codes in multi-agent reinforcement learning problem", "author": ["T. Kasai", "H. Tenmoto", "A. Kamiya"], "venue": "In IEEE Conference on Soft Computing in Industrial Applications, pp", "citeRegEx": "Kasai et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kasai et al\\.", "year": 2008}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "An algorithm for distributed reinforcement learning in cooperative multi-agent systems", "author": ["M. Lauer", "M. Riedmiller"], "venue": "In ICML,", "citeRegEx": "Lauer and Riedmiller,? \\Q2000\\E", "shortCiteRegEx": "Lauer and Riedmiller", "year": 2000}, {"title": "End-toend training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Recurrent reinforcement learning: A hybrid approach", "author": ["X. Li", "L. Li", "J. Gao", "X. He", "J. Chen", "L. Deng", "J. He"], "venue": "arXiv preprint 1509.03044,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Reinforcement learning for robots using neural networks", "author": ["L.J. Lin"], "venue": "PhD thesis,", "citeRegEx": "Lin,? \\Q1993\\E", "shortCiteRegEx": "Lin", "year": 1993}, {"title": "Markov games as a framework for multiagent reinforcement learning", "author": ["M.L. Littman"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Littman,? \\Q1994\\E", "shortCiteRegEx": "Littman", "year": 1994}, {"title": "Move Evaluation in Go Using Deep Convolutional Neural Networks", "author": ["C.J. Maddison", "A. Huang", "I. Sutskever", "D. Silver"], "venue": "In ICLR,", "citeRegEx": "Maddison et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2015}, {"title": "Coordination of communication in robot teams by reinforcement learning", "author": ["D. Maravall", "J. De Lope", "R. Domnguez"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Maravall et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maravall et al\\.", "year": 2013}, {"title": "Reinforcement learning in the multi-robot domain", "author": ["M.J. Matari"], "venue": "Autonomous Robots,", "citeRegEx": "Matari,? \\Q1997\\E", "shortCiteRegEx": "Matari", "year": 1997}, {"title": "QueryPOMDP: POMDP-based communication in multiagent systems", "author": ["F.S. Melo", "M. Spaan", "S.J. Witwicki"], "venue": "In Multi-Agent Systems, pp", "citeRegEx": "Melo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Melo et al\\.", "year": 2011}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": "In EMNLP,", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Action-conditional video prediction using deep networks in Atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "In NIPS,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Consensus and cooperation in networked multi-agent systems", "author": ["R. Olfati-Saber", "J.A. Fax", "R.M. Murray"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Olfati.Saber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Olfati.Saber et al\\.", "year": 2007}, {"title": "Cooperative multi-agent learning: The state of the art", "author": ["L. Panait", "S. Luke"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Panait and Luke,? \\Q2005\\E", "shortCiteRegEx": "Panait and Luke", "year": 2005}, {"title": "Are You Smart Enough to Work at Google?: Fiendish Puzzles and Impossible Interview Questions from the World\u2019s Top Companies", "author": ["W. Poundstone"], "venue": "Oneworld Publications,", "citeRegEx": "Poundstone,? \\Q2012\\E", "shortCiteRegEx": "Poundstone", "year": 2012}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "In ICLR,", "citeRegEx": "Schaul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations", "author": ["Y. Shoham", "K. Leyton-Brown"], "venue": null, "citeRegEx": "Shoham and Leyton.Brown,? \\Q2009\\E", "shortCiteRegEx": "Shoham and Leyton.Brown", "year": 2009}, {"title": "If multi-agent learning is the answer, what is the question", "author": ["Y. Shoham", "R. Powers", "T. Grenager"], "venue": "Artificial Intelligence,", "citeRegEx": "Shoham et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shoham et al\\.", "year": 2007}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["D. sabis"], "venue": "search. Nature,", "citeRegEx": "sabis,? \\Q2016\\E", "shortCiteRegEx": "sabis", "year": 2016}, {"title": "100 prisoners and a light bulb", "author": ["Y. Song"], "venue": "Technical report, University of Washington,", "citeRegEx": "Song,? \\Q2012\\E", "shortCiteRegEx": "Song", "year": 2012}, {"title": "Decentralized planning under uncertainty for teams of communicating agents", "author": ["M. Spaan", "G.J. Gordon", "N. Vlassis"], "venue": "In International joint conference on Autonomous agents and multiagent systems,", "citeRegEx": "Spaan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Spaan et al\\.", "year": 2006}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["B.C. Stadie", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1507.00814,", "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "Introduction to reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Multiagent cooperation and competition with deep reinforcement learning", "author": ["A. Tampuu", "T. Matiisen", "D. Kodelja", "I. Kuzovkin", "K. Korjus", "J. Aru", "R. Vicente"], "venue": "arXiv preprint arXiv:1511.08779,", "citeRegEx": "Tampuu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tampuu et al\\.", "year": 2015}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["M. Tan"], "venue": "In ICML,", "citeRegEx": "Tan,? \\Q1993\\E", "shortCiteRegEx": "Tan", "year": 1993}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "In AAAI,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "N. de Freitas", "M. Lanctot"], "venue": "arXiv preprint 1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["M. Watter", "J.T. Springenberg", "J. Boedecker", "M.A. Riedmiller"], "venue": "In NIPS,", "citeRegEx": "Watter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watter et al\\.", "year": 2015}, {"title": "100 prisoners and a lightbulb", "author": ["W. Wu"], "venue": "Technical report, OCF, UC Berkeley,", "citeRegEx": "Wu,? \\Q2002\\E", "shortCiteRegEx": "Wu", "year": 2002}, {"title": "Empirically evaluating multiagent learning algorithms", "author": ["E. Zawadzki", "A. Lipson", "K. Leyton-Brown"], "venue": "arXiv preprint 1401.8074,", "citeRegEx": "Zawadzki et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zawadzki et al\\.", "year": 2014}, {"title": "Coordinating multi-agent reinforcement learning with limited communication", "author": ["C. Zhang", "V. Lesser"], "venue": null, "citeRegEx": "Zhang and Lesser,? \\Q2013\\E", "shortCiteRegEx": "Zhang and Lesser", "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control (Levine et al., 2015; Assael et al., 2015; Watter et al., 2015), visual attention (Ba et al.", "startOffset": 180, "endOffset": 243}, {"referenceID": 0, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control (Levine et al., 2015; Assael et al., 2015; Watter et al., 2015), visual attention (Ba et al.", "startOffset": 180, "endOffset": 243}, {"referenceID": 38, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control (Levine et al., 2015; Assael et al., 2015; Watter et al., 2015), visual attention (Ba et al.", "startOffset": 180, "endOffset": 243}, {"referenceID": 1, "context": ", 2015), visual attention (Ba et al., 2015), and the Atari learning environment (ALE) (Guo et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 7, "context": ", 2015), and the Atari learning environment (ALE) (Guo et al., 2014; Mnih et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).", "startOffset": 50, "endOffset": 234}, {"referenceID": 32, "context": ", 2015), and the Atari learning environment (ALE) (Guo et al., 2014; Mnih et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).", "startOffset": 50, "endOffset": 234}, {"referenceID": 37, "context": ", 2015), and the Atari learning environment (ALE) (Guo et al., 2014; Mnih et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).", "startOffset": 50, "endOffset": 234}, {"referenceID": 26, "context": ", 2015), and the Atari learning environment (ALE) (Guo et al., 2014; Mnih et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).", "startOffset": 50, "endOffset": 234}, {"referenceID": 22, "context": ", 2015), and the Atari learning environment (ALE) (Guo et al., 2014; Mnih et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).", "startOffset": 50, "endOffset": 234}, {"referenceID": 2, "context": ", 2015), and the Atari learning environment (ALE) (Guo et al., 2014; Mnih et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).", "startOffset": 50, "endOffset": 234}, {"referenceID": 17, "context": "ing for Go (Maddison et al., 2015; Silver et al., 2016) has recently shown success.", "startOffset": 11, "endOffset": 55}, {"referenceID": 28, "context": "Their approach is based on independent Qlearning (Shoham et al., 2007; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014), in which all agents learn their own Q-functions independently in parallel.", "startOffset": 49, "endOffset": 122}, {"referenceID": 40, "context": "Their approach is based on independent Qlearning (Shoham et al., 2007; Shoham & Leyton-Brown, 2009; Zawadzki et al., 2014), in which all agents learn their own Q-functions independently in parallel.", "startOffset": 49, "endOffset": 122}, {"referenceID": 17, "context": "ing for Go (Maddison et al., 2015; Silver et al., 2016) has recently shown success. In cooperative settings, Tampuu et al. (2015) have adapted deep Q-networks (Mnih et al.", "startOffset": 12, "endOffset": 130}, {"referenceID": 19, "context": "Such problems arise naturally in a variety of settings, such as multi-robot systems and sensor networks (Matari, 1997; Fox et al., 2000; Gerkey & Matari, 2004; Olfati-Saber et al., 2007; Cao et al., 2013).", "startOffset": 104, "endOffset": 204}, {"referenceID": 4, "context": "Such problems arise naturally in a variety of settings, such as multi-robot systems and sensor networks (Matari, 1997; Fox et al., 2000; Gerkey & Matari, 2004; Olfati-Saber et al., 2007; Cao et al., 2013).", "startOffset": 104, "endOffset": 204}, {"referenceID": 23, "context": "Such problems arise naturally in a variety of settings, such as multi-robot systems and sensor networks (Matari, 1997; Fox et al., 2000; Gerkey & Matari, 2004; Olfati-Saber et al., 2007; Cao et al., 2013).", "startOffset": 104, "endOffset": 204}, {"referenceID": 3, "context": "Such problems arise naturally in a variety of settings, such as multi-robot systems and sensor networks (Matari, 1997; Fox et al., 2000; Gerkey & Matari, 2004; Olfati-Saber et al., 2007; Cao et al., 2013).", "startOffset": 104, "endOffset": 204}, {"referenceID": 1, "context": "While these environments do not require convolutional networks for perception, the presence of partial observability means that they do require recurrent networks to deal with complex sequences, as in some single-agent works (Hausknecht & Stone, 2015; Ba et al., 2015) and languagebased (Narasimhan et al.", "startOffset": 225, "endOffset": 268}, {"referenceID": 21, "context": ", 2015) and languagebased (Narasimhan et al., 2015) tasks.", "startOffset": 26, "endOffset": 51}, {"referenceID": 15, "context": "DQN uses experience replay (Lin, 1993; Mnih et al., 2015): during learning, the agent builds a datasetDt = {e1, e2, .", "startOffset": 27, "endOffset": 57}, {"referenceID": 28, "context": "While independent Q-learning can in principle lead to convergence problems (since one agent\u2019s learning makes the environment appear non-stationary to other agents), it has a strong empirical track record (Shoham et al., 2007; Shoham & LeytonBrown, 2009; Zawadzki et al., 2014).", "startOffset": 204, "endOffset": 276}, {"referenceID": 40, "context": "While independent Q-learning can in principle lead to convergence problems (since one agent\u2019s learning makes the environment appear non-stationary to other agents), it has a strong empirical track record (Shoham et al., 2007; Shoham & LeytonBrown, 2009; Zawadzki et al., 2014).", "startOffset": 204, "endOffset": 276}, {"referenceID": 33, "context": "Tampuu et al. (2015) address this setting with a framework that combines DQN with independent Q-learning, applied to two-player pong, in which all agents independently and simultaneously learn their own Q-functions Q(s, a; \u03b8 i ).", "startOffset": 0, "endOffset": 21}, {"referenceID": 25, "context": "What should they do?\u201d (Poundstone, 2012).", "startOffset": 22, "endOffset": 40}, {"referenceID": 39, "context": "Can they agree on a protocol that will guarantee their freedom?\u201d (Wu, 2002).", "startOffset": 65, "endOffset": 75}, {"referenceID": 30, "context": "A number of strategies (Song, 2012; Wu, 2002) have been analysed for the infinite time-horizon version of this problem in which the goal is to guarantee survival.", "startOffset": 23, "endOffset": 45}, {"referenceID": 39, "context": "A number of strategies (Song, 2012; Wu, 2002) have been analysed for the infinite time-horizon version of this problem in which the goal is to guarantee survival.", "startOffset": 23, "endOffset": 45}, {"referenceID": 35, "context": ", (Tan, 1993; Melo et al., 2011; Panait & Luke, 2005; Zhang & Lesser, 2013; Maravall et al., 2013).", "startOffset": 2, "endOffset": 98}, {"referenceID": 20, "context": ", (Tan, 1993; Melo et al., 2011; Panait & Luke, 2005; Zhang & Lesser, 2013; Maravall et al., 2013).", "startOffset": 2, "endOffset": 98}, {"referenceID": 18, "context": ", (Tan, 1993; Melo et al., 2011; Panait & Luke, 2005; Zhang & Lesser, 2013; Maravall et al., 2013).", "startOffset": 2, "endOffset": 98}, {"referenceID": 10, "context": "One exception is the work of Kasai et al. (2008), in which the tabular Q-learning agents have to learn the content of a message to solve a predator-prey task.", "startOffset": 29, "endOffset": 49}, {"referenceID": 31, "context": "Furthermore, planning-based RL methods have been employed to include messages as an integral part of the multi-agent reinforcement learning challenge (Spaan et al., 2006).", "startOffset": 150, "endOffset": 170}, {"referenceID": 21, "context": "Deep recurrent reinforcement learning has also been applied to textbased games, which are naturally partially observable (Narasimhan et al., 2015).", "startOffset": 121, "endOffset": 146}, {"referenceID": 14, "context": "Recurrent DQN was also successful in the email campaign challenge (Li et al., 2015).", "startOffset": 66, "endOffset": 83}, {"referenceID": 35, "context": "Another avenue for improvement is to extend DDRQN to make use of various multi-agent adaptations of Q-learning (Tan, 1993; Littman, 1994; Lauer & Riedmiller, 2000; Panait & Luke, 2005).", "startOffset": 111, "endOffset": 184}, {"referenceID": 16, "context": "Another avenue for improvement is to extend DDRQN to make use of various multi-agent adaptations of Q-learning (Tan, 1993; Littman, 1994; Lauer & Riedmiller, 2000; Panait & Luke, 2005).", "startOffset": 111, "endOffset": 184}], "year": 2016, "abstractText": "We propose deep distributed recurrent Qnetworks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks. In these tasks, the agents are not given any pre-designed communication protocol. Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol. We present empirical results on two multiagent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so. To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols. In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.", "creator": "LaTeX with hyperref package"}}}