{"id": "1602.02685", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Predicting Clinical Events by Combining Static and Dynamic Information Using Recurrent Neural Networks", "abstract": "In clinical data sets we often find static information (e.g. gender of the patients, blood type, etc.) combined with sequences of data that are recorded during multiple hospital visits (e.g. medications prescribed, tests performed, etc.). Recurrent Neural Networks (RNNs) have proven to be very successful for modelling sequences of data in many areas of Machine Learning. In this work we present an approach based on RNNs that is specifically designed for the clinical domain and that combines static and dynamic information in order to predict future events. We work with a database collected in the Charit\\'{e} Hospital in Berlin that contains all the information concerning patients that underwent a kidney transplantation. After the transplantation three main endpoints can occur: rejection of the kidney, loss of the kidney and death of the patient. Our goal is to predict, given the Electronic Health Record of each patient, whether any of those endpoints will occur within the next six or twelve months after each visit to the clinic. We compared different types of RNNs that we developed for this work, a model based on a Feedforward Neural Network and a Logistic Regression model. We found that the RNN that we developed based on Gated Recurrent Units provides the best performance for this task. We also performed an additional experiment using these models to predict next actions and found that for such use case the model based on a Feedforward Neural Network outperformed the other models. Our hypothesis is that long-term dependencies are not as relevant in this task.", "histories": [["v1", "Mon, 8 Feb 2016 18:30:58 GMT  (374kb,D)", "http://arxiv.org/abs/1602.02685v1", null], ["v2", "Thu, 17 Nov 2016 11:52:19 GMT  (254kb,D)", "http://arxiv.org/abs/1602.02685v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["crist\\'obal esteban", "oliver staeck", "yinchong yang", "volker tresp"], "accepted": false, "id": "1602.02685"}, "pdf": {"name": "1602.02685.pdf", "metadata": {"source": "CRF", "title": "Predicting Clinical Events by Combining Static and Dynamic Information Using Recurrent Neural Networks", "authors": ["Crist\u00f3bal Esteban", "Oliver Staeck", "Yinchong Yang", "Volker Tresp"], "emails": ["volker.tresp}@siemens.com", "oliver.staeck@charite.de"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of us are able to assert ourselves that we are able to change the world and change it, and that we are able to change the world and change it, \"he said.\" We must put ourselves in a position to change the world, \"he said in an interview with the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\""}, {"heading": "2 Related Work", "text": "In terms of the task of predicting clinical events, Esteban et al. [6] introduced the Temporal Latent Embedding Model, based on a feedback neural network, which surpasses its foundations for the task of predicting what events will be observed next given the recorded previous events (i.e. the goal was to predict what laboratory analyses and drug prescriptions will be observed the next time each patient visits).The architecture of this model may be shown in Figure 1.The Temporal Latent Embedding Model requires an explicit definition of the number of past time steps that we want to take into account in order to predict the next step. In some scenarios, this limitation may actually be an advantage, since many recent studies have shown how attention mechanisms can actually improve the performance of neural networks [7] [8] [Temporal latent Embedding], which focus all attention on the last \"n\" samples, and therefore provide an advantage over NRA events that are dependent on the previous ones we want to predict in the GRA data sets."}, {"heading": "3 Kidney Transplantation Endpoints", "text": "It affects several organ systems and can lead to a final stage of kidney disease. An increasing number of patients with terminal kidney disease require dialysis or transplantation. The main goals after the transplantation are to reduce complications and increase the survival of patients. However, kidney transplants are associated with high risks."}, {"heading": "4 Recurrent Neural Networks for Clinical Event Prediction", "text": "In this way, RNNs can learn to memorize past events that are relevant to predicting future outcomes. In Figure 2, you can see the architecture of an RNN. Unlike models based on feedback neural networks, RNNs can theoretically learn to remember events (or combination of events) that occur in the patient's life and are useful for predicting future events. This feature can be very valuable when our clinical dataset presents such long-term dependencies. Another advantage of using RNNs for sequence learning is that we can begin to predict future events that are useful to such a patient in the face of a new patient."}, {"heading": "4.1 Standard Recurrent Neural Network", "text": "This is the classic way to update the hidden state in RNNs: ht = \u03c3 (Wxt + Uht \u2212 1). (3) As we have already mentioned, one of our main interests in using RNNs is their ability to capture long-term dependencies. However, Bengio et al. [14] observed that it is difficult for standard RNNs to capture such long-term dependencies due to the problem of the disappearance of the gradient."}, {"heading": "4.2 Long Short-Term Memory Units", "text": "In order to alleviate the problem of the disappearance of the gradient, Hochreiter et al. [15] have developed a gating mechanism that specifies when and how the hidden state of an RNN needs to be updated. There are several versions with minor modifications to the gating mechanism in long-term memory (LSTM), using the one defined by Graves et al. [16]: it = \u03c3 (Wxixt + Whiht \u2212 1 + Wcict \u2212 1 + bi) (4) rt = \u03c3 (Wxr xt + Whrht \u2212 1 + br) (5) ct = rt ct \u2212 1 + it tanh (Wxcxt + Whcht \u2212 1 + bc) (6) ot = \u03c3 (Wxoxt \u2212 1 + Whoht \u2212 1 + Whrht \u2212 1 + bo) (7) ht = ot tanh (ct) (8), where the elemental product and i, r and o are the input values, or the initial values are forgotten."}, {"heading": "4.3 Gated Recurrent Units", "text": "Another gating mechanism called gated recurrent units (GRUs) was introduced by Cho et al. [17] with the aim of designing each recurring unit in such a way that dependencies of different time scales can be captured adaptively. We follow the equations defined by Chung et al. [18]: rt = \u03c3 (Wr xt + Urht \u2212 1) (9) h = tanh (Wxt + U (rt ht \u2212 1))) (10) zt = \u03c3 (Wzxt + Uzht \u2212 1) (11) ht = (1 \u2212 zt) ht \u2212 1 + zth \u0445t (12), with z and r being the updated and reset gates respectively. As can be seen, GRUs have an architecture that is less complex than LSTM units, and the former generally work at least as well as the latter."}, {"heading": "4.4 Combining RNNs with Static Data", "text": "In this case, it is such that we see ourselves as being able to put ourselves in a position to put ourselves in the position in which we find ourselves. (...) We are able to put ourselves in the position in which we find ourselves. (...) We are able to put ourselves in the position in which we find ourselves. (...) We are able to put ourselves in the position in which we find ourselves. (...) We are not able to put ourselves in the position. (...) We are not able to put ourselves in the position. (...) We are not able to put ourselves in the position. (...) We are not able to put ourselves in the position. (...) We are not able to put ourselves in the position. (...) We are not able to be in the position. (...) We are not able to be in the position. (...) We are not able to be in the position. (...) We are not able to be in the position. (...) We are not able to be in the position. (...) We are not able to be in the position. (...) We are not able to be in the position. (...) We are not able to be in the position. (... We are not able to be in the position. (...)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data Pre-processing and Experimental Setup", "text": "We have three sets of variables in our dataset: endpoints, prescribed and prescribed data. Both endpoints and prescribed drugs are binary data. On the other hand, the laboratory results are a set of real numbers. To deal with such missing data, we experiment with imputation and imputation. We also experiment with scaling and normalizing the data before we insert it into the neural system."}, {"heading": "5.2 Results", "text": "Table 1 shows the results of predicting endpoints without taking into account different types of them (i.e., for evaluation we link all the predictions in the set). \"GRU + static,\" \"LSTM + static\" and \"RNN + static\" stand for the architectures presented in this paper, which combine an RNN with static information. \"TLE\" stands for the Temporal Latent Embeddings model [6]. \"Random\" stands for the results obtained from random predictions. \"The recurring models outperform the other models in both the AUROC score and the AUPRC score, as they use the best model with an AUPRC of 0.345 and an AUROC of 0.833.\" The AUPRC scores are quite low compared to their highest possible value (i.e. 1), but they are quite good compared to the random baseline of 0.073, which is so low because we have repeated the prediction of the five times we have the PRA repeating the differing data."}, {"heading": "5.3 Additional Experiments", "text": "As already mentioned, the Temporal Latent Embeddings Model exceeded the fundamentals presented in [6] for predicting the events observed for each patient on their next visit to the clinic (i.e., what laboratory analyses are performed next, what results are achieved in such analyses, and what medications are prescribed next), but none of these fundamentals was based on RNN. Therefore, we have reproduced the experiments presented in [6], including the models presented in this article on RNNs. Table 3 shows the outcome of such an experiment, in which we can see that the Temporal Latent Embeddings Model still delivers better values than the other models. In Table 3, we have also included an entry called \"Static Embeddings,\" which corresponds to the predictions made with a feedback forward neural network, using only the static information of each patient. We assume that the Temporal Latent Embeddings Model provides the best performance due to the lack of complex long-term dependencies that are relevant to the task."}, {"heading": "6 Conclusion", "text": "We developed and compared various algorithms based on RNNs, which can combine both static and dynamic clinical variables to solve the task of predicting endpoints in patients undergoing kidney transplantation. This is an application that will provide physicians with important information and help them make better decisions. We found that an RNN with GRUs combined with a Feedforward Neural Network provides the best values for this task. We also compared these recurring models with other models for predicting next actions. We found that the RNNNs for such a use case do not exceed any other model based on a Feedforward Neural Network. We suspect that this is due to the lack of complex long-term dependencies in the data relevant to this task, and therefore it is beneficial to use a model that draws all attention to recent events."}], "references": [{"title": "Desensitization of HLA-incompatible kidney recipients", "author": ["L Huber", "M Naik", "K. Budde"], "venue": "N Engl J Med", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Everolimus-based, calcineurin-inhibitor-free regimen in recipients of de-novo kidney transplants: an open-label, randomised, controlled trial", "author": ["K Budde", "T Becker", "W Arns", "C Sommerer", "P Reinke", "U Eisenberger", "S Kramer", "W Fischer", "H Gschaidmeier", "F. Pietruck"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Conversion from cyclosporine to everolimus at 4.5 months posttransplant: 3-year results from the randomized ZEUS study", "author": ["K Budde", "F Lehner", "C Sommerer", "W Arns", "P Reinke", "U Eisenberger", "RP Wthrich", "S Scheidl", "C May", "EM Paulus", "A Mhlfeld", "HH Wolters", "K Pressmar", "R Stahl", "O. Witzke"], "venue": "Am J Transplant", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Everolimus for angiomyolipoma associated with tuberous sclerosis complex or sporadic lymphangioleiomyomatosis: a multicentre, randomised, doubleblind, placebo-controlled trial", "author": ["JJ Bissler", "JC Kingswood", "E Radzikowska", "BA Zonnenberg", "M Frost", "E Belousova", "M Sauter", "N Nonomura", "S Brakemeier", "PJ de Vries", "VH Whittemore", "D Chen", "T Sahmoud", "G Shah", "J Lincy", "D Lebwohl", "K. Budde"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Predicting sequences of clinical events by using a personalized temporal latent embedding model", "author": ["C Esteban", "D Schmidt", "D Krompa\u00df", "V. Tresp"], "venue": "In Proceedings of the IEEE International Conference on Healthcare Informatics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Describing multimedia content using attention-based encoder-decoder networks, arXiv preprint arXiv:1507.01053", "author": ["K Cho", "A Courville", "Y Bengio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K Xu", "J Ba", "R Kiros", "K Cho", "A Courville", "R Salakhutdinov", "R Zemel", "Y. Bengio"], "venue": "In Proceedings of The 32-nd International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Doctor AI: Predicting clinical events via recurrent neural networks", "author": ["E Choi", "M Taha Bahadori", "J. Sun"], "venue": "arXiv preprint arXiv:1511.05942,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T Mikolov", "I Sutskever", "K Chen", "G Corrado", "J. Dean"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Statistical Language Models based on Neural Networks", "author": ["T. Mikolov"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I Sutskever", "O Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Prevalence of chronic kidney disease", "author": ["J Coresh", "E Selvin", "LA Stevens", "J Manzi", "JW Kusek", "P Eggers", "F Van Lente", "AS. Levey"], "venue": "in the United States. JAMA,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "The problem of learning long-term dependencies in recurrent networks. pages 11831195", "author": ["Y Bengio", "P Frasconi", "P. Simard"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Long Short-Term Memory", "author": ["S Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A Graves", "A Mohamed", "G. Hinton"], "venue": "In ICASSP2013,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "On the properties of neural machine translation: Encoderdecoder approaches", "author": ["K Cho", "B van Merrienboer", "D Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J Duchi", "E Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T Tieleman", "G. Hinton"], "venue": "Coursera: Neural Networks for Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "The database has been the basis for many studies in the past [2, 3, 4, 5].", "startOffset": 61, "endOffset": 73}, {"referenceID": 1, "context": "The database has been the basis for many studies in the past [2, 3, 4, 5].", "startOffset": 61, "endOffset": 73}, {"referenceID": 2, "context": "The database has been the basis for many studies in the past [2, 3, 4, 5].", "startOffset": 61, "endOffset": 73}, {"referenceID": 3, "context": "The database has been the basis for many studies in the past [2, 3, 4, 5].", "startOffset": 61, "endOffset": 73}, {"referenceID": 4, "context": "[6] introduced the Temporal Latent Embeddings model which is based on a Feedforward Neural Network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In some scenarios, this constrain can actually be an advantage since many recent papers have shown how attention mechanisms can actually improve the performance of Neural Networks [7] [8].", "startOffset": 180, "endOffset": 183}, {"referenceID": 6, "context": "In some scenarios, this constrain can actually be an advantage since many recent papers have shown how attention mechanisms can actually improve the performance of Neural Networks [7] [8].", "startOffset": 184, "endOffset": 187}, {"referenceID": 7, "context": "[9] use an RNN with Gated Recurrent Units (GRUs) combined with skip-gram vectors [10] in order to predict diagnosis, medication prescription and procedures.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] use an RNN with Gated Recurrent Units (GRUs) combined with skip-gram vectors [10] in order to predict diagnosis, medication prescription and procedures.", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "Outside of the medical domain, regarding the tasks of using sequences of data together with RNNs in order to predict events, we can find in the literature multiple successful examples, specially in the field of Natural Language Processing [11] [12].", "startOffset": 239, "endOffset": 243}, {"referenceID": 10, "context": "Outside of the medical domain, regarding the tasks of using sequences of data together with RNNs in order to predict events, we can find in the literature multiple successful examples, specially in the field of Natural Language Processing [11] [12].", "startOffset": 244, "endOffset": 248}, {"referenceID": 11, "context": "The growing number of patients with end stage renal disease requiring dialysis or transplantation is a major challenge for healthcare systems around the world [13].", "startOffset": 159, "endOffset": 163}, {"referenceID": 12, "context": "[14] that it is difficult for standard RNNs to capture such long-term dependencies due to the gradient vanishing problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] developed a gating mechanism that dictates when and how the hidden state of an RNN has to be updated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16]:", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "LSTM units have been successfully used many times before [7] [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 6, "context": "LSTM units have been successfully used many times before [7] [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 15, "context": "[17] with the goal of making each recurrent unit to adaptively capture dependencies of different time scales.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "We feed our network with the latent representation of the inputs, as it is often done in Natural Language Processing [19] [12].", "startOffset": 122, "endOffset": 126}, {"referenceID": 5, "context": "We also tried an attention mechanism similar to the ones shown in [7] [8], which also has the advantage of bringing a high interpretability to the model (i.", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "We also tried an attention mechanism similar to the ones shown in [7] [8], which also has the advantage of bringing a high interpretability to the model (i.", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "It turned out that encoding the laboratory measurements in a binary way by representing each of them as three event types as in [6], i.", "startOffset": 128, "endOffset": 131}, {"referenceID": 16, "context": "Besides we will test our models with two optimization algorithms, which are Adagrad [20] and RMSProp [21].", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "Besides we will test our models with two optimization algorithms, which are Adagrad [20] and RMSProp [21].", "startOffset": 101, "endOffset": 105}, {"referenceID": 4, "context": "We run the experiments using the models presented in this paper and also using Logistic Regression, since it provided the second best performance in the task of predicting sequences of clinical data in [6].", "startOffset": 202, "endOffset": 205}, {"referenceID": 4, "context": "TLE stands for the Temporal Latent Embeddings model [6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "As we mentioned earlier, the Temporal Latent Embeddings model outperformed the baselines presented in [6] for the task of predicting the events that will be observed for each patient in his or her next visit to the clinic (i.", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "Thus we reproduced the experiments presented in [6] including the models based on RNNs introduced in this article.", "startOffset": 48, "endOffset": 51}], "year": 2016, "abstractText": "In clinical data sets we often find static information (e.g. gender of the patients, blood type, etc.) combined with sequences of data that are recorded during multiple hospital visits (e.g. medications prescribed, tests performed, etc.). Recurrent Neural Networks (RNNs) have proven to be very successful for modelling sequences of data in many areas of Machine Learning. In this work we present an approach based on RNNs that is specifically designed for the clinical domain and that combines static and dynamic information in order to predict future events. We work with a database collected in the Charit\u00e9 Hospital in Berlin that contains all the information concerning patients that underwent a kidney transplantation. After the transplantation three main endpoints can occur: rejection of the kidney, loss of the kidney and death of the patient. Our goal is to predict, given the Electronic Health Record of each patient, whether any of those endpoints will occur within the next six or twelve months after each visit to the clinic. We compared different types of RNNs that we developed for this work, a model based on a Feedforward Neural Network and a Logistic Regression model. We found that the RNN that we developed based on Gated Recurrent Units provides the best performance for this task. We also performed an additional experiment using these models to predict next actions and found that for such use case the model based on a Feedforward Neural Network outperformed the other models. Our hypothesis is that long-term dependencies are not as relevant in this task.", "creator": "LaTeX with hyperref package"}}}