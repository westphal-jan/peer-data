{"id": "1602.02697", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Practical Black-Box Attacks against Machine Learning", "abstract": "Deep Learning is increasingly used in several machine learning tasks as Deep Neural Networks (DNNs) frequently outperform other techniques. Yet, previous work showed that, once deployed, DNNs are vulnerable to integrity attacks. Indeed, adversaries can control DNN outputs and for instance force them to misclassify inputs by adding a carefully crafted and undistinguishable perturbation. However, these attacks assumed knowledge of the targeted DNN's architecture and parameters. In this paper however, we release these assumptions and introduce an attack conducted under the more realistic, yet more complex, threat model of an oracle: adversaries are only capable of accessing DNN label predictions for chosen inputs. We evaluate our attack in real-world settings by successfully forcing an oracle served by MetaMind, an online API for DNN classifiers, to misclassify inputs at a 84.24% rate. We also perform an evaluation to fine-tune our attack strategy and maximize the oracle's misclassification rate for adversarial samples.", "histories": [["v1", "Mon, 8 Feb 2016 19:12:25 GMT  (5529kb,D)", "http://arxiv.org/abs/1602.02697v1", null], ["v2", "Fri, 19 Feb 2016 01:49:44 GMT  (5484kb,D)", "http://arxiv.org/abs/1602.02697v2", null], ["v3", "Mon, 7 Nov 2016 00:01:18 GMT  (5724kb,D)", "http://arxiv.org/abs/1602.02697v3", null], ["v4", "Sun, 19 Mar 2017 14:50:18 GMT  (5512kb,D)", "http://arxiv.org/abs/1602.02697v4", "Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security, Abu Dhabi, UAE"]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["nicolas papernot", "patrick mcdaniel", "ian goodfellow", "somesh jha", "z berkay celik", "ananthram swami"], "accepted": false, "id": "1602.02697"}, "pdf": {"name": "1602.02697.pdf", "metadata": {"source": "CRF", "title": "On the Integrity of Deep Learning Oracles", "authors": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow"], "emails": ["ngp5056@cse.psu.edu", "mcdaniel@cse.psu.edu", "goodfellow@google.com", "jha@cs.wisc.edu", "zbc102@cse.psu.edu", "ananthram.swami.civ@mail.mil"], "sections": [{"heading": null, "text": "Deep learning is increasingly used in multiple machine learning tasks, as Deep Neural Networks (DNNs) often outperform other techniques. However, previous work has shown that DNNs are prone to integrity attacks once deployed. In fact, adversaries can control DNN outputs and force them, for example, to misclassify input by adding a carefully constructed and indistinguishable fault. However, these attacks require knowledge of the architecture and parameters of the targeted DNN. In this paper, however, we release these assumptions and launch an attack that is carried out under the more realistic but complex threat model of an oracle: adversaries are only able to access DNN label predictions for selected inputs. We evaluate our attack in real-world environments by successfully forcing an oracle of MetaMind, an online API for DNN classifiers, to misclassify input at a rate of 84.24%."}, {"heading": "1 Introduction", "text": "The fact is that we will be able to manoeuvre ourselves into a situation in which we are able, in which we are able to assert ourselves, in which we are able, in which we are able, in which we are in and in which we are able to assert ourselves, in which we are able to remain in the world in which we are."}, {"heading": "2 About Deep Neural Network Training", "text": "We refer readers interested in a more complete and detailed representation of deep neural networks to the excellent book by Goodfellow et al. [3]. However, a deep neural network, as represented in its simplest form in Figure 1, is a machine learning method that uses a hierarchical composition of n parametric functions to model a high-dimensional input ~ x [3, 4]. Each function for i-1.. n is modeled using a layer of neurons that are essentially elementary computing units that apply an activation function to the previous layer, weighted representation of the input to the output of a new representation \u2212 x. Each neuron layer is parametrically modeled using a weight vector applied to the input of each neuron. Such weights essentially include the knowledge of a deep neural network model F and are evaluated during its training phase, as detailed below."}, {"heading": "3 Threat Model", "text": "We now describe our threat model for attacks against deep learning oracles. The goal is to force a targeted DNN network to avoid wrongdoing: if the network is used for classification, the opponent is able to force the network to make a different input from its correct class. (To achieve this attack, we consider a weak adversary with the only adequate ability to access the results of the targeted neural network.) In other terms, the adversary has no knowledge of the architectural decisions that are made to design the DNN, which include the number, type, and size of layers, nor the training data used to evaluate the parameters of the TheDNN. Since all previous attacks depended on such knowledge, this threat model is particularly challenging, and to the best of our knowledge, it was previously unknown whether a tractable attack was possible."}, {"heading": "4 Attack Methodology", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4.1 Substitute Model Training", "text": "The question we have to ask ourselves is whether our augmentation technology is not aimed at improving the accuracy of the DNN substitute, but rather at ensuring that it approximates the decision limits of the oracle. Figure 3.Substitute Architecture illustrates the training method described below - factor (1) is not severely limiting as the opponent must have at least partial knowledge of the intended oracle (e.g. images, text, web documents) and the expected output (e.g. multi-class classification)."}, {"heading": "4.2 Adversarial Sample Crafting", "text": "Once the opponent has successfully trained a replacement DNN, the next step in our strategy is to create opposing samples. (This part of the attack is carried out by implementing two previously introduced approaches described in [10, 20]. We now offer a brief overview of both opposing sample crafting techniques, namely the Goodfellow et al. selected sample strategies and the Papernot et al. selected attack strategy. Both strategies share a similar intuition of evaluating the sensitivity of the model to input components in order to select a small disturbance that the opposing failure-crafting strategy achieves. Goodfellow et al. Attack - This strategy, also known as the fast gradient drawing method, is suitable for failure-to-miss attacks [10]. Considering a model F with an associated cost function c (F, ~ x, y) the opponent will develop an adversarial sample strategy. Goodfellow et al - Attack - This method, also known as the quick-to-attack method, is the grasp attack method."}, {"heading": "5 Validation", "text": "To test the applicability of our attack strategy under real-world conditions, we first apply it to an oracle provided online by MetaMind, a start-up that provides an online1 classification API built using techniques from deep1The MetaMind API and available online at www.metamind.iolearning.The MetaMind API provides labels created by a trained DNN model for a given input, but does not provide access to the model architecture and parameters. We use the MetaMind API as an oracle DNN classifier for the handwritten MNIST classification task [16]. We show that an adversary implementing our oracle attack strategy can reliably force the oracle provided by MetaMind to discard 84.24% of its inputs by introducing a disorder that is not recognizable to humans."}, {"heading": "5.1 Validation Setup", "text": "The dataset used in this set of validation experiments is the handwritten dataset of the MNIST digits [16]. It consists of 60,000 training samples, 10,000 validation samples and 10,000 test samples. Each sample is an image of a handwritten digit. The task associated with this dataset is to train a classifier to identify the digit written in each image. Each 28x28 grayscale image is encoded as a vector for pixel intensities that are normalized at the interval [0.1] and obtained by reading the image pixel matrix line width. We registered for an API access key on the MetaMind website, which gave us access to the following functionalities: dataset upload, model training and model prediction query. We have the 50,000 samples included in the MNIST training to form the MetaMind servers, which have been transferred to the API classifier, and the MPI."}, {"heading": "5.2 Attack Using a Substitute Training Set extracted from the MNIST test set", "text": "We are a replacement for DNN as described in section 4. (In this case, we have not become accustomed to the way it is solved by an adversary in the threat situation in section 3 with minimal knowledge of the classification task: in our case, handwritten digit classification. Later in section 5.3, we show that replacement training also works with samples that are not extracted from the MNIST database. Of the 150 samples that form the initial replacement training group, 100 are used for training and 50 for validation. The architecture of our replacement DNN is a standard for computer classification tasks: a revolutionary layer of 32 kernels of size 2x2 [9, 15], a pooling layer of 2x2 layers of 2x2 layers."}, {"heading": "5.3 Attack Using a Handcrafted Substitute Training Set", "text": "To confirm this result, we repeat the entire process with another first replacement training set = 34 replacement training sets. We are now using a handmade data set completely independent of the MNIST data set. This shows that even an attacker with limited knowledge of the input can effectively use our strategy to force misclassification from DNN oracles. We repeat the replacement training and opponent pattern-crafting steps with this handmade data set to create samples that have been misclassified by the oracle. Replacement DNN training - We handwrite our own samples by using 10 digits for each class between 0 and 9 using a laptop trackpad and adapt them to the format of the 28x28 grayscale pixels used in the MNIST data set. Examples of these samples are shown in Figure 6. In the third column of Table 1, we report on the accuracy of replacement DNN training, which we do not compare to replacement 4chs."}, {"heading": "6 Conclusions", "text": "Our work is a first and important step toward releasing strong assumptions from previous attacks on DNNs about the enemy's ability to access the architecture and parameters of DNN. Our proposed attack builds on previous attacks and a novel substitute training technique that we have introduced to successfully misclassify enemy samples by oracles. We validated our attack concept by targeting a real-world oracle operated by MetaMind, forcing it to misclassify 84.24% of the enemy's input."}, {"heading": "7 Acknowledgments", "text": "The research was sponsored by the Army Research Laboratory and conducted under the collaboration agreement number W911NF-13-2-0045 (ARL Cyber Security CRA). The views and conclusions contained in this document are those of the authors and should not be interpreted to represent, either explicitly or implicitly, the official guidelines of the Army Research Laboratory or the U.S. Government."}], "references": [{"title": "Predicting the sequence specificities of dna-and rna-binding proteins by deep learning. Nature biotechnology", "author": ["B. ALIPANAHI", "A. DELONG", "M.T. WEIRAUCH", "B.J. FREY"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. BASTIEN", "P. LAMBLIN", "R. PASCANU", "J. BERGSTRA", "I. GOODFELLOW", "A. BERGERON", "N. BOUCHARD", "D. WARDE- FARLEY", "Y. BENGIO"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Deep learning. Book in preparation for", "author": ["I.G.Y. BENGIO", "A. COURVILLE"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Learning deep architectures for ai", "author": ["Y. BENGIO"], "venue": "Foundations and trends R  \u00a9 in Machine Learning 2,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["J. BERGSTRA", "O. BREULEUX", "F. BASTIEN", "P. LAMBLIN", "R. PASCANU", "G. DESJARDINS", "J. TURIAN", "D. WARDE- FARLEY", "Y. BENGIO"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy) (2010),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Multi-column deep neural network for traffic sign classification", "author": ["D. CIRE\u015eAN", "U. MEIER", "J. MASCI", "J. SCHMIDHUBER"], "venue": "Neural Networks", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Largescale malware classification using random projections and neural networks", "author": ["G.E. DAHL", "J.W. STOKES", "L. DENG", "YU"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position", "author": ["K. FUKUSHIMA", "S. MIYAKE"], "venue": "Pattern recognition 15,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1982}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. GOODFELLOW", "J. SHLENS", "C. SZEGEDY"], "venue": "In Proceedings of the 2015 International Conference on Learning Representations", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["HE K", "ZHANG X", "REN S", "SUN"], "venue": "In IEEE International Conference on Computer Vision", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. HINTON", "L. DENG", "D. YU", "G.E. DAHL", "MOHAMED", "A.-R", "N. JAITLY", "A. SENIOR", "V. VANHOUCKE", "P. NGUYEN", "SAINATH", "T. N"], "venue": "Signal Processing Magazine, IEEE 29,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "How paypal beats the bad guys with machine learning", "author": ["E. KNORR"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems", "author": ["A. KRIZHEVSKY", "I. SUTSKEVER", "G.E. HINTON"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LECUN", "L. BOTTOU", "Y. BENGIO", "P. HAFFNER"], "venue": "Proceedings of the IEEE 86,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "The mnist database of handwritten digits", "author": ["Y. LECUN", "C. CORTES"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. MNIH", "K. KAVUKCUOGLU", "D. SILVER", "A. GRAVES", "I. ANTONOGLOU", "D. WIERSTRA", "M. RIEDMILLER"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. NAIR", "G.E. HINTON"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. PAPERNOT", "P. MCDANIEL", "S. JHA", "M. FREDRIKSON", "Z.B. CELIK", "A. SWAMI"], "venue": "In Proceedings of the 1st IEEE European Symposium on Security and Privacy (2016),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. PAPERNOT", "P. MCDANIEL", "X. WU", "S. JHA", "A. SWAMI"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["J. PENNINGTON", "R. SOCHER", "C.D. MANNING"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014)", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "A new method for mapping optimization problems onto neural networks", "author": ["C. PETERSON", "B. S\u00d6DERBERG"], "venue": "International Journal of Neural Systems 1,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1989}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M.A. RANZATO", "F.J. HUANG", "BOUREAU", "Y.-L", "Y. LE- CUN"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Deep boltzmann machines", "author": ["R. SALAKHUTDINOV", "G.E. HINTON"], "venue": "In International Conference on Artificial Intelligence and Statistics", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Recognizing functions in binaries with neural networks", "author": ["E.C.R. SHIN", "D. SONG", "R. MOAZZEZI"], "venue": "In 24th USENIX Security Symposium (USENIX Security", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C. SZEGEDY", "W. ZAREMBA", "I. SUTSKEVER", "J. BRUNA", "D. ER- HAN", "I. GOODFELLOW", "R. FERGUS"], "venue": "In Proceedings of the 2014 International Conference on Learning Representations", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. WERBOS"], "venue": "Neural Networks 1,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}], "referenceMentions": [{"referenceID": 9, "context": "Deep Neural Networks are transforming the field of machine learning by outperforming humans at various tasks like classification [11] and reinforcement learning [17], or allowing for breakthroughs in promising areas like unsupervised learning [25].", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "Deep Neural Networks are transforming the field of machine learning by outperforming humans at various tasks like classification [11] and reinforcement learning [17], or allowing for breakthroughs in promising areas like unsupervised learning [25].", "startOffset": 161, "endOffset": 165}, {"referenceID": 22, "context": "Deep Neural Networks are transforming the field of machine learning by outperforming humans at various tasks like classification [11] and reinforcement learning [17], or allowing for breakthroughs in promising areas like unsupervised learning [25].", "startOffset": 243, "endOffset": 247}, {"referenceID": 2, "context": "This generated a wide interest in deep learning, the machine learning technique which uses neural networks to represent a hierarchy of increasingly complex concepts [3].", "startOffset": 165, "endOffset": 168}, {"referenceID": 22, "context": "The benefits of using hierarchical representations of data include automatic feature extraction from the inputs [25].", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 112, "endOffset": 115}, {"referenceID": 12, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 189, "endOffset": 193}, {"referenceID": 10, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 214, "endOffset": 218}, {"referenceID": 19, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 248, "endOffset": 252}, {"referenceID": 6, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 281, "endOffset": 288}, {"referenceID": 23, "context": "Indeed, deep learning has been adopted by many industries like car manufacturing [19], finance [13], and health [1], as well as many research communities, leading to applications in vision [14], speech recognition [12], natural language processing [22], but also computer security [7, 26].", "startOffset": 281, "endOffset": 288}, {"referenceID": 8, "context": "It has however been shown that deployed deep neural networks are vulnerable to integrity attacks [10, 20, 27].", "startOffset": 97, "endOffset": 109}, {"referenceID": 17, "context": "It has however been shown that deployed deep neural networks are vulnerable to integrity attacks [10, 20, 27].", "startOffset": 97, "endOffset": 109}, {"referenceID": 24, "context": "It has however been shown that deployed deep neural networks are vulnerable to integrity attacks [10, 20, 27].", "startOffset": 97, "endOffset": 109}, {"referenceID": 5, "context": "For instance, consider a DNN used by a car\u2019s driverless system to classify signs identified on the roadside [6].", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "Previous approaches [10, 20, 27] evaluated the perturbations required to craft adversarial samples using knowledge of the DNN architecture and parameters.", "startOffset": 20, "endOffset": 32}, {"referenceID": 17, "context": "Previous approaches [10, 20, 27] evaluated the perturbations required to craft adversarial samples using knowledge of the DNN architecture and parameters.", "startOffset": 20, "endOffset": 32}, {"referenceID": 24, "context": "Previous approaches [10, 20, 27] evaluated the perturbations required to craft adversarial samples using knowledge of the DNN architecture and parameters.", "startOffset": 20, "endOffset": 32}, {"referenceID": 8, "context": "We then leverage the generalizability property of adversarial samples [10]: adversarial samples crafted for a given DNN are also largely misclassified by other DNNs trained to solve the same machine learning task.", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "A Deep Neural Network, as illustrated in its simplest form in Figure 1, is a machine learning technique that uses a hierarchical composition of n parametric functions to model a high dimensional input~x [3, 4].", "startOffset": 203, "endOffset": 209}, {"referenceID": 3, "context": "A Deep Neural Network, as illustrated in its simplest form in Figure 1, is a machine learning technique that uses a hierarchical composition of n parametric functions to model a high dimensional input~x [3, 4].", "startOffset": 203, "endOffset": 209}, {"referenceID": 25, "context": "The adjustment is typically performed using techniques derived from the backpropagation algorithm [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "This model maps images of digits with probability vectors indicating the digit identified in the input image (Figure adapted from [21]).", "startOffset": 130, "endOffset": 134}, {"referenceID": 8, "context": "However, this is not the case in practice as shown by previous attacks manipulating DNN outputs using adversarial samples [10, 20, 27].", "startOffset": 122, "endOffset": 134}, {"referenceID": 17, "context": "However, this is not the case in practice as shown by previous attacks manipulating DNN outputs using adversarial samples [10, 20, 27].", "startOffset": 122, "endOffset": 134}, {"referenceID": 24, "context": "However, this is not the case in practice as shown by previous attacks manipulating DNN outputs using adversarial samples [10, 20, 27].", "startOffset": 122, "endOffset": 134}, {"referenceID": 8, "context": "As all previous attacks depended on such knowledge [10, 20, 27], this threat model is particularly more challenging and, to the best of our knowledge, it was previously unknown whether a tractable attack was possible.", "startOffset": 51, "endOffset": 63}, {"referenceID": 17, "context": "As all previous attacks depended on such knowledge [10, 20, 27], this threat model is particularly more challenging and, to the best of our knowledge, it was previously unknown whether a tractable attack was possible.", "startOffset": 51, "endOffset": 63}, {"referenceID": 24, "context": "As all previous attacks depended on such knowledge [10, 20, 27], this threat model is particularly more challenging and, to the best of our knowledge, it was previously unknown whether a tractable attack was possible.", "startOffset": 51, "endOffset": 63}, {"referenceID": 17, "context": "A taxonomy of threat models for deep learning deployed in adversarial settings is introduced in [20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 8, "context": "Accessing labels \u00d5 is the only capability of adversaries considered in our threat model, unlike previous approaches describing attacks against DNN output integrity [10, 20, 27].", "startOffset": 164, "endOffset": 176}, {"referenceID": 17, "context": "Accessing labels \u00d5 is the only capability of adversaries considered in our threat model, unlike previous approaches describing attacks against DNN output integrity [10, 20, 27].", "startOffset": 164, "endOffset": 176}, {"referenceID": 24, "context": "Accessing labels \u00d5 is the only capability of adversaries considered in our threat model, unlike previous approaches describing attacks against DNN output integrity [10, 20, 27].", "startOffset": 164, "endOffset": 176}, {"referenceID": 8, "context": "In previous work [10, 20, 27], an approximated solution to Equation 2 was found by exploiting a precise understanding of the mapping learned by DNNs between inputs and", "startOffset": 17, "endOffset": 29}, {"referenceID": 17, "context": "In previous work [10, 20, 27], an approximated solution to Equation 2 was found by exploiting a precise understanding of the mapping learned by DNNs between inputs and", "startOffset": 17, "endOffset": 29}, {"referenceID": 24, "context": "In previous work [10, 20, 27], an approximated solution to Equation 2 was found by exploiting a precise understanding of the mapping learned by DNNs between inputs and", "startOffset": 17, "endOffset": 29}, {"referenceID": 8, "context": "Such an understanding was built by exploiting gradients of the DNN\u2019s cost function with respect to network inputs [10, 27] or the DNN\u2019s Jacobian matrix [20].", "startOffset": 114, "endOffset": 122}, {"referenceID": 24, "context": "Such an understanding was built by exploiting gradients of the DNN\u2019s cost function with respect to network inputs [10, 27] or the DNN\u2019s Jacobian matrix [20].", "startOffset": 114, "endOffset": 122}, {"referenceID": 17, "context": "Such an understanding was built by exploiting gradients of the DNN\u2019s cost function with respect to network inputs [10, 27] or the DNN\u2019s Jacobian matrix [20].", "startOffset": 152, "endOffset": 156}, {"referenceID": 8, "context": "To overcome this challenge, we introduce in this paper a new attack against deep neural network integrity that builds on attacks previously described [10, 20, 27], but executable under the oracle threat model described in Section 3.", "startOffset": 150, "endOffset": 162}, {"referenceID": 17, "context": "To overcome this challenge, we introduce in this paper a new attack against deep neural network integrity that builds on attacks previously described [10, 20, 27], but executable under the oracle threat model described in Section 3.", "startOffset": 150, "endOffset": 162}, {"referenceID": 24, "context": "To overcome this challenge, we introduce in this paper a new attack against deep neural network integrity that builds on attacks previously described [10, 20, 27], but executable under the oracle threat model described in Section 3.", "startOffset": 150, "endOffset": 162}, {"referenceID": 8, "context": "The intuition behind our approach comes from the transferability property of adversarial samples: an adversarial sample misclassified by a given DNN architecture A is likely to be misclassified by a different DNN architecture B [10].", "startOffset": 228, "endOffset": 232}, {"referenceID": 8, "context": "Indeed, as adversaries have full knowledge of the DNN architecture and parameters of the substitute network, they can use one of the previously described attack techniques [10, 20] to craft adversarial samples misclassified by F .", "startOffset": 172, "endOffset": 180}, {"referenceID": 17, "context": "Indeed, as adversaries have full knowledge of the DNN architecture and parameters of the substitute network, they can use one of the previously described attack techniques [10, 20] to craft adversarial samples misclassified by F .", "startOffset": 172, "endOffset": 180}, {"referenceID": 8, "context": "This part of the attack is performed by implementing two previously introduced approaches described in [10, 20].", "startOffset": 103, "endOffset": 111}, {"referenceID": 17, "context": "This part of the attack is performed by implementing two previously introduced approaches described in [10, 20].", "startOffset": 103, "endOffset": 111}, {"referenceID": 8, "context": "attack - This strategy, also known as the fast gradient sign method, is suitable for misclassification attacks [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "attack - This strategy is suitable for source-target misclassification attacks where adversaries seek to take samples from any legitimate source class to any chosen target class [20].", "startOffset": 178, "endOffset": 182}, {"referenceID": 14, "context": "We use the MetaMind API as an oracle DNN classifier for the MNIST handwritten digit classification task [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "The dataset used in this set of validation experiments is the MNIST handwritten digit dataset [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "Each 28x28 grayscale image is encoded as a vector of pixel intensities normalized in the interval [0,1] and obtained by reading the image pixel matrix row-wise.", "startOffset": 98, "endOffset": 103}, {"referenceID": 7, "context": "The architecture of our substitute DNN is a standard one for computer vision classification tasks: a convolutional layer of 32 kernels of size 2x2 [9, 15], a max pooling layer of kernel 2x2 [24], a convolutional layer of 64 kernels of size 2x2, a max pooling layer of kernel 2x2, two layers of 200 rectified linear units [18], and a softmax layer to produce a probability vector [23].", "startOffset": 147, "endOffset": 154}, {"referenceID": 13, "context": "The architecture of our substitute DNN is a standard one for computer vision classification tasks: a convolutional layer of 32 kernels of size 2x2 [9, 15], a max pooling layer of kernel 2x2 [24], a convolutional layer of 64 kernels of size 2x2, a max pooling layer of kernel 2x2, two layers of 200 rectified linear units [18], and a softmax layer to produce a probability vector [23].", "startOffset": 147, "endOffset": 154}, {"referenceID": 21, "context": "The architecture of our substitute DNN is a standard one for computer vision classification tasks: a convolutional layer of 32 kernels of size 2x2 [9, 15], a max pooling layer of kernel 2x2 [24], a convolutional layer of 64 kernels of size 2x2, a max pooling layer of kernel 2x2, two layers of 200 rectified linear units [18], and a softmax layer to produce a probability vector [23].", "startOffset": 190, "endOffset": 194}, {"referenceID": 16, "context": "The architecture of our substitute DNN is a standard one for computer vision classification tasks: a convolutional layer of 32 kernels of size 2x2 [9, 15], a max pooling layer of kernel 2x2 [24], a convolutional layer of 64 kernels of size 2x2, a max pooling layer of kernel 2x2, two layers of 200 rectified linear units [18], and a softmax layer to produce a probability vector [23].", "startOffset": 321, "endOffset": 325}, {"referenceID": 20, "context": "The architecture of our substitute DNN is a standard one for computer vision classification tasks: a convolutional layer of 32 kernels of size 2x2 [9, 15], a max pooling layer of kernel 2x2 [24], a convolutional layer of 64 kernels of size 2x2, a max pooling layer of kernel 2x2, two layers of 200 rectified linear units [18], and a softmax layer to produce a probability vector [23].", "startOffset": 379, "endOffset": 383}, {"referenceID": 1, "context": "We use Theano [2, 5] together with Lasagne [8] to train all DNNs in this paper.", "startOffset": 14, "endOffset": 20}, {"referenceID": 4, "context": "We use Theano [2, 5] together with Lasagne [8] to train all DNNs in this paper.", "startOffset": 14, "endOffset": 20}, {"referenceID": 17, "context": "A possible explanation to this phenomenon is that the DNN might more easily classify adversarial samples in these classes, as already observed by previous work on adversarial samples [20].", "startOffset": 183, "endOffset": 187}], "year": 2016, "abstractText": "Deep Learning is increasingly used in several machine learning tasks as Deep Neural Networks (DNNs) frequently outperform other techniques. Yet, previous work showed that, once deployed, DNNs are vulnerable to integrity attacks. Indeed, adversaries can control DNN outputs and for instance force them to misclassify inputs by adding a carefully crafted and undistinguishable perturbation. However, these attacks assumed knowledge of the targeted DNN\u2019s architecture and parameters. In this paper however, we release these assumptions and introduce an attack conducted under the more realistic, yet more complex, threat model of an oracle: adversaries are only capable of accessing DNN label predictions for chosen inputs. We evaluate our attack in real-world settings by successfully forcing an oracle served by MetaMind, an online API for DNN classifiers, to misclassify inputs at a 84.24% rate. We also perform an evaluation to finetune our attack strategy and maximize the oracle\u2019s misclassification rate for adversarial samples.", "creator": "LaTeX with hyperref package"}}}