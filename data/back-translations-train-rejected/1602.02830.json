{"id": "1602.02830", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1", "abstract": "We introduce BinaryNet, a method which trains DNNs with binary weights and activations when computing parameters' gradient. We show that it is possible to train a Multi Layer Perceptron (MLP) on MNIST and ConvNets on CIFAR-10 and SVHN with BinaryNet and achieve nearly state-of-the-art results. At run-time, BinaryNet drastically reduces memory usage and replaces most multiplications by 1-bit exclusive-not-or (XNOR) operations, which might have a big impact on both general-purpose and dedicated Deep Learning hardware. We wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST MLP 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for BinaryNet is available.", "histories": [["v1", "Tue, 9 Feb 2016 01:01:59 GMT  (71kb,D)", "http://arxiv.org/abs/1602.02830v1", "9 pages and 2 figures"], ["v2", "Mon, 29 Feb 2016 21:26:53 GMT  (94kb,D)", "http://arxiv.org/abs/1602.02830v2", "11 pages and 3 figures"], ["v3", "Thu, 17 Mar 2016 14:54:25 GMT  (94kb,D)", "http://arxiv.org/abs/1602.02830v3", "11 pages and 3 figures"]], "COMMENTS": "9 pages and 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["matthieu courbariaux", "itay hubara", "daniel soudry", "ran el-yaniv", "yoshua bengio"], "accepted": false, "id": "1602.02830"}, "pdf": {"name": "1602.02830.pdf", "metadata": {"source": "META", "title": "BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1", "authors": ["Matthieu Courbariaux", "Yoshua Bengio"], "emails": ["MATTHIEU.COURBARIAUX@GMAIL.COM", "YOSHUA.UMONTREAL@GMAIL.COM"], "sections": [{"heading": "Introduction", "text": "Deep Neural Networks (DNNs) have pushed the boundaries of Artificial Intelligence (AI) in a wide range of tasks, including, but not limited to, object recognition from images (Krizhevsky et al., 2012; Szegedy et al., 2014), speech recognition (Hinton et al., 2012; Sainath et al., 2016), statistical machine translation (Devlin et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), Atari and Go games (Mnih et al., 2015; Silver et al., 2016), and even abstract art (Mordvintsev et al., 2015).Today, DNNs are trained almost exclusively on one or many very fast and power-hungry Graphic Processing Units (GPUs) (Coates et al., 2013)."}, {"heading": "1. BinaryNet", "text": "In this section, we explain our binarization function, how we use it to calculate the parameter gradient and how we propagate backwards through it."}, {"heading": "Sign function", "text": "BinaryNet limits both the weights and the activations to either + 1 or \u2212 1. These two values are very advantageous from a hardware point of view, as we explain in Section 3. Our binarization function is simply the character func-ar Xiv: 160 2.02 830v 1 [cs.L G] 9F eb2 01tion: xb = character (x) = {+ 1 if x \u2265 0, \u2212 1 otherwise. (1) where xb is the binarized variable (weight or activation) and x is the real variable. It is very easy to implement and works quite well in practice (see Section 2.) Stochastic binarization could be used as in (Courbariaux et al., 2015) and is theoretically more attractive, but also a more expensive alternative, as it requires the hardware to generate random bits during quantification."}, {"heading": "Gradients computation and accumulation", "text": "An essential point to understand BinaryNet is that while we calculate the gradient of the parameters using binary weights and activations, we still accumulate the real gradient of the weights in real variables, according to algorithm 1. Real weights are probably needed for the Stochasic Gradient Descent (SGD) to work at all. SGD explores the space of the parameters by small and loud steps, and the noise is averaged by the stochastic gradient contributions accumulated in each weight. Therefore, it is important to maintain sufficient resolution for these accumulators, which at first glance suggests that high precision is absolutely necessary. Furthermore, adding noise to weights and activations in the calculation of the parameter gradient provides a form of regulation that can help to better generalize how it was previously demonstrated using variable weight noise (Graves, 2011), Dropout (Srivastava, 2014 and Sneal)."}, {"heading": "Propagating Gradients Through Discretization", "text": "The derivative of the drawing function is almost everywhere, which apparently makes it incompatible with backward propagation, since exact gradients of cost in terms of quantities prior to discretization (pre-activation or weights) would be zero. Note that this is also true when using stochastic quantization. Bengio (2013) investigated the question of estimating or propagating gradients by stochastic discrete neurons. They found in their experiments that the fastest training was achieved using the \"straight-through estimator\" previously introduced in Hinton (2012). We take a similar approach, but use the version of the straight-through estimator that takes into account the saturation effect and uses instead of stochastic sampling of the bit. Consider the drawing function quanti-algorithm 1 Training with BinaryNet. C is the cost function for minibatch, the number of layers."}, {"heading": "A few helpful ingredients", "text": "Some elements of our experiments, although not strictly necessary, significantly improve the accuracy of BinaryNets, as stated in Algorithm 1:"}, {"heading": "2. Benchmark results", "text": "With BinaryNet on MNIST, CIFAR-10 and SVHN benchmarks, we have achieved almost state-of-the-art results and the code for reproducing these results is available 1."}, {"heading": "MLP on MNIST", "text": "MNIST is a benchmark image classification dataset (LeCun et al., 1998). It consists of a training set of 60K and a test set of 10K 28 x 28 grayscale images, digits from 0 to 9. For this benchmark to remain a challenge, we have not used folding, data enlargement, pre-processing or unattended learning.The MLP we train on MNIST consists of 3 hidden layers of 4096 binary units (see Section 1) and an L2 SVM output layer; L2-SVM has proven to be more powerful than Softmax on several classification benchmarks (Tang, 2013; Lee et al., 2014).We regulate the model with Dropout (Srivastava, 2013; Srivastava et al., 2014).The square hinges loss is matched with the ADAM adap-1https: / / github.com / Matthieystava et al. (Srivastava et al, 2014)."}, {"heading": "ConvNet on CIFAR-10", "text": "CIFAR-10 is a benchmark image classification dataset consisting of a 50K training set and a 10K 32 x 32 color test set representing airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships and trucks. We do not use pre-processing or data enlargement (which can really be a game changer for this dataset (Graham, 2014)).The architecture of our ConvNet is in Table 4. It is the same architecture as that of Courbariaux et al. (2015), with the exception of binarization activation. Courbariaux et al. (2015) The architecture of our ConvNet itself is strongly inspired by VGG (Simonyan & Zisserman, 2015).The loss of square hinges is minimized with ADAM. We use an exponentially decreasing learning rate as with MNIST. We scale the weights learning rates with the weight initialization coefficients from VGG (Gloryan & Zisserman, 2015).The loss of square hinges is minimized with ADAM. We use an exponentially decreasing learning rate as with MNIST."}, {"heading": "ConvNet on SVHN", "text": "SVHN is a benchmark data set for image classification. It consists of a training set with 604K examples and a test set with 26K 32 x 32 color images, the digits between 0 and 9. We follow the same procedure we used for CIFAR10, with a few notable exceptions: We use half of the units in the folding layers and we train for 200 epochs instead of 500 (because SVHN is a much larger data set than CIFAR-10). The results are shown in Table 3."}, {"heading": "3. Much faster at run-time", "text": "Required: a vector of 8-bit input a0, the binary weights W b and the BatchNorm parameters \u03b8. Ensure: the MLP output aL. {1. First layer:} a1 \u2190 0 for n = 1 to 8 do a1 \u2190 a1 + 2n \u2212 1 \u00d7 XnorDot product (an0, Wb1) End for ab1 \u2190 characters (BatchNorm (a1, \u03b81) {2. Remaining hidden layers:} for k = 2 to L \u2212 1 do ak \u2190 XnorDot product (abk \u2212 1, W bk) abk \u2190 Sign (BatchNorm (ak, \u041ak)) End for {3. Output layer:} aL \u2190 XnorDot product (abL \u2212 1, W bL) end for this optimized hardware (abk \u2212 1, W bk) end for {3. Output layer:} aL \u2190 XnorDot product (abl \u2212 1, W abk) end for {3. Output layer: \u2190 BatchNork}"}, {"heading": "First layer", "text": "In a BinaryNet, both the weights and the activations are binary. As the output of one layer is the input of the next, all inputs of the layers are binary, except for the first layer. Consequently, we do not believe that this is a big problem. First, the input representation in computer vision typically has many fewer channels (e.g. red, green, and blue) than internal representations (e.g. 512). As a result, the first layer of a ConvNet is often the smallest folding layer, both in terms of parameters and calculations (Szegedy et al., 2014). Second, it is relatively easy to handle continuous inputs as fixed point numbers with m-bit precision. For example, in the usual case of 8-bit fixed point numbers, the first input is: s = x \u00b7 wb (4) s = 1024 x b i (5) s = 1 (8 x n = 1 2xb vb \u00b7 vni \u00b7 vb = 10b = 1 (1) bit = 1 (1) bit = 107 (1) (1) bit = (1 (1) (1) bit = 107 (1 (1)."}, {"heading": "XNOR-accumulate", "text": "The application of a DNN consists mainly of coils and matrix multiplications, so the most important arithmetic operation of deep learning is the multiplier accumulation process. Artificial neurons are basically multipliers that calculate weighted sums of their inputs. BinaryNet allows both activations and weights to be set to either \u2212 1 or + 1. As a result, most of the 32-bit floating-point multiplications are replaced by 1-bit XNOR operations, which could have a huge impact on dedicated hardware for deep learning. Thus, a 32-bit floating-point multiplier costs about 200 FPGA discs (Govindu et al., 2004; Beauchamp et al., 2006), while a 1-bit XNOR gate costs only a single disk."}, {"heading": "7 times faster on GPU at run-time", "text": "The basic idea of SWAR is to combine groups of 32 binary variables into 32-bit registers, thus achieving a 32-fold acceleration of bitwise operations (e.g. XNOR). With SWAR, it is possible to evaluate 32 connections with only 4 statements: a1 + = pop number (not (xor (a 32b 0, w 32b 1)))) (9) Where a1 is the resulting weighted sum, and a32b0 and w 32b 1 the concatenated inputs and weights. These 4 statements take 7 clock cycles on the latest Nvidia GPUs (and if they become a safe statement, it will only require a single clock cycle."}, {"heading": "4. Related work", "text": "In some of their experiments, they also quantify the activations in some parts (but not all) of the calculations. In contrast, we are able to improve the results in terms of hardware (see Section 3). Also, their method is lower than ours (see Figure 1), it yields worse results in terms of MNIS (see Table 1), but the better results in terms of CIFAR-10 and SVHN (see Sections 2 and 3)."}, {"heading": "Conclusion", "text": "We have introduced BinaryNet, a method that trains binary weights DNNs and activations in calculating the gradient of the parameters (see Section 1). We have shown that it is possible to train MLP on MNIST and ConvNets on CIFAR-10 and SVHN on BinaryNet to achieve almost state-of-the-art results (see Section 2). In addition, BinaryNet drastically reduces memory consumption at runtime and replaces most multiplications with 1-bit exclusive non-or (XNOR) operations that could have a major impact on both universal and dedicated deep learning hardware. We have written a GPU kernel for binary matrix multiplication that allows our MNIST-MLP to run seven times faster than with an unoptimized GPU kernel without sacrificing classification accuracy (see Section 3)."}, {"heading": "Acknowledgments", "text": "We thank our colleagues in the MILA lab who took the time to read the article and give us feedback. We thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012), a Python library that enabled us to easily develop a fast and optimized code for GPU. We also thank the developers of Pylearn2 (Goodfellow et al., 2013) and Lasagne (Dieleman et al., 2015), two deep learning libraries based on Theano. We are also grateful for the support of CIFAR, NSERC, IBM and Samsung."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR\u20192015,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Embedded floatingpoint units in FPGAs", "author": ["Beauchamp", "Michael J", "Hauck", "Scott", "Underwood", "Keith D", "Hemmert", "K Scott"], "venue": "In Proceedings of the 2006 ACM/SIGDA 14th international symposium on Field programmable gate arrays,", "citeRegEx": "Beauchamp et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Beauchamp et al\\.", "year": 2006}, {"title": "Estimating or propagating gradients through stochastic neurons", "author": ["Bengio", "Yoshua"], "venue": "Technical Report arXiv:1305.2982, Universite de Montreal,", "citeRegEx": "Bengio and Yoshua.,? \\Q2013\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2013}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Chen", "Yunji", "Luo", "Tao", "Liu", "Shaoli", "Zhang", "Shijin", "He", "Liqiang", "Wang", "Jia", "Li", "Ling", "Tianshi", "Xu", "Zhiwei", "Sun", "Ninghui"], "venue": "In Microarchitecture (MICRO),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Deep learning with COTS HPC systems", "author": ["Coates", "Adam", "Huval", "Brody", "Wang", "Tao", "Wu", "David", "Catanzaro", "Bryan", "Andrew", "Ng"], "venue": "In Proceedings of the 30th international conference on machine learning,", "citeRegEx": "Coates et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2013}, {"title": "JeanPierre. Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua", "David"], "venue": "ArXiv e-prints,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin", "Jacob", "Zbib", "Rabih", "Huang", "Zhongqiang", "Lamar", "Thomas", "Schwartz", "Richard", "Makhoul", "John"], "venue": "In Proc. ACL\u20192014,", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Backpropagation for energy-efficient neuromorphic computing", "author": ["Esser", "Steve K", "Appuswamy", "Rathinakumar", "Merolla", "Paul", "Arthur", "John V", "Modha", "Dharmendra S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Esser et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Esser et al\\.", "year": 2015}, {"title": "Large-scale FPGA-based convolutional networks", "author": ["Farabet", "Cl\u00e9ment", "LeCun", "Yann", "Kavukcuoglu", "Koray", "Culurciello", "Eugenio", "Martini", "Berin", "Akselrod", "Polina", "Talay", "Selcuk"], "venue": "Machine Learning on Very Large Data Sets,", "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Neuflow: A runtime reconfigurable dataflow processor for vision", "author": ["Farabet", "Cl\u00e9ment", "Martini", "Berin", "Corda", "Benoit", "Akselrod", "Polina", "Culurciello", "Eugenio", "LeCun", "Yann"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In AISTATS\u20192010,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Pylearn2: a machine learning research", "author": ["Goodfellow", "Ian J", "Warde-Farley", "David", "Lamblin", "Pascal", "Dumoulin", "Vincent", "Mirza", "Mehdi", "Pascanu", "Razvan", "Bergstra", "James", "Bastien", "Fr\u00e9d\u00e9ric", "Bengio", "Yoshua"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Analysis of high-performance floating-point arithmetic on FPGAs", "author": ["Govindu", "Gokul", "Zhuo", "Ling", "Choi", "Seonil", "Prasanna", "Viktor"], "venue": "In Parallel and Distributed Processing Symposium,", "citeRegEx": "Govindu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Govindu et al\\.", "year": 2004}, {"title": "Spatially-sparse convolutional neural networks", "author": ["Graham", "Benjamin"], "venue": "arXiv preprint arXiv:1409.6070,", "citeRegEx": "Graham and Benjamin.,? \\Q2014\\E", "shortCiteRegEx": "Graham and Benjamin.", "year": 2014}, {"title": "Practical variational inference for neural networks", "author": ["Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves and Alex.,? \\Q2011\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2011}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Neural networks for machine learning", "author": ["Hinton", "Geoffrey"], "venue": "Coursera, video lectures,", "citeRegEx": "Hinton and Geoffrey.,? \\Q2012\\E", "shortCiteRegEx": "Hinton and Geoffrey.", "year": 2012}, {"title": "Fixed-point feedforward deep neural network design using weights+", "author": ["Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Signal Processing Systems (SiPS),", "citeRegEx": "Hwang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": null, "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "X1000 real-time phoneme recognition vlsi using feedforward deep neural networks", "author": ["Kim", "Jonghong", "Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS\u20192012", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "Leon", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed", "author": ["Lee", "Chen-Yu", "Gallagher", "Patrick W", "Tu", "Zhuowen"], "venue": "gated, and tree. arXiv preprint arXiv:1509.08985,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Neural networks with few multiplications", "author": ["Lin", "Zhouhan", "Courbariaux", "Matthieu", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "ArXiv e-prints,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Expectation propagation for approximate bayesian inference", "author": ["Minka", "Thomas P"], "venue": "In UAI\u20192001,", "citeRegEx": "Minka and P.,? \\Q2001\\E", "shortCiteRegEx": "Minka and P.", "year": 2001}, {"title": "Human-level control through deep reinforcement learning", "author": ["len", "Kumaran", "Dharsan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": "Nature, 518:529\u2013533,", "citeRegEx": "len et al\\.,? \\Q2015\\E", "shortCiteRegEx": "len et al\\.", "year": 2015}, {"title": "Neuflow: dataflow vision processing system-on-a-chip", "author": ["Pham", "Phi-Hung", "Jelaca", "Darko", "Farabet", "Clement", "Martini", "Berin", "LeCun", "Yann", "Culurciello", "Eugenio"], "venue": "In Circuits and Systems (MWSCAS),", "citeRegEx": "Pham et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2012}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Deep convolutional neural networks for LVCSR", "author": ["Sainath", "Tara", "rahman Mohamed", "Abdel", "Kingsbury", "Brian", "Ramabhadran", "Bhuvana"], "venue": null, "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["Kavukcuoglu", "Koray", "Graepel", "Thore", "Hassabis", "Demis"], "venue": "search. Nature,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["Soudry", "Daniel", "Hubara", "Itay", "Meir", "Ron"], "venue": "In NIPS\u20192014,", "citeRegEx": "Soudry et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Soudry et al\\.", "year": 2014}, {"title": "Improving neural networks with dropout", "author": ["Srivastava", "Nitish"], "venue": "Master\u2019s thesis, U. Toronto,", "citeRegEx": "Srivastava and Nitish.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava and Nitish.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1958}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In NIPS\u20192014,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "Technical report,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Deep learning using linear support vector machines", "author": ["Tang", "Yichuan"], "venue": "Workshop on Challenges in Representation Learning,", "citeRegEx": "Tang and Yichuan.,? \\Q2013\\E", "shortCiteRegEx": "Tang and Yichuan.", "year": 2013}, {"title": "Improving the speed of neural networks on CPUs", "author": ["Vanhoucke", "Vincent", "Senior", "Andrew", "Mao", "Mark Z"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In ICML\u20192013,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 23, "context": "Deep Neural Networks (DNNs) have substantially pushed Artificial Intelligence (AI) limits in a wide range of tasks, including but not limited to object recognition from images (Krizhevsky et al., 2012; Szegedy et al., 2014), speech recognition (Hinton et al.", "startOffset": 176, "endOffset": 223}, {"referenceID": 38, "context": "Deep Neural Networks (DNNs) have substantially pushed Artificial Intelligence (AI) limits in a wide range of tasks, including but not limited to object recognition from images (Krizhevsky et al., 2012; Szegedy et al., 2014), speech recognition (Hinton et al.", "startOffset": 176, "endOffset": 223}, {"referenceID": 31, "context": ", 2014), speech recognition (Hinton et al., 2012; Sainath et al., 2013), statistical machine translation (Devlin et al.", "startOffset": 28, "endOffset": 71}, {"referenceID": 7, "context": ", 2013), statistical machine translation (Devlin et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), Atari and Go games (Mnih et al.", "startOffset": 41, "endOffset": 109}, {"referenceID": 37, "context": ", 2013), statistical machine translation (Devlin et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), Atari and Go games (Mnih et al.", "startOffset": 41, "endOffset": 109}, {"referenceID": 0, "context": ", 2013), statistical machine translation (Devlin et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), Atari and Go games (Mnih et al.", "startOffset": 41, "endOffset": 109}, {"referenceID": 5, "context": "Today, DNNs are almost exclusively trained on one or many very fast and power-hungry Graphic Processing Units (GPUs) (Coates et al., 2013).", "startOffset": 117, "endOffset": 138}, {"referenceID": 40, "context": "As a result, it is often a challenge to run DNNs on target low-power devices, and much research work is done to speed-up DNNs at run-time on both general-purpose (Vanhoucke et al., 2011; Gong et al., 2014; Romero et al., 2014; Han et al., 2015) and specialized computer hardware (Farabet et al.", "startOffset": 162, "endOffset": 244}, {"referenceID": 12, "context": "As a result, it is often a challenge to run DNNs on target low-power devices, and much research work is done to speed-up DNNs at run-time on both general-purpose (Vanhoucke et al., 2011; Gong et al., 2014; Romero et al., 2014; Han et al., 2015) and specialized computer hardware (Farabet et al.", "startOffset": 162, "endOffset": 244}, {"referenceID": 30, "context": "As a result, it is often a challenge to run DNNs on target low-power devices, and much research work is done to speed-up DNNs at run-time on both general-purpose (Vanhoucke et al., 2011; Gong et al., 2014; Romero et al., 2014; Han et al., 2015) and specialized computer hardware (Farabet et al.", "startOffset": 162, "endOffset": 244}, {"referenceID": 17, "context": "As a result, it is often a challenge to run DNNs on target low-power devices, and much research work is done to speed-up DNNs at run-time on both general-purpose (Vanhoucke et al., 2011; Gong et al., 2014; Romero et al., 2014; Han et al., 2015) and specialized computer hardware (Farabet et al.", "startOffset": 162, "endOffset": 244}, {"referenceID": 29, "context": ", 2015) and specialized computer hardware (Farabet et al., 2011a;b; Pham et al., 2012; Chen et al., 2014a;b; Esser et al., 2015).", "startOffset": 42, "endOffset": 128}, {"referenceID": 8, "context": ", 2015) and specialized computer hardware (Farabet et al., 2011a;b; Pham et al., 2012; Chen et al., 2014a;b; Esser et al., 2015).", "startOffset": 42, "endOffset": 128}, {"referenceID": 6, "context": "Stochastic binarization could be used, as in (Courbariaux et al., 2015), and is more theoretically appealing but is also a more costly alternative because it requires the hardware to generate random bits when quantizing.", "startOffset": 45, "endOffset": 71}, {"referenceID": 41, "context": ", 2014) and DropConnect (Wan et al., 2013).", "startOffset": 24, "endOffset": 42}, {"referenceID": 6, "context": "\u2022 Lastly, scaling the weights\u2019 learning rates with the weights\u2019 initialization coefficients from (Glorot & Bengio, 2010) also seems to help, as suggested by Courbariaux et al. (2015).", "startOffset": 157, "endOffset": 183}, {"referenceID": 6, "context": "33% BinaryConnect (Courbariaux et al., 2015) 1.", "startOffset": 18, "endOffset": 44}, {"referenceID": 6, "context": "40% BinaryConnect (Courbariaux et al., 2015) 8.", "startOffset": 18, "endOffset": 44}, {"referenceID": 25, "context": "27% Gated pooling (Lee et al., 2015) 7.", "startOffset": 18, "endOffset": 36}, {"referenceID": 6, "context": "Intriguingly, BinaryNet is faster to train and yields worse results than BinaryConnect (Courbariaux et al., 2015), suggesting that it is slightly overfitting and might benefit from additional noise (e.", "startOffset": 87, "endOffset": 113}, {"referenceID": 6, "context": "80% BinaryConnect (Courbariaux et al., 2015) 2.", "startOffset": 18, "endOffset": 44}, {"referenceID": 25, "context": "15% Gated pooling (Lee et al., 2015) 1.", "startOffset": 18, "endOffset": 36}, {"referenceID": 24, "context": "MNIST is a benchmark image classification dataset (LeCun et al., 1998).", "startOffset": 50, "endOffset": 70}, {"referenceID": 6, "context": "It is the same architecture as Courbariaux et al. (2015)\u2019s except for the activations binarization.", "startOffset": 31, "endOffset": 57}, {"referenceID": 6, "context": "It is the same architecture as Courbariaux et al. (2015)\u2019s except for the activations binarization. Courbariaux et al. (2015)\u2019s architecture is itself greatly inspired from VGG (Simonyan & Zisserman, 2015).", "startOffset": 31, "endOffset": 126}, {"referenceID": 1, "context": "We can see that our XNOR kernel is significantly faster than our baseline and Theano\u2019s (Bergstra et al., 2010; Bastien et al., 2012) kernels.", "startOffset": 87, "endOffset": 132}, {"referenceID": 38, "context": "As a result, the first layer of a ConvNet is often the smallest convolution layer, both in terms of parameters and computations (Szegedy et al., 2014).", "startOffset": 128, "endOffset": 150}, {"referenceID": 14, "context": "For instance, a 32-bit floating point multiplier costs about 200 FPGA slices (Govindu et al., 2004; Beauchamp et al., 2006), whereas a 1-bit XNOR gate only costs a single slice.", "startOffset": 77, "endOffset": 123}, {"referenceID": 2, "context": "For instance, a 32-bit floating point multiplier costs about 200 FPGA slices (Govindu et al., 2004; Beauchamp et al., 2006), whereas a 1-bit XNOR gate only costs a single slice.", "startOffset": 77, "endOffset": 123}, {"referenceID": 1, "context": "5 times faster than Theano\u2019s (Bergstra et al., 2010; Bastien et al., 2012), as shown in Figure 2.", "startOffset": 29, "endOffset": 74}, {"referenceID": 21, "context": "Hwang & Sung (2014); Kim et al. (2014) retrain neural networks with ternary weights and 3-bit activations, i.", "startOffset": 21, "endOffset": 39}, {"referenceID": 26, "context": ", our training procedure could be hardware accelerated as it only needs very few multiplications, as in (Lin et al., 2015), and our binary DNNs are likely more efficient at run-time.", "startOffset": 104, "endOffset": 122}, {"referenceID": 1, "context": "We thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012), a Python library which allowed us to easily develop a fast and optimized code for GPU.", "startOffset": 34, "endOffset": 79}, {"referenceID": 13, "context": "We also thank the developers of Pylearn2 (Goodfellow et al., 2013) and Lasagne (Dieleman et al.", "startOffset": 41, "endOffset": 66}], "year": 2016, "abstractText": "We introduce BinaryNet, a method which trains DNNs with binary weights and activations when computing parameters\u2019 gradient. We show that it is possible to train a Multi Layer Perceptron (MLP) on MNIST and ConvNets on CIFAR-10 and SVHN with BinaryNet and achieve nearly state-of-the-art results. At run-time, BinaryNet drastically reduces memory usage and replaces most multiplications by 1-bit exclusive-not-or (XNOR) operations, which might have a big impact on both general-purpose and dedicated Deep Learning hardware. We wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST MLP 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for BinaryNet is available.", "creator": "LaTeX with hyperref package"}}}