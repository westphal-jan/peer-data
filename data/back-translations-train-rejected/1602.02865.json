{"id": "1602.02865", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "The Role of Typicality in Object Classification: Improving The Generalization Capacity of Convolutional Neural Networks", "abstract": "Deep artificial neural networks have made remarkable progress in different tasks in the field of computer vision. However, the empirical analysis of these models and investigation of their failure cases has received attention recently. In this work, we show that deep learning models cannot generalize to atypical images that are substantially different from training images. This is in contrast to the superior generalization ability of the visual system in the human brain. We focus on Convolutional Neural Networks (CNN) as the state-of-the-art models in object recognition and classification; investigate this problem in more detail, and hypothesize that training CNN models suffer from unstructured loss minimization. We propose computational models to improve the generalization capacity of CNNs by considering how typical a training image looks like. By conducting an extensive set of experiments we show that involving a typicality measure can improve the classification results on a new set of images by a large margin. More importantly, this significant improvement is achieved without fine-tuning the CNN model on the target image set.", "histories": [["v1", "Tue, 9 Feb 2016 05:30:33 GMT  (2500kb,D)", "http://arxiv.org/abs/1602.02865v1", "In Submission"]], "COMMENTS": "In Submission", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["babak saleh", "ahmed elgammal", "jacob feldman"], "accepted": false, "id": "1602.02865"}, "pdf": {"name": "1602.02865.pdf", "metadata": {"source": "CRF", "title": "The Role of Typicality in Object Classification: Improving The Generalization Capacity of Convolutional Neural Networks", "authors": ["Babak Saleh", "Ahmed Elgammal", "Jacob Feldman"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "To name just a few recent advances, CNN-based models have greatly improved object classification performance. [Simonyan and Zisserman, 2015], object recognition [Ren et al., 2015], image retrieval [Sharif Razavian et al., 2015], and generalization of image descriptions [Vinyals et al., 2014]. Despite overcoming human categorization on large-format visual objects datasets. [Er et al., 2015], Convolution neural networks cannot emulate the generalization of the power of the human visual system in the real world. [Ghodrati et al., 2014]."}, {"heading": "2 Related Work", "text": "The space does not allow an encyclopedic review of the existing literature on \"deep learning,\" but we refer interested readers to extensive literature reviews of [LeCun et al., 2015]. For our research, we focus on Convolutionary Neural Networks [Fukushima, 2013; Krizhevsky et al., 2012; LeCun et al., 1998] as a state-of-the-art, deep learning model for the task of visual recognition. CNN [LeCun et al., 1989] has its roots in Neocognitron [Fukushima, 1980] but is a hierarchical model based on the classical notion of simple and complex cells in visual neuroscience [Hubel and Wiesel, 1962]. CNN, however, has additional hidden layers to model more complex nonlinearities in visual data and their overall architecture, reminiscent of LGN 7 \u2192 V1 \u2192 V2 7 \u2192 V4 \u2192 V4 \u2192 IT hierarchy in the visual cortex."}, {"heading": "3 Computational Framework", "text": "In this section, we will first discuss the theoretical background and convincing theories about learning visual concepts in both areas of psychology and computer vision. We will explain how to measure the typicality of objects in an image, and then propose three hypotheses to use these typicity values to improve the generalization capacity of visual classifiers."}, {"heading": "3.1 Framework Motivation", "text": "People learn a visual object class by looking at examples that are more representative of that object category, or what is called typical examples [Sloman, 1993; Rips, 1975]. It has been shown that children who learn a category by looking at more typical examples can later better recognize their members [Rosh, 1978]. If training examples look more typical, they come close to each other in an underlying space of visual characteristics. This learning strategy not only helps people form a concept, but also allows them to more easily apply the concept to novel images. This great ability of the human visual system allows them to recognize completely different variations of an object, even to the extent of the atypical objects. This suggests that the emphasis on typical examples may be helpful in improving the generalization ability of classifiers."}, {"heading": "3.2 Sample-Based Weighted Loss", "text": "The CNN architecture consists of several blocks, each block having an implicit predictive layer, followed by pooling and normalization layers. On top of these blocks are fully interconnected layers designed to learn more complex structures of object categories. However, the last layer of CNN calculates the \"loss\" as a function of the mismatch between model prediction and soil truth label. CNN's training is formulated as minimizing this loss function [LeCun et al., 1989]. However, our work is the first to look at the effect of sample weighting and the use of various loss functions, including typicality values, to improve the generalization of CNN. We associate each sample X with a weight increase as a function of its typicity, which we will explain later. We build our models by weighting samples based on two loss functions: Softmax protocol and multi-class structure hinge. While the first layer C. is the quickest and most convenient learning layer for our earlier use, it is far more common for the latter problem and for our earlier use."}, {"heading": "3.3 Measuring Typicality of Objects", "text": "We have two approaches to measuring the typicity of objects. On the one hand, we calculate the probability value P (T | X) as typical (T), the object is only based on its visual characteristics X. In the case of class-specific typicity, we can conclude: P (T | X) - P (X | C), where C indicates the category, and independently of the class: P (T | X) - P (X). Then its complement (1 \u2212 P (T | X)))) is the probability of atypical calculation. To implement this probability, we use a class SVM, in which only positive samples of a category (here typical images) are used and there is no negative (atypical) training example. This model can be understood as a density estimation model, in which there is no prior knowledge of the family of the underlying distribution. We learn this class SVM in two scenarios: 1) General class-independent typicity: All images are used; 2) Class-specific object categories: we form a calculation."}, {"heading": "3.4 Hypotheses", "text": "We propose three hypotheses to improve the generalization of visual classifiers, especially when the test image looks substantially different from the training images (atypical). First, inspired by the prototype theories from psychology, we suspect that learning with a stronger emphasis on representative (typical) samples would increase the generalization capacity of visual classification (atypical). Second, learning with a focus on more atypical examples in the training set would increase generalization capacity. Third, we assume that emphasizing both typical and atypical examples could be the key to better generalization performance and should be used for learning visual classifiers. This hypothesis puts additional emphasis on other possible directions of atypicity in training data that may not be at the limit. Third, we assume that emphasizing both typical and atypical examples could be the key to better generalization performance and should be used for learning visual classifiers. The main idea behind this hypothesis is the fact that each type learning object function should be constructed based mainly on the classifier as an atypical one."}, {"heading": "4 Experimental Results", "text": "Datasets: We used three image sets: 1) ImageNet Challenge (ILSVRC 2012 & 2015), 2) Abnormal Object Dataset [Saleh et al., 2013], 3) PASCAL VOC 2011 Train and Validation Set. We conducted our experiments with six object categories: Aeroplane, Boat, Car, Chair, Motorcycle and Sofa. We did this to improve our generalization for atypical images in Abnormal Objects Dataset, which contains these categories. We merged related synsets of ILSVRC 2012 to obtain images of these categories, resulting in 16153 images that we call \"Train Set I.\" Additionally, we experimented with Train and Validation Set of PASCAL 2011. This is necessary because due to a higher level of supervision in PASCAL data collection process, images are rather typical."}, {"heading": "4.1 Comparison of Loss Functions", "text": "In order to find the correct loss function for fine-tuning the network, we conducted an experiment with two losses: Softmax and Multi-structured charge (MS-Scharge) loss. For this experiment, we refined only the last fully connected layer with \"Train Set I.\" Table 2 shows the performance comparison based on different loss functions and weighting methods. We conclude that Multi-structured charge loss (MS-Scharge) performs better than Softmax loss regardless of the weighting strategy. Consequently, the remaining experiments were carried out on the basis of fine-tuning with MS-scharge loss."}, {"heading": "4.2 Comparison of Weighting Functions", "text": "We have conducted a series of experiments to compare the performance of CNN models for the task of object classification when refined with different weight functions. Table 3 shows the result of these experiments on the two test booths of Typical and Atypical. We report on the average accuracy after the first and tenth epochs. While the result of the first epoch shows how quickly the network can learn a category, the tenth epoch works on performance when the network has matured (trains for a longer time). The first epoch in Table 3 shows that the first epoch can do without any weighting; the second epoch shows the weighting of images with a random number between zero and one. The result shows that random training data have no effect on generalization."}, {"heading": "4.3 Investigation of The Effect of Depth", "text": "We investigated the importance of fine-tuning lower levels of CNN to train models with better generalization capacity. Table 5 shows the results of fine-tuning two or three fully interconnected levels of AlexNet. In the first row of each box, we changed the FC7 to 2048 nodes. Likewise, in the second row of each box, we halved the number of nodes in both FC6 and FC7. In all three models (including one reported in previous sections), we used MS hinge loss to learn the parameters of the network. These experiments show that deeper recess can harm the fine-tuned network, especially when tested on atypical images. In part, we would correlate this to the limited number of images available for fine-tuning, so the network fits the training schedule."}, {"heading": "5 Conclusion", "text": "There are several points we can conclude from this study: Atypicity is not a necessary equivalent to borderline sampling, which attempts to emphasize typical loss functions in learning. The main result of this work is that the inclusion of information on the typicity / atypicity of training samples as a weighting term in the loss function significantly contributes to improving the performance of invisible atypical examples in training using only typical examples. We suggest various ways to achieve this weighting of samples on the basis of external (from the sample distribution) and internal signals to the network. We also found that symmetrical weighting of highly typical and highly atypical examples in training improves generalization performance. We believe that this is due to the fact that the typicity / atypicity of the data contains information on the distribution of the samples and therefore includes in generative \"clues\" to the discriminatory classifier. Typicity weighting not only helps the network, but also helps the generalization to learn better, where faster."}], "references": [{"title": "Pixels to voxels: Modeling visual representation in the human brain", "author": ["Pulkit Agrawal", "Dustin Stansbury", "Jitendra Malik", "Jack L Gallant"], "venue": "arXiv preprint arXiv:1407.5104,", "citeRegEx": "Agrawal et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Journal of mathematical psychology", "author": ["F Gregory Ashby", "Leola A Alfonso-Reese. Categorization as probability density estimation"], "venue": "39(2):216\u2013233,", "citeRegEx": "Ashby and Alfonso.Reese. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "In Proceedings of the 26th annual international conference on machine learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston. Curriculum learning"], "venue": "pages 41\u201348. ACM,", "citeRegEx": "Bengio et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Artificial intelligence", "author": ["Avrim L Blum", "Pat Langley. Selection of relevant features", "examples in machine learning"], "venue": "97(1):245\u2013271,", "citeRegEx": "Blum and Langley. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Liefeng Bo", "Xiaofeng Ren", "Dieter Fox. Kernel descriptors for visual recognition"], "venue": "pages 244\u2013252,", "citeRegEx": "Bo et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "PLoS computational biology", "author": ["Charles F Cadieu", "Ha Hong", "Daniel LK Yamins", "Nicolas Pinto", "Diego Ardila", "Ethan A Solomon", "Najib J Majaj", "James J DiCarlo. Deep neural networks rival the representation of primate it cortex for core visual object recognition"], "venue": "10(12),", "citeRegEx": "Cadieu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The Journal of Machine Learning Research", "author": ["Koby Crammer", "Yoram Singer. On the algorithmic implementation of multiclass kernel-based vector machines"], "venue": "2:265\u2013292,", "citeRegEx": "Crammer and Singer. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE,", "citeRegEx": "Deng et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Journal of Experimental Psychology: Human Perception and Performance", "author": ["Jacob Feldman. Bias toward regular form in mental shape spaces"], "venue": "26(1):152,", "citeRegEx": "Feldman. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Neocognitron: A selforganizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["Kunihiko Fukushima"], "venue": "Biological cybernetics, 36(4):193\u2013202,", "citeRegEx": "Fukushima. 1980", "shortCiteRegEx": null, "year": 1980}, {"title": "Artificial vision by multi-layered neural networks: Neocognitron and its advances", "author": ["Kunihiko Fukushima"], "venue": "Neural Networks, 37:103\u2013119,", "citeRegEx": "Fukushima. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Feedforward object-vision models only tolerate small image variations compared to human", "author": ["Masoud Ghodrati", "Amirhossein Farzmahdi", "Karim Rajaei", "Reza Ebrahimpour", "Seyed-Mahdi KhalighRazavi"], "venue": "Frontiers in computational neuroscience,", "citeRegEx": "Ghodrati et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Jitendra Malik", "author": ["Georgia Gkioxari", "Ross Girshick"], "venue": "Contextual action recognition with r*cnn.", "citeRegEx": "Gkioxari et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Cognitive Science", "author": ["Noah D Goodman", "Joshua B Tenenbaum", "Jacob Feldman", "Thomas L Griffiths. A rational analysis of rule-based concept learning"], "venue": "32(1):108\u2013154,", "citeRegEx": "Goodman et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Receptive fields, binocular interaction and functional architecture", "author": ["Hubel", "Wiesel", "1962] David H Hubel", "Torsten N Wiesel"], "venue": null, "citeRegEx": "Hubel et al\\.,? \\Q1962\\E", "shortCiteRegEx": "Hubel et al\\.", "year": 1962}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "but not unsupervised", "author": ["Seyed-Mahdi KhalighRazavi", "Nikolaus Kriegeskorte. Deep supervised"], "venue": "models may explain it cortical representation.", "citeRegEx": "Khaligh.Razavi and Kriegeskorte. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Aude Oliva", "author": ["Aditya Khosla", "Akhil Raju S.", "Antonio Torralba"], "venue": "Understanding and predicting image memorability at a large scale.", "citeRegEx": "Khosla et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in neural information processing systems", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks"], "venue": "pages 1097\u20131105,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Todd M Gureckis", "author": ["Brenden M Lake", "Wojciech Zaremba", "Rob Fergus"], "venue": "Deep neural networks predict category typicality ratings for images.", "citeRegEx": "Lake et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural computation", "author": ["Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition"], "venue": "1(4):541\u2013551,", "citeRegEx": "LeCun et al.. 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Proceedings of the IEEE", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner. Gradient-based learning applied to document recognition"], "venue": "86(11):2278\u20132324,", "citeRegEx": "LeCun et al.. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Nature", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton. Deep learning"], "venue": "521(7553):436\u2013444,", "citeRegEx": "LeCun et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Bilinear cnn models for fine-grained visual recognition", "author": ["Tsung-Yu Lin", "Aruni RoyChowdhury", "Subhransu Maji"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "Lin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Prototypes in category learning: the effects of category size", "author": ["John Paul Minda", "J David Smith"], "venue": "category structure, and stimulus complexity. Journal of Experimental Psychology: Learning, Memory, and Cognition, 27(3):775,", "citeRegEx": "Minda and Smith. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "The Big Book of Concepts (Bradford Books)", "author": ["G.L. Murphy"], "venue": "The MIT Press, March", "citeRegEx": "Murphy. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "and Jeff Clune", "author": ["Anh Nguyen", "Jason Yosinski"], "venue": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.", "citeRegEx": "Nguyen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "similarity", "author": ["Robert M Nosofsky. Choice"], "venue": "and the context theory of classification. Journal of Experimental Psychology: Learning, memory, and cognition, 10(1):104,", "citeRegEx": "Nosofsky. 1984", "shortCiteRegEx": null, "year": 1984}, {"title": "and James J DiCarlo", "author": ["Nicolas Pinto", "David D Cox"], "venue": "Why is real-world visual object recognition hard?", "citeRegEx": "Pinto et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "venue": "arXiv preprint arXiv:1506.01497,", "citeRegEx": "Ren et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Inductive judgments about natural categories", "author": ["L.J. Rips"], "venue": "Journal of verbal learning and verbal behavior, 14:665\u2013 681", "citeRegEx": "Rips. 1975", "shortCiteRegEx": null, "year": 1975}, {"title": "Principles of categorization", "author": ["E. Rosch"], "venue": "E. Rosch and B. Lloyd, editors, Cognition and categorization. Lawrence Erlbaum", "citeRegEx": "Rosch. 1978", "shortCiteRegEx": null, "year": 1978}, {"title": "In Conference on Computer Vision and Pattern Recognition (CVPR)", "author": ["Babak Saleh", "Ali Farhadi", "Ahmed Elgammal. Object-centric anomaly detection by attribute-based reasoning"], "venue": "IEEE,", "citeRegEx": "Saleh et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Overfeat: Integrated recognition", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "localization and detection using convolutional networks. In International Conference on Learning Representations (ICLR 2014), page 16. CBLS,", "citeRegEx": "Sermanet et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "May 7-9", "author": ["Ali Sharif Razavian", "Josephine Sullivan", "Atsuto Maki", "Stefan Carlsson. A baseline for visual instance retrieval with deep convolutional networks. In International Conference on Learning Representations"], "venue": "2015, San Diego, CA. ICLR,", "citeRegEx": "Sharif Razavian et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Zisserman", "2015] Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Feature-based induction", "author": ["S.A. Sloman"], "venue": "Cognitive Psychology, 25:231\u2013280", "citeRegEx": "Sloman. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["Antonio Torralba", "Alexei Efros"], "venue": "Unbiased look at dataset bias. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1521\u2013 1528. IEEE,", "citeRegEx": "Torralba et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Proceedings of the National Academy of Sciences", "author": ["Daniel LK Yamins", "Ha Hong", "Charles F Cadieu", "Ethan A Solomon", "Darren Seibert", "James J DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex"], "venue": "111(23):8619\u20138624,", "citeRegEx": "Yamins et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Object detectors emerge in deep scene cnns", "author": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba"], "venue": "arXiv preprint arXiv:1412.6856,", "citeRegEx": "Zhou et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Do we need more training data? International Journal of Computer Vision", "author": ["Xiangxin Zhu", "Carl Vondrick", "Charless C Fowlkes", "Deva Ramanan"], "venue": "pages 1\u201317,", "citeRegEx": "Zhu et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "To just name few of the recent advances, CNN-based models greatly improved the performance in object classification [Simonyan and Zisserman, 2015], object detection [Ren et al., 2015], image retrieval [Sharif Razavian et al.", "startOffset": 165, "endOffset": 183}, {"referenceID": 35, "context": ", 2015], image retrieval [Sharif Razavian et al., 2015], fine-grained recognition [Lin et al.", "startOffset": 25, "endOffset": 55}, {"referenceID": 24, "context": ", 2015], fine-grained recognition [Lin et al., 2015], scene classification [Zhou et al.", "startOffset": 34, "endOffset": 52}, {"referenceID": 42, "context": ", 2015], scene classification [Zhou et al., 2014], action classification [Gkioxari et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 12, "context": ", 2014], action classification [Gkioxari et al., 2015] and generating image descriptions [Vinyals et al.", "startOffset": 31, "endOffset": 54}, {"referenceID": 40, "context": ", 2015] and generating image descriptions [Vinyals et al., 2014].", "startOffset": 42, "endOffset": 64}, {"referenceID": 14, "context": "Despite surpassing the human categorization performance on large-scale visual object datasets [He et al., 2015], convolution neural networks cannot emulate the generalization power of the human visual system in real-world object categorization [Ghodrati et al.", "startOffset": 94, "endOffset": 111}, {"referenceID": 11, "context": ", 2015], convolution neural networks cannot emulate the generalization power of the human visual system in real-world object categorization [Ghodrati et al., 2014; Pinto et al., 2008], especially when it comes to objects that differ substantially from the training examples.", "startOffset": 140, "endOffset": 183}, {"referenceID": 29, "context": ", 2015], convolution neural networks cannot emulate the generalization power of the human visual system in real-world object categorization [Ghodrati et al., 2014; Pinto et al., 2008], especially when it comes to objects that differ substantially from the training examples.", "startOffset": 140, "endOffset": 183}, {"referenceID": 19, "context": "AlexNet [Krizhevsky et al., 2012] 38.", "startOffset": 8, "endOffset": 33}, {"referenceID": 34, "context": "07 OverFeat [Sermanet et al., 2013] 35.", "startOffset": 12, "endOffset": 35}, {"referenceID": 16, "context": "73 Caffe [Jia et al., 2014] 39.", "startOffset": 9, "endOffset": 27}, {"referenceID": 33, "context": "Humans are capable of perceiving atypical objects and reasoning about them, even though they had not seen them before [Saleh et al., 2013].", "startOffset": 118, "endOffset": 138}, {"referenceID": 39, "context": "One might argue that this issue of cross-dataset generalization is implicitly rooted in dataset biases, and not limited to CNN models [Torralba et al., 2011].", "startOffset": 134, "endOffset": 157}, {"referenceID": 7, "context": "We support our argument by testing same networks on a new set of images that are disjoint from the training set of ImageNet [Deng et al., 2009], but look typical.", "startOffset": 124, "endOffset": 143}, {"referenceID": 26, "context": "Humans begin to form categories and abstractions at an early age[Murphy, 2002].", "startOffset": 64, "endOffset": 78}, {"referenceID": 25, "context": "The mechanisms underlying human category formation are the subject of many competing accounts, including those based on prototypes[Minda and Smith, 2001], exemplars[Nosofsky, 1984], density estimation[Ashby and Alfonso-Reese, 1995], and Bayesian inference[Goodman et al.", "startOffset": 130, "endOffset": 153}, {"referenceID": 28, "context": "The mechanisms underlying human category formation are the subject of many competing accounts, including those based on prototypes[Minda and Smith, 2001], exemplars[Nosofsky, 1984], density estimation[Ashby and Alfonso-Reese, 1995], and Bayesian inference[Goodman et al.", "startOffset": 164, "endOffset": 180}, {"referenceID": 1, "context": "The mechanisms underlying human category formation are the subject of many competing accounts, including those based on prototypes[Minda and Smith, 2001], exemplars[Nosofsky, 1984], density estimation[Ashby and Alfonso-Reese, 1995], and Bayesian inference[Goodman et al.", "startOffset": 200, "endOffset": 231}, {"referenceID": 13, "context": "The mechanisms underlying human category formation are the subject of many competing accounts, including those based on prototypes[Minda and Smith, 2001], exemplars[Nosofsky, 1984], density estimation[Ashby and Alfonso-Reese, 1995], and Bayesian inference[Goodman et al., 2008].", "startOffset": 255, "endOffset": 277}, {"referenceID": 37, "context": "Humans learn object categories and form their visual biases by looking at typical samples [Sloman, 1993; Rips, 1975].", "startOffset": 90, "endOffset": 116}, {"referenceID": 31, "context": "Humans learn object categories and form their visual biases by looking at typical samples [Sloman, 1993; Rips, 1975].", "startOffset": 90, "endOffset": 116}, {"referenceID": 23, "context": "Space does not allow an encyclopedic review of the prior literature on deep learning, but we refer interested readers to literature reviews of [LeCun et al., 2015].", "startOffset": 143, "endOffset": 163}, {"referenceID": 10, "context": "For our research, we focus on convolutional neural networks [Fukushima, 2013; Krizhevsky et al., 2012; LeCun et al., 1998] as the state-ofthe-art deep learning model for the task of object recognition.", "startOffset": 60, "endOffset": 122}, {"referenceID": 19, "context": "For our research, we focus on convolutional neural networks [Fukushima, 2013; Krizhevsky et al., 2012; LeCun et al., 1998] as the state-ofthe-art deep learning model for the task of object recognition.", "startOffset": 60, "endOffset": 122}, {"referenceID": 22, "context": "For our research, we focus on convolutional neural networks [Fukushima, 2013; Krizhevsky et al., 2012; LeCun et al., 1998] as the state-ofthe-art deep learning model for the task of object recognition.", "startOffset": 60, "endOffset": 122}, {"referenceID": 21, "context": "CNN [LeCun et al., 1989] has its roots in Neocognitron [Fukushima, 1980], which is a hierarchical model based on the classic notion of simple and complex cells in visual neuroscience [Hubel and Wiesel, 1962].", "startOffset": 4, "endOffset": 24}, {"referenceID": 9, "context": ", 1989] has its roots in Neocognitron [Fukushima, 1980], which is a hierarchical model based on the classic notion of simple and complex cells in visual neuroscience [Hubel and Wiesel, 1962].", "startOffset": 38, "endOffset": 55}, {"referenceID": 14, "context": "More importantly, even when CNN models achieve human-level performance on visual recognition tasks [He et al., 2015], what will be the difference between computer and human vision? On the one hand, Szegedy et al.", "startOffset": 99, "endOffset": 116}, {"referenceID": 19, "context": "Khaligh-Razavi and Kriegeskorte[2014] studied 37 computational model representations and found that the CNN model of [Krizhevsky et al., 2012] came the closest to explaining the brain representation.", "startOffset": 117, "endOffset": 142}, {"referenceID": 0, "context": "Also it has been shown that CNN models predict human brain activity accurately in early and intermediate stages of visual pathway [Agrawal et al., 2014].", "startOffset": 130, "endOffset": 152}, {"referenceID": 3, "context": "There are some prior works on finding the right features [Blum and Langley, 1997], choosing the appropriate train set and how to order training examples for learning better classifiers [Bengio et al.", "startOffset": 57, "endOffset": 81}, {"referenceID": 2, "context": "There are some prior works on finding the right features [Blum and Langley, 1997], choosing the appropriate train set and how to order training examples for learning better classifiers [Bengio et al., 2009].", "startOffset": 185, "endOffset": 206}, {"referenceID": 43, "context": "This is because the greatest gains in detection performance will continue to derive from improved representations and learning algorithms that can make efficient use of large datasets [Zhu et al., 2015].", "startOffset": 184, "endOffset": 202}, {"referenceID": 37, "context": "Humans learn a visual object class by looking at examples that are more representative for that object category, or what is called typical samples [Sloman, 1993; Rips, 1975].", "startOffset": 147, "endOffset": 173}, {"referenceID": 31, "context": "Humans learn a visual object class by looking at examples that are more representative for that object category, or what is called typical samples [Sloman, 1993; Rips, 1975].", "startOffset": 147, "endOffset": 173}, {"referenceID": 32, "context": "It has been shown that children who learn a category by looking at more typical samples, later can recognize its members better [Rosch, 1978].", "startOffset": 128, "endOffset": 141}, {"referenceID": 8, "context": "Very typical examples are expected to be located close to the mean of each class distribution (center of clouds), with a high probability [Feldman, 2000].", "startOffset": 138, "endOffset": 153}, {"referenceID": 21, "context": "The training of CNN is formulated as minimization of this loss function [LeCun et al., 1989].", "startOffset": 72, "endOffset": 92}, {"referenceID": 6, "context": "This loss function is similar to hinge-loss, but it is computed based on the margin between score of the desired category and all other prediction scores ( \u03c6(i)) [Crammer and Singer, 2002].", "startOffset": 162, "endOffset": 188}, {"referenceID": 33, "context": "Datasets: We used three image datasets: 1) ImageNet challenge (ILSVRC 2012 & 2015), 2) Abnormal Object Dataset [Saleh et al., 2013], 3) PASCAL VOC 2011 train and validation set.", "startOffset": 111, "endOffset": 131}, {"referenceID": 33, "context": "Images of [Saleh et al., 2013] form our \u201ctest atypical\u201d set, which represents confirmed atypical/abnormal objects.", "startOffset": 10, "endOffset": 30}, {"referenceID": 4, "context": "We extracted kernel descriptors of [Bo et al., 2010] at three scales as the input features.", "startOffset": 35, "endOffset": 52}, {"referenceID": 19, "context": "Visual classifier: We evaluated our three hypotheses with the CNN model of AlexNet [Krizhevsky et al., 2012].", "startOffset": 83, "endOffset": 108}, {"referenceID": 16, "context": "We acquired the implementation of Caffe [Jia et al., 2014] for AlexNet and fine-tuned the network for all the following experiments.", "startOffset": 40, "endOffset": 58}, {"referenceID": 18, "context": "The last row of the fourth box, indicates that fine-tuning AlexNet with the memorability score [Khosla et al., 2015] will increase its generalization performance (comparing to baselines).", "startOffset": 95, "endOffset": 116}], "year": 2016, "abstractText": "Deep artificial neural networks have made remarkable progress in different tasks in the field of computer vision. However, the empirical analysis of these models and investigation of their failure cases has received attention recently. In this work, we show that deep learning models cannot generalize to atypical images that are substantially different from training images. This is in contrast to the superior generalization ability of the visual system in the human brain. We focus on Convolutional Neural Networks (CNN) as the state-of-the-art models in object recognition and classification; investigate this problem in more detail, and hypothesize that training CNN models suffer from unstructured loss minimization. We propose computational models to improve the generalization capacity of CNNs by considering how typical a training image looks like. By conducting an extensive set of experiments we show that involving a typicality measure can improve the classification results on a new set of images by a large margin. More importantly, this significant improvement is achieved without finetuning the CNN model on the target image set.", "creator": "LaTeX with hyperref package"}}}