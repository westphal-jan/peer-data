{"id": "1602.02887", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Classification with Boosting of Extreme Learning Machine Over Arbitrarily Partitioned Data", "abstract": "Machine learning based computational intelligence methods are widely used to analyze large scale data sets in this age of big data. Extracting useful predictive modeling from these types of data sets is a challenging problem due to their high complexity. Analyzing large amount of streaming data that can be leveraged to derive business value is another complex problem to solve. With high levels of data availability (\\textit{i.e. Big Data}) automatic classification of them has become an important and complex task. Hence, we explore the power of applying MapReduce based Distributed AdaBoosting of Extreme Learning Machine (ELM) to build a predictive bag of classification models. Accordingly, (i) data set ensembles are created; (ii) ELM algorithm is used to build weak learners (classifier functions); and (iii) builds a strong learner from a set of weak learners. We applied this training model to the benchmark knowledge discovery and data mining data sets.", "histories": [["v1", "Tue, 9 Feb 2016 08:09:26 GMT  (165kb)", "http://arxiv.org/abs/1602.02887v1", "Springer Soft Computing"]], "COMMENTS": "Springer Soft Computing", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ferhat \\\"ozg\\\"ur \\c{c}atak"], "accepted": false, "id": "1602.02887"}, "pdf": {"name": "1602.02887.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ferhat \u00d6zg\u00fcr \u00c7atak"], "emails": ["ozgur.catak@tubitak.gov.tr"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.02 887v 1 [cs.L G] 9F eb2 01Keywords Extreme Learning Machine \u00b7 AdaBoost \u00b7 Ensemble Methods \u00b7 MapReduce"}, {"heading": "1 Introduction", "text": "It is clear that the data that we have made in recent years is not that of ourselves, but that of ourselves, but that of ourselves, that we are able to change the world, \"he told the Deutsche Presse-Agentur."}, {"heading": "2 Related work", "text": "In this section, we describe the general overview of the literature review. Section 2.1 describes the general distributed ELM methods. Section 2.2 shows the MapReduce-based ELM training methods. Section 2.1 Bibliography OverviewMapReduce-based learning algorithms from distributed pieces of data have been studied by many researchers lately. Many different MapReduce-based learning solutions for arbitrarily partitioned data have been proposed recently. Some popular MapReduce-based solutions for training machine learning algorithms in literature include the following. Panda et al. proposed a learning tree model based on a series of distributed calculations and implements each one using the MapReduce model of distributed computation [9]. Zhang et al. develops some algorithms using MapReduce to perform parallel data connections on large data sets [10]. Sun et al. use batch dating based hierarchical linking to reduce data communication [11]."}, {"heading": "2.2.1 ELM\u22c6", "text": "Xin et al. proposed the MapReduce-based ELM training method, called ELM * [14]. Main idea behind this method is to calculate the matrix multiplication of ELM to find a weight vector. They show that Moore-Penrose is the most expensive computational component of the algorithm. As we know, matrix multiplication can be divided into smaller parts. Using this property, they proposed an efficient implementation of the training phase for the management of massive data sets. The final output of this method is a single classification function. In this paper, they proposed two different versions of ELM \u0445, naive and improved. In naive ELM *, the algorithm has two classes, Class Mapper and Class Reducer. Both classes contain only one method. In the improved ELM *, they dissect the calculation of matrix multiplication using the MapReduce framework. In addition, the proposed algorithm reduces the cost of communication pillows used in the Maptic platform."}, {"heading": "2.2.2 OS-ELM based Classification in Hierarchical P2P Network", "text": "Sun et al. proposed the OS-ELM [15] -based distributed ensemble classification in P2P networks [16]. They applied the incremental learning principle of OSELM to hierarchical P2P networks. They proposed two different versions of the ensemble classifier in hierarchical P2P, a one-by-one classification and a parallel ensemble classification. In a learning method, each peer, one by one, calculates the classifier with all the data. Therefore, this approach has a long network delay. In parallel ensemble learning, all classifiers learn in parallel from all the data. Unlike conventional ELM, their experimental results are based on three different real data sets downloaded from the UCI repository."}, {"heading": "2.2.3 Parallel online sequential ELM: POS-ELM", "text": "Wang et al. have proposed a parallel online sequential method for extreme learning machines (POS-ELM) [17]. The main idea of this approach is the analysis of dependency relationships and matrix calculations of OS-ELM [15]. Their experimental results are based on nine different real data sets downloaded from the UCI repository."}, {"heading": "2.2.4 Distributed and Kernelized ELM: DK-ELM", "text": "Bi et al. both distributed and kerelized ELM (DK-ELM) have been proposed, based on MapReduce [18]. The difference between ELM and kerelized ELM is that K-ELM uses cores in the opposite direction to create random feature mappings. They provide a distributed calculation of the RBF kernel matrix in massive data learning applications. Their experimental results are based on four different real data sets downloaded from the UCI repository and four synthetic data sets."}, {"heading": "2.2.5 ELM-MapReduce", "text": "In fact, it is so that it is able to hide in order to achieve the objectives mentioned."}, {"heading": "3 Preliminaries", "text": "The main advantages of the ELM classification method are that ELM can be trained a hundred times faster than traditional neural networks or supporting vector machine algorithms, since their input weights and hidden nodes are generated involuntarily and output layers can be weighted using a method of least squares. The most notable feature of the ELM is that its hidden layer parameters are selected at random."}, {"heading": "4 Proposed Approach", "text": "The basic idea of AdaBoost-ELM is based on the MapReduce implementation of AdaBoosted ELM is described in Section 4.3.4.1. The main task of AdaBoosted ELM is to perform the calculation of AdaBoosted ELM classification method in parallel and distributed manner. The basic idea of AdaBoosted ELM is to calculate the interaction of classification functions using partitioned data (Xm, Ym) in parallel manner. Table 2 provides a summary of the commonly used variables and notations used to evaluate the classification model performance of the AdaBoosted ELM method for convenience. 4.2 Analysis of the proposed Barlett algorithms showed that the size of weights is more important than the size of the neural network."}, {"heading": "5 Experiments", "text": "In this section, we conduct experiments with real datasets from publicly available datasets. In Section 5.1, we explain the datasets and parameters used in experiments. Conventional ELM is applied to all datasets, and we find the accuracy of the number of hidden nodes in Section 5.3. In Section 5.2, we show the empirical results of the proposed distributed ELM training algorithm. 5.1 Experimental SetupIn this section, we apply our approach to five different datasets to test its effectiveness and efficiency. To demonstrate the effectiveness and performance of the proposed model, we apply it to various classification datasets from public datasets. In order to obtain an optimal value of the mapper size, we range it from 20 to 100."}, {"heading": "5.1.1 Commonly Used Classification Data Sets", "text": "We are experimenting with five public datasets summarized in Table 3, including Pendigit, Letter, Statlog, Page Blocks and Waveform. All experiments are repeated 5 times and the results are averaged. All datasets are publicly available in svmlight format on the LIBSVM website [38]. Pendigit dataset is a collection of pen-based detection of handwritten digits [39]. The dataset contains 250 samples from 44 people. The first 7494 cases written by 30 people are used for the dataset, and the numbers written by another 14 people are used for independent testing purpose.Skin dataset is a collection of skin segmentations constructed via R, G, B color space [40]. The dataset contains facial images of various age groups (young, middle, old), genders and racial groups (white, black, Asian). The dataset contains 245057 cases of which is skin."}, {"heading": "5.1.2 Large Scale Classification Data Sets", "text": "The mentioned rf\u00fc the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the"}, {"heading": "5.4.1 Commonly Used Classification Data Sets", "text": "According to these results, the size of AdaBoost T and the size of the mapper have a greater influence on the accuracy of the ELM classifier than the number of hidden nodes in the ELM network.The accuracy of the classification models is represented by color coding of the heatmap by map size (M) - AdaBoost size (T) - map size (M) - number of hidden nodes (nh) - number of hidden nodes (nh) Figure 2 - Figure 6 to show the quantitative differences in the accuracy scale of each dataset. Heatmaps are two-dimensional graphical representations of data with a predefined color chart to show values of a matrix [52]. Heatmaps can be used to understand which parameters affect the accuracy of the classification model."}, {"heading": "5.4.2 Large Scale Classification Data Sets", "text": "To assess the effectiveness of the learning algorithm, time is measured with different mapper sizes. Due to the high dimensionality, the data sets cannot be trained on a single computer. Then, the default percentage of acceleration is modified so that: Sp = targminm-Mtp (16), where targminm-M is the total time on a minimum mapper that can be achieved to build a classification model. As shown in the figure, the data sets lead to an improvement in the learning time of the algorithm. If we examine the observed trends as the number of mappers increases, we can see that a non-linear acceleration is achieved. 5.5 Stability analysis Standard deviation of the test accuracy of the method is shown in Figure 8a and Figure 8b. We analyzed the stability of the Ensemble ELM classifier with two aspects, mapper size and AdaBoost size."}, {"heading": "6 Conclusion and Future Works", "text": "The experimental results show that the AdaBoost ELM-based learning algorithm not only reduces the training time of large data sets, but also the evaluation metrics of accuracy performance compared to conventional ELM. The proposed AdaBoost-based ELM has three different trade-off parameters, namely (i) the data fraction size, (ii) the maximum number of iterations, (T) the AdaBoost algorithm and (iii) the number of hidden layer nodes nh in the ELM algorithm. The empirical results in heatmap numbers show that the parameters M and T are more dominant than the parameters nh for classifying the problems that are considered empirically."}], "references": [{"title": "Extreme learning machine: Theory and applications,", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "Neurocomputing, vol. 70,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Xml document classification based on elm,", "author": ["X.-g. Zhao", "G. Wang", "X. Bi", "P. Gong", "Y. Zhao"], "venue": "Neurocomputing, vol. 74,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "A protein secondary structure prediction framework based on the extreme learning machine,", "author": ["G. Wang", "Y. Zhao", "D. Wang"], "venue": "Neurocomputing, vol. 72,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Face recognition based on extreme learning machine,", "author": ["W. Zong", "G.-B. Huang"], "venue": "Neurocomputing, vol. 74,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "An extreme learning machine approach for speaker recognition,", "author": ["Y. Lan", "Z. Hu", "Y.C. Soh", "G.-B. Huang"], "venue": "Neural Computing and Applications,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Distributed parallel support vector machines in strongly connected networks,", "author": ["Y. Lu", "V. Roychowdhury", "L. Vandenberghe"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Study on parallel svm based on mapreduce,", "author": ["Z. Sun", "G. Fox"], "venue": "in International Conference on Parallel and Distributed Processing Techniques and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Cloudsvm: Training an svm classifier in cloud computing systems,\u201d in Pervasive Computing and the Networked World (Q", "author": ["F. Catak", "M. Balaban"], "venue": "vol. 7719 of Lecture Notes in Computer Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Planet: Massively parallel learning of tree ensembles with mapreduce,", "author": ["B. Panda", "J.S. Herbach", "S. Basu", "R.J. Bayardo"], "venue": "Proc. VLDB Endow.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Efficient parallel knn joins for large data in mapreduce,", "author": ["C. Zhang", "F. Li", "J. Jestes"], "venue": "Proceedings of the 15th International Conference on Extending Database Technology, EDBT \u201912,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "An efficient hierarchical clustering method for large datasets with map-reduce,", "author": ["T. Sun", "C. Shu", "F. Li", "H. Yu", "L. Ma", "Y. Fang"], "venue": "Parallel and Distributed Computing, Applications and Technologies,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Mr-dbscan: An efficient parallel density-based clustering algorithm using mapreduce,", "author": ["Y. He", "H. Tan", "W. Luo", "H. Mao", "D. Ma", "S. Feng", "J. Fan"], "venue": "Parallel and Distributed Systems (ICPADS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Parallel k-means clustering based on mapreduce,", "author": ["W. Zhao", "H. Ma", "Q. He"], "venue": "in Cloud Computing", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Elm: distributed extreme learning machine with mapreduce,", "author": ["J. Xin", "Z. Wang", "C. Chen", "L. Ding", "G. Wang", "Y. Zhao"], "venue": "World Wide Web,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks,", "author": ["N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "An os-elm based distributed ensemble classification framework in {P2P} networks,", "author": ["Y. Sun", "Y. Yuan", "G. Wang"], "venue": "Neurocomputing, vol. 74,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Elm-mapreduce: Mapreduce accelerated extreme learning machine for big spatial data analysis,", "author": ["J. Chen", "G. Zheng", "H. Chen"], "venue": "in Control and Automation (ICCA),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy,", "author": ["L.I. Kuncheva", "C.J. Whitaker"], "venue": "Machine learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Extreme learning machine: A new learning scheme of feedforward neural networks,", "author": ["G. bin Huang", "Q. yu Zhu", "C. kheong Siew"], "venue": "IN PROC. INT. JOINT CONF. NEURAL NETW,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Siew, \u201cUniversal approximation using incremental constructive feedforward networks with random hidden nodes,", "author": ["G.-B. Huang", "L. Chen", "C.-K"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Convex incremental extreme learning machine,", "author": ["G.-B. Huang", "L. Chen"], "venue": "Neurocomputing, vol. 70,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Enhanced random search based incremental extreme learning machine,", "author": ["G.-B. Huang", "L. Chen"], "venue": "Neurocomputing, vol. 71,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Compressed-domain ship detection on spaceborne optical image using deep neural network and extreme learning machine,", "author": ["J. Tang", "C. Deng", "G.-B. Huang", "B. Zhao"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Siew, \u201cIncremental extreme learning machine with fully complex hidden nodes,", "author": ["G.-B. Huang", "M.-B. Li", "L. Chen", "C.-K"], "venue": "Neurocomputing, vol. 71,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting,", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Computational learning theory,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1995}, {"title": "A short introduction to boosting,", "author": ["Y. Freund", "R. Schapire", "N. Abe"], "venue": "JournalJapanese Society For Artificial Intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Double-base asymmetric adaboost,", "author": ["I. Landesa-Vzquez", "J.L. Alba-Castro"], "venue": "Neurocomputing, vol. 118,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Mapreduce: Simplified data processing on large clusters,", "author": ["J. Dean", "S. Ghemawat"], "venue": "Commun. ACM,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "CloudBurst: highly sensitive read mapping with MapReduce.,", "author": ["M.C. Schatz"], "venue": "Bioinformatics (Oxford, England),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Hepdoop: High-energy physics analysis using hadoop,", "author": ["W. Bhimji", "T. Bristow", "A. Washbrook"], "venue": "Journal of Physics: Conference Series,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Privacy preserving large scale dna read-mapping in mapreduce framework using fpgas,", "author": ["L. Xu", "H. Kim", "X. Wang", "W. Shi", "T. Suh"], "venue": "in Field Programmable Logic and Applications (FPL),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network,", "author": ["P. Bartlett"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1998}, {"title": "Vedelsby, \u201cNeural network ensembles, cross validation, and active learning,", "author": ["J.A. Krogh"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1995}, {"title": "Methods of combining multiple classifiers based on different representations for pen-based handwritten digit recognition,", "author": ["F. Alimoglu", "E. Alpaydin"], "venue": "Proceedings of the Fifth Turkish Artificial Intelligence and Artificial Neural Networks Symposium (TAINN", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1996}, {"title": "Efficient skin region segmentation using low complexity fuzzy decision tree model,", "author": ["R. Bhatt", "G. Sharma", "A. Dhall", "S. Chaudhury"], "venue": "in India Conference (INDICON),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "A comparison of methods for multiclass support vector machines,", "author": ["C.-W. Hsu", "C.-J. Lin"], "venue": "Trans. Neur. Netw.,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2002}, {"title": "A further comparison of simplification methods for decision-tree induction,", "author": ["D. Malerba", "F. Esposito", "G. Semeraro"], "venue": "In D. Fisher and H. Lenz (Eds.),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1996}, {"title": "Searching for exotic particles in high-energy physics with deep learning,", "author": ["P. Baldi", "P. Sadowski", "D. Whiteson"], "venue": "Nature communications,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2014}, {"title": "User performance versus precision measures for simple search tasks,", "author": ["A. Turpin", "F. Scholer"], "venue": "Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2006}, {"title": "Performance measures for information extraction,", "author": ["J. Makhoul", "F. Kubala", "R. Schwartz", "R. Weischedel"], "venue": "Proceedings of DARPA Broadcast News Workshop,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1999}, {"title": "Heatmapgenerator: high performance rnaseq and microarray visualization software suite to examine differential gene expression levels using an r and c++ hybrid computational pipeline,", "author": ["B. Khomtchouk", "D. Van Booven", "C. Wahlestedt"], "venue": "Source Code for Biology and Medicine,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Extreme Learning Machine (ELM) was proposed by [1] based on generalized Single-hidden Layer Feedforward Networks (SLFNs).", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "ELM algorithm is used in many different areas including document classification [2], bioinformatics [3] multimedia recognition [4,5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": "ELM algorithm is used in many different areas including document classification [2], bioinformatics [3] multimedia recognition [4,5].", "startOffset": 100, "endOffset": 103}, {"referenceID": 3, "context": "ELM algorithm is used in many different areas including document classification [2], bioinformatics [3] multimedia recognition [4,5].", "startOffset": 127, "endOffset": 132}, {"referenceID": 4, "context": "ELM algorithm is used in many different areas including document classification [2], bioinformatics [3] multimedia recognition [4,5].", "startOffset": 127, "endOffset": 132}, {"referenceID": 5, "context": "[6,7,8].", "startOffset": 0, "endOffset": 7}, {"referenceID": 6, "context": "[6,7,8].", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[6,7,8].", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "proposed a learning tree model which is based on series of distributed computations, and implements each one using the MapReduce model of distributed computation [9].", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "develops some algorithms using MapReduce to perform parallel data joins on large scale data sets [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "use batch updating based hierarchical clustering to reduce computational time and data communication [11].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "They developed a partitioning strategy for large scale non-indexed data with a 4-stages MapReduce paradigm [12].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "proposed parallel k-means clustering based on MapReduce [13].", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "proposed MapReduce based ELM training method called as ELM [14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "proposed OS-ELM [15] based distributed ensemble classification in P2P networks [16].", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "proposed OS-ELM [15] based distributed ensemble classification in P2P networks [16].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "Main idea behind in this approach is to analyze the dependency relationships and the matrix calculations of OS-ELM [15].", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "have been proposed MapReduce based ELM ensemble classifier called ELM-MapReduce, for large scale land cover classification of remote sensing data [19].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "If an ensemble method is applied, then the performance of final model will have better accuracy result [20].", "startOffset": 103, "endOffset": 107}, {"referenceID": 18, "context": "1 Extreme learning machine ELM was originally proposed for the single-hidden layer feedforward neural networks [21,22,1] .", "startOffset": 111, "endOffset": 120}, {"referenceID": 19, "context": "1 Extreme learning machine ELM was originally proposed for the single-hidden layer feedforward neural networks [21,22,1] .", "startOffset": 111, "endOffset": 120}, {"referenceID": 0, "context": "1 Extreme learning machine ELM was originally proposed for the single-hidden layer feedforward neural networks [21,22,1] .", "startOffset": 111, "endOffset": 120}, {"referenceID": 20, "context": "Then, ELM was extended to the generalized single-hidden layer feedforward networks where the hidden layer may not be neuron like [23, 24].", "startOffset": 129, "endOffset": 137}, {"referenceID": 21, "context": "Then, ELM was extended to the generalized single-hidden layer feedforward networks where the hidden layer may not be neuron like [23, 24].", "startOffset": 129, "endOffset": 137}, {"referenceID": 22, "context": "The main advantages of the ELM classification algorithm are that ELM can be trained hundred times faster than traditional neural network or support vector machine algorithm since its input weights and hidden node biases are randomly created and output layer weights can be analytically calculated by using a least-squares method [25,26].", "startOffset": 329, "endOffset": 336}, {"referenceID": 23, "context": "The main advantages of the ELM classification algorithm are that ELM can be trained hundred times faster than traditional neural network or support vector machine algorithm since its input weights and hidden node biases are randomly created and output layer weights can be analytically calculated by using a least-squares method [25,26].", "startOffset": 329, "endOffset": 336}, {"referenceID": 24, "context": "2 AdaBoost The AdaBoost [27] is a supervised learning algorithm designed to solve classification problems [28].", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "2 AdaBoost The AdaBoost [27] is a supervised learning algorithm designed to solve classification problems [28].", "startOffset": 106, "endOffset": 110}, {"referenceID": 26, "context": "Given a space of feature vectors X and two possible class labels, y \u2208 {\u22121,+1}, AdaBoost goal is to learn a strong classifier H(x) as a weighted ensemble of weak classifiers ht(x) predicting the label of any instance x \u2208 X [29].", "startOffset": 222, "endOffset": 226}, {"referenceID": 27, "context": "The MapReduce was originally developed by Google and built on principles in parallel manner [30].", "startOffset": 92, "endOffset": 96}, {"referenceID": 28, "context": "The MapReduce framework automatically executes all those functions in a parallel manner over any number of processors/servers [31].", "startOffset": 126, "endOffset": 130}, {"referenceID": 29, "context": "cyber-security [32,33], high energy physics [34], biology [35].", "startOffset": 44, "endOffset": 48}, {"referenceID": 30, "context": "cyber-security [32,33], high energy physics [34], biology [35].", "startOffset": 58, "endOffset": 62}, {"referenceID": 31, "context": "2 Analysis of the proposed algorithm Barlett showed that the size of the weights is more important than the size of the neural network [36].", "startOffset": 135, "endOffset": 139}, {"referenceID": 32, "context": "also showed that ensemble methods of neural networks get better accuracy performance over unseen examples [37].", "startOffset": 106, "endOffset": 110}, {"referenceID": 33, "context": "Pendigit data set is a collection of pen-based recognition of handwritten digits [39].", "startOffset": 81, "endOffset": 85}, {"referenceID": 34, "context": "Skin data set is a collection of skin segmentation constructed over R, G, B color space [40].", "startOffset": 88, "endOffset": 92}, {"referenceID": 35, "context": "Statlog / Shuttle data set is a collection of space shuttle created by NASA [41].", "startOffset": 76, "endOffset": 80}, {"referenceID": 36, "context": "Page Blocks data set is a collection of page layout of a document that has been detected by a segmentation process [42].", "startOffset": 115, "endOffset": 119}, {"referenceID": 37, "context": "SUSY is a classification data set that distinguish between a signal process which produces supersymmetric particles and a background process which does not [46].", "startOffset": 156, "endOffset": 160}, {"referenceID": 37, "context": "HIGSS is a classification problem to distinguish between a signal process which produces Higgs bosons and a background process which does not [46].", "startOffset": 142, "endOffset": 146}, {"referenceID": 38, "context": "We used four different metrics, the overall prediction accuracy, average recall, average precision [49] and F -score, to evaluate the classification accuracy which are common measurement metrics in information retrieval [50,51].", "startOffset": 99, "endOffset": 103}, {"referenceID": 39, "context": "We used four different metrics, the overall prediction accuracy, average recall, average precision [49] and F -score, to evaluate the classification accuracy which are common measurement metrics in information retrieval [50,51].", "startOffset": 220, "endOffset": 227}, {"referenceID": 40, "context": "Heatmaps are two dimensional graphical representations of data with a pre-defined colormap to display values of a matrix [52].", "startOffset": 121, "endOffset": 125}], "year": 2016, "abstractText": "Machine learning based computational intelligence methods are widely used to analyze large scale data sets in this age of big data. Extracting useful predictive modeling from these types of data sets is a challenging problem due to their high complexity. Analyzing large amount of streaming data that can be leveraged to derive business value is another complex problem to solve. With high levels of data availability (i.e. Big Data) automatic classification of them has become an important and complex task. Hence, we explore the power of applying MapReduce based Distributed AdaBoosting of Extreme Learning Machine (ELM) to build a predictive bag of classification models. Accordingly, (i) data set ensembles are created; (ii) ELM algorithm is used to build weak learners (classifier functions); and (iii) builds a strong learner from a set of weak learners. We applied this training model to the benchmark knowledge discovery and data mining data sets.", "creator": "LaTeX with hyperref package"}}}