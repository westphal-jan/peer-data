{"id": "1602.02888", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Robust Ensemble Classifier Combination Based on Noise Removal with One-Class SVM", "abstract": "In machine learning area, as the number of labeled input samples becomes very large, it is very difficult to build a classification model because of input data set is not fit in a memory in training phase of the algorithm, therefore, it is necessary to utilize data partitioning to handle overall data set. Bagging and boosting based data partitioning methods have been broadly used in data mining and pattern recognition area. Both of these methods have shown a great possibility for improving classification model performance. This study is concerned with the analysis of data set partitioning with noise removal and its impact on the performance of multiple classifier models. In this study, we propose noise filtering preprocessing at each data set partition to increment classifier model performance. We applied Gini impurity approach to find the best split percentage of noise filter ratio. The filtered sub data set is then used to train individual ensemble models.", "histories": [["v1", "Tue, 9 Feb 2016 08:14:29 GMT  (306kb)", "http://arxiv.org/abs/1602.02888v1", "22nd International Conference, ICONIP 2015"]], "COMMENTS": "22nd International Conference, ICONIP 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ferhat \\\"ozg\\\"ur \\c{c}atak"], "accepted": false, "id": "1602.02888"}, "pdf": {"name": "1602.02888.pdf", "metadata": {"source": "CRF", "title": "Robust Ensemble Classifier Combination Based on Noise Removal with One-Class SVM", "authors": ["Ferhat \u00d6zg\u00fcr \u00c7atak"], "emails": ["ozgur.catak@tubitak.gov.tr"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.02 888v 1 [cs.L G] 9F ebKeywords: one-class SVM, data partitioning, noise filtering, Gini contamination, large-scale data classification"}, {"heading": "1 Introduction", "text": "It is clear that we are collecting and storing large amounts of data in databases, and the need for efficient and effective analysis and use of the information contained in the data is increasing. As big data technologies have evolved, the quantity and diversity of data has increased, and they are more focused on storing any type of data. The main purpose of storage is to obtain information from a variety of machine learning methods. One of the primary machine learning methods is classification, which characterizes the new samples based on a training set whose class names are provided [1,2]. Classification methods are used in various areas such as bioinformatics, word processing, social network analysis, etc. New challenges are introduced in traditional classification algorithms to address large-scale databases."}, {"heading": "2 Preliminaries", "text": "The approach presented in this paper uses best-in-class SVM algorithms to remove noisy instances, AdaBoost to create ensemble classification models, and data partitioning to train across all record instances, all of which are briefly presented here."}, {"heading": "2.1 One-Class SVM", "text": "SVM [3] method is used to find classification models using the maximum range that separates the hyperplane. Scholkopf et al. [4] proposed a training method that only treats a class classification called a \"single-class classification.\" One-class SVM algorithm is a method used to determine the outliers in the data. Basically, the method finds soft boundaries of the data set, and then the model determines whether a new instance belongs to that data set or not. Suppose we get a data set x1,.. xm \"X,\" drawn from an unknown underlying probability distribution. We are interested in estimating a set of S so that the probability that a test point of P lies within S with a primary specified probability value. As shown in Figure 1, the origin is designated as \u2212 1, and all training instances are designated as x. \u2212 M. \""}, {"heading": "2.2 AdaBoost", "text": "The AdaBoost [5] is a supervised learning algorithm for solving classification problems [6]. The algorithm uses as input a training set (x1, y1),..., (xn, yn) in which the input sample xi-R p and the output value yi, in a limited space y-1,... The AdaBoost algorithm assumes a set of independently and identically distributed training data (i.i.d.) from an unknown distribution X. In view of a range of feature vectors X and two possible class names y-1, + 1}, the aim of AdaBoost is to learn a strong classification H (x) as a weighted interaction of weak classifiers ht (x), which predict the designation of any instance x-X [7]. H (x) = character (f (x) = character (T-t = 1e (x)) (3)."}, {"heading": "2.3 Data Partitioning Strategies", "text": "When using multiple classifiers, learning methods are applied to basic classifiers using different methods, partitioning the data is done for a variety of reasons. The first is the variety that means uncorrelated base classifiers [8,9]. Another reason is the reduction in the input complexity of large data sets [10]. The last is the creation of classification models for the specific portion of input instances [11]. Partitioning of the data is essentially divided into two different groups: filter-based data partitioning and wrapper-based data partitioning [12]. In wrapper-based data partitioning, partial data sets are created using base classifiers [13]. In filter-based data partitioning, partial data sets are created before individual classifiers are trained [14]."}, {"heading": "3 Proposed Approach", "text": "In this section we present the details of the proposed training method for partial data sets based on noise filters. The basic idea of noise removal based on a single-stage SVM technique is presented in Section 3.1. Analysis of the proposed method is described in Section 3.2."}, {"heading": "3.1 Basic Idea", "text": "Our main task is to divide the input dataset into partial datasets (Xm, Ym) and create local classification ensembles for each partial dataset. Noise removal process is applied to each individual partial dataset as pre-processing. Weighted tuning method is used to combine the individual ensemble classifiers, and then a single classification model is created. Overall, the proposed method is shown in Figure 2."}, {"heading": "3.2 Analysis of the proposed algorithm", "text": "Kragh et al. showed that ensemble methods of neural networks achieve better accuracy than invisible examples [15]. The main motivation for this work is the idea that small ensembles can obtain more precise classification models that are comparable to individual classifiers. In the proposed model, there is a set of classification functions (ensemble classifiers) for each partial dataset, H (m), which acts as a single classification model.The single model for each partial dataset, m, is defined as follows: H (m) (x) = argmax kM \u2211 t = 1\u03b1tht (x) (4) The selected ensemble classification models from the last phase of our algorithm are combined into a single classification model, H (x), using a majority tuning method based on accuracy. H (x) = argmax km \u00b2 i = 1\u03b2H (x) (5), whereby the accuracy of the ensemble classifier is."}, {"heading": "4 Experiments", "text": "In this section, we conduct experiments with real data sets from publicly available data repositories. Public data sets are used to evaluate the proposed learning method. Classification models of each data set are compared for accuracy results without removing disturbing samples from them."}, {"heading": "4.1 Experimental setup", "text": "In this section, our approach is applied to five different data sets to test the effectiveness and efficiency of the model. Data sets are summarized in Table 1, including cod-rna, ijcnn1, letter, shuttle, and SensIT Vehicle. We select 50 as file sharing size, m, and 3 different classification methods, including Extra Trees [16], k-nn, and SVM."}, {"heading": "4.2 Effect of Noise Removing on Input Matrix", "text": "In this section, we show the effects of pre-processing noise removal on the sample data sets. To show the effects of noise removal, we used the \"Gini contamination\" to measure process quality. Gini approaches deal appropriately with the data diversity of a data set. Gini measures the class distribution of the variables y = {y1, \u00b7 \u00b7 \u00b7, ym}. Cleaning results are presented in Figure 3a and Figure 3b. Gini contamination is expected to decrease with the cleaning of the noisy instances from the data and increases with separate noise data. Our goal is to minimize Gini contamination on clean dataset, Xclean, and to maximize the value on noisy dataset setXnoisy. As a result, this division ratio, which minimizes the ratio between the two values, was considered the optimal value. Split-Xcini cleaning table (Gnoisy) shows the best (7)."}, {"heading": "4.3 Simulation Results", "text": "The sequence of experiments is as follows: First, we trained our Noise Removal datasets, then we performed classifications on test datasets and calculated the accuracy of classifiers. We repeated the experiments 50 times and the average accuracy is calculated. Table 3 shows the average accuracy of each sample dataset with and without Noise Removal using a single-class SVM method. As shown in Table 3, the partitioned proposed algorithm based on Noise Removal clearly exceeds the split classification structure in most cases."}, {"heading": "5 Conclusions", "text": "In this work, we have introduced a novel method of data sharing based on the construction of classifiers that improves the partial data sets by removing the noisy instances using a class SVM and finding the best noise-to-impurity ratio with the Gini value. We conducted a series of computer experiments to find a global ensemble classifier and the performance of our proposed methodology. The training process of a distributed data set is simple, fast and conclusive with the classification model for general training instances. Our experimental results show that the storage requirement of the training phase has been significantly reduced and the accuracy increased by using the noise removal process. The proposed method is a practical training model for multiple ensemble classifiers to classify large data sets. In future work, our plan is to investigate various methods of noise removal in order to clean the part set. We plan to design an adaptive noise distance ratio as possible as our method of autonomous removal."}], "references": [{"title": "Machine learning: An artificial intelligence approach", "author": ["J.R. Anderson", "R.S. Michalski", "J.G. Carbonell", "T.M. Mitchell"], "venue": "Volume 2. Morgan Kaufmann", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "Database management systems", "author": ["R. Ramakrishnan", "J. Gehrke"], "venue": "Osborne/McGrawHill", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": "Springer Science & Business Media", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["B. Sch\u00f6lkopf", "J.C. Platt", "J. Shawe-Taylor", "A.J. Smola", "R.C. Williamson"], "venue": "Neural computation 13(7)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Computational learning theory, Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "A short introduction to boosting", "author": ["Y. Freund", "R. Schapire", "N. Abe"], "venue": "JournalJapanese Society For Artificial Intelligence 14(771-780)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Double-base asymmetric adaboost", "author": ["I. Landesa-Vzquez", "J.L. Alba-Castro"], "venue": "Neurocomputing 118(0)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Using diversity measures for generating error-correcting output codes in classifier ensembles", "author": ["L.I. Kuncheva"], "venue": "Pattern Recognition Letters 26(1)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Filter-based data partitioning for training multiple classifier systems", "author": ["R.A. Dara", "M. Makrehchi", "M.S. Kamel"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 22(4)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed learning with bagging-like performance", "author": ["N.V. Chawla", "T.E. Moore", "L.O. Hall", "K.W. Bowyer", "W.P. Kegelmeyer", "C. Springer"], "venue": "Pattern recognition letters 24(1)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Combination of multiple classifiers using local accuracy estimates", "author": ["K. Woods", "K. Bowyer", "W.P. Kegelmeyer Jr"], "venue": "Computer Vision and Pattern Recognition, 1996. Proceedings CVPR\u201996, 1996 IEEE Computer Society Conference on, IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Selection of relevant features and examples in machine learning", "author": ["A.L. Blum", "P. Langley"], "venue": "Artificial intelligence 97(1)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Experiments with a new boosting algorithm", "author": ["Y. Freund", "Schapire", "R.E"], "venue": "ICML. Volume 96.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1996}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine learning 24(2)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Neural network ensembles, cross validation, and active learning", "author": ["A. Krogh", "J. Vedelsby"], "venue": "Advances in Neural Information Processing Systems, MIT Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Extremely randomized trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Machine learning 63(1)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "One of the primary machine learning techniques is classification, which labels the new samples based on a training set whose class labels are provided [1,2].", "startOffset": 151, "endOffset": 156}, {"referenceID": 1, "context": "One of the primary machine learning techniques is classification, which labels the new samples based on a training set whose class labels are provided [1,2].", "startOffset": 151, "endOffset": 156}, {"referenceID": 2, "context": "1 One-Class SVM SVM [3] method is used to find classification models using the maximum margin separating hyper plane.", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "[4] proposed a training methodology that handles only one class classification called as \u201done-class\u201d classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "2 AdaBoost The AdaBoost [5] is a supervised learning algorithm designed to solve classification problems [6].", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "2 AdaBoost The AdaBoost [5] is a supervised learning algorithm designed to solve classification problems [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "Given a space of feature vectorsX and two possible class labels, y \u2208 {\u22121,+1}, AdaBoost goal is to learn a strong classifierH(x) as a weighted ensemble of weak classifiers ht(x) predicting the label of any instance x \u2208 X [7].", "startOffset": 220, "endOffset": 223}, {"referenceID": 7, "context": "First reason is the diversity that means uncorrelated base classifiers [8,9].", "startOffset": 71, "endOffset": 76}, {"referenceID": 8, "context": "First reason is the diversity that means uncorrelated base classifiers [8,9].", "startOffset": 71, "endOffset": 76}, {"referenceID": 9, "context": "Another reason is the reducing the input complexity of large-scale data sets [10].", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "Last one is to build classifier models for the specific part of the input instances [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Data partitioning is basically divided into two different groups; filter based data partitioning and wrapper based data partitioning [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "In wrapper based data partitioning, sub-data sets are created using base classifier outputs [13].", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "In filter based data partitioning, sub-data sets are created before individual classifiers are trained [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "showed that ensemble methods of neural networks gets better accuracy performance over unseen examples [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 15, "context": "We choose 50 as the data split size, m, and 3 different classification methods including Extra Trees [16], k-nn and SVM.", "startOffset": 101, "endOffset": 105}], "year": 2016, "abstractText": "In machine learning area, as the number of labeled input samples becomes very large, it is very difficult to build a classification model because of input data set is not fit in a memory in training phase of the algorithm, therefore, it is necessary to utilize data partitioning to handle overall data set. Bagging and boosting based data partitioning methods have been broadly used in data mining and pattern recognition area. Both of these methods have shown a great possibility for improving classification model performance. This study is concerned with the analysis of data set partitioning with noise removal and its impact on the performance of multiple classifier models. In this study, we propose noise filtering preprocessing at each data set partition to increment classifier model performance. We applied Gini impurity approach to find the best split percentage of noise filter ratio. The filtered sub data set is then used to train individual ensemble models.", "creator": "LaTeX with hyperref package"}}}