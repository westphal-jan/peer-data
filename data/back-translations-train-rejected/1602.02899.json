{"id": "1602.02899", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Secure Multi-Party Computation Based Privacy Preserving Extreme Learning Machine Algorithm Over Vertically Distributed Data", "abstract": "Especially in the Big Data era, the usage of different classification methods is increasing day by day. The success of these classification methods depends on the effectiveness of learning methods. Extreme learning machine (ELM) classification algorithm is a relatively new learning method built on feed-forward neural-network. ELM classification algorithm is a simple and fast method that can create a model from high-dimensional data sets. Traditional ELM learning algorithm implicitly assumes complete access to whole data set. This is a major privacy concern in most of cases. Sharing of private data (i.e. medical records) is prevented because of security concerns. In this research, we propose an efficient and secure privacy-preserving learning algorithm for ELM classification over data that is vertically partitioned among several parties. The new learning method preserves the privacy on numerical attributes, builds a classification model without sharing private data without disclosing the data of each party to others.", "histories": [["v1", "Tue, 9 Feb 2016 08:37:26 GMT  (431kb)", "http://arxiv.org/abs/1602.02899v1", "22nd International Conference, ICONIP 2015"]], "COMMENTS": "22nd International Conference, ICONIP 2015", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["ferhat \\\"ozg\\\"ur \\c{c}atak"], "accepted": false, "id": "1602.02899"}, "pdf": {"name": "1602.02899.pdf", "metadata": {"source": "CRF", "title": "Secure Multi-Party Computation Based Privacy Preserving Extreme Learning Machine Algorithm Over Vertically Distributed Data", "authors": ["Ferhat \u00d6zg\u00fcr \u00c7atak"], "emails": ["ozgur.catak@tubitak.gov.tr"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.02 899v 1 [cs.C R] 9Keywords: extreme learning machine, privacy preserving data analysis, secure multi-party calculation"}, {"heading": "1 Introduction", "text": "The main purpose of machine learning is to find patterns and summarize the data form of the high-dimensional datasets. Classification algorithms [1,2] are one of the most widely used methods of machine learning in real problems. Therefore, the datasets used in real problems are high-dimensional, their analysis is a complicated process. Extreme Learning Machine (ELM) has been proposed by [3] generalized single-hidden layer feed-forward networks (SLFNs). Main features of ELM are short training times compared to conventional gradient-based learning methods, high generalizations and invisible examples with multicultural labels and parameters that are free of hidden notes. Background knowledge attributes of a dataset and reduce the possible values of output-sensitive information."}, {"heading": "2 Related Works", "text": "In this section, we review the existing work developed for different methods of machine learning, highlighting the major differences between our learning model and existing work. Recently, there have been significant contributions in the field of privacy-preserving machine learning. Secretans et al. [9] presents a probabilistic neural network (PNN) model. PNN is an approximation of the theoretically optimal classifier, the so-called Bayesian optimal classifier. At least three parties are involved in the calculation of the sum of the safe matrix to merge the partial probability vectors. Aggarwal et al et al. [10] developed a condensation method. They show that anonymized data closely correspond to the characteristics of the original data. Samet al. [11] present new data-preserving protocols for both reverse dissemination and for ELM algorithms between several parties. The protocols are presented for the perception of learning algorithms, and only layered models for privacy are presented [12]."}, {"heading": "3 Preliminaries", "text": "In this section we briefly present preliminary knowledge about ELM, secure multi-party calculation and secure completion."}, {"heading": "3.1 Extreme learning machine", "text": "ELM was originally proposed for the single-layer forward-facing neural networks [15,16,3] = hidden Y-layers = X-layers. Then, ELM was extended to the generalized single-layer feed networks, where the hidden layer must not be neuron like [17,18].The main advantages of the ELM classification algorithm are that ELM can be trained a hundred times faster than traditional neural networks or support vector machine algorithms, since its input weights and hidden nodes are generated randomly and the output layer weights can be calculated using a method of the smallest squares [19,20]. The most notable feature of ELM is that its hidden layer parameters are selected randomly."}, {"heading": "3.2 Secure Multi Party Computation", "text": "In vertically distributed data, each party has different attributes of the same data set. Let's have n input instances, D = {(xi, yi) | xi, yi, yi, R} ni = 1. The partition strategy is shown in Figure 1. Secure multi-party addition. In Secure multi-party addition (SMA), each party, Pi, has a private local value, xi. at the end of the calculation, we get the sum, x = advantageous k \u2212 1i = 0. To do this, we apply the secure addition method Yu et al. [21]. Their approach is a generalization of existing work [22], which uses secure communication and trusted party. Canonical order based, P0, \u00b7 \u00b7 \u00b7, Pk \u2212 1, protocol is applied. The SMA method is shown in algorithm 1. [21] This protocol calculates the required sum in a secure manner. \u2212 Algorithm V \u2212 Secure multi-party addition 1: PMA (random number): 2 \u00b7 PR \u00b7 P: 1 (1): PR \u00b7 V: 1 (random number)."}, {"heading": "4 Privacy-preserving ELM over vertically partitioned data", "text": "The dataset you want to find for radial functions consists of m instances in n-dimensional space (b) x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "5 Experiments", "text": "In this section, we conduct experiments with real data sets from publicly available data set repositories. Public data sets are used to evaluate the proposed learning method. Classification models of each data set are compared for accuracy results without using secure multi-party calculations. Experimental setup: In this section, our approach is applied to six different data sets to verify the affectivity and efficiency of the model.The data sets are summarized in Table 1, including Australians, colon cancer, diabetes, Duke, heart, ionosphere. For each data set in Table 1, we vary the number of party size, k from 2 to the number of characteristics, n, of the dataset. For example, if our party size is three, k = 3 and the attribute size is fourteen, n = 14, then the first two parties have 5 attributes and the last party 4 attributes. Simulation results: The accuracy of safe multi-party calculation based on the ELM algorithm is exactly equal to the number of ELM in traditional ELM algorithm formation."}, {"heading": "6 Conclusion and Future Works", "text": "ELM learning algorithm, a new method compared to other classification algorithms. ELM surpasses traditional single layer feed-forward neural networks and supports vector machines for big data [29]. ELM is used in many areas. Privacy is an important aspect in almost all areas where ELM is used (e.g. medical records, companies, government). For ELM, a new privacy-preserving learning model is proposed in vertically partitioned data for multi-party partitioning without passing the data from each site to the others. To protect the privacy of the input data, the master divides the weight vector, and each party calculates the activation result with its data and weight vectors. Extending the data-preserving ELM to horizontally distributed data sets is a future work for this approach."}], "references": [{"title": "Machine learning: An artificial intelligence approach", "author": ["J.R. Anderson", "R.S. Michalski", "J.G. Carbonell", "T.M. Mitchell"], "venue": "Volume 2. Morgan Kaufmann", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "Database management systems", "author": ["R. Ramakrishnan", "J. Gehrke"], "venue": "Osborne/McGrawHill", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Extreme learning machine: Theory and applications", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing 70(13)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "k-anonymity: A model for protecting privacy", "author": ["L. Sweeney"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 10(05)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "L-diversity: Privacy beyond k-anonymity", "author": ["A. Machanavajjhala", "D. Kifer", "J. Gehrke", "M. Venkitasubramaniam"], "venue": "ACM Trans. Knowl. Discov. Data 1(1)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "t-closeness: Privacy beyond k-anonymity and l-diversity", "author": ["N. Li", "T. Li", "S. Venkatasubramanian"], "venue": "Data Engineering, 2007. ICDE 2007. IEEE 23rd International Conference on, IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Differential privacy and machine learning: a survey and review", "author": ["Z. Ji", "Z.C. Lipton", "C. Elkan"], "venue": "arXiv preprint arXiv:1412.7584", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Secure multiparty computation for privacy-preserving data mining", "author": ["Y. Lindell", "B. Pinkas"], "venue": "Journal of Privacy and Confidentiality 1(1)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "A privacy preserving probabilistic neural network for horizontally partitioned databases", "author": ["J. Secretan", "M. Georgiopoulos", "J. Castro"], "venue": "Neural Networks, 2007. IJCNN 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "A condensation approach to privacy preserving data mining", "author": ["C. Aggarwal", "P. Yu"], "venue": "In Bertino, E., Christodoulakis, S., Plexousakis, D., Christophides, V., Koubarakis, M., Bhm, K., Ferrari, E., eds.: Advances in Database Technology - EDBT 2004. Volume 2992 of Lecture Notes in Computer Science. Springer Berlin Heidelberg", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Privacy-preserving back-propagation and extreme learning machine algorithms", "author": ["S. Samet", "A. Miri"], "venue": "Data Knowl. Eng. 79-80", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Privacy preserving clustering by data transformation", "author": ["S.R. Oliveira", "O.R. Zaiane"], "venue": "Journal of Information and Data Management 1(1)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "A privacy preserving neural network learning algorithm for horizontally partitioned databases", "author": ["L. Guang", "W. Ya-Dong", "S. Xiao-Hong"], "venue": "Inform. Technol. J 9", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Privacy-preserving svm using nonlinear kernels on horizontally partitioned data", "author": ["H. Yu", "X. Jiang", "J. Vaidya"], "venue": "Proceedings of the 2006 ACM Symposium on Applied Computing. SAC \u201906, New York, NY, USA, ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme learning machine: A new learning scheme of feedforward neural networks", "author": ["G. bin Huang", "Q. yu Zhu", "C. kheong Siew"], "venue": "IN PROC. INT. JOINT CONF. NEURAL NETW.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Universal approximation using incremental constructive feedforward networks with random hidden nodes", "author": ["G.B. Huang", "L. Chen", "C.K. Siew"], "venue": "Neural Networks, IEEE Transactions on 17(4)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex incremental extreme learning machine", "author": ["G.B. Huang", "L. Chen"], "venue": "Neurocomputing 70(1618)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Enhanced random search based incremental extreme learning machine", "author": ["G.B. Huang", "L. Chen"], "venue": "Neurocomputing 71(1618)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Compressed-domain ship detection on spaceborne optical image using deep neural network and extreme learning machine", "author": ["J. Tang", "C. Deng", "G.B. Huang", "B. Zhao"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 53(3)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental extreme learning machine with fully complex hidden nodes", "author": ["G.B. Huang", "M.B. Li", "L. Chen", "C.K. Siew"], "venue": "Neurocomputing 71(46)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Privacy-preserving svm classification on vertically partitioned data", "author": ["H. Yu", "J. Vaidya", "X. Jiang"], "venue": "Advances in Knowledge Discovery and Data Mining. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Multiparty computation for randomly ordering players and making random selections", "author": ["L. Sweeney", "M. Shamos"], "venue": "Technical report, Carnegie Mellon University", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Simplifying decision trees", "author": ["J.R. Quinlan"], "venue": "International journal of man-machine studies 27(3)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1987}, {"title": "Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays", "author": ["U. Alon", "N. Barkai", "D.A. Notterman", "K. Gish", "S. Ybarra", "D. Mack", "A.J. Levine"], "venue": "Proceedings of the National Academy of Sciences 96(12)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Using the adap learning algorithm to forecast the onset of diabetes mellitus", "author": ["J.W. Smith", "J. Everhart", "W. Dickson", "W. Knowler", "R. Johannes"], "venue": "Proceedings of the Annual Symposium on Computer Application in Medical Care, American Medical Informatics Association", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1988}, {"title": "Predicting the clinical status of human breast cancer by using gene expression profiles", "author": ["M. West", "C. Blanchette", "H. Dressman", "E. Huang", "S. Ishida", "R. Spang", "H. Zuzan", "J.A. Olson", "J.R. Marks", "J.R. Nevins"], "venue": "Proceedings of the National Academy of Sciences 98(20)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Statlog (heart) data set", "author": ["UCI"], "venue": "https://archive.ics.uci.edu/ml/datasets/Statlog+(Heart)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Classification of radar returns from the ionosphere using neural networks", "author": ["V.G. Sigillito", "S.P. Wing", "L.V. Hutton", "K.B. Baker"], "venue": "Johns Hopkins APL Technical Digest 10", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1989}, {"title": "Extreme learning machines [trends controversies", "author": ["E. Cambria", "Huang", "G.B.e.a."], "venue": "Intelligent Systems, IEEE 28(6)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The classification algorithms [1,2] is one of the most widely used method of machine learning in real-life problems.", "startOffset": 30, "endOffset": 35}, {"referenceID": 1, "context": "The classification algorithms [1,2] is one of the most widely used method of machine learning in real-life problems.", "startOffset": 30, "endOffset": 35}, {"referenceID": 2, "context": "Extreme Learning Machine (ELM) was proposed by [3] based on generalized Single-hidden Layer Feed-forward Networks (SLFNs).", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "example about background knowledge attack is the personal health information about of Massachusetts governor William Weld using a anonymized data set [4].", "startOffset": 150, "endOffset": 153}, {"referenceID": 3, "context": "In order to overcome these types of attacks, various anonymization methods have been developed like k -anonymity [4], l -diversity [5], t -closeness [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "In order to overcome these types of attacks, various anonymization methods have been developed like k -anonymity [4], l -diversity [5], t -closeness [6].", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "In order to overcome these types of attacks, various anonymization methods have been developed like k -anonymity [4], l -diversity [5], t -closeness [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 6, "context": "Although the anonymization methods are applied to data sets to protect sensitive data, though, the sensitive data is still accessed by an attacker in various ways [7].", "startOffset": 163, "endOffset": 166}, {"referenceID": 7, "context": "In another scenario, consider the situation when two or more hospitals wants to analyze patient data [8] through collaborative processes that require using each other\u2019s databases.", "startOffset": 101, "endOffset": 104}, {"referenceID": 8, "context": "[9] presents a probabilistic neural network (PNN) model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] developed condensation based learning method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] present new privacy-preserving protocols for both the back-propagation and ELM algorithms among several parties.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] proposed methods distort confidential numerical features to protect privacy for clustering analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] proposed a privacy-preserving back-propagation algorithm for horizontally partitioned databases for multi-party case.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] proposed a privacy-preserving solution for support", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "1 Extreme learning machine ELM was originally proposed for the single-hidden layer feed-forward neural networks [15,16,3] .", "startOffset": 112, "endOffset": 121}, {"referenceID": 15, "context": "1 Extreme learning machine ELM was originally proposed for the single-hidden layer feed-forward neural networks [15,16,3] .", "startOffset": 112, "endOffset": 121}, {"referenceID": 2, "context": "1 Extreme learning machine ELM was originally proposed for the single-hidden layer feed-forward neural networks [15,16,3] .", "startOffset": 112, "endOffset": 121}, {"referenceID": 16, "context": "Then, ELM was extended to the generalized single-hidden layer feed-forward networks where the hidden layer may not be neuron like [17,18].", "startOffset": 130, "endOffset": 137}, {"referenceID": 17, "context": "Then, ELM was extended to the generalized single-hidden layer feed-forward networks where the hidden layer may not be neuron like [17,18].", "startOffset": 130, "endOffset": 137}, {"referenceID": 18, "context": "Main advantages of ELM classification algorithm is that ELM can be trained hundred times faster than traditional neural network or support vector machine algorithm since its input weights and hidden node biases are randomly created and output layer weights can be analytically calculated by using a least-squares method [19,20].", "startOffset": 320, "endOffset": 327}, {"referenceID": 19, "context": "Main advantages of ELM classification algorithm is that ELM can be trained hundred times faster than traditional neural network or support vector machine algorithm since its input weights and hidden node biases are randomly created and output layer weights can be analytically calculated by using a least-squares method [19,20].", "startOffset": 320, "endOffset": 327}, {"referenceID": 20, "context": "[21] secure addition procedure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Their approach is a generalization of the existing works [22] that uses secure communication and trusted party.", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "Data set #Train #Classes #Attributes australian [23] 690 2 14 colon-cancer [24] 62 2 2,000 diabetes [25] 768 2 8 duke breast cancer [26] 44 2 7,129 heart [27] 270 2 13 ionosphere [28] 351 2 34", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "Data set #Train #Classes #Attributes australian [23] 690 2 14 colon-cancer [24] 62 2 2,000 diabetes [25] 768 2 8 duke breast cancer [26] 44 2 7,129 heart [27] 270 2 13 ionosphere [28] 351 2 34", "startOffset": 75, "endOffset": 79}, {"referenceID": 24, "context": "Data set #Train #Classes #Attributes australian [23] 690 2 14 colon-cancer [24] 62 2 2,000 diabetes [25] 768 2 8 duke breast cancer [26] 44 2 7,129 heart [27] 270 2 13 ionosphere [28] 351 2 34", "startOffset": 100, "endOffset": 104}, {"referenceID": 25, "context": "Data set #Train #Classes #Attributes australian [23] 690 2 14 colon-cancer [24] 62 2 2,000 diabetes [25] 768 2 8 duke breast cancer [26] 44 2 7,129 heart [27] 270 2 13 ionosphere [28] 351 2 34", "startOffset": 132, "endOffset": 136}, {"referenceID": 26, "context": "Data set #Train #Classes #Attributes australian [23] 690 2 14 colon-cancer [24] 62 2 2,000 diabetes [25] 768 2 8 duke breast cancer [26] 44 2 7,129 heart [27] 270 2 13 ionosphere [28] 351 2 34", "startOffset": 154, "endOffset": 158}, {"referenceID": 27, "context": "Data set #Train #Classes #Attributes australian [23] 690 2 14 colon-cancer [24] 62 2 2,000 diabetes [25] 768 2 8 duke breast cancer [26] 44 2 7,129 heart [27] 270 2 13 ionosphere [28] 351 2 34", "startOffset": 179, "endOffset": 183}], "year": 2016, "abstractText": "Especially in the Big Data era, the usage of different classification methods is increasing day by day. The success of these classification methods depends on the effectiveness of learning methods. Extreme learning machine (ELM) classification algorithm is a relatively new learning method built on feed-forward neural-network. ELM classification algorithm is a simple and fast method that can create a model from high-dimensional data sets. Traditional ELM learning algorithm implicitly assumes complete access to whole data set. This is a major privacy concern in most of cases. Sharing of private data (i.e. medical records) is prevented because of security concerns. In this research, we propose an efficient and secure privacy-preserving learning algorithm for ELM classification over data that is vertically partitioned among several parties. The new learning method preserves the privacy on numerical attributes, builds a classification model without sharing private data without disclosing the data of each party to others.", "creator": "LaTeX with hyperref package"}}}