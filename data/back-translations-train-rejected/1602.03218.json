{"id": "1602.03218", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "abstract": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory.", "histories": [["v1", "Tue, 9 Feb 2016 23:24:33 GMT  (33kb)", "https://arxiv.org/abs/1602.03218v1", "9 pages, 6 figures, 2 tables"], ["v2", "Tue, 23 Feb 2016 10:22:25 GMT  (33kb)", "http://arxiv.org/abs/1602.03218v2", "Added soft attention appendix"]], "COMMENTS": "9 pages, 6 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marcin", "rychowicz", "karol kurach"], "accepted": false, "id": "1602.03218"}, "pdf": {"name": "1602.03218.pdf", "metadata": {"source": "META", "title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "authors": ["Marcin Andrychowicz", "Karol Kurach"], "emails": ["MARCINA@GOOGLE.COM", "KKURACH@GOOGLE.COM"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.03 218v 2 [cs.L G] February 23, 2016We show that an LSTM network extended with HAM can learn algorithms for problems such as merging, sorting or binary search from pure input-output examples. In particular, it learns to sort n numbers over time. It can generalize much longer than the input sequences seen during the training. We also show that HAM can be trained to act like classical data structures: a stack, a FIFO queue, and a priority list."}, {"heading": "1. Intro", "text": "Deep Recurrent Neural Networks (RNNs) have recently proven to be very successful with real-world Word tasks, such as machine translation (Sutskever et al., 2014) and computer vision (Vinyals et al., 2014), but success has only been achieved with tasks that do not require a large amount of memory to solve the problem, for example, we can translate sentences with RNNs, but we cannot produce reasonable translations of really long pieces of text such as books. High-capacity memory is a critical component necessary to deal with large problems that have many far-reaching dependencies. RNNs currently used do not scale well to larger memories, such as the number of parameters in an LSTM (Hochreiter & Schmidhuber, 1997) grows square with the size of the network that has many far-reaching dependencies."}, {"heading": "1.1. Our contribution", "text": "In this paper, we propose a novel neural network memory module, called Hierarchical Attentive Memory (HAM). The HAM module is generic and can be used as a building block of larger neural architectures. Its key feature is that it scales well with memory size - memory access requires only log operations, where n is the size of memory. This complexity is achieved by using a new attention mechanism based on a binary tree with leaves that correspond to the memory cells. Not only is the novel attention mechanism faster than the standard used in deep learning (Bahdanau et al., 2014), but it also enables the learning of algorithms based on a built-in bias against the operation of intervals. We show that an LSTM enhanced with HAM is capable of sorting algorithms for tasks such as merging, sorting or binary search, while learning the neural network in particular is the first one that has a lot to learn about neural structures in the classical FIAL."}, {"heading": "2. Related work", "text": "In this section we mention a number of recently proposed neural architectures with an external memory whose size is independent of the number of model parameters. Attention-based memory architectures are a newer but already extremely successful technique in deep learning. This mechanism allows networks to observe portions of the (potentially preprocessed) input sequence (Bahdanau et al., 2014) while generating the output sequence. It is implemented by providing the network with a linear combination of input symbols as a tool, where the weights of this linear combination can be controlled by the network. Attention mechanisms were used to access memory in neural turing machines (NTMs et al., 2014). It was the first paper to explicitly attempt to train a computorial, universal neural."}, {"heading": "3. Hierarchical Attentive Memory", "text": "In this section, we will describe our novel memory module, called Hierarchical Attentive Memory (HAM). To specify our description, we will consider a model that consists of an LSTM \"controller\" enhanced with a HAM module. The overarching idea behind the HAM module is that memory is structured as a complete binary tree, with the leaves containing the data stored in memory. The inner nodes contain some auxiliary data that allow us to efficiently perform some types of \"queries\" on memory. To access memory, start from the root of the tree and perform a sequence from top to bottom in the tree similar to the hierarchical Softmax procedure (Morin & Bengio, 2005)."}, {"heading": "3.1. Notation", "text": "The model takes as input a sequence x1, x2,... and outputs a sequence y1, y2,... Let's assume that each element of these sequences is a binary vector of size b-N, i.e. xi, yi-0, 1-b. Let's assume for a moment that we want to process input sequences of length \u2264 n, where n-N is a power of two (we'll show later how sequences of arbitrary length are processed).The model is based on the complete binary tree with n leaves. Let V denote the set of nodes in that tree (note that | V | = 2n-1) and let L-V denote the set of its leaves. Let l (e) for e-V-L be the left child of the node e and let r (e) be its right child. We'll now introduce the inference method for the model and then discuss how to train it."}, {"heading": "3.2. Inference", "text": "The mentioned hsrc\u00fceFnlhsrc\u00fc\u00fc\u00fch hsci hsci nvo eeinm hsci-eSrcnlhhsrc\u00fceaeSrlhsrc\u00fceSrlhsrc\u00fcehnlc\u00fceaeSn ni rde eSrlhsrc\u00fce ni nde nlrhee\u00fccnlhsrc\u00fcehnc\u00fcehnlc\u00fcehnlc\u00fcehnlcsrcehnlc\u00fce eaJhreh hreh hre\u00fccehnlc\u00fc.nlrf\u00fc nI \"s\" D \"s tlsa f\u00fcf\u00fc ide hsci-eSrcehnlcehnlcehnlcsrcsrcsrcehnlc\u00fce\" l\u00fc \"llllllrh\" so \"l\u00fc,\" lsa os os fro cehncos c\u00fcf\u00fc \"ros c\u00fcf\u00fc Sros cehnl\u00fc,\" ros c\u00fcf\u00fc Srcehnlci-cehnlcehnlcehnlcehnlci Srh Srh rh rh rh rh ru, \"rh rh rh rh rh rh rh,\" lrh rh rh \"lrh rh\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh,\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh\" lrh, \"lrh\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh\" lrh \"lrh\" ll"}, {"heading": "3.3. Training", "text": "In this section, we describe how we maintain our model of pure input-output examples using REINFORCE (Williams, 1992). (Appendix A) We also present another variant of HAM that is fully differentiable and can be trained using end-to-end backpropagation techniques. (Appendix A) We want to maximize the log probability of producing the correct output results, i.e.L = log p (y) x (x) p (y) p (y) p (y) p (y) p (y) p (c) p (e) p (e) p (e) p (e) p (p) p (y) p (y) p (y) p (y) p (y) x (c) x (c) x (y) p (y) p (e) c (c) c (c) p \"p\" p (e) p (p) p (e) p (p) p) p (e) p (e) p (c) c (c) p (p) p) p (p) p) p (p) p) p (c) c (e) p (c) p (c) p (p) p (p) p) p (p) p) p (e) p (p) p (p) p (p) p) p (p) p) p (e) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (c) p (e) p (p (p) p) p (p) p (e) p (p (p (p) p) p (p (e) p (p) p (p) p (p) p (p (p) p (p) p) p (p (p) p) p (e) p (p (p (p) p) p) p (p) p (e) p (p (p (p) p) p (e) p) p (p (p (p) p (p (p) p) p (e) p (e) p (p (p) p) p (p (e) p) p"}, {"heading": "4. Experiments", "text": "In this section, we will examine two variants of using the HAM module: the first is the model described in paragraph 3, which combines an LSTM controller with a HAM module (referred to as LSTM + HAM); then, in paragraph 4.3, we will examine the \"raw\" HAM (without the LSTM controller) to test its ability to function as a classical data structure: a stack, a FIFO queue, and a priority queue."}, {"heading": "4.1. Test setup", "text": "For each test we perform, we use the following procedure: First, we train the model with a memory size of up to n = 32 using the syllabus described in Sec. 3.3. The model is trained using the minibatch Adam algorithm with exponentially decreasing learning rate. We use random search to determine the best hyperparameters for the model. We use gradient clipping (Pascanu et al., 2012) with constant 5. The depth of our MLPs is either 1 or 2, the LSTM controller has l = 20 memory cells and the hidden values in the tree have dimensionality d = 20. Constant determination of a number of memory accesses between the production of each output of symbols (Fig. 4) is equal to 1 or 2. We train for 100 epochs, each consisting of 1000 batches of size."}, {"heading": "4.2. LSTM+HAM", "text": "We evaluate the model using a number of algorithmic tasks described below: Conversely, given a sequence of 10-bit vectors, they are output in reverse order, i.e., yi = xm + 1 \u2212 i \u2264 m, where m is the length of the input sequence. Search: Given a sequence of pairs xi = keyi | valuei for 1 \u2264 i \u2264 m + 1 sorted by keys and a query xm = q, we find the smallest problem such that Keyi = q and output y1 = valuei.Keys and values are 5-bit vectors and keys that are lexicographically compared with each other. The LSTM + HAM model is given only with two timesteps to solve this problem, forcing it to use a form of binary search.Merge: Given two sorted sequences of pairs - (p1, v1), (p1, vm) and (p \u2032 1, 1), it is."}, {"heading": "4.3. Raw HAM", "text": "In this section, we evaluate \"raw\" HAM module (without the LSTM controller) to see if it can act as a drop-in replacement for 3 classic data structures: a stack, a FIFO queue, and a priority list. For each task, the network is given a sequence of PUSH and POP operations in an online manner: with Timestep t, the network only sees the t operation to perform xt. This is a more realistic scenario for data structures, as it prevents the network from cheating in the future. Raw HAM module differs from the LSTM + HAM model of Sec. 3 in the following way: The HAM memory is initialized with zeros. \u2022 The t output symbol yt is set with an MLP value of the future.Raw HAM module differs from the LSTM-HAM model."}, {"heading": "4.4. Analysis", "text": "In this section, we present some insights into the algorithms learned through the LSTM + HAM model by examining the hidden representations he has learned for a variant of the sorting problem, in which we lexicographically sort 4-bit vectors5. For demonstration purposes, we use a small tree with n = 8 leaves and d = 6. The trained network sorts perfectly. It takes care of the leaves in the order of sorted input values, i.e. at each time step, HAM matches the sheet that corresponds to the smallest input value among the leaves that have not been visited so far. It would be interesting to understand exactly the algorithm the network uses for this operation. A natural solution to this problem would be to store the smallest input value among the (hitherto unattended) leaves below e in each hidden node, along with the information on whether the smallest value is in the right or left subtree. We present two tiers of our model together with the input value of the network that is used in some insights in the LSTM + HAM model."}, {"heading": "5. Comparison to other models", "text": "In fact, it is the case that most of us are able to abide by the rules that we have set ourselves. (...) In fact, it is the case that we are able to understand the rules that we have set ourselves. (...) It is not the case that we are able to understand the rules that we have set ourselves. (...) It is the case that we are able to understand the rules that we have set ourselves. (...) It is the case that we are able to break the rules, to break them. \"(...) It is the case that we are able to break the rules.\" (...), (...). (...) It is the case that we are able to rethink them. \"(...) It is the case that we are able to rethink them.\" (...). (...), (...). (...)"}, {"heading": "6. Conclusions", "text": "We introduced a new neural network memory architecture called Hierarchical Attentive Memory. Its key feature is that it scales well with memory size - memory access only requires BA (logn) operations - and this complexity is achieved by using a new binary tree-based attention mechanism. Not only is the novel attention mechanism faster than the standard mechanism used in deep learning, but it also allows learning algorithms based on the embedded tree structure. We showed that an LSTM enhanced with HAM can learn a number of algorithms, such as merge, sort or binary search, from pure input-output examples. In particular, it is the first neural architecture to be able to learn a sorting algorithm and generalize sequences that take much longer than those used in training. We believe that some of the concepts used in HAM, namely the idea of attention-binding, could be applied outside the tree."}, {"heading": "Acknowledgements", "text": "We would like to thank Nando de Freitas, Alexander Graves, Serkan Cabi, Misha Denil and Jonathan Hunt for their helpful comments and discussions."}, {"heading": "A. Using soft attention", "text": "One of the open questions in the field of designing neural networks with attention mechanisms is whether to use soft or hard attention. The model described in the paper belongs to the latter class of attention mechanisms because it makes hard, stochastic decisions; the other solution would be to use a soft, differentiable mechanism that looks for a linear combination of potential attention goals and does not include a sample; the main advantage of such models is that their gradients can be precisely calculated; we will now describe how to modify the model to make it completely differentiable (\"DHAM\"); remember that in the original model, the leaf visited at each time step is stochastically sampled; instead, at each time step, we will now calculate the probability p (e) that this leaf p (e) would be visited if we use the stochastic method described in the figure."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Handbook of Neuroscience for the Behavioral Sciences. Number v. 1 in Handbook of Neuroscience for the Behavioral Sciences", "author": ["G.G. Berntson", "J.T. Cacioppo"], "venue": null, "citeRegEx": "Berntson and Cacioppo,? \\Q2009\\E", "shortCiteRegEx": "Berntson and Cacioppo", "year": 2009}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Neural gpus learn algorithms", "author": ["Kaiser", "\u0141ukasz", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "Kaiser et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Kalchbrenner", "Nal", "Danihelka", "Ivo", "Graves", "Alex"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Neural random-access machines", "author": ["Kurach", "Karol", "Andrychowicz", "Marcin", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.06392,", "citeRegEx": "Kurach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "Gated graph sequence neural networks", "author": ["Li", "Yujia", "Tarlow", "Daniel", "Brockschmidt", "Marc", "Zemel", "Richard"], "venue": "arXiv preprint arXiv:1511.05493,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In Aistats,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Understanding the exploding gradient problem", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "Computing Research Repository (CoRR) abs/1211.5063,", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["Schulman", "John", "Heess", "Nicolas", "Weber", "Theophane", "Abbeel", "Pieter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Learning simple algorithms from examples", "author": ["Zaremba", "Wojciech", "Mikolov", "Tomas", "Joulin", "Armand", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1511.07275,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": ", 2014) and computer vision (Vinyals et al., 2014).", "startOffset": 28, "endOffset": 50}, {"referenceID": 2, "context": "The first versatile and highly successful architecture with this property was Neural Turing Machine (NTM) (Graves et al., 2014).", "startOffset": 106, "endOffset": 127}, {"referenceID": 0, "context": "The novel attention mechanism is not only faster than the standard one used in Deep Learning (Bahdanau et al., 2014), but it also facilities learning algorithms due to a built-in bias towards operating on intervals.", "startOffset": 93, "endOffset": 116}, {"referenceID": 0, "context": "This mechanism allows networks to attend to parts of the (potentially preprocessed) input sequence (Bahdanau et al., 2014) while generating the output sequence.", "startOffset": 99, "endOffset": 122}, {"referenceID": 2, "context": "Attention mechanism was used to access the memory in Neural Turing Machines (NTMs) (Graves et al., 2014).", "startOffset": 83, "endOffset": 104}, {"referenceID": 15, "context": "The followup work of (Sukhbaatar et al., 2015) combined the memory network with the soft attention mechanism, which allowed it to be trained with less supervision.", "startOffset": 21, "endOffset": 46}, {"referenceID": 0, "context": ", 2015), which is very similar to the attention model of Bahdanau et al. (2014). Despite not having a memory, this model was able to solve a number of difficult algorithmic problems that include the Convex Hull and the approximate 2D Travelling Salesman Problem.", "startOffset": 57, "endOffset": 80}, {"referenceID": 3, "context": "In another paper (Grefenstette et al., 2015) authors consider extending an LSTM with a stack, a FIFO queue or a double-ended queue and show some promising results.", "startOffset": 17, "endOffset": 44}, {"referenceID": 18, "context": "Memory architectures based on pointers In two recent papers (Zaremba & Sutskever, 2015; Zaremba et al., 2015) authors consider extending neural networks with nondifferentiable memories based on pointers and trained using Reinforcement Learning.", "startOffset": 60, "endOffset": 109}, {"referenceID": 9, "context": "Another type of pointer-based memory was presented in Neural Random-Access Machine (Kurach et al., 2015), which is a neural architecture mimicking classic computers.", "startOffset": 83, "endOffset": 104}, {"referenceID": 7, "context": "Grid-LSTM (Kalchbrenner et al., 2015) is an extension of LSTM to multiple dimensions.", "startOffset": 10, "endOffset": 37}, {"referenceID": 14, "context": "2 For a general discussion of computing gradients in computation graphs, which contain stochastic nodes see (Schulman et al., 2015).", "startOffset": 108, "endOffset": 131}, {"referenceID": 13, "context": "We use gradient clipping (Pascanu et al., 2012) with constant 5.", "startOffset": 25, "endOffset": 47}, {"referenceID": 0, "context": ", 2014) and encoder-decoder LSTM with attention (denoted LSTM+A) (Bahdanau et al., 2014).", "startOffset": 65, "endOffset": 88}, {"referenceID": 18, "context": "2 the only ones, which can copy a sequence of length n without \u0398(n) operations are: ReinforcementLearning NTM (Zaremba & Sutskever, 2015), the model from (Zaremba et al., 2015), Neural Random-Access Machine (Kurach et al.", "startOffset": 154, "endOffset": 176}, {"referenceID": 9, "context": ", 2015), Neural Random-Access Machine (Kurach et al., 2015), and Queue-Augmented LSTM (Grefenstette et al.", "startOffset": 38, "endOffset": 59}, {"referenceID": 3, "context": ", 2015), and Queue-Augmented LSTM (Grefenstette et al., 2015).", "startOffset": 34, "endOffset": 61}, {"referenceID": 2, "context": "Neural Turing Machines (NTM) (Graves et al., 2014).", "startOffset": 29, "endOffset": 50}, {"referenceID": 10, "context": "Our model may be seen as a special case of Gated Graph Neural Network (Li et al., 2015).", "startOffset": 70, "endOffset": 87}], "year": 2016, "abstractText": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in \u0398(logn) complexity, which is a significant improvement over the standard attention mechanism that requires \u0398(n) operations, where n is the size of the memory. We show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples. In particular, it learns to sort n numbers in time \u0398(n logn) and generalizes well to input sequences much longer than the ones seen during the training. We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.", "creator": "LaTeX with hyperref package"}}}