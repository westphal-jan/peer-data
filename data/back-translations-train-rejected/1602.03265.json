{"id": "1602.03265", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "Simple Search Algorithms on Semantic Networks Learned from Language Use", "abstract": "Recent empirical and modeling research has focused on the semantic fluency task because it is informative about semantic memory. An interesting interplay arises between the richness of representations in semantic memory and the complexity of algorithms required to process it. It has remained an open question whether representations of words and their relations learned from language use can enable a simple search algorithm to mimic the observed behavior in the fluency task. Here we show that it is plausible to learn rich representations from naturalistic data for which a very simple search algorithm (a random walk) can replicate the human patterns. We suggest that explicitly structuring knowledge about words into a semantic network plays a crucial role in modeling human behavior in memory search and retrieval; moreover, this is the case across a range of semantic information sources.", "histories": [["v1", "Wed, 10 Feb 2016 04:54:15 GMT  (183kb,D)", "https://arxiv.org/abs/1602.03265v1", null], ["v2", "Thu, 11 Feb 2016 04:46:21 GMT  (172kb,D)", "http://arxiv.org/abs/1602.03265v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aida nematzadeh", "filip miscevic", "suzanne stevenson"], "accepted": false, "id": "1602.03265"}, "pdf": {"name": "1602.03265.pdf", "metadata": {"source": "CRF", "title": "Simple Search Algorithms on Semantic Networks Learned from Language Use", "authors": ["Aida Nematzadeh", "Filip Miscevic", "Suzanne Stevenson"], "emails": ["aida@cs.toronto.edu", "miscevic@cs.toronto.edu", "suzanne@cs.toronto.edu"], "sections": [{"heading": null, "text": "Keywords: semantic networks; semantic search; semantic memory; computer modelling"}, {"heading": "Introduction", "text": "There are a number of competing hypotheses for the representation of semantic memory, such as semantic networks (e.g. Collins & Loftus, 1975; Steyvers & Tenenbaum, 2005), vector-space models (e.g. Landauer & Dumais, 1997), and theme models (Griffiths, Steyvers, & Tenenbaum, 2007).The content and structure of semantic memory is of great interest because it affects how effectively people can store, search for, and retrieve information. Recurrent work in computational modeling has an interesting trade-off between the representation of semantic memory and the nature of the algorithms needed to process it (Hills, Jones, & Todd, 2012; Abbott, Austerweil, & Griffiths, 2015).The models in question focus on the explicit representation of semantic memories and the nature of the algorithms needed to process them."}, {"heading": "Semantic Fluency Data and Models", "text": "Hills et al. (2012) argue that the search through semantic memory is guided by the same strategy used by animals in the search for food. To support this view, they found that participants \"responses in a semantic flow task (i.e.,\" name as many animals as possible in 3 minutes \") occurred in bursts of semantically related\" patches \"(animal categories defined by Troyer, Moscovitch and Winocur (1997), such as\" pets \"or\" farm animals. \"Moreover, the timing of these responses was consistent with the limit theorem of optimal search in physical space (Charnov, 1976). Specifically, the tized model Xiv 2.03 265v 2 [cs.C L] 11 Feb 2016 was necessary for participants to retrieve the next novel element relative to the last one - relative to the time between elements (IRT) - increased with each element within a patch."}, {"heading": "Our Semantic Representation", "text": "We briefly review our word learning process and then describe the process of building semantic networks."}, {"heading": "The Word Learner", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "Constructing a Semantic Network", "text": "Other recent research has used free association norms or conceptual hierarchies such as WordNet as the basis for a semantic network that is not connected; two words are connected by an edge in the network if there is a direct link between them in representation (Nemyvers & Tenenbaum, 2005; Abbott et al., 2015). In contrast, for our meaning representation (as for BEAGLE data), the corresponding network connections between the words must be determined by taking into account how related each word pair is in that representation (since all words are implicitly more or less related). We will follow Nematzadeh, Fazly, and Stevenson (2014b) in their approach to creating a semantic network through the learned meaning representations. As we model the empirical data from Hills et al, we will consider semantic frequency in the category of animals, we will focus on the subset of words in our education."}, {"heading": "Experimental Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "The Semantic Networks", "text": "The child-driven language that forms the basis for the input of our word learner is the Manchester corpus (Theakston, Lieven, Pine, & Rowland, 2001) of CHILDES (MacWhinney, 2000). Of the 518 unique animals classified by Hills et al. (2012) using the categories described by Troyer et al. (1997), 111 of them are present in the complete corpus and thus in our gold standard lexicon. However, only 93 of these appear in the 481K word corpus (120K utterances) that we use for training. Thus, a semantic network of taught meanings - referred to as the Learner network - will have a maximum of 94 nodes (93 words from the animal subcategories plus the animal itself). Remember that the learned representations from our model reflect both definitional and contextual aspects of the word meaning; this contextualization of meanings has shown that the resulting structure of the Nemadetic 3h is influenced."}, {"heading": "Simulating Behavior with Random Walks", "text": "Our goal is to see if the structure of our semantic networks is sufficient to reflect the observed search behavior using a simple, uniform search algorithm. To this end, we perform random walks with variations such as those of Abbott et al. (2015). Each random walk starts at the word threshold to simulate the fact that animal is the keyword for the fluid task (i.e., as many animals as possible). Each step in a random walk - i.e., the step from the current node nc to the next node nn - is determined by a probable selection via the edge occurrences on nc. The selection process can select the edge that follows in proportion to the edge weights (a weighted walk), or uses a uniform distribution across all edges associated with nc (an unweighted walk). (Another variation in which there is a probability of the leap back to the animal after each word)."}, {"heading": "Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Parameter Search and Selection", "text": "Various parameters affect both the number of words produced in a random walk on our networks and the exact pattern of IRTs and patch switches. Thresholds \u03c4 and \u03c4a used in determining the edges to be included in the networks (for non-animal or animal nodes) clearly affect the number of words produced, but it also affects the pattern. Thus, longer walks do not necessarily have more opportunity to explore more subcategories of words that can influence the patch change. Also, a longer walk does not necessarily mean that more4Hills et al. (2012) Note that imitating the value simply has to be higher than the average IRTs."}, {"heading": "Overall Patterns Observed", "text": "We select a range of four \u03c4 values (Learner [.70-.85], Gold [.75-.90], BEAGLE [.40-.55]) and nine settings of L (60-100) that show the best performance in depicting human behavior (as in (i) and (ii) above), resulting in a series of 36 paths in each of the weighted and unweighted settings that need to be analyzed; see Table 1. Overall, BEAGLE performs slightly better with weighted paths and our networks than with unweighted paths. The high value in our networks means that edge weights have a short range and are therefore very similar - i.e., they are not much more informative than the selection of terms trained on a body over 800 times our size."}, {"heading": "Comparing Best Results", "text": "To examine specific patterns more closely, we compared the networks under the parameter \"Best \u03c4\" for each of these networks (learner: 0.80, gold: 0.85, BEAGLE: 0.50) with the full range of L = 35 \u2212 135; see Table 2. For these settings, we found that all networks performed equally or slightly better on weighted walking compared to unweighted walking. All networks performed very similarly, with the primary difference that the learner network corresponded to human target behavior for more walking. Furthermore, both gold and learner met the stricter IRT ratio of 1.2 in most cases of weighted walking, while BEAGLE only met the less strict ratio of 1.1, including random walks using the BEAGLE data (L = 95 [learner], 85 [gold], 80 [BEAGLE]). In summary, human-like IRT patterns were observed during random walks in each of the three networks."}, {"heading": "Analyzing the Structure and Semantics of Networks", "text": "Previous research suggests that a small-world network - a sparse graph with highly networked subnetworks organized around \"hubs\" - provides efficient access to semantic information (Steyvers & Tenenbaum, 2005). The idea is similar to the search for food: first, the hubs are examined, and then a new subnetwork is used that connects to the customized hub. Indeed, a semantic network formed from the association norms used by Abbott et al. (2015) has been shown to have a small-world structure (Steyvers & Tenenbaum, 2005). As in Nematzadeh et al. (2014b), we calculate a \"smallworldness\" score for each of our networks by using well-known graphical metrics; if the network corresponds to a small-world structure (Steyvers & Tenenbaum, 2005). See Table 3 for the best networks, as in Figure 1. We find out that all networks have a small-world structure, the IRT indicating a pattern."}, {"heading": "Discussion and Future Work", "text": "We show that it is plausible to learn rich representations from naturalistic data for which a very simple search algorithm (a random walk) is sufficient to replicate the patterns observed in humans. Two key factors play a role in the success of our approach: (1) Our learned representations capture the hierarchical relationships between words and their contextual similarities. (2) We explicitly impose a structure on our learned representations by creating a semantic network in which words are connected only when their similarity crosses a certain threshold. Our work builds on recent research by Hills et al. (2012) and Abbott et al. (2015) in which different pairs of representations and algorithms (vectors of co-occurrence statistics and strategic search versus association norms and random search) replicate the same behavioral data from an explicit task."}], "references": [{"title": "Random walks on semantic networks can resemble optimal foraging", "author": ["J.T. Abbott", "J.L. Austerweil", "T.L. Griffiths"], "venue": "Psyc. Rev.,", "citeRegEx": "Abbott et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abbott et al\\.", "year": 2015}, {"title": "Optimal foraging, the marginal value theorem", "author": ["E.L. Charnov"], "venue": "Theoretical Population Biology,", "citeRegEx": "Charnov,? \\Q1976\\E", "shortCiteRegEx": "Charnov", "year": 1976}, {"title": "A spreading-activation theory of semantic processing", "author": ["A.M. Collins", "E.F. Loftus"], "venue": "Psyc. Rev.,", "citeRegEx": "Collins and Loftus,? \\Q1975\\E", "shortCiteRegEx": "Collins and Loftus", "year": 1975}, {"title": "A probabilistic computational model of cross-situational word learning", "author": ["A. Fazly", "A. Alishahi", "S. Stevenson"], "venue": "Cog. Sci.,", "citeRegEx": "Fazly et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fazly et al\\.", "year": 2010}, {"title": "Optimal foraging in semantic memory", "author": ["T.T. Hills", "M.N. Jones", "P.M. Todd"], "venue": "Psyc. Rev.,", "citeRegEx": "Hills et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hills et al\\.", "year": 2012}, {"title": "Optimal foraging in semantic memory", "author": ["T.T. Hills", "P.M. Todd", "M.N. Jones"], "venue": "In CogSci Proceedings", "citeRegEx": "Hills et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hills et al\\.", "year": 2009}, {"title": "Hidden processes in structural representations: A reply", "author": ["M.N. Jones", "T.T. Hills", "P.M. Todd"], "venue": null, "citeRegEx": "Jones et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jones et al\\.", "year": 2015}, {"title": "Representing word meaning and order information in a composite holographic lexicon", "author": ["M.N. Jones", "D.J. Mewhort"], "venue": "Psyc. Rev.,", "citeRegEx": "Jones and Mewhort,? \\Q2007\\E", "shortCiteRegEx": "Jones and Mewhort", "year": 2007}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psyc. Rev.,", "citeRegEx": "Landauer and Dumais,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "The CHILDES project: Tools for analyzing talk", "author": ["B. MacWhinney"], "venue": "(3rd ed.,", "citeRegEx": "MacWhinney,? \\Q2000\\E", "shortCiteRegEx": "MacWhinney", "year": 2000}, {"title": "The University of South Florida free association, rhyme, and word fragment norms", "author": ["D.L. Nelson", "C.L. McEvoy", "T.A. Schreiber"], "venue": null, "citeRegEx": "Nelson et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Nelson et al\\.", "year": 1998}, {"title": "A cognitive model of semantic network learning", "author": ["A. Nematzadeh", "A. Fazly", "S. Stevenson"], "venue": "In Proceed. Conf. on Empirical Methods in Natural Lang. Processing", "citeRegEx": "Nematzadeh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nematzadeh et al\\.", "year": 2014}, {"title": "Structural differences in the semantic networks of simulated word learners", "author": ["A. Nematzadeh", "A. Fazly", "S. Stevenson"], "venue": "In CogSci Proceedings (pp. 1072\u20131077)", "citeRegEx": "Nematzadeh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nematzadeh et al\\.", "year": 2014}, {"title": "A computational cognitive model of novel word generalization", "author": ["A. Nematzadeh", "E. Grant", "S. Stevenson"], "venue": "In Proceed. Conf. on Empirical Methods in Natural Lang. Processing (pp. 1795\u20131804)", "citeRegEx": "Nematzadeh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nematzadeh et al\\.", "year": 2015}, {"title": "The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth", "author": ["M. Steyvers", "J.B. Tenenbaum"], "venue": "Cog. Sci.,", "citeRegEx": "Steyvers and Tenenbaum,? \\Q2005\\E", "shortCiteRegEx": "Steyvers and Tenenbaum", "year": 2005}, {"title": "The role of performance limitations in the acquisition of verb\u2013argument structure: An alternative account", "author": ["A.L. Theakston", "E.V. Lieven", "J.M. Pine", "C.F. Rowland"], "venue": "Journal of Child Language,", "citeRegEx": "Theakston et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Theakston et al\\.", "year": 2001}, {"title": "Clustering and switching as two components of verbal fluency: Evidence from younger and older healthy adults", "author": ["A.K. Troyer", "M. Moscovitch", "G. Winocur"], "venue": "Neuropsychology,", "citeRegEx": "Troyer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Troyer et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 4, "context": "Based on their empirical data in such a task, Hills et al. (2012) argue that people follow an optimal foraging pattern that is similar to animals searching for food: a semantic patch is exploited until the rate of word retrieval is less than the long-term average rate of re-", "startOffset": 46, "endOffset": 66}, {"referenceID": 0, "context": "(2012) and Abbott et al. (2015) suggest that very", "startOffset": 11, "endOffset": 32}, {"referenceID": 3, "context": "Hills et al. (2012) adopted a vector space representation of semantic memory \u2013 one that encodes word\u2013 word co-occurrence patterns.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "In contrast, Abbott et al. (2015) showed that a simple random walk that operates uniformly was sufficient to model the pattern of behavior.", "startOffset": 13, "endOffset": 34}, {"referenceID": 0, "context": "Creating a semantic network by directly encoding human association norms, as Abbott et al. (2015) do, avoids the statistical learning problem that people face (Jones, Hills, & Todd, 2015).", "startOffset": 77, "endOffset": 98}, {"referenceID": 4, "context": "Moreover, we also show that if an explicit semantic network is created from the vectorspace semantic information of Hills et al. (2012), the same random walk algorithm on that network shows the desired match with human behavior.", "startOffset": 116, "endOffset": 136}, {"referenceID": 1, "context": "Moreover, the timing of these responses was consistent with the marginal value theorem of optimal foraging in physical space (Charnov, 1976).", "startOffset": 125, "endOffset": 140}, {"referenceID": 3, "context": "Hills et al. (2012) investigated the ability of different search algorithms to model this empirical data, using semantic representations of words learned by a vector space model, BEAGLE, on the Wikipedia corpus (Jones & Mewhort, 2007).", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "In contrast, Abbott et al. (2015) showed that a simple random walk on a semantic network could replicate human IRT patterns just as well as the two-stage algorithm of Hills et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 0, "context": "In contrast, Abbott et al. (2015) showed that a simple random walk on a semantic network could replicate human IRT patterns just as well as the two-stage algorithm of Hills et al. (2012). However, their semantic representation was created using human association norms (Nelson, McEvoy, & Schreiber, 1998).", "startOffset": 13, "endOffset": 187}, {"referenceID": 0, "context": "In contrast, Abbott et al. (2015) showed that a simple random walk on a semantic network could replicate human IRT patterns just as well as the two-stage algorithm of Hills et al. (2012). However, their semantic representation was created using human association norms (Nelson, McEvoy, & Schreiber, 1998). Jones et al. (2015) raised the issue that this semantic representation implicitly encodes the structure of a search process similar to the fluency task, thereby making it possible for a search algorithm simpler than that used by Hills et al.", "startOffset": 13, "endOffset": 326}, {"referenceID": 0, "context": "In contrast, Abbott et al. (2015) showed that a simple random walk on a semantic network could replicate human IRT patterns just as well as the two-stage algorithm of Hills et al. (2012). However, their semantic representation was created using human association norms (Nelson, McEvoy, & Schreiber, 1998). Jones et al. (2015) raised the issue that this semantic representation implicitly encodes the structure of a search process similar to the fluency task, thereby making it possible for a search algorithm simpler than that used by Hills et al. to replicate the empirical data. In the remainder of the paper, we explore whether a structured representation that results in a simpler search and retrieval algorithm can be learned from the kind of data that people are naturally exposed to. Similarly to Abbott et al. (2015) and in contrast to Hills et al.", "startOffset": 13, "endOffset": 825}, {"referenceID": 0, "context": "In contrast, Abbott et al. (2015) showed that a simple random walk on a semantic network could replicate human IRT patterns just as well as the two-stage algorithm of Hills et al. (2012). However, their semantic representation was created using human association norms (Nelson, McEvoy, & Schreiber, 1998). Jones et al. (2015) raised the issue that this semantic representation implicitly encodes the structure of a search process similar to the fluency task, thereby making it possible for a search algorithm simpler than that used by Hills et al. to replicate the empirical data. In the remainder of the paper, we explore whether a structured representation that results in a simpler search and retrieval algorithm can be learned from the kind of data that people are naturally exposed to. Similarly to Abbott et al. (2015) and in contrast to Hills et al. (2012), we construct a semantic network to explicitly encode the appropriate relations among words.", "startOffset": 13, "endOffset": 864}, {"referenceID": 0, "context": "Other recent research has used free-association norms or conceptual hierarchies like WordNet as the basis for a semantic network; two words are connected by an edge in the network if there is a direct connection between them in the representation (Steyvers & Tenenbaum, 2005; Abbott et al., 2015).", "startOffset": 247, "endOffset": 296}, {"referenceID": 4, "context": "Since we aim to model the empirical data from Hills et al. (2012) that looked at semantic fluency in the category of animals, we focus on the subset of words in our training data that", "startOffset": 46, "endOffset": 66}, {"referenceID": 4, "context": "occur in the dataset of Hills et al. (2012). Each such word is represented as a node in the network, and pairs of nodes are connected if the cosine similarity of their associated meaning probability vectors exceeds a certain threshold \u03c4 .", "startOffset": 24, "endOffset": 44}, {"referenceID": 11, "context": "(Future work will look at mechanisms as in Nematzadeh et al. (2015) for adequately capturing the meanings of hierarchically organized entities.", "startOffset": 43, "endOffset": 68}, {"referenceID": 9, "context": "The child-directed speech that forms the basis for the input to our word learner is the Manchester corpus (Theakston, Lieven, Pine, & Rowland, 2001) of CHILDES (MacWhinney, 2000).", "startOffset": 160, "endOffset": 178}, {"referenceID": 4, "context": "Of the 518 unique animals classified by Hills et al. (2012) using the categories described by Troyer et al.", "startOffset": 40, "endOffset": 60}, {"referenceID": 4, "context": "Of the 518 unique animals classified by Hills et al. (2012) using the categories described by Troyer et al. (1997), 111 of these are present in the full corpus and thus in our gold standard lexicon.", "startOffset": 40, "endOffset": 115}, {"referenceID": 4, "context": "Finally, we used the same method to create BEAGLE semantic networks using the data reported by Hills et al. (2012).", "startOffset": 95, "endOffset": 115}, {"referenceID": 4, "context": "The BEAGLE data contains 364 animal words that appear in Hills et al. (2012), and thus these networks have a maximum of 365 nodes.", "startOffset": 57, "endOffset": 77}, {"referenceID": 0, "context": "To that end, we perform random walks with variations as discussed by Abbott et al. (2015). Each random walk begins at the word animal to simulate the fact that animal is the cue for the fluency task (i.", "startOffset": 69, "endOffset": 90}, {"referenceID": 0, "context": "To reflect the time limit in the semantic fluency task, Abbott et al. (2015) fix the number of steps in the random walks to produce approximately the same number of words", "startOffset": 56, "endOffset": 77}, {"referenceID": 0, "context": "by Abbott et al. (2015). Only the first visit to a node counts as producing a word (just as repeats of words are not counted in the human task); the IRT is thus counted between such first visits: i.", "startOffset": 3, "endOffset": 24}, {"referenceID": 0, "context": "by Abbott et al. (2015). Only the first visit to a node counts as producing a word (just as repeats of words are not counted in the human task); the IRT is thus counted between such first visits: i.e., the IRT is the number of steps in the walk between a node ni the first time it is visited and the next node nj in the walk that has not been previously visited. Any nodes revisited between such an ni and nj increase the IRT between them. Patch switches occur in the fluency task when participants switch from listing animals in one subcategory (such as \u2018farm animals\u2019) to another (such as \u2018pets\u2019). Motivated by findings in Hills, Todd, and Jones (2009), we use a \u201cfluid patch model\u201d with the Troyer et al.", "startOffset": 3, "endOffset": 655}, {"referenceID": 0, "context": "by Abbott et al. (2015). Only the first visit to a node counts as producing a word (just as repeats of words are not counted in the human task); the IRT is thus counted between such first visits: i.e., the IRT is the number of steps in the walk between a node ni the first time it is visited and the next node nj in the walk that has not been previously visited. Any nodes revisited between such an ni and nj increase the IRT between them. Patch switches occur in the fluency task when participants switch from listing animals in one subcategory (such as \u2018farm animals\u2019) to another (such as \u2018pets\u2019). Motivated by findings in Hills, Todd, and Jones (2009), we use a \u201cfluid patch model\u201d with the Troyer et al. (1997) categories of animals in analyzing our results.", "startOffset": 3, "endOffset": 715}, {"referenceID": 0, "context": "Interestingly, we get these patterns with walk lengths in the range of 60\u2013100, where Abbott et al. (2015) used lengths of 2000 to produce words at the rate of people.", "startOffset": 85, "endOffset": 106}, {"referenceID": 4, "context": "Importantly, this includes random walks using the BEAGLE data, which Hills et al. (2012) previously showed could not produce such a pattern when used directly.", "startOffset": 69, "endOffset": 89}, {"referenceID": 4, "context": "Figure 1: (a) Human IRTs reproduced from Hills et al. (2012). (b\u2013d) Modeling IRTs in weighted random walks using the parameters described in Comparing Best Results.", "startOffset": 41, "endOffset": 61}, {"referenceID": 0, "context": "a semantic network created from the association norms used by Abbott et al. (2015) has been shown to have a small-world structure (Steyvers & Tenenbaum, 2005).", "startOffset": 62, "endOffset": 83}, {"referenceID": 11, "context": "As in Nematzadeh et al. (2014b), we calculate a \u201csmallworldness\u201d score (\u03c3) for each of our networks, using wellknown graph metrics; when \u03c3 > 1, the network conforms to a small-world structure.", "startOffset": 6, "endOffset": 32}, {"referenceID": 0, "context": "Indeed, Abbott et al. (2015) also find that their network captures appropriate groupings of animals.", "startOffset": 8, "endOffset": 29}, {"referenceID": 16, "context": "with the most frequently occurring category from Troyer et al. (1997). We take a mean of precision and recall for each such cluster, weighted by its size, and compute the F-score (see Table 3).", "startOffset": 49, "endOffset": 70}, {"referenceID": 0, "context": "We note here that Abbott et al. (2015) claim the BEAGLE data shows only a \u201cweak signature of animal clusters\u201d.", "startOffset": 18, "endOffset": 39}, {"referenceID": 3, "context": "Our work builds on recent research by Hills et al. (2012) and Abbott et al.", "startOffset": 38, "endOffset": 58}, {"referenceID": 0, "context": "(2012) and Abbott et al. (2015) in which different representation\u2013 algorithm pairs (vectors of co-occurrence statistics and strategic search vs.", "startOffset": 11, "endOffset": 32}, {"referenceID": 4, "context": "We further demonstrate that a random walk on a semantic network created from the vector representations of Hills et al. (2012) can produce the observed human pattern.", "startOffset": 107, "endOffset": 127}], "year": 2016, "abstractText": "Recent empirical and modeling research has focused on the semantic fluency task because it is informative about semantic memory. An interesting interplay arises between the richness of representations in semantic memory and the complexity of algorithms required to process it. It has remained an open question whether representations of words and their relations learned from language use can enable a simple search algorithm to mimic the observed behavior in the fluency task. Here we show that it is plausible to learn rich representations from naturalistic data for which a very simple search algorithm (a random walk) can replicate the human patterns. We suggest that explicitly structuring knowledge about words into a semantic network plays a crucial role in modeling human behavior in memory search and retrieval; moreover, this is the case across a range of semantic information sources.", "creator": "TeX"}}}