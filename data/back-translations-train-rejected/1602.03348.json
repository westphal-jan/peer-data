{"id": "1602.03348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)", "abstract": "Reinforcement Learning (RL) aims to learn an optimal policy for a Markov Decision Process (MDP). For complex, high-dimensional MDPs, it may only be feasible to represent the policy with function approximation. If the policy representation used cannot represent good policies, the problem is misspecified and the learned policy may be far from optimal. We introduce IHOMP as an approach for solving misspecified problems. IHOMP iteratively refines a set of specialized policies based on a limited representation. We refer to these policies as policy threads. At the same time, IHOMP stitches these policy threads together in a hierarchical fashion to solve a problem that was otherwise misspecified. We prove that IHOMP enjoys theoretical convergence guarantees and extend IHOMP to exploit Option Interruption (OI) enabling it to learn where policy threads can be reused. Our experiments demonstrate that IHOMP can find near-optimal solutions to otherwise misspecified problems and that OI can further improve the solutions.", "histories": [["v1", "Wed, 10 Feb 2016 12:27:04 GMT  (5461kb,D)", "https://arxiv.org/abs/1602.03348v1", "arXiv admin note: text overlap witharXiv:1506.03624"], ["v2", "Tue, 7 Jun 2016 20:05:14 GMT  (4958kb,D)", "http://arxiv.org/abs/1602.03348v2", "arXiv admin note: text overlap witharXiv:1506.03624"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1506.03624", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["daniel j mankowitz", "timothy a mann", "shie mannor"], "accepted": false, "id": "1602.03348"}, "pdf": {"name": "1602.03348.pdf", "metadata": {"source": "CRF", "title": "Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)", "authors": ["Daniel J. Mankowitz", "Timothy A. Mann"], "emails": ["danielm@tx.technion.ac.il", "timothymann@google.com", "shie@ee.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Background", "text": "Let M = < S, A, P, R, \u03b3 > be an MDP, where S is a (possibly infinite) set of states, A is a finite set of actions, P is a mapping of state-action pairs on probability distributions over the next states, R maps each state-action pair to a reward in [0, 1], and Q [0, 1) is the discount factor. A policy \u03c0 (a | s) indicates the likelihood of performing actions in relation to the state s. Let M be an MDP. The value function of a policy \u03c0 in relation to a state s \u00b2 S is V \u03c0M (s) = E [\u2211 \u00b2 t \u2212 1R (st, at) | s0 = s], where the expectation is taken in relation to the trajectory produced by following the policy s. The value function of a policy \u03c0 can also be written recursively as V \u03c0M (s)."}, {"heading": "3 Learning Options", "text": "An option is typically defined by a triple o = < I, \u03c0, \u03b2 >. However, we focus on a specific case of options where an option \u03c3 is defined by a tuple \u03c3 = < \u03c0\u03b8, \u03b2 >, which is a parametric policy with parameter vector Douglas and \u03b2: S \u2192 {0, 1} indicates whether the option is terminated (\u03b2 (s) = 1) or not (\u03b2 (s) = 0) in view of the current state. In view of a number of options \u03a3 with size m \u2265 1, the interoption policy is defined by \u00b5: S \u2192 [m], where S is the state space, and [m] is the index set above the options in the DP. An interoption policy selects which options should be executed from the current state by returning the index of one of the options."}, {"heading": "4 Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)", "text": "The Iterative Hierarchical Optimization for Misstated Problems (IHOMP, Algorithm 1) takes the original MDP M, a partition P across the state space, and a number of iterations K \u2265 1, and returns a pair < p > that contains a set of options DP and a set of options. The number of options m = | P | is equal to the number of classes (subpartitions) in the partition P (line 1). The interoption policy returned by IHOMP is (line 2) replaced by \u00b5 (s) = arg maxi [m] I {s, Pi}, where I {\u00b7} is the indicator function that returns if its reasoning is true and 0 otherwise, and Pi denotes the ith class in partition P. Thus, it simply returns the index of the option that contains the current state."}, {"heading": "5 Analysis of IHOMP", "text": "We offer the first convergence guarantee for combining hierarchically and iteratively learned options in a continuous state MDP with IHOMP (Lemma 1 and Lemma 2, proven in supplementary material).We use this guarantee as well as Lemma 2 to prove Theorem 1. However, this theorem error allows us to analyze the quality of the interoption policy returned by IHOMP. It turns out that the quality of the policy critically depends on the quality of the option learning algorithm. An important parameter for determining the quality of a policy returned by IHOMP is the misspecification error defined below.Definition 1 Let us be a partition about the target MDP policy. The misspecification error is P = maxi class, (2) in which we are the smallest solution."}, {"heading": "6 Learning Partitions via Regularized Option Interruption", "text": "So far, IHOMP has assumed that a partition is a priori given. < Q > However, designing a partition cannot be trivial, and in many cases, the partition may be suboptimal. To ease this assumption, we include the Regularized Option Interruption (ROI) option [Mankowitz et al., 2014] in this work to enable IHOMP to automatically learn a near-optimal partition from an initially incorrect problem. IHOMP pursues this option of the action value function Q < \u00b5, \u03a3 > (s, j), which represents the expected value of being in state S and executing option j, given the option policy MP MP and the set option MP MP."}, {"heading": "7 Experiments and Results", "text": "This year it is more than ever before."}, {"heading": "8 Discussion", "text": "We have introduced IHOMP an RL planning algorithm for iterative learning options and an interoption policy [Sutton et al., 1999] to repair an MP. We provide theoretical results for IHOMP that directly relate the quality of the final interoption policy to the error in the error specification. IHOMP is the first algorithm to provide theoretical convergence guarantees while learning iteratively a number of options in a continuous state space. Furthermore, we have developed IHOMP ROI that uses regulated option interruptions [Sutton et al., 1999, Mankowitz et al., 2014] to learn an improved partition to solve an initially incorrectly specified problem. IHOMP ROI is also able to discover regions in state space where the options should be reused. In high-dimensional areas, partitions can be learned from expert demonstrations [Abbeel and Ng, 2005] and Intra-Definition Policies for 2015 [can be particularly useful]."}], "references": [{"title": "Exploration and apprenticeship learning in reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y Ng"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Abbeel and Ng.,? \\Q2005\\E", "shortCiteRegEx": "Abbeel and Ng.", "year": 2005}, {"title": "A two-stage framework for power transformer asset maintenance management\u2014part i: Models and formulations", "author": ["Amir Abiri-Jahromi", "Masood Parvania", "Francois Bouffard", "Mahmud Fotuhi-Firuzabad"], "venue": "Power Systems, IEEE Transactions on,", "citeRegEx": "Abiri.Jahromi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Abiri.Jahromi et al\\.", "year": 2013}, {"title": "Technical update: Least-squares temporal difference learning", "author": ["Justin A Boyan"], "venue": "Machine Learning,", "citeRegEx": "Boyan.,? \\Q2002\\E", "shortCiteRegEx": "Boyan.", "year": 2002}, {"title": "PAC-inspired option discovery in lifelong reinforcement learning", "author": ["Emma Brunskill", "Lihong Li"], "venue": null, "citeRegEx": "Brunskill and Li.,? \\Q2014\\E", "shortCiteRegEx": "Brunskill and Li.", "year": 2014}, {"title": "Accelerating Multi-agent Reinforcement Learning with Dynamic Co-learning", "author": ["Daniel Garant", "Bruno C. da Silva", "Victor Lesser", "Chongjie Zhang"], "venue": "Technical report,", "citeRegEx": "Garant et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garant et al\\.", "year": 2015}, {"title": "A bayesian approach to finding compact representations for reinforcement learning", "author": ["A Geramifard", "S Tellex", "D Wingate", "N Roy", "JP How"], "venue": "In European Workshops on Reinforcement Learning (EWRL),", "citeRegEx": "Geramifard et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Geramifard et al\\.", "year": 2012}, {"title": "Hierarchical solution of markov decision processes using macro-actions", "author": ["Milos Hauskrecht", "Nicolas Meuleau", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier"], "venue": "In Proceedings of the 14th Conference on Uncertainty in AI,", "citeRegEx": "Hauskrecht et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hauskrecht et al\\.", "year": 1998}, {"title": "Efficient planning under uncertainty with macro-actions", "author": ["Ruijie He", "Emma Brunskill", "Nicholas Roy"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "He et al\\.,? \\Q2011\\E", "shortCiteRegEx": "He et al\\.", "year": 2011}, {"title": "Value function approximation in reinforcement learning using the fourier basis", "author": ["G.D. Konidaris", "S. Osentoski", "P.S. Thomas"], "venue": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Konidaris et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2011}, {"title": "Skill discovery in continuous reinforcement learning domains using skill chaining", "author": ["George Konidaris", "Andrew G Barto"], "venue": "In NIPS", "citeRegEx": "Konidaris and Barto.,? \\Q2009\\E", "shortCiteRegEx": "Konidaris and Barto.", "year": 2009}, {"title": "Learning object detection from a small number of examples: the importance of good features", "author": ["Kobi Levi", "Yair Weiss"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Levi and Weiss.,? \\Q2004\\E", "shortCiteRegEx": "Levi and Weiss.", "year": 2004}, {"title": "Time regularized interrupting options", "author": ["Daniel J Mankowitz", "Timothy A Mann", "Shie Mannor"], "venue": null, "citeRegEx": "Mankowitz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mankowitz et al\\.", "year": 2014}, {"title": "Scaling up approximate value iteration with options: Better policies with fewer iterations", "author": ["Timothy A Mann", "Shie Mannor"], "venue": "In Proceedings of the 31 st ICML,", "citeRegEx": "Mann and Mannor.,? \\Q2014\\E", "shortCiteRegEx": "Mann and Mannor.", "year": 2014}, {"title": "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density", "author": ["Amy McGovern", "Andrew G Barto"], "venue": "In Proceedings of the 18th ICML,", "citeRegEx": "McGovern and Barto.,? \\Q2001\\E", "shortCiteRegEx": "McGovern and Barto.", "year": 2001}, {"title": "Mnih. Human-level control through deep reinforcement learning", "author": ["Volodymyr et. al"], "venue": "Nature,", "citeRegEx": "al.,? \\Q2015\\E", "shortCiteRegEx": "al.", "year": 2015}, {"title": "Hierarchical reinforcement learning: Assignment of behaviours to subpolicies by selforganization", "author": ["Wilco Moerman"], "venue": "PhD thesis,", "citeRegEx": "Moerman.,? \\Q2009\\E", "shortCiteRegEx": "Moerman.", "year": 2009}, {"title": "A tutorial on linear function approximators for dynamic programming and reinforcement learning", "author": ["N Roy", "JP How"], "venue": null, "citeRegEx": "Roy and How.,? \\Q2013\\E", "shortCiteRegEx": "Roy and How.", "year": 2013}, {"title": "Reinforcement learning with soft state aggregation", "author": ["Satinder P Singh", "Tommi Jaakkola", "Michael I Jordan"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Singh et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1995}, {"title": "Effective reinforcement learning for mobile robots", "author": ["William D Smart", "Leslie Pack Kaelbling"], "venue": "In Robotics and Automation,", "citeRegEx": "Smart and Kaelbling.,? \\Q2002\\E", "shortCiteRegEx": "Smart and Kaelbling.", "year": 2002}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["Richard Sutton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutton.,? \\Q1996\\E", "shortCiteRegEx": "Sutton.", "year": 1996}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard Sutton", "Andrew Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["Chen Tessler", "Shahar Givony", "Tom Zahavy", "Daniel J Mankowitz", "Shie Mannor"], "venue": "arXiv preprint arXiv:1604.07255,", "citeRegEx": "Tessler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tessler et al\\.", "year": 2016}, {"title": "Security-constrained generation and transmission outage scheduling with uncertainties", "author": ["Lei Wu", "Mohammad Shahidehpour", "Yong Fu"], "venue": "Power Systems, IEEE Transactions on,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Object tracking using sift features and mean shift", "author": ["Huiyu Zhou", "Yuan Yuan", "Chunmei Shi"], "venue": "Computer vision and image understanding,", "citeRegEx": "Zhou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 18, "context": ", 2010] and robotics [Smart and Kaelbling, 2002] to inventory management systems [Mann and Mannor, 2014].", "startOffset": 21, "endOffset": 48}, {"referenceID": 12, "context": ", 2010] and robotics [Smart and Kaelbling, 2002] to inventory management systems [Mann and Mannor, 2014].", "startOffset": 81, "endOffset": 104}, {"referenceID": 8, "context": ", 2009] and RL [Konidaris et al., 2011] literature that \u201cgood\u201d features can have a dramatic impact on performance.", "startOffset": 15, "endOffset": 39}, {"referenceID": 5, "context": "(3) Learning on Large Data: After learning on large amounts of data, augmenting a feature set with new features to get improved performance is non-trivial and often inefficient [Geramifard et al., 2012].", "startOffset": 177, "endOffset": 202}, {"referenceID": 21, "context": "ii are an example of abstract actions, called options [Sutton et al., 1999], macro-actions [Hauskrecht et al.", "startOffset": 54, "endOffset": 75}, {"referenceID": 9, "context": ", 2011], or skills [Konidaris and Barto, 2009].", "startOffset": 19, "endOffset": 46}, {"referenceID": 18, "context": "Consider navigation tasks (which we use in this paper for ease of visualization), which are ever-present in robotics [Smart and Kaelbling, 2002], where partitions naturally lead an agent from one location to another in the state space.", "startOffset": 117, "endOffset": 144}, {"referenceID": 12, "context": "Examples include inventory management systems [Mann and Mannor, 2014] as well as maintenance scheduling of generation units and transmission lines in smart grids [Abiri-Jahromi et al.", "startOffset": 46, "endOffset": 69}, {"referenceID": 20, "context": "Any number of policy evaluation algorithms could be used here, such as TD(\u03bb) with function approximation [Sutton and Barto, 1998] or LSTD [Boyan, 2002], modified to be used with options.", "startOffset": 105, "endOffset": 129}, {"referenceID": 2, "context": "Any number of policy evaluation algorithms could be used here, such as TD(\u03bb) with function approximation [Sutton and Barto, 1998] or LSTD [Boyan, 2002], modified to be used with options.", "startOffset": 138, "endOffset": 151}, {"referenceID": 6, "context": "The guarantee provided by Theorem 1 may appear similar to [Hauskrecht et al., 1998, Theorem 1]. However, Hauskrecht et al. [1998] derive options only at the beginning of the learning process and do not update them.", "startOffset": 59, "endOffset": 130}, {"referenceID": 11, "context": "To relax this assumption, we incorporate Regularized Option Interruption (ROI) [Mankowitz et al., 2014] into this work to enable IHOMP to automatically learn a near-optimal partition from an initially misspecified problem.", "startOffset": 79, "endOffset": 103}, {"referenceID": 19, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) [Sutton, 1996] and the Pinball domain [Konidaris and Barto, 2009].", "startOffset": 97, "endOffset": 111}, {"referenceID": 9, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) [Sutton, 1996] and the Pinball domain [Konidaris and Barto, 2009].", "startOffset": 135, "endOffset": 162}, {"referenceID": 9, "context": "Pinball: We tested IHOMP on the challenging pinball-world task (Figure 4a) Konidaris and Barto [2009]. The agent is initially provided with a 5-feature flat policy \u30081, x, y, \u1e8b, \u1e8f\u3009.", "startOffset": 75, "endOffset": 102}, {"referenceID": 21, "context": "8 Discussion We introduced IHOMP a RL planning algorithm for iteratively learning options and an inter-option policy [Sutton et al., 1999] to repair a MP.", "startOffset": 117, "endOffset": 138}, {"referenceID": 0, "context": "In high-dimensional domains, partitions can be learned from expert demonstrations [Abbeel and Ng, 2005] and intra-option policies can be", "startOffset": 82, "endOffset": 103}, {"referenceID": 22, "context": "Option reuse can be especially useful for transfer learning [Tessler et al., 2016] and multi-agent settings [Garant et al.", "startOffset": 60, "endOffset": 82}, {"referenceID": 4, "context": ", 2016] and multi-agent settings [Garant et al., 2015].", "startOffset": 33, "endOffset": 54}], "year": 2016, "abstractText": "For complex, high-dimensional Markov Decision Processes (MDPs), it may be necessary to represent the policy with function approximation. A problem is misspecified whenever, the representation cannot express any policy with acceptable performance. We introduce IHOMP : an approach for solving misspecified problems. IHOMP iteratively learns a set of context specialized options and combines these options to solve an otherwise misspecified problem. Our main contribution is proving that IHOMP enjoys theoretical convergence guarantees. In addition, we extend IHOMP to exploit Option Interruption (OI) enabling it to decide where the learned options can be reused. Our experiments demonstrate that IHOMP can find near-optimal solutions to otherwise misspecified problems and that OI can further improve the solutions.", "creator": "LaTeX with hyperref package"}}}