{"id": "1602.03426", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2016", "title": "Automatic Sarcasm Detection: A Survey", "abstract": "Automatic detection of sarcasm has witnessed interest from the sentiment analysis research community. With diverse approaches, datasets and analyses that have been reported, there is an essential need to have a collective understanding of the research in this area. In this survey of automatic sarcasm detection, we describe datasets, approaches (both supervised and rule-based), and trends in sarcasm detection research. We also present a research matrix that summarizes past work, and list pointers to future work.", "histories": [["v1", "Wed, 10 Feb 2016 16:02:46 GMT  (178kb,D)", "http://arxiv.org/abs/1602.03426v1", "11 pages. This paper is likely to submitted to a NLP conference in the near future, with the same author list. All necessary regulations of arxiv and the conference will be followed"], ["v2", "Tue, 20 Sep 2016 22:15:52 GMT  (326kb,D)", "http://arxiv.org/abs/1602.03426v2", "This paper is likely to be submitted to ACM CSUR. This copy on arXiv is to obtain feedback from stakeholders"]], "COMMENTS": "11 pages. This paper is likely to submitted to a NLP conference in the near future, with the same author list. All necessary regulations of arxiv and the conference will be followed", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aditya joshi", "pushpak bhattacharyya", "mark james carman"], "accepted": false, "id": "1602.03426"}, "pdf": {"name": "1602.03426.pdf", "metadata": {"source": "CRF", "title": "Automatic Sarcasm Detection: A Survey", "authors": ["Aditya Joshi", "Pushpak Bhattacharyya", "Mark James Carman"], "emails": ["adityaj@cse.iitb.ac.in,", "pb@cse.iitb.ac.in,", "mark.carman@monash.edu"], "sections": [{"heading": "1 Introduction", "text": "Sarcasm is a peculiar form of sentiment expression, in which the surface mood differs from the implicit sensation. Free Dictionary1 defines sarcasm as a form of verbal irony intended to ridicule oneself. Sarcasm is an oft-cited challenge to sentiment analysis (Liu, 2010), because sarcasm intends to express a negative feeling, but a positive surface mood. This led to the introduction of sarcasm recognition as a research problem. Automatic sarcasm recognition refers to computational approaches to detect sarcasm in the text. This problem is difficult because nuanced ways in which sarcasm can be expressed might be sarcasm. These nuances arise in multiple ways: general understanding (\"I love to be ignored\"), in relation to the background of an author (\"I love solving mathematical problems throughout the weekend\"), in reference to a well-known sarcasm (in Chinese culture)."}, {"heading": "2 Sarcasm in Linguistics", "text": "In fact, the majority of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "3 Problem Definition", "text": "The most common formulation for detecting sarcasm is a classification task. In the face of a text, the goal is to predict whether or not it is sarcastic. However, alternative definitions are also possible. Reyes et al. (2013) use a similar formulation and offer paired classification services for these terms. (2014b) Consider classifier terms as: politics, humor, irony, and sarcasm. (2013) Use a similar formulation and offer paired classification services for these terms. Even in the context of classification, there are interesting variations in the data units that are commented on and thus classified. Tepperman et al. (2006) is an early paper on sarcasm detection that refers to occurrences of a common sarcasm phrase."}, {"heading": "4 Datasets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Short text", "text": "One approach to obtaining terms for tweets is manual commentary. They experiment with around 600 tweets that are marked for subjectivity, sensation and sarcasm. Pta \u0301 cek et al. (2014) present a dataset of 7,000 manually labeled tweets in the Czech Republic. The second approach is the use of hashtag-based monitoring. Many approaches use hashtags in tweets as indicators of sarcasm. The popularity of this approach can be attributed to several factors. (a) Nobody, but the author of a tweet can determine whether it was sarcastic."}, {"heading": "4.2 Long text", "text": "Lukin and Walker (2013) use Internet Argument Corpus, which identifies a dataset of discussion forums with multiple labels in which one of the labels is associated with sarcasm. Reyes and Rosso (2014) use a dataset of movies, book reviews, and news articles that are labeled with sarcasm and sentiment. Reyes and Rosso (2012) deal with products that have suddenly experienced a flood of sarcastic reviews. They look at a total of 11,000 reviews. Filatova (2012) use a dataset of about 1,000 reviews that are labeled as sarcastic or unlabeled. Buschmeier et al. (2014) use 1254 Amazon reviews, of which 437 are ironic. Tsur et al. (2010) look at a large dataset of 66,000 Amazon reviews. Liu et al. (2014) use a dataset from multiple sources such as Amazon, Twitter, Netease, and Netcena."}, {"heading": "4.3 Other datasets", "text": "Tepperman et al. (2006) use 131 call center transcripts. Any occurrence of \"yeah right\" is marked as sarcastic or not. The goal is to find features in a transcript that indicate which \"yeah right\" is sarcastic. Cross and Caucci (2007) use 20 sarcastic marked and 15 non-sarcastic marked excerpts that are marked by 101 students. The goal is to identify lexical indicators of sarcasm. Veale and Hao (2010) focus on finding out which comparisons are sarcastic. Therefore, they first use Google for the pattern \"* as *.\" This results in 20,000 unique comparisons that are then marked as sarcastic or not. Rakov and Rosenberg (2013) create a crowdsourced dataset of sentences from an MTV show, Daria. Ghosh et al. (2015b) use a crowdsourcing tool that is not expected to ignore a crowd-sourced sample."}, {"heading": "5 Approaches", "text": "Based on basic studies, there have been many reports of monitored / semi-monitored sarcasm classification, focusing on the use of specific patterns or novel features. Then, as Twitter emerged as a viable source of data, hashtag-based surveillance became popular. Lately, it has become popular to use contexts outside of the text to be classified, and this can be done in various ways, as we will describe hereafter. Generally, approaches to sarcasm detection can be divided into rules-based and statistical approaches, which we will consider in the next sections."}, {"heading": "5.1 Rule-based Approaches", "text": "Veale and Hao (2010) focus on determining whether a given parable (in the form \"* as *\") should be sarcastic. They use Google Search to determine how likely a parable is. They present a 9-step approach in which, at each step / rule, a parable is validated by the number of search results. One strength of this approach is that they present an error analysis that follows several rules. Maynard and Greenwood (2014) suggest that the hashtag sentiment is a key indicator of sarcasm. Hashtags are often used by authors to highlight sarcasm, and therefore, if the sentiment expressed by a hashtag does not match the rest of the tweet, the tweet is predicted to be sarcastic. They use a hashtag tokenizer to split hashtags from linked words. If the sentiment is expressed by a hashtag, a positive tweet is expressed as if the rest of the tweet does not match the rest of the tweet."}, {"heading": "5.2 Statistical Approaches", "text": "In order to detect sarcasm, several statistical approaches were examined, which differ in terms of their characteristics and classifiers."}, {"heading": "5.2.1 Features Used", "text": "In this subsection, we look at a number of characteristics reported for detection of sarcasm. Table 3 summarizes some outstanding characteristics for statistical approaches. In this subsection, we focus on characteristics related to the text to be classified. Contextual characteristics (i.e. characteristics that use information beyond the text to be classified) are described in the latter subsection. Most approaches use characteristics based on pouches as characteristics. In addition, there are characteristics that have been introduced in various papers. We look at them here. (2010) use pattern-based characteristics extracted from a corpus called sarcasm. These patent-based characteristics are included as numerical characteristics that assume three possible values: exact match, partial overlap, and no match."}, {"heading": "5.3 Classifiers", "text": "Gonza \u0301 lez-Iba \u0301 nez et al. (2011) use SVM with SMO and logistic regression. The chi-square test is used to identify differentiating characteristics. Reyes and Rosso (2012) use Naive Bayes and SVM. They also show Jaccard similarities between labels and characteristics. Riloff et al. (2013) compare rule-based techniques with an SVM-based classifier. Liebrecht et al. (2013) use a balanced Winnow algorithm to determine high-level characteristics. Reyes et al. (2013) use naive bayes and decision trees for several label pairs between irony, humor, politics and education. Bamman and Smith (2015) use binary logistic regression. Wang et al. (2015) use SVM-HMM to influence the nature of output labels in a conversation. Liu et al. (2014) compare several approaches, including classification and enhancement of data."}, {"heading": "5.4 Key Findings", "text": "These observations will be useful clues for future work. Gonza \u0301 lezIba \u0301 nez et al. (2011) show that unigram-based features exceed the use of a subset of words derived from a sentiment lexicon, comparing the accuracy of the sarcasm classifier with the ability of humans to detect sarcasm. While the best classifier reaches 57.41%, human performance in sarcasm identification is 62.59%. Reyes and Rosso (2012) note that sentiment-based features are their top distinguishing features. The logistics classifier in Rakov and Rosenberg (2013) results in an accuracy of 81.5%. Joshi et al. (2015) present an analysis of errors such as inconsistencies due to numbers and granularity of the annotation. Rajadesingan et al. (2015) show that historical features together with flip-based features are the most differentiating features."}, {"heading": "6 Trends in Sarcasm Detection Approaches", "text": "Two research trends can be observed in approaches to detecting sarcasm: (a) the discovery of sarcastic patterns and the use of these patterns as traits, and (b) the use of contextual information, i.e. information that goes beyond the target text to detect sarcasm."}, {"heading": "6.1 Pattern discovery", "text": "The discovery of sarcasm patterns was an early trend in the detection of sarcasm. Several approaches dealt with the extraction of patterns that indicate sarcasm or have implicit sensations, which can then be used as characteristics for a statistical classifier or as rules in a rules-based classifier. Tsur et al. (2010) extract sarcastic patterns from a series of labeled sentences. They first select words that occur either above an upper threshold or below a lower threshold. Among these words, they identify a large number of candidate patterns. Patterns that occur in both classes are then selected. Pta \u0301 cek et al. (2014) also use a similar approach for Czech and English tweets. Riloff et al. (2013) hypothesize that sarcasm occurs due to a contrast between positive verbs and negative situations phrases. To discover a lexicon of these verbs and phrases, suggest an algorithm."}, {"heading": "6.2 Role of context in sarcasm detection", "text": "The term context must be considered as any information that goes beyond the predicted text and goes beyond general knowledge. As we will see, this context can be included in many ways. In the rest of this section, we refer to the text unit that is to be classified as \"target text.\" Wallace et al. (2014) is an annotation study that first highlights the need for context for detecting sarcasm. Commentators mark Reddit comments with sarcasm tags. During this comment, commentators often ask for additional context in the form of Reddit comments. Authors also present a transitional matrix showing how often authors change their designations after the context is shown to them. Following this observation and the promise of context for detecting sarcasm, several newer approaches have dealt with the inclusion of this context. The contexts that have been reported consist of three types: 1. Historical context refers to the context for detecting sarcasm."}, {"heading": "7 Issues in Sarcasm Detection", "text": "This section describes three important topics related to current techniques for automatically detecting sarcasm. The first topic deals with notes: hashtag-based monitoring, data imbalance, and agreements between annotators. The second topic deals with a specific type of characteristics used for classification: feeling as a label. Finally, the third topic is in the context of classification techniques, where we look at how past work deals with distortions of data sets."}, {"heading": "7.1 Issues in Annotation", "text": "In addition to the specific issues described in the previous subsections, some issues have been analyzed in sarcasm notes. \u2022 Hashtag-based monitoring: Although a hashtag-based caption can provide comprehensive monitoring, the quality of the record may be questionable, especially when using # not to indicate insincere feelings. (Liebrecht et al., 2013) shows how # cannot be used to express sarcasm - while the rest of the sentence is not sarcastic. For example: \"I love Italian food. # not.\" The speaker does not express sarcasm by #. In most cases where hashtag-based supervision is used, the hashtag is removed in pre-processing, reducing the above sentence to \"I love cooking Italian food\" - which may have no sarcastic interpretation at all. \u2022 Data imbalance: Sarcasm is a rare phenomenon of perception."}, {"heading": "7.2 Sarcasm before sentiment or sentiment before sarcasm?", "text": "Bharti et al. (2015) is a rules-based approach that predicts a sentence as sarcastic when a negative phrase occurs in a positive sentence. As previously described, Khattri et al. (2015) use the mood of a past tweet by the author to predict sarcasm. In a statistical classifier, surface polarity can be used directly as a trait to use the polarity of the tweet as a trait (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015). Reyes et al. (2013) capture the polarity value in relation to two emotional dimensions: activation and agreeableness. Buschmeier et al. (2014) integrate helplessness as a balance between a situation in which a sentiment and a sentiment is in balance."}, {"heading": "7.3 Dealing with Dataset Skews", "text": "Liebrecht et al. (2013) state that \"detecting sarcasm is like a needle in a haystack. Some approaches focus on mitigating the effects of this distortion. Liu et al. (2014) uses a multi-strategic approach to learning ensembles and majority decisions. Similarly, Wallace et al. (2015) introduce an LSS regulatory strategy to deal with sparse features and distorted data. Therefore, they use a thrifty L1 regulator for contextual traits and L2 standard for word traits. Liebrecht et al. (2013) report AUC for balanced and distorted data sets to demonstrate the utility of their classifier."}, {"heading": "8 Conclusion & Future Directions", "text": "We observe three trends in sarcasm detection research: semi-supervised pattern extraction to identify implicit feelings, the use of hashtag-based monitoring, and the use of contexts beyond the target text. We have tabulated data sets and approaches that have been reported. We also highlight two key problems in detecting sarcasm: the relationship between feeling and sarcasm and data distortion in the case of data sets labeled with sarcasm. Research on detection of sarcasm has increased significantly in recent years and requires a look back at the overall picture that these individual works have led to. Based on our examination of these works, we propose the following possible directions for the future: 1. Implicit detection of sensory impressions & sarcasm: Based on past work, it is well established that sarcasm is closely linked to sensory limitations (Liebrecht et al., 2013). Several related works incorporate sensory impressions as 2."}], "references": [{"title": "Contextualized sarcasm detection on twitter", "author": ["Bamman", "Smith2015] David Bamman", "Noah A Smith"], "venue": "In Ninth International AAAI Conference on Web and Social Media", "citeRegEx": "Bamman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bamman et al\\.", "year": 2015}, {"title": "Italian irony detection in twitter: a first approach", "author": ["Francesco Ronzano", "Horacio Saggion"], "venue": "In The First Italian Conference on Computational Linguistics CLiC-it", "citeRegEx": "Barbieri et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barbieri et al\\.", "year": 2014}, {"title": "Modelling sarcasm in twitter, a novel approach", "author": ["Horacio Saggion", "Francesco Ronzano"], "venue": "ACL", "citeRegEx": "Barbieri et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barbieri et al\\.", "year": 2014}, {"title": "Parsing-based sarcasm sentiment recognition in twitter data", "author": ["Korra Sathya Babu", "Sanjay Kumar Jena"], "venue": "In Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Anal-", "citeRegEx": "Bharti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bharti et al\\.", "year": 2015}, {"title": "Opinion mining in twitter how to make use of sarcasm to enhance", "author": ["Bouazizi", "Ohtsuki2015] Mondher Bouazizi", "Tomoaki Ohtsuki"], "venue": null, "citeRegEx": "Bouazizi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bouazizi et al\\.", "year": 2015}, {"title": "An impact analysis of features in a classification approach to irony detection in product reviews", "author": ["Philipp Cimiano", "Roman Klinger"], "venue": null, "citeRegEx": "Buschmeier et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Buschmeier et al\\.", "year": 2014}, {"title": "Sarcasm, pretense, and the semantics/pragmatics distinction", "author": ["Elisabeth Camp"], "venue": "Nou\u0302s,", "citeRegEx": "Camp.,? \\Q2012\\E", "shortCiteRegEx": "Camp.", "year": 2012}, {"title": "Are there necessary conditions for inducing a sense of sarcastic irony? Discourse Processes, 49(6):459\u2013480", "author": ["Campbell", "Katz2012] John D Campbell", "Albert N Katz"], "venue": null, "citeRegEx": "Campbell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Campbell et al\\.", "year": 2012}, {"title": "Semi-supervised recognition of sarcastic sentences in twitter and amazon", "author": ["Oren Tsur", "Ari Rappoport"], "venue": "In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Davidov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Reactions to irony in discourse: Evidence for the least disruption principle", "author": ["Salvatore Attardo", "Diana Boxer"], "venue": "Journal of Pragmatics,", "citeRegEx": "Eisterhold et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Eisterhold et al\\.", "year": 2006}, {"title": "Detecting irony and sarcasm in microblogs: The role of expressive signals and ensemble classifiers", "author": ["Federico Alberto Pozzi", "Enza Messina"], "venue": "In Data Science and Advanced Analytics (DSAA),", "citeRegEx": "Fersini et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fersini et al\\.", "year": 2015}, {"title": "Irony and sarcasm: Corpus generation and analysis using crowdsourcing", "author": ["Elena Filatova"], "venue": "In LREC,", "citeRegEx": "Filatova.,? \\Q2012\\E", "shortCiteRegEx": "Filatova.", "year": 2012}, {"title": "2015a. Semeval-2015 task 11: Sentiment analysis of figurative language in twitter", "author": ["Guofu Li", "Tony Veale", "Paolo Rosso", "Ekaterina Shutova", "Antonio Reyes", "John Barnden"], "venue": "In Int. Workshop on Semantic Evaluation", "citeRegEx": "Ghosh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2015}, {"title": "Sarcastic or not: Word embeddings to predict the literal or sarcastic meaning of words", "author": ["Weiwei Guo", "Smaranda Muresan"], "venue": null, "citeRegEx": "Ghosh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2015}, {"title": "On irony and negation", "author": ["Rachel Giora"], "venue": "Discourse processes,", "citeRegEx": "Giora.,? \\Q1995\\E", "shortCiteRegEx": "Giora.", "year": 1995}, {"title": "Identifying sarcasm in twitter: a closer look", "author": ["Smaranda Muresan", "Nina Wacholder"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Gonz\u00e1lezIb\u00e1nez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gonz\u00e1lezIb\u00e1nez et al\\.", "year": 2011}, {"title": "Applying basic features from sentiment analysis for automatic irony detection", "author": ["Jos\u00e9-Miguel Bened\u0131", "Paolo Rosso"], "venue": "In Pattern Recognition and Image Analysis,", "citeRegEx": "Hern\u00e1ndez.Far\u0131\u0301as et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Far\u0131\u0301as et al\\.", "year": 2015}, {"title": "Context incongruity and irony processing", "author": ["Ivanko", "Pexman2003] Stacey L Ivanko", "Penny M Pexman"], "venue": "Discourse Processes,", "citeRegEx": "Ivanko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ivanko et al\\.", "year": 2003}, {"title": "Harnessing context incongruity for sarcasm detection", "author": ["Joshi et al.2015] Aditya Joshi", "Vinita Sharma", "Pushpak Bhattacharyya"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International", "citeRegEx": "Joshi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2015}, {"title": "Your sentiment precedes you: Using an authors historical tweets to predict sarcasm", "author": ["Aditya Joshi", "Pushpak Bhattacharyya", "Mark James Carman"], "venue": "In 6TH WORKSHOP ON COMPUTATIONAL", "citeRegEx": "Khattri et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Khattri et al\\.", "year": 2015}, {"title": "Lexical influences on the perception of sarcasm", "author": ["Kreuz", "Caucci2007] Roger J Kreuz", "Gina M Caucci"], "venue": "In Proceedings of the Workshop on computational approaches to Figurative Language,", "citeRegEx": "Kreuz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kreuz et al\\.", "year": 2007}, {"title": "The perfect solution for detecting sarcasm in tweets", "author": ["FA Kunneman", "APJ van den Bosch"], "venue": null, "citeRegEx": "Liebrecht et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liebrecht et al\\.", "year": 2013}, {"title": "Sarcasm detection in social media based on imbalanced classification", "author": ["Liu et al.2014] Peng Liu", "Wei Chen", "Gaoyan Ou", "Tengjiao Wang", "Dongqing Yang", "Kai Lei"], "venue": "In Web-Age Information Management,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Sentiment analysis and subjectivity", "author": ["Bing Liu"], "venue": "Handbook of natural language processing,", "citeRegEx": "Liu.,? \\Q2010\\E", "shortCiteRegEx": "Liu.", "year": 2010}, {"title": "Really? well. apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue", "author": ["Lukin", "Walker2013] Stephanie Lukin", "Marilyn Walker"], "venue": "In Proceedings of the Workshop on Language Analysis in Social Me-", "citeRegEx": "Lukin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lukin et al\\.", "year": 2013}, {"title": "Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis", "author": ["Maynard", "Greenwood2014] Diana Maynard", "Mark A Greenwood"], "venue": "In Proceedings of LREC", "citeRegEx": "Maynard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Maynard et al\\.", "year": 2014}, {"title": "Identification of nonliteral language in social media: A case study on sarcasm", "author": ["Roberto Gonzalez-Ibanez", "Debanjan Ghosh", "Nina Wacholder"], "venue": "Journal of the Association", "citeRegEx": "Muresan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Muresan et al\\.", "year": 2016}, {"title": "Sarcasm detection on czech and english twitter", "author": ["Pt\u00e1cek et al.2014] Tom\u00e1\u0161 Pt\u00e1cek", "Ivan Habernal", "Jun Hong"], "venue": "In Proceedings COLING 2014. COLING", "citeRegEx": "Pt\u00e1cek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pt\u00e1cek et al\\.", "year": 2014}, {"title": "Sarcasm detection on twitter: A behavioral modeling approach", "author": ["Reza Zafarani", "Huan Liu"], "venue": "In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Rajadesingan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rajadesingan et al\\.", "year": 2015}, {"title": "sure, i did the right thing\u201d: a system for sarcasm detection in speech", "author": ["Rakov", "Rosenberg2013] Rachel Rakov", "Andrew Rosenberg"], "venue": "In INTERSPEECH,", "citeRegEx": "Rakov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rakov et al\\.", "year": 2013}, {"title": "Making objective decisions from subjective data: Detecting irony in customer reviews", "author": ["Reyes", "Rosso2012] Antonio Reyes", "Paolo Rosso"], "venue": "Decision Support Systems,", "citeRegEx": "Reyes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Reyes et al\\.", "year": 2012}, {"title": "On the difficulty of automatically detecting irony: beyond a simple case of negation", "author": ["Reyes", "Rosso2014] Antonio Reyes", "Paolo Rosso"], "venue": "Knowledge and Information Systems,", "citeRegEx": "Reyes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reyes et al\\.", "year": 2014}, {"title": "From humor recognition to irony detection: The figurative language of social media", "author": ["Reyes et al.2012] Antonio Reyes", "Paolo Rosso", "Davide Buscaldi"], "venue": "Data & Knowledge Engineering,", "citeRegEx": "Reyes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Reyes et al\\.", "year": 2012}, {"title": "A multidimensional approach for detecting irony in twitter", "author": ["Reyes et al.2013] Antonio Reyes", "Paolo Rosso", "Tony Veale"], "venue": "Language Resources and Evaluation,", "citeRegEx": "Reyes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Reyes et al\\.", "year": 2013}, {"title": "Sarcasm as contrast between a positive sentiment and negative situation", "author": ["Riloff et al.2013] Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang"], "venue": null, "citeRegEx": "Riloff et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riloff et al\\.", "year": 2013}, {"title": " yeah right\u201d: sarcasm recognition for spoken dialogue systems", "author": ["David R Traum", "Shrikanth Narayanan"], "venue": "In INTERSPEECH. Citeseer", "citeRegEx": "Tepperman et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tepperman et al\\.", "year": 2006}, {"title": "Icwsm-a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews", "author": ["Tsur et al.2010] Oren Tsur", "Dmitry Davidov", "Ari Rappoport"], "venue": null, "citeRegEx": "Tsur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tsur et al\\.", "year": 2010}, {"title": "Detecting ironic intent in creative comparisons", "author": ["Veale", "Hao2010] Tony Veale", "Yanfen Hao"], "venue": "In ECAI,", "citeRegEx": "Veale et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Veale et al\\.", "year": 2010}, {"title": "Humans require context to infer ironic intent (so computers probably do, too)", "author": ["Laura Kertz Do Kook Choe", "Eugene Charniak"], "venue": "In Proceedings of the Annual Meeting of the Association", "citeRegEx": "Wallace et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wallace et al\\.", "year": 2014}, {"title": "Computational irony: A survey and new perspectives", "author": ["Byron C Wallace"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Wallace.,? \\Q2013\\E", "shortCiteRegEx": "Wallace.", "year": 2013}, {"title": "Sparse, contextually informed models for irony detection: Exploiting user communities,entities and sentiment", "author": ["Byron C Wallace"], "venue": null, "citeRegEx": "Wallace.,? \\Q2015\\E", "shortCiteRegEx": "Wallace.", "year": 2015}, {"title": "Twitter sarcasm detection exploiting a context-based model", "author": ["Wang et al.2015] Zelin Wang", "Zhijian Wu", "Ruimin Wang", "Yafeng Ren"], "venue": "In Web Information Systems Engineering\u2013WISE", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "The pragmatics of verbal irony: Echo or pretence? Lingua, 116(10):1722\u20131743", "author": ["Deirdre Wilson"], "venue": null, "citeRegEx": "Wilson.,? \\Q2006\\E", "shortCiteRegEx": "Wilson.", "year": 2006}], "referenceMentions": [{"referenceID": 23, "context": "Sarcasm is an often-quoted challenge for sentiment analysis (Liu, 2010) because sarcasm intends a negative sentiment, but a positive surface sentiment.", "startOffset": 60, "endOffset": 71}, {"referenceID": 23, "context": "Sarcasm is an often-quoted challenge for sentiment analysis (Liu, 2010) because sarcasm intends a negative sentiment, but a positive surface sentiment. This led to introduction of sarcasm detection as a research problem. Automatic sarcasm detection refers to computational approaches to detect sarcasm in text. This problem is hard because of nuanced ways in which sarcasm may be expressed. These nuances arise in several ways: general understanding (\u2018I love to get ignored\u2019), in terms of an author\u2019s background (\u2018I love solving math problems all weekend\u2019), in terms of a culture (popular use of honorifics in Chinese sarcasm), etc. Starting from the earliest known work by Tepperman et al. (2006) which deals with speech and text-related features, sarcasm detection research has seen wide interest.", "startOffset": 61, "endOffset": 698}, {"referenceID": 39, "context": "One past work to summarize computational irony is by Wallace (2013). However, they focus on linguistic challenges of computational irony, and deliberate on possible future work.", "startOffset": 53, "endOffset": 68}, {"referenceID": 42, "context": "Situational disparity theory: According to Wilson (2006), sarcasm arises when there is situational disparity between text and a contextual information.", "startOffset": 43, "endOffset": 57}, {"referenceID": 14, "context": "Negation theory of sarcasm: Giora (1995) state that irony/sarcasm is a form of negation in which an explicit negation marker is lacking.", "startOffset": 28, "endOffset": 41}, {"referenceID": 1, "context": "Understanding the relationship between sarcasm, irony and humor, Barbieri et al. (2014b) consider labels for the classifier as: politics, humor, irony and sarcasm.", "startOffset": 65, "endOffset": 89}, {"referenceID": 1, "context": "Understanding the relationship between sarcasm, irony and humor, Barbieri et al. (2014b) consider labels for the classifier as: politics, humor, irony and sarcasm. Reyes et al. (2013) use a similar formulation and provide pair-wise classification performance for these labels.", "startOffset": 65, "endOffset": 184}, {"referenceID": 1, "context": "Understanding the relationship between sarcasm, irony and humor, Barbieri et al. (2014b) consider labels for the classifier as: politics, humor, irony and sarcasm. Reyes et al. (2013) use a similar formulation and provide pair-wise classification performance for these labels. Even in the context of classification, there have been interesting variations in the data units that will be annotated and hence, classified. Tepperman et al. (2006) is an early paper in sarcasm detection that looks at occurrences of a common sarcastic phrase \u2018yeah right\u2019 and classifies each occurrence of \u2018yeah right\u2019 as sarcastic or not.", "startOffset": 65, "endOffset": 443}, {"referenceID": 1, "context": "Understanding the relationship between sarcasm, irony and humor, Barbieri et al. (2014b) consider labels for the classifier as: politics, humor, irony and sarcasm. Reyes et al. (2013) use a similar formulation and provide pair-wise classification performance for these labels. Even in the context of classification, there have been interesting variations in the data units that will be annotated and hence, classified. Tepperman et al. (2006) is an early paper in sarcasm detection that looks at occurrences of a common sarcastic phrase \u2018yeah right\u2019 and classifies each occurrence of \u2018yeah right\u2019 as sarcastic or not. Veale and Hao (2010) annotate similes such as \u2018as excited as a patient before a surgery\u2019 with sarcastic or not labels.", "startOffset": 65, "endOffset": 639}, {"referenceID": 12, "context": "Ghosh et al. (2015b) model sarcasm detection as a sense disambiguation task.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "Ghosh et al. (2015b) model sarcasm detection as a sense disambiguation task. They state that a word may have a literal sense and a sarcastic sense. Their goal is to identify the sense of a word in order to detect sarcasm. Wang et al. (2015) formulate the problem as a sequence labeling problem.", "startOffset": 0, "endOffset": 241}, {"referenceID": 20, "context": "Liu et al. (2014) use Chinese and English social media content.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "Barbieri et al. (2014a) present a first approach to detect sarcasm in Italian tweets.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Barbieri et al. (2014a) present a first approach to detect sarcasm in Italian tweets. Pt\u00e1cek et al. (2014)", "startOffset": 0, "endOffset": 107}, {"referenceID": 21, "context": "Liebrecht et al. (2013) explore sarcasm detection for Dutch.", "startOffset": 0, "endOffset": 24}, {"referenceID": 33, "context": "Riloff et al. (2013) use a dataset of tweets, manually annotated as sarcastic or not.", "startOffset": 0, "endOffset": 21}, {"referenceID": 33, "context": "Riloff et al. (2013) use a dataset of tweets, manually annotated as sarcastic or not. Maynard and Greenwood (2014) study sarcastic tweets and their impact to sarcasm classification.", "startOffset": 0, "endOffset": 115}, {"referenceID": 27, "context": "Pt\u00e1cek et al. (2014) present a dataset of 7,000 manually labeled tweets in Czech.", "startOffset": 0, "endOffset": 21}, {"referenceID": 36, "context": "(Tsur et al., 2010) X X X X X", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "(Davidov et al., 2010) X X X X X", "startOffset": 0, "endOffset": 22}, {"referenceID": 30, "context": "(Reyes et al., 2012) X X X X X X X", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "(Filatova, 2012) X X", "startOffset": 0, "endOffset": 16}, {"referenceID": 34, "context": "(Riloff et al., 2013) X X X X X X", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "(Liebrecht et al., 2013) X X X X X X", "startOffset": 0, "endOffset": 24}, {"referenceID": 33, "context": "(Reyes et al., 2013) X X X X X X X", "startOffset": 0, "endOffset": 20}, {"referenceID": 38, "context": "(Wallace et al., 2014) X X", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "(Buschmeier et al., 2014) X X X X X X X", "startOffset": 0, "endOffset": 25}, {"referenceID": 18, "context": "(Joshi et al., 2015) X X X X X X X X X", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "(Khattri et al., 2015) X X X X X X", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "(Rajadesingan et al., 2015) X X X X X X X X", "startOffset": 0, "endOffset": 27}, {"referenceID": 40, "context": "(Wallace, 2015) X X X X X X X X", "startOffset": 0, "endOffset": 15}, {"referenceID": 16, "context": "(Hern\u00e1ndez-Far\u0131\u0301as et al., 2015) X X X X X X X", "startOffset": 0, "endOffset": 32}, {"referenceID": 41, "context": "(Wang et al., 2015) X X X X X", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "(Liu et al., 2014) X X X X X X X X", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "(Bharti et al., 2015) X X X X X X", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "(Fersini et al., 2015) X X X X X X", "startOffset": 0, "endOffset": 22}, {"referenceID": 26, "context": "(Muresan et al., 2016) X X X X X X", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "Davidov et al. (2010) use a dataset of tweets, which are labeled with hashtags such as #sarcasm, #sarcastic, #not, etc.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "Davidov et al. (2010) use a dataset of tweets, which are labeled with hashtags such as #sarcasm, #sarcastic, #not, etc. Gonz\u00e1lez-Ib\u00e1nez et al. (2011) also use hashtag-based supervision for tweets.", "startOffset": 0, "endOffset": 150}, {"referenceID": 6, "context": "Davidov et al. (2010) use a dataset of tweets, which are labeled with hashtags such as #sarcasm, #sarcastic, #not, etc. Gonz\u00e1lez-Ib\u00e1nez et al. (2011) also use hashtag-based supervision for tweets. However, they eliminate cases where the hashtag is a part of the running text. For example, \u2018#sarcasm is popular in india\u2019 is eliminated. Reyes et al. (2012) use hashtag-based supervision for tweets.", "startOffset": 0, "endOffset": 355}, {"referenceID": 6, "context": "Davidov et al. (2010) use a dataset of tweets, which are labeled with hashtags such as #sarcasm, #sarcastic, #not, etc. Gonz\u00e1lez-Ib\u00e1nez et al. (2011) also use hashtag-based supervision for tweets. However, they eliminate cases where the hashtag is a part of the running text. For example, \u2018#sarcasm is popular in india\u2019 is eliminated. Reyes et al. (2012) use hashtag-based supervision for tweets. Reyes et al. (2013) use a dataset of 40000 tweets labeled as sarcastic or not, using hashtags.", "startOffset": 0, "endOffset": 417}, {"referenceID": 1, "context": "Barbieri et al. (2014a) introduce a dataset of 25K Italian tweets.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Barbieri et al. (2014a) introduce a dataset of 25K Italian tweets. Joshi et al. (2015) present a dataset of tweets", "startOffset": 0, "endOffset": 87}, {"referenceID": 34, "context": "Tweets Manual: (Riloff et al., 2013; Maynard and Greenwood, 2014; Pt\u00e1cek et al., 2014) Hashtag-based: (Davidov et al.", "startOffset": 15, "endOffset": 86}, {"referenceID": 27, "context": "Tweets Manual: (Riloff et al., 2013; Maynard and Greenwood, 2014; Pt\u00e1cek et al., 2014) Hashtag-based: (Davidov et al.", "startOffset": 15, "endOffset": 86}, {"referenceID": 8, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 30, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 33, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 18, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 3, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 21, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 41, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 10, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 19, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 28, "context": ", 2014) Hashtag-based: (Davidov et al., 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Barbieri et al., 2014a; Joshi et al., 2015; Ghosh et al., 2015a; Bharti et al., 2015; Liebrecht et al., 2013; Bouazizi and Ohtsuki, 2015; Wang et al., 2015; Barbieri et al., 2014b; Bamman and Smith, 2015; Fersini et al., 2015; Khattri et al., 2015; Rajadesingan et al., 2015)", "startOffset": 23, "endOffset": 391}, {"referenceID": 38, "context": "Reddits (Wallace et al., 2014; Wallace, 2015)", "startOffset": 8, "endOffset": 45}, {"referenceID": 40, "context": "Reddits (Wallace et al., 2014; Wallace, 2015)", "startOffset": 8, "endOffset": 45}, {"referenceID": 5, "context": ") (Lukin and Walker, 2013; Reyes and Rosso, 2014; Reyes and Rosso, 2012; Buschmeier et al., 2014; Liu et al., 2014; Filatova, 2012)", "startOffset": 2, "endOffset": 131}, {"referenceID": 22, "context": ") (Lukin and Walker, 2013; Reyes and Rosso, 2014; Reyes and Rosso, 2012; Buschmeier et al., 2014; Liu et al., 2014; Filatova, 2012)", "startOffset": 2, "endOffset": 131}, {"referenceID": 11, "context": ") (Lukin and Walker, 2013; Reyes and Rosso, 2014; Reyes and Rosso, 2012; Buschmeier et al., 2014; Liu et al., 2014; Filatova, 2012)", "startOffset": 2, "endOffset": 131}, {"referenceID": 35, "context": "Other datasets (Tepperman et al., 2006; Kreuz and Caucci, 2007; Veale and Hao, 2010; Rakov and Rosenberg, 2013; Ghosh et al., 2015b)", "startOffset": 15, "endOffset": 132}, {"referenceID": 11, "context": "Ghosh et al. (2015a) use hashtag-based dataset of tweets.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bharti et al. (2015) use a dataset of 50K tweets marked using hashtags.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bharti et al. (2015) use a dataset of 50K tweets marked using hashtags. Liebrecht et al. (2013) identify how the hashtag \u2018#not\u2019 is a popular modern form of expressing sarcasm.", "startOffset": 0, "endOffset": 96}, {"referenceID": 3, "context": "Bharti et al. (2015) use a dataset of 50K tweets marked using hashtags. Liebrecht et al. (2013) identify how the hashtag \u2018#not\u2019 is a popular modern form of expressing sarcasm. The dataset involves tweets that contain such a hashtag. Bouazizi and Ohtsuki (2015) use Sentiment140 dataset of tweets.", "startOffset": 0, "endOffset": 261}, {"referenceID": 3, "context": "Bharti et al. (2015) use a dataset of 50K tweets marked using hashtags. Liebrecht et al. (2013) identify how the hashtag \u2018#not\u2019 is a popular modern form of expressing sarcasm. The dataset involves tweets that contain such a hashtag. Bouazizi and Ohtsuki (2015) use Sentiment140 dataset of tweets. Wang et al. (2015) use a dataset of tweets marked as happy, sad and sarcastic.", "startOffset": 0, "endOffset": 316}, {"referenceID": 1, "context": "Barbieri et al. (2014b) create a dataset using hashtag-based supervision to create multiple labels: politics, sarcasm, humor and irony.", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "Fersini et al. (2015) use a dataset of 8K tweets marked using hashtags.", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "Khattri et al. (2015) use a dataset of tweets, introduced in the past.", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "Khattri et al. (2015) use a dataset of tweets, introduced in the past. They look up the complete twitter timeline (limited to 3200 tweets, by Twitter) to establish context. Rajadesingan et al. (2015) use a dataset of tweets, labeled by hashtagbased supervision along with a historical context of 80 tweets per author.", "startOffset": 0, "endOffset": 200}, {"referenceID": 38, "context": "Wallace et al. (2014) create a corpus of reddit posts of 10K sentences, from 6 reddit topics.", "startOffset": 0, "endOffset": 22}, {"referenceID": 38, "context": "Wallace et al. (2014) create a corpus of reddit posts of 10K sentences, from 6 reddit topics. Wallace (2015) present a dataset of reddit comments: 5625 sentences.", "startOffset": 0, "endOffset": 109}, {"referenceID": 10, "context": "Filatova (2012) use a dataset of around 1000 reviews, marked as sarcastic or not.", "startOffset": 0, "endOffset": 16}, {"referenceID": 5, "context": "Buschmeier et al. (2014) use 1254 Amazon reviews, out of which 437 are ironic.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "Buschmeier et al. (2014) use 1254 Amazon reviews, out of which 437 are ironic. Tsur et al. (2010) consider a large dataset of 66000 amazon reviews.", "startOffset": 0, "endOffset": 98}, {"referenceID": 5, "context": "Buschmeier et al. (2014) use 1254 Amazon reviews, out of which 437 are ironic. Tsur et al. (2010) consider a large dataset of 66000 amazon reviews. Liu et al. (2014) use a dataset from multiple sources such as Amazon, Twitter, Netease and Netcena.", "startOffset": 0, "endOffset": 166}, {"referenceID": 35, "context": "Tepperman et al. (2006) use 131 call center transcripts.", "startOffset": 0, "endOffset": 24}, {"referenceID": 35, "context": "Tepperman et al. (2006) use 131 call center transcripts. Each occurrence of \u2018yeah right\u2019 is marked as sarcastic or not. The goal is to find features in a transcripts that identify which \u2018yeah right\u2019 is sarcastic. Kreuz and Caucci (2007) use 20 sarcastic excerpts and 15 non-sarcastic", "startOffset": 0, "endOffset": 237}, {"referenceID": 12, "context": "Ghosh et al. (2015b) use a crowdsourcing tool to obtain a non-sarcastic version of a sentence if applicable.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bharti et al. (2015) present two rule-based classifiers.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bharti et al. (2015) present two rule-based classifiers. The first uses a parse\u2013based lexicon generation algorithm that creates parse trees of sentences and identifies situation phrases that bear sentiment. If a negative phrase occurs in a positive sentence, it is predicted as sarcastic. The second algorithm aims to capture hyperboles by using interjection and intensifiers occur together. Riloff et al. (2013) present rule-based classifiers that look for a positive verb and a negative situation phrase in a sentence.", "startOffset": 0, "endOffset": 413}, {"referenceID": 20, "context": "Reyes et al. (2012) use features related to ambiguity, unexpectedness, emotional scenario, etc.", "startOffset": 0, "endOffset": 20}, {"referenceID": 20, "context": "Reyes et al. (2012) use features related to ambiguity, unexpectedness, emotional scenario, etc. Ambiguity features cover structural, morpho-syntactic, semantic ambiguity, while unexpectedness features measure semantic relatedness. Riloff et al. (2013) use a set of patterns, specifically positive verbs and negative situation phrases, as features for a classifier.", "startOffset": 0, "endOffset": 252}, {"referenceID": 16, "context": "Liebrecht et al. (2013) introduce bigrams and trigrams as features.", "startOffset": 0, "endOffset": 24}, {"referenceID": 16, "context": "Liebrecht et al. (2013) introduce bigrams and trigrams as features. Reyes et al. (2013) explore skip-gram and character n-gram-based features.", "startOffset": 0, "endOffset": 88}, {"referenceID": 16, "context": "Liebrecht et al. (2013) introduce bigrams and trigrams as features. Reyes et al. (2013) explore skip-gram and character n-gram-based features. Maynard and Greenwood (2014) include seven sets of features.", "startOffset": 0, "endOffset": 172}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators.", "startOffset": 30, "endOffset": 54}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators. Buschmeier et al. (2014) experiment with a suite of features to incorporate ellipsis, hyperbole and imbalance.", "startOffset": 30, "endOffset": 128}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators. Buschmeier et al. (2014) experiment with a suite of features to incorporate ellipsis, hyperbole and imbalance. Joshi et al. (2015) use features corresponding to the linguistic theory of incongruity.", "startOffset": 30, "endOffset": 234}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators. Buschmeier et al. (2014) experiment with a suite of features to incorporate ellipsis, hyperbole and imbalance. Joshi et al. (2015) use features corresponding to the linguistic theory of incongruity. The features are classified into two sets: implicit and explicit incongruitybased features. Pt\u00e1cek et al. (2014) use word-shape and pointedness features given in the form of 24 classes.", "startOffset": 30, "endOffset": 415}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators. Buschmeier et al. (2014) experiment with a suite of features to incorporate ellipsis, hyperbole and imbalance. Joshi et al. (2015) use features corresponding to the linguistic theory of incongruity. The features are classified into two sets: implicit and explicit incongruitybased features. Pt\u00e1cek et al. (2014) use word-shape and pointedness features given in the form of 24 classes. Rajadesingan et al. (2015) use extensions of words, number of flips, readability features in addition to others.", "startOffset": 30, "endOffset": 515}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators. Buschmeier et al. (2014) experiment with a suite of features to incorporate ellipsis, hyperbole and imbalance. Joshi et al. (2015) use features corresponding to the linguistic theory of incongruity. The features are classified into two sets: implicit and explicit incongruitybased features. Pt\u00e1cek et al. (2014) use word-shape and pointedness features given in the form of 24 classes. Rajadesingan et al. (2015) use extensions of words, number of flips, readability features in addition to others. Hern\u00e1ndez-Far\u0131\u0301as et al. (2015) present features that measure semantic relatedness between words using Wordnet::similarity.", "startOffset": 30, "endOffset": 633}, {"referenceID": 1, "context": "Apart from a subset of these, Barbieri et al. (2014a) use frequency and rarity of words as indicators. Buschmeier et al. (2014) experiment with a suite of features to incorporate ellipsis, hyperbole and imbalance. Joshi et al. (2015) use features corresponding to the linguistic theory of incongruity. The features are classified into two sets: implicit and explicit incongruitybased features. Pt\u00e1cek et al. (2014) use word-shape and pointedness features given in the form of 24 classes. Rajadesingan et al. (2015) use extensions of words, number of flips, readability features in addition to others. Hern\u00e1ndez-Far\u0131\u0301as et al. (2015) present features that measure semantic relatedness between words using Wordnet::similarity. Liu et al. (2014) introduce POS sequences and semantic imbalance as features.", "startOffset": 30, "endOffset": 743}, {"referenceID": 29, "context": "Riloff et al. (2013) compare rule-based techniques with a SVM-based classifier.", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "Liebrecht et al. (2013) use balanced winnow algorithm in order to determine high-ranking features.", "startOffset": 0, "endOffset": 24}, {"referenceID": 21, "context": "Liebrecht et al. (2013) use balanced winnow algorithm in order to determine high-ranking features. Reyes et al. (2013) use", "startOffset": 0, "endOffset": 119}, {"referenceID": 39, "context": "Wang et al. (2015) use SVM-HMM in order to incorporate sequence nature of output labels in a conversation.", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "Liu et al. (2014) compare several classification approaches including bagging, boosting, etc.", "startOffset": 0, "endOffset": 18}, {"referenceID": 14, "context": "Gonz\u00e1lezIb\u00e1nez et al. (2011) show that unigram-based features outperform the use of a subset of words as derived from a sentiment lexicon.", "startOffset": 0, "endOffset": 29}, {"referenceID": 14, "context": "Gonz\u00e1lezIb\u00e1nez et al. (2011) show that unigram-based features outperform the use of a subset of words as derived from a sentiment lexicon. They compare the accuracy of the sarcasm classifier with the human ability to detect sarcasm. While the best classifier achieves 57.41%, the human performance for sarcasm identification is 62.59%. Reyes and Rosso (2012) observe that sentiment-based features are their top discriminating features.", "startOffset": 0, "endOffset": 359}, {"referenceID": 14, "context": "Gonz\u00e1lezIb\u00e1nez et al. (2011) show that unigram-based features outperform the use of a subset of words as derived from a sentiment lexicon. They compare the accuracy of the sarcasm classifier with the human ability to detect sarcasm. While the best classifier achieves 57.41%, the human performance for sarcasm identification is 62.59%. Reyes and Rosso (2012) observe that sentiment-based features are their top discriminating features. The logistic classifier in Rakov and Rosenberg (2013) results in an accuracy of 81.", "startOffset": 0, "endOffset": 490}, {"referenceID": 14, "context": "Gonz\u00e1lezIb\u00e1nez et al. (2011) show that unigram-based features outperform the use of a subset of words as derived from a sentiment lexicon. They compare the accuracy of the sarcasm classifier with the human ability to detect sarcasm. While the best classifier achieves 57.41%, the human performance for sarcasm identification is 62.59%. Reyes and Rosso (2012) observe that sentiment-based features are their top discriminating features. The logistic classifier in Rakov and Rosenberg (2013) results in an accuracy of 81.5%. Joshi et al. (2015) present an analysis of errors like incongruity due to numbers and granularity of annotation.", "startOffset": 0, "endOffset": 543}, {"referenceID": 14, "context": "Gonz\u00e1lezIb\u00e1nez et al. (2011) show that unigram-based features outperform the use of a subset of words as derived from a sentiment lexicon. They compare the accuracy of the sarcasm classifier with the human ability to detect sarcasm. While the best classifier achieves 57.41%, the human performance for sarcasm identification is 62.59%. Reyes and Rosso (2012) observe that sentiment-based features are their top discriminating features. The logistic classifier in Rakov and Rosenberg (2013) results in an accuracy of 81.5%. Joshi et al. (2015) present an analysis of errors like incongruity due to numbers and granularity of annotation. Rajadesingan et al. (2015) show that historical features along with flip-based features are the most discriminating features.", "startOffset": 0, "endOffset": 663}, {"referenceID": 14, "context": "Gonz\u00e1lezIb\u00e1nez et al. (2011) show that unigram-based features outperform the use of a subset of words as derived from a sentiment lexicon. They compare the accuracy of the sarcasm classifier with the human ability to detect sarcasm. While the best classifier achieves 57.41%, the human performance for sarcasm identification is 62.59%. Reyes and Rosso (2012) observe that sentiment-based features are their top discriminating features. The logistic classifier in Rakov and Rosenberg (2013) results in an accuracy of 81.5%. Joshi et al. (2015) present an analysis of errors like incongruity due to numbers and granularity of annotation. Rajadesingan et al. (2015) show that historical features along with flip-based features are the most discriminating features. These are also the features that Khattri et al. (2015) use.", "startOffset": 0, "endOffset": 817}, {"referenceID": 3, "context": "Bharti et al. (2015) report a F-score of around 84%.", "startOffset": 0, "endOffset": 21}, {"referenceID": 35, "context": "Tsur et al. (2010) extract sarcastic patterns from a seed set of labeled sentences.", "startOffset": 0, "endOffset": 19}, {"referenceID": 27, "context": "Pt\u00e1cek et al. (2014) also use a similar approach for Czech and English tweets.", "startOffset": 0, "endOffset": 21}, {"referenceID": 36, "context": "(Tsur et al., 2010) Sarcastic patterns, Punctuations", "startOffset": 0, "endOffset": 19}, {"referenceID": 30, "context": "(Reyes et al., 2012) Ambiguity-based, semantic relatedness", "startOffset": 0, "endOffset": 20}, {"referenceID": 34, "context": "(Riloff et al., 2013) Sarcastic patterns (Positive verbs, negative phrases)", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "(Liebrecht et al., 2013) N-grams, emotion marks, intensifiers", "startOffset": 0, "endOffset": 24}, {"referenceID": 33, "context": "(Reyes et al., 2013) Skip-grams, Polarity skip-grams", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "(Buschmeier et al., 2014) Interjection, ellipsis, hyperbole, imbalance-based", "startOffset": 0, "endOffset": 25}, {"referenceID": 18, "context": "(Joshi et al., 2015) Unigrams, Implicit incongruity-based, Explicit incongruity-based", "startOffset": 0, "endOffset": 20}, {"referenceID": 28, "context": "(Rajadesingan et al., 2015) Readability, flips, etc.", "startOffset": 0, "endOffset": 27}, {"referenceID": 16, "context": "(Hern\u00e1ndez-Far\u0131\u0301as et al., 2015) Length, capitalization, semantic similarity", "startOffset": 0, "endOffset": 32}, {"referenceID": 22, "context": "(Liu et al., 2014) POS sequences, Semantic imbalance.", "startOffset": 0, "endOffset": 18}, {"referenceID": 27, "context": "(Pt\u00e1cek et al., 2014) Word shape, Pointedness, etc.", "startOffset": 0, "endOffset": 21}, {"referenceID": 18, "context": "Joshi et al. (2015) adapt this algorithm by eliminating subsumption, and show that it adds value.", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "Khattri et al. (2015) follow the intuition \u2018A tweet is sarcastic either because it has words of contrasting sentiment in it, or because there is sentiment that contrasts with historical sentiment\u2019.", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": "Khattri et al. (2015) follow the intuition \u2018A tweet is sarcastic either because it has words of contrasting sentiment in it, or because there is sentiment that contrasts with historical sentiment\u2019. Historical tweets by the same author are considered as the context. Named entity phrases in the target tweet are looked up in the timeline of the author in order to gather the true sentiment of the author. This historical sentiment is then used to predict whether the author is likely to be sarcastic, given the sentiment expressed towards the entity in the target tweet. Rajadesingan et al. (2015) incorporate context about author using the author\u2019s past tweets.", "startOffset": 0, "endOffset": 597}, {"referenceID": 19, "context": "Khattri et al. (2015) follow the intuition \u2018A tweet is sarcastic either because it has words of contrasting sentiment in it, or because there is sentiment that contrasts with historical sentiment\u2019. Historical tweets by the same author are considered as the context. Named entity phrases in the target tweet are looked up in the timeline of the author in order to gather the true sentiment of the author. This historical sentiment is then used to predict whether the author is likely to be sarcastic, given the sentiment expressed towards the entity in the target tweet. Rajadesingan et al. (2015) incorporate context about author using the author\u2019s past tweets. This context is captured as features for a classifier. The features deal with various dimensions. They use features about author\u2019s familiarity with twitter (in terms of use of hashtags), familiarity with language (in terms of words and structures), and familiarity with sarcasm. Bamman and Smith (2015) consider historical context in features such as historical salient terms, historical topic, profile info, historical sentiment (how likely is he/she to be negative), etc.", "startOffset": 0, "endOffset": 965}, {"referenceID": 18, "context": "Joshi et al. (2015) show that concatenation of the previous post in a discussion forum thread along with the target post leads to an improvement in precision.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "Joshi et al. (2015) show that concatenation of the previous post in a discussion forum thread along with the target post leads to an improvement in precision. Wallace (2015) look at comments in the thread structure to obtain context for sarcasm detection.", "startOffset": 0, "endOffset": 174}, {"referenceID": 41, "context": "Wang et al. (2015) use three types of context: history, conversation and topic.", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "(Liebrecht et al., 2013) show how #not can be used to express sarcasm - while the rest of the sentence is non-sarcastic.", "startOffset": 0, "endOffset": 24}, {"referenceID": 32, "context": "Tsur et al. (2010) use a dataset with a small set of sentences are marked as sarcastic.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "5% of tweets in the Italian dataset given by Barbieri et al. (2014a) are sarcastic.", "startOffset": 45, "endOffset": 69}, {"referenceID": 1, "context": "5% of tweets in the Italian dataset given by Barbieri et al. (2014a) are sarcastic. On the other hand, Rakov and Rosenberg (2013) present a balanced dataset of 15k tweets.", "startOffset": 45, "endOffset": 130}, {"referenceID": 1, "context": "5% of tweets in the Italian dataset given by Barbieri et al. (2014a) are sarcastic. On the other hand, Rakov and Rosenberg (2013) present a balanced dataset of 15k tweets. Liu et al. (2014) focus on this imbalance in data, and present sarcasm detection that is robust to this imbalance.", "startOffset": 45, "endOffset": 190}, {"referenceID": 34, "context": "Tsur et al. (2010) indicate an agreement of 0.", "startOffset": 0, "endOffset": 19}, {"referenceID": 34, "context": "Tepperman et al. (2006) observe an agreement of 52.", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "Fersini et al. (2015) report an agreement", "startOffset": 0, "endOffset": 22}, {"referenceID": 34, "context": "Riloff et al. (2013) observe an agreement of 0.", "startOffset": 0, "endOffset": 21}, {"referenceID": 30, "context": "In a statistical classifier, surface polarity may be used directly as a feature use polarity of the tweet as a feature (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015).", "startOffset": 119, "endOffset": 210}, {"referenceID": 18, "context": "In a statistical classifier, surface polarity may be used directly as a feature use polarity of the tweet as a feature (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015).", "startOffset": 119, "endOffset": 210}, {"referenceID": 28, "context": "In a statistical classifier, surface polarity may be used directly as a feature use polarity of the tweet as a feature (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015).", "startOffset": 119, "endOffset": 210}, {"referenceID": 3, "context": "Bharti et al. (2015) is a rule-based approach that predicts a sentence as sarcastic if a negative phrase occurs in a positive sentence.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bharti et al. (2015) is a rule-based approach that predicts a sentence as sarcastic if a negative phrase occurs in a positive sentence. As described earlier, Khattri et al. (2015) uses sentiment of a past tweet by the author to predict sarcasm.", "startOffset": 0, "endOffset": 180}, {"referenceID": 3, "context": "Bharti et al. (2015) is a rule-based approach that predicts a sentence as sarcastic if a negative phrase occurs in a positive sentence. As described earlier, Khattri et al. (2015) uses sentiment of a past tweet by the author to predict sarcasm. In a statistical classifier, surface polarity may be used directly as a feature use polarity of the tweet as a feature (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015). Reyes et al. (2013) capture polarity value in terms of two emotion dimensions: activation and pleasantness.", "startOffset": 0, "endOffset": 477}, {"referenceID": 3, "context": "Bharti et al. (2015) is a rule-based approach that predicts a sentence as sarcastic if a negative phrase occurs in a positive sentence. As described earlier, Khattri et al. (2015) uses sentiment of a past tweet by the author to predict sarcasm. In a statistical classifier, surface polarity may be used directly as a feature use polarity of the tweet as a feature (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015). Reyes et al. (2013) capture polarity value in terms of two emotion dimensions: activation and pleasantness. Buschmeier et al. (2014) incorporate sentiment imbalance as a feature.", "startOffset": 0, "endOffset": 590}, {"referenceID": 3, "context": "Bharti et al. (2015) is a rule-based approach that predicts a sentence as sarcastic if a negative phrase occurs in a positive sentence. As described earlier, Khattri et al. (2015) uses sentiment of a past tweet by the author to predict sarcasm. In a statistical classifier, surface polarity may be used directly as a feature use polarity of the tweet as a feature (Reyes et al., 2012; Joshi et al., 2015; Rajadesingan et al., 2015; Bamman and Smith, 2015). Reyes et al. (2013) capture polarity value in terms of two emotion dimensions: activation and pleasantness. Buschmeier et al. (2014) incorporate sentiment imbalance as a feature. Sentiment imbalance is a situation where star rating of a review disagrees with the surface polarity. Bouazizi and Ohtsuki (2015) cascade sarcasm detection and sentiment detection and observes an improvement of about 4% in accuracy when sentiment detection is aware of sarcastic nature.", "startOffset": 0, "endOffset": 766}, {"referenceID": 21, "context": "Liebrecht et al. (2013) state that \u201cdetecting sarcasm is like a needle in a haystack.", "startOffset": 0, "endOffset": 24}, {"referenceID": 21, "context": "Liebrecht et al. (2013) state that \u201cdetecting sarcasm is like a needle in a haystack. Some approaches focus on mitigating the effects of this skew. In Liu et al. (2014), a multi-strategy ensemble learning approach is used that uses ensembles and majority voting.", "startOffset": 0, "endOffset": 169}, {"referenceID": 21, "context": "Liebrecht et al. (2013) state that \u201cdetecting sarcasm is like a needle in a haystack. Some approaches focus on mitigating the effects of this skew. In Liu et al. (2014), a multi-strategy ensemble learning approach is used that uses ensembles and majority voting. Similarly, in order to deal with sparse features and skew of data, Wallace (2015) introduce a LSS-regularization strategy.", "startOffset": 0, "endOffset": 345}, {"referenceID": 21, "context": "Liebrecht et al. (2013) state that \u201cdetecting sarcasm is like a needle in a haystack. Some approaches focus on mitigating the effects of this skew. In Liu et al. (2014), a multi-strategy ensemble learning approach is used that uses ensembles and majority voting. Similarly, in order to deal with sparse features and skew of data, Wallace (2015) introduce a LSS-regularization strategy. Thus, they use a sparsifying L1 regularizer over contextual features and L2-norm for bag of word features. Liebrecht et al. (2013) report AUC for balanced as well as skewed datasets, to demonstrate the benefit of their classifier.", "startOffset": 0, "endOffset": 517}, {"referenceID": 21, "context": "Implicit sentiment detection & sarcasm: Based on past work, it is well-established that sarcasm is closely linked to sentiment incongruity (Liebrecht et al., 2013).", "startOffset": 139, "endOffset": 163}, {"referenceID": 18, "context": "Incongruity in numbers: Joshi et al. (2015) point out how numerical values convey sentiment and hence, is related to sarcasm.", "startOffset": 24, "endOffset": 44}, {"referenceID": 22, "context": "Culture-specific aspects of sarcasm detection: As shown in Liu et al. (2014), sarcasm is closely related to language/culture-specific traits.", "startOffset": 59, "endOffset": 77}], "year": 2016, "abstractText": "Automatic detection of sarcasm has witnessed interest from the sentiment analysis research community. With diverse approaches, datasets and analyses that have been reported, there is an essential need to have a collective understanding of the research in this area. In this survey of automatic sarcasm detection, we describe datasets, approaches (both supervised and rule-based), and trends in sarcasm detection research. We also present a research matrix that summarizes past work, and list pointers to future work.", "creator": "LaTeX with hyperref package"}}}