{"id": "1602.03606", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2016", "title": "Variations of the Similarity Function of TextRank for Automated Summarization", "abstract": "This article presents new alternatives to the similarity function for the TextRank algorithm for automatic summarization of texts. We describe the generalities of the algorithm and the different functions we propose. Some of these variants achieve a significative improvement using the same metrics and dataset as the original publication.", "histories": [["v1", "Thu, 11 Feb 2016 02:39:21 GMT  (43kb,D)", "http://arxiv.org/abs/1602.03606v1", "8 pages, 2 figures. Presented at the Argentine Symposium on Artificial Intelligence (ASAI) 2015 - 44 JAIIO (September 2015)"]], "COMMENTS": "8 pages, 2 figures. Presented at the Argentine Symposium on Artificial Intelligence (ASAI) 2015 - 44 JAIIO (September 2015)", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["federico barrios", "federico l\\'opez", "luis argerich", "rosa wachenchauzer"], "accepted": false, "id": "1602.03606"}, "pdf": {"name": "1602.03606.pdf", "metadata": {"source": "CRF", "title": "Variations of the Similarity Function of TextRank for Automated Summarization", "authors": ["Federico Barrios", "Federico L\u00f3pez", "Luis Argerich", "Rosita Wachenchauzer"], "emails": ["fbarrios@fi.uba.ar", "fjlopez@fi.uba.ar"], "sections": [{"heading": null, "text": "Keywords: TextRank variations, automated summary, information retrieval ranking functions"}, {"heading": "1 Introduction", "text": "In the field of natural language processing, an extractive summary task can be described as selecting the most important sentences in a document. Using different levels of compression, a summarized version of the document can be obtained of any length. TextRank is a graph-based extractive summary algorithm. It is independent of domain and language, as it does not require deep linguistic knowledge or domain or language-specific commented corpora [16]. These characteristics make the algorithm widely used: it provides a good summary of structured texts such as news articles, but has also shown good results in other areas of use, such as summarizing transcriptions [8] and evaluating the credibility of web content [1]. In this article, we present various suggestions for the construction of the graph and report on the results obtained with them. The first sections of this article describe previous work in this area and an overview of the TextRank algorithm. We then present the variations and describe the different metrics we use and the results obtained for the evaluation."}, {"heading": "2 Previous work", "text": "The field of automated summaries has aroused interest since the late 1950s [14]. Traditional methods for summarizing texts analyze the frequency of words or sentences in the first paragraphs of the text to identify the most important Xiv: 160 2.03 606v 1 [cs.C L] February 11, 20lexical elements. Mainstream research in this area emphasizes extractive approaches to summarizing with statistical methods [4]. Several statistical models have been developed based on training corpses to combine different hayristics using keywords, position and length of sentences, word frequency or titles [13]. Other methods are based on the representation of the text as a graph. The graph-based ranking approaches consider the intrinsic structure of the texts, rather than treating texts as simple aggregations of terms. Thus, it is possible to capture and express richer information in order to determine important concepts [19]. The selected text phrases, which can be used in graphs, 18 or 6, are to form phrases with each other."}, {"heading": "3 TextRank", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Description", "text": "TextRank is an unattended algorithm for the automated summary of texts that can also be used to obtain the most important keywords in a document. It was introduced by Rada Mihalcea and Paul Tarau in [17]. The algorithm applies a variation of PageRank [20] to a graph specifically designed for the purpose of summarizing, resulting in a ranking of the elements in the graph: the most important elements are those that better describe the text. This approach enables TextRank to create summaries without the need for a training corpus or labeling, and allows the algorithm to be used in different languages."}, {"heading": "3.2 Text as a Graph", "text": "For the purpose of automatic summary, TextRank models each document as a graph using sentences as nodes [3]. A function for calculating the similarity of sentences is required to form edges in between. This function is used to weight the edges of the graph, the higher the similarity between sentences, the more significant the edge between them will be in the graph. In the area of a random walker, as it is often used in PageRank [20], we can say that we tend to go from one sentence to another if they are very similiar.TextRank determines the similarity relationship between two sentences based on the content they share. This overlap is simply calculated as the number of common lexical tokens between them, divided by the length of each to avoid promoting long sentences. The function presented in the original algorithm can be formalized as: Definition 1."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Our Variations", "text": "This section describes the various modifications we propose to the original TextRank algorithm. These ideas are based on changing the way in which the distances between sentences are calculated to weight the edges of the graph used for the PageRank. These similarities are orthogonal to the TextRank model and can therefore be easily integrated into the algorithm. We found some of these variations in order to achieve significant improvements over the original algorithm. We used a classic TF-IDF model to represent the documents as vectors and calculated the cosine between vectors as its length [9]. Cosine distance The cosine-like similarity is a widely used metric to represent texts as vectors. We used a classic TF-IDF model to represent the vectors and calculated the cosine between vectors as a measure of similarity."}, {"heading": "4.2 Evaluation", "text": "To test the proposed variations, we used the database of the Document Understanding Conference (DUC) of 2002 [5]. The corpus comprises 567 documents grouped to 20% of their size, and is the same corpus used in [17]. To evaluate the results, we used version 1.5.5 of the ROUGE package [12]. The configuration settings were the same as in DUC, where ROUGE-1, ROUGE2 and ROUGE-SU4 were used as metrics, using a 95% confidence level and applying stem. The end result is an average of these three values. To verify the correct behavior of our test suite, we implemented the reference method used in [17], which extracts the first sets of each document. We found that the resulting values of the original algorithm were identical to those in [17]: an improvement of 2.3% over the baseline."}, {"heading": "4.3 Results", "text": "We tested LCS, Cosine Sim, BM25 and BM25 + in different ways to weight the edges of the TextRank chart. We achieved the best results with BM25 and BM25 + using the correction formula shown in Equation 3. We achieved an improvement of 2.92% over the original TextRank result with BM25 and \u03b5 = 0.25. The following chart shows the results for the various variations we proposed. Also, the result of Cosine Similarity was satisfactory with an improvement of 2.54% over the original method. LCS variation also performed better than the original TextRank algorithm with a 1.40% improvement. Time performance was also improved. We were able to process the 567 documents from the DUC2002 database in 84% of the time required in the original version."}, {"heading": "5 Reference Implementation and Gensim Contribution", "text": "A reference implementation of our proposals was encoded as Python module 3 and can be obtained for testing and reproducing the results. We also contributed the BM25 TextRank algorithm to Gensim project4 [21]."}, {"heading": "6 Conclusions", "text": "This paper presented three different variations of the TextRank auto-summary algorithm. The three alternatives significantly improved the results of the algorithm by using the same test configuration as in the original release. Since TextRank performs 2.84% above baseline, our improvement of 2.92% over the TextRank score is an important result. Combining TextRank with modern information retrieval ranking functions such as BM25 and BM25 + creates a robust auto-summary methodology that performs better than the standard techniques used to date. Based on these results, we propose using BM25 together with TextRank for the task of unattended auto-summary of texts. The results obtained and the examples analyzed show that this variation is better than the original TextRank algorithm without sacrificing performance."}], "references": [{"title": "Application of textrank algorithm for credibility assessment", "author": ["B. Balcerzak", "W. Jaworski", "A. Wierzbicki"], "venue": "Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Sentence fusion for multidocument news summarization", "author": ["R. Barzilay", "K. McKeown"], "venue": "Computational Linguistics 31(3),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Introduction to Information Retrieval", "author": ["Christopher D. Manning", "H.S. Prabhakar Raghavan"], "venue": "Cambridge University Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "A survey on automatic text summarization", "author": ["D. Das", "A.F.T. Martins"], "venue": "Tech. rep., Carnegie Mellon University, Language Technologies Institute", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Using lexical chains for keyword extraction", "author": ["G. Ercan", "I. Cicekli"], "venue": "Inf. Process. Manage", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "J. Artif. Intell. Res. (JAIR) 22,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Clusterrank: a graph based method for meeting summarization", "author": ["N. Garg", "B. Favre", "K. Riedhammer", "D. Hakkani-T\u00fcr"], "venue": "INTERSPEECH", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology", "author": ["D. Gusfield"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Measuring the power of nodes in digraphs", "author": ["P.J.J. Herings", "G. van der Laan", "D. Talman"], "venue": "Research Memorandum 007,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Authoritative sources in a hyperlinked environment", "author": ["J.M. Kleinberg"], "venue": "J. ACM 46(5),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Rouge: a package for automatic evaluation of summaries", "author": ["C.Y. Lin"], "venue": "Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004), Barcelona, Spain", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Identifying topics by position", "author": ["C.Y. Lin", "E.H. Hovy"], "venue": "Proceedings of 5th Conference on Applied Natural Language Processing. Washington D.C.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "The automatic creation of literature abstracts", "author": ["H.P. Luhn"], "venue": "IBM J. Res. Dev", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1958}, {"title": "Lower-bounding term frequency normalization", "author": ["Y. Lv", "C. Zhai"], "venue": "Proceedings of the 20th ACM Conference on Information and Knowledge Management, CIKM 2011, Glasgow, United Kingdom, October 24-28, 2011. pp. 7\u201316", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Graph-based ranking algorithms for sentence extraction, applied to text summarization", "author": ["R. Mihalcea"], "venue": "Proceedings of the ACL 2004 on Interactive Poster and Demonstration Sessions. ACLdemo \u201904,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Textrank: Bringing order into texts", "author": ["R. Mihalcea", "P. Tarau"], "venue": "Lin, D., Wu, D. (eds.) Proceedings of EMNLP 2004. pp. 404\u2013411. Association for Computational Linguistics, Barcelona, Spain", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Automatic text summarization by paragraph extraction", "author": ["M. Mitrat", "A. Singhal", "C. Buckleytt"], "venue": "Intelligent Scalable Text Summarization. pp", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Learning similarity functions in graphbased document summarization", "author": ["Y. Ouyang", "W. Li", "F. Wei", "Q. Lu"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "The pagerank citation ranking: Bringing order to the web", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": "Proceedings of the 7th International World Wide Web Conference", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["R. \u0158eh\u030au\u0159ek", "P. Sojka"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Okapi at TREC-3", "author": ["S. Robertson", "S. Walker", "S. Jones", "M. Hancock-Beaulieu", "M. Gatford"], "venue": "Proceedings of The Third Text REtrieval Conference,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "Automatic text structuring and summarization", "author": ["G. Salton", "A. Singhal", "M. Mitra", "C. Buckley"], "venue": "Information Processing and Management 33(2), 193 \u2013 207", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "Modern information retrieval: A brief overview", "author": ["A. Singhal"], "venue": "Bulletin of the IEEE Computer Society Technical Committee on Data Engineering", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}], "referenceMentions": [{"referenceID": 14, "context": "It is domain and language independent since it does not require deep linguistic knowledge, nor domain or language specific annotated corpora [16].", "startOffset": 141, "endOffset": 145}, {"referenceID": 6, "context": "These features makes the algorithm widely used: it performs well summarizing structured text like news articles, but it has also shown good results in other usages such as summarizing meeting transcriptions [8] and assessing web content credibility [1].", "startOffset": 207, "endOffset": 210}, {"referenceID": 0, "context": "These features makes the algorithm widely used: it performs well summarizing structured text like news articles, but it has also shown good results in other usages such as summarizing meeting transcriptions [8] and assessing web content credibility [1].", "startOffset": 249, "endOffset": 252}, {"referenceID": 12, "context": "The field of automated summarization has attracted interest since the late 50\u2019s [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 3, "context": "The mainstream research in this field emphasizes extractive approaches to summarization using statistical methods [4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 11, "context": "Several statistical models have been developed based on training corpora to combine different heuristics using keywords, position and length of sentences, word frequency or titles [13].", "startOffset": 180, "endOffset": 184}, {"referenceID": 17, "context": "Thus it is able to capture and express richer information in determining important concepts [19].", "startOffset": 92, "endOffset": 96}, {"referenceID": 4, "context": "The selected text fragments to use in the graph construction can be phrases [6], sentences [14], or paragraphs [18].", "startOffset": 76, "endOffset": 79}, {"referenceID": 12, "context": "The selected text fragments to use in the graph construction can be phrases [6], sentences [14], or paragraphs [18].", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "The selected text fragments to use in the graph construction can be phrases [6], sentences [14], or paragraphs [18].", "startOffset": 111, "endOffset": 115}, {"referenceID": 1, "context": "According to these approach the most important sentences are the most connected ones in the graph and are used for building a final summary [2].", "startOffset": 140, "endOffset": 143}, {"referenceID": 17, "context": "Also, some authors have proposed combinations of the previous with supervised learning functions [19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 21, "context": "These algorithms use different information retrieval techniques to determine the most important sentences (vertices) and build the summary [23].", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "The TextRank algorithm developed by Mihalcea and Tarau [17] and the LexRank algorithm by Erkan and Radev [7] are based in ranking the lexical units of the text (sentences or words) using variations of PageRank [20].", "startOffset": 55, "endOffset": 59}, {"referenceID": 5, "context": "The TextRank algorithm developed by Mihalcea and Tarau [17] and the LexRank algorithm by Erkan and Radev [7] are based in ranking the lexical units of the text (sentences or words) using variations of PageRank [20].", "startOffset": 105, "endOffset": 108}, {"referenceID": 18, "context": "The TextRank algorithm developed by Mihalcea and Tarau [17] and the LexRank algorithm by Erkan and Radev [7] are based in ranking the lexical units of the text (sentences or words) using variations of PageRank [20].", "startOffset": 210, "endOffset": 214}, {"referenceID": 9, "context": "Other graph-based ranking algorithms such as HITS [11] or Positional Function [10] may be also applied.", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "Other graph-based ranking algorithms such as HITS [11] or Positional Function [10] may be also applied.", "startOffset": 78, "endOffset": 82}, {"referenceID": 15, "context": "It was introduced by Rada Mihalcea and Paul Tarau in [17].", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "The algorithm applies a variation of PageRank [20] over a graph constructed specifically for the task of summarization.", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "For the task of automated summarization, TextRank models any document as a graph using sentences as nodes [3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 18, "context": "In the domain of a Random Walker, as used frequently in PageRank [20], we can say that we are more likely to go from one sentence to another if they are very similar.", "startOffset": 65, "endOffset": 69}, {"referenceID": 7, "context": "Longest Common Substring From two sentences we identify the longest common substring and report the similarity to be its length [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 0, "context": "Since the vectors are defined to be positive, the cosine results in values in the range [0,1] where a value of 1 represents identical vectors and 0 represents orthogonal vectors [24].", "startOffset": 88, "endOffset": 93}, {"referenceID": 22, "context": "Since the vectors are defined to be positive, the cosine results in values in the range [0,1] where a value of 1 represents identical vectors and 0 represents orthogonal vectors [24].", "startOffset": 178, "endOffset": 182}, {"referenceID": 20, "context": "BM25 is a variation of the TF-IDF model using a probabilistic model [22].", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "We also used BM25+, a variation of BM25 that changes the way long documents are penalized [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "The corpus has 567 documents that are summarized to 20% of their size, and is the same corpus used in [17].", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": "5 of the ROUGE package [12].", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "To check the correct behaviour of our test suite we implemented the reference method used in [17], which extracts the first sentences of each document.", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "We found the resulting scores of the original algorithm to be identical to those reported in [17]: a 2.", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "We also contributed the BM25-TextRank algorithm to the Gensim project [21].", "startOffset": 70, "endOffset": 74}], "year": 2016, "abstractText": "This article presents new alternatives to the similarity function for the TextRank algorithm for automated summarization of texts. We describe the generalities of the algorithm and the different functions we propose. Some of these variants achieve a significative improvement using the same metrics and dataset as the original publication.", "creator": "LaTeX with hyperref package"}}}