{"id": "1602.03960", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2016", "title": "TabMCQ: A Dataset of General Knowledge Tables and Multiple-choice Questions", "abstract": "We describe two new related resources that facilitate modelling of general knowledge reasoning in 4th grade science exams. The first is a collection of curated facts in the form of tables, and the second is a large set of crowd-sourced multiple-choice questions covering the facts in the tables. Through the setup of the crowd-sourced annotation task we obtain implicit alignment information between questions and tables. We envisage that the resources will be useful not only to researchers working on question answering, but also to people investigating a diverse range of other applications such as information extraction, question parsing, answer type identification, and lexical semantic modelling.", "histories": [["v1", "Fri, 12 Feb 2016 03:54:43 GMT  (76kb,D)", "http://arxiv.org/abs/1602.03960v1", "Keywords: Data, General Knowledge, Tables, Question Answering, MCQ, Crowd-sourcing, Mechanical Turk"]], "COMMENTS": "Keywords: Data, General Knowledge, Tables, Question Answering, MCQ, Crowd-sourcing, Mechanical Turk", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sujay kumar jauhar", "peter turney", "eduard hovy"], "accepted": false, "id": "1602.03960"}, "pdf": {"name": "1602.03960.pdf", "metadata": {"source": "CRF", "title": "TabMCQ: A Dataset of General Knowledge Tables and Multiple-choice Questions", "authors": ["Sujay Kumar Jauhar", "Peter Turney", "Eduard Hovy"], "emails": ["hovy}@cs.cmu.edu", "petert@allenai.org"], "sections": [{"heading": null, "text": "Keywords: general knowledge, tables, answering questions, MCQ, crowd-sourcing, Mechanical Turk"}, {"heading": "1. Introduction", "text": "The Aristo project at AI2 (Clark, 2015) uses standardized science testing as a driver for research in artificial intelligence. Aristo's question-and-answer format exposes a variety of interesting issues and challenges in NLP (Weston et al., 2015), such as information extraction, semantic modeling, and reasoning. An important component of a system that answers questions (QA) is a store of background knowledge for fact-checking and reasoning. This store can be in a variety of modes and formalities: large-scale extracted and curated knowledge bases (Fader et al., 2014), structured models like Markov Logic Networks (Khot et al., 2015), or simple text corporations in retrievable approaches (Tellex et al., 2003)."}, {"heading": "2. Related Work", "text": "Pasupat and Liang (2015) create a dataset of QA pairs over tables. However, their annotation structure does not impose structural constraints on tables and generates simple QA pairs instead of MCQs. (Yin et al., 2015) and (Neelakantan et al., 2015) use tables related to answering questions, but deal with synthetically generated query data for these tables. More generally, tables refer to QA related to queries over relational databases (Cafarella et al., 2008; Pimplikar and Sarawagi, 2012). Concerning crowdsourcing for the creation of questions, Aydin et al. (2014) harvest MCQs via a gamified app. However, their work does not relate to tables. Monolingual datasets have also been examined separately, for example by Brockett (2007) in the context of Textual Entailment."}, {"heading": "3. Motivation for the Dataset", "text": "Our motivation for creating and publishing this data stems from promising preliminary results on the use of tables and their alignment with MCQs to solve AI2 \"s Aristo challenge. We commented on 77 of the 108 Grade 4 scientific exam questions in the Regents dataset 2 for table alignment, and the resulting data was just like the one in this paper, but only on a fraction of the scale. Even with this small amount of data, we were able to build a system that could match our best internal solvers of the Aristo challenge. A larger dataset will allow us to surpass this performance and also investigate models that require larger amounts of data (such as neural networks). A detailed description of our system goes beyond the scope of this paper, but in summary, it uses a feature-rich log-linear model to test table cells for their relevance to answering a2http: / / allenai.org / / data / regent. Xiv 160: 2.01 60b [content / / allenai.org]"}, {"heading": "4. General Knowledge Tables", "text": "In this section we present a series of tables of curated natural language facts. We discuss their semi-structured form and their usefulness for the presentation of knowledge. Afterwards we explain the data and possibilities of their extension."}, {"heading": "4.1. Tables as a Form of Knowledge Representation", "text": "An example of the type of table we are constructing is in Table 1. This format is semi-structured: The rows of the table (with the exception of the header) are essentially a list of sentences, but with well-defined recurring fill patterns. Along with the header, these patterns divide the rows into meaningful columns. The resulting table structure has some interesting semantics. A row in a table corresponds to a fact in world3. The cells in a row correspond to concepts, units or processes involved in this fact. A substantive column4 corresponds to a group of concepts, units or processes that are of the same type. The header of the column is an abstract description of the type. We can consider the header as a hypernym and the cells in the column below as co-hyponyms of the header. The structure of the tables also represents analogies."}, {"heading": "4.2. The Dataset", "text": "Our spreadsheet dataset was largely created manually by the second author of this paper, and a Knowl-3Rows can also be considered a predicate or more generally a frame with typed reasons.4We distinguish these from fill columns that contain only one recurring pattern, and no information in their headers engineer.The target area for the tables was the science exam of the 4th grade regents. The majority of the tables were designed to include topics and facts in this exam (with additional facts added once a related topic has been identified), with the remainder targeted at an additional internal questionnaire of 500 questions.The dataset consists of 65 handmade tables organized by topic, ranging from limited tables such as the one about phase changes to virtually unlimited tables, such as the type of energy used in performing an action. An additional collection of 5 semi-automatically generated tables contains a heterogeneous mix of 44 additional facts, each of which are automatically represented by five facts across a total of 51 times."}, {"heading": "4.3. Future Extensions", "text": "Although the tables are reasonably complete in terms of the target area for which they were created, they are not comprehensive for an endless search for knowledge. We are exploring ways to expand the tables semi-automatically to cover a larger collection of facts by using the set of existing facts as seed. An in-house fuzzy search engine allows us to search patterns that consist not only of words and wild card symbols, but also parts of the language and semantically related concepts. An interesting way for future research is to fully automate the process of this enhancement and even create semi-structured knowledge tables for targeted queries."}, {"heading": "5. Crowd-sourcing Multiple-choice Questions from Tables", "text": "In this section, we will outline the MCQs generated from tables. First, we will describe the annotation task and argue for constraints on the table structure. Next, we will describe the data we have generated through crowdsourcing. Finally, we will list some possible extensions to the type of questions we generate."}, {"heading": "5.1. The Annotation Task \u2013 Constraining MCQs with Tables", "text": "We use MTurk to generate MCQs by imposing constraints derived from the structure of the tables. These constraints help annotators to create questions with scaffolding information and result in consistent quality in the output generated. An additional benefit of this format is the alignment information of cells in the tables to the MCQs that are generated as a by-product. The annotation task guides a Turk through the process of creating an MCQ. When we get a table, we select a target cell to be the right answer for a new MCQ. First, we ask Turks to create a question by primarily using information from the rest of the line that contains the target cell so that the target cell is their correct answer. Then, annotators must select all the cells in the row that they have actually used to construct the question. Afterwards, Turks must make four consecutive succinct decisions for the question, one of which is the right answer, the other is the 3."}, {"heading": "5.2. The Dataset", "text": "We paid annotators a reward of 10 cents per MCQ and asked for 1 annotation per HIT for most tables. However, for an initial set of 4 tables that we used in a pilot study, we asked for 3 annotations per HIT5. Regarding qualifications, we required Turks to have an HIT approval rate of 95% or higher, with a minimum of at least 500 approved HITs. In addition, we limited the demographics of our workers to the U.S. The annotators were able to create an MCQ from a table in about 70 seconds. They were also largely successful in their trials. Manual inspection of the results generated also showed that questions are of consistently good quality. They are certainly good enough for the training of machine learning models and many are even good enough as evaluation data for QA. A sample of generated MCQs is presented in Table 2. Once annotations were completed, we implemented some simple selections before evaluating QMS, such as QMM will be repeated."}, {"heading": "5.3. Future Extensions", "text": "In the future, we hope to generate MCQs with other types of constraints from tables. Currently, the constraints are row-dominant: the questions are based on cells from the row in which a target cell occurs. Such questions are often questions for information - the simplest kind. We plan to investigate column-dominant questions that would lead to questions about abstraction or specification (based on the hypernym-hyponym relationship in columns). Other structural semantics we want to investigate are multi-row constraints that lead to comparisons, or connections on multiple tables with related columns that lead to concatenation or reasoning."}, {"heading": "6. Utility to the Research Community", "text": "We believe that the data we collect will be useful for people with different research interests in the NLP community. Although the tables are designed for facts to be covered in 4th grade science exams, the content is general enough to be used as background knowledge in simple areas. The additional structure presents interesting challenges for people interested in information extraction, especially when viewed with the MCQs orientation, and can be used for applications such as question and answer type extraction. Structural semantics of tables (as described in Section 4.1) can also be an interesting challenge for those interested in lexical semantics and analog reasoning. Together, the tables and MCQs can be used for QA - with great effect, as summarized in Section 3."}, {"heading": "7. Conclusion", "text": "We have presented a dataset of tables, MCQs and alignment information between the two. Our preliminary experiments with this resource trio, even on a much smaller scale, showed promising results that compete with the best current systems of the Aristo QA Challenge. Therefore, we are led to believe that the data described in this paper will be very useful for researchers working on QA. However, the usefulness of the dataset potentially extends to other areas of the NLP. Tables and MCQs individually represent resources that are of interest to people who need background knowledge or are working on semantic modeling and information extraction. Furthermore, alignment information is expensive and time consuming to comment on, and therefore scarce; nevertheless, alignment of textual fragments is a recurring theme in NLP. Our setup allows us to harvest this resource on a large scale, making it useful for people to tweak, paraphrase, and paraphrase the problems that arise."}, {"heading": "8. Acknowledgements", "text": "We thank the Allen Institute for Artificial Intelligence for generously funding the compilation of this dataset and permission to publish it. Thanks also go to Isaac Cowhey for his efforts in painstakingly constructing the tables. The first and third authors of this paper were partially supported by the following grants: NSF grant IIS-1143703, NSF grant IIS-1147810, DARPA grant FA87501220342."}, {"heading": "9. References", "text": "In the second half of 2015, the number of new cases in the United States will continue to increase. In the second half of the year, the number of new cases in the United States will continue to increase. In the second half of the year, the number of new cases in the United States will continue to increase. In the third half of the year, the number of new cases in the United States will continue to increase. In the third half of the year, the number of new cases in the United States will continue to increase. In the second half of the year, the number of new cases in the United States will continue to increase. In the third half of the year, the number of new cases in the United States will continue to increase."}], "references": [{"title": "Crowdsourcing for multiplechoice question answering", "author": ["B.I. Aydin", "Y.S. Yilmaz", "Y. Li", "Q. Li", "J. Gao", "M. Demirbas"], "venue": "Twenty-Sixth IAAI Conference.", "citeRegEx": "Aydin et al\\.,? 2014", "shortCiteRegEx": "Aydin et al\\.", "year": 2014}, {"title": "Aligning the rte 2006 corpus", "author": ["C. Brockett"], "venue": "Technical Report MSR-TR-2007-77, Microsoft Research, June.", "citeRegEx": "Brockett,? 2007", "shortCiteRegEx": "Brockett", "year": 2007}, {"title": "Webtables: exploring the power of tables on the web", "author": ["M.J. Cafarella", "A. Halevy", "D.Z. Wang", "E. Wu", "Y. Zhang"], "venue": "Proceedings of the VLDB Endowment, 1(1):538\u2013549.", "citeRegEx": "Cafarella et al\\.,? 2008", "shortCiteRegEx": "Cafarella et al\\.", "year": 2008}, {"title": "Elementary school science and math tests as a driver for ai: Take the aristo challenge", "author": ["P. Clark"], "venue": "Proceedings of IAAI, 2015.", "citeRegEx": "Clark,? 2015", "shortCiteRegEx": "Clark", "year": 2015}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1156\u20131165. ACM.", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["A. Neelakantan", "Q.V. Le", "I. Sutskever"], "venue": "arXiv preprint arXiv:1511.04834.", "citeRegEx": "Neelakantan et al\\.,? 2015", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["P. Pasupat", "P. Liang"], "venue": "arXiv preprint arXiv:1508.00305.", "citeRegEx": "Pasupat and Liang,? 2015", "shortCiteRegEx": "Pasupat and Liang", "year": 2015}, {"title": "Answering table queries on the web using column keywords", "author": ["R. Pimplikar", "S. Sarawagi"], "venue": "Proceedings of the VLDB Endowment, 5(10):908\u2013919.", "citeRegEx": "Pimplikar and Sarawagi,? 2012", "shortCiteRegEx": "Pimplikar and Sarawagi", "year": 2012}, {"title": "Learning surface text patterns for a question answering system", "author": ["D. Ravichandran", "E. Hovy"], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 41\u201347. Association for Computational Linguistics.", "citeRegEx": "Ravichandran and Hovy,? 2002", "shortCiteRegEx": "Ravichandran and Hovy", "year": 2002}, {"title": "Quantitative evaluation of passage retrieval algorithms for question answering", "author": ["S. Tellex", "B. Katz", "J. Lin", "A. Fernandes", "G. Marton"], "venue": "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages", "citeRegEx": "Tellex et al\\.,? 2003", "shortCiteRegEx": "Tellex et al\\.", "year": 2003}, {"title": "Towards ai-complete question answering: a set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint arXiv:1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Neural enquirer: Learning to query tables", "author": ["P. Yin", "Z. Lu", "H. Li", "B. Kao"], "venue": "arXiv preprint arXiv:1512.00965.", "citeRegEx": "Yin et al\\.,? 2015", "shortCiteRegEx": "Yin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "The Aristo project at AI2 (Clark, 2015) uses standardized science exams as drivers for research in Artificial Intelligence.", "startOffset": 26, "endOffset": 39}, {"referenceID": 10, "context": "Aristo\u2019s question answering format exposes a variety of interesting problems and challenges in NLP (Weston et al., 2015), such as information extraction, semantic modelling and reasoning.", "startOffset": 99, "endOffset": 120}, {"referenceID": 4, "context": "This store can be in a variety of modes and formalisms: largescale extracted and curated knowledge bases (Fader et al., 2014), structured models such as Markov Logic Networks (Khot et al.", "startOffset": 105, "endOffset": 125}, {"referenceID": 9, "context": ", 2015), or simple text corpora in information retrieval approaches (Tellex et al., 2003).", "startOffset": 68, "endOffset": 89}, {"referenceID": 11, "context": "(Yin et al., 2015) and (Neelakantan et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": ", 2015) and (Neelakantan et al., 2015) use tables in the context of question answering, but deal with synthetically generated query data for those tables.", "startOffset": 12, "endOffset": 38}, {"referenceID": 2, "context": "More generally tables have been related to QA in the context of queries over relational databases (Cafarella et al., 2008; Pimplikar and Sarawagi, 2012).", "startOffset": 98, "endOffset": 152}, {"referenceID": 7, "context": "More generally tables have been related to QA in the context of queries over relational databases (Cafarella et al., 2008; Pimplikar and Sarawagi, 2012).", "startOffset": 98, "endOffset": 152}, {"referenceID": 2, "context": "In related recent work Pasupat and Liang (2015) create a dataset of QA pairs over tables.", "startOffset": 23, "endOffset": 48}, {"referenceID": 0, "context": "Regarding crowd-sourcing for question creation, Aydin et al. (2014) harvest MCQs via a gamified app.", "startOffset": 48, "endOffset": 68}, {"referenceID": 0, "context": "Regarding crowd-sourcing for question creation, Aydin et al. (2014) harvest MCQs via a gamified app. However their work does not involve tables. Monolingual alignment datasets have also been explored separately, for example by Brockett (2007) in the context of Textual Entailment.", "startOffset": 48, "endOffset": 243}, {"referenceID": 8, "context": "Regarding construction, the recurring filler patterns can be used as templates to extend the tables semi-automatically by searching over large corpora for similar facts (Ravichandran and Hovy, 2002).", "startOffset": 169, "endOffset": 198}], "year": 2016, "abstractText": "We describe two new related resources that facilitate modelling of general knowledge reasoning in 4th grade science exams. The first is a collection of curated facts in the form of tables, and the second is a large set of crowd-sourced multiple-choice questions covering the facts in the tables. Through the setup of the crowd-sourced annotation task we obtain implicit alignment information between questions and tables. We envisage that the resources will be useful not only to researchers working on question answering, but also to people investigating a diverse range of other applications such as information extraction, question parsing, answer type identification, and lexical semantic modelling.", "creator": "LaTeX with hyperref package"}}}