{"id": "1602.04278", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2016", "title": "Signer-independent Fingerspelling Recognition with Deep Neural Network Adaptation", "abstract": "We study the problem of recognition of fingerspelled letter sequences in American Sign Language in a signer-independent setting. Fingerspelled sequences are both challenging and important to recognize, as they are used for many content words such as proper nouns and technical terms. Previous work has shown that it is possible to achieve almost 90% accuracies on fingerspelling recognition in a signer-dependent setting. However, the more realistic signer-independent setting presents challenges due to significant variations among signers, coupled with the dearth of available training data. We investigate this problem with approaches inspired by automatic speech recognition. We start with the best-performing approaches from prior work, based on tandem models and segmental conditional random fields (SCRFs), with features based on deep neural network (DNN) classifiers of letters and phonological features. Using DNN adaptation, we find that it is possible to bridge a large part of the gap between signer-dependent and signer-independent performance. Using only about 115 transcribed words for adaptation from the target signer, we obtain letter accuracies of up to 82.7% with frame-level adaptation labels and 69.7% with only word labels.", "histories": [["v1", "Sat, 13 Feb 2016 03:30:34 GMT  (148kb,D)", "http://arxiv.org/abs/1602.04278v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.NE", "authors": ["taehwan kim", "weiran wang", "hao tang", "karen livescu"], "accepted": false, "id": "1602.04278"}, "pdf": {"name": "1602.04278.pdf", "metadata": {"source": "CRF", "title": "SIGNER-INDEPENDENT FINGERSPELLING RECOGNITION WITH DEEP NEURAL NETWORK ADAPTATION", "authors": ["Taehwan Kim", "Weiran Wang", "Hao Tang", "Karen Livescu"], "emails": ["taehwan@ttic.edu", "weiranwang@ttic.edu", "haotang@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "In fact, the fact is that most of them are able to outdo themselves, and that they are able to outdo themselves, \"he told the German Press Agency.\" But it's not that they feel able to survive. \""}, {"heading": "2.1. Recognizers", "text": "The first detection mechanism is a tandem model [20] based on [7]. Characteristics at the frame level are transferred to neural network classifiers after reducing dimensionality, one of which predicts the letter name of the frame and six others that predict phonological features of the hand shape.2 The second detection mechanism is a segmental CRF model (SCRF) based on [8]. SCRFs [21, 22] are conditional log-linear models with feature functions that can be based on segments of input frames of varying lengths, allowing great flexibility in defining feature functions. As in [8], we use an SCRF to resort lattices generated by a based frame-based detection mechanism (in this case the tandem model)."}, {"heading": "2.2. DNN adaptation", "text": "The aforementioned persons are able to move, to move, without being able to move."}, {"heading": "3.1. Frame classification", "text": "The first mismatched signature-independent DNNs are trained on all but the test signature for each of the seven tasks (letters and the six phonological features).The input is the 128-dimensional HoG features linked via a 21-frame window, and the networks have three hidden layers of 3000 ReLUs [32] at a rate of 0.5 at each hidden layer, fixed dynamics of 0.95 and initial learning rate of 0.01 at held precision. These hyperparameters were set to held minibatches, at a rate of 0.5 at fixed dynamics of 0.95, and initial dynamics of 0.01 at held precision."}, {"heading": "3.2. Connected letter recognition", "text": "In the connected letter recognition we measure the power over the letter accuracy, analogous to the word or telephone accuracy in the language recognition. The letter adaptations, which we have received in the last years, are about as pronounced as in the real world, in which we move in the world, in which we move, in the world, in which we move, in the world, in the world, in the world, in which we live, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in, in the world, in the world, in the world, in the world, in the world, in the world, in, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in, in the world, in the world, in the world, in the, in the world, in the, in the world, in the, in the world, in the, in the, in the world, in the, in the world, in the world, in the world, in the, in the world, in the, in the, in the world, in the, in the world, in the, in the world,"}], "references": [{"title": "Speech recognition techniques for a sign language recognition system", "author": ["P. Dreuw", "D. Rybach", "T. Deselaers", "M. Zahedi", "H. Ney"], "venue": "Proc. Interspeech, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Sign language recognition using a combination of new vision based features", "author": ["M.M. Zaki", "S.I. Shaheen"], "venue": "Pattern Recognition Letters, pp. 3397\u20133415, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "A linguistic feature vector for the visual interpretation of sign language", "author": ["R. Bowden", "D. Windridge", "T. Kadir", "A. Zisserman", "M. Brady"], "venue": "Proc. ECCV, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Automatic recognition of fingerspelled words in British Sign Language", "author": ["S. Liwicki", "M. Everingham"], "venue": "Proc. 2nd IEEE Workshop on CVPR for Human Communicative Behavior Analysis, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Model-level data-driven sub-units for signs in videos of continuous sign language", "author": ["S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "Proc. ICASSP, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Toward scalability in ASL recognition: Breaking down signs into phonemes", "author": ["C. Vogler", "D. Metaxas"], "venue": "Proc. Gesture Workshop, 1999.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "American Sign Language fingerspelling recognition with phonological feature-based tandem models", "author": ["T. Kim", "K. Livescu", "G. Shakhnarovich"], "venue": "Proc. SLT, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Fingerspelling recognition with semi-Markov conditional random fields", "author": ["T. Kim", "G. Shakhnarovich", "K. Livescu"], "venue": "Proc. ICCV, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving continuous sign language recognition: Speech recognition techniques and system design", "author": ["J. Forster", "O. Koller", "C. Oberd\u00f6rfer", "Y. Gweth", "H. Ney"], "venue": "Proc. SLPAT, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "How the alphabet came to be used in a sign language", "author": ["C. Padden", "D.C. Gunsauls"], "venue": "Sign Language Studies, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "A Prosodic Model of Sign Language Phonology", "author": ["D. Brentari"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Toward a phonetic representation of signs: sequentiality and contrast", "author": ["R.E. Johnson", "S.K. Liddell"], "venue": "Sign Language Studies, vol. 11, no. 2, pp. 241\u2013274, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "BoostMap: A method for efficient approximate similarity rankings", "author": ["V. Athitsos", "J. Alon", "S. Sclaroff", "G. Kollios"], "venue": "Proc. CVPR, 2004.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Fingerspelling recognition through classification of letter-to-letter transitions", "author": ["S. Ricco", "C. Tomasi"], "venue": "Proc. ACCV, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Learningbased dynamic coupling of discrete and continuous trackers", "author": ["G. Tsechpenakis", "D. Metaxas", "C. Neidle"], "venue": "Computer Vision and Image Understanding, vol. 104, no. 2\u20133, 2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "A non-linear model of shape and motion for tracking finger spelt American sign language", "author": ["R. Bowden", "M. Sarhadi"], "venue": "Image and Vision Computing, vol. 20, no. 9\u201310, 2002.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Dynamic fingerspelling recognition using geometric and motion features", "author": ["P. Goh", "E. Holden"], "venue": "Proc. ICIP, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Dynamic affine-invariant shape-appearance handshape features and classification in sign language videos", "author": ["A. Roussos", "S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "Journal of Machine Learning Research, vol. 14, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Spelling it out: Real-time ASL fingerspelling recognition", "author": ["N. Pugeault", "R. Bowden"], "venue": "Proc. ICCV, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Tandem acoustic modeling in large-vocabulary recognition", "author": ["D.P.W. Ellis", "R. Singh", "S. Sivadas"], "venue": "Proc. ICASSP, 2001.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Semi-Markov conditional random fields for information extraction", "author": ["S. Sarawagi", "W.W. Cohen"], "venue": "NIPS, 2004.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "A segmental CRF approach to large vocabulary continuous speech recognition", "author": ["G. Zweig", "P. Nguyen"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Discriminative segmental cascades for feature-rich phone recognition", "author": ["H. Tang", "W. Wang", "K. Gimpel", "K. Livescu"], "venue": "Proc. ASRU, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker adaptation of context dependent deep neural networks", "author": ["H. Liao"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["P. Swietojanski", "S. Renals"], "venue": "Proc. SLT, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition", "author": ["R. Doddipatla", "M. Hasan", "T. Hain"], "venue": "Proc. Interspeech, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system", "author": ["J. Neto", "L. Almeida", "M. Hochberg", "C. Martins", "L. Nunes", "S. Renals", "T. Robinson"], "venue": "Proc. Eurospeech, 1995.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1995}, {"title": "Adaptation of context-dependent deep neural networks for automatic speech recognition", "author": ["K. Yao", "D. Yu", "F. Seide", "H. Su", "L. Deng", "Y. Gong"], "venue": "Proc. SLT, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Comparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems", "author": ["B. Li", "K.C. Sim"], "venue": "Proc. Interspeech, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Proc. CVPR, 2005.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G.E. Hinton"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 1929\u20131958, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1929}], "referenceMentions": [{"referenceID": 9, "context": "Fingerspelling accounts for roughly 12-35% of ASL [10] and is typically used for proper nouns or borrowings from English, which can often be the most important content words.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "Some aspects of fingerspelling can be characterized through the phonology of handshape [11, 12], which can be described in terms of phonological features.", "startOffset": 87, "endOffset": 95}, {"referenceID": 11, "context": "Some aspects of fingerspelling can be characterized through the phonology of handshape [11, 12], which can be described in terms of phonological features.", "startOffset": 87, "endOffset": 95}, {"referenceID": 12, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 13, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 14, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 15, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 16, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 17, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 18, "context": "Most prior research on fingerspelling recognition has focused on constrained tasks such as single-letter or handshape classification or word recognition from a known vocabulary [13, 14, 15, 16, 17, 18, 19].", "startOffset": 177, "endOffset": 205}, {"referenceID": 6, "context": "[7, 8] obtained \u223c 90% average letter accuracies in a signer-dependent setting, using either tandem hidden Markov models (HMMs) or segmental conditional random fields (SCRFs), with features from neural network classifiers of letters and phonological features.", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "[7, 8] obtained \u223c 90% average letter accuracies in a signer-dependent setting, using either tandem hidden Markov models (HMMs) or segmental conditional random fields (SCRFs), with features from neural network classifiers of letters and phonological features.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "Prior work has address signer adaptation for large-vocabulary German Sign Language recognition [9], but to our knowledge this paper is the first to address adaptation for fingerspelling.", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "We start with the recognition approaches that have achieved the best prior results on this task [7, 8], with updates for improved performance with deeper neural networks.", "startOffset": 96, "endOffset": 102}, {"referenceID": 7, "context": "We start with the recognition approaches that have achieved the best prior results on this task [7, 8], with updates for improved performance with deeper neural networks.", "startOffset": 96, "endOffset": 102}, {"referenceID": 19, "context": "The first recognizer is a tandem model [20] based on [7].", "startOffset": 39, "endOffset": 43}, {"referenceID": 6, "context": "The first recognizer is a tandem model [20] based on [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "The second recognizer is a segmental CRF (SCRF) model based on [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 20, "context": "SCRFs [21, 22] are conditional log-linear models with feature functions that can be based on variablelength segments of input frames, allowing for great flexibility in defining feature functions.", "startOffset": 6, "endOffset": 14}, {"referenceID": 21, "context": "SCRFs [21, 22] are conditional log-linear models with feature functions that can be based on variablelength segments of input frames, allowing for great flexibility in defining feature functions.", "startOffset": 6, "endOffset": 14}, {"referenceID": 7, "context": "As in [8], we use an SCRF to rescore lattices produced by a baseline frame-based recognizer (in this case, the tandem model).", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "We use the same feature functions as in [8], which include language model features, a feature that measures agreement with the baseline recognizer, means of letter/phonological feature neural network classifier outputs over each segment, and \u201cpeak detection\u201d features that measure the dynamics of each segment.", "startOffset": 40, "endOffset": 43}, {"referenceID": 22, "context": "[23], which is independent of any frame-based recognizer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "We use the same feature functions as in [23], namely average DNN outputs over each segment, samples of DNN outputs within the segment, duration and bias, all lexicalized.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "2See [11, 12, 7] for details of the phonological features.", "startOffset": 5, "endOffset": 16}, {"referenceID": 11, "context": "2See [11, 12, 7] for details of the phonological features.", "startOffset": 5, "endOffset": 16}, {"referenceID": 6, "context": "2See [11, 12, 7] for details of the phonological features.", "startOffset": 5, "endOffset": 16}, {"referenceID": 23, "context": ", [24, 25, 26, 27]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 24, "context": ", [24, 25, 26, 27]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 25, "context": ", [24, 25, 26, 27]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 26, "context": ", [24, 25, 26, 27]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 27, "context": "We first consider two simple approaches based on linear input networks (LIN) and linear output networks (LON) [28, 29, 30], shown in Fig.", "startOffset": 110, "endOffset": 122}, {"referenceID": 28, "context": "We first consider two simple approaches based on linear input networks (LIN) and linear output networks (LON) [28, 29, 30], shown in Fig.", "startOffset": 110, "endOffset": 122}, {"referenceID": 29, "context": "We first consider two simple approaches based on linear input networks (LIN) and linear output networks (LON) [28, 29, 30], shown in Fig.", "startOffset": 110, "endOffset": 122}, {"referenceID": 6, "context": "use either ground-truth frame-level letter labels (using human annotation as described in [7]) or labels obtained by forced alignment if only word labels are available.", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "We use the ASL video data set of [8], comprising four signers each fingerspelling 600 word tokens consisting of two repetitions of a 300-word list, including common English words, names, and foreign words.", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "Following [8], the hand portion of each image is extracted via hand detection and segmentation using a signer-specific Gaussian color model, followed by suppression of irrelevant pixels.", "startOffset": 10, "endOffset": 13}, {"referenceID": 30, "context": "The extracted hand images are resized to 128 \u00d7 128 and Histogram of Gradient (HoG) [31] features are extracted using multiple spatial grids (4 \u00d7 4, 8 \u00d7 8, and 16 \u00d7 16), followed by dimensionality reduction with principal components analysis (PCA).", "startOffset": 83, "endOffset": 87}, {"referenceID": 31, "context": "The input is the 128-dimensional HoG features concatenated over a 21-frame window, and the networks have three hidden layers of 3000 ReLUs [32].", "startOffset": 139, "endOffset": 143}, {"referenceID": 32, "context": "Cross-entropy training is done with a weight decay penalty of 10\u22125 via stochastic gradient descent (SGD) over 100-sample minibatches for up to 30 epochs, with dropout [33] at a rate of 0.", "startOffset": 167, "endOffset": 171}, {"referenceID": 7, "context": "As shown in Table 1, without adaptation both tandem and SCRF models do poorly, achieving only roughly 40% letter accuracies, with the rescoring SCRF slightly outperforming the others (recall that signer-dependent recognition achieves about 90% letter accuracies [8]).", "startOffset": 262, "endOffset": 265}, {"referenceID": 6, "context": "3See [7, 8, 23] for details of the tuning parameters.", "startOffset": 5, "endOffset": 15}, {"referenceID": 7, "context": "3See [7, 8, 23] for details of the tuning parameters.", "startOffset": 5, "endOffset": 15}, {"referenceID": 22, "context": "3See [7, 8, 23] for details of the tuning parameters.", "startOffset": 5, "endOffset": 15}], "year": 2016, "abstractText": "We study the problem of recognition of fingerspelled letter sequences in American Sign Language in a signer-independent setting. Fingerspelled sequences are both challenging and important to recognize, as they are used for many content words such as proper nouns and technical terms. Previous work has shown that it is possible to achieve almost 90% accuracies on fingerspelling recognition in a signer-dependent setting. However, the more realistic signer-independent setting presents challenges due to significant variations among signers, coupled with the dearth of available training data. We investigate this problem with approaches inspired by automatic speech recognition. We start with the best-performing approaches from prior work, based on tandem models and segmental conditional random fields (SCRFs), with features based on deep neural network (DNN) classifiers of letters and phonological features. Using DNN adaptation, we find that it is possible to bridge a large part of the gap between signerdependent and signer-independent performance. Using only about 115 transcribed words for adaptation from the target signer, we obtain letter accuracies of up to 82.7% with framelevel adaptation labels and 69.7% with only word labels.", "creator": "LaTeX with hyperref package"}}}