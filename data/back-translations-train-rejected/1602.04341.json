{"id": "1602.04341", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2016", "title": "Attention-Based Convolutional Neural Network for Machine Comprehension", "abstract": "Understanding open-domain text is one of the primary challenges in natural language processing (NLP). Machine comprehension benchmarks evaluate the system's ability to understand text based on the text content only. In this work, we investigate machine comprehension on MCTest, a question answering (QA) benchmark. Prior work is mainly based on feature engineering approaches. We come up with a neural network framework, named hierarchical attention-based convolutional neural network (HABCNN), to address this task without any manually designed features. Specifically, we explore HABCNN for this task by two routes, one is through traditional joint modeling of passage, question and answer, one is through textual entailment. HABCNN employs an attention mechanism to detect key phrases, key sentences and key snippets that are relevant to answering the question. Experiments show that HABCNN outperforms prior deep learning approaches by a big margin.", "histories": [["v1", "Sat, 13 Feb 2016 14:38:47 GMT  (439kb,D)", "http://arxiv.org/abs/1602.04341v1", "7 pages, 4 figures"]], "COMMENTS": "7 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "sebastian ebert", "hinrich sch\\\"utze"], "accepted": false, "id": "1602.04341"}, "pdf": {"name": "1602.04341.pdf", "metadata": {"source": "CRF", "title": "Attention-Based Convolutional Neural Network for Machine Comprehension", "authors": ["Wenpeng Yin", "Sebastian Ebert", "Hinrich Sch\u00fctze"], "emails": ["ebert@cis.lmu.de"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if most people who are able to determine themselves, to put themselves and their interests at the center, must put themselves at the center. (...) In fact, it is as if most people are able to determine themselves what they want. (...) In fact, it is as if most of them are able to determine themselves. (...) It is as if they put themselves in the role of the individual. (...) It is as if they slip into the role of the individual. (...) It is as if they put themselves in the role of the individual. (...) It is as if they slip into the role of the individual. (...) It is as if they slip into the role of the individual. (...) It is as if they slip into the role of the individual. (...) It is as if they slip into the role of the individual. (...) It is as if they slip into the role."}, {"heading": "2 Related Work", "text": "Existing systems for MCTest tasks are largely based on manually developed functions. Representative work includes [Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang and McAllester, 2015; Smith et al., 2015]. In this work, a common path is first to define a regulated loss function based on adopted feature vectors, then to focus efforts on designing effective functions based on various rules. Although this research is groundbreaking for this task, its flexibility and generalization ability are limited. Deep learning approaches appeal to increasing interest in analog tasks. Weston et al., [2014] introduce storage networks for factoid QA tasks. Storage networks are tested in [Weston et al., 2015; Kumar et al., 2015] for Facebook bAbI Dataset al. Peng et al. [2015] s Neural Reasoner Infers over several supporting facts to provide an answer to a question that is also given to M and Q."}, {"heading": "3 Model", "text": "We examine this task using three approaches, illustrated in Figure 2. (i) We can calculate two different document (D) representations in a common space, one based on question (Q) attention, one based on answer (A) attention and compare. We call this architecture HABCNN-QAP. (ii) We calculate a representation of D based on Q attention (as before), but now we compare it directly with a representation of A. We call this architecture HABCNN-QP. (iii) We treat this QA task as a textual entanglement (TE) by first reformatting the Q-A pair into a statement (S) and then assigning it directly to S and D. We call this architecture HABCNN-TE. All three approaches are implemented within the common framework of HABCNN."}, {"heading": "3.1 HABCNN", "text": "We remember that we used the abbreviations A (answer), Q (question), S (statement), D (document). HABCNN performs representation learning for three (Q, A, D) in HABCNN-QAP and HABCNN-QAP, for tuples (S, D) in HABCNN-TE. For convenience we use \"Query\" to refer to Q, A, or S. HABCNN, represented in Figure 3, has the following sentences. The input is (query, D). Query is two single sentences (for Q, A) or a single sentence (for S), D is a sequence of sentences. Words are initialized by dimensional pre-trained word embeddings. As a result, each sentence is represented as a characteristic map of d \u00d7 s (s is sentence length). In Figure 3, each sentence is represented in the input layer."}, {"heading": "3.2 HABCNN-QP & HABCNN-QAP", "text": "HABCNN-QP / QAP calculates the representation of D as a projection of D, either on the basis of the attention of CNN or on the basis of the attention of A. We hope that these two projections of the document are close for a correct A and less close for a wrong A. As we said in related work, machine understanding can be considered as a response selection task using document D as critical background information. In this case, HABCNNQP / QAP compares Q and A not directly, but in HABCNN-QP / QAP we use Q and A to filter and extract the document differently, which is crucial for the Q / A match, by attention bundling. Then they match the two document representations in the new space. To simplify the exposure we have used the symbol vo so far, but in HABCNN-QP / QAP we calculate two different document representations: voq, for the attention related to Q / A, QABQ is calculated a QA, for the attention related to Q / A, and a CNN is calculated for the attention of QABri, a response to CNN; and a, for CNN, the attention is calculated in relation to Q, a response to CNN: voq is a response to CNN."}, {"heading": "3.3 HABCNN-TE", "text": "HABCNN-TE treats machine understanding as a textual sequence. We use the statements provided within the framework of MCTest. Each statement corresponds to a question-answer pair; e.g., the question-answer pair \"Why did Grandpa answer the door?\" / \"Because he saw the insects\" (Figure 1) is reformatted to the statement \"Grandpa answered the door because he saw the insects.\" The answer to the question is then: \"Does the document contain the statement?\" For HABCNN-TE, the input for Figure 2 (below) is the pair (S, D). HABCNN-TE tries to reconcile the S representation ri with the D representation vo."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "MCTest-160 is a set of 160 items, each consisting of a document, four questions followed by a correct answer and three wrong answers (divided into 70 train, 30 train and 60 test) and MCTest-500 a set of 500 items (divided into 300 train, 50 train and 150 test)."}, {"heading": "4.2 Training Setup and Tricks", "text": "\"Our training goal is to minimize the following rank loss function: L (d, a +, a \u2212) = max (0, \u03b1 + S (d, a \u2212) \u2212 S (4), where S (\u00b7, \u00b7) is a matching score between two representation vectors. Cosine similarity is used throughout. \u03b1 is a constancy. For this frequent rank loss, we also have two ways to use the data for each positive answer with three negative answers. Treat (d, a \u2212 1, a \u2212 2, a \u2212 3) as a training example, then our loss function can have three\" max () \"terms, each used for a positive-negative pair; treat the other (d, a \u2212 i) as an individual training example. In practice, we find the second way works better, a \u2212 3) as a training example."}, {"heading": "4.3 Baseline Systems", "text": "This paper focuses on the comparison with systems of distributed representation learning and deep learning: addition. Compare questions and answers directly without taking into account the D. Sentence representations are calculated by elementary addition over word representations. Addition-proj. The Neural Reasoner [Peng et al., 2015] has a coding layer, several layers of argumentation, and a final answer layer. Input for the coding layer is a question and the sentences of the document (referred to as facts); each sentence is encoded into a vector by a GRU. In each argument layer, NR lets the question representation interact with each fact representation as an argument process. Finally, all temporary arguments are summarized as an answer representation. The attention reader [Hermann et al., 2015] represents a word by using word representation as a whole / level."}, {"heading": "4.4 HABCNN Variants", "text": "In addition to the main architectures described above, we also examine two variants of ABCHNN, inspired by [Peng et al., 2015] and [Hermann et al., 2015]. Variant I: Since RNNs are widely recognized as competitors to CNNs in sentence modeling, similar to [Peng et al., 2015], we replace the sentence CNN in Figure 3 with a GRU, while other parts remain unchanged. Variant II: How to model attention to the granularity of words was shown in [Hermann et al., 2015]; see their work for details. We develop their attention idea and model their attention to the granularity of sentences and snippets. Our attention attaches different weights to sentences / snippets (not words) and then calculates the document representation as a weighted average of all sentence / snippets representations."}, {"heading": "4.5 Results", "text": "In fact, our HABCNN systems surpass all baselines, especially the two competing, deep learning-based systems AR and NR. The margin between our most powerful ABHCNN-TE and NR is 15.6 / 16.5 (Accuracy / NDCG) on MCTest-150 and 7.3 / 4.6 on MCTest-500. This shows the promise of our architecture in this area. As I said, both the AR and NR systems aim to generate answers in a holistic form. Their designs do not fit this machine-based comprehension task in which the answers to sums are summarized or abstracted."}, {"heading": "4.6 Case Study and Error Analysis", "text": "In Figure 4, we visualize the sentence-level attention distribution as well as the snippet-level dependencies for the statement \"Grandpa answered the door because Jimmy knocked,\" the corresponding question requiring several sentences to be answered. On the left, we see that \"Grandpa answered the door with a smile and greeted Jimmy inside\" has the highest attention weight, corresponding to the intuition that this sentence overlaps semantically with the statement, and yet this sentence does not contain the answer. Let's look further into the right part, where the CNN layer on sentence representations is supposed to extract high-level features from snippet. At this level, the highest attention weight is thrown at the best snippet. \"Finally, Jimmy... came knocking. Grandpa answered the door....\" And the adjacent snippet also gets relatively higher attention than other regions. Let's remember that our system selects the one sentence with the highest attention on the left and selects the top 3 snippet on the right-hand side of the K (relative to the K)."}, {"heading": "5 Conclusion", "text": "This work takes the lead in the presentation of a CNN-based neural network system for open domain machine understanding tasks. Our systems have attempted to solve this task in both a way of document projection and word processing, the latter showing slightly better performance. Overall, our architecture, which models dynamic document representation by attention pattern from sentence to snippet level, shows promising results in this task. In the future, finer-grained learning approaches are expected for the representation of complex response types and question types."}], "references": [{"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al", "2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "Proceedings of EMNLP,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "Proceedings of ICML, pages 160\u2013167,", "citeRegEx": "Collobert and Weston. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "The Journal of Machine Learning Research", "author": ["John Duchi", "Elad Hazan", "Yoram Singer. Adaptive subgradient methods for online learning", "stochastic optimization"], "venue": "12:2121\u20132159,", "citeRegEx": "Duchi et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Proceedings of NIPS", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom. Teaching machines to read", "comprehend"], "venue": "pages 1684\u20131692,", "citeRegEx": "Hermann et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "In Proceedings of EMNLP", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III. A neural network for factoid question answering over paragraphs"], "venue": "pages 633\u2013644,", "citeRegEx": "Iyyer et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "ACM Transactions on Information Systems (TOIS)", "author": ["Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen. Cumulated gain-based evaluation of ir techniques"], "venue": "20(4):422\u2013446,", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "Kumar et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of INTERSPEECH", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur. Recurrent neural network based language model"], "venue": "pages 1045\u20131048,", "citeRegEx": "Mikolov et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of ACL", "author": ["Karthik Narasimhan", "Regina Barzilay. Machine comprehension with discourse relations"], "venue": "pages 1253\u20131262,", "citeRegEx": "Narasimhan and Barzilay. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CoRR", "author": ["Baolin Peng", "Zhengdong Lu", "Hang Li", "Kam-Fai Wong. Towards neural network-based reasoning"], "venue": "abs/1508.05508,", "citeRegEx": "Peng et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of EMNLP, 12:1532\u20131543,", "citeRegEx": "Pennington et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension", "author": ["Richardson et al", "2013] Matthew Richardson", "Christopher JC Burges", "Erin Renshaw"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "In Proceedings of ACL", "author": ["Mrinmaya Sachan", "Avinava Dubey", "Eric P Xing", "Matthew Richardson. Learning answerentailing structures for machine comprehension"], "venue": "pages 239\u2013249,", "citeRegEx": "Sachan et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of SIGIR", "author": ["Aliaksei Severyn", "Alessandro Moschitti. Learning to rank short text pairs with convolutional deep neural networks"], "venue": "pages 373\u2013382,", "citeRegEx": "Severyn and Moschitti. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Question/answer matching for cqa system via combining lexical and sequential information", "author": ["Yikang Shen", "Wenge Rong", "Zhiwei Sun", "Yuanxin Ouyang", "Zhang Xiong"], "venue": "Proceedings of AAAI, pages 275\u2013 281,", "citeRegEx": "Shen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of EMNLP", "author": ["Ellery Smith", "Nicola Greco", "Matko Bosnjak", "Andreas Vlachos. A strong lexical matching method for the machine comprehension test"], "venue": "pages 1693\u20131698,", "citeRegEx": "Smith et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In NIPS", "author": ["Rupesh K Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber. Training very deep networks"], "venue": "pages 2368\u20132376,", "citeRegEx": "Srivastava et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "frames", "author": ["Hai Wang", "Mohit Bansal Kevin Gimpel David McAllester. Machine comprehension with syntax"], "venue": "and semantics. In Proceedings of ACL, Volume 2: Short Papers, pages 700\u2013706,", "citeRegEx": "Wang and McAllester. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of ACL", "author": ["Baoxun Wang", "Xiaolong Wang", "Chengjie Sun", "Bingquan Liu", "Lin Sun. Modeling semantic relevance for question-answer pairs in web social communities"], "venue": "pages 1230\u20131238,", "citeRegEx": "Wang et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "venue": "Proceedings of ICLR,", "citeRegEx": "Weston et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards ai-complete question answering: a set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "Weston et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek"], "venue": "Proceedings of EMNLP, pages 2013\u2013 2018,", "citeRegEx": "Yang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning for answer sentence selection", "author": ["Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman"], "venue": "ICLR workshop,", "citeRegEx": "Yu et al.. 2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "Despite intensive recent work [Weston et al., 2014; Weston et al., 2015; Hermann et al., 2015; Sachan et al., 2015], the problem is far from solved.", "startOffset": 30, "endOffset": 115}, {"referenceID": 21, "context": "Despite intensive recent work [Weston et al., 2014; Weston et al., 2015; Hermann et al., 2015; Sachan et al., 2015], the problem is far from solved.", "startOffset": 30, "endOffset": 115}, {"referenceID": 3, "context": "Despite intensive recent work [Weston et al., 2014; Weston et al., 2015; Hermann et al., 2015; Sachan et al., 2015], the problem is far from solved.", "startOffset": 30, "endOffset": 115}, {"referenceID": 13, "context": "Despite intensive recent work [Weston et al., 2014; Weston et al., 2015; Hermann et al., 2015; Sachan et al., 2015], the problem is far from solved.", "startOffset": 30, "endOffset": 115}, {"referenceID": 8, "context": "The reason of choosing a CNN rather than other sequence models like recurrent neural network [Mikolov et al., 2010], long short-term memory unit (LSTM [Hochreiter and Schmidhuber, 1997]), gated recurrent unit (GRU [Cho et al.", "startOffset": 93, "endOffset": 115}, {"referenceID": 4, "context": ", 2010], long short-term memory unit (LSTM [Hochreiter and Schmidhuber, 1997]), gated recurrent unit (GRU [Cho et al.", "startOffset": 43, "endOffset": 77}, {"referenceID": 9, "context": "[Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang and McAllester, 2015; Smith et al., 2015].", "startOffset": 0, "endOffset": 99}, {"referenceID": 13, "context": "[Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang and McAllester, 2015; Smith et al., 2015].", "startOffset": 0, "endOffset": 99}, {"referenceID": 18, "context": "[Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang and McAllester, 2015; Smith et al., 2015].", "startOffset": 0, "endOffset": 99}, {"referenceID": 16, "context": "[Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang and McAllester, 2015; Smith et al., 2015].", "startOffset": 0, "endOffset": 99}, {"referenceID": 21, "context": "Memory network framework is extended in [Weston et al., 2015; Kumar et al., 2015] for Facebook bAbI dataset.", "startOffset": 40, "endOffset": 81}, {"referenceID": 7, "context": "Memory network framework is extended in [Weston et al., 2015; Kumar et al., 2015] for Facebook bAbI dataset.", "startOffset": 40, "endOffset": 81}, {"referenceID": 23, "context": "Some other deep learning systems are developed for answer selection task [Yu et al., 2014; Yang et al., 2015; Severyn and Moschitti, 2015; Shen et al., 2015; Wang et al., 2010].", "startOffset": 73, "endOffset": 176}, {"referenceID": 22, "context": "Some other deep learning systems are developed for answer selection task [Yu et al., 2014; Yang et al., 2015; Severyn and Moschitti, 2015; Shen et al., 2015; Wang et al., 2010].", "startOffset": 73, "endOffset": 176}, {"referenceID": 14, "context": "Some other deep learning systems are developed for answer selection task [Yu et al., 2014; Yang et al., 2015; Severyn and Moschitti, 2015; Shen et al., 2015; Wang et al., 2010].", "startOffset": 73, "endOffset": 176}, {"referenceID": 15, "context": "Some other deep learning systems are developed for answer selection task [Yu et al., 2014; Yang et al., 2015; Severyn and Moschitti, 2015; Shen et al., 2015; Wang et al., 2010].", "startOffset": 73, "endOffset": 176}, {"referenceID": 19, "context": "Some other deep learning systems are developed for answer selection task [Yu et al., 2014; Yang et al., 2015; Severyn and Moschitti, 2015; Shen et al., 2015; Wang et al., 2010].", "startOffset": 73, "endOffset": 176}, {"referenceID": 1, "context": "For the query and each sentence of D, we do element-wise 1-max-pooling (\u201cmax-pooling\u201d for short) [Collobert and Weston, 2008] over phrase representations to form their representations at this level.", "startOffset": 97, "endOffset": 125}, {"referenceID": 1, "context": "(If k = all, attention-pooling returns to the common max-pooling in [Collobert and Weston, 2008].", "startOffset": 68, "endOffset": 96}, {"referenceID": 17, "context": "In order to create a flexible choice for open Q/A, we develop a highway network [Srivastava et al., 2015] to combine the two levels of representations as an overall representation vo of D:", "startOffset": 80, "endOffset": 105}, {"referenceID": 13, "context": "Multitask learning: Question typing is commonly used and proved to be very helpful in QA tasks [Sachan et al., 2015].", "startOffset": 95, "endOffset": 116}, {"referenceID": 2, "context": "We train with AdaGrad [Duchi et al., 2011] and use 50dimensional GloVe [Pennington et al.", "startOffset": 22, "endOffset": 42}, {"referenceID": 11, "context": ", 2011] and use 50dimensional GloVe [Pennington et al., 2014] to initialize word representations,2 kept fixed during training.", "startOffset": 36, "endOffset": 61}, {"referenceID": 6, "context": "We consider two evaluation metrics: accuracy (proportion of questions correctly answered) and NDCG4 [J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002].", "startOffset": 100, "endOffset": 131}, {"referenceID": 10, "context": "The Neural Reasoner [Peng et al., 2015] has an encod-", "startOffset": 20, "endOffset": 39}, {"referenceID": 3, "context": "The Attentive Reader [Hermann et al., 2015] is implemented by modeling the whole D as a word sequence \u2013 without specific sentence / snippet representations \u2013 using an", "startOffset": 21, "endOffset": 43}, {"referenceID": 10, "context": "In addition to the main architectures described above, we also explore two variants of ABCHNN, inspired by [Peng et al., 2015] and [Hermann et al.", "startOffset": 107, "endOffset": 126}, {"referenceID": 3, "context": ", 2015] and [Hermann et al., 2015], respectively.", "startOffset": 12, "endOffset": 34}, {"referenceID": 10, "context": "Variant-I: As RNNs are widely recognized as a competitor of CNNs in sentence modeling, similar with [Peng et al., 2015], we replace the sentence-CNN in Figure 3 by a GRU while keeping other parts unchanged.", "startOffset": 100, "endOffset": 119}, {"referenceID": 3, "context": "Variant-II: How to model attention at the granularity of words was shown in [Hermann et al., 2015]; see their paper for details.", "startOffset": 76, "endOffset": 98}], "year": 2016, "abstractText": "Understanding open-domain text is one of the primary challenges in natural language processing (NLP). Machine comprehension benchmarks evaluate the system\u2019s ability to understand text based on the text content only. In this work, we investigate machine comprehension on MCTest, a question answering (QA) benchmark. Prior work is mainly based on feature engineering approaches. We come up with a neural network framework, named hierarchical attention-based convolutional neural network (HABCNN), to address this task without any manually designed features. Specifically, we explore HABCNN for this task by two routes, one is through traditional joint modeling of document, question and answer, one is through textual entailment. HABCNN employs an attention mechanism to detect key phrases, key sentences and key snippets that are relevant to answering the question. Experiments show that HABCNN outperforms prior deep learning approaches by a big margin.", "creator": "LaTeX with hyperref package"}}}