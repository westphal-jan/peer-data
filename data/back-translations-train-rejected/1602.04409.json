{"id": "1602.04409", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2016", "title": "Convex Optimization For Non-Convex Problems via Column Generation", "abstract": "We apply column generation to approximating complex structured objects via a set of primitive structured objects under either the cross entropy or L2 loss. We use L1 regularization to encourage the use of few structured primitive objects. We attack approximation using convex optimization over an infinite number of variables each corresponding to a primitive structured object that are generated on demand by easy inference in the Lagrangian dual. We apply our approach to producing low rank approximations to large 3-way tensors.", "histories": [["v1", "Sun, 14 Feb 2016 03:10:30 GMT  (2598kb,D)", "http://arxiv.org/abs/1602.04409v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["julian yarkony", "kamalika chaudhuri"], "accepted": false, "id": "1602.04409"}, "pdf": {"name": "1602.04409.pdf", "metadata": {"source": "CRF", "title": "Convex Optimization For Non-Convex Problems via Column Generation", "authors": ["Julian Yarkony"], "emails": ["julian.e.yarkony@gmail.com", "kamalika@cs.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "1.1 Outline", "text": "In Section 2, we will formally present our two problem families. In Section 3, we will formulate the problems in the sense of a convex optimization over enormous or possibly infinite variable spaces, each corresponding to a primitive. In Section 4, we will then formulate optimization in the form of column generation, where variables are generated as required to make a convex optimization feasible. In Section 5, we will show how primitives can be identified to improve the goal. We will show sample derivatives in the area of tensors (Sections 5.1,5.2) and Gaussian mixing models (Section 5.3). In Section 6, we will show experimental results on tensor problems. We will show the value of the results of optimization for synthetic 3-way tensor problems of different sizes depending on time and iteration. We will use examples from Family One and Two. In Section 7, we will briefly discuss relevant papers and their relationship to our work. In Section 8, we will discuss implementing examples and future work."}, {"heading": "2 Formal Model", "text": "Consider a structured object consisting of a finite number of values called T-shaped. Here, T-shaped can correspond to a tensor or a probability distribution. We refer to the vectorization of T-shaped by the column vector T. We refer to the set of primitives that can be used to construct T-shaped and to reference its members with m. Primitives can be rank-1 tensors or Gaussian distributions in the previous examples. We define a matrix M by stringing the column vectors m-shaped together. We define a non-negatively weighted combination of the columns of M using a non-negative column vector equal to the length | M |. In this document, we examine the two families in parallel, since the approximations introduced in this essay are highly related. Also, much of the notation used to discuss the two families is divided. Likewise, many problems that are formally addressed with family problems, two in corresponding families."}, {"heading": "2.1 Family One", "text": "We define the optimal model according to L2 loss under L1 regularization as follows: We use t to indicate transposition. We use ~ \u0442 to define a positive constant column vector of length | M |, where the constant value is' E R +.min w \u2265 01 2 (T \u2212 Mw) t (T \u2212 Mw) + ~ E \u00b7 tw (1)."}, {"heading": "2.2 Family Two", "text": "We now define the optimal model according to the cross entropy loss under L1 regularization as follows: We use Log to mark the elementary logarithm. We use ~ \u0445a to define a positive constant column vector of length | M | where the constant value is \"a\" a \"R +.\" In addition, a single zero value entry is used. This entry is associated with a special column m0 corresponding to the uniform distribution or other distribution of white noise. Input of w corresponding to m0 is called w0. This is useful for modelling probability distributions where T is a complex probability distribution and each m is a primitive probability distribution. The corresponding optimization problem is listed below. min w \u2265 0 wt1 = 1 \u2212 T log (Mw) + ~ taw (2) Since the equation 2 in w0 does not decrease, we write equation 2 as the following."}, {"heading": "3 Formulating Optimization", "text": "We will now consider the approach of structured objects using the tools of quadratic programming (QP) for family one and linear programming (LP) for family two."}, {"heading": "3.1 Family One", "text": "We now present a surrogate object K that corresponds to the elementary distance between T and Mw. We now write the optimization as a square program. Eq 1 = min w \u2265 0 K \u2265 01 2 KtK + ~ tw (4) s.t. K \u2265 T \u2212 Mw K \u2265 Mw \u2212 THere Eq 4 corresponds to a convex square program with an intractable | M | number of variables (one for each primitive) and a small number of constraints equal to 2 | T |. We call Eq 4 the primary problem for family one and it is associated with a dual problem that is also a convex square program. This dual problem is written below with dual variables, each of which is equal in length to | T. Eq 1 = max."}, {"heading": "3.2 Family Two", "text": "We now present a substitute object K, which follows the elementary logarithm of Mw.Eq 2 = min w \u2265 01tw \u2264 1 K \u2265 0T tK + ~ \u0442 taw (6) s.t. \u2212 K \u2264 log (Mw) Since the logarithm function is concave, we express it as the lower hull of a set of affinity upper limits, each constructed by a first order Taylor expansion. We formally write this protocol at the bottom of Eq 2. This results in the linear program following.Eq 2 = min w \u2265 01tw \u2264 1 K \u2265 0T tK + ~ taw (8) s.t. We now apply the lower hull expression of Log in Eq 7 to Eq 2. This leads to the linear program following.Eq 2 = min w \u2265 01tw \u2264 1 K \u2265 0T + ~ taw (8) s.t. We refer to the epimal problem below the problem, which is a dual-Dability problem (also a D1)."}, {"heading": "4 Inference in General Terms", "text": "The solution of a square or linear program is done by various methods such as inner dots or simplex. However, in order to apply them, there must be a finite number of variables and constraints, and our problems do not meet these criteria. To get around them, we solve modified versions of problems that only take into account a subset of constraints and variables. Variables or constraints are added when and if they are violated or would improve the goal. We iterate between solving the modified problems and adding variables or constraints. The duplicate problems facilitate the addition of primary variables by analyzing the dual variables."}, {"heading": "4.1 Family One", "text": "To solve the dual problem, we need to identify a subset of the constraints so that no other constraints are violated in enforcing them. We form this subset called M-q, which is greedily referred to as a work set. Here, M-q is initialized to the empty set or a subset of columns of M. We write the corresponding square program below. Eq 1 \u2264 max-0; 0 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 2 \u2212 T-3; 2 + T-3 (10) ~ 4 \u00b7 M-4 \u2212 M-4 Determining the most violated constraint corresponds to selecting a column of M to maximize the next one for each case. To simplify the notation, we define a term."}, {"heading": "4.2 Family Two", "text": "To address this difficulty, we store a subset of the constraints / variables added in the primal and dual. We then produce a solution that uses only these constraints / variables. To support our discussion, we index T as follows: We index T with x, where Tx is the x 'th value of the vector T. We index K and Mw similarly. We use S to denounce the working subset of x, and use M to denounce the working subset of M. We initialize M with x, where Tx is the x' th value of the vector T. We use S to denounce the working subset of x, and use M to denounce the working subset of M. We initialize M with the uniform distribution / high entropy distribution m0, and initialize S."}, {"heading": "5 Identifying the Most Violating m or Highly Violated m", "text": "The previous sections refer to the solution for the optimal m as the following optimization maxm. The difficulty of the solution for the optimal m is problem and problem specific. However, these problems tend to be relatively easy to solve.Consider the problem of approaching a high-level 3-way tensor with a low tensor below L2 or cross entropy loss and L1 regularization. In this section, we show that the solution for m matches the optimal rank-1 tensor with a tensor described by the dual variables. In the case of Gauss mixture models, we show that the solution for maxm and L1 regularization corresponds to the maximum probability of Gauss with a weighting of the points that can be solved by a tensor exactly in closed form. In fact, rod is an unnormalized probability distribution. Here and in all family-2 examples, two examples must be normalized before the inference and the maximum displacement of the xal in the maxm."}, {"heading": "5.1 Example, Family One: Symmetric 3-way Tensors", "text": "Consider the case that T corresponds to the vectorization of a symmetrical 3-way tensor. Therefore, each column of M corresponds to a symmetrical 3-way tensor. We describe m with a vector v. Here v (| T | 1 3), where v is a unit vector. The non-vectorized form of m is called m and is indexed by i, j, k \u00b2 0, 1, 2... N \u2212 1}. We define m \u00b2 in relation to v \u00b2 ijk = vivvvk (16) We write optimization below."}, {"heading": "5.1.1 Note on Even way Tensors and Optimization", "text": "In the case of uneven tensors (2-way, 4-way, 6-way...), an outer product does not generate all possible rank-1 tensors for M. It is not possible to generate those that are constructed by an outer product, which is then multiplied by \u2212 1. This can be seen from the following observation: If you multiply the vector v by \u2212 1, you do not reverse the sign of all elements of m (it does not turn around), whereas this is achieved with odd path types (3-way, 5-way, 7-way...). Thus, at the end of the optimization performance optimization optimization, we add m and \u2212 m to the working principle M. The predicted gradient optimization must be modified similarly in the case of odd tensors. This is done by calculating maxm \u00b2 M tensors and maxm \u00b2 M (\u2212 TB) tm and by adding m, \u2212 m to M tensors that correspond to injury-related constraints. This is important, however, because it is not necessary to limit the negative component in One."}, {"heading": "5.2 Example Family Two: Fitting a symmetric 3-way tensor defined by probability distribution", "text": "In this section, we solve the optimization based on Jenson's inequality (27, 28] to force m to correspond to a probability distribution at all times and to avoid a gradient descent. + In this section, we write the optimization based on Jenson's inequality (27, 28] to force m to apply a probability distribution at all times and to shift the normalization constant outside the maximum limit. + In this section, we write the optimization via the maxm-zijkjvk = 1 log (20) recall protocol, which is not negative, and we normalize the sum to one and move the normalization constant outside the maximum limit. + We write the optimization via the maxm-zijk-zijvk protocol."}, {"heading": "5.3 Example Gaussian Mixture models", "text": "Let us consider that T represents samples taken from a continuous probability distribution. Here, T has an index for each N sample. Let us consider the problem of the approximation of T by a set of basic functions. Let us consider in particular the case of a Gaussian basis on a dimension of fixed variance. Each m describes a certain gauss by defining the density at each data point x. Let us define each m by a unique mean. Let us use px \u2212 \u2212 \u2212 \u2212 \u2212 to denote the spatial position of a given point x. Let us apply the optimization via Jenson's inequality as in Section 5.2 and use the corresponding notation. Let us consider a probability distribution z index.Let us now write the goal for the optimization and apply Jenson's inequality [28]. Let us recognize that this inequality is not negative and normalize it to one and move the normalization constant outside the max. x. Let us now write the optimization via the protocol x."}, {"heading": "6 Experiments", "text": "In this section we show the effectiveness of our approach to converging large symmetric 3-way tensors in family one and two."}, {"heading": "6.1 Family One", "text": "We use various sizes (30.35.40.45) constructed from a convex combination of three to ten unique rank-1 tensors with weights that add up to one. We construct each unique rank-1 tensor by the triple outer product of a unique random unit vector. Each such vector has between six and fifteen non-zero elements. We inject sounds that describe between one and twenty percent of the tensor. For each problem example, we use four different L1 regulators [0.1, 0.4, 0.7, 1.0]. We look at 9000 problem cases and continue the optimization for up to five minutes per instance, after which the termination occurs after the solution of the current QP. In Figure 1 (a) we show the loss in terms of the mean time compared with unended problem cases. This diagram shows that we quickly produce cost-effective solutions.In Fig 1 (F1) we will not optimize the total time loss."}, {"heading": "6.2 Family Two", "text": "We test the effectiveness of our approach on symmetrical 3-way tensors in family two just like in family one, albeit on a smaller scale of problems. We use size twenty tensors, where the base vectors used to construct the tensor have between four and six non-zero values. Each base vector was not negative and its elements added up to one, rather than having unit standards as in family one. We look at 750 problem cases and continue optimization for up to five minutes per instance after which the termination after solving the current LP. In figure 1 (d) we show the loss in terms of time averaged over unscheduled problem cases. This chart shows that we produce cost-effective solutions quickly, though not as fast as in family one.In figure 1 (e) we show the corresponding chart for family two tensors for 1 (b)."}, {"heading": "7 Literature Review", "text": "Our work can be positioned at the interface of two well-studied areas: 1) column generation for combinatorial optimization and 2) efficient representation of data, especially with regard to the decomposition of low-level tensors. In this section we will discuss some of the related work in these two large areas in the context of our work."}, {"heading": "7.1 Column Generation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1.1 The Cutting Stock Problem", "text": "This year, we will be able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "7.1.2 Marginals in Graphical Models", "text": "Our work refers to the work of [9] (which is extended in [34]). While column generation is not explicitly used or mentioned, its approach is very similar to column generation and its problem has a cross entropy loss as in Family 2. In this paper, the authors address the problem of calculating the boundary distributions of variables in a Markov Random Field (MRF) by minimizing the standard Bethe-style convex variation targets. The authors introduce an algorithm to construct a probability distribution over solutions and greedily add solutions to the probability distribution. At each step of the algorithm, a new solution is generated and added to the probability distribution.The selection of the optimal solution for the addition corresponds to the MAP inference, which is a holistic program and is often NP-hard. The potentials for the MAP inference are a function of the current probability distribution and the potentials that can be used to define the original MRAP as an inference, as opposed to the MAP structure that can be used as an inductive."}, {"heading": "7.1.3 Generating Multiple Primal Variables", "text": "Finding the optimal primitive variable that can be added in our applications is NP-hard, but several local optima can be generated simultaneously if the same dual variables are generated simultaneously, each Optima being calculated on a separate CPU. This can be compared to the work of [50], where a large number of variables, each of which improves the target, are generated simultaneously in polynomial time as part of a common operation. In [50] the tracking of thousands of objects in the video is formulated as a holistic program that relaxes compared to a linear program, where each variable corresponds to an entire track of an object. Finding the optimal variable that can be added to the workset corresponds to solving a dynamic program that has polynomial time solvable with global optimality. However, dynamic programming not only provides the optimal solution, but the optimal solution that allows each position to pass through spatially time corresponds to the solution set. Each of these approaches can be added per this subset of the 50, or one of the highest number of variables can be violated by a very large number of advantages."}, {"heading": "7.2 Low Rank Tensor Decompositions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.2.1 Orthogonal Tensor Decompositions", "text": "In [2], the authors address the problem of latent variable modeling by using modeling assumptions about how the data was generated. If these assumptions are followed, they can guarantee a globally optimal restoration of the data-generating model. [2] The work of [2] comes with powerful statistical guarantees. However, if the modeling assumptions are not followed by the process of data generation, the recovered solution, which is intended to describe a mixed probability of distributions, does not have to generate probability distributions, although these can be rounded up to probability distributions in order to produce approximate solutions. In contrast to the work of [2], we focus on optimization and ensure that the given representation follows the rules that define the primitives."}, {"heading": "7.3 Gradient Descent and Alternating Least Squares", "text": "Two common and powerful approaches to the decomposition of tensors are gradients descending [1] and alternately lowest squares (ALS) [16, 32, 23]. Gradient decently optimizes all modes of the tensor simultaneously and converges to a local optimum of the target. Alternatively, ALS solves for one of the tensor modes at a time when the other modes are fixed. This is achieved via lowest squares and is coordinatingly optimal. ALS is not guaranteed to achieve global optimality, but is highly successful in practice and benefits from the lack of a step size as in gradient methods [32]. In Appendix C, we show coordinated updates for Family One and Family Two, while gradient descending updates are available in Section 5.1, and in Appendix C. Since we calculate only one, ranking-1 tensor at a time, our method does not solve the least squares problems with coordinated updates."}, {"heading": "7.3.1 Constrained Tensor Decomposition via the Alternating Direction of Multipliers Method", "text": "In [36], the authors use the alternating direction of multipliers [12] (ADMM) method to divide limited tensor factorization problems into separate problems (sub-problems) that are forced to have a common solution with lagrange multipliers. Solutions to the sub-problems are much easier to calculate than solving the originally limited problem, although they need to be solved many times over. This difficulty can be reduced by parallel computation. Lagrange multipliers are used to couple the problems and function in a way that the unlimited problems are not more difficult to solve. [36] \"s work is extended to [25], where it is shown to provide current results for non-negative tensor factoring and other problems."}, {"heading": "8 Conclusions and Future Work", "text": "This year we have the opportunity to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "A Family One: Dual Derivation", "text": "We start with the objective function of family One after the introduction of the complex replacement object K.min w \u2265 0 K \u2265 01 2 KtK + ~ tw (33) s.t. K \u2265 T \u2212 Mw K \u2265 Mw \u2212 TNext, we replace the constraints with Lagrange multiplier.Eq 4 = min w \u2265 0 K \u2265 0 KtK + ~ tw (33) s.t. K \u2265 T \u2212 Mw \u2212 K \u2265 Mw \u2212 K) + \u03bbt (T \u2212 Mw \u2212 K) Since Eq 4 is a convex square program, we are able to reverse the order of the minimum and maximum without changing the value of the object.Eq 4 = max \u00b2 0 = 0 min w = 0 min w = (K \u00b2 01 2 KtK + tw (35) + tw \u2212 t (T \u2212 tw \u2212 K).tw"}, {"heading": "B Family Two: Dual Derivation", "text": "We start with the objective function of the family Two after the introduction of the complex substitute object K and the replacement of the protocol by a concave shell of affinies.min w \u2265 01tw \u2264 1 K \u2265 0T tK + ~ taw (40) s.t. \u2212 K \u2264 log (1\u03b7) + (Mw \u2212 1\u03b7). We now add Lagrange multipliers \u03b1 and \u03b2.Eq 40 = min w \u2265 0 K \u2265 0 max \u03b1 0 \u03b2 0 T tK + ~ taw (41) \u03b1 (1tw \u2212 1) \u03b7 (0,1] \u03b2t\u03b7 (\u2212 1) \u03b2t\u03b7 (\u2212 1 \u00b0 Mw + 1 \u2212 1 log (\u03b7) We are now grouped by primary variables."}, {"heading": "C Optimizing \u03b8tm, Family One: Non-Symmetric Tensors", "text": "We now examine the maxm-M tensor in the domain where m corresponds to the fit of a non-symmetrical 3-way tensor. Thus, each column of M corresponds to a non-symmetrical 3-way tensor. We describe m with vectors unit vectors va, vb, vc, indexed by i, j, k. The non-vectorized form of m is designated as m and defined below. We describe m with vectors unit vectors va, vb, vc k (45) We write optimizations below. Max m \"M\" Va, vb vcvatva = 1 vctvc = 1 vctvkv, c k (46) We write optimizations for Vai, the optimizations for all i VAV, VAV, VAV, VAV, VAV, VAV, VAV, VAV."}, {"heading": "D Optimizing \u03b8tm, Family Two: Non-Symmetric Tensors", "text": "s remember that we are non-negative and that we switch non-symmetric tensors to one and move the normalization constant outside the maximum. We are now writing optimizations about the protocol of maxm. (49) As in EM methods for conclusions in probability models, we define a probability distribution of z = 1, 1tvb = 1, 1tvc = 1log. (49) As in EM methods for conclusions in probability models, we define a probability distribution from z to ijk. We initialize z as a consequence of the probability distribution, although this initialization is heuristic and random initialization of z is also valid."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We apply column generation to approximating complex structured objects via a<lb>set of primitive structured objects under either the cross entropy or L2 loss. We<lb>use L1 regularization to encourage the use of few structured primitive objects.<lb>We attack approximation using convex optimization over an infinite number of<lb>variables each corresponding to a primitive structured object that are generated<lb>on demand by easy inference in the Lagrangian dual. We apply our approach to<lb>producing low rank approximations to large 3-way tensors.", "creator": "LaTeX with hyperref package"}}}