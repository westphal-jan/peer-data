{"id": "1602.04489", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2016", "title": "Convolutional Tables Ensemble: classification in microseconds", "abstract": "We study classifiers operating under severe classification time constraints, corresponding to 1-1000 CPU microseconds, using Convolutional Tables Ensemble (CTE), an inherently fast architecture for object category recognition. The architecture is based on convolutionally-applied sparse feature extraction, using trees or ferns, and a linear voting layer. Several structure and optimization variants are considered, including novel decision functions, tree learning algorithm, and distillation from CNN to CTE architecture. Accuracy improvements of 24-45% over related art of similar speed are demonstrated on standard object recognition benchmarks. Using Pareto speed-accuracy curves, we show that CTE can provide better accuracy than Convolutional Neural Networks (CNN) for a certain range of classification time constraints, or alternatively provide similar error rates with 5-200X speedup.", "histories": [["v1", "Sun, 14 Feb 2016 19:21:17 GMT  (413kb,D)", "http://arxiv.org/abs/1602.04489v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["aharon bar-hillel", "eyal krupka", "noam bloom"], "accepted": false, "id": "1602.04489"}, "pdf": {"name": "1602.04489.pdf", "metadata": {"source": "CRF", "title": "Convolutional Tables Ensemble: classification in microseconds", "authors": ["Aharon Bar-Hillel", "Eyal Krupka"], "emails": ["aharonb@microsoft.com", "eyalk@microsoft.com", "t-noblo@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is such that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is in which there is a process in which there is"}, {"heading": "2. Convolutional Tables Ensemble", "text": "We present the classification structure in Section 2.1 and derive the learning algorithm in Section 2.2. The details and variations of structure and training appear in Section 2.3 and 2.4 respectively."}, {"heading": "2.1. Notation and classifier structure", "text": "In fact, it is true that this is a cul-de-sac type, in which it is a cul-de-sac type, in which people are able to hide. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2.2. Training", "text": "The main ideas behind these methods are that we have to choose between solving a convex loss function of [26, 2] and increasing the representation of [26, 2]. Assume a which is a described training sample with a fixed representation of [26, 2]. (Hi, yi)} Ni, yi, yi. (Hi, yi)} Nm2 K, yi, and denote the c-th series of the weight matrix W of Wc. (We want a described training sample with a fixed representation. (Hi, yi)} Ni, yi. (We want the c-th series of the weight matrix W of Wc.) We want a linear classifier of the form C, maxc sc sc sc with sc = WcHt."}, {"heading": "2.3. Structural variants", "text": "In fact, it is such that it is a matter of a way in which people move in a world, in which they move in a world, in which they are in a world, in which they are in a world, in which they live in a world, in which they are in a world, in which they are in a world, in which they are in a world, in which they are in a world, in which they are in a world, in which they live in a world, in which they are in a world, in which they are in a world, in which they are in a world, in which they are in a world, in which they live in a world, in which they are one world, in which they live a world, in which they are one world, in which they are one world, in which they are one world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2.4. Training variants", "text": "As already explained in the first phase, the calculation method is between gradient calculation and global optimization of tabular weight.We will now describe the methods we are exploring for these two components: We will consider several mechanisms for optimizing R (B), including forward bit function selection, optimal limit determination and iterative bit function. In forward selection, we will optimize R (B) by adding one bit after another. There are such steps for growth. In phase l = 1, {F} Ncj = 1 candidate bit func replacement / refinement.In forward selection, we will optimize R (B) by adding one bit after another. [B + = F] and select the one with the highest score. However, we have found that the simple greedy calculation of R (B +) in each step is not the best method for optimizing R (B), and an auxiliary number of points will be introduced."}, {"heading": "3. Empirical results", "text": "In Section 3.2 we compare with related art and evaluate the contribution of algorithmic components to performance. Results of trade-offs between speed and accuracy are presented in Section 3.3."}, {"heading": "3.1. Implementation and data details", "text": "The experiments were conducted on 4 publicly available data sets: MNIST, CIFAR-10, SVHN and 3-HANDPOSE. The first three are standard detection benchmarks in grayscale (MNIST) or RGB (CIFAR-10, SVHN), each with 10 classes. 3-HANDPOSE is a 4-class data set with 3 hand positions and a fourth class of \"others,\" and its images contain depth and IR channels. Image sizes range from 28 x 28 (MNIST) to 36 x 36 (3-HANDPOSE). The size of the training set ranges from 50,000 (CIFAR-10) to 604,000 (SVHN). The CTE training code was written in Matlab, with some routines using code from the packages [8, 11, 37] and the test time classifiers implemented and optimized."}, {"heading": "3.2. Comparison and variation", "text": "The first two columns in Table 1 contain errors of DFE and CTE on the 4 data sets, with 50 ferns used for MNIST, SVHN, 3-HANDPOSE, and 100 for CIFAR-10. MNIST was trained with softmax distillation losses (see below), and the others with SVM losses. The aggregation area {Am} Mm = 1 was selected to be identical for all tables in a classifier, with the formation of a centered square encompassing most of the image. For comparison, M-class error rates are extracted from DFE (not reported in [20] such errors, and 3-class average positive rates are reported."}, {"heading": "3.3. Speed-Accuracy trade-off", "text": "We are able to achieve the best results in the way that they are able to achieve the best results."}, {"heading": "4. Conclusions and further work", "text": "We have shown that CTE can provide higher accuracy than CNNs for highly computationally constrained tasks. A natural direction for future research is to replace the flat structure of CTEs with a multi-layered approach to try to enjoy the accuracy of CNNs at the speed of CTEs."}, {"heading": "5. Appendix", "text": "Evidence2.1 of section 2.4.Proof 1st1 of the definition (1) = (1) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _"}], "references": [{"title": "Sparse convolutional neural networks", "author": ["L. Baoyuan", "W. Min", "F. Hassan", "T. Marshall", "P. Marianna"], "venue": "CVPR,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Object class recognition by boosting a part based model", "author": ["A. Bar-Hillel", "T. Hertz", "D. Weinshall"], "venue": "CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Partbased feature synthesis for human detection", "author": ["A. Bar-Hillel", "D. Levi", "E. Krupka", "C. Goldberg"], "venue": "ECCV,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Pedestrian detection at 100 frames per second", "author": ["R. Benenson", "M. Mathias", "R. Timofte", "L.J.V. Gool"], "venue": "CVPR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Image classification using random forests and ferns", "author": ["A. Bosch", "A. Zisserman", "X. Mu\u00f1oz"], "venue": "ICCV, pages 1\u20138,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Decision forests for classification, regression, density estimation, manifold learning and semi-supervised learning", "author": ["A. Criminisi", "J. Shotton", "E. Konukoglu"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast, accurate detection of 100,000 object classes on a single machine", "author": ["T. Dean", "M. Ruzon", "M. Segal", "J. Shlens", "S. Vijayanarasimhan", "J. Yagnik"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, Washington, DC, USA,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Integral channel features", "author": ["P. Dollar", "Z. Tu", "P. Perona", "S. Belongie"], "venue": "BMVC,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.- J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Extremely randomized trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Mach. Learn., 63(1):3\u201342, Apr.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Spatially-sparse convolutional neural networks", "author": ["B. Graham"], "venue": "CoRR, abs/1409.6070,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X.S. Zhang", "Ren", "J. Sun"], "venue": "CoRR, abs/1406.4729v2,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "CoRR, abs/1503.02531,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "A dual coordinate descent method for largescale linear svm", "author": ["C.-J. Hsieh", "K.-W. Chang", "C.-J. Lin", "S.S. Keerthi", "S. Sundararajan"], "venue": "ICML,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Do deep nets really need to be deep", "author": ["B. Jimmy", "C. Rich"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classifcation", "author": ["H. Kaiming", "Z. Xiangyu", "R. Shaoqing", "S. Jian"], "venue": "CoRR, abs/1502.01852,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Technical report, Masters thesis, Department of Computer Science, University of Toronto,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Discriminative ferns ensemble for hand pose recognition", "author": ["E. Krupka", "A. Vinnikov", "B. Klein", "A.B. Hillel", "D. Freedman", "S. Stachniak"], "venue": "CVPR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR, pages 2169\u20132178,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Keypoint recognition using randomized trees", "author": ["V. Lepetit", "P. Fua"], "venue": "PAMI, 28:1465\u20131479,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast multiple-part based object detection using kd-ferns", "author": ["D. Levi", "S. Silberstein", "A. Bar-Hillel"], "venue": "CVPR,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "CoRR, abs/1312.4400,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Boosting algorithms as gradient descent", "author": ["L. Mason", "J. Baxter", "P. Bartlett", "M. Frean"], "venue": "NIPS, pages 512\u2013518,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Face alignment at 3000 fps via regressing local binary features", "author": ["S. Ren", "X. Cao", "Y. Wei", "J. Sun"], "venue": "CVPR, June", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "CoRR, abs/1412.6550,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "ICML,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Real-time human pose recognition in parts from single depth images", "author": ["J. Shotton", "T. Sharp", "A. Kipman", "A.W. Fitzgibbon", "M. Finocchio", "A. Blake", "M. Cook", "R. Moore"], "venue": "Commun. ACM, 56(1):116\u2013124,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["J. Snoek", "O. Rippel", "K. Swersky", "R. Kiros", "N. Satish", "N. Sundaram", "M.A. Patwary", "Prabhat", "R.P. Adams"], "venue": "CoRR, abs/1502.05700,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "J. Mach. Learn. Res., 15(1):1929\u2013 1958,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "A Fast Local Descriptor for Dense Matching", "author": ["E. Tola", "V.Lepetit", "P. Fua"], "venue": "In CVPR,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition", "author": ["L. Vadim", "G. Yaroslav", "R. Maksim", "O. Ivan", "L. Victor"], "venue": "CoRR, abs/1412.6553,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving the speed of neural networks on cpus", "author": ["V. Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": "Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceeding of the ACM Int. Conf. on Multimedia,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M. Jones"], "venue": "CVPR,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2001}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["J. Yangqing", "S. Evan", "D. Jeff", "K. Sergey", "L. Jonathan", "G. Ross", "G. Sergio", "D. Trevor"], "venue": "Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "The accuracy-speed trade-off has thus been widely discussed in the literature, and various architectures have been suggested [14, 35, 7, 4, 24, 31].", "startOffset": 125, "endOffset": 147}, {"referenceID": 32, "context": "The accuracy-speed trade-off has thus been widely discussed in the literature, and various architectures have been suggested [14, 35, 7, 4, 24, 31].", "startOffset": 125, "endOffset": 147}, {"referenceID": 6, "context": "The accuracy-speed trade-off has thus been widely discussed in the literature, and various architectures have been suggested [14, 35, 7, 4, 24, 31].", "startOffset": 125, "endOffset": 147}, {"referenceID": 3, "context": "The accuracy-speed trade-off has thus been widely discussed in the literature, and various architectures have been suggested [14, 35, 7, 4, 24, 31].", "startOffset": 125, "endOffset": 147}, {"referenceID": 21, "context": "The accuracy-speed trade-off has thus been widely discussed in the literature, and various architectures have been suggested [14, 35, 7, 4, 24, 31].", "startOffset": 125, "endOffset": 147}, {"referenceID": 28, "context": "The accuracy-speed trade-off has thus been widely discussed in the literature, and various architectures have been suggested [14, 35, 7, 4, 24, 31].", "startOffset": 125, "endOffset": 147}, {"referenceID": 19, "context": "This is analogous to a convolutional layer in a convolutional neural network (CNN) [22], where the same set of filters is applied at each image position.", "startOffset": 83, "endOffset": 87}, {"referenceID": 17, "context": "Variants of this architecture have been used successfully mainly for classification of depth images [20, 31].", "startOffset": 100, "endOffset": 108}, {"referenceID": 28, "context": "Variants of this architecture have been used successfully mainly for classification of depth images [20, 31].", "startOffset": 100, "endOffset": 108}, {"referenceID": 17, "context": "We start with the simple functions employed in [20, 31], which were suitable for depth images, and extend them using gradient and color based channels and features employed in [9].", "startOffset": 47, "endOffset": 55}, {"referenceID": 28, "context": "We start with the simple functions employed in [20, 31], which were suitable for depth images, and extend them using gradient and color based channels and features employed in [9].", "startOffset": 47, "endOffset": 55}, {"referenceID": 7, "context": "We start with the simple functions employed in [20, 31], which were suitable for depth images, and extend them using gradient and color based channels and features employed in [9].", "startOffset": 176, "endOffset": 179}, {"referenceID": 28, "context": "A second important choice is between conditional computation of bit functions, leading to tree structures like used in [31], and unconditional computation as in fern structures [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "A second important choice is between conditional computation of bit functions, leading to tree structures like used in [31], and unconditional computation as in fern structures [20].", "startOffset": 177, "endOffset": 181}, {"referenceID": 9, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 4, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 31, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 20, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 5, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 17, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 25, "context": "Several works have addressed the challenges of learning a tables-based classifier [12, 5, 34, 23, 6, 20, 28].", "startOffset": 82, "endOffset": 108}, {"referenceID": 9, "context": "These vary in optimization effort from extremely random forests [12] to global optimization of table weights and greedy forward choice of bit functions [28, 20].", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "These vary in optimization effort from extremely random forests [12] to global optimization of table weights and greedy forward choice of bit functions [28, 20].", "startOffset": 152, "endOffset": 160}, {"referenceID": 17, "context": "These vary in optimization effort from extremely random forests [12] to global optimization of table weights and greedy forward choice of bit functions [28, 20].", "startOffset": 152, "endOffset": 160}, {"referenceID": 17, "context": "Our approach builds on previous approaches, mostly [20], and extends them with new possibilities.", "startOffset": 51, "endOffset": 55}, {"referenceID": 23, "context": "We learn the table ensemble by adding one table at a time, using a framework similar to the \u2019anyboost\u2019 algorithm [26, 2].", "startOffset": 113, "endOffset": 120}, {"referenceID": 1, "context": "We learn the table ensemble by adding one table at a time, using a framework similar to the \u2019anyboost\u2019 algorithm [26, 2].", "startOffset": 113, "endOffset": 120}, {"referenceID": 17, "context": "For the global optimization we used two main options: an SVM loss as used in [20] and a softmax loss as commonly used in CNN training.", "startOffset": 77, "endOffset": 81}, {"referenceID": 14, "context": "In several recent studies [17, 15, 29], the output of an accurate but computationally expensive classifier is used to train another classifier, with a different and often computationally cheaper architecture.", "startOffset": 26, "endOffset": 38}, {"referenceID": 12, "context": "In several recent studies [17, 15, 29], the output of an accurate but computationally expensive classifier is used to train another classifier, with a different and often computationally cheaper architecture.", "startOffset": 26, "endOffset": 38}, {"referenceID": 26, "context": "In several recent studies [17, 15, 29], the output of an accurate but computationally expensive classifier is used to train another classifier, with a different and often computationally cheaper architecture.", "startOffset": 26, "endOffset": 38}, {"referenceID": 12, "context": "We made a preliminary attempt to use this technique, termed distillation in [15], to train a CTE classifier with a CNN teacher, with encouraging results on the MNIST data.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "2 we present experiments demonstrating the performance gains of our techniques by comparison with the DFE method of [20], ablation studies, fern-tree trade-off experiments, and distillation results.", "startOffset": 116, "endOffset": 120}, {"referenceID": 19, "context": "We use several publicly available object recognition benchmarks: MNIST [22], CIFAR-10 [19], SVHN [27] and 3-HANDPOSE [20].", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "We use several publicly available object recognition benchmarks: MNIST [22], CIFAR-10 [19], SVHN [27] and 3-HANDPOSE [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 24, "context": "We use several publicly available object recognition benchmarks: MNIST [22], CIFAR-10 [19], SVHN [27] and 3-HANDPOSE [20].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "We use several publicly available object recognition benchmarks: MNIST [22], CIFAR-10 [19], SVHN [27] and 3-HANDPOSE [20].", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "CTE achieves error improvements of 24 \u2212 45% over [20], with 38% improvement on 3-HANDPOSE, the original data used in [20].", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "CTE achieves error improvements of 24 \u2212 45% over [20], with 38% improvement on 3-HANDPOSE, the original data used in [20].", "startOffset": 117, "endOffset": 121}, {"referenceID": 22, "context": "For the latter we trained NIN networks [25], combining state-of-the-art accuracy with significant speed advantages, and further accelerated them by scaling parameters of breadth, NIN output dimension and convolution stride.", "startOffset": 39, "endOffset": 43}, {"referenceID": 32, "context": "Alternatives to CTE may be provided by the literature dealing with CNN acceleration [35, 36, 1].", "startOffset": 84, "endOffset": 95}, {"referenceID": 33, "context": "Alternatives to CTE may be provided by the literature dealing with CNN acceleration [35, 36, 1].", "startOffset": 84, "endOffset": 95}, {"referenceID": 0, "context": "Alternatives to CTE may be provided by the literature dealing with CNN acceleration [35, 36, 1].", "startOffset": 84, "endOffset": 95}, {"referenceID": 17, "context": "In [20, 28] instances of convolutional tables ensemble were discriminatively optimized for specific tasks and losses (hand pose recognition using SVM in [20], and face alignment using l regression in [28]).", "startOffset": 3, "endOffset": 11}, {"referenceID": 25, "context": "In [20, 28] instances of convolutional tables ensemble were discriminatively optimized for specific tasks and losses (hand pose recognition using SVM in [20], and face alignment using l regression in [28]).", "startOffset": 3, "endOffset": 11}, {"referenceID": 17, "context": "In [20, 28] instances of convolutional tables ensemble were discriminatively optimized for specific tasks and losses (hand pose recognition using SVM in [20], and face alignment using l regression in [28]).", "startOffset": 153, "endOffset": 157}, {"referenceID": 25, "context": "In [20, 28] instances of convolutional tables ensemble were discriminatively optimized for specific tasks and losses (hand pose recognition using SVM in [20], and face alignment using l regression in [28]).", "startOffset": 200, "endOffset": 204}, {"referenceID": 23, "context": "Here we adapt these ideas to linear M -classification with an arbitrary l2-regularized convex loss function, using techniques from [26, 2].", "startOffset": 131, "endOffset": 138}, {"referenceID": 1, "context": "Here we adapt these ideas to linear M -classification with an arbitrary l2-regularized convex loss function, using techniques from [26, 2].", "startOffset": 131, "endOffset": 138}, {"referenceID": 7, "context": "\u2022 Gradient-based channels: Two kinds of gradient maps are computed from the original channels following [9].", "startOffset": 104, "endOffset": 107}, {"referenceID": 35, "context": "\u2022 Integral channels: Integral images [38] of channels from the previous two forms, again following [9].", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "\u2022 Integral channels: Integral images [38] of channels from the previous two forms, again following [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 18, "context": "This effectively puts a spatial grid over the image, thus turning the global summation pooling into local summation using a pyramid-like structure [21].", "startOffset": 147, "endOffset": 151}, {"referenceID": 17, "context": "Comparison and ablation: Columns one and two present errors of a DFE [20] and a baseline CTE.", "startOffset": 69, "endOffset": 73}, {"referenceID": 27, "context": "Its advantage lies in the availability of fast and scalable methods for solving large and sparse SVM programs [30, 16].", "startOffset": 110, "endOffset": 118}, {"referenceID": 13, "context": "Its advantage lies in the availability of fast and scalable methods for solving large and sparse SVM programs [30, 16].", "startOffset": 110, "endOffset": 118}, {"referenceID": 2, "context": "In [3] a first order approximation for minW LSVM is derived for new feature addition, in which the example gradients are \u2212\u03b1iyi,c with \u03b1i the dual SVM variables at the optimum.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "Conveniently, it can be extended to a distillation loss [15], which enables guidance of the classifier using an internal representation of a well-trained CNN classifier.", "startOffset": 56, "endOffset": 60}, {"referenceID": 8, "context": "CTE training code was written in Matlab, with some routines using code from the packages [8, 11, 37].", "startOffset": 89, "endOffset": 100}, {"referenceID": 34, "context": "CTE training code was written in Matlab, with some routines using code from the packages [8, 11, 37].", "startOffset": 89, "endOffset": 100}, {"referenceID": 34, "context": "CNN models were trained using MatConvNet [37].", "startOffset": 41, "endOffset": 45}, {"referenceID": 36, "context": "The implementation is efficient, reported to be comparable to Caffe [39] in [37], with the convolutional and global layers reduced to matrix multiplication done using an SSEoptimized BLAS package.", "startOffset": 68, "endOffset": 72}, {"referenceID": 34, "context": "The implementation is efficient, reported to be comparable to Caffe [39] in [37], with the convolutional and global layers reduced to matrix multiplication done using an SSEoptimized BLAS package.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "Comparison with DFE: The Discriminative Ferns Ensemble (DFE) was suggested in [20] for classification of 3-HANDPOSE, and can be seen a baseline for CTE, which enhances it in many aspects.", "startOffset": 78, "endOffset": 82}, {"referenceID": 17, "context": "To enable the comparison, M-class error rates are extracted from DFE (in [20] such errors are not reported, and 3-class average true positive rates are reported instead).", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "Distillation experiments: We experimented with knowledge distillation from a CNN to a CTE using the method suggested in [15].", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "Our CNN baseline architectures are variations of DeepCNiN(l,k) [13], with l = 3 \u2212 4 convolutional layers and k = 60\u2212 100, implying usage of i \u00b7k maps at the i-th layer.", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "It was shown in [13] that higher l, k values provide better accuracy, but such architectures are much slower than 1 CPU millisecond and so they are outside our domain of interest.", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": "We experimented with dropout [33], parametric RELU units [18], affine image transformations following [13], and HSV image transformations following [32].", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "We experimented with dropout [33], parametric RELU units [18], affine image transformations following [13], and HSV image transformations following [32].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "We experimented with dropout [33], parametric RELU units [18], affine image transformations following [13], and HSV image transformations following [32].", "startOffset": 102, "endOffset": 106}, {"referenceID": 29, "context": "We experimented with dropout [33], parametric RELU units [18], affine image transformations following [13], and HSV image transformations following [32].", "startOffset": 148, "endOffset": 152}], "year": 2016, "abstractText": "We study classifiers operating under severe classification time constraints, corresponding to 1\u22121000 CPU microseconds, using Convolutional Tables Ensemble (CTE), an inherently fast architecture for object category recognition. The architecture is based on convolutionally-applied sparse feature extraction, using trees or ferns, and a linear voting layer. Several structure and optimization variants are considered, including novel decision functions, tree learning algorithm, and distillation from CNN to CTE architecture. Accuracy improvements of 24\u221245% over related art of similar speed are demonstrated on standard object recognition benchmarks. Using Pareto speed-accuracy curves, we show that CTE can provide better accuracy than Convolutional Neural Networks (CNN) for a certain range of classification time constraints, or alternatively provide similar error rates with 5\u2212 200\u00d7 speedup.", "creator": "LaTeX with hyperref package"}}}