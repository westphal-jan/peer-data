{"id": "1602.04741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2016", "title": "Delay and Cooperation in Nonstochastic Bandits", "abstract": "We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than $d$ hops to arrive, where $d$ is a delay parameter. We introduce \\textsc{Exp3-Coop}, a cooperative version of the {\\sc Exp3} algorithm and prove that with $K$ actions and $N$ agents the average per-agent regret after $T$ rounds is at most of order $\\sqrt{\\bigl(d+1 + \\tfrac{K}{N}\\alpha_{\\le d}\\bigr)(T\\ln K)}$, where $\\alpha_{\\le d}$ is the independence number of the $d$-th power of the connected communication graph $G$. We then show that for any connected graph, for $d=\\sqrt{K}$ the regret bound is $K^{1/4}\\sqrt{T}$, strictly better than the minimax regret $\\sqrt{KT}$ for noncooperating agents. More informed choices of $d$ lead to bounds which are arbitrarily close to the full information minimax regret $\\sqrt{T\\ln K}$ when $G$ is dense. When $G$ has sparse components, we show that a variant of \\textsc{Exp3-Coop}, allowing agents to choose their parameters according to their centrality in $G$, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay.", "histories": [["v1", "Mon, 15 Feb 2016 17:20:28 GMT  (195kb,D)", "https://arxiv.org/abs/1602.04741v1", "27 pages"], ["v2", "Wed, 1 Jun 2016 16:20:30 GMT  (199kb,D)", "http://arxiv.org/abs/1602.04741v2", "30 pages"]], "COMMENTS": "27 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nicolo' cesa-bianchi", "claudio gentile", "yishay mansour", "alberto minora"], "accepted": false, "id": "1602.04741"}, "pdf": {"name": "1602.04741.pdf", "metadata": {"source": "CRF", "title": "Delay and Cooperation in Nonstochastic Bandits", "authors": ["Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile", "Yishay Mansour", "MANSOUR MINORA"], "emails": ["NICOLO.CESA-BIANCHI@UNIMI.IT", "CLAUDIO.GENTILE@UNINSUBRIA.IT", "MANSOUR@TAU.AC.IL", "AMINORA@UNINSUBRIA.IT"], "sections": [{"heading": null, "text": "We then show that for each linked graph for d = \u221a K the limit of regret is K1 / 4 \u221a T, strictly better than the minimax remorse \u221a KT for non-cooperating agents. More informed decisions for d lead to limits that are arbitrarily close to the full information minimax remorse \u221a T lnK when G is dense. If G has sparse components, we show that a variant of EXP3-COOP that allows agents to choose their parameters according to their centrality in G strictly improves regret. Finally, as a by-product of our analysis, we provide the first characterization of minimax remorse for bandit learning with delay."}, {"heading": "1. Introduction", "text": "This leads to a problem that each of us learns that depends on other observers who depend on the usefulness of a recommendation by detecting the occurrence of certain events (e.g. a user conversion) that can happen with variable delay after the recommendation is issued. Other examples include the communication delays experienced by interacting learning agents in order to maximize the overall revenue of the network and share feedback information with other servers to speed up learning. However, the rate at which information is exchanged over the communication network is lower than the typical rate at which ads are placed. Cesa-Bianchi, C. Gentile, Y. ar Xiv: 160 2.04 741v 2 [cs.L G] 1"}, {"heading": "2. Additional Related Work", "text": "In fact, it is a matter of a way in which one puts oneself and oneself in the centre. (...) It is a matter of a way in which one puts oneself in the centre of attention. (...) It is a question of how one puts oneself in the centre. (...) It is a question of how one puts oneself in the centre. (...) It is a question of how one puts oneself in the centre. (...) It is a question of how one puts oneself in the centre. (...) It is a question of how one puts oneself in the centre. (...) It is a question of how one puts oneself in the centre. (...) It is a question of how one puts oneself in the centre. (...) It is a question of how one puts oneself in the centre. (...) It is a question of how one puts oneself in the centre. (...) It is a question of how one puts oneself in the centre. (...) It is a question of how one puts oneself in the centre. (...) It is a question of how one puts oneself in the centre."}, {"heading": "3. Preliminaries", "text": "We now establish our notation, along with basic assumptions and preliminary facts related to our algorithms. Notation and setting here both relate to the individual case. Cooperative placement with multiple agents (and their notation) is already in Section 4. Evidence of all the results given here can, however, be found in (Cesa-Bianchi et al., 2016). Let A = {1,.., K} be the plot set. A Learning Agent performs an exponentially weighted algorithm with weights wt (i) and learning rate (i) > 0. First, w1 (i) = 1 for all i-based skills. At any point in time step t = 1, 2,.,., the agent draws actions with the probability P (It = i) = pt (i) = wt (i) / Wt, where Wt = j."}, {"heading": "4. The Cooperative Setting on a Communication Network", "text": "In our multi-agent bandit setting (v), there are N agents who sit on the vertices of a connected and undirected communication course (v) (v = 1). The agents work together to solve the same instance of a non-stochastic bandit problem while limiting the communication between them. Let's call Ns (v) the amount of messages that are set by A. Note that each action i-A triggers the same loss (i). In each step t = 1, 2,. Each agent v draws an action It (v) from the common action that he has taken. Note that each action i-A triggers the same loss \"t (i), 1) to all agents. It (v) = i at the end of the round t, each agent v observes his own loss\" t (v), and sends the message to his neighbors in G (v)."}, {"heading": "4.1. The Exp3-Coop algorithm", "text": "Our first algorithm, called EXP3-COOP (Cooperative Exp3), is described in Figure 3: \"The algorithm works in the learning protocol of Figure 1. Each agent v \u2212 V performs the exponentially weighted algorithm (1), combined with a\" delayed \"weight-weighted loss estimate (i, v), which contains the delayed information sent by the other agents. Specifically, this means that each agent of N \u2264 d (v) = s \u2264 dNs (v) represents the number of nodes in G whose shortest path from v is at most d, and note that for all v, {v} d = N \u2264 0 (v) N \u2264 N \u2264 2 (\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pt) played by the agents in N \u2264 d (v), at the time t \u2212 d action i (i.e., Bd \u2212 d (i, v \u2212 d) = 1 in Eq."}, {"heading": "5. Extensions: Cooperation with Individual Parameters", "text": "In this section, we will analyze a modification of EXP3-COOP that allows each agent to use this additional arrangement. In addition, we can use a delay parameter d (v) that differs from that of the other agents. We will then show how such individual delays can improve the average well-being of the agents. In the previous setting, in which all agents use the same delay parameter d, the messages have an implicit time span that differs from the delay parameter d (v). However, in this setting, the agents cannot have detailed knowledge of the delay parameters used by the other agents. For this reason, we will allow an agent v to generate messages with a time to life ttl (v) possibly different from the delay parameter d (v). Note that the role of the two parameters d (v) and ttl (v) is inherently different. Whered (the message v) is controlled by the other time the agent is passed on to the number of messages (v)."}, {"heading": "6. Delayed Losses (for a Single Agent)", "text": "EXP3-COOP can specialize in the environment in which a single agent is confronted with a bandit problem, in which the loss of the chosen action is observed with a fixed delay. In this environment, the agent accepts a loss' t (It) at the end of each round and observes' t \u2212 d (It \u2212 d), if t > d, and nothing else. Regrets are defined in the usual way, RT = E [T \u2211 t = 1 't (It) \u2212 min i = 1,..., K T \u0445 t = 1' t (i).This problem has been investigated by Weinberger and Orddlich (2002) in the complete information case, for which they have proven that (d + 1) T lnK is the optimal order for minimax regret. The result has been extended to the bandit case of Neu et al. (2010, 2014) - see also Joulani et al. (2013) - whose techniques can be used to maintain a regret of order."}, {"heading": "7. Conclusions and Ongoing Research", "text": "We have investigated a cooperative and non-stochastic bandit scenario in which cooperation comes at the price of delayed information. We have shown average well-being regret limits that exhibit a natural trade-off between cooperation and delay, with the trade-off determined by the underlying topology of the communication network. Indeed, as a by-product of our analysis, we have provided the first characterization of regret of learning with (constant) delayed feedback in an adverse bandit attitude. There are a number of possible extensions that we are currently considering: 1. So far, our analysis only provides average welfare limits. It would be interesting to point out concurrent regrets limits that apply to each agent individually. We suspect that the individual regret of an agent v is bound up in the form of a delay (d + K | N)."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their careful reading and thoughtful suggestions, which have greatly improved the presentation of this paper. Yishay Mansour is partially supported by the program of the Israeli Centers of Research Excellence (I-CORE) (Center No. 4 / 11), by a scholarship from the Israel Science Foundation (ISF), by a scholarship from the United States-Israel Binational Science Foundation (BSF), and by a scholarship from Len Blavatnik and the Blavatnik Family Foundation."}, {"heading": "Appendix A. Proofs from Section 3", "text": "The proof of term 1.Proof directly from the definition of the update (1), wt + 1 (i) \u2264 pt (i) for all i-A, so that Wt + 1 \u2264 1, which in turn results in wt + 1 (i) \u2264 wt + 1 (i) \u2264 wt (i) / Wt + 1 = pt + 1 (i). After that, + 1 (i) \u2212 pt (i) \u2264 wt + 1 (i) \u2212 pt (i) = pt (i) \u2212 t (i) \u2212 t (i); the last inequality applies to 1 \u2212 e \u2212 x x for x 0. Likewise, pt + 1 (i) \u2212 pt (i) \u2212 wt (i) \u2212 wt (i) = pt + 1 (i) \u2212 t (i) \u2212 t + 1 (i) \u2212 t (i) \u2212 t (i)."}, {"heading": "Appendix B. Proofs from Section 4.1", "text": "The next dilemma refers to the variance of the estimates (3) to the structure of the communication graph G. The dilemma is for a generic, undirected communication graph v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v) v (v) v (v) v (v) v) v (v) v (v) v) v (v) v (v) v) v (v) v (v) v) v (v) v (v) v) v (v) v (v) v) v (v) v (v) v) v (v) v (v) v) v (v) v (v) v (v) v) v (v) v (v) v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v (v) v) v (v (v) v (v) v (v (v) v (v) v (v) v (v) v (v (v) v (v) v (v) v (v) v (v) v (v (v) v (v) v (v) v (v (v) v (v) v (v) v (v (v) v (v) v (v) v ("}, {"heading": "Appendix C. Proofs from Section 5", "text": "We must first pass the preliminary Lemmas 1 and 2 to the new update rule of EXP3-COOP2 (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v + 1 (i) v (v) v \u2212 t (v) v (v) v (v) v \u2212 t (v) v (v) v (v) v (v) v (v) v (v) v (v) v \u2212 t (v) v (v) v \u2212 t (v) v (v) v \u2212 t (v) v (v) v (v) v (v) v (v) v v v v v v (v) v v v v (v) v) v (v) v v (v) v v (v) v (v) v (v) v v) v (v) v v v (v) v) v (v) v v v (v) v) v v (v) v v (v) v) v v v v (v) v v v (v) v (v) v v) v v (v) v (v) v v (v) v v v v v (v) v v v (v) v v v v v v (v) v v v v (t (t) (v) (v) v v v v v v (v) v v v (v v v v v v v v (v) v (v v v v v v v v) v (t (t (v), v (v (v) v (v v (v v v) v v (v v v v v v v (v) v (v v v v v (t (v) v v v v v v v (t (v) v (t (v) v (v v v v v (v) v (v v v v v v v v v v v v (t (t), v (v (v) v (v) v (v v v v v v v v v v (v) v (v v v v v v v v v v v v v"}, {"heading": "Appendix D. Proofs regarding Section 6", "text": "To prove the upper limit, we use the exponentially weighted algorithm with Estimate (3) specializing in the case of a single agent, namely Bt \u2212 d (i) = I {It \u2212 d = i} andqd, t \u2212 d (i) = lp (i). Note that this boils down to the standard Exp3 algorithm performing an update as soon as a new loss becomes available. In this case, we now prove that N = a (G \u2264 d) = 1, the limit of Theorem 3, with a suitable selection of (that of T, K, and d) reduces toRT = O (d + d) T lnK).We prove a lower limit that matches logarithmic factors. The proof that we can apply the lower limit (KT) to bandits without delay."}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["Alekh Agarwal", "John C Duchi"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Agarwal and Duchi.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal and Duchi.", "year": 2011}, {"title": "Nonstochastic multi-armed bandits with graph-structured feedback", "author": ["Noga Alon", "Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile", "Shie Mannor", "Yishay Mansour", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1409.8428,", "citeRegEx": "Alon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2014}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Competitive collaborative learning", "author": ["Baruch Awerbuch", "Robert Kleinberg"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Awerbuch and Kleinberg.,? \\Q2008\\E", "shortCiteRegEx": "Awerbuch and Kleinberg.", "year": 2008}, {"title": "Ad hoc teamwork modeled with multi-armed bandits: An extension to discounted infinite rewards", "author": ["Samuel Barrett", "Peter Stone"], "venue": "In Proceedings of 2011 AAMAS Workshop on Adaptive and Learning Agents,", "citeRegEx": "Barrett and Stone.,? \\Q2011\\E", "shortCiteRegEx": "Barrett and Stone.", "year": 2011}, {"title": "Delay and cooperation in nonstochastic bandits", "author": ["Nicolo\u2019 Cesa-Bianchi", "Claudio Gentile", "Yishay Mansour", "Alberto Minora"], "venue": "arXiv preprint,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2016}, {"title": "Estimation, optimization, and parallelism when data is sparse", "author": ["John Duchi", "Michael I Jordan", "Brendan McMahan"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Duchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2013}, {"title": "Asynchronous stochastic convex optimization", "author": ["John C Duchi", "Sorathan Chaturapruek", "Christopher R\u00e9"], "venue": "arXiv preprint arXiv:1508.00882,", "citeRegEx": "Duchi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2015}, {"title": "Efficient optimal learning for contextual bandits", "author": ["Miroslav Dud\u0131\u0301k", "Daniel J. Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "UAI", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "Independence and average distance in graphs", "author": ["P. Firby", "J. Haviland"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "Firby and Haviland.,? \\Q1997\\E", "shortCiteRegEx": "Firby and Haviland.", "year": 1997}, {"title": "Online learning under delayed feedback", "author": ["Pooria Joulani", "Andr\u00e1s Gy\u00f6rgy", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Joulani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Joulani et al\\.", "year": 2013}, {"title": "Delay-tolerant online convex optimization: Unified analysis and adaptive-gradient algorithms", "author": ["Pooria Joulani", "Andr\u00e1s Gy\u00f6rgy", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17,", "citeRegEx": "Joulani et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Joulani et al\\.", "year": 2016}, {"title": "Bandit problems in networks: Asymptotically efficient distributed allocation rules", "author": ["Soummya Kar", "H Vincent Poor", "Shuguang Cui"], "venue": "In 50th IEEE Conference on Decision and Control and European Control Conference (CDC-ECC),", "citeRegEx": "Kar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kar et al\\.", "year": 2011}, {"title": "Multiplicative updates outperform generic no-regret learning in congestion games", "author": ["Robert Kleinberg", "Georgios Piliouras", "\u00c9va Tardos"], "venue": "In Proceedings of the forty-first annual ACM symposium on Theory of computing,", "citeRegEx": "Kleinberg et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2009}, {"title": "Efficient learning by implicit exploration in bandit problems with side observations", "author": ["Tom\u00e1\u0161 Koc\u00e1k", "Gergely Neu", "Michal Valko", "Remi Munos"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2014}, {"title": "On distributed cooperative decision-making in multiarmed bandits", "author": ["Peter Landgren", "Vaibhav Srivastava", "Naomi Ehrich Leonard"], "venue": "arXiv preprint arXiv:1512.06888,", "citeRegEx": "Landgren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Landgren et al\\.", "year": 2015}, {"title": "Distributed delayed proximal gradient methods", "author": ["Mu Li", "David G. Andersen", "Alexander Smola"], "venue": "In NIPS Workshop on Optimization for Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Locality in distributed graph algorithms", "author": ["Nathan Linial"], "venue": "SIAM J. Comput.,", "citeRegEx": "Linial.,? \\Q1992\\E", "shortCiteRegEx": "Linial.", "year": 1992}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["Ji Liu", "Stephen J Wright", "Christopher R\u00e9", "Victor Bittorf", "Srikrishna Sridhar"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Perturbed iterate analysis for asynchronous stochastic optimization", "author": ["Horia Mania", "Xinghao Pan", "Dimitris Papailiopoulos", "Benjamin Recht", "Kannan Ramchandran", "Michael I Jordan"], "venue": "arXiv preprint arXiv:1507.06970,", "citeRegEx": "Mania et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mania et al\\.", "year": 2015}, {"title": "Delay-tolerant algorithms for asynchronous distributed online learning", "author": ["Brendan McMahan", "Matthew Streeter"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "McMahan and Streeter.,? \\Q2014\\E", "shortCiteRegEx": "McMahan and Streeter.", "year": 2014}, {"title": "On-line learning with delayed label feedback. In Algorithmic Learning Theory, pages 399\u2013413", "author": ["Chris Mesterharm"], "venue": null, "citeRegEx": "Mesterharm.,? \\Q2005\\E", "shortCiteRegEx": "Mesterharm.", "year": 2005}, {"title": "Improving Online Learning", "author": ["Chris Mesterharm"], "venue": "PhD thesis,", "citeRegEx": "Mesterharm.,? \\Q2007\\E", "shortCiteRegEx": "Mesterharm.", "year": 2007}, {"title": "Explore no more: Improved high-probability regret bounds for non-stochastic bandits", "author": ["Gergely Neu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Neu.,? \\Q2015\\E", "shortCiteRegEx": "Neu.", "year": 2015}, {"title": "Online Markov decision processes under bandit feedback", "author": ["Gergely Neu", "Andras Antos", "Andr\u00e1s Gy\u00f6rgy", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Neu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2010}, {"title": "Online markov decision processes under bandit feedback", "author": ["Gergely Neu", "Andras Gyorgy", "Csaba Szepesvari", "Andras Antos"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Neu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2014}, {"title": "Parallel correlation clustering on big graphs", "author": ["Xinghao Pan", "Dimitris Papailiopoulos", "Samet Oymak", "Benjamin Recht", "Kannan Ramchandran", "Michael I Jordan"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Pan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2015}, {"title": "Online learning with adversarial delays", "author": ["Kent Quanrud", "Daniel Khashabi"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Quanrud and Khashabi.,? \\Q2015\\E", "shortCiteRegEx": "Quanrud and Khashabi.", "year": 2015}, {"title": "Szlak. Multi-player bandits \u2013 a musical chairs approach", "author": ["Jonathan Rosenski", "Ohad Shamir", "Liran"], "venue": "CoRR, abs/1512.02866,", "citeRegEx": "Rosenski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rosenski et al\\.", "year": 2015}, {"title": "Prediction with limited advice and multiarmed bandits with paid observations", "author": ["Yevgeny Seldin", "Peter Bartlett", "Koby Crammer", "Yasin Abbasi-Yadkori"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Seldin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seldin et al\\.", "year": 2014}, {"title": "Dcops and bandits: Exploration and exploitation in decentralised coordination", "author": ["Ruben Stranders", "Long Tran-Thanh", "Francesco M Delle Fave", "Alex Rogers", "Nicholas R Jennings"], "venue": "In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume", "citeRegEx": "Stranders et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stranders et al\\.", "year": 2012}, {"title": "Survey of local algorithms", "author": ["Jukka Suomela"], "venue": "ACM Computing Surveys,", "citeRegEx": "Suomela.,? \\Q2013\\E", "shortCiteRegEx": "Suomela.", "year": 2013}, {"title": "Gossip-based distributed stochastic bandit algorithms", "author": ["Balazs Szorenyi", "R\u00f3bert Busa-Fekete", "Istv\u00e1n Heged\u00fcs", "R\u00f3bert Orm\u00e1ndi", "M\u00e1rk Jelasity", "Bal\u00e1zs K\u00e9gl"], "venue": "In 30th International Conference on Machine Learning (ICML 2013),", "citeRegEx": "Szorenyi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szorenyi et al\\.", "year": 2013}, {"title": "Distributed online learning via cooperative contextual bandits", "author": ["Cem Tekin", "Mihaela van der Schaar"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Tekin and Schaar.,? \\Q2015\\E", "shortCiteRegEx": "Tekin and Schaar.", "year": 2015}, {"title": "Distributed online learning in social recommender systems", "author": ["Cem Tekin", "Simpson Z. Zhang", "Mihaela van der Schaar"], "venue": "J. Sel. Topics Signal Processing,", "citeRegEx": "Tekin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tekin et al\\.", "year": 2014}, {"title": "Daccer: Distributed assessment of the closeness centrality ranking in complex networks", "author": ["Klaus Wehmuth", "Artur Ziviani"], "venue": "Computer Networks,", "citeRegEx": "Wehmuth and Ziviani.,? \\Q2013\\E", "shortCiteRegEx": "Wehmuth and Ziviani.", "year": 2013}, {"title": "On delayed prediction of individual sequences", "author": ["Marcelo J Weinberger", "Erik Ordentlich"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Weinberger and Ordentlich.,? \\Q2002\\E", "shortCiteRegEx": "Weinberger and Ordentlich.", "year": 2002}, {"title": "Slow learners are fast", "author": ["Martin Zinkevich", "John Langford", "Alex J. Smola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Zinkevich et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3).", "startOffset": 101, "endOffset": 120}, {"referenceID": 2, "context": "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent\u2019s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent\u2019s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t\u2212 s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret). In the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like \u221a KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with communication graph G can achieve an average welfare regret of order \u221a( d+ 1 + KN \u03b1\u2264d ) (T lnK). Here \u03b1\u2264d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = \u221a K this bound is at mostK1/4 \u221a T lnK+ \u221a K(lnT ) for any connected graph \u2014see Remark 7 in Section 4.1\u2014 which is asymptotically better than \u221a KT . Networks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay.", "startOffset": 101, "endOffset": 1988}, {"referenceID": 2, "context": "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent\u2019s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent\u2019s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t\u2212 s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret). In the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like \u221a KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with communication graph G can achieve an average welfare regret of order \u221a( d+ 1 + KN \u03b1\u2264d ) (T lnK). Here \u03b1\u2264d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = \u221a K this bound is at mostK1/4 \u221a T lnK+ \u221a K(lnT ) for any connected graph \u2014see Remark 7 in Section 4.1\u2014 which is asymptotically better than \u221a KT . Networks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay. Awerbuch and Kleinberg (2008) prove a bound on the average welfare regret of order \u221a( 1 + KN ) T ignoring polylog factors.", "startOffset": 101, "endOffset": 2109}, {"referenceID": 2, "context": "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent\u2019s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent\u2019s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t\u2212 s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret). In the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like \u221a KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with communication graph G can achieve an average welfare regret of order \u221a( d+ 1 + KN \u03b1\u2264d ) (T lnK). Here \u03b1\u2264d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = \u221a K this bound is at mostK1/4 \u221a T lnK+ \u221a K(lnT ) for any connected graph \u2014see Remark 7 in Section 4.1\u2014 which is asymptotically better than \u221a KT . Networks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay. Awerbuch and Kleinberg (2008) prove a bound on the average welfare regret of order \u221a( 1 + KN ) T ignoring polylog factors.1 We recover the same bound as a special case of our bound when G is a clique and d = 1. In the clique case our bound is also similar to the bound \u221a K N (T lnK) achieved by Seldin et al. (2014) in a single-agent bandit setting where, at each time step, the agent can choose a subset ofN \u2264 K actions and observe their loss.", "startOffset": 101, "endOffset": 2395}, {"referenceID": 2, "context": "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent\u2019s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent\u2019s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t\u2212 s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret). In the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like \u221a KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with communication graph G can achieve an average welfare regret of order \u221a( d+ 1 + KN \u03b1\u2264d ) (T lnK). Here \u03b1\u2264d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = \u221a K this bound is at mostK1/4 \u221a T lnK+ \u221a K(lnT ) for any connected graph \u2014see Remark 7 in Section 4.1\u2014 which is asymptotically better than \u221a KT . Networks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay. Awerbuch and Kleinberg (2008) prove a bound on the average welfare regret of order \u221a( 1 + KN ) T ignoring polylog factors.1 We recover the same bound as a special case of our bound when G is a clique and d = 1. In the clique case our bound is also similar to the bound \u221a K N (T lnK) achieved by Seldin et al. (2014) in a single-agent bandit setting where, at each time step, the agent can choose a subset ofN \u2264 K actions and observe their loss. In the case whenN = 1 (single agent), our analysis can be applied to the nonstochastic bandit problem where the player observes the loss of each played action with a delay of d steps. In this case we improve on the previous result of \u221a (d+ 1)KT by Neu et al. (2010, 2014), and give the first characterization (up to logarithmic factors) of the minimax regret, which is of order \u221a (d+K)T . In principle, the problem of delays in online learning could be tackled by simple reductions. Yet, these reductions give rise to suboptimal results. In the single agent setting, where the delay is constant and equal to d, one can use the technique of Weinberger and Ordentlich (2002) and run d+1 in1.", "startOffset": 101, "endOffset": 3197}, {"referenceID": 11, "context": "Additional works (Joulani et al., 2016; Quanrud and Khashabi, 2015) prove regret bounds for the full-information case of the form \u221a (D + T ) lnK, where D is the total delay experienced over the T rounds.", "startOffset": 17, "endOffset": 67}, {"referenceID": 27, "context": "Additional works (Joulani et al., 2016; Quanrud and Khashabi, 2015) prove regret bounds for the full-information case of the form \u221a (D + T ) lnK, where D is the total delay experienced over the T rounds.", "startOffset": 17, "endOffset": 67}, {"referenceID": 3, "context": "To the best of our knowledge, the first paper about nonstochastic cooperative bandit networks is (Awerbuch and Kleinberg, 2008).", "startOffset": 97, "endOffset": 127}, {"referenceID": 32, "context": "More papers analyze the stochastic setting, and the closest one to our work is perhaps (Szorenyi et al., 2013).", "startOffset": 87, "endOffset": 110}, {"referenceID": 15, "context": "A more recent paper is (Landgren et al., 2015), where the communication network is a fixed graph and a cooperative version of the UCB algorithm is introduced which uses a distributed consensus algorithm to estimate the mean rewards of the arms.", "startOffset": 23, "endOffset": 46}, {"referenceID": 16, "context": "Additional Related Work Many important ideas in delayed online learning, including the observation that the effect of delays can be limited by controlling the amount of change in the agent strategy, were introduced by Mesterharm (2005) \u2014see also (Mesterharm, 2007, Chapter 8).", "startOffset": 218, "endOffset": 236}, {"referenceID": 8, "context": "Furher progress is made by Joulani et al. (2013), who also study delays in the general partial monitoring setting.", "startOffset": 27, "endOffset": 49}, {"referenceID": 7, "context": "In the stochastic case, bandit learning with delayed feedback was considered by Dud\u0131\u0301k et al. (2011); Joulani et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 7, "context": "In the stochastic case, bandit learning with delayed feedback was considered by Dud\u0131\u0301k et al. (2011); Joulani et al. (2013). To the best of our knowledge, the first paper about nonstochastic cooperative bandit networks is (Awerbuch and Kleinberg, 2008).", "startOffset": 80, "endOffset": 124}, {"referenceID": 12, "context": "Another interesting paper about cooperating bandits in a stochastic setting is (Kar et al., 2011).", "startOffset": 79, "endOffset": 97}, {"referenceID": 28, "context": "The resulting bandit problem is one of coordination in a competitive environment, because every time two or more agents select the same action at the same time step they both get a zero reward due to the interference \u2014see (Rosenski et al., 2015) for recent work on stochastic competitive bandits and (Kleinberg et al.", "startOffset": 222, "endOffset": 245}, {"referenceID": 13, "context": ", 2015) for recent work on stochastic competitive bandits and (Kleinberg et al., 2009) for a study of more general congestion games in a game-theoretic setting.", "startOffset": 62, "endOffset": 86}, {"referenceID": 37, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 0, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 16, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 20, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 27, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 18, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 7, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 3, "context": "Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control.", "startOffset": 65, "endOffset": 90}, {"referenceID": 3, "context": "Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control. Tekin and van der Schaar (2015) investigate a stochastic contextual bandit model where each agent can either privately select an action or have another agent select an action on his behalf.", "startOffset": 65, "endOffset": 251}, {"referenceID": 3, "context": "Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control. Tekin and van der Schaar (2015) investigate a stochastic contextual bandit model where each agent can either privately select an action or have another agent select an action on his behalf. In a related paper, Tekin et al. (2014) look at a stochastic bandit model with combinatorial actions in a distributed recommender system setting, and study incentives among agents who can now recommend items taken from other agents\u2019 inventories.", "startOffset": 65, "endOffset": 449}, {"referenceID": 3, "context": "Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control. Tekin and van der Schaar (2015) investigate a stochastic contextual bandit model where each agent can either privately select an action or have another agent select an action on his behalf. In a related paper, Tekin et al. (2014) look at a stochastic bandit model with combinatorial actions in a distributed recommender system setting, and study incentives among agents who can now recommend items taken from other agents\u2019 inventories. Another line of relevant work involves problems of decentralized bandit coordination. For example, Stranders et al. (2012) consider a bandit coordination problem where the the reward function is global and can be represented as a factor graph in which each agent controls a subset of the variables.", "startOffset": 65, "endOffset": 778}, {"referenceID": 5, "context": "Proofs of all the results stated here can be found in (Cesa-Bianchi et al., 2016).", "startOffset": 54, "endOffset": 81}, {"referenceID": 17, "context": "Our model is similar to the LOCAL communication model in distributed computing (Linial, 1992; Suomela, 2013), where the output of a node depends only on the inputs of other nodes in a constantsize neighborhood of it, and the goal is to derive algorithms whose running time is independent of the network size.", "startOffset": 79, "endOffset": 108}, {"referenceID": 31, "context": "Our model is similar to the LOCAL communication model in distributed computing (Linial, 1992; Suomela, 2013), where the output of a node depends only on the inputs of other nodes in a constantsize neighborhood of it, and the goal is to derive algorithms whose running time is independent of the network size.", "startOffset": 79, "endOffset": 108}, {"referenceID": 36, "context": "This is very much reminiscent of a full information scenario, and in fact our bound becomes of order \u221a (dG + 1)T lnK + dG lnT , which is close to the full information minimax rate \u221a (d+ 1)T lnK when feedback has a constant delay d (Weinberger and Ordentlich, 2002).", "startOffset": 231, "endOffset": 264}, {"referenceID": 14, "context": "There has been some recent work on adaptive learning rate tuning applied to nonstochastic bandit algorithms (Koc\u00e1k et al., 2014; Neu, 2015).", "startOffset": 108, "endOffset": 139}, {"referenceID": 23, "context": "There has been some recent work on adaptive learning rate tuning applied to nonstochastic bandit algorithms (Koc\u00e1k et al., 2014; Neu, 2015).", "startOffset": 108, "endOffset": 139}, {"referenceID": 9, "context": ", in (Firby and Haviland, 1997).", "startOffset": 5, "endOffset": 31}, {"referenceID": 2, "context": ", (Auer et al., 2002)\u2014 but in a slightly different manner.", "startOffset": 2, "endOffset": 21}, {"referenceID": 35, "context": ", (Wehmuth and Ziviani, 2013), and references therein.", "startOffset": 2, "endOffset": 29}, {"referenceID": 31, "context": "This problem was studied by Weinberger and Ordentlich (2002) in the full information case, for which they proved that \u221a (d+ 1)T lnK is the optimal order for the minimax regret.", "startOffset": 28, "endOffset": 61}, {"referenceID": 10, "context": "(2010, 2014) \u2014see also Joulani et al. (2013)\u2014 whose techniques can be used to obtain a regret bound of order \u221a (d+ 1)KT .", "startOffset": 23, "endOffset": 45}, {"referenceID": 26, "context": "A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).", "startOffset": 102, "endOffset": 188}, {"referenceID": 6, "context": "A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).", "startOffset": 102, "endOffset": 188}, {"referenceID": 19, "context": "A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).", "startOffset": 102, "endOffset": 188}, {"referenceID": 20, "context": "A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).", "startOffset": 102, "endOffset": 188}, {"referenceID": 11, "context": "Even for the single-agent setting, we do not know whether regret bounds of the form \u221a (D + T ) lnK, where D is the total delay experienced over the T rounds, could be proven \u2014see (Joulani et al., 2016; Quanrud and Khashabi, 2015) for similar results in the fullinformation setting.", "startOffset": 179, "endOffset": 229}, {"referenceID": 27, "context": "Even for the single-agent setting, we do not know whether regret bounds of the form \u221a (D + T ) lnK, where D is the total delay experienced over the T rounds, could be proven \u2014see (Joulani et al., 2016; Quanrud and Khashabi, 2015) for similar results in the fullinformation setting.", "startOffset": 179, "endOffset": 229}, {"referenceID": 2, "context": "The proof hinges on combining the known lower bound \u03a9 (\u221a KT ) for bandits without delay of Auer et al. (2002) with the following argument by Weinberger and Ordentlich (2002) that provides a lower bound for the full information case with delay.", "startOffset": 91, "endOffset": 110}, {"referenceID": 2, "context": "The proof hinges on combining the known lower bound \u03a9 (\u221a KT ) for bandits without delay of Auer et al. (2002) with the following argument by Weinberger and Ordentlich (2002) that provides a lower bound for the full information case with delay.", "startOffset": 91, "endOffset": 174}], "year": 2016, "abstractText": "We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than d hops to arrive, where d is a delay parameter. We introduce EXP3-COOP, a cooperative version of the EXP3 algorithm and prove that with K actions and N agents the average per-agent regret after T rounds is at most of order \u221a( d+ 1 + KN \u03b1\u2264d ) (T lnK), where \u03b1\u2264d is the independence number of the d-th power of the communication graphG. We then show that for any connected graph, for d = \u221a K the regret bound is K \u221a T , strictly better than the minimax regret \u221a KT for noncooperating agents. More informed choices of d lead to bounds which are arbitrarily close to the full information minimax regret \u221a T lnK when G is dense. When G has sparse components, we show that a variant of EXP3-COOP, allowing agents to choose their parameters according to their centrality in G, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay.", "creator": "LaTeX with hyperref package"}}}