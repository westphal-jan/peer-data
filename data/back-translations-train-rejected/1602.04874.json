{"id": "1602.04874", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation", "abstract": "Recurrent neural network(RNN) has been broadly applied to natural language processing(NLP) problems. This kind of neural network is designed for modeling sequential data and has been testified to be quite efficient in sequential tagging tasks. In this paper, we propose to use bi-directional RNN with long short-term memory(LSTM) units for Chinese word segmentation, which is a crucial preprocess task for modeling Chinese sentences and articles. Classical methods focus on designing and combining hand-craft features from context, whereas bi-directional LSTM network(BLSTM) does not need any prior knowledge or pre-designing, and it is expert in keeping the contextual information in both directions. Experiment result shows that our approach gets state-of-the-art performance in word segmentation on both traditional Chinese datasets and simplified Chinese datasets.", "histories": [["v1", "Tue, 16 Feb 2016 00:45:19 GMT  (194kb,D)", "http://arxiv.org/abs/1602.04874v1", "2 figures"]], "COMMENTS": "2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["yushi yao", "zheng huang"], "accepted": false, "id": "1602.04874"}, "pdf": {"name": "1602.04874.pdf", "metadata": {"source": "CRF", "title": "Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation", "authors": ["Yushi Yao", "Zheng Huang"], "emails": ["yys12345@sjtu.edu.cn", "huangzheng@sjtu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "With the rapid development of deep learning, neural networks are beginning to show their great ability in NLP tasks [Auli et al., 2013] and recent research has shown that recurring neural networks (RNN) significantly outperform popular statistical algorithms such as Hidden Markov Model (HMM). [Zhang el al., 2003], CRF (Conditional Random Field) and neural probability models (Bengio et al., 2003] Like a special type of neural networks [Hochreiter et al., 1997], it is efficient to model sequential data such as language and text. [Sundermeyer et al., 2015]."}, {"heading": "2 BLSTM network architecture", "text": "BLSTM neural network is structurally similar to the LSTM network in that both are built with LSTM units [Schuster et al., 1997]. The special unit of this network is able to learn long-term dependencies without retaining redundant context information. They work tremendously well with sequential modeling problems and are now widely used in NLP tasks."}, {"heading": "2.1 LSTM unit", "text": "The basic structure of the LSTM storage unit consists of three essential gates and a cell status.As shown in Figure 1, the memory cell contains the information it memorized in due course, the state of the memory cell is connected to three gates, the input vector of each gate consists of input part and recurring part. The input gate determines what new information is stored in the cell state, the output gate decides which part of the cell state is output, and the recurring part is updated according to the current cell state and fed into the next iteration [Hochreiter et al., 1997]. The formal formulas for updating each gate and the cell state are defined as follows: zt = g (Wzx t + Rzy t \u2212 1 + bz) (1) es = circuit (Wix t + Riy t \u2212 1 + pi ct \u2212 1 + bt) (2)."}, {"heading": "2.2 BLSTM Network", "text": "In contrast to the LSTM network, the BLSTM network has two parallel layers that propagate in two directions, with the forward and backward trajectories of each layer executed in a similar manner to normal neural networks, and these two layers store the information of the sentences from both directions. [Schuster et al., 1997] Since there are two LSTM layers in our network, the vector formula should also be adapted. hf t = H (Wxhfxt + Whfhfhf t \u2212 1 + bhf) (7) hbt = H (Wxhbxt + Whbhbt \u2212 1 + bhb) (8) hf \"Rd\" and hb \"Rd\" denote the output vector of the forward and backward layers respectively, which differs from previous research results. The final output in our work yt = [hft, hbt] is the concatenation of these two parts, which means that we define the forward and backward layers as individual layers respectively."}, {"heading": "3 Training Method", "text": "To turn the segmentation problem into a tagging problem, we assign a label to each character to indicate the segmentation. There are four types of designations: B, M, E, S, which correspond to the beginning, middle, end of a word or word with a single character."}, {"heading": "3.1 Training framework", "text": "The basic approach to language modeling in our study is shown in Figure 2.Each character has an ID defined in a reference book, the dictionary is constructed by collecting unique characters in the training set. Instead of a uniform representation, the characters are projected into a d-dimensional space and initialized as dense vectors v-Rd, we consider this initialization step to be a construction of embedding for characters. Each embedding is stored in a matrix M-Rd \u00d7 | C | and can be retrieved by their character ID. Since embedding is efficient in describing features at word level [Miklov et al., 2013], we hope that character level embedding can also achieve good performance in CWS. Then they are embedded in the BLSTM network and the final output of the BLSTM network is finally passed to a hidden layer and the Softmax layer determines the maximum day probability of the character."}, {"heading": "3.2 Model variant", "text": "In order to further improve the structure of the BLSTM network, we stack BLSTM layers based on the method of constructing RNN [Pascanu et al., 2013]. We expect to extract contextual characteristics at a higher level. However, the output of a BLSTM layer is twice as large as the input vector, since it consists of two LSTM layers, and its dimension will expand dramatically as the network goes deeper, here we use a transformation matrix to compress the dimension of the output vectors and keep it level with input vectors.vtran = Wtran \u00d7 vo (9) Suppose that the output vector of the BLSTM layer vo-R2d, the transformation matrix Wtran-Rd-2d converts the vectors to a lower dimension, thus keeping the output of each BLSTM layer the same dimension. As our STM network becomes increasingly complicated, we avoid using the number of parameters as we train and rapidly overlay it."}, {"heading": "4 Experiments", "text": "The data set we used to evaluate our model of word segmentation is from 2005, which contains benchmark data sets for both Simplified Chinese (PKU and MSRA) and Traditional Chinese (AS and HK). All models are trained in the field of GPU memory. In this section, we will also change the procedure of our experiments and the way we obtain the model with the best performance in the field of networking."}, {"heading": "5 Conclusions", "text": "In this paper, we propose to use bidirectional neural LSTM networks to train the model for Chinese word segmentation, BLSTM networks are very efficient for sequential tagging tasks. The model automatically learns to extract discriminatory traits at the character level, and it does not require manual traits for segmentation or prior knowledge. Experiments with SIGHAN Backoff 2005 datasets show that our model performs well and generalizes in both Simplified Chinese and Traditional Chinese. Our results suggest that deep neural networks work well in segmentation tasks, and that word-embedded BLSTM networks are an effective tagging solution and worth further research."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In EMNLP,", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Chinese word segmentation: A decade review", "author": ["Changning Huang", "Hai Zhao"], "venue": "Journal of Chinese Information Processing,", "citeRegEx": "Huang and Zhao.,? \\Q2007\\E", "shortCiteRegEx": "Huang and Zhao.", "year": 2007}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1312.6026,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "From feedforward to recurrent lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Hermann Ney", "Ralf Schluter"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2015}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In INTERSPEECH,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "A unified tagging solution: Bidirectional lstm recurrent neural network with word embedding", "author": ["Peilu Wang", "Yao Qian", "Frank K Soong", "Lei He", "Hai Zhao"], "venue": "arXiv preprint arXiv:1511.00215,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Exploring representations from unlabeled data with co-training for chinese word segmentation", "author": ["Longkai Zhang", "Wang Houfeng", "Sun Xu", "Mairgup Mansur"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "An improved chinese word segmentation system with conditional random field", "author": ["Hai Zhao", "Chang-Ning Huang", "Mu Li"], "venue": "In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,", "citeRegEx": "Zhao et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "With the rapid development of deep learning, neural networks start to show its great capability in NLP tasks[Auli et al., 2013] and recent research revealed that recurrent neural networks(RNN) significantly outperforms popular statistical algorithms like Hidden Markov Model(HMM)[Zhang el al.", "startOffset": 108, "endOffset": 127}, {"referenceID": 1, "context": ", 2004] and neural probabilistic models[Bengio et al., 2003].", "startOffset": 39, "endOffset": 60}, {"referenceID": 12, "context": ", 1997] is verified to be efficient in modeling sequential data like speech and text [Sundermeyer et al., 2015].", "startOffset": 85, "endOffset": 111}, {"referenceID": 13, "context": "analyzed LSTM neural network by modeling English and French[Sundermeyer et al., 2012].", "startOffset": 59, "endOffset": 85}, {"referenceID": 14, "context": "resentations are learnt from unlabeled text for all tasks[Wang et al., 2015].", "startOffset": 57, "endOffset": 76}, {"referenceID": 6, "context": "combined LSTM with CRF and verified the efficiency and robustness of their model in sequential tagging[Huang et al., 2015].", "startOffset": 102, "endOffset": 122}, {"referenceID": 2, "context": "is close to ours is Chen et al, which introduced LSTM neural network into Chinese word segmentation[Chen et al., 2015], while LSTM can just memorize the past contextual information from the context.", "startOffset": 99, "endOffset": 118}, {"referenceID": 8, "context": "In order to further improve the structure of BLSTM network, we stack BLSTM layers based on the method of constructing RNN[Pascanu et al., 2013].", "startOffset": 121, "endOffset": 143}, {"referenceID": 16, "context": "Models PKU MSRA CityU (Zhao et al., 2006) - - 97.", "startOffset": 22, "endOffset": 41}, {"referenceID": 15, "context": "2 (Zhang et al., 2013) 96.", "startOffset": 2, "endOffset": 22}, {"referenceID": 2, "context": "4 (Chen et al., 2015) 96.", "startOffset": 2, "endOffset": 21}, {"referenceID": 16, "context": "(Zhao et al., 2006) is a CRF model with rich feature template, (Sun and Xu, 2011) improved supervised word segmentation by exploiting features of unlabeled data and the system of (Zhang et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": ", 2006) is a CRF model with rich feature template, (Sun and Xu, 2011) improved supervised word segmentation by exploiting features of unlabeled data and the system of (Zhang et al., 2013) applied semi-supervised approach to extract representations of label distributions from unlabeled and labeled datasets[Zhang et al.", "startOffset": 167, "endOffset": 187}, {"referenceID": 15, "context": ", 2013) applied semi-supervised approach to extract representations of label distributions from unlabeled and labeled datasets[Zhang et al., 2013].", "startOffset": 126, "endOffset": 146}], "year": 2016, "abstractText": "Recurrent neural network(RNN) has been broadly applied to natural language processing(NLP) problems. This kind of neural network is designed for modeling sequential data and has been testified to be quite efficient in sequential tagging tasks. In this paper, we propose to use bi-directional RNN with long short-term memory(LSTM) units for Chinese word segmentation, which is a crucial preprocess task for modeling Chinese sentences and articles. Classical methods focus on designing and combining hand-craft features from context, whereas bi-directional LSTM network(BLSTM) does not need any prior knowledge or pre-designing, and it is expert in keeping the contextual information in both directions. Experiment result shows that our approach gets stateof-the-art performance in word segmentation on both traditional Chinese datasets and simplified Chinese datasets.", "creator": "TeX"}}}