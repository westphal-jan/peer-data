{"id": "1602.04889", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "Unsupervised Domain Adaptation Using Approximate Label Matching", "abstract": "Domain adaptation, and transfer learning more generally, seeks to remedy the problem created when training and testing datasets are generated by different distributions. In this work, we introduce a new unsupervised domain adaptation algorithm for when there are multiple sources available to a learner. Our technique assigns a rough labeling on the target samples, then uses it to learn a transformation that aligns the two datasets before final classification. In this article we give a convenient implementation of our method, show several experiments using it, and compare it to other methods commonly used in the field.", "histories": [["v1", "Tue, 16 Feb 2016 02:38:25 GMT  (263kb,D)", "http://arxiv.org/abs/1602.04889v1", null], ["v2", "Wed, 17 Feb 2016 05:25:20 GMT  (518kb,D)", "http://arxiv.org/abs/1602.04889v2", null], ["v3", "Wed, 1 Mar 2017 19:17:35 GMT  (6995kb,D)", "http://arxiv.org/abs/1602.04889v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jordan t ash", "robert e schapire", "barbara e engelhardt"], "accepted": false, "id": "1602.04889"}, "pdf": {"name": "1602.04889.pdf", "metadata": {"source": "CRF", "title": "Multi-Source Domain Adaptation Using Approximate Label Matching", "authors": ["Jordan T. Ash", "Robert E. Schapire"], "emails": ["JORDANTASH@GMAIL.COM", "SCAHPIRE@MICROSOFT.EDU"], "sections": [{"heading": "1. Introduction", "text": "It is indeed the case that we are able to go in search of a solution."}, {"heading": "1.1. Related Work", "text": "As described in (Mohri, 2015), domain matching has historically been approached essentially in two ways. Firstly, the training samples are reweighted to make the resulting hypothesis more suitable for classification in the test set. Kernel Mean Matching (KMM) is a well-known domain matching technique that falls into this category (Gretton et al., 2009).This algorithm reweights source and / or target data sets to bring the source and target data sets as close as possible to each other in a reproducing Hilbert core space. An advantage of KMM is that it bypasses the need for an approximation of any distributions. Alternatively, source and / or target data sets are transformed into another space where they fit better to each other. Transfer Component Analysis (TCA) works this way (Pan et al., 2011) and attempts to find a \"transfer component\" when they appear to have new source data distributions in the same way."}, {"heading": "2. Our Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Problem Setup", "text": "We assume that different source data sets are generated within each source example (xsi, j, y, j) depending on the distribution."}, {"heading": "2.2. Implementation", "text": "These ideas can be implemented with the help of a neural network. Specifically, we turn each fi into a neural network of arbitrary architecture and then use it to learn \u03c6i. Reminder: by the time we solve the tasks for \u03c6i, fi and g are already formed and repaired. To find \u03c6i for source i, we simply take the already trained fi-neural network and add one or more additional layers to its existing input layer. These additional layers are connected to fi by linear neurons and represent \u03c6i. In the formation of this aggregated network, we use g (xtj) as a label on x t j for retransmission, and we do not update any of the weights in the layers corresponding to fi. This implementation is convenient because it allows us to easily optimize (2) even if Khali is nested in fives. It is worth noting that the same basic method can also be applied when we are interested in transforming a particular class."}, {"heading": "2.3. An Illustration", "text": "In this figure, we create a sine wave as the correct dichotomizer between positives and negatives. We then create five sources centered at different random locations along the sine wave. Each source consists of a thousand points that are called positive when they fall over the sine wave and negative when they fall. Figure 3 shows an example of this scenario.For this figure, g is logistic regression and each fi is a neural network with a single hidden layer of three hidden nodes. A randomly selected domain is presented as a target, and we try to assign designations to it by transferring knowledge from the four remaining marked domains. When looking at Figure 3, it becomes clear that g, trained on four sources at a time, will be able to classify each heldout domain relatively well (since the correct decision boundary is sinusoidal and curved)."}, {"heading": "3. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Experimental Setting", "text": "This section describes three separate experiments performed with our technique. For each experiment, we add an additional layer to reach fi, if we solve the solution for \u03c6, and limit ourselves to a linear transformation. In each subsection, we report different error rates for each available domain, by specifying a fixed domain as the target and then using each other domain as the described source. To compare, we also include error rates for the two natural hypotheses described above, g (xtj) and return a canonical characteristic representation. Once a neural network with the same architecture as g and each fi is used in this space for classification, the TCA (Pan et al., 2011) algorithm takes one source and one target as inputs and returns a canonical representation. As soon as a neural network with the same architecture as g and each fi is used in this space for classification, it is used for classification."}, {"heading": "3.2. MEG data", "text": "The MEG dataset was provided by Kaggle, a data science competition website, in a challenge sponsored by several companies, including Elekta Oy, MEG International Services, Fondazione and Besa. Characteristics of this data are derived by estimating the covariance matrix of each subject (Barachant & Congedo, 2014) and then calculating a vector by (Barachant et al., 2013), which, when projected in two dimensions, is what is shown in Figure 1. As already mentioned, labels correspond to the question of whether or not the subjects were looking at a face at the time the MEG data was corrected. For this challenge, g and fi are both neural networks with a single hidden layer consisting of ten nodes. In this dataset, 16 different subjects are available, each containing an average of 588 samples, and errors corresponding to the experiments being performed, each of which is shown in Table 1.4."}, {"heading": "3.3. Sentiment Classification", "text": "In the sentimental dataset, we get product ratings from Amazon.com (Blitzer et al., 2007). Each review comes from either Amazon's book, DVD, electronics, or kitchen departments, and each of these areas contains 2,000 samples. Reviews are delivered as term frequencies, which we converted into a TF-IDF representation. Subsequently, we performed Principal Components Analysis (PCA) for the entire dataset, capturing 95% of its total variance and dramatically reducing its dimensionality. In this experiment, g and fi exhibit the same architecture as in the MEG experiment, but with one hundred hidden nodes instead of ten. As in the previous experiment, Table 2 shows a breakdown of percentage errors. Our approach achieves lower classification errors than any other methods used."}, {"heading": "3.4. Remote Sensing", "text": "These are nine-dimensional representations of radar signals. These data cover 29 different areas, each of which is associated with different geographical conditions, and the labels indicate whether or not a particular sample corresponds to the existence of a landmine. Each domain averages about 511 samples, and in contrast to the two previous experiments in which the classes were balanced, there are about 15 negative samples for each positive sample. More information on this data can be found in (Xue et al., 2007).In this experiment, g and fi both have a single hidden layer of five neurons. Detailed results can be found in 3. In this experiment, our method performs slightly worse than TCA on average."}, {"heading": "4. Discussion", "text": "While our method seems to work well in each of the experiments, it performs well in comparison to other approaches, especially in the tasks of MEG and sentiment classification. In the MEG experiment, our approach outperforms other classification algorithms in 12 of the 16 domains. In addition, it performs significantly better on average than both g and f. In the problem of sentiment classification, our approach produces hypotheses that are superior to other approaches in all domains. The method is less successful in landmine data sets. This is probably because KMM and TCA only consider samples from the source and the target, rather than using both samples and labels. While this seems to be a disadvantage in the other two experiments, where the classes are well balanced, it seems to be an advantage in this case because there are far more negative than positive samples."}], "references": [{"title": "A plug & play P300 BCI using information geometry", "author": ["A. Barachant", "M. Congedo"], "venue": "arXiv preprint arXiv:1409.0107,", "citeRegEx": "Barachant and Congedo,? \\Q2014\\E", "shortCiteRegEx": "Barachant and Congedo", "year": 2014}, {"title": "Classification of covariance matrices using a Riemannian-based kernel for BCI applications", "author": ["A. Barachant", "S. Bonnet", "M. Congedo", "C. Jutten"], "venue": null, "citeRegEx": "Barachant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Barachant et al\\.", "year": 2013}, {"title": "Instance weighting for domain adaptation in NLP", "author": ["J. Jiang", "C. Zhai"], "venue": "In ACL,", "citeRegEx": "Jiang and Zhai,? \\Q2007\\E", "shortCiteRegEx": "Jiang and Zhai", "year": 2007}, {"title": "Attitude determination using vector observations and the singular value decomposition", "author": ["F.L. Markley"], "venue": "The Journal of the Astronautical Sciences,", "citeRegEx": "Markley,? \\Q1988\\E", "shortCiteRegEx": "Markley", "year": 1988}, {"title": "Adaptation based on generalized discrepancy", "author": ["M. Mohri"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mohri,? \\Q2015\\E", "shortCiteRegEx": "Mohri", "year": 2015}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Pan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2011}, {"title": "Visual domain adaptation: A survey of recent advances", "author": ["V.M. Patel", "R. Gopalan", "R. Li", "R. Chellappa"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Patel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Patel et al\\.", "year": 2015}, {"title": "Multitask learning for classification with Dirichlet process priors", "author": ["Y. Xue", "X. Liao", "L. Carin", "B. Krishnapuram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Xue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 6, "context": ", 2011) and computer vision (Beijbom, 2012; Patel et al., 2015).", "startOffset": 28, "endOffset": 63}, {"referenceID": 4, "context": "As described in (Mohri, 2015), domain adaptation has historically been approached in mainly two ways.", "startOffset": 16, "endOffset": 29}, {"referenceID": 5, "context": "Transfer Component Analysis (TCA) operates in this way (Pan et al., 2011), attempting to find a set of \u201ctransfer components\u201d such that source and target datasets appear to have the same underlying distribution when projected into this new space.", "startOffset": 55, "endOffset": 73}, {"referenceID": 3, "context": "For example, to constrain \u03c6i to the space of rotation matrices, we have been able to leverage a solution to Wahba\u2019s problem (Markley, 1988) at every iteration of back propagation.", "startOffset": 124, "endOffset": 139}, {"referenceID": 5, "context": "The TCA (Pan et al., 2011) algorithm takes a source and target as inputs and returns a canonical feature representation.", "startOffset": 8, "endOffset": 26}, {"referenceID": 1, "context": "The features of this data are derived by estimating each subject\u2019s covariance matrix (Barachant & Congedo, 2014) and then computing a vector according to (Barachant et al., 2013).", "startOffset": 154, "endOffset": 178}, {"referenceID": 7, "context": "More information about this data can be found in (Xue et al., 2007).", "startOffset": 49, "endOffset": 67}], "year": 2017, "abstractText": "Domain adaptation, and transfer learning more generally, seeks to remedy the problem created when training and testing datasets are generated by different distributions. In this work, we introduce a new unsupervised domain adaptation algorithm for when there are multiple sources available to a learner. Our technique assigns a rough labeling on the target samples, then uses it to learn a transformation that aligns the two datasets before final classification. In this article we give a convenient implementation of our method, show several experiments using it, and compare it to other methods commonly used in the field.", "creator": "LaTeX with hyperref package"}}}