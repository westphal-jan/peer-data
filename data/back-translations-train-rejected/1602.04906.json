{"id": "1602.04906", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "Segmentation Rectification for Video Cutout via One-Class Structured Learning", "abstract": "Recent works on interactive video object cutout mainly focus on designing dynamic foreground-background (FB) classifiers for segmentation propagation. However, the research on optimally removing errors from the FB classification is sparse, and the errors often accumulate rapidly, causing significant errors in the propagated frames. In this work, we take the initial steps to addressing this problem, and we call this new task \\emph{segmentation rectification}. Our key observation is that the possibly asymmetrically distributed false positive and false negative errors were handled equally in the conventional methods. We, alternatively, propose to optimally remove these two types of errors. To this effect, we propose a novel bilayer Markov Random Field (MRF) model for this new task. We also adopt the well-established structured learning framework to learn the optimal model from data. Additionally, we propose a novel one-class structured SVM (OSSVM) which greatly speeds up the structured learning process. Our method naturally extends to RGB-D videos as well. Comprehensive experiments on both RGB and RGB-D data demonstrate that our simple and effective method significantly outperforms the segmentation propagation methods adopted in the state-of-the-art video cutout systems, and the results also suggest the potential usefulness of our method in image cutout system.", "histories": [["v1", "Tue, 16 Feb 2016 04:31:20 GMT  (3520kb,D)", "http://arxiv.org/abs/1602.04906v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.GR cs.LG", "authors": ["junyan wang", "sai-kit yeung", "jue wang", "kun zhou"], "accepted": false, "id": "1602.04906"}, "pdf": {"name": "1602.04906.pdf", "metadata": {"source": "CRF", "title": "Segmentation Rectification for Video Cutout via One-Class Structured Learning", "authors": ["Junyan Wang", "Sai-Kit Yeung", "Jue Wang", "Kun Zhou"], "emails": ["ejywang@ucla.edu", "saikit@sutd.edu.sg", "juewang@ieee.org", "kunzhou@acm.org."], "sections": [{"heading": null, "text": "Fig. 0: With user-provided keyframe segmentation (left), our approach automatically generates precise object segments in subsequent frames (center) that can be used for novel compositing (right). Index terms - video segment, segmentation correction, a class structured SVM, object segmentationF"}, {"heading": "1 INTRODUCTION", "text": "V IDEO snippet as one of the most successful applications of computer vision for video editing and compositing has received much attention from the computer graphics community [1], [2], [3], [4], [5], [6]. Although practical useful systems have been developed, some fundamental issues remain unaddressed. In this essay, we treat video snippet under a newly identified fundamental aspect. \u2022 Junyan Wang is affiliated with the Doheny Eye Institute at the University of California, Los Angeles, CA 90033, USA E-mail: ejywang @ ucla.edu \u2022 Sai-Kit Yeung is affiliated with the Pillar of Information Technology and Design, Singapore University of Technology and Design, Singapore, 487372. E-mail: e-mail: saikit @ sutd.edu.sg \u2022 Jue Wang is affiliated with Adobe Research, Seattle, WA 98103, USA. E-mail: juewang @ ieangus.org \u2022 Kun Zhun is affiliated with the Chou-Zhejiang University of China @ Zheji.org."}, {"heading": "1.1 Related works", "text": "The latest video object cropping systems [5], [7], [8], [9] include three important steps: (1) Keyframe segmentation; performing keyframe image segmentation and refinement; (2) Foreground background classification; performing classifications on other frames in light of keyframe segmentation; (3) Segmentation refinement; converting classification results into final crop results on non-keyframes. Steps 2 and 3 are usually applied iteratively to subsequent frames until the user creates a new keyframe due to the occurrence of visible errors. Keyframe segmentation can be performed using interactive single-frame segmentation techniques. In foreground background classification, using motion estimations, foreground background classifications, foreground background classified foreground background classifications, up to foreground classifications based on a new user's keyframe, are created."}, {"heading": "1.2 Contributions", "text": "In this paper, we deal with the possibly asymmetrically distributed FP and FN errors in the output of the foreground classifier, which we call segmentation correction. Initially, the significance of this partial problem in the context of the video clip is identified to the best of our knowledge and belief. Our contribution is twofold. First, we propose a novel two-layer MRF model, in which the data term can treat the false positive and false negative errors of any classifier with separate weights differently. Second, we propose a novel one-class model of structured support vectors (OSSVM) to learn the weights, as a mathematically more favorable alternative to the conventional two-class structured SVM frameworks [17], [18]. Furthermore, we establish the conditional equivalence between the OSSVM and the conventional (2CSSVM) model. This theoretical justification of OSSVM is also restructured learning."}, {"heading": "1.3 Organization", "text": "The rest of the work is structured as follows: Section 2 derives our two-layer MRF model, Section 3 describes the training process, and Section 4 discusses the practical aspects of video editing. Section 5 describes our experiment. Finally, we conclude our work and discuss the difference between video editing and other video segmentation tasks in Section 6."}, {"heading": "2 MODELING SEGMENTATION RECTIFICATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Segmentation refinement in video cutout", "text": "In conventional video cutout systems [5], [6] the classification output is refined by using the MRF-based segmentation model. MRF model can be described as follows: min f-p-P Up (fp) + \u2211 {p, q-N Vpq (fp, fq), (1) where p refers to a pixel, P is the set of all pixels, fp is the pixel label and N is a neighbourhood system. Up and Vpq are the conventional simple and picturesque terms. The non-standard term can be used to represent the hard constraint given by the user, such as the seed of the foreground and background regions, or it can be a regional model or a form in front of it. The picturesque potential is often used to model the object boundaries, and it has also been used to represent advanced priors in the segmentation [19].The unary term for the inclusion of the foreground = hp = hp-can be fixed."}, {"heading": "2.2 A case study on the classification error", "text": "At this point, we are conducting a quantitative study of the classification errors caused by the state-of-the-art foreground classifier for video clips [6]. In this study, we are performing segmentation propagation using the classifier from Zhong et al. [6] for all consecutive pairs of images in their training dataset, and calculating the false positive ratio (FPR) and false negative ratio (FNR) for each target image. The quantitative results are shown in Figure 4: The FPR is about 11.22 times larger than the FNR, implying that the two terms in Equation (2) should not be considered equally important in the model. This case study of universal fidelity refutes Figure 4: FPR vs. FNR from Zhong et al.'s FB classifier [6] of the uneven term based on symmetrical distance in the conventional MRF model."}, {"heading": "2.3 A generic shape distance function", "text": "Our segmentation rectification method is inspired by the form-based MRF segmentation model q [20], [21] in which the shape distance in the segment model is used to handle occlusion and background clutter. In this thesis, we consider the data distance as the shape distance and show why and how we could reformulate this shape distance for segment rectification. To handle the possibly asymmetrically distributed FP and FN errors, we propose the following generic shape distance function to model the relationship between the classification output and true segmentation: Sw (f, h) = p, q} Nfhwwwoutsidepq + w within pq fphq, (5) where Nfh represents the neighborhood system across f and h."}, {"heading": "2.4 The MRF model for segmentation rectification", "text": "We can translate our generic form determination function in Eq. (5) into conventional MRF = q = q = q = q = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "3 LEARNING THE OPTIMAL MODEL FOR SEGMENTATION RECTIFICATION WITH ONE-CLASS STRUCTURED SVM", "text": "A popular method for this task is structured learning [17], [18], [23], [24]. The basic idea is to treat the MRF parameter learning problem as a classification problem, where good segmentation forms one class and bad segmentation forms the other class. This idea is illustrated in Figure 7. To eliminate the need for negative samples, we propose to apply the single-class method SVM to the structured learning problem instead of the traditional two-class formulation [17], [18], [23], [24] which can be time consuming. To eliminate the need for negative samples, we propose to apply the single-class method SVM to the structured learning problem instead of the conventional two-class formulation [17]. The single-class method SVM only requires samples from one class, e.g. positive class, for training [25], [26] [27]."}, {"heading": "3.1 The two-class structured SVM", "text": "Before introducing our model, we briefly describe the conventional two-class structure SSVM (2CSSVM). We start with the generic compact form of MRF. The MRF energy for an image can be written as an internal product form in which the weights w: E (f | w, x) = w \u00b7 dung (x, f) (13), where w is defined in equation. (11) and x = \u2212 ht, wepp \u2032}, f = f = t, in order to be in line with equation. (11) Note that this general notation adopted in this subsection allows us to slightly extend our approach to other MRF models. We expect the MRF energy for the basic truth f \u00b2 to be as small as possible and we expect it to be used for others not ideal f. This principle can be used for learning the weight vector w from the data [17], [18] and it can be expressed as:."}, {"heading": "3.2 One-class structured SVM", "text": "The question that arises is whether it is even possible to differ from the original SSVM formulations. We call this model a single-class structured support engine (OSSVM) because there is no answer to fk. This optimization problem is known as a class support vector, and it has been thoroughly investigated. [25] Here, we continue to establish the rationality of OSSVM in the context of structured learning. We observe that the OSSVM, which requires only a basic truth mask, is equivalent to the conventional two-class SSVM model."}, {"heading": "3.3 Learning algorithms", "text": "Both the SSVM and the OSSVM can be used to learn the weights in our model. We adopt the cutting plane algorithm for the two-class method SSVM [24]. We include the pseudo code for this method in algorithm 1 for self-constraint. The pseudo code for our OSSVM learning method is presented in algorithm 2. After obtaining the weights w *, we can use it in the rectification model presented in Equation (6) or Equation (11)."}, {"heading": "4 PRACTICAL CONCERNS IN IMPLEMENTATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 The shrinking bias of graph cut", "text": "It is known that the graph section for the MRF model with 2ndorder pair-by-pair potential suffers from shrinking bias [19], [29]. The recently proposed local foreground-background algorithm 2: OSSVM Learning Input: Same as in algorithm 1 Output: Same as in algorithm 1 1 1 forall the images do 2 * * * k * (f * k | xk, hk);\\\\ By Eq. (12) 3 w * 2 * min w, ~ \u03b51 2 * 2 + CN (\u2211 N = 1 \u03b5k) \u2212 wedges.t.: \u0451k, w \u00b7 VP \u00b2 k \u2264 \u2212 1 + \u03b5k, \u2211 i wi = 1, w \u2265 0;\\\\ According to Equation (18) classifiers [5] [6] are shown to be able to correct local errors close to the object boundary. The shrinking bias falls into this category because it introduces small errors close to the boundary."}, {"heading": "4.2 Computational complexity", "text": "There are a number of efficient algorithms for solving graph sections, i.e. the max flow / min cut problem. The computing time for the Boykov-Kolmogorov (BK) algorithm on a 2 MP image on the CPU is about 160 ms, and the GPU implementation of graph sections can be 2 times faster than on the CPU [30]. The foreground / background classifier we use is the one reported in [6], and its average computing time is about 1.5 s for an image on a 3.3 GHz quad-core CPU PC. Optical flows and edge cards can all be pre-calculated."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "In the experiments, we compare our method with state-of-the-art methods for full-sequence segmentation with the initial keyframe segmentation. We avoid end-to-end system comparison, as the interactive segmentation phase, i.e. step 1, is not the focus of this work in the video cropping system. We are unable to present all the results that are worth showing due to the page limitation. We include them in the supplementary material."}, {"heading": "5.1 Experimental Setup", "text": "This year, we are talking about just under a million euros."}, {"heading": "5.2 Rectifying global classifier", "text": "First, we apply our framework to the Global Gaussian Mixture Model (GMM) for segmentation correction. In general, a global classifier such as GMM is not suitable for video clipping [5], we conduct the experiment to validate the generality of our framework. In segmentation propagation, we train a GMM classifier using soil truth segmentation on frame t and apply it to frame t + 1 as propagated segmentation, and then apply our correction approach with different weights, e.g. with or without prior knowledge, for further refinement. Average boundary deviation is summarized in Figure 9. Some typical results are shown in Figure 10. Results suggest that our correction approach with learned weights can significantly improve the segmentation results generated by the GMM classification."}, {"heading": "5.3 Rectifying local classifier", "text": "We also demonstrate the effectiveness of our approach to correcting the state-of-the-art local classifier proposed in [6]. We perform the complete sequence propagation on the test set and Figure 15 shows the quantitative results. Here, too, we see the segmentation errors accumulate faster than ours due to matting and diagram sections with uniform weight. After 10 images of propagation, matting and diagram section generate 4 times more errors than our method. We also present the visual results for a \"car\" sequence in the test set in Figure 12 and a \"bear\" sequence from the training set in Figure 11. We also compare our method with Video SnapCut (Rotobrush) [5] and [6] on additional video sequences. Some of the typical results are in Figures 13 and 14. We can observe that the Rotobrush may suffer from the temporal discontinuity, while the segmentation error could easily manifest in other methods based on Zhong et al's classifier."}, {"heading": "5.4 Experiment on RGB-D videos", "text": "Since our OSSVM framework can be easily extended to the processing of the additional depth dimension, we can also apply our method to the RGB-D data for valuation. Segmentation of the RGB-D data is the same as the RGB-D-data. (11), except that the potentials on the RGB-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D"}, {"heading": "5.5 Cross validation for prior weight selection", "text": "There is an optional edge in front of our main formulation of OSSVM. The default value, which is the coefficient of \u2212 Wedge, is 1. From our experiment, we determine that this is primarily critical for performance. To select the optimal weights, we perform 10-fold cross-validation with 7 possible previous weights on a uniform grid {0, 0.5, 1, 1.5, 2, 3} for both the RGB and RGB-D datasets used in our experiments. The best previous one is to give the overall smallest error in all 10-fold cross-validations. 11TABLE 3: Average Limit Deviation (ABD) for the 10-frame sequencing Zhong et al + Matting + Uniform GC + OCSSVM + before the 10-fold cross-validation."}, {"heading": "5.6 Limitations", "text": "Although experiments show that our correction approach can effectively improve the performance of existing video cropping systems, it can fail in difficult cases. A typical error is imperfect edge extraction. Modern edge detection techniques are often reliable, but their results can still contain errors. Such errors can affect the correction of segmentation. See Figure 19 for such an example. The edge map shows that there is a clear strong edge caused by the shadow between the legs, which makes correcting segmentation in that region less effective. Another common problem for segmentation is abrupt changes in the object itself, especially the abrupt appearance of object parts. Figure 20 shows a typical example. Apart from more complex user interactions, we believe that this problem can be solved by using a more sophisticated propagation model, such as a long-term shape."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "We propose a novel generic approach to automatically correct the propagated segmentation in video cropping systems. The basic idea of our work is to incorporate a generic shape-distance measurement into a bilayer MRF framework that has been learned from data to eliminate the intrinsic bias of the classifier in our two-dimensional propagation step. This work is motivated by our observation that different classifiers can differentiate the bias with respect to FP and FN, but they were treated equally in the previous video cropping systems. We found that FP and FN can be treated differently in our two-dimensional MRF, and the optimal shape of MRF can be learned from the data. Extensive evaluations show that our approach can significantly improve the state of video cropping systems in segmentation. There are several vision tasks related to the interactive video cropping problem, such as [34, 37] [39]."}], "references": [{"title": "Keyframe-based tracking for rotoscoping and animation,", "author": ["A. Agarwala", "A. Hertzmann", "D.H. Salesin", "S.M. Seitz"], "venue": "in SIGGRAPH,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Video object cut and paste,", "author": ["Y. Li", "J. Sun", "H.-Y. Shum"], "venue": "in SIGGRAPH,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Interactive video cutout,", "author": ["J. Wang", "P. Bhat", "R.A. Colburn", "M. Agrawala", "M.F. Cohen"], "venue": "in SIGGRAPH,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Video snapcut: Robust video object cutout using localized classifiers,", "author": ["X. Bai", "J. Wang", "D. Simons", "G. Sapiro"], "venue": "in SIGGRAPH,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Discontinuity-aware video object cutout,", "author": ["F. Zhong", "X. Qin", "Q. Peng", "X. Meng"], "venue": "in SIGGRAPH Asia,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Dynamic color flow: a motionadaptive color model for object segmentation in video,", "author": ["X. Bai", "J. Wang", "G. Sapiro"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Transductive segmentation of live video with non-stationary background,", "author": ["F. Zhong", "X. Qin", "Q. Peng"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Jumpcut: Non-successive mask transfer and interpolation for video cutout,", "author": ["Q. Fan", "F. Zhong", "D. Lischinski", "D. Cohen-Or", "B. Chen"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Random walks for image segmentation,", "author": ["L. Grady"], "venue": "IEEE TPAMI, vol. 28,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "A seeded image segmentation framework unifying graph cuts and random walker which yields a new algorithm.,", "author": ["A.K. Sinop", "L. Grady"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Geodesic active contour,", "author": ["V. Caselles", "R. Kimmel", "G. Sapiro"], "venue": "International Journal of Computer Vision, vol. 22,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Fronts propagating with curvaturedependent speed: Algorithms based on Hamilton-Jacobi formulations,", "author": ["S. Osher", "J.A. Sethian"], "venue": "Journal of Computational Physics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1988}, {"title": "Video brush: A novel interface for efficient video cutout,", "author": ["R.-F. Tong", "Y. Zhang", "M. Ding"], "venue": "in CGF,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "EXCOL: an extract-and-complete layering approach to cartoon animation reusing,", "author": ["L. Zhang", "H. Huang", "H. Fu"], "venue": "IEEE TVCG,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Efficient video cutout by paint selection,", "author": ["Y. Zhang", "Y.-L. Tang", "K.-L. Cheng"], "venue": "Journal of Computer Science and Technology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Learning structured prediction models: A large margin approach,", "author": ["B. Taskar", "V. Chatalbashev", "D. Koller", "C. Guestrin"], "venue": "in ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Large margin methods for structured and interdependent output variables,", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "JMLR, vol", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Star shape prior for graph-cut image segmentation,", "author": ["O. Veksler"], "venue": "in ECCV,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Interactive graph cut based segmentation with shape priors,", "author": ["D. Freedman", "T. Zhang"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Shape prior segmentation of multiple objects with graph cuts,", "author": ["N. Vu", "B. Manjunath"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Active visual segmentation,", "author": ["A.K. Mishra", "Y. Aloimonos", "L.F. Cheong", "A. Kassim"], "venue": "IEEE TPAMI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Learning crfs using graph cuts,", "author": ["M. Szummer", "P. Kohli", "D. Hoiem"], "venue": "in ECCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Cutting-plane training of structural svms,", "author": ["T. Joachims", "T. Finley", "C.-N.J. Yu"], "venue": "Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Estimating the support of a high-dimensional distribution,", "author": ["B. Sch\u00f6lkopf", "J.C. Platt", "J.C. Shawe-Taylor", "A.J. Smola", "R.C. Williamson"], "venue": "Neural Computation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "One-class svm for learning in image retrieval,", "author": ["Y. Chen", "X.S. Zhou", "T.S. Huang"], "venue": "in ICIP,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "One-class svms for document classification,", "author": ["L.M. Manevitz", "M. Yousef"], "venue": "JMLR, vol", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Structured learning and prediction in computer vision,", "author": ["S. Nowozin", "C.H. Lampert"], "venue": "Foundations and Trends R  \u00a9 in Computer Graphics and Vision,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Geodesic graph cut for interactive image segmentation,", "author": ["B.L. Price", "B. Morse", "S. Cohen"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Cuda cuts: Fast graph cuts on the gpu,", "author": ["V. Vineet", "P. Narayanan"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "grabcut\u201d: interactive foreground extraction using iterated graph cuts,", "author": ["C. Rother", "V. Kolmogorov", "A. Blake"], "venue": "in ACM SIG- GRAPH,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Interactive graph cuts for optimal boundary & region segmentation of objects in n-d images,", "author": ["Y. Boykov", "M.-P. Jolly"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "Pose estimation and segmentation of people in 3d movies,", "author": ["G. Seguin", "K. Alahari", "J. Sivic", "I. Laptev"], "venue": "IEEE TPAMI,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Motion layer extraction in the presence of occlusion using graph cuts,", "author": ["J. Xiao", "M. Shah"], "venue": "IEEE TPAMI, vol. 27,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2005}, {"title": "Key-segments for video object segmentation,", "author": ["Y.J. Lee", "J. Kim", "K. Grauman"], "venue": "in ICCV, pp. 1995\u20132002,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Foreground segmentation of live videos using locally competing 1svms,", "author": ["M. Gong", "L. Cheng"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Fast object segmentation in unconstrained video,", "author": ["A. Papazoglou", "V. Ferrari"], "venue": "in ICCV, pp", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Deep learning shape priors for object segmentation,", "author": ["F. Chen", "H. Yu", "R. Hu", "X. Zeng"], "venue": "in CVPR, pp", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Supervoxel-consistent foreground propagation in video,", "author": ["S.D. Jain", "K. Grauman"], "venue": "in ECCV,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Classifier based graph construction for video segmentation,", "author": ["A. Khoreva", "F. Galasso", "M. Hein", "B. Schiele"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "V IDEO cutout, as one of the most successful applications of computer vision for video editing and compositing, has gained much attention from the computer graphics community [1], [2], [3], [4], [5], [6].", "startOffset": 180, "endOffset": 183}, {"referenceID": 1, "context": "V IDEO cutout, as one of the most successful applications of computer vision for video editing and compositing, has gained much attention from the computer graphics community [1], [2], [3], [4], [5], [6].", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "V IDEO cutout, as one of the most successful applications of computer vision for video editing and compositing, has gained much attention from the computer graphics community [1], [2], [3], [4], [5], [6].", "startOffset": 190, "endOffset": 193}, {"referenceID": 3, "context": "V IDEO cutout, as one of the most successful applications of computer vision for video editing and compositing, has gained much attention from the computer graphics community [1], [2], [3], [4], [5], [6].", "startOffset": 195, "endOffset": 198}, {"referenceID": 4, "context": "V IDEO cutout, as one of the most successful applications of computer vision for video editing and compositing, has gained much attention from the computer graphics community [1], [2], [3], [4], [5], [6].", "startOffset": 200, "endOffset": 203}, {"referenceID": 3, "context": "The latest video object cutout systems [5], [6], [7], [8], [9] comprise three major steps: (1) Keyframe segmentattion.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "The latest video object cutout systems [5], [6], [7], [8], [9] comprise three major steps: (1) Keyframe segmentattion.", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "The latest video object cutout systems [5], [6], [7], [8], [9] comprise three major steps: (1) Keyframe segmentattion.", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "The latest video object cutout systems [5], [6], [7], [8], [9] comprise three major steps: (1) Keyframe segmentattion.", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "The latest video object cutout systems [5], [6], [7], [8], [9] comprise three major steps: (1) Keyframe segmentattion.", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "ground truth of t+ 1 Result by [6] Our method", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "In Video Snapcut [5], Bai et al.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "al [6] applied matting directly after classification.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "The matting step behaves like random walk segmentation [10] and the latter is closely related to the MRF model [11].", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "The matting step behaves like random walk segmentation [10] and the latter is closely related to the MRF model [11].", "startOffset": 111, "endOffset": 115}, {"referenceID": 7, "context": "[9] proposed a novel method for propagating segmentation to non-successive frames in order to handle large object displacement.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "The propagated segmentation mask was refined using geodesic active contour (GAC) model [12] and the level set method [13], rather than MRF with graph cuts.", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "The propagated segmentation mask was refined using geodesic active contour (GAC) model [12] and the level set method [13], rather than MRF with graph cuts.", "startOffset": 117, "endOffset": 121}, {"referenceID": 12, "context": "Besides, other video cutout frameworks have been proposed [14], [15], [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "Besides, other video cutout frameworks have been proposed [14], [15], [16].", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "Besides, other video cutout frameworks have been proposed [14], [15], [16].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "Second, we propose a novel one-class structured support vector machine (OSSVM) model to learn the weights, as a computationally more favorable alternative to the conventional two-class structured SVM (2CSSVM) frameworks [17], [18].", "startOffset": 220, "endOffset": 224}, {"referenceID": 16, "context": "Second, we propose a novel one-class structured support vector machine (OSSVM) model to learn the weights, as a computationally more favorable alternative to the conventional two-class structured SVM (2CSSVM) frameworks [17], [18].", "startOffset": 226, "endOffset": 230}, {"referenceID": 3, "context": "Our proposed method for segmentation rectification adapts to different classifiers and achieves significant improvement on error reduction over previous methods [5], [6] in segmentation propagation in the experiments.", "startOffset": 161, "endOffset": 164}, {"referenceID": 4, "context": "Our proposed method for segmentation rectification adapts to different classifiers and achieves significant improvement on error reduction over previous methods [5], [6] in segmentation propagation in the experiments.", "startOffset": 166, "endOffset": 169}, {"referenceID": 4, "context": "Note that the confidence map adopted in [6] can be used to eliminate unreliable/ambiguous results from classifier output.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "In conventional video cutout systems [5], [6], the classifier output is refined by using the MRF-based segmentation model.", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "In conventional video cutout systems [5], [6], the classifier output is refined by using the MRF-based segmentation model.", "startOffset": 42, "endOffset": 45}, {"referenceID": 17, "context": "The pairwise potential is often used to model the object boundaries, and it has also been used to represent advanced priors in segmentation [19].", "startOffset": 140, "endOffset": 144}, {"referenceID": 4, "context": "2 A case study on the classification error Here, we conduct a quantitative study on the classification errors produced by the state-of-the-art foregroundbackground classifier for video cutout [6].", "startOffset": 192, "endOffset": 195}, {"referenceID": 4, "context": "\u2019s classifier [6] on all consecutive frame pairs in their training dataset, and we compute the false positive ratio (FPR) and false negative ratio (FNR) for each target frame.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "\u2019s FB classifier [6]", "startOffset": 17, "endOffset": 20}, {"referenceID": 18, "context": "Our segmentation rectification method is inspired by the shape-prior based MRF segmentation model [20], [21] in which shape distance is used in the segmentation model for handling occlusion and background clutter.", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "Our segmentation rectification method is inspired by the shape-prior based MRF segmentation model [20], [21] in which shape distance is used in the segmentation model for handling occlusion and background clutter.", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "in [22], which has been shown to be effective for interactive segmentation:", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "w pp\u2032 in [22] was defined as:", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "One popular method for this task is known as structured learning [17], [18], [23], [24].", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "One popular method for this task is known as structured learning [17], [18], [23], [24].", "startOffset": 71, "endOffset": 75}, {"referenceID": 21, "context": "One popular method for this task is known as structured learning [17], [18], [23], [24].", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "One popular method for this task is known as structured learning [17], [18], [23], [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "for negative label samples fk, or the worst case [17], [18], [23], [24], which can be time-consuming.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "for negative label samples fk, or the worst case [17], [18], [23], [24], which can be time-consuming.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "for negative label samples fk, or the worst case [17], [18], [23], [24], which can be time-consuming.", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "for negative label samples fk, or the worst case [17], [18], [23], [24], which can be time-consuming.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "To remove the need for negative samples, we propose to apply the oneclass SVM, instead of the conventional two-class formulation [17], to the structured learning problem.", "startOffset": 129, "endOffset": 133}, {"referenceID": 23, "context": "positive class, for training [25], [26], [27].", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "positive class, for training [25], [26], [27].", "startOffset": 35, "endOffset": 39}, {"referenceID": 25, "context": "positive class, for training [25], [26], [27].", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "This principle can be used for learning the weight vector w from data [17], [18], and it can be expressed as:", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "This principle can be used for learning the weight vector w from data [17], [18], and it can be expressed as:", "startOffset": 76, "endOffset": 80}, {"referenceID": 26, "context": "For segmentation problem, \u2206 is a specialized cost and is often set as \u2206 = mean(|fk \u2212 f\u2217 k |) [28].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "With the additional requirements of positiveness and boundedness of w, the above yields the conventional SSVM of the following form [17], [18], [23], [24].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "With the additional requirements of positiveness and boundedness of w, the above yields the conventional SSVM of the following form [17], [18], [23], [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "With the additional requirements of positiveness and boundedness of w, the above yields the conventional SSVM of the following form [17], [18], [23], [24].", "startOffset": 144, "endOffset": 148}, {"referenceID": 22, "context": "With the additional requirements of positiveness and boundedness of w, the above yields the conventional SSVM of the following form [17], [18], [23], [24].", "startOffset": 150, "endOffset": 154}, {"referenceID": 23, "context": "This optimization problem is well-known as one class support vector machine, and it has been thoroughly studied previously in the classification literature [25].", "startOffset": 156, "endOffset": 160}, {"referenceID": 22, "context": "Algorithm 1: Two-class SSVM learning [24]", "startOffset": 37, "endOffset": 41}, {"referenceID": 22, "context": "We adopt the cutting plane algorithm for two-class SSVM [24].", "startOffset": 56, "endOffset": 60}, {"referenceID": 17, "context": "It is well known that graph cut for MRF model with 2ndorder pairwise potential suffers from the shrinking bias [19], [29].", "startOffset": 111, "endOffset": 115}, {"referenceID": 27, "context": "It is well known that graph cut for MRF model with 2ndorder pairwise potential suffers from the shrinking bias [19], [29].", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "classifiers [5], [6] are shown to be able to correct local errors near the object boundary.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "classifiers [5], [6] are shown to be able to correct local errors near the object boundary.", "startOffset": 17, "endOffset": 20}, {"referenceID": 28, "context": "The computational time for the Boykov-Kolmogorov (BK) algorithm on a 2 MP image on CPU is about 160 ms, and the GPU implementation of graph cuts can be 2 times faster than on the CPU [30].", "startOffset": 183, "endOffset": 187}, {"referenceID": 4, "context": "The foreground-background classifier we use is the one reported in [6], and its average computational time is about 1.", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "\u2019s classifier [6].", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "(GMM), which is a typical global foreground-background (FB) classifier, and the other is the state-of-the-art local FB classifier proposed in [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 4, "context": "We applied our method to GMM based FB classifier and the state-of-the-art FB classifier proposed in [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "We main compare with the FB classifier + Matting, as adopted in [6], and FB classifier + uniformly weighted graph cuts (UWGC), which is adopted in Rotobrush [5].", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "We main compare with the FB classifier + Matting, as adopted in [6], and FB classifier + uniformly weighted graph cuts (UWGC), which is adopted in Rotobrush [5].", "startOffset": 157, "endOffset": 160}, {"referenceID": 4, "context": "Original image FB Classifier [6] + Matting FB Classifier + UWGC Zoom in Our method (2CSSVM) Zoom in Our method (OSSVM) Zoom in Fig.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "Original image FB Classifier [6] + Matting FB Classifier + UWGC Zoom in Our method (2CSSVM) Zoom in Our method (OSSVM) Zoom in Fig.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "Generally, a global classifier like GMM is not suitable to video cutout [5], we conduct the experiment to validate the generality of our framework.", "startOffset": 72, "endOffset": 75}, {"referenceID": 29, "context": "The experimental results shown in this subsection also imply that the errors from GMM for image cutout can be more effectively removed by using our method, compared with the conventional methods based on graph cuts and matting [31], [32], and the 2CSSVM outperforms the rest in this case while the OSSVM is the best alternative when computational efficiency in learning is a concern.", "startOffset": 227, "endOffset": 231}, {"referenceID": 30, "context": "The experimental results shown in this subsection also imply that the errors from GMM for image cutout can be more effectively removed by using our method, compared with the conventional methods based on graph cuts and matting [31], [32], and the 2CSSVM outperforms the rest in this case while the OSSVM is the best alternative when computational efficiency in learning is a concern.", "startOffset": 233, "endOffset": 237}, {"referenceID": 29, "context": "It is also interesting to note that global classifiers, such as GMM, are commonly adopted in image cutout [31], [32].", "startOffset": 106, "endOffset": 110}, {"referenceID": 30, "context": "It is also interesting to note that global classifiers, such as GMM, are commonly adopted in image cutout [31], [32].", "startOffset": 112, "endOffset": 116}, {"referenceID": 4, "context": "We further demonstrate the efficacy of our approach for rectifying the state-of-the-art local classifier proposed in [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 3, "context": "We also compare our method with Video SnapCut (Rotobrush) [5] and [6] on additional video sequences.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "We also compare our method with Video SnapCut (Rotobrush) [5] and [6] on additional video sequences.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "FB C la ss ifi er [6 ]+", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "The dataset we used is from the INRIA 3D movie dataset [33].", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "FB C la ss ifi er [6 ] +", "startOffset": 18, "endOffset": 22}, {"referenceID": 4, "context": "\u2019s classifier [6], Classifier with Matting, OSSVM with RGB only, uniform weights for GC on RGB-D, 2CSSVM for RGB-D and OSSVM for RGB-D.", "startOffset": 14, "endOffset": 17}, {"referenceID": 32, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 88, "endOffset": 92}, {"referenceID": 33, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 94, "endOffset": 98}, {"referenceID": 34, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 100, "endOffset": 104}, {"referenceID": 35, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 106, "endOffset": 110}, {"referenceID": 36, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 112, "endOffset": 116}, {"referenceID": 37, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 118, "endOffset": 122}, {"referenceID": 38, "context": "There are several vision tasks related to the interactive video cutout problem, such as [34], [35], [36], [37], [38], [39], [40].", "startOffset": 124, "endOffset": 128}, {"referenceID": 3, "context": "It has been noted in [5] that localized optimization allows user to have better control of the video cutout process.", "startOffset": 21, "endOffset": 24}], "year": 2016, "abstractText": "Recent works on interactive video object cutout mainly focus on designing dynamic foreground-background (FB) classifiers for segmentation propagation. However, the research on optimally removing errors from the FB classification is sparse, and the errors often accumulate rapidly, causing significant errors in the propagated frames. In this work, we take the initial steps to addressing this problem, and we call this new task segmentation rectification. Our key observation is that the possibly asymmetrically distributed false positive and false negative errors were handled equally in the conventional methods. We, alternatively, propose to optimally remove these two types of errors. To this effect, we propose a novel bilayer Markov Random Field (MRF) model for this new task. We also adopt the well-established structured learning framework to learn the optimal model from data. Additionally, we propose a novel one-class structured SVM (OSSVM) which greatly speeds up the structured learning process. Our method naturally extends to RGB-D videos as well. Comprehensive experiments on both RGB and RGB-D data demonstrate that our simple and effective method significantly outperforms the segmentation propagation methods adopted in the state-of-the-art video cutout systems, and the results also suggest the potential usefulness of our method in image cutout system. Fig. 0: Given a keyframe segmentation provided by the user (left), our approach generates accurate object cutout results in subsequent frames fully automatically (middle), which can be used for creating a novel compositing (right).", "creator": "LaTeX with hyperref package"}}}