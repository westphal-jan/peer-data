{"id": "1602.04921", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "A diffusion and clustering-based approach for finding coherent motions and understanding crowd scenes", "abstract": "This paper addresses the problem of detecting coherent motions in crowd scenes and presents its two applications in crowd scene understanding: semantic region detection and recurrent activity mining. It processes input motion fields (e.g., optical flow fields) and produces a coherent motion filed, named as thermal energy field. The thermal energy field is able to capture both motion correlation among particles and the motion trends of individual particles which are helpful to discover coherency among them. We further introduce a two-step clustering process to construct stable semantic regions from the extracted time-varying coherent motions. These semantic regions can be used to recognize pre-defined activities in crowd scenes. Finally, we introduce a cluster-and-merge process which automatically discovers recurrent activities in crowd scenes by clustering and merging the extracted coherent motions. Experiments on various videos demonstrate the effectiveness of our approach.", "histories": [["v1", "Tue, 16 Feb 2016 06:25:30 GMT  (16211kb,D)", "http://arxiv.org/abs/1602.04921v1", "This manuscript is the accepted version for TIP (IEEE Transactions on Image Processing), 2016"]], "COMMENTS": "This manuscript is the accepted version for TIP (IEEE Transactions on Image Processing), 2016", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.MM", "authors": ["weiyao lin", "yang mi", "weiyue wang", "jianxin wu", "jingdong wang", "tao mei"], "accepted": false, "id": "1602.04921"}, "pdf": {"name": "1602.04921.pdf", "metadata": {"source": "CRF", "title": "A Diffusion and Clustering-based Approach for Finding Coherent Motions and Understanding Crowd Scenes", "authors": ["Weiyao Lin", "Yang Mi", "Weiyue Wang", "Jianxin Wu", "Jingdong Wang", "Tao Mei"], "emails": ["laughter}@sjtu.edu.cn).", "wujx2001@nju.edu.cn).", "tmei}@microsoft.com)."], "sections": [{"heading": null, "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "II. RELATED WORKS", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "III. OVERVIEW OF THE APPROACH", "text": "The framework of the proposed approach is presented in Fig. 3. The optical flow fields [17], [32] are first extracted from the input videos. Second, the coarse-fine thermal diffusion method is applied to translate the entered motion fields into coherent motion fields, i.e. into thermal energy fields (TEFs). Third, the triangulation scheme is applied to identify coherent movements. Fourth, the two-step cluster scheme is performed with the obtained coherent movements to bundle coherent movements from multiple TEFs and construct semantic regions for the target scene. Finally, based on these semantic regions, we can extract effective features to describe crowd activities in the target scene and detect predefined crowd activities accordingly. At the same time, the cluster and merge method is also applied based on the extracted coherent movements and semantic regions to detect recurring activities in the target scene."}, {"heading": "IV. FINDING COHERENT MOTIONS", "text": "In order to find precise coherent motions, it is important to construct a coherent field of motion to emphasize the correlation of motion between particles while preserving the original motion information. To fulfill this requirement, we introduce a thermal diffusion process to model particle correlations. In view of an optical input field, we consider each particle (i.e. each pixel in a frame) to be a \"heat source\" and it can scatter energies to affect other particles. By appropriately modelling this thermal diffusion process, a precise correlation between particles can be achieved. The formulation is based on the following intuitions: 1) Particles further away from the heat source should achieve less thermal energies; 2) Particles residing in the direction of motion of the heat source particle should receive more thermal energies; 3) Heat source particles with greater motions should transport more thermal energies."}, {"heading": "A. Thermal Diffusion Process", "text": "Based on the above mentioned discussions, we borrow the idea of the physical thermal expansion [33] and model Q = Q = Q = Q Q = Q Q = Q = Q = Q = Q = Q = Q = Q = Q (Q = Q = Q) Q (Q = Q = Q) Q (Q = Q = Q Q = Q = Q) Q (Q = Q = Q = Q) Q (Q = Q = Q) Q (Q = Q = Q) Q (Q = Q Q = Q) Q (Q = Q = Q Q) Q (Q = Q = Q = Q Q = Q) Q (Q = Q = Q) Q = Q) Q (Q = Q = Q) Q (Q = Q = Q = Q) Q (Q = Q = Q = Q) Q) Q (Q = Q = Q) Q) Q (Q = Q = Q) Q (Q = Q = Q) Q (Q = Q = Q) Q (Q = Q = Q) Q (Q = Q = Q) Q (Q = Q = Q = Q) Q (Q = Q = Q) Q (Q = Q = Q = Q) Q (Q = Q = Q = Q = Q) Q (Q = Q = Q = Q) Q (Q = Q = Q = Q = Q) Q (Q = Q = Q) Q = Q = Q = Q) Q (Q = Q = Q = Q = Q) Q (Q = Q = Q = Q = Q) Q = Q (Q = Q = Q = Q = Q) Q (Q = Q = Q) Q = Q = Q) Q (Q = Q = Q = Q) Q (Q = Q = Q = Q) Q (Q = Q = Q = Q) Q (Q = Q = Q = Q = Q) Q (Q = Q = Q) Q (Q = Q = Q) Q (Q = Q (Q = Q = Q) Q = Q = Q) Q (Q = Q) Q (Q = Q = Q = Q) Q (Q = Q = Q (Q = Q) Q = Q = Q) Q (Q = Q = Q = Q) Q (Q = Q = Q) Q = Q = Q = Q (Q) Q (Q = Q = Q = Q (Q = Q = Q"}, {"heading": "B. The Coarse-to-Fine Scheme", "text": "Although it effectively strengthens the coherence of the particles, it is based on a single field of motion, and only short-term motion information that is volatile and spacious is taken into account. Thus, we propose a rough plan to include long-term motion information."}, {"heading": "C. Finding Coherent Motions through Triangulation", "text": "We propose a triangulation-based scheme as follows: Step 1: Triangulation. In this step, we randomly test particles from the entire scene and apply the triangulation process [34] to link the collected particles to each other. [34] The block described in Fig. 3 as \"triangulation\" shows a triangulation result in which red dots are the collected particles and the lines are generated by the triangulation process [34]. Step 2: Boundary sensing. First, we obtain each triangulation chain weight by: \u03c9 (P, Q) = | | EP \u2212 Q | | (5), where P and Q are two interconnected particles, EP and Q are the thermal energy limits of P and Q in TEF. A large weight is assigned if the connected particles come from different coherent ranges of motion."}, {"heading": "V. CONSTRUCTING SEMANTIC REGIONS", "text": "This year, the time has come for an agreement to be reached, which can only take a few days."}, {"heading": "A. Recognizing Pre-defined Activities", "text": "Based on the constructed semantic regions, we are able to detect predefined activities (i.e. activities with marked training data) in the scene. In this paper, we simply intersect the TEF vectors in each semantic region and concatenate these average TEF vectors as the last characteristic vector to describe the activity patterns in a TEF. Subsequently, a linear support vector engine (SVM) [37] is used to train and detect predefined activities. Experimental results show that we can achieve satisfactory results with accurate TEF and precise semantic regions using this simple method."}, {"heading": "B. Merging Disconnected Coherent Motions", "text": "Since TEF also includes long-distance correlations between distant particles, implementing our cluster scheme also has the advantage of effectively fusing disjointed coherent motions that can be caused by occlusion from other objects or a low density of mass. For example, the two disjointed blue regions in the far right of Figure 6a are merged into the same cluster by our approach. Note that this problem is not well studied in existing coherent motion research."}, {"heading": "VI. MINING RECURRENT ACTIVITIES", "text": "With the resulting coherent movements and constructed semantic regions, crowd activities can be detected through the construction and pre-labeling of training data, as in Section V-A. However, since pre-defining or labeling crowd activities requires a lot of human work, it is also desirable to automatically break down recurring activity patterns in a crowd scene without human intervention. To this end, we propose a three-step cluster-and-margin process: frame-level clustering, coherent motion merging, and flow curve extraction. 7"}, {"heading": "A. Frame-level Clustering", "text": "The frame-level cluster cluster (8), where the frames represent contiguous movements and semantic regions, so that the frames with the same recurring activity patterns can be placed in the same group. \u2212 In this paper, we can first use the similarities between frames t and t - the similarities between all coherent motions from frames t and t - to calculate the meaning, which is first calculated using Eq. 6. Then, the inter-frame similarity between frames SF (t -), which is achieved from these coherent motion similarities and the segmented semantic regions, can be achieved. Specifically, we define SF (t -) asSF (t, t -) asSF (t \u2212) = SFU (t \u2212)."}, {"heading": "B. Coherent Motion Merging", "text": "In this paper, we present a coherent motion that merges contiguous motion regions within the same recurring activity group, then merges coherent motion regions within the same recurring activity group to form a motion pattern region, the merging process can be performed by Eq. 13 and Fig. 10.EP to merge coherent motion regions within the same recurring activity group, then coherent motion of the same cluster is merged to form a motion pattern region, the merging of processes can be performed by Eq. 10.EP to filter coherent motion regions within the same recurring activity group, and then coherent motion of the same cluster is merged to form a motion pattern region."}, {"heading": "C. Flow Curve Extraction", "text": "The motion pattern regions attained in the previous step can represent the complete motion information for each recurrent activity. However, since the motion pattern regions may overlap and the contours of the motion pattern regions may also be irregular, it is necessary to extract flow curves from these motion pattern regions so that recurrent activities can be described and visualized more clearly. Our proposed flow pattern extraction method can be described by algorithm 3 and Fig. 11. According to algorithm 3 and Fig. 11, our approach first sequentially intersects a motion pattern region in sub-regions along the direction of motion in Rig. j. Then the centering of the sub-regions are linked to reach the output flow curve. By the above method, the extracted flow curve can accurately capture the main motion pattern area of a motion pattern region. Furthermore, in step 5 of algorithm 3, when the line becomes the vector bifurcated to this motion area, it is important to note that in step 3 of the algorithm the line becomes the EP2-bifurcated motion area + 1."}, {"heading": "VII. EXPERIMENTAL RESULTS", "text": "Our approach is implemented by Matlab and the optical flow fields [32] are used as input motion vector fields, while each pixel in the image is treated as particles. To achieve motion vector fields with T-frame intervals (T = 10 in our experiments), the particle advection method [17] is used, which tracks the motion of each particle over T-frames."}, {"heading": "A. Results for Coherent Motion Detection", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "B. Results for Semantic Region Construction and Pre-defined Activity Recognition", "text": "This year it is so far that it is only a matter of time before it is so far, until it is so far."}, {"heading": "C. Results for Recurrent Activity Mining", "text": "In fact, it is the case that most of them are in a position to move into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "VIII. CONCLUSION", "text": "In this paper, we examine the problem of coherent motion detection, semantic region construction and recurring activity mining in crowd scenes. A thermal diffusion-based algorithm is introduced along with a two-step cluster scheme that can lead to more meaningful coherent results in the areas of movement and semantic region. Based on the extracted coherent movements and semantic regions, a cluster and merging process is proposed that can effectively detect desirable activity patterns from a crowd video. Experiments on various videos show that our approach is state-of-the-art."}], "references": [{"title": "Finding coherent motions and semantic regions in crowd scenes: a diffusion and clustering approach,", "author": ["W. Wang", "W. Lin", "Y. Chen", "J. Wu", "J. Wang", "B. Sheng"], "venue": "European Conf. Computer Vision (ECCV),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Learning motion patterns in crowded scenes using motion flow field,", "author": ["M. Hu", "S. Ali", "M. Shah"], "venue": "Intl. Conf. Pattern Recognition (ICPR), pp", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Towards good practices for action video encoding,", "author": ["J. Wu", "Y. Zhang", "W. Lin"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp. 2577\u20132584,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Multi-camera activity correlation analysis,", "author": ["C.C. Loy", "T. Xiang", "S. Gong"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Multimedia search reranking: A literature survey,", "author": ["T. Mei", "Y. Rui", "S. Li", "Q. Tian"], "venue": "ACM Computing Surveys,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Finding perfect rendezvous on the go: accurate mobile visual localization and its applications to routing,", "author": ["H. Liu", "T. Mei", "J. Luo", "H. Li", "S. Li"], "venue": "ACM Intl. Conf. Multimedia (MM),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Crowd motion partitioning in a scattered motion field,", "author": ["S. Wu", "H. Wong"], "venue": "IEEE Trans. Systems, Man, and Cybernetics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Coherent filtering: detecting coherent motions from crowd clutters,", "author": ["B. Zhou", "X. Tang", "X. Wang"], "venue": "European Conf. Computer Vision (ECCV),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Measuring crowd collectiveness,", "author": ["B. Zhou", "X. Wang", "X. Tang"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp. 3049\u20133056,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Temporal analysis of motif mixtures using Dirichlet processes,", "author": ["R. Emonet", "J. Varadarajan", "J.-M. Odobez"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "A sequential topic model for mining recurrent activities from long term video logs,", "author": ["V. Jagannadan", "R. Emonet", "J.-M. Odobez"], "venue": "Intl. J. Computer Vision, vol. 103,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Random field topic model for semantic region analysis in crowded scenes from tracklets,", "author": ["B. Zhou", "X. Wang", "X. Tang"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Trajectory analysis and semantic region modeling using nonparametric Bayesian models,", "author": ["X. Wang", "K.T. Ma", "G. Ng", "E. Grimson"], "venue": "Intl. J. Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "An incremental DPMM-based method for trajectory clustering, modeling, and retrieval,", "author": ["W. Hu", "X. Li", "G. Tian", "S. Maybank", "Z. Zhang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Trajectory classification using switched dynamical Hidden Markov Models,", "author": ["J. Nascimento", "M.A.T. Figueiredo", "J.S. Marques"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "A Lagrangian particle dynamics approach for crowd flow segmentation and stability analysis,", "author": ["S. Ali", "M. Shah"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Abnormal crowd behavior detection using social force model,", "author": ["R. Mehran", "A. Oyama", "M. Shah"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Crowd analysis: a survey,", "author": ["B. Zhan", "D. Monekosso", "P. Remagnino", "S. Velastin", "L. Xu"], "venue": "Machine Vision and Applications,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Motion competition: A variational approach to piecewise parametric motion segmentation,", "author": ["D. Cremers", "S. Soatto"], "venue": "Intl. J. Computer Vision, vol. 62,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Colour, texture, and motion in level set based segmentation and tracking,", "author": ["T. Brox", "M. Rousson", "R. Deriche", "J. Weickert"], "venue": "Image and Vision Computing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Abnormal detection using interaction energy potentials,", "author": ["X. Cui", "Q. Liu", "M. Gao", "D.N. Metaxas"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Optical flow estimation for a periodic image sequence,", "author": ["L. Li", "Y. Yang"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Motion detail preserving optical flow estimation.", "author": ["L. Xu", "J. Jia", "Y. Matsushita"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Learning visual flows: a Lie algebraic approach,", "author": ["D. Lin", "E. Grimson", "J. Fisher"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Optical flow computation using extended constraints,", "author": ["A. Bimbo", "P. Nesi", "J. Sanz"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1996}, {"title": "Robust Anisotropic Diffusion,", "author": ["M.J. Black", "G. Sapiro", "D.H. Marimont", "D. Heeger"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "Adaptive diffusion flow active contours for image segmentation,", "author": ["Y. Wu", "Y. Wang", "Y. Jia"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Anisotropic diffusion in image processing", "author": ["J. Weickert"], "venue": "Stuttgart: Teubner,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1998}, {"title": "Single and multiple view detection, tracking and video analysis in crowded environments,", "author": ["T. Xu", "P. Peng", "X. Fang", "C. Su", "Y. Wang", "Y. Tian", "W. Zeng", "T. Huang"], "venue": "Intl. Conf. Advanced Video and Signal-Based Surveillance (AVSS),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Scene segmentation for behavior correlation,", "author": ["J. Li", "S. Gong", "T. Xiang"], "venue": "European Conf. Computer Vision (ECCV),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Lucas/Kanade meets Horn/Schunck: combining local and global optic flow methods,", "author": ["A. Bruh", "J. Weickert", "C. Schnorr"], "venue": "Intl. J. Computer Vision, vol. 61,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "Conduction of heat in solids", "author": ["H. Carslaw", "J. Jaeger"], "venue": "Oxford Science Publications,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1959}, {"title": "Incremental topological flipping works for regular triangulations,", "author": ["H. Edelsbrunner", "N. Shah"], "venue": "Algorithmica, vol. 15,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1996}, {"title": "Digital image processing", "author": ["R. Gonzales", "R. Woods"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Inferring user image search goals under the implicit guidance of users,", "author": ["Z. Lu", "X. Yang", "W. Lin", "H. Zha", "X. Chen"], "venue": "IEEE Trans. Circuits and Systems for Video Technology,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "LIBSVM: a library for support vector machines,", "author": ["C. Chang", "C. Lin"], "venue": "ACM Trans. Intelligent Systems Technology,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "Object segmentation by long term analysis of point trajectories,", "author": ["T. Brox", "J. Malik"], "venue": "European Conf. Computer Vision (ECCV),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Since coherent motions can effectively decompose scenes into meaningful semantic parts and facilitate the analysis of complex crowd scenes, they are of increasing importance in crowd-scene understanding and activity recognition [2], [3], [4], [5], [6].", "startOffset": 228, "endOffset": 231}, {"referenceID": 2, "context": "Since coherent motions can effectively decompose scenes into meaningful semantic parts and facilitate the analysis of complex crowd scenes, they are of increasing importance in crowd-scene understanding and activity recognition [2], [3], [4], [5], [6].", "startOffset": 233, "endOffset": 236}, {"referenceID": 3, "context": "Since coherent motions can effectively decompose scenes into meaningful semantic parts and facilitate the analysis of complex crowd scenes, they are of increasing importance in crowd-scene understanding and activity recognition [2], [3], [4], [5], [6].", "startOffset": 238, "endOffset": 241}, {"referenceID": 4, "context": "Since coherent motions can effectively decompose scenes into meaningful semantic parts and facilitate the analysis of complex crowd scenes, they are of increasing importance in crowd-scene understanding and activity recognition [2], [3], [4], [5], [6].", "startOffset": 243, "endOffset": 246}, {"referenceID": 5, "context": "Since coherent motions can effectively decompose scenes into meaningful semantic parts and facilitate the analysis of complex crowd scenes, they are of increasing importance in crowd-scene understanding and activity recognition [2], [3], [4], [5], [6].", "startOffset": 248, "endOffset": 251}, {"referenceID": 0, "context": "The basic idea of this paper appeared in our conference version [1].", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "Although many algorithms have been proposed for coherent motion detection [7], [8], [9], [2], this problem is not yet effectively addressed.", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "Although many algorithms have been proposed for coherent motion detection [7], [8], [9], [2], this problem is not yet effectively addressed.", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "Although many algorithms have been proposed for coherent motion detection [7], [8], [9], [2], this problem is not yet effectively addressed.", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Although many algorithms have been proposed for coherent motion detection [7], [8], [9], [2], this problem is not yet effectively addressed.", "startOffset": 89, "endOffset": 92}, {"referenceID": 9, "context": "Many crowd scenes are composed of recurrent activities [10], [11], [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "Many crowd scenes are composed of recurrent activities [10], [11], [12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "Many crowd scenes are composed of recurrent activities [10], [11], [12].", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "Although many researches have been done for parsing recurrent activities in low-crowd scenes [13], [14], [15], [16], this issue is not well addressed in crowd scene scenarios where reliable motion trajectories are unavailable.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Although many researches have been done for parsing recurrent activities in low-crowd scenes [13], [14], [15], [16], this issue is not well addressed in crowd scene scenarios where reliable motion trajectories are unavailable.", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "Although many researches have been done for parsing recurrent activities in low-crowd scenes [13], [14], [15], [16], this issue is not well addressed in crowd scene scenarios where reliable motion trajectories are unavailable.", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 17, "endOffset": 20}, {"referenceID": 7, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 32, "endOffset": 35}, {"referenceID": 16, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 43, "endOffset": 47}, {"referenceID": 18, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "Many works [17], [7], [8], [9], [2], [18], [19], [20], [21], [22] have been proposed on coherent motion detection.", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "Cremers and Soatto [20] and Brox et al.", "startOffset": 19, "endOffset": 23}, {"referenceID": 19, "context": "[21] model the intensity variation of optical flow by an objective functional minimization scheme.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Other works introduce external spatial-temporal correlation traits to model the motion coherency among particles [7], [8], [9].", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "Other works introduce external spatial-temporal correlation traits to model the motion coherency among particles [7], [8], [9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "Other works introduce external spatial-temporal correlation traits to model the motion coherency among particles [7], [8], [9].", "startOffset": 123, "endOffset": 126}, {"referenceID": 21, "context": "These methods try to improve the estimation accuracy of the input motion field by including global constraints over particles [23], [24], [25], [26].", "startOffset": 126, "endOffset": 130}, {"referenceID": 22, "context": "These methods try to improve the estimation accuracy of the input motion field by including global constraints over particles [23], [24], [25], [26].", "startOffset": 132, "endOffset": 136}, {"referenceID": 23, "context": "These methods try to improve the estimation accuracy of the input motion field by including global constraints over particles [23], [24], [25], [26].", "startOffset": 138, "endOffset": 142}, {"referenceID": 24, "context": "These methods try to improve the estimation accuracy of the input motion field by including global constraints over particles [23], [24], [25], [26].", "startOffset": 144, "endOffset": 148}, {"referenceID": 25, "context": "The anisotropic diffusion based methods, used in image segmentation, is also related to our work [27], [28], [29].", "startOffset": 97, "endOffset": 101}, {"referenceID": 26, "context": "The anisotropic diffusion based methods, used in image segmentation, is also related to our work [27], [28], [29].", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "The anisotropic diffusion based methods, used in image segmentation, is also related to our work [27], [28], [29].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "In [17], Ali and Shah detected instability regions in a scene by comparing with its normal coherent motions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "Furthermore, besides the works on coherent motion, there are also other works which directly extract global features from the entire scene to recognize crowd activities [3], [30].", "startOffset": 169, "endOffset": 172}, {"referenceID": 28, "context": "Furthermore, besides the works on coherent motion, there are also other works which directly extract global features from the entire scene to recognize crowd activities [3], [30].", "startOffset": 174, "endOffset": 178}, {"referenceID": 3, "context": "Although there are some works [4], [31] which recognize crowd activities by segmenting scenes into semantic regions, our approach differs from them.", "startOffset": 30, "endOffset": 33}, {"referenceID": 29, "context": "Although there are some works [4], [31] which recognize crowd activities by segmenting scenes into semantic regions, our approach differs from them.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "[13] and Hu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] further introduced Dirichlet processes to model the activity patterns of different trajectory groups.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] extracted fragments of trajectories (called tracklets)", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] and Emonet et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] extracted low-level motion flows as motion descriptors and introduced a Probabilistic Latent Sequential Motif (PLSM) model to achieve recurrent activities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "The optical flow fields [17], [32] are first extracted from input videos.", "startOffset": 24, "endOffset": 28}, {"referenceID": 30, "context": "The optical flow fields [17], [32] are first extracted from input videos.", "startOffset": 30, "endOffset": 34}, {"referenceID": 31, "context": "physical thermal propagation [33] and model the thermal diffusion process by Eq.", "startOffset": 29, "endOffset": 33}, {"referenceID": 27, "context": "The inclusion of this term is one of the major differences between the proposed approach and the anisotropic-diffusion methods [29].", "startOffset": 127, "endOffset": 131}, {"referenceID": 15, "context": "It is very difficult to include these particles into the coherent region by traditional methods [17], [7], [8], [9] because they are", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "It is very difficult to include these particles into the coherent region by traditional methods [17], [7], [8], [9] because they are", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "It is very difficult to include these particles into the coherent region by traditional methods [17], [7], [8], [9] because they are", "startOffset": 107, "endOffset": 110}, {"referenceID": 8, "context": "It is very difficult to include these particles into the coherent region by traditional methods [17], [7], [8], [9] because they are", "startOffset": 112, "endOffset": 115}, {"referenceID": 32, "context": "In this step, we randomly sample particles from the entire scene and apply the triangulation process [34] to link the sampled particles.", "startOffset": 101, "endOffset": 105}, {"referenceID": 32, "context": "3 shows one triangulation result, where red dots are the sampled particles and the lines are links created by the triangulation process [34].", "startOffset": 136, "endOffset": 140}, {"referenceID": 33, "context": "algorithm [35].", "startOffset": 10, "endOffset": 14}, {"referenceID": 34, "context": "Then, we construct a similarity graph for the M coherent motions, and perform clustering [36] on this similarity graph with the optimal number of clusters being determined automatically, the cluster results are grouped coherent regions.", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "Comparing with previous semantic region segmentation methods [4], [31] which perform clustering using local similarity among particles, our scheme utilizes the guidance from the global coherent motion clustering results to strengthen the correlations among particles.", "startOffset": 61, "endOffset": 64}, {"referenceID": 29, "context": "Comparing with previous semantic region segmentation methods [4], [31] which perform clustering using local similarity among particles, our scheme utilizes the guidance from the global coherent motion clustering results to strengthen the correlations among particles.", "startOffset": 66, "endOffset": 70}, {"referenceID": 35, "context": "Then, a linear support vector machine (SVM) [37] is utilized to train and recognize predefined activities.", "startOffset": 44, "endOffset": 48}, {"referenceID": 34, "context": "In this paper, we first calculate inter-frame similarities for all frame pairs and then utilize spectral clustering [36] to cluster frames according to these inter-frame similarities.", "startOffset": 116, "endOffset": 120}, {"referenceID": 30, "context": "Our approach is implemented by Matlab and the optical flow fields [32] are used as the input motion vector fields while each", "startOffset": 66, "endOffset": 70}, {"referenceID": 15, "context": "In order to achieve motion vector fields with T -frame intervals (T = 10 in our experiments), the particle advection method [17] is used which tracks the movement of each particle over T frames.", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "We perform experiments on a dataset including 30 different crowd videos collected from the UCF dataset [17], the UCSD dataset [39], the CUHK dataset [9], and our own collected set.", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "We perform experiments on a dataset including 30 different crowd videos collected from the UCF dataset [17], the UCSD dataset [39], the CUHK dataset [9], and our own collected set.", "startOffset": 149, "endOffset": 152}, {"referenceID": 15, "context": "(a): Ground Truth, (b): Results of our approach, (c): Results of [17], (d): Results of [7], (e): Results of [8], (f): Results of [9], (g): Results of [40], (h): Results of [28].", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "(a): Ground Truth, (b): Results of our approach, (c): Results of [17], (d): Results of [7], (e): Results of [8], (f): Results of [9], (g): Results of [40], (h): Results of [28].", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "(a): Ground Truth, (b): Results of our approach, (c): Results of [17], (d): Results of [7], (e): Results of [8], (f): Results of [9], (g): Results of [40], (h): Results of [28].", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "(a): Ground Truth, (b): Results of our approach, (c): Results of [17], (d): Results of [7], (e): Results of [8], (f): Results of [9], (g): Results of [40], (h): Results of [28].", "startOffset": 129, "endOffset": 132}, {"referenceID": 36, "context": "(a): Ground Truth, (b): Results of our approach, (c): Results of [17], (d): Results of [7], (e): Results of [8], (f): Results of [9], (g): Results of [40], (h): Results of [28].", "startOffset": 150, "endOffset": 154}, {"referenceID": 26, "context": "(a): Ground Truth, (b): Results of our approach, (c): Results of [17], (d): Results of [7], (e): Results of [8], (f): Results of [9], (g): Results of [40], (h): Results of [28].", "startOffset": 172, "endOffset": 176}, {"referenceID": 33, "context": "Input: A motion pattern region R\u03a8j merged from coherent region cluster \u03a8j Output: A flow curve extracted from R\u03a8j 1: Calculate the skeleton of R\u03a8j [35] 2: Find the end point Ps of the skeleton which is on \u201cbackward\u201d position to all other end points, where the \u201cbackward\u201d direction is defined as the reversed direction of the motion flows in R\u03a8j 3: PK = Ps, where PK is the current segmentation point 4: while PK+1 is inside R\u03a8j do 5: Pmov,s as the middle point of the line perpendicular to the motion vector EPK ,\u03a8j at PK 6: for n=0 to Nummov {Nummov is the number of movements} do 7: Move from Pmov,s to Pmov,e by EPmov,s,\u03a8j , where EPmov,s,\u03a8j is motion vector at Pmov,s in R\u03a8j 8: Pmov,s=Pmov,e 9: end for", "startOffset": 147, "endOffset": 151}, {"referenceID": 15, "context": "dynamics approach [17], the local-translation domain segmentation approach [7], the coherent-filtering approach [8], and the collectiveness measuring-based approach [9].", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "dynamics approach [17], the local-translation domain segmentation approach [7], the coherent-filtering approach [8], and the collectiveness measuring-based approach [9].", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "dynamics approach [17], the local-translation domain segmentation approach [7], the coherent-filtering approach [8], and the collectiveness measuring-based approach [9].", "startOffset": 112, "endOffset": 115}, {"referenceID": 8, "context": "dynamics approach [17], the local-translation domain segmentation approach [7], the coherent-filtering approach [8], and the collectiveness measuring-based approach [9].", "startOffset": 165, "endOffset": 168}, {"referenceID": 36, "context": "include the results of a general motion segmentation method [40] and an anisotropic-diffusion-based image segmentation method [28].", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "include the results of a general motion segmentation method [40] and an anisotropic-diffusion-based image segmentation method [28].", "startOffset": 126, "endOffset": 130}, {"referenceID": 15, "context": "Comparatively, the method in [17] can only detect part of the circle while the methods in [8] and [9] fail to work since few reliable key points are extracted from this overcrowded scene.", "startOffset": 29, "endOffset": 33}, {"referenceID": 7, "context": "Comparatively, the method in [17] can only detect part of the circle while the methods in [8] and [9] fail to work since few reliable key points are extracted from this overcrowded scene.", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "Comparatively, the method in [17] can only detect part of the circle while the methods in [8] and [9] fail to work since few reliable key points are extracted from this overcrowded scene.", "startOffset": 98, "endOffset": 101}, {"referenceID": 36, "context": "Furthermore, the methods in [40] and [28] do not show satisfying results, e.", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "Furthermore, the methods in [40] and [28] do not show satisfying results, e.", "startOffset": 37, "endOffset": 41}, {"referenceID": 36, "context": "This is because: (1) the crowd scenes are extremely complicated such that the extracted particle flows or trajectories become unreliable, thus making the general motion segmentation methods [40] difficult to create precise results; (2) Since many coherent region boundaries in the crowd motion fields are rather vague and unrecognizable, good boundaries cannot be easily achieved without suitably utilizing the characteristics of the motion vector fields.", "startOffset": 190, "endOffset": 194}, {"referenceID": 26, "context": "Thus, simply applying the existing anisotropicdiffusion segmentation methods [28] cannot achieve satisfying results.", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "Proposed [17] [7] [8] [9] [40] [28] PER (%) 7.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "Proposed [17] [7] [8] [9] [40] [28] PER (%) 7.", "startOffset": 14, "endOffset": 17}, {"referenceID": 7, "context": "Proposed [17] [7] [8] [9] [40] [28] PER (%) 7.", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "Proposed [17] [7] [8] [9] [40] [28] PER (%) 7.", "startOffset": 22, "endOffset": 25}, {"referenceID": 36, "context": "Proposed [17] [7] [8] [9] [40] [28] PER (%) 7.", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": "Proposed [17] [7] [8] [9] [40] [28] PER (%) 7.", "startOffset": 31, "endOffset": 35}, {"referenceID": 6, "context": "15 compares the results of four methods: (1) Our approach (\u201cOur\u201d), (2) Directly cluster regions based on the particles\u2019 TEF vectors (\u201cDirect\u201d, note that our approach differs from this method by clustering over the cluster label vectors), (3) Use [7] to achieve coherent motion regions and then apply our two-step clustering scheme to construct semantic regions (\u201c[7]+Two-Step\u201d, we show the results of [7] because in our experiments, [7] has the best semantic region construction results among the compared methods in Table I), (4) The activity-based scene segmentation method in [4] (\u201c[4]\u201d).", "startOffset": 246, "endOffset": 249}, {"referenceID": 6, "context": "15 compares the results of four methods: (1) Our approach (\u201cOur\u201d), (2) Directly cluster regions based on the particles\u2019 TEF vectors (\u201cDirect\u201d, note that our approach differs from this method by clustering over the cluster label vectors), (3) Use [7] to achieve coherent motion regions and then apply our two-step clustering scheme to construct semantic regions (\u201c[7]+Two-Step\u201d, we show the results of [7] because in our experiments, [7] has the best semantic region construction results among the compared methods in Table I), (4) The activity-based scene segmentation method in [4] (\u201c[4]\u201d).", "startOffset": 363, "endOffset": 366}, {"referenceID": 6, "context": "15 compares the results of four methods: (1) Our approach (\u201cOur\u201d), (2) Directly cluster regions based on the particles\u2019 TEF vectors (\u201cDirect\u201d, note that our approach differs from this method by clustering over the cluster label vectors), (3) Use [7] to achieve coherent motion regions and then apply our two-step clustering scheme to construct semantic regions (\u201c[7]+Two-Step\u201d, we show the results of [7] because in our experiments, [7] has the best semantic region construction results among the compared methods in Table I), (4) The activity-based scene segmentation method in [4] (\u201c[4]\u201d).", "startOffset": 401, "endOffset": 404}, {"referenceID": 6, "context": "15 compares the results of four methods: (1) Our approach (\u201cOur\u201d), (2) Directly cluster regions based on the particles\u2019 TEF vectors (\u201cDirect\u201d, note that our approach differs from this method by clustering over the cluster label vectors), (3) Use [7] to achieve coherent motion regions and then apply our two-step clustering scheme to construct semantic regions (\u201c[7]+Two-Step\u201d, we show the results of [7] because in our experiments, [7] has the best semantic region construction results among the compared methods in Table I), (4) The activity-based scene segmentation method in [4] (\u201c[4]\u201d).", "startOffset": 433, "endOffset": 436}, {"referenceID": 3, "context": "15 compares the results of four methods: (1) Our approach (\u201cOur\u201d), (2) Directly cluster regions based on the particles\u2019 TEF vectors (\u201cDirect\u201d, note that our approach differs from this method by clustering over the cluster label vectors), (3) Use [7] to achieve coherent motion regions and then apply our two-step clustering scheme to construct semantic regions (\u201c[7]+Two-Step\u201d, we show the results of [7] because in our experiments, [7] has the best semantic region construction results among the compared methods in Table I), (4) The activity-based scene segmentation method in [4] (\u201c[4]\u201d).", "startOffset": 579, "endOffset": 582}, {"referenceID": 3, "context": "15 compares the results of four methods: (1) Our approach (\u201cOur\u201d), (2) Directly cluster regions based on the particles\u2019 TEF vectors (\u201cDirect\u201d, note that our approach differs from this method by clustering over the cluster label vectors), (3) Use [7] to achieve coherent motion regions and then apply our two-step clustering scheme to construct semantic regions (\u201c[7]+Two-Step\u201d, we show the results of [7] because in our experiments, [7] has the best semantic region construction results among the compared methods in Table I), (4) The activity-based scene segmentation method in [4] (\u201c[4]\u201d).", "startOffset": 585, "endOffset": 588}, {"referenceID": 6, "context": "15 shows that the methods utilizing \u201ccoherent motion cluster label\u201d information (\u201cour\u201d and \u201c[7]+two-step\u201d) create more meaningful semantic regions than the other methods,", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "Furthermore, comparing our approach with the \u201c[7]+TwoStep\u201d method, it is obvious that the semantic regions by our", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "After that, we (a) Original (b) Our (c) Direct (d) [7] (e) [4]", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "After that, we (a) Original (b) Our (c) Direct (d) [7] (e) [4]", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "(f) Original (g) Our (h) Direct (i) [7] (j) [4]", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "(f) Original (g) Our (h) Direct (i) [7] (j) [4]", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "(k) Original (l) Our (m) Direct (n) [7] (o) [4]", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "(k) Original (l) Our (m) Direct (n) [7] (o) [4]", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "The caption \u201c[7]\u201d denotes the method \u201c[7]+Two step\u201d.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "The caption \u201c[7]\u201d denotes the method \u201c[7]+Two step\u201d.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Our (%) Our+ OF (%) Direct (%) [7]+TwoStep (%) [4] (%) [3] (%) Fig.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "Our (%) Our+ OF (%) Direct (%) [7]+TwoStep (%) [4] (%) [3] (%) Fig.", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "Our (%) Our+ OF (%) Direct (%) [7]+TwoStep (%) [4] (%) [3] (%) Fig.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Besides, we also include the results of two additional methods: (1) a state-ofthe-art dense-trajectory-based recognition method [3] (\u201cDenseTraj\u201d); (2) the method which uses our semantic regions but uses the input motion field (i.", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": ", \u201cour\u201d, \u201cour+OF\u201d, and \u201c[7]+Two step\u201d) achieve better results than other methods.", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "3) The dense-trajectory method [3] which extracts global features does not achieve satisfying results.", "startOffset": 31, "endOffset": 34}, {"referenceID": 10, "context": "Besides, we also compare our approach with a state-of-the-art activity mining method which utilizes a Probabilistic Latent Sequential Motif (PLSM) model to discover recurrent activities [11], which are shown as the last rows in Figs 17a, 17b, and 17c.", "startOffset": 186, "endOffset": 190}, {"referenceID": 10, "context": "3) Comparing our approach with the PLSM-based method [11], we can see that: (i) By introducing coherent regions to measure inter-frame similarities and derive motion pattern regions, our approach can achieve cleaner activity flows which are more coherent with the humanobserved activity types in Fig.", "startOffset": 53, "endOffset": 57}], "year": 2016, "abstractText": "This paper addresses the problem of detecting coherent motions in crowd scenes and presents its two applications in crowd scene understanding: semantic region detection and recurrent activity mining. It processes input motion fields (e.g., optical flow fields) and produces a coherent motion filed, named as thermal energy field. The thermal energy field is able to capture both motion correlation among particles and the motion trends of individual particles which are helpful to discover coherency among them. We further introduce a two-step clustering process to construct stable semantic regions from the extracted time-varying coherent motions. These semantic regions can be used to recognize pre-defined activities in crowd scenes. Finally, we introduce a cluster-and-merge process which automatically discovers recurrent activities in crowd scenes by clustering and merging the extracted coherent motions. Experiments on various videos demonstrate the effectiveness of our approach.", "creator": "LaTeX with hyperref package"}}}