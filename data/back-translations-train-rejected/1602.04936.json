{"id": "1602.04936", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "Reinforcement Learning approach for Real Time Strategy Games Battle city and S3", "abstract": "In this paper we proposed reinforcement learning algorithms with the generalized reward function. In our proposed method we use Q-learning and SARSA algorithms with generalised reward function to train the reinforcement learning agent. We evaluated the performance of our proposed algorithms on two real-time strategy games called BattleCity and S3. There are two main advantages of having such an approach as compared to other works in RTS. (1) We can ignore the concept of a simulator which is often game specific and is usually hard coded in any type of RTS games (2) our system can learn from interaction with any opponents and quickly change the strategy according to the opponents and do not need any human traces as used in previous works. Keywords : Reinforcement learning, Machine learning, Real time strategy, Artificial intelligence.", "histories": [["v1", "Tue, 16 Feb 2016 08:17:17 GMT  (3449kb,D)", "http://arxiv.org/abs/1602.04936v1", "13 pages, vol 9 issue 4 of IJIP"]], "COMMENTS": "13 pages, vol 9 issue 4 of IJIP", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["harshit sethy", "amit patel"], "accepted": false, "id": "1602.04936"}, "pdf": {"name": "1602.04936.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning approach for Real Time Strategy Games Battle city and S3", "authors": ["Harshit Sethy", "Amit Patel"], "emails": ["hsethy1@gmail.com", "amtptl93@gmail.com"], "sections": [{"heading": null, "text": "Reinforcement Learning approach for Real Time Strategy Games Battle city and S3Harshit Sethya, Amit PatelbaCTO of Gymtrekker Fitness Private Limited, Mumbai, India, Email: hsethy1 @ gmail.combAssistant Professor, Department of Computer Science and Engineering, RGUKT IIIT Nuzvid, Krishna-521202 India, Email: amtptl93 @ gmail.com In this paper, we proposed reinforcement learning algorithms with the generalized reward function. In our proposed method, we use Q-Learning and SARSA generalized reward function algorithms to train the reinforcement learner. We evaluated the performance of our proposed algorithms on two real-time strategy games called BattleCity and S3. There are two main advantages of such an approach compared to other work in RTS. (1) We can ignore the concept of a simulator, which is often game specific and is RT2-S-specific in each type of games."}, {"heading": "1. INTRODUCTION", "text": "The presence of good artificial intelligence (AI) in the background of a game is one of the main factors of fun and playability in commercial computer games. Although AI has been used successfully in several games such as chess, backgammon or checkers when it comes to real-time games, the predefined scripts that are normally used to simulate artificial intelligence in chess, backgammon, etc., do not seem to work because real-time games require real-time decisions, the search space is huge and as such does not contain real AI to learn [2]. Conventional planning approaches are difficult in the case of RTS games because they have various factors such as huge decision spaces, conflicting domains, partially available, non-deterministic and real-time (real-time means that the game continues while deciding on the best actions and states change simultaneously)."}, {"heading": "1.1. Real Time Strategy Games", "text": "Unlike turn-based strategy games, where you can take your time, in real-time strategy games, all moves, constructions, battles, etc. take place in real-time. In a typical RTS game, the screen contains a map area consisting of the game world with buildings, units, and terrain. In an RTS game, there are usually several players. In addition to the players, there are various game units called participants, units, and structures, which are under player control, and players must store their assets and / or destroy opposing players \"assets by using their control of the units. We use 2 RTS games (1) BattleCity and (2) S3 for our rating. A snapshot of two RTS games called BattleCity and S3 is shown in Figure 1."}, {"heading": "1.2. BattleCity Game", "text": "BattleCity is a multi-directional shooter video game that can be played with two basic actions: Move and Fire. The player who controls a tank must destroy enemy tanks or enemy bases and also protect his own base. The player can move tanks in four directions (left, right, up and down) and fire bullets in which direction the tank was last moved, while bases are static. There are three types of obstacles. (1) Brick wall tanks can destroy it by firing this type of wall. (2) Marble wall tanks cannot destroy it by firing. (3) Water1ar Xiv: 160 2.04 936v 1 [cs.A I] 1 6Fe b20 16tank can shoot through it."}, {"heading": "1.3. S3 Game", "text": "S3 is a real-time strategy game in which each player has the goal of staying alive after destroying the rest of the players. Four basic actions in this game are Harvest: Collecting Resources (Gold and Wood), Build: Build Buildings (Barrack, Blacksmith, Tower, etc.), Train: Produce Troops (Archers, Foot Soldiers, Catapults, Knights), Attack: Attack Enemies. This essay is structured as follows. In addition to the introduction, there are five other sections. In Section 2, the review of related work is highlighted. In Section 3, we discuss reinforcement techniques in real-time strategy games and outline the different learning algorithms used in reinforcement learning. In Section 4, we outline the implementation details related to the proposed reinforcement learning algorithms with the general reward function for two real-time strategy games (1) BattCity and (2) S3."}, {"heading": "2. Related Work", "text": "One of the most important tasks associated with online planning [6] is to play in the real world. It leads to a series of algorithms that can be used to develop plans that are executed by one or more people. Another task of the authors who solve the problems of the plan is to carry out simulation games."}, {"heading": "3. Reinforcement Learning", "text": "Reinforcement Learning [1] is the field of machine learning, which is about what to do, how situations are mapped to actions to maximize a numerical reward signal.The learner does not know what actions to take, but instead has to figure out which actions are most rewarded by their application.In the most interesting and challenging cases, actions can affect not only the immediate situation, but also the next situation and with it all subsequent rewards. Comparing Reinforcement Learning [12] with the RTS game environment, an AI player learns by interacting with the environment and observing the feedback of these interactions, which is the same as the basic way humans (and animals) learn. As humans, we can perform actions and observe the results of these actions on the environment."}, {"heading": "3.1. Reinforcement Learning Architecture", "text": "RL Architecture has two main features; one is learning and the other is playing with the experiences learned. First, RLearner has no knowledge of the game. So it performs random actions and observes the resulting state based on some sensor information of the game and gives feedback (in the form of a reward, which is further used to calculate the Q values for the state action pairs or Q tables) on the previous state according to the desirability of the current state. Q values of the state action pairs are known as Q tables, which define a policy. After each action policy, Q values for the state action pairs (Q tables) are updated to predict the best action during the game. RL Agent learns during the game, so there is again feedback and the entire process that continues until the end of the game."}, {"heading": "3.2. Basic components of RL", "text": "Reinforcement Learning contains five basic components, listed below: 1. a set of environmental states S2. a set of actions A3. Rules of transition between councils of state 4. Rules determining the scalar immediate reward of a transition (reward functions) 5. rules describing what the agent observes (value functions)."}, {"heading": "3.2.1. Reward Function", "text": "The scalar value, which represents the degree to which a state or action is desirable, is called a reward. This scalar reward is assigned to the action for the respective transition and the resulting state of play. If the resulting state is desirable and safe, then this action is assigned a positive scalar value as a reward, otherwise, if the state is not safe or undesirable, a negative scalar value as a negative reward is assigned to that action."}, {"heading": "3.2.2. Value Function", "text": "Value functions are used to map states or pairs of state actions to real numbers, where the value of a state represents the long-term reward achieved from that state (state action), and to execute a specific policy. It is estimated how good a particular action will be in a given state or how high the return on that action is likely to be. There are two types of value functions (state action), the expected return when a state performs the action and follows it. There are two methods to define these value functions: 1. Monte Carlo [1] Method: In this method, the actor would have to wait until the final reward was received before any state action pairs can be updated. Once the final reward is received, the way to reach the final state must be traced."}, {"heading": "3.3. Sensor representation for S3 and BattleCity Game", "text": "We use two types of sensor information to assign rewards in the Battle City game, which are explained as follows: 1. EnemyInline: If the enemy position directly matches the player without block or wall, the sensor is represented by number 2. If there is a wall or block between enemy and player, the sensor is represented by number 1. If the position of the enemy base does not match the player's position, then the sensor is represented by number 2. EnemyBaseInline: This sensor information is presented in the same way as above, but instead of taking into account the enemy base position, the position of the enemy base is taken into account. If the position of the enemy base directly matches the position of the player without block or wall, the sensor is represented by number 2. If there is a wall or block between enemy base and player, then the sensor is represented by number 1. If the position of the enemy base does not match the position of the player, then the sensor is called 0, the current position of the wood base and the current position of the 3."}, {"heading": "3.4. Action Selection Policies", "text": "We have the following action selection guidelines that can be used to select the desired action according to the behavior of each policy. \u2212 greedy: Mostly, the action with the highest appreciated reward is selected, the greediest action. But, with little chance, an action is chosen randomly to ensure that optimal actions are discovered. \u2212 soft: very similar to \u2212 greedy. The best action is selected with probability 1 - and the rest of the time a random action is uniformly selected.3. Softmax: One disadvantage of the above methods is that they select random actions with a certain probability. So there is a case where the worst possible action is selected as second best. Softmax remedies this by assigning each action a rank or weight according to its measure rating. Therefore, it is unlikely that the worst actions will be selected."}, {"heading": "3.5. Steps while learning", "text": "1. The learner observes an input game state.2. The learner then creates a new guideline based on the dimensions of the world.3. Set the parameters (\u03b1, \u03b3 and number of episodes) for the learner and start to learning.4. Start to run epochs. You can run each epoch individually. An epoch contains the following levels.1. An action is determined by a decision function (e.g. greedy).2. The action is accomplished.3. Depending on the reward function, the learner receives a scalar reward or reinforcement from the environment. 4. Information about the reward given for this state / action pair is recorded.5. Update the Q values in the Q table according to the learning algorithm (e.g. Q-Learning or SARSA)."}, {"heading": "4. Proposed learning algorithm", "text": "In this section we outline our proposed learning algorithms, which we have integrated into the two RTS games Battlecity and S3. We also present the implementation details related to the selection of parameters and reward features."}, {"heading": "4.1. Parameters", "text": "This section contains the information on the reward algorithms and their parameters that we use for the two games BattleCity and S3. \u2022 Learning Rate \u03b1: The learning rate 0 < \u03b1 < 1 determines which fraction of the old estimate we update with the newest estimation. \u03b1 = 0 prevents the RL agent from learning anything, while \u03b1 = 1 completely changes the previous values with the new. \u2022 Discount factor \u03b3: The discount factor 0 < \u03b3 < 1 determines which fraction of the coming reward values will be taken into consideration for evaluation. \u03b3 = 0 ignores all upcoming rewards because \u03b3 = 1 means that the RL agent considers the current and upcoming rewards to be equivalent. \u2022 Exploration Rate: In the selection policy there is a policy called a greedy method that uses the exploration rate 0 < < < 1 to determine the ratio between exploration and exploitation."}, {"heading": "4.2. Reward function for BattleCity", "text": "Algorithm 1: Reward Function is used to calculate the reward after executing the action based on the current state. Depending on the outcome of the action, reward or punishment are assigned. In steps 1 to 9, the positions (X-Y coordinates) of the player, enemy and enemy base on the map are calculated. In steps 10 to 16, when the game is over and the RL Agent (player) wins, the reward is added to the total reward (new reward value), otherwise the penalty is deducted from the total reward. In steps 17 to 18, when the enemy is in line with the RL Agent, the penalty is deducted from the total reward so that he always tries to be out of line with the enemy. In steps 19 to 21, if the enemy base is in line with the RL Agent, the distance between the enemy base and the RL Agent is then calculated and subtracted from the 2 total reward and subtracted."}, {"heading": "4.3. Reward function for S3", "text": "Algorithm 2: In steps 1 to 6, the sensors get in relation to the total gold, the total wood and the size of the troop algorithm 1: calcReward for BattleCityInput: state: - contains positions of units, reward, punishmentList: - contains sensors of the game. gameState: - contains the state of the game running or not Output: Reward1 Playerx = zero, Playery = zero, Enemyx = zero, Enemyy = zero; 2 EnemyBasex = zero, EnemyBasey = zero, Winner = zero; 3 newReward = 0, Distance = 0; 4 Playerx = getPositionx (state, player); 5 Playery = getPositiony (state, player); 6 Enemyx = getPositionx (state, enemy); 7 Enemyy = getPositiony (state, enemy) = getPositiony. \""}, {"heading": "5. Experimental Results", "text": "In the previous section, we discussed how we successfully applied reinforcement learning in two real-time strategy games called BattleCity and S3. In this section, we outline the experimental results associated with reinforcement learning in BattleCity and S3."}, {"heading": "5.1. BattleCity:", "text": "This year, the number of arrivals has tripled compared to the previous year, tripling the number of arrivals in the first three months of the current year, while the number of arrivals increased by 0.2% in the second half of the year."}, {"heading": "5.1.1. Map: Bridge-26x18", "text": "This card has a size of 262x18 (see Figure 4), so the total statespace for this card is a total combination of the x \u2212 y coordinates of the player and the enemy, which is 262x182. This card has a marble wall between which the tank cannot destroy by firing, so this is an advantage for the tank to hide from enemies and attack when enemies come to their side."}, {"heading": "5.1.2. Map: Bridges-34x24", "text": "This is the most complex map (see Figure 9) of all on which we have performed our evaluation, due to its size and structure. It is a 34x24 map and has 342x242 search spaces. It contains many brick walls and bodies of water. Brick walls can be destroyed by firing. Their size and bodies of water make it a difficult and complex map. In time frames versus episode diagram (see Figures 10 and 11), the plot (see Figure 6 and 5) shows that the time to win the game for all strategies varies for each episode. This map has more bodies of water, making it difficult to win a strategy quickly."}, {"heading": "5.2. S3", "text": "In our experiments we found RL-Agent for S3 with the two approaches Q-Learning and SARSA-Agent for the game."}, {"heading": "6. Conclusions", "text": "In this thesis, we proposed a reinforcement learning model for real-time strategy games. To achieve this goal, we use two reinforcement learning algorithms SARSA and QLearning. The idea is to achieve the best action using one of the RL algorithms so as not to use the tracks generated by the players. In previous work on real-time strategy games using \"on-line case based learning,\" human tracks are an important component in the learning process. In the proposed method, we do not use previous knowledge like tracks and therefore follow an unattended approach. Another important contribution of our work is to achieve the best action using two algorithms (SARSA and Q-Learning) that fall under reinforcement learning, without the tracks generated by the player as in the previous work \"on line case based learning\" using Darmok2. Another important contribution of our work is the reward function by performing two types of reward functions that are called conditional and generalized functions."}], "references": [{"title": "Barto,Reinforcement Learning: An Introduction. A book publisher MIT", "author": ["A.G.R.S. Sutton"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Using first order inductive learning as an alternative to a simulator in a game arficial intelligence", "author": ["Katie Long Genter"], "venue": "under-graduate thesis. In Georgia Institute of Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Learning opponent strategies through first order induction", "author": ["Katie Long Genter", "Santiago Onta\u00f1\u00f3n", "Ashwin Ram"], "venue": "In : FLAIRS Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Mmpm: a generic platform for case-based planning research", "author": ["P.P. Gomez-Martin", "D. Llanso", "M.A. Gomez-Martin", "Santiago Onta\u00f1\u00f3n", "Ashwin Ram"], "venue": "In : ICCBR 2010 Workshop on Case-Based Reasoning for Computer Games,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Using Reinforcement Learning for City Site Selection in the Turn-Based Strategy Game Civilization IV", "author": ["Stefan Wender", "Ian Watson"], "venue": "In : Computational Intelligence and Games (CIG-2008),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "An introduction to casebased reasoning", "author": ["Janet L. Kolodner"], "venue": "In : Artificial Intelligence Review,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "On-line casebased planning", "author": ["Santi Onta\u00f1\u00f3n", "Kinshuk Mishra", "Neha Sugandh", "Ashwin Ram"], "venue": "In : Computational Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Learning from human demonstrations for real-time casebased planning", "author": ["Santiago Onta\u00f1\u00f3n", "K.Bonnette", "P.Mahindrakar", "M.A. Gomez-Martin", "Katie Long Genter", "J.Radhakrishnan", "R.Shah", "Ashwin Ram"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "On-line case-based plan adaptation for real-time strategy games. In :Association for the Advancement of Artificial Intelligence", "author": ["Neha Sugandh", "Santiago Onta\u00f1\u00f3n", "Ashwin Ram"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Improving Adaptive Game AI with Evolutionary Learning In Computer Games: Artificial Intelligence, Design and Education, pages", "author": ["Marc Ponsen", "Pieter Spronck"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Concurrent hierarchical reinforcement learning Turn-Based Strategy Game Civilization IV", "author": ["Bhaskara Marthi", "Stuart Russell", "David Latham", "Carlos Guestrin"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}], "referenceMentions": [{"referenceID": 9, "context": "Although AI has been applied successfully in several games such as chess, backgammon or checkers when it comes to real-time games the pre-defined scripts which is usually used to simulate the artificial intelligence in chess, backgammon etc [11].", "startOffset": 241, "endOffset": 245}, {"referenceID": 1, "context": "This is because in real-time games decisions has to be made in real-time as well as the search space is huge and as such they do not contain any true AI for learning [2].", "startOffset": 166, "endOffset": 169}, {"referenceID": 5, "context": "One of the major works using Online casebased planning [6] techniques for Real Time Strategy Games was published in [9].", "startOffset": 55, "endOffset": 58}, {"referenceID": 8, "context": "One of the major works using Online casebased planning [6] techniques for Real Time Strategy Games was published in [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 7, "context": "In [8] a case-based planning system called Darmok2 is introduced that can play RTS games.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "Another work by the same authors which uses Darmok2 but addresses the issues of plan acquisition, on-line plan execution, interleaved planning and execution and on-line plan adaptation is [7].", "startOffset": 188, "endOffset": 191}, {"referenceID": 2, "context": "In [3] the authors summarize their work in exploring the use of the first order inductive learning (FOIL) algorithm for learning rules which can be used to represent opponent strategies.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Drawbacks of all case based learning [4] approaches as mentioned above are (1) It requires expert demonstrations for making plans (2) after training is done, no further learning takes place (3) to cover large state spaces it would require large number of rules in the plan base (4) no exploration for optimal solution.", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "Stefan Wender [5] uses Reinforcement Learning for City Site Selection in the TurnBased Strategy Game Civilization IV.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "Stefan Wender [5] uses Reinforcement Learning for City Site Selection in the Turn-Based Strategy Game Civilization IV.", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "In this paper we aim to do away with the hard coded simulator and propose a learning approach based on Reinforcement Learning [1](RL) wherein sensor information from the current game-state is used to select the best action.", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "Reinforcement Learning [1] is the field of Machine Learning which deals with what to do, how to map situations to actions so as to maximize a numerical reward signal.", "startOffset": 23, "endOffset": 26}, {"referenceID": 10, "context": "With comparing reinforcement learning [12] to RTS game environment an AI player learns by interacting with the environment and observing the feed-backs of these interactions.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "Monte Carlo [1] Method : In this method the agent would need to wait until the final reward was received before any state-action pair values can be updated.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "Temporal Difference [1] Method : It is used to estimate the value functions after each step.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "Statistics about the performance of the SARSA[1], Q-Learning[1] and Darmok2 in the various maps are represented below in the form of graphs.", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": "Statistics about the performance of the SARSA[1], Q-Learning[1] and Darmok2 in the various maps are represented below in the form of graphs.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "It is clear from the experimental results that reinforcement learning agent with the SARSA [1] algorithm performs better than other techniques like Q-Learning [1] and online case based learn-", "startOffset": 91, "endOffset": 94}, {"referenceID": 0, "context": "It is clear from the experimental results that reinforcement learning agent with the SARSA [1] algorithm performs better than other techniques like Q-Learning [1] and online case based learn-", "startOffset": 159, "endOffset": 162}], "year": 2016, "abstractText": "In this paper we proposed reinforcement learning algorithms with the generalized reward function. In our proposed method we use Q-learning and SARSA algorithms with generalised reward function to train the reinforcement learning agent. We evaluated the performance of our proposed algorithms on two real-time strategy games called BattleCity and S3. There are two main advantages of having such an approach as compared to other works in RTS. (1) We can ignore the concept of a simulator which is often game specific and is usually hard coded in any type of RTS games (2) our system can learn from interaction with any opponents and quickly change the strategy according to the opponents and do not need any human traces as used in previous works.", "creator": "LaTeX with hyperref package"}}}