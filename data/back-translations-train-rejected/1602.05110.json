{"id": "1602.05110", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "Generating images with recurrent adversarial networks", "abstract": "Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional network with respect reference image features is a way to render images of high visual quality. We show that unrolling this gradient-based optimization yields a recurrent computation that creates images by incrementally adding onto a visual \"canvas\". We propose a recurrent generative model inspired by this view, and show that it can be trained using adversarial training to generate very good image samples. We also propose a way to quantitatively compare adversarial networks by having the generators and discriminators of these networks compete against each other.", "histories": [["v1", "Tue, 16 Feb 2016 17:51:39 GMT  (2145kb,D)", "http://arxiv.org/abs/1602.05110v1", null], ["v2", "Wed, 17 Feb 2016 14:41:52 GMT  (2145kb,D)", "http://arxiv.org/abs/1602.05110v2", null], ["v3", "Sun, 10 Apr 2016 19:17:27 GMT  (2148kb,D)", "http://arxiv.org/abs/1602.05110v3", null], ["v4", "Sun, 29 May 2016 01:17:59 GMT  (2489kb,D)", "http://arxiv.org/abs/1602.05110v4", null], ["v5", "Tue, 13 Dec 2016 03:21:03 GMT  (4969kb,D)", "http://arxiv.org/abs/1602.05110v5", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["daniel jiwoong im", "chris dongjoo kim", "hui jiang", "roland memisevic"], "accepted": false, "id": "1602.05110"}, "pdf": {"name": "1602.05110.pdf", "metadata": {"source": "META", "title": "Generating images with recurrent adversarial networks", "authors": ["Daniel Jiwoong Im", "Chris Dongjoo Kim", "Hui Jiang", "Roland Memisevic"], "emails": ["IMDANIEL@IRO.UMONTREAL.CA", "KIMDON20@GMAIL.COM", "HJ@CSE.YORKU.CA", "MEMISEVR@IRO.UMONTREAL.CA"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are a form of self-realization, capable of focusing on oneself."}, {"heading": "2. Background", "text": "Generative networks (GAN) are built on the concept of game theory, where two models play an uncooperative game (Nash, 1951). The game is structured between a generative model and a discriminatory model, G and D, each in such a way that the generative model generates examples that are difficult for discriminator D to distinguish from real data, and the discriminator tries to avoid using the generative model G. Formally, the discriminative model is a classifying D: 0, 1} that attempts to distinguish whether a certain point x RM comes from the data or not. Generative model G: RK \u2192 RM generates samples x RM that are indistinguishable from the data by pulling an example z RK that is randomly drawn from some previous distributions p (z) to the data space. These models can be trained by playing a minmax game as follows: min."}, {"heading": "3. Model", "text": "In fact, most of them will be able to abide by the rules that they have applied in practice."}, {"heading": "3.1. Generative Recurrent Adversarial Networks", "text": "The underlying structure of a Generative Recurrent Adversarial Networks (GRAN) is similar to other GANs. The main difference between GRAN and other generative, contradictory models is that generator G consists of a recursive feedback loop that takes a sequence of noise samples from the previous distribution z \u0445 p (z) and draws the output at various time steps C1, C2, \u00b7 \u00b7, CT. Figure 3 describes the high-level abstraction of GRAN.Each time, a sample z from the previous distribution is transferred to a function f (\u00b7) with the hidden state hc, t where hc, t represents the current encoded state of the previous drawing Ct \u2212 1. Ct is what is drawn on the canvas and it contains the output of the function f (\u00b7). Specifically, hc, t is a hidden state encoded by the previous drawing Ct \u2212 1."}, {"heading": "4. Model Evaluation: Battle between GANs", "text": "It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is not. (...) It is not. (...) It is not. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. (...) It is. It is. It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. (... It is. It is. It is. (...) It is. It is. It is. (... It is. It is. It is. (...). It is. It is. It is. (... It is. It is. It is. (...). It is. It is. (...). It is. It is. It is. It is. It is. (...). It is. It is. It is. It is. It is. It is. (...). It is. It is. It is. It is. It is. (...). It is. It is. It is. It is. It is. It is. It is. It is. (...). It is. It is. It is. It is. () most people are. (...) most people are able to survive. (...) they are able to survive. (... (...) they are able to survive. (...) They are able to survive. (...) They are able to survive. (...). (...) They are able to survive. ("}, {"heading": "5. Experiments", "text": "To assess whether sequential generation expansion increases performance, we have three different sets of image data, both quantitatively and qualitatively. We have conducted several empirical studies on GRAN within the model selection metrics discussed in Section 4. In addition, we have generated samples and analyzed them to evaluate the GRAN quality. MNIST data sets contain 60,000 images for training and 10,000 images for testing, and each of the images is 28 x 28 pixels for the handwritten numbers from 0 to 9 (LeCun et al., 1998)."}, {"heading": "Q: How does GRAN perform?", "text": "We focused on comparing GRANs with 1, 3, and 5 time steps, called GRAN1, GRAN3, and GRAN5. For all three datasets, GRAN3 and GRAN5 performed better than GRAN1, as shown in Table 2."}, {"heading": "Q: How do GRAN and other GAN type of models perform compared to non generative adversarial models?", "text": "Although this may not be the best way to evaluate the two models, since the GRAN generator is explicitly evaluated, we tested the comparison of our model with other generative models, such as the Denoising UAE (DVAE) (Im et al., 2015) and DRAW on the MNIST dataset. Table 3 shows the results of the application of GAM. The error rates were all below 50% and particularly low for the DVAE samples. Surprisingly, the error rate for the samples from DRAW was very nice, but also quite low with GRAN3. This shows that the discriminator of generative countermodels can easily distinguish between the samples generated by DVAE and DRAW. Our hypothesis is that the samples look nicer in their target due to the smoothing effect of a medium square error, but they do not capture all relevant aspects of the statics of real handwritten images."}, {"heading": "Q: How do GRAN\u2019s samples look?", "text": "We present samples of GRAN for MNIST, cifar10, and LSUN in Figure 6, Figure 7, and Figure 8. Similarly, most of the samples of MNIST and cifar10 shown in Figure 6 and Figure 7 appear to be recognizable and reasonably classifiable to humans. Similarly, the LSUN samples from Figure 8 appear to cover the diversity of church buildings and contain finely detailed textures; the \"image static\" of two real-world image data sets is embedded in both types of samples. As it is impossible to look over the training data and determine whether a given sample looks like a training case, it is common (though somewhat questionable) to look at the closest neighbors to perform basic health checks. As shown in Figure 9, Figure 12, and Figure 11, one does not find replicas of training data sets that generally do not match the training data. Empirically, we have found that GRAN data tends to generate interpolarity between samples."}, {"heading": "Q: How do the samples look like during the intermediate time steps t?", "text": "Figure 10, Figure 29, and Figure 35 present the intermediate samples when the total number of steps is 3. From the numbers we can observe the gradual evolution of the samples over time. Common observation from the intermediate samples is that the images become finer-grained and introduce details from the previous time step image. Intermediate samples for models with a total of 5 can be found in the supplementary materials. This behavior is somewhat similar to (Denton et al., 2015), as one might expect it is more than just a coarse to fine behavior."}, {"heading": "6. Discussion", "text": "We proposed a new generative model based on the contrasting training of a relapsing neural network (Gatys et al., 2015). We showed conditions under which the model performs well and showed that it can produce higher quality visual samples than an equivalent single-stage model. We also introduced a new metric for the quantitative comparison of opposing networks and showed that the relapsing generative model performs better than existing generative models on this scale."}, {"heading": "Acknowledgements", "text": "We thank the members of the LISA laboratory in Montreal, in particular Mohammed Pezeshki and Donghyun Lee, for their helpful discussions."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional network with respect reference image features is a way to render images of high visual quality. We show that unrolling this gradient-based optimization yields a recurrent computation that creates images by incrementally adding onto a visual \u201ccanvas\u201d. We propose a recurrent generative model inspired by this view, and show that it can be trained using adversarial training to generate very good image samples. We also propose a way to quantitatively compare adversarial networks by having the generators and discriminators of these networks compete against each other.", "creator": "LaTeX with hyperref package"}}}