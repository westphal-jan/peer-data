{"id": "1602.05127", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "A Harmonic Extension Approach for Collaborative Ranking", "abstract": "We present a new perspective on graph-based methods for collaborative ranking for recommender systems. Unlike user-based or item-based methods that compute a weighted average of ratings given by the nearest neighbors, or low-rank approximation methods using convex optimization and the nuclear norm, we formulate matrix completion as a series of semi-supervised learning problems, and propagate the known ratings to the missing ones on the user-user or item-item graph globally. The semi-supervised learning problems are expressed as Laplace-Beltrami equations on a manifold, or namely, harmonic extension, and can be discretized by a point integral method. We show that our approach does not impose a low-rank Euclidean subspace on the data points, but instead minimizes the dimension of the underlying manifold. Our method, named LDM (low dimensional manifold), turns out to be particularly effective in generating rankings of items, showing decent computational efficiency and robust ranking quality compared to state-of-the-art methods.", "histories": [["v1", "Tue, 16 Feb 2016 18:35:25 GMT  (173kb,D)", "http://arxiv.org/abs/1602.05127v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["da kuang", "zuoqiang shi", "stanley osher", "rea bertozzi"], "accepted": false, "id": "1602.05127"}, "pdf": {"name": "1602.05127.pdf", "metadata": {"source": "CRF", "title": "A Harmonic Extension Approach for Collaborative Ranking", "authors": ["Da Kuang", "Zuoqiang Shi", "Stanley Osher", "Andrea Bertozzi"], "emails": ["dakuang@math.ucla.edu.", "sjo@math.ucla.edu.", "bertozzi@math.ucla.edu.", "zqshi@math.tsinghua.edu.cn."], "sections": [{"heading": "1 Introduction", "text": "Recommendation systems are important components of contemporary e-commerce platforms (Amazon, eBay, Netflix, etc.) and have been popularized by the Netflix challenge. Detailed surveys of this field can be found in [11, 13]. Recommendation algorithms are often based on collaborative filtering methods or \"crowd of wisdom\" and can be categorized into memory and model-based approaches. Memory-based approaches include user-based and item-based recommendations [18]. For a user, for example, we call up the highly rated articles from u's closest neighboring countries and recommend those articles that have not been consumed by u. Memory-based methods are actually based on a graph that defines a user-by-user or item-item similarity as each user's closest neighbor or item. In contrast, model-based methods are formulated as matrix complete problems that assume that the entire user-by-rating matrix is matrix."}, {"heading": "2 Harmonic Extension Formulation", "text": "The question we have to ask is whether there is a \"real\" evaluation problem given by an oracle without missing entries that we are not aware of, and as mentioned in the introduction, we formulate the question of whether there is a \"real\" evaluation problem given by an oracle without missing entries that we are not aware of, and as mentioned in the introduction, the question is whether there is a \"real\" evaluation problem referred to as M embedded in Rn. The number of users in our user-by-item evaluation system is presented as U = {uj, 1 \u2264 j \u2264 m} in which the j-th series of A and U \u2012 M is a sample of U."}, {"heading": "3 Point Integral Method (PIM)", "text": "The key observation in the PIM is that the Laplace-Beltrami-Operator has the following holistic approach: \"M\" (x, y), \"M,\" \"M,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"\" S, \"\" \"S,\" \"\" S, \"\" \"S,\" \"\" S, \"\" \"\" S, \"\" \"\" S, \"\" \"\" S, \"\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" \"S,\" S, \"S,\" S, \"S,\" S, \"S,\" S, \"S,\" S, \"S,\" S, \"S,\" S, \"S,\" S, \"S,\" S, \"S,\" S, \"S,\" S, S, \"S,\" S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S. \"S, S, S, S, S, S, S, S, S, S, S, S, S."}, {"heading": "4 Low Dimensional Manifold (LDM) Interpretation", "text": "In this section we emphasize the other interpretation of our method, which is based on the low dimensionality of the user multiplicity. In the User Multiplicity Rating System, a user is represented by an n-dimensional vector consisting of the ratings for n items, and the User Multiplicity is a multiplicity embedded in Rn. Normally, n, the number of items, is a large number in the order of 103 \u0445 106. The intrinsic dimension of the user multiplicity is much lower than n. On the basis of this observation, it is natural to restore the rating matrix by searching for the user multiplicity with the lowest dimension, which implies the optimization problem in (2): min X-Rm-n, M-Rndim (M), M-Rndim (M)."}, {"heading": "5 Weight Matrix", "text": "Weight Matrix W plays an important role in our algorithm as well as other graph-based approaches [24, 18, 10, 7]. We deal with the typical user-item graph with cosmic similarities that are used in existing memory systems for recommendations. However, we have made significant changes to make the procedure for an economical evaluation of matrices efficient. Our algorithm for constructing the weight matrix is described in detail in Algorithm 3. Again, we consider the user-user-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph"}, {"heading": "6 Experiments", "text": "In this section, we evaluate our proposed method LDM both in terms of runtime and ranking quality. Since the literature on recommendation systems, cooperative filtering and matrix completion is huge, we select only a few existing methods to compare with. All experiments are performed on a Linux laptop with an Intel i7-5600U CPU (4 physical threads) and 8 GB of memory. Algorithm 3 Baugewicht matrix of incomplete evaluation data Input: Incomplete evaluation matrix R-Rm-n, number of closest neighbors K 1: Generate binary rating matrix RB-Rm-n: (RB) j,: 1, j-Rj, j-m-m-m: (1, Rj, j-m) not missing 0, Rj, j-m missingle2: Normalize each set of RB elements so that they (RB) j,:: 1, 2 = Rm-j, \u2264 j-j-m-m: on the 3 rows of RB-m-b: an approximate number of cosm x:"}, {"heading": "6.1 Data Sets", "text": "In our experiments, we use three MovieLens1 datasets: MovieLens-100k, MovieLens-1m, and MovieLens-10m. In each dataset, each user has at least 20 ratings. In accordance with the convention in previous collaborative ranking work, we randomly select N ratings for each user as a training set, and the other ratings were used for testing. To keep at least 10 ratings in the test set for each user, we remove users with less than N + 10. After this pre-processing, we can create multiple versions of these datasets with different N, the information of which is summarized in Table 1.1http: / / grouplens.org / datasets / movielens /."}, {"heading": "6.2 Methods for Comparison", "text": "We compare our LDM method with Singular Value Decomposition (SVD) as a starting point and two state-of-the-art methods specifically designed for collaborative ranking. All three methods for comparison optimize a pair ranking loss function. We do not perform hyperparameter selection using a separate validation set because it is time consuming, but we examine the effects of the hyperparameters in Section 6.4; and in Section 6.5 we use a fixed set of hyperparameters that can achieve a good balance between runtime and ranking quality found empirically across multiple datasets. We list these methods below (their program options used in Section 6.5, see the footnote): \u2022 SVD: We use Java implementation of rank-based SVD toolkits 2,3. This version uses gradient pedigree to optimize the shunting-based loss function (14), as opposed to the quadriplexed loss function in SVD."}, {"heading": "6.3 Evaluation Measure", "text": "We evaluate the ranking quality by normalized discounted cumulative gain (NDCG) @ K [6], averaged across all users. Given a ranking of t-items i1, \u00b7 \u00b7 \u00b7, it and their fundamental relevance is calculated by items ri1, \u00b7 \u00b7, rit, the DCG @ K score (K \u2264 t) as DCG @ K (i1, \u00b7 \u00b7 \u00b7, it) = K-items preferred items \u2212 1 log2 (j + 1). (18) Then we sort the list of items in the decreasing order of relevance scores and get the list i-items 1, \u00b7 \u00b7, i-t items where ri-items 1, \u00b7 ri-t items are preferred, and this sorted list reaches the maximum DCG @ K score over all possible permutations. The NDCG score is defined as the normalized version of DG (DG \u00b7 Items), (NG) (NG): (NG @)."}, {"heading": "6.4 Effect of Parameter Selection", "text": "First, we examine the influence of the kD tree parameters on the performance of LDM, namely the number of nearest neighbors k and the maximum number of distance comparisons D. Fig. 1 shows the change in NDCG @ 10 in the variation of k and D on several small datasets (due to time constraints). In general, the ranking quality is much better for moderately large k, D values than for very small k, D values, but does not improve much if k and D. Therefore, we can use sufficiently large k, D values to obtain a good ranking quality, but not too large to be mathematically efficient. In the experimental comparison in Section 6.5, we set the parameters to k = 64 and D = 256. Next, we vary the hyperparameters in each of the four methods and simultaneously compare the ranking quality and runtime of LLCs, where good performance of a collaborative ranking method means achieving higher NDCG @ 10 in a shorter time."}, {"heading": "6.5 Results", "text": "Now we correct the hyperparameters as described in Section 6.2. Table 2 gives the runtime and NDCG @ 10 values for all compared methods on the larger datasets. LDM does not always reach the highest NDCG @ 10 values, but produces robust ranking quality with decent runtime (except MovieLens-10m, N = 50). For LCR and SVD, the time cost of larger datasets increases dramatically. AltSVM achieves superior ranking quality when the number of training ratings is high, but its performance depends on the number of iterations, which in turn depends on the dataset and the given tolerance parameter. We conclude that LDM is an overall competitive method that is efficient and robust compared to hyperparameters and the underlying datasets. In addition, LDM has special advantages when the information available is relatively sparse, i.e. when N is small, which we consider to be a more difficult problem than the cases with richer training information."}, {"heading": "7 Related Work", "text": "Both user-based and item-based collaborative filtering methods [4, 18] can be considered graph-based local label propagation methods, and the idea of discrete harmonic enhancement for semi-supervised learning in general was originally proposed in [24]. Graph-based methods have also recently been considered for matrix completion [7] and Top-N recommendations [23]. In light of all the existing work, we have presented a continuous harmonic label propagation extension formulation and a rigorously diverse learning algorithm for collaborative ranking."}, {"heading": "8 Conclusion and Discussion", "text": "In this thesis, we have proposed a novel perspective on graph-based methods of matrix completion and collaborative precedence. For each element, we consider the user-user graph as a partially labeled dataset (or vice versa), and our method propagates the known labels to the unlabeled graph nodes through the edges of the graph. The continuous harmonic expansion problem associated with the semi-supervised learning problem described above is defined on a user or item multiplicity, which is solved by a point-integrated method. Our formulation can be considered as minimizing the dimension of the user or item multiplicity, thus forming a smooth model for the user or item, but with higher complexity than a low-level matrix approximation. In addition, our method can be fully paralleled on distributed machines, since the linear systems that must be solved for all items are independent of each other, but with higher complexity than a low-level matrix approximation."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Commun. ACM, vol. 51, no. 1, pp. 117\u2013122, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "Commun. ACM, vol. 18, no. 9, pp. 509\u2013517, 1975.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1975}, {"title": "Learning collaborative information filters", "author": ["D. Billsus", "M.J. Pazzani"], "venue": "ICML \u201998: Proc. of the 15th Int. Conf. on Machine learning, 1998, pp. 46\u201354.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Empirical analysis of predictive algorithms for collaborative filtering", "author": ["J.S. Breese", "D. Heckerman", "C. Kadie"], "venue": "UAI \u201998: Proc. of the 14th Conf. on Uncertainty in Artificial Intelligence, 1998, pp. 43\u201352.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Item-based top-n recommendation algorithms", "author": ["M. Deshpande", "G. Karypis"], "venue": "ACM Trans. Inf. Syst., vol. 22, no. 1, pp. 143\u2013177, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Cumulated gain-based evaluation of IR techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM Trans. Inf. Syst., vol. 20, no. 4, pp. 422\u2013446, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Matrix completion on graphs", "author": ["V. Kalofolias", "X. Bresson", "M. Bronstein", "P. Vandergheynst"], "venue": "NIPS Workshop on \u201cOut of the Box: Robustness in High Dimension\u201d, 2014. 13", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluation of item-based top-n recommendation algorithms", "author": ["G. Karypis"], "venue": "CIKM \u201901: Proc. of the 10th Int. Conf. on Information and Knowledge Management, 2001, pp. 247\u2013254.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Local collaborative ranking", "author": ["J. Lee", "S. Bengio", "S. Kim", "G. Lebanon", "Y. Singer"], "venue": "WWW \u201914: Proc. of the 23rd Int. Conf. on World Wide Web, 2014, pp. 85\u201396.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Local low-rank matrix approximation", "author": ["J. Lee", "S. Kim", "G. Lebanon", "Y. Singer"], "venue": "ICML \u201913: Proc. of the 30th Int. Conf. on Machine learning, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "A comparative study of collaborative filtering algorithms", "author": ["J. Lee", "M. Sun", "G. Lebanon"], "venue": "2012, http://arxiv.org/abs/1205.3193.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D.G. Lowe"], "venue": "VISAPP: Proc. of the Int. Conf. on Computer Vision Theory and Applications, 2009, pp. 331\u2013340.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Recent advances in recommender systems and future directions", "author": ["X. Ning", "G. Karypis"], "venue": "Pattern Recognition and Machine Intelligence, ser. Lecture Notes in Computer Science. Springer, 2015, vol. 9124, pp. 3\u20139.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Are you using the right approximate nearest neighbor algorithm?", "author": ["S. O\u2019Hara", "B.A. Draper"], "venue": "WACV \u201913: Workshop on Applications of Computer Vision,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "An iterative regularization method for total variation-based image restoration", "author": ["S. Osher", "M. Burger", "D. Goldfarb", "J. Xu", "W. Yin"], "venue": "Multiscale Modeling & Simulation, vol. 4, no. 2, pp. 460\u2013489, 2005.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Low dimensional manifold model for image processing", "author": ["S. Osher", "Z. Shi", "W. Zhu"], "venue": "UCLA, Tech. Rep. CAM report 16-04, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Preference completion: Largescale collaborative ranking from pairwise comparisons", "author": ["D. Park", "J. Neeman", "J. Zhang", "S. Sanghavi", "I. Dhillon"], "venue": "ICML \u201915: Proc. of the 32th Int. Conf. on Machine learning, 2015, pp. 1907\u20131916.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["B. Sarwar", "G. Karypis", "J. Konstan", "J. Riedl"], "venue": "WWW \u201901: Proc. of the 10th Int. Conf. on World Wide Web, 2001, pp. 285\u2013295.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Convergence of the point integral method for the poisson equation on manifolds I: the Neumann boundary", "author": ["Z. Shi", "J. Sun"], "venue": "2014, http://arxiv.org/abs/1509.06458.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Harmonic extension", "author": ["Z. Shi", "J. Sun", "M. Tian"], "venue": "2015, http://arxiv.org/abs/1509.06458.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "2008, http://www.vlfeat.org/.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Nomad: Non-locking, stochastic multi-machine algorithm for asynchronous and decentralized matrix completion", "author": ["H. Yun", "H.-F. Yu", "C.-J. Hsieh", "S.V.N. Vishwanathan", "I. Dhillon"], "venue": "Proc. VLDB Endow., vol. 7, no. 11, pp. 975\u2013986, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Random walk models for top-n recommendation task", "author": ["Y. Zhang", "J.-q. Wu", "Y.-t. Zhuang"], "venue": "Journal of Zhejiang University SCIENCE A, vol. 10, no. 7, pp. 927\u2013936, 2009. 14", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "ICML \u201903: Proc. of the 20th Int. Conf. on Machine learning, 2003, pp. 912\u2013919.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "A fast parallel sgd for matrix factorization in shared memory systems", "author": ["Y. Zhuang", "W.-S. Chin", "Y.-C. Juan", "C.-J. Lin"], "venue": "RecSys \u201913: Proc. of the 7th ACM Conf. on Recommender Systems, 2013, pp. 249\u2013256. 15", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "Detailed surveys of this field can be found in [11, 13].", "startOffset": 47, "endOffset": 55}, {"referenceID": 12, "context": "Detailed surveys of this field can be found in [11, 13].", "startOffset": 47, "endOffset": 55}, {"referenceID": 17, "context": "Memory-based approaches include user-based and item-based recommendation [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": "In contrast, model-based methods are formulated as matrix completion problems which assume that the entire user-by-item rating matrix is low-rank [3], and the goal is to predict the missing ratings given the observed ratings.", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "Popular model-based methods such as regularized SVD [3] minimize the sum-of-squares error over all the observed ratings.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "The problem that places priority on the top recommended items rather than the absolute accuracy of predicted ratings is referred to as top-N recommendation [5], or more recently collaborative ranking [9, 17], and is our focus in this paper.", "startOffset": 156, "endOffset": 159}, {"referenceID": 8, "context": "The problem that places priority on the top recommended items rather than the absolute accuracy of predicted ratings is referred to as top-N recommendation [5], or more recently collaborative ranking [9, 17], and is our focus in this paper.", "startOffset": 200, "endOffset": 207}, {"referenceID": 16, "context": "The problem that places priority on the top recommended items rather than the absolute accuracy of predicted ratings is referred to as top-N recommendation [5], or more recently collaborative ranking [9, 17], and is our focus in this paper.", "startOffset": 200, "endOffset": 207}, {"referenceID": 23, "context": "We start with the matrix completion problem and formulate it as a series of semi-supervised learning problems, or in particular, harmonic extension problems on a manifold that can be solved by label propagation [24, 20].", "startOffset": 211, "endOffset": 219}, {"referenceID": 19, "context": "We start with the matrix completion problem and formulate it as a series of semi-supervised learning problems, or in particular, harmonic extension problems on a manifold that can be solved by label propagation [24, 20].", "startOffset": 211, "endOffset": 219}, {"referenceID": 19, "context": "Then the Laplace-Beltrami equation can be solved by a novel point integral method [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "To solve the above optimization problem, we first use the Bregman iteration to enforce the constraint [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "[19] If f \u2208 C3(M) is a smooth function on M, then for any x \u2208M, \u2016r(f)\u2016L2(M) = O(t ), (8)", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "However, it has been observed that the graph Laplacian is not consisitent in solving the harmonic extension problem [20, 16], and PIM gives much better results.", "startOffset": 116, "endOffset": 124}, {"referenceID": 15, "context": "However, it has been observed that the graph Laplacian is not consisitent in solving the harmonic extension problem [20, 16], and PIM gives much better results.", "startOffset": 116, "endOffset": 124}, {"referenceID": 23, "context": "The optimization problem we defined in (1) can be viewed as a continuous analog of the discrete harmonic extension problem [24], which we write in our notations:", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "This form of loss function considers all the possible pairwise rankings of items, which is different from the loss function in previous work on collaborative ranking [9, 17]: \u2211 j,j\u2032\u2208Ui L ( [aji \u2212 aj\u2032i]\u2212 [(fi)j \u2212 (fi)j\u2032 ] ) , (14)", "startOffset": 166, "endOffset": 173}, {"referenceID": 16, "context": "This form of loss function considers all the possible pairwise rankings of items, which is different from the loss function in previous work on collaborative ranking [9, 17]: \u2211 j,j\u2032\u2208Ui L ( [aji \u2212 aj\u2032i]\u2212 [(fi)j \u2212 (fi)j\u2032 ] ) , (14)", "startOffset": 166, "endOffset": 173}, {"referenceID": 15, "context": "Using differential geometry, we have the following formula [16].", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "The weight matrix W plays an important role in our algorithm as well as other graph-based approaches [24, 18, 10, 7].", "startOffset": 101, "endOffset": 116}, {"referenceID": 17, "context": "The weight matrix W plays an important role in our algorithm as well as other graph-based approaches [24, 18, 10, 7].", "startOffset": 101, "endOffset": 116}, {"referenceID": 9, "context": "The weight matrix W plays an important role in our algorithm as well as other graph-based approaches [24, 18, 10, 7].", "startOffset": 101, "endOffset": 116}, {"referenceID": 6, "context": "The weight matrix W plays an important role in our algorithm as well as other graph-based approaches [24, 18, 10, 7].", "startOffset": 101, "endOffset": 116}, {"referenceID": 17, "context": "We employ the typical user-user or item-item graph with cosine similarity used in existing memory-based approaches for recommendation [18].", "startOffset": 134, "endOffset": 138}, {"referenceID": 11, "context": "We use a binary rating matrix RB that records \u201crated or not-rated\u201d information (Algorithm 3, line 1-2), and determine the K nearest neighbors using an kd-tree based approximate nearest neighbor algorithm (line 3, line 5) [12].", "startOffset": 221, "endOffset": 225}, {"referenceID": 20, "context": "Second, we extended the VLFeat package [21] to enable building a kd-tree from a sparse data matrix (in our case, RB) and querying the tree with sparse vectors.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "kd-tree uses a space partitioning scheme for efficient neighbor search [2].", "startOffset": 71, "endOffset": 74}, {"referenceID": 11, "context": "For high-dimensional data, we employ the greedy way that chooses the most varying dimension for space partitioning at each step of building the tree [12], and the procedure terminates when each leaf partition has one data point.", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "Thus, the complexity of building the tree is not exponential, contrary to common understanding; and the practical performance of kd-tree can be very efficient and better than that of locality sensitive hashing [1, 14].", "startOffset": 210, "endOffset": 217}, {"referenceID": 13, "context": "Thus, the complexity of building the tree is not exponential, contrary to common understanding; and the practical performance of kd-tree can be very efficient and better than that of locality sensitive hashing [1, 14].", "startOffset": 210, "endOffset": 217}, {"referenceID": 17, "context": "Also the cosine similarity for two data points with incomplete information is defined by using the co-rated items only (Algorithm 3, line 8-9) [18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 8, "context": "\u2022 LCR: Local collaborative ranking [9], as implemented in the PREA toolkit4.", "startOffset": 35, "endOffset": 38}, {"referenceID": 16, "context": "\u2022 AltSVM: Alternating support vector machine [17], as implemented in the collranking package5.", "startOffset": 45, "endOffset": 49}, {"referenceID": 5, "context": "3 Evaluation Measure We evaluate the ranking quality by normalized discounted cumulative gain (NDCG) @K [6], averaged over all the users.", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "In contrast to previous work on top-N recommender systems [8, 5], we discourage the use of Precision@K in the context of collaborative ranking for recommender systems, which measures the proportion of actually rated items out of the top K items in the ranked list of all the items in the data set.", "startOffset": 58, "endOffset": 64}, {"referenceID": 4, "context": "In contrast to previous work on top-N recommender systems [8, 5], we discourage the use of Precision@K in the context of collaborative ranking for recommender systems, which measures the proportion of actually rated items out of the top K items in the ranked list of all the items in the data set.", "startOffset": 58, "endOffset": 64}, {"referenceID": 3, "context": "Both user-based and item-based collaborative filtering [4, 18] can be considered as graph-based local label propagation methods.", "startOffset": 55, "endOffset": 62}, {"referenceID": 17, "context": "Both user-based and item-based collaborative filtering [4, 18] can be considered as graph-based local label propagation methods.", "startOffset": 55, "endOffset": 62}, {"referenceID": 23, "context": "The idea of discrete harmonic extension for semi-supervised learning in general was originally proposed in [24].", "startOffset": 107, "endOffset": 111}, {"referenceID": 6, "context": "Graph-based methods were recently considered for matrix completion [7] and top-N recommendation [23] as well.", "startOffset": 67, "endOffset": 70}, {"referenceID": 22, "context": "Graph-based methods were recently considered for matrix completion [7] and top-N recommendation [23] as well.", "startOffset": 96, "endOffset": 100}, {"referenceID": 21, "context": "An important direction is to further improve the efficiency of the algorithm to be comparable with recent large-scale matrix completion methods [22, 25].", "startOffset": 144, "endOffset": 152}, {"referenceID": 24, "context": "An important direction is to further improve the efficiency of the algorithm to be comparable with recent large-scale matrix completion methods [22, 25].", "startOffset": 144, "endOffset": 152}], "year": 2016, "abstractText": "We present a new perspective on graph-based methods for collaborative ranking for recommender systems. Unlike user-based or item-based methods that compute a weighted average of ratings given by the nearest neighbors, or low-rank approximation methods using convex optimization and the nuclear norm, we formulate matrix completion as a series of semi-supervised learning problems, and propagate the known ratings to the missing ones on the user-user or item-item graph globally. The semi-supervised learning problems are expressed as LaplaceBeltrami equations on a manifold, or namely, harmonic extension, and can be discretized by a point integral method. We show that our approach does not impose a low-rank Euclidean subspace on the data points, but instead minimizes the dimension of the underlying manifold. Our method, named LDM (low dimensional manifold), turns out to be particularly effective in generating rankings of items, showing decent computational efficiency and robust ranking quality compared to state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}