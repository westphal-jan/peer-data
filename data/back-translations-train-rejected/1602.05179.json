{"id": "1602.05179", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation", "abstract": "This work follows Bengio and Fischer (2015) in which theoretical foundations were laid to show how iterative inference can backpropagate error signals. Neurons move their activations towards configurations corresponding to lower energy and smaller prediction error: a new observation creates a perturbation at visible neurons that propagates into hidden layers, with these propagated perturbations corresponding to the back-propagated gradient. This avoids the need for a lengthy relaxation in the positive phase of training (when both inputs and targets are observed), as was believed with previous work on fixed-point recurrent networks. We show experimentally that energy-based neural networks with several hidden layers can be trained at discriminative tasks by using iterative inference and an STDP-like learning rule. The main result of this paper is that we can train neural networks with 1, 2 and 3 hidden layers on the permutation-invariant MNIST task and get the training error down to 0.00%. The results presented here make it more biologically plausible that a mechanism similar to back-propagation may take place in brains in order to achieve credit assignment in deep networks. The paper also discusses some of the remaining open problems to achieve a biologically plausible implementation of backprop in brains.", "histories": [["v1", "Tue, 16 Feb 2016 20:46:51 GMT  (300kb,D)", "http://arxiv.org/abs/1602.05179v1", null], ["v2", "Wed, 24 Feb 2016 11:13:08 GMT  (301kb,D)", "http://arxiv.org/abs/1602.05179v2", null], ["v3", "Tue, 20 Sep 2016 16:15:26 GMT  (436kb,D)", "http://arxiv.org/abs/1602.05179v3", null], ["v4", "Mon, 26 Sep 2016 09:55:15 GMT  (439kb,D)", "http://arxiv.org/abs/1602.05179v4", null], ["v5", "Tue, 28 Mar 2017 18:31:11 GMT  (388kb,D)", "http://arxiv.org/abs/1602.05179v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["benjamin scellier", "yoshua bengio"], "accepted": false, "id": "1602.05179"}, "pdf": {"name": "1602.05179.pdf", "metadata": {"source": "CRF", "title": "Towards a Biologically Plausible Backprop", "authors": ["Benjamin Scellier", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It has been hypothesized several times (Hinton and Sejnowski, 1986; Friston and Stephan, 2011) that, faced with a state of sensory information (current and past interventions), neurons collectively draw a conclusion, i.e. that they are moving toward a complete theory of learning in deep biological networks that represent a credible configuration of internal neurons (hidden units or latent variables), focusing on a simple constellation of sensory data."}, {"heading": "2 Previous work (revisited)", "text": "In this section we present the model first presented in Bengio and Fischer (2015); Bengio et al. (2015a, b). The model is a continuous time process {(\u03b8 (t), s (t))): t-R}, where, as is common in neural networks, s is the vector that represents the state of the units and \u03b8 = (W, b) represents the set of free parameters that includes the synaptic weights Wi, j and the neuron distortions bi (which control the activation threshold for each unit i and also correspond to the weight of a virtual constant input).The units are continuously evaluated and would correspond to the averaged voltage potential over time, peaks and possibly neurons in the same miniature."}, {"heading": "2.1 Neural computation as leaky integrator", "text": "As usual in models of biological neurons, we assume that the neurons perform a leaky temporal integration of their past inputs. It is assumed that the temporal evolution of the neurons follows the leaky integration equation dt = Ri (s) \u2212 si \u03c4, (1) where Ri (s) represents the pressure on the neuron i from the rest of the network (and the value to which si would converge if Ri (s) did not change), while \u03c4 is a characteristic integration time. Furthermore, it is assumed that Ri (s) belongs to the formRi (s).j 6 = i Wj, i\u03c1 (sj) + bi. (2)"}, {"heading": "2.2 Neural computation does inference: going down the energy", "text": "One hypothesis in computer neuroscience is that biological neurons perform iterative connections (j = j = j = j = 6). In our model, this means that the hidden units of the network gradually move towards configurations that are more likely because the sensory input and according to the current \"world model\" are associated with the parameters of the model. A class of models based on iterative inferences is the class of energy-based models in which an energy function E (\u03b8, s) drives the states of the units according to a dynamic of the formulas dt \u2212 mps E \u00b7 s. With this dynamic, the network moves spontaneously towards energy-based configurations. Consider the following energy function investigated by Bengio et \u2212 al. (2015a): E (\u03b8, s): = 12 x i \u2212 1 = 2 = Dynamic i = 6 = j = 7 (Wi = 4)."}, {"heading": "2.3 Early inference recovers backpropagation", "text": "In Bengio and Fischer (2015) it is shown how the iterative conclusion also includes retrograde error signals in a multilayered network (\u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = in a multilayered network). As in previous work inspired by the Boltzmann machine, we will use the terminology of the \"positive phase\" and \"negative phase\" to distinguish two phases of the training, where the positive phase is fully observed (or pinched) with v and the negative phase with v is partially or completely not observed. In fact, they correspond to the network following the course of the energy function, but with or without a term that can drive some or all visible units towards the value of external signals (inputs and outputs of the network). To this end, we introduce a new term into the energy function that corresponds to the prediction error and that can push visible units towards the values."}, {"heading": "2.4 STDP learning rule", "text": "Spike-Timing Dependent Plasticity (STDP) is considered the main form of synaptic change in neurons (Markram and Sakmann, 1995; Gerstner et al., 1996) and it refers the expected change in synaptic weights to the time difference between post-synaptic spikes and pre-synaptic spikes. Although it is the result of experimental observations in biological neurons, its generalization (outside the experimental setups in which it was measured) and interpretation as part of a learning process that could explain learning in deep networks where more exploration is needed. Experimental results in Bengio et al. (2015a) show that when the weights change, the STDP (si) dsj dt dt dt dt dt dt dt dt dt. (19) then we recover the biological observations made in Bi and Poo (2001) about Spike-Timing Dependent."}, {"heading": "2.5 Deriving Contrastive Hebbian Learning from the STDP learning rule", "text": "A link between back propagation and contrastive Hebbic learning was already shown in Xie and Seung (2003). We have seen in subsection 2.3 a link between iterative inference and back propagation. In this subsection we show a link between the STDP learning rule and contrastive Hebbic learning in energy-based models. In a differential form we can call the state of the network s = (x, h, y), where x, h and y are the input, hidden and output units. Consider the following procedure: 1. negative phase (\u03b2x = +) and \u03b2y = 0): Let the network relax and fix itself to a fixed point s \u2212 = (xdata, h \u2212, y \u2212); do not update the weights during this phase \u2212 \u2212 b \u2212.- (\u03b2x, \u03b2y) 1. phase with fixed and \u03b2y = i weight adjustments (xdata, h) \u2212."}, {"heading": "3 Link to Recurrent Back-Propagation and Getting Rid of the Positive Phase Relaxation", "text": "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent backward propagation (for recurrent networks converging to a fixed point to obtain their prediction), and with the work of Xie and Seung (2003) on the backward propagation of contrastive Hebbic learning, this may indicate an apparent contradiction: these earlier works all require an iterative relaxation in the positive phase (after the goal has been observed and pressed on the output units), while we suggest here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients - we present an explanation for this discrepancy here.We have seen in Section 2.3 that the early steps of consequence in the energy-based model only restores the backward propagation of errors through the network. To obtain a complete backward propagation algorithm, we also show that such a short forward reference, combined with our STDP, represents an increase in enrichment rule."}, {"heading": "3.1 Reformulation of the problem", "text": "Remember that we write the state s = (\u03b2, h, y), in which \u03b2 = \u03b2 = \u03b2 (\u03b2) and \u03b2 = \u03b2 (\u03b2) are the inputs, hiddens and outputs (\u03b2). The \"negative phase\" corresponds to the choice of \u03b2 = (\u03b2x, \u03b2y) with \u03b2x = + \u221e and \u03b2y = 0 for the \"generalized energy function\" F. We will argue that after the network is set to s \u2212 s, the right thing to do from the perspective of machine learning is not a positive phase with \u03b2y = + \u221e, but a positive phase with a \"small\" \u03b2y > 0.We can present the training goal as the following limited optimization problem: Find min, s C (s), be subject to the solution of E (s) = 0.As usual for limited optimization problems, we will introduce the Lagrangian method: L (\u03b8, s, s) and we will introduce this solution."}, {"heading": "4 Implementation of the model", "text": "In this section, we provide experimental evidence that our model is traceable, and analyze the influence of various factors on the training time, generalization error, and biological plausibility of the model. We show that the STDP learning rule (21) can be used to train a neural network with several layers of hidden units to classify the MNIST digitalizations. For each example, the training takes place in two phases. During the first phase (which we call negative phase analogous to Boltzmann machines), we strain x = xdata and let the network relax until it settles down to a (negative) fixed point s \u2212 = (xdata, h \u2212, y \u2212). During the negative phase, the weights W remain fixed. Then, during a second phase (referred to as positive phase or \u03b2 phase as discussed in Section 3), we drive to ydata according to updates y (1 \u2212) y + ydata and let the network relax \"a bit\" (see the next section for the duration of this discussion about relaxation)."}, {"heading": "4.1 Finite difference method", "text": "The obvious way to implement the iterative follow-up procedure according to (5) is to discredit time into short periods and to update each unit si according to tosi \u2190 si \u2212 \u2202 E \u2202 si = (1 \u2212) si + Ri (s), that is, to shift time into short periods of time. (39) This is simply a step of the descent of the gradient to energy, with step size, as described in Bengio and Fischer (2015). For our experiments (see subsection 4.2) we choose the hard sigmoid as an activation function, i.e. it is simply a step of descent to energy, with step size, as described in Bengio and Fischer (2015)."}, {"heading": "4.2 Implementation details and results", "text": "We train multilayer neural networks with 1, 2 and 3 hidden layers. Each hidden layer has 500 units. There is no connection within a layer and no connections between layers. For the efficiency of the experiments we use mini-batches with 20 training examples and use = 0.5 as step size for each iteration. Only a global weight update is performed at the end of the positive phase according to Eq. 27, rather than multiple weight updates after each step of the positive phase (Eq. 26).To address the problem of relaxation of the long negative phase and to speed up the simulations, we use \"persistent particles\" for the latent variables to use the previous fixed point configuration for a specific example of the next negative phase relaxation. This means that for each training example in the dataset we store the state of the hidden layers at the end of the negative phase and the negative layers at the end of the negative phase."}, {"heading": "4.3 Speeding up the relaxation", "text": "In this subsection, we introduce an algorithm to accelerate relaxation in both the negative phase and the positive phase. By using the finite difference method (subsection 4.1), the number of iterations required to achieve a negative fixed point with sufficient accuracy becomes very large as the number of hidden layers increases. The algorithm we are designing here allows us to train our networks 5 times as fast by accelerating convergence towards a fixed point. This algorithm is less biologically plausible, but it establishes a connection with forward and backward propagation in feedforward networks, as well as Gibbs sampling in Boltzmann machines. The spirit of this algorithm is similar to the path of ordinary Gibbs sampling (where only one unit is updated at a time) to block Gibbs sampling (where an entire layer is updated in parallel, as the layer below and the solution we can analyze the layer below and the face of the layer)."}, {"heading": "5 Discussion and future work", "text": "This year, it will be able to find a solution that paves the way for the future, to pave the way for the future."}, {"heading": "Acknowledgments", "text": "The authors thank Akram Erraqabi, Samira Shabanian and Asja Fischer for feedback and discussion, NSERC, CIFAR, Samsung and Canada Research Chairs for funding and Compute Canada for computing resources."}], "references": [{"title": "A learning rule for asynchronous perceptrons with feedback in a combinatorial environment", "author": ["L.B. Almeida"], "venue": "IEEE International Conference on Neural Networks,", "citeRegEx": "Almeida,? \\Q1987\\E", "shortCiteRegEx": "Almeida", "year": 1987}, {"title": "Why are deep nets reversible: a simple theory, with implications for training", "author": ["S. Arora", "Y. Liang", "T. Ma"], "venue": "Technical report,", "citeRegEx": "Arora et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "Early inference in energy-based models approximates back-propagation", "author": ["Y. Bengio", "A. Fischer"], "venue": "arXiv preprint arXiv:1510.02777", "citeRegEx": "Bengio and Fischer,? \\Q2015\\E", "shortCiteRegEx": "Bengio and Fischer", "year": 2015}, {"title": "STDP as presynaptic activity times rate of change of postsynaptic activity", "author": ["Y. Bengio", "T. Mesnard", "A. Fischer", "S. Zhang", "Y. Wu"], "venue": "arXiv preprint arXiv:1509.05936", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Towards biologically plausible deep learning", "author": ["Y. Bengio", "Lee", "D.-H", "J. Bornschein", "Z. Lin"], "venue": "arXiv preprint arXiv:1502.04156", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment", "author": ["P. Berkes", "G. Orban", "M. Lengyel", "J. Fiser"], "venue": null, "citeRegEx": "Berkes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berkes et al\\.", "year": 2011}, {"title": "Synaptic modification by correlated activity: Hebb\u2019s postulate revisited", "author": ["Bi", "G.-q", "Poo", "M.-m"], "venue": "Annual review of neuroscience,", "citeRegEx": "Bi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bi et al\\.", "year": 2001}, {"title": "A neuronal learning rule for sub-millisecond temporal coding", "author": ["W. Gerstner", "R. Kempter", "J.L. van Hemmen", "H. Wagner"], "venue": null, "citeRegEx": "Gerstner et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Gerstner et al\\.", "year": 1996}, {"title": "Learning and releaming in boltzmann machines. Parallel distributed processing: Explorations in the microstructure", "author": ["G.E. Hinton", "T.J. Sejnowski"], "venue": "of cognition,", "citeRegEx": "Hinton and Sejnowski,? \\Q1986\\E", "shortCiteRegEx": "Hinton and Sejnowski", "year": 1986}, {"title": "Action potentials propagating back into dendrites triggers changes in efficacy", "author": ["H. Markram", "B. Sakmann"], "venue": "Soc. Neurosci. Abs,", "citeRegEx": "Markram and Sakmann,? \\Q1995\\E", "shortCiteRegEx": "Markram and Sakmann", "year": 1995}, {"title": "Training recurrent networks online without backtracking", "author": ["Y. Ollivier", "C. Tallec", "G. Charpiat"], "venue": "Technical report,", "citeRegEx": "Ollivier et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ollivier et al\\.", "year": 2015}, {"title": "Generalization of back-propagation to recurrent neural networks", "author": ["F.J. Pineda"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Pineda,? \\Q1987\\E", "shortCiteRegEx": "Pineda", "year": 1987}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Training restricted boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Tieleman,? \\Q2008\\E", "shortCiteRegEx": "Tieleman", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A"], "venue": "J. Machine Learning Res.,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Spike-based learning rules and stabilization of persistent neural activity", "author": ["X. Xie", "H.S. Seung"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Xie and Seung,? \\Q2000\\E", "shortCiteRegEx": "Xie and Seung", "year": 2000}, {"title": "Equivalence of backpropagation and contrastive hebbian learning in a layered network", "author": ["X. Xie", "H.S. Seung"], "venue": "Neural computation,", "citeRegEx": "Xie and Seung,? \\Q2003\\E", "shortCiteRegEx": "Xie and Seung", "year": 2003}], "referenceMentions": [{"referenceID": 2, "context": "Abstract This work follows Bengio and Fischer (2015) in which theoretical foundations were laid to show how iterative inference can backpropagate error signals.", "startOffset": 27, "endOffset": 53}, {"referenceID": 8, "context": "It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.", "startOffset": 40, "endOffset": 116}, {"referenceID": 5, "context": "It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.", "startOffset": 40, "endOffset": 116}, {"referenceID": 2, "context": "When target values for output units are observed, this creates a perturbation in the network that we show corresponds to propagating gradients into hidden layers, as initially proposed by Bengio and Fischer (2015). We show how this can be used to perform gradient descent on the prediction error, when the updates correspond to the STDP-like rule proposed by Bengio et al.", "startOffset": 188, "endOffset": 214}, {"referenceID": 2, "context": "When target values for output units are observed, this creates a perturbation in the network that we show corresponds to propagating gradients into hidden layers, as initially proposed by Bengio and Fischer (2015). We show how this can be used to perform gradient descent on the prediction error, when the updates correspond to the STDP-like rule proposed by Bengio et al. (2015a). Several points, elaborated at the end of this paper, still need to be elucidated before a complete theory of learning, inference and credit assignment is elaborated that is both biologically plausible and makes sense from a machine learning point of view for whole networks (with global optimization of the whole network and not being limited to learning of visible neurons that receive a target).", "startOffset": 188, "endOffset": 381}, {"referenceID": 2, "context": "In this section, we present the model first introduced in Bengio and Fischer (2015); Bengio et al.", "startOffset": 58, "endOffset": 84}, {"referenceID": 3, "context": "Consider the following energy function, studied by Bengio et al. (2015a):", "startOffset": 51, "endOffset": 73}, {"referenceID": 2, "context": "3 Early inference recovers backpropagation In Bengio and Fischer (2015), it is shown how iterative inference can also backpropagate error signals in a multi-layer network.", "startOffset": 46, "endOffset": 72}, {"referenceID": 2, "context": "A related demonstration was made by Bengio and Fischer (2015) in the case of a regular feedforward network and the discrete-time setting, relying on small difference approximations.", "startOffset": 36, "endOffset": 62}, {"referenceID": 9, "context": "4 STDP learning rule Spike-Timing Dependent Plasticity (STDP) is believed to be the main form of synaptic change in neurons (Markram and Sakmann, 1995; Gerstner et al., 1996) and it relates the expected change in synaptic weights to the timing difference between postsynaptic spikes and presynaptic spikes.", "startOffset": 124, "endOffset": 174}, {"referenceID": 7, "context": "4 STDP learning rule Spike-Timing Dependent Plasticity (STDP) is believed to be the main form of synaptic change in neurons (Markram and Sakmann, 1995; Gerstner et al., 1996) and it relates the expected change in synaptic weights to the timing difference between postsynaptic spikes and presynaptic spikes.", "startOffset": 124, "endOffset": 174}, {"referenceID": 3, "context": "Experimental results in Bengio et al. (2015a) show that if the weight changes satisfy dWij dt \u221d \u03c1(si) dsj dt , (19)", "startOffset": 24, "endOffset": 46}, {"referenceID": 3, "context": "In this paper, we change the STDP learning rule from Bengio et al. (2015a) into dWij dt \u221d \u03c1(si) d\u03c1(sj) dt .", "startOffset": 53, "endOffset": 75}, {"referenceID": 15, "context": "Note that this form of the STDP update rule is the same as the one studied by Xie and Seung (2000). An advantage of this form of the STDP update rule is that it leads to a more natural view of the update for the tied symmetric value Wij = Wji.", "startOffset": 78, "endOffset": 99}, {"referenceID": 15, "context": "5 Deriving Contrastive Hebbian Learning from the STDP learning rule A connection between Backpropagation and Contrastive Hebbian Learning was shown previously in Xie and Seung (2003). We have seen in subsection 2.", "startOffset": 162, "endOffset": 183}, {"referenceID": 11, "context": "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients.", "startOffset": 58, "endOffset": 87}, {"referenceID": 0, "context": "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients.", "startOffset": 58, "endOffset": 87}, {"referenceID": 0, "context": "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients.", "startOffset": 73, "endOffset": 250}, {"referenceID": 0, "context": "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients.", "startOffset": 73, "endOffset": 566}, {"referenceID": 0, "context": "Comparing the analysis presented here with the early work (Pineda, 1987; Almeida, 1987) on recurrent back-propagation (for recurrent networks that converge to a fixed point to obtain their prediction) as well as with the work of Xie and Seung (2003) on the back-propagation interpretation of contrastive Hebbian learning may suggest an apparent contradiction: these earlier work all require an iterative relaxation in the positive phase (after the target y is observed and clamped on the output units), whereas we are proposing here and in Bengio and Fischer (2015) that the early part of this relaxation is sufficient to obtain the required gradients. We present an explanation for this discrepency here. We have seen in Section 2.3 that the early steps of inference in the energy-based model recovers backpropagation of errors through the network. To obtain a full back-propagation algorithm we also show that such a short inference, combined with our STDP update rule, gives rise to stochastic gradient descent on the prediction error. To achieve this, we will consider a value of \u03b2 that is only barely greater than zero, which corresponds to only nudging the output units towards a value that would reduce prediction error. Besides computational efficiency, another reason for avoiding the positive phase relaxation suggested by Eq. 32 is that it does not follow exactly the same kind of dynamics as the negative phase relaxation because it uses a linearization of the neural activation rather than the fully non-linear activation. From a biological plausibility point of view, having to use a different kind of hardware and computation for the \u201cforward\u201d and \u201cbackward\u201d phases is not satisfying. This issue is \u201caddressed\u201d by Xie and Seung (2003) by assuming that the feedback weights are tiny compared to the feedforward weights (thus making the feedback weights only indice infinitesimal perturbations on the hidden units\u2019 state).", "startOffset": 73, "endOffset": 1750}, {"referenceID": 10, "context": "(31) Note that solving the above equation in \u03bb\u2217 can in principle be achieved by a fixed point iteration in a linearized form of the recurrent network, and this is the method proposed by Pineda (1987); Almeida (1987): \u2202C \u2202s (s) + (I \u2212R(s))\u03bb = 0\u21d2 \u03bb = R(s)\u03bb \u2212 \u2202C \u2202s .", "startOffset": 186, "endOffset": 200}, {"referenceID": 0, "context": "(31) Note that solving the above equation in \u03bb\u2217 can in principle be achieved by a fixed point iteration in a linearized form of the recurrent network, and this is the method proposed by Pineda (1987); Almeida (1987): \u2202C \u2202s (s) + (I \u2212R(s))\u03bb = 0\u21d2 \u03bb = R(s)\u03bb \u2212 \u2202C \u2202s .", "startOffset": 201, "endOffset": 216}, {"referenceID": 3, "context": "Hence we have found that (a) stochastic gradient descent of the prediction error in a recurrent network with clamped inputs can be achieved with a very brief relaxation (just enough for signals to propagate from outputs into all the hidden layers) in which the output units are slightly driven towards their target and that (b) the update corresponds to the STDP update rule from Bengio et al. (2015a) as well as (when incorporating the symmetry constraint) to the contrastive Hebbian learning update.", "startOffset": 380, "endOffset": 402}, {"referenceID": 2, "context": "This is simply one step of gradient descent on the energy, with step size , as described in Bengio and Fischer (2015).", "startOffset": 92, "endOffset": 118}, {"referenceID": 2, "context": "This issue was already mentioned in Bengio and Fischer (2015).", "startOffset": 36, "endOffset": 62}, {"referenceID": 13, "context": "This method is similar in spirit to the PCD algorithm for sampling from other energy-based models like the Boltzmann machine (Tieleman, 2008).", "startOffset": 125, "endOffset": 141}, {"referenceID": 14, "context": "Encouraging cues from the observation that denoising autoencoders without tied weights often end up learning symmetric weights (Vincent et al., 2010).", "startOffset": 127, "endOffset": 149}, {"referenceID": 1, "context": "Another encouraging piece of evidence, also linked to autoencoders, is the theoretical result from Arora et al. (2015), showing that the symmetric solution minimizes the autoencoder reconstruction error between two successive layers of rectifying (ReLU) units.", "startOffset": 99, "endOffset": 119}, {"referenceID": 12, "context": "Injecting noise might also help to address the overfitting observed in our experiments, just like dropout (Srivastava et al., 2014) is acting as a powerful regularizer for deep neural networks.", "startOffset": 106, "endOffset": 131}, {"referenceID": 2, "context": "As pointed out by Bengio and Fischer (2015), if appropriate noise is injected in the differential equation that makes the state go down the energy, we obtain a Langevin Monte-Carlo Markov chain that samples from a probability distribution associated with that energy function.", "startOffset": 18, "endOffset": 44}, {"referenceID": 2, "context": "As pointed out by Bengio and Fischer (2015), if appropriate noise is injected in the differential equation that makes the state go down the energy, we obtain a Langevin Monte-Carlo Markov chain that samples from a probability distribution associated with that energy function. Injecting noise might also help to address the overfitting observed in our experiments, just like dropout (Srivastava et al., 2014) is acting as a powerful regularizer for deep neural networks. Also connected to the above question of having to wait for a negative phase fixed point is the question of timevarying input. Although this work makes back-propagation more plausible for the case of a static input, the brain is a recurrent network with time-varying inputs, and back-propagation through time seems even less plausible than static back-propagation. An encouraging direction is that proposed by Ollivier et al. (2015), which shown that computationally efficient estimators of the gradient can be obtained using a forward method (online estimation of the gradient), which avoids to need to store all past states in training sequences, at the price of a noisy estimator of the gradient.", "startOffset": 18, "endOffset": 903}], "year": 2016, "abstractText": "This work follows Bengio and Fischer (2015) in which theoretical foundations were laid to show how iterative inference can backpropagate error signals. Neurons move their activations towards configurations corresponding to lower energy and smaller prediction error: a new observation creates a perturbation at visible neurons that propagates into hidden layers, with these propagated perturbations corresponding to the back-propagated gradient. This avoids the need for a lengthy relaxation in the positive phase of training (when both inputs and targets are observed), as was believed with previous work on fixed-point recurrent networks. We show experimentally that energy-based neural networks with several hidden layers can be trained at discriminative tasks by using iterative inference and an STDP-like learning rule. The main result of this paper is that we can train neural networks with 1, 2 and 3 hidden layers on the permutation-invariant MNIST task and get the training error down to 0.00%. The results presented here make it more biologically plausible that a mechanism similar to back-propagation may take place in brains in order to achieve credit assignment in deep networks. The paper also discusses some of the remaining open problems to achieve a biologically plausible implementation of backprop in brains.", "creator": "LaTeX with hyperref package"}}}