{"id": "1602.05350", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2016", "title": "Relative Error Embeddings for the Gaussian Kernel Distance", "abstract": "A reproducing kernel can define an embedding of a data point into an infinite dimensional reproducing kernel Hilbert space (RKHS). The norm in this space describes a distance, which we call the kernel distance. The random Fourier features (of Rahimi and Recht) describe an oblivious approximate mapping into finite dimensional Euclidean space that behaves similar to the RKHS. We show in this paper that for the Gaussian kernel the Euclidean norm between these mapped to features has $(1+\\epsilon)$-relative error with respect to the kernel distance. When there are $n$ data points, we show that $O((1/\\epsilon^2) \\log(n))$ dimensions of the approximate feature space are sufficient and necessary.", "histories": [["v1", "Wed, 17 Feb 2016 09:35:08 GMT  (643kb)", "https://arxiv.org/abs/1602.05350v1", null], ["v2", "Tue, 20 Sep 2016 17:13:17 GMT  (714kb)", "http://arxiv.org/abs/1602.05350v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["di chen", "jeff m phillips"], "accepted": false, "id": "1602.05350"}, "pdf": {"name": "1602.05350.pdf", "metadata": {"source": "CRF", "title": "Relative Error Embeddings of the Gaussian Kernel Distance", "authors": ["Di Chen", "Jeff M. Phillips"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 2.05 350v 2 [cs.L G] 20 Sep 20 and have a diameter limited by M, then we show that O (d / \u03b52) logM) dimensions are sufficient, and that these are many, up to and including log (1 / \u03b5) factors. We empirically confirm that relative errors for kernel PCA are actually preserved by using these approximate feature cards."}, {"heading": "1 Introduction", "text": "The kernel trick in machine learning enables non-linear analysis of data using many techniques such as PCA and SVM, which were originally designed for linear analysis. < The \"trick\" is that these methods only access data through inner products between data points, here we are looking at this non-linear inner product defined by a kernel K (\u00b7, \u00b7). Now, however, one can calculate the n \u00b7 n gram matrix G of all paired inner products; this is called Gi, j = K (xi, xj) for all input data set X. Then the analysis can continue with only the gram matrix G. However, for large data sets, the construction of this n \u00b7 n matrix is a computational bottleneck, so methods have been developed to lift n data points P into a high-dimensional space (but where m \u00b2 n) so that the Euclidean dot product in this space approaches."}, {"heading": "1.1 Existing Properties of Gaussian Kernel Embeddings", "text": "[16] defined two approximate embedding functions: either Rd (Rm) or i (> i) (Rd (Rm). Only the former appears in the final version of the paper, but the latter is also commonly used in literature [19]. Both features use random variables (Rd) that are drawn evenly from the Fourier transformation of the kernel function. In the first case, they define m functions of the form f-i (x) = cos (T i x + \u03b3i), with the f-Unif (0, 2) representing a random shift universally from the interval (0, 2 x). Applying each f-i function to a datapoint x x x x x (x) yields the ith coordinates of the x-shaped x-shaped x-shaped x-shaped x-shaped x-shaped x-shaped x-shaped x)."}, {"heading": "1.2 Our Results", "text": "In this paper, we show that we are capable of capturing the data we need for each x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "2 Basic Bounds", "text": "We can first examine some of the properties of the Gaussian kernel and the theory of random Fourier features (< p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p p = p = p = p = p = p = p = p = p = p = p"}, {"heading": "3 Approximations and Relation to \u21132 on Small Distances", "text": "For the rest of the work, it will be convenient to define the vector between any pair of points x, y, and x. In this section, we will consider the alternative case in which the number of points 1, 2, and 2, and show that DK is actually close to DK, and at different degrees also approximately 2.As a warming, and as similarly observed in [15], a simple Taylor expansion, if the number of points 1, 2, and 4 implies that the number of points 1, 2, and 2 is equal."}, {"heading": "3.1 Lower Bounds, Based on Very Small \u2016x\u2212 y\u2016", "text": "Lemma 3.1 implies that if \"X \u2212 y\" is small, \"K\" (x, y) behaves like a Johnson-Lindenstrauss (JL) random projection of \"X \u2212 y\" and we can refer to known JL sub-boundaries. In particular, Lemma 3.1 implies that if the input dataset \"X-Rd\" is in a sufficiently small neighborhood of \"zero,\" the relative error is preserved only if \"X \u2212 y\" sub-boundaries (x \u2212 y) sub-boundaries (x \u2212 y) exist, (x \u2212 y) sub-boundaries (x \u2212 y) sub-boundaries for all x, y sub-boundaries for all x-boundaries, and for all arbitrary boundaries. Hence, for arbitrary x, y X-X, and Lemm sub-boundaries for these sub-boundaries."}, {"heading": "4 Relative Error Bounds For Small Distances and Small Data Sets", "text": "The Taylor expansion in the equation (4) and the additive errors about Lemma 2.1 are only sufficient to provide us with the limits for both sides (2). In this section, we must use a more powerful technique or a moment to generate functions to fill this gap. In particular, we will provide a more precise limitation of the moment generation function of 1 \u2212 cos (2, 2, 3, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5"}, {"heading": "5 Relative Error Bounds for Low Dimensions and Diameter", "text": "A common approach in sub-ranges replaces n with the size of a sufficiently fine net value. If the error is tied to the net points, the warranty is extended to the \"gaps\" in between. To limit the size of the gaps, we derive the Lipschitz constant from DK (\u00b7) 2 in relation to the vector (not individual points in Rd). Contrary to previous work regarding the norms of Euclidean space, the Gaussian kernel distance is not linear; nevertheless, we find that it is almost linearly close to 0, and use a special construct that allows us to take advantage of this capability."}, {"heading": "6 Lower Bounds for Low Dimensions", "text": "If a recent essay [18] implies that even for small d, DK, there is no (1 + \u03b5) approximate DK, unless an explicit and general lower limit is specified as a function of M and d, which refers to our upper limit to an O (log 1\u03b5) factor.First, we need the following overall result ([2] Theorem 9.3) with respect to embedding in the lower limit. Let B have a n \u00b7 n real matrix with bi, i = 1 for all i and | bi for all i = j. If the precedence of B r, and 1 \u00b0 n < \u03b5 n < 1 / 2, then r \u2264 2 log (1 / 2 log) log n) log n) log n)))). Geometrically, r is the minimum number of dimensions that a set of n can contain almost orthogonal vectors."}, {"heading": "7 Empirical Demonstration of Relative Error", "text": "We show that relative errors actually result from \u03c6 kernel embedding in two ways. First, we show relative error limits for kernel PCA. Second, we show this explicitly for pairs of embedded distances."}, {"heading": "7.1 Relative Error for Kernel PCA", "text": "We look at two ways PCA is tailored to the USPS data: the first n = 2000 data points in Rd = 256, the first n / 10 data points in each digit. \u2212 The first way we embed each point in Rm is to generate a n \u00b7 m matrix G of all internal products, and then use the uppermost k eigenvectors to describe the best subspace of RKHS to represent the data. \u2212 The second way we embed each point in Rm is to generate an n \u00b7 m matrix Q (after centering). The uppermost k right singular values Vk of Q describe the PCA subspace.Error in PCA is typically measured as the sum of the square residuals, which is q Q-Q-Rm for each point, its projection on Vk Vkq, and its residuality is rq =."}], "references": [{"title": "Sketching, embedding, and dimensionality reduction for information spaces", "author": ["Amirali Abdullah", "Ravi Kumar", "Andrew McGregor", "Sergei Vassilvitskii", "Suresh Venkatasubramanian"], "venue": "AIStats,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Problems and results in extremal combinatorics-i", "author": ["Noga Alon"], "venue": "Discrete Math.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Subspace embeddings for the polynomial kernel", "author": ["Haim Avron", "Huy L. Nguyen", "David P. Woodruff"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["P. Massart B. Laurent"], "venue": "The Annals of Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Metric characterization of random variables and random processes. Translations of mathematical monographs", "author": ["Valerij Vladimirovi\u010d Buldygin", "IU.V. Kozachenko", "V. Zaiats"], "venue": "Providence, R.I. American Mathematical Society,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Compact random feature maps", "author": ["Raffay Hamid", "Ying Xiao", "Alex Gittens", "Dennis DeCoste"], "venue": "arXiv preprint arXiv:1312.4626,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Extensions of Lipschitz maps into a Hilbert space", "author": ["William B. Johnson", "Joram Lindenstrauss"], "venue": "Contemporary Mathematics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1984}, {"title": "Random feature maps for dot product kernels", "author": ["Purushottam Kar", "Harish Karnick"], "venue": "arXiv preprint arXiv:1201.6530,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "The johnson-lindenstrauss lemma is optimal for linear dimensionality reduction", "author": ["Kasper Green Larsen", "Jelani Nelson"], "venue": "CoRR, abs/1411.2404,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Random fourier approximations for skewed multiplicative histogram kernels", "author": ["Fuxin Li", "Catalin Ionescu", "Cristian Sminchisescu"], "venue": "In Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Randomized nonlinear component analysis", "author": ["David Lopez-Paz", "Suvrit Sra", "Alex Smola", "Zoubin Ghahramani", "Bernhard Sch\u00f6lkopf"], "venue": "arXiv preprint arXiv:1402.0119,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Max-margin additive classifiers for detection", "author": ["Subhransu Maji", "Alexander C Berg"], "venue": "In Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Integral probability metrics and their generating classes of functions", "author": ["Alfred M\u00fcller"], "venue": "Advances in Applied Probability,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Optimality of the johnson-lindenstrauss lemma", "author": ["Jelani Nelson", "Kasper Green Larsen"], "venue": "Technical Report 1609.02094,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Geomtric inference on kernel density estimates", "author": ["Jeff M. Phillips", "Bei Wang", "Yan Zheng"], "venue": "In SOCG,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["Bharath K. Sriperumbudur", "Arthur Gretton", "Kenji Fukumizu", "Bernhard Sch\u00f6lkopf", "Gert R.G. Lanckriet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Optimal rates for random fourier features", "author": ["Bharath K. Sriperumbudur", "Zoltan Szabo"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "On the error of random fourier features", "author": ["Dougal J. Sutherland", "Jeff Schneider"], "venue": "In UAI,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "For reproducing kernels (actually a slightly smaller set called characteristic kernels) this is a metric [17, 13].", "startOffset": 105, "endOffset": 113}, {"referenceID": 12, "context": "For reproducing kernels (actually a slightly smaller set called characteristic kernels) this is a metric [17, 13].", "startOffset": 105, "endOffset": 113}, {"referenceID": 6, "context": "It turns out, one can always construct such a lifting with m = O((1/\u03b52) log(n/\u03b4)) by the famous Johnson-Lindenstrauss (JL) Lemma [7].", "startOffset": 129, "endOffset": 132}, {"referenceID": 15, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 165, "endOffset": 181}, {"referenceID": 10, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 165, "endOffset": 181}, {"referenceID": 17, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 165, "endOffset": 181}, {"referenceID": 18, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 165, "endOffset": 181}, {"referenceID": 9, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 206, "endOffset": 210}, {"referenceID": 11, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 237, "endOffset": 241}, {"referenceID": 7, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 263, "endOffset": 266}, {"referenceID": 0, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 287, "endOffset": 290}, {"referenceID": 5, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 315, "endOffset": 321}, {"referenceID": 2, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 315, "endOffset": 321}, {"referenceID": 15, "context": "In this document we reanalyze one of the most widely used and first variants, the Random Fourier Features, introduced by [16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 15, "context": "1 Existing Properties of Gaussian Kernel Embeddings [16] defined two approximate embedding functions: \u03c6\u0303 : Rd \u2192 Rm and \u03c6\u0302 : Rd \u2192 Rm.", "startOffset": 52, "endOffset": 56}, {"referenceID": 18, "context": "Only the former appears in the final version of paper, but the latter is also commonly used throughout the literature [19].", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "Recently [18] tightened the above asymptotic bounds to show actual constants.", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "It is folklore (apparently removed from final version of [16]; reproved in Section 2) that also E[\u03c6\u0302(x)T \u03c6\u0302(y)] = K(x, y), and thus all of the above PAC bounds hold for \u03c6\u0302 as well.", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "Also recently, [19] compared \u03c6\u0303 and \u03c6\u0302 (they used symbol \u03c6\u0306 in place of our symbol \u03c6\u0302), and demonstrated that \u03c6\u0302 performs better (for the same m) and has provably lower variance in approximating K(x, y) with \u03c6\u0302(x)T \u03c6\u0302(y) as opposed to with \u03c6\u0303(x)T \u03c6\u0303(y).", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "Our results to do not describe unrestricted OSEs, as are possible with polynomial kernels [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 15, "context": "An earlier version of Rahimi-Recht [16] seemed to prove the following lemma.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "As a warm up, and as was similarly observed in [15], a simple Taylor expansion when \u2016\u2206\u2016 \u2264 1, implies that \u2016\u2206\u2016 \u2212 1 4 \u2016\u2206\u2016 \u2264 DK(\u2206) = 2\u2212 2 exp(\u2016\u2206\u2016/2) \u2264 \u2016\u2206\u2016, and by 1 4\u2016\u2206\u2016 \u2264 4\u2016\u2206\u2016 and a square root 0.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "Here we use Lemma 1 from [4]; if X is a \u03c72 random variable with t degrees of freedom Pr[t\u2212 2 \u221a tx \u2264 X \u2264 t+ 2 \u221a tx+ 2x] \u2265 1\u2212 2e\u2212x.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "For \u03b5 \u2208 (0, 1) and \u03b4 \u2208 (0, 1/2), if \u2016\u2206\u2016 \u2264 \u221a \u03b5 log(1/\u03b4) , and t = \u03a9( 1 \u03b5 log(1/\u03b4)), then with probability at least 1\u2212O(\u03b4), for all \u03bb \u2208 [0, 1] we have DK\u0302(\u03bb\u00b7\u2206) 2 DK(\u03bb\u00b7\u2206)2 \u2208 [1\u2212 \u03b5, 1 + \u03b5].", "startOffset": 134, "endOffset": 140}, {"referenceID": 0, "context": "as long as \u03bb \u2208 [0, 1].", "startOffset": 15, "endOffset": 21}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Remark: A new result of Larsen and Nelson [14] provides a t = \u03a9( 1 \u03b5 log n) lower bound for even non-linear embeddings of a size n point set in Rd into Rt that preserve distances within (1 \u00b1 \u03b5).", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "However, it is not clear that any point set (including the ones used in the strong lower bound proof [14]), can result from an isomorphic (or approximate) embedding of RKHS into Rn.", "startOffset": 101, "endOffset": 105}, {"referenceID": 4, "context": "We next combine this result with an existing bound on sub-exponential random variables [5](Lemma 4.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "(10) Since this holds for all \u03bb \u2208 [0, 1], we obtain relative error bounds over {\u2206 : \u2016\u2206\u2016 \u03c3 \u2264 \u221a \u03b5 logn}.", "startOffset": 34, "endOffset": 40}, {"referenceID": 17, "context": "6 Lower Bounds for Low Dimensions When is n is unbounded, a recent paper [18] implies that, even for small d, DK\u0302 cannot (1 + \u03b5)approximate DK unless M is bounded.", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "First we need the following general result ([2] Theorem 9.", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "Then [2] Theorem 9.", "startOffset": 5, "endOffset": 8}, {"referenceID": 17, "context": "for an infinite number of points, answering a question raised by [18].", "startOffset": 65, "endOffset": 69}], "year": 2016, "abstractText": "A reproducing kernel defines an embedding of a data point into an infinite dimensional reproducing kernel Hilbert space (RKHS). The norm in this space describes a distance, which we call the kernel distance. The random Fourier features (of Rahimi and Recht) describe an oblivious approximate mapping into finite dimensional Euclidean space that behaves similar to the RKHS. We show in this paper that for the Gaussian kernel the Euclidean norm between these mapped to features has (1 + \u03b5)-relative error with respect to the kernel distance. When there are n data points, we show that O((1/\u03b5) logn) dimensions of the approximate feature space are sufficient and necessary. Without a bound on n, but when the original points lie in R and have diameter bounded by M, then we show that O((d/\u03b5) logM) dimensions are sufficient, and that this many are required, up to log(1/\u03b5) factors. We empirically confirm that relative error is indeed preserved for kernel PCA using these approximate feature maps.", "creator": "LaTeX with hyperref package"}}}