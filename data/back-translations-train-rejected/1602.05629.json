{"id": "1602.05629", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2016", "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data", "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data-center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.", "histories": [["v1", "Wed, 17 Feb 2016 23:40:56 GMT  (903kb,D)", "http://arxiv.org/abs/1602.05629v1", null], ["v2", "Fri, 21 Oct 2016 22:39:11 GMT  (955kb,D)", "http://arxiv.org/abs/1602.05629v2", null], ["v3", "Tue, 28 Feb 2017 21:03:49 GMT  (1075kb,D)", "http://arxiv.org/abs/1602.05629v3", "This version updates the large-scale LSTM experiments, along with other minor changes. In earlier versions, an inconsistency in our implementation of FedSGD caused us to report much lower learning rates for the large-scale LSTM. We reran these experiments, and also found that fewer local epochs offers better performance, leading to slightly better results for FedAvg than previously reported"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["h brendan mcmahan", "eider moore", "daniel ramage", "seth hampson", "blaise ag\\\"uera y arcas"], "accepted": false, "id": "1602.05629"}, "pdf": {"name": "1602.05629.pdf", "metadata": {"source": "META", "title": "Federated Learning of Deep Networks using Model Averaging", "authors": ["H. Brendan McMahan", "Eider Moore", "Daniel Ramage"], "emails": ["MCMAHAN@GOOGLE.COM", "EIDERM@GOOGLE.COM", "DRAMAGE@GOOGLE.COM", "BLAISEA@GOOGLE.COM"], "sections": [{"heading": null, "text": "We present a practical method for federated deep network learning that proves resilient to the unbalanced and non-IID data distributions that naturally occur. This method enables high-quality models to be trained in relatively few rounds of communication, the main obstacle to federated learning. The most important finding is that, despite the non-convex loss functions that we optimize, averaging parameters through updates from multiple clients yields surprisingly good results, such as a reduction in communication required to achieve an LSTM language model by two orders of magnitude."}, {"heading": "1. Introduction", "text": "As datasets become larger and more complex, machine learning increasingly requires distributing the optimization of model parameters across multiple machines, e.g. (Dean et al., 2012) Many algorithms exist for distributed optimization, but these algorithms typically have communication requirements that are met only by one data center. Taken together, these requirements amount to an assumption that the complete training dataset is controlled by the modeler and stored in a centralized location. A parallel trend is the rise of phones and tablets as primary computing devices for many people. The powerful sensors that are present on these devices (including cameras, microphones, and GPS) are associated with the fact that these devices are frequently used."}, {"heading": "1.1. Federated Learning", "text": "What is most suitable for tasks is federated learning (Corrado, 2015).The ideal problems have the following characteristics: \u2022 Training on real data protection systems of mobile devices offers a clear advantage over training of proxy data generally available in the data center. \u2022 These data are sensitive in size (compared to the size of the model), so it is better not to log them into the data center than to use them only for modeling purposes (in the service of the focused collection principle).For supervised tasks, labels can of course infer the data from a user's interaction with their device. Many models that have intelligent behavior on mobile devices fit the above criteria, we consider as two examples: \u2022 Image classification, for which photos are most likely to be viewed in the future, or shared. \u2022 Language models that can be used to improve speech recognition by applying them to touch-screen keyboards."}, {"heading": "1.2. Federated Optimization", "text": "We point to the optimization problem implicit in federated optimization and the complex issues of distributed optimization. As indicated above, federated optimization has several key characteristics that distinguish it from typical distributed optimization problems. - For example, training data will typically rely on a particular user's use of the mobile device, while others will not represent local data. - Unbalanced users will make much heavier use of training data, resulting in large local training data, while others will have little or no data. - Massive distributed scenarios in which we expect the number of customers to participate in an optimization that is much greater than the average number of examples per client. - In this work, our focus will be on non-IID and unbalanced characteristics."}, {"heading": "1.3. Related Work", "text": "In fact, most people are able to understand themselves and understand what they are doing to change the world. (...) Most of them are able to change the world. (...) Most of them are able to change the world. (...) Most of them are not able to change the world. (...) Most of them are not able to change the world. (...) Most of them are able to change the world. (...) Most of them are not able to change the world. (...) Most of them are not able to change the world. (...) Most of them are not able to change the world. (...) Most of them are not able to change the world. (...) Most of them are able to change the world. (...) Most of them are able to change the world. (...) Most of them are not able to change the world. (...) Most of them are not able to change the world. (...) Most of them are able to change the world. (...) Most of them are able to change the world. (...) Most of them are able to change the world. (...) Most of them are able to change the world."}, {"heading": "3. Experimental Results", "text": "We are motivated by both image classifications and speech modeling, where good models can vastly improve the usability of mobile devices. So, for each of these tasks, we have chosen a proxy dataset of modest size that we can thoroughly explore to explore the hyperparameters of the FedAvg algorithm. Thus, while each training session is relatively small, we have examined three model families on two datasets, the first two being for the MNIST detection task (LeCun et al., 1998): a simple two-shift model with 200 units per shift using ReLu activations (199,210 total parameters), which we refer to MNIST detection."}, {"heading": "4. Conclusions and Future Work", "text": "Our experiments show that federated learning is promising because high-quality models can be trained with relatively few rounds of communication. Further empirical evaluation of the proposed approach based on larger datasets that truly capture the massively distributed nature of real-world problems is an important next step. To keep the scope of the algorithms studied comprehensible, we limited ourselves to building on vanilla SGD. Compatibility of our approach with other optimization algorithms such as AdaGrad (McMahan & Streeter, 2010; Duchi et al., 2011) and ADAM (Kingma & Ba, 2015), as well as with changes in the model structure that can contribute to optimization, such as suspensions (Srivastava et al., 2014) and batch normalization (Ioffe & Szegedy, 2015), are another natural direction for future work."}], "references": [{"title": "Communication complexity of distributed convex learning and optimization", "author": ["Arjevani", "Yossi", "Shamir", "Ohad"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Arjevani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arjevani et al\\.", "year": 2015}, {"title": "Distributed learning, communication complexity and privacy", "author": ["Balcan", "Maria-Florina", "Blum", "Avrim", "Fine", "Shai", "Mansour", "Yishay"], "venue": "arXiv preprint arXiv:1204.3514,", "citeRegEx": "Balcan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Janvin", "Christian"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Untraceable electronic mail, return addresses, and digital pseudonyms", "author": ["Chaum", "David L"], "venue": "Commun. ACM,", "citeRegEx": "Chaum and L.,? \\Q1981\\E", "shortCiteRegEx": "Chaum and L.", "year": 1981}, {"title": "The loss surfaces of multilayer networks", "author": ["Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Micha\u00ebl", "Arous", "G\u00e9rard Ben", "LeCun", "Yann"], "venue": "In AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Computer, respond to this email", "author": ["Corrado", "Greg"], "venue": "http://googleresearch.blogspot.com/2015/ 11/computer-respond-to-this-email.html,", "citeRegEx": "Corrado and Greg.,? \\Q2015\\E", "shortCiteRegEx": "Corrado and Greg.", "year": 2015}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Dauphin", "Yann N", "Pascanu", "Razvan", "G\u00fcl\u00e7ehre", "\u00c7aglar", "Cho", "KyungHyun", "Ganguli", "Surya", "Bengio", "Yoshua"], "venue": "uNIPS,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Large scale distributed deep networks", "author": ["Dean", "Jeffrey", "Corrado", "Greg S", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Le", "Quoc V", "Mao", "Mark Z", "Ranzato", "Marc\u2019Aurelio", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Privacy aware learning", "author": ["Duchi", "John", "Jordan", "Michael I", "Wainwright", "Martin J"], "venue": "Journal of the Association for Computing Machinery,", "citeRegEx": "Duchi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2014}, {"title": "The Algorithmic Foundations of Differential Privacy", "author": ["Dwork", "Cynthia", "Roth", "Aaron"], "venue": "Foundations and Trends in Theoretical Computer Science. Now Publishers,", "citeRegEx": "Dwork et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2014}, {"title": "Fast distributed coordinate descent for non-strongly convex losses", "author": ["Fercoq", "Olivier", "Qu", "Zheng", "Richt\u00e1rik", "Peter", "Tak\u00e1c", "Martin"], "venue": "In Machine Learning for Signal Processing (MLSP),", "citeRegEx": "Fercoq et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fercoq et al\\.", "year": 2014}, {"title": "Model inversion attacks that exploit confidence information and basic countermeasures", "author": ["Fredrikson", "Matt", "Jha", "Somesh", "Ristenpart", "Thomas"], "venue": "In ACM Conference on Computer and Communications Security,", "citeRegEx": "Fredrikson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fredrikson et al\\.", "year": 2015}, {"title": "Deep learning. Book in preparation for", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["Goodfellow", "Ian J", "Vinyals", "Oriol", "Saxe", "Andrew M"], "venue": "In ICLR,", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Secure multiparty aggregation with differential privacy: A comparative study", "author": ["Goryczka", "Slawomir", "Xiong", "Li", "Sunderam", "Vaidy"], "venue": "In Proceedings of the Joint EDBT/ICDT 2013 Workshops,", "citeRegEx": "Goryczka et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goryczka et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "What can we learn privately", "author": ["Kasiviswanathan", "Shiva Prasad", "Lee", "Homin K", "Nissim", "Kobbi", "Raskhodnikova", "Sofya", "Smith", "Adam"], "venue": "In FOCS,", "citeRegEx": "Kasiviswanathan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kasiviswanathan et al\\.", "year": 2008}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Adding vs. averaging in distributed primal-dual optimization", "author": ["Ma", "Chenxin", "Smith", "Virginia", "Jaggi", "Martin", "Jordan", "Michael I", "Richt\u00e1rik", "Peter", "Tak\u00e1\u010d"], "venue": "In ICML,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Adaptive bound optimization for online convex optimization", "author": ["McMahan", "H. Brendan", "Streeter", "Matthew"], "venue": "In COLT,", "citeRegEx": "McMahan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McMahan et al\\.", "year": 2010}, {"title": "Distributed stochastic optimization and learning", "author": ["Shamir", "Ohad", "Srebro", "Nathan"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "Shamir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2014}, {"title": "Communication efficient distributed optimization using an approximate newton-type method", "author": ["Shamir", "Ohad", "Srebro", "Nathan", "Zhang", "Tong"], "venue": "arXiv preprint arXiv:1312.7853,", "citeRegEx": "Shamir et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2013}, {"title": "Privacy-preserving deep learning", "author": ["Shokri", "Reza", "Shmatikov", "Vitaly"], "venue": "In Proceedings of the 22Nd ACM SIGSAC Conference on Computer and Communications Security, CCS", "citeRegEx": "Shokri et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shokri et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Simple demographics often identify people uniquely", "author": ["Sweeney", "Latanya"], "venue": null, "citeRegEx": "Sweeney and Latanya.,? \\Q2000\\E", "shortCiteRegEx": "Sweeney and Latanya.", "year": 2000}, {"title": "Regression model fitting under differential privacy and model inversion attack", "author": ["Wang", "Yue", "Si", "Cheng", "Wu", "Xintao"], "venue": "In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Consumer data privacy in a networked world: A framework for protecting privacy and promoting innovation in the global digital economy", "author": ["White House Report"], "venue": "Journal of Privacy and Confidentiality,", "citeRegEx": "Report.,? \\Q2013\\E", "shortCiteRegEx": "Report.", "year": 2013}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["Yang", "Tianbao"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yang and Tianbao.,? \\Q2013\\E", "shortCiteRegEx": "Yang and Tianbao.", "year": 2013}, {"title": "Communication-efficient distributed optimization of self-concordant empirical loss", "author": ["Zhang", "Yuchen", "Xiao", "Lin"], "venue": "arXiv preprint arXiv:1501.00263,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Zhang", "Yuchen", "Wainwright", "Martin J", "Duchi", "John C"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Information-theoretic lower bounds for distributed statistical estimation with communication constraints", "author": ["Zhang", "Yuchen", "Duchi", "John", "Jordan", "Michael I", "Wainwright", "Martin J"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Parallelized stochastic gradient descent", "author": ["Zinkevich", "Martin", "Weimer", "Markus", "Smola", "Alexander J", "Li", "Lihong"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zinkevich et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": ", (Dean et al., 2012).", "startOffset": 2, "endOffset": 21}, {"referenceID": 21, "context": "For image classification feed-forward deep networks, and in particular convolutional networks, are wellknown to provide state-of-the-art results (LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 145, "endOffset": 190}, {"referenceID": 20, "context": "For image classification feed-forward deep networks, and in particular convolutional networks, are wellknown to provide state-of-the-art results (LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 145, "endOffset": 190}, {"referenceID": 2, "context": "For language modeling tasks recurrent neural networks, and in particular LSTMs, have achieved state-of-the-art results (Hochreiter & Schmidhuber, 1997; Bengio et al., 2003; Kim et al., 2015).", "startOffset": 119, "endOffset": 190}, {"referenceID": 29, "context": "Additional steps may also be needed to address model inversion attacks (Wang et al., 2015; Fredrikson et al., 2015).", "startOffset": 71, "endOffset": 115}, {"referenceID": 12, "context": "Additional steps may also be needed to address model inversion attacks (Wang et al., 2015; Fredrikson et al., 2015).", "startOffset": 71, "endOffset": 115}, {"referenceID": 18, "context": "A stronger guarantee can be achieved by enforcing local differential privacy (Kasiviswanathan et al., 2008; Duchi et al., 2014), where rather than adding noise to the final model, we noise the individual updates, which precludes the central server from making any definitive inference about a client.", "startOffset": 77, "endOffset": 127}, {"referenceID": 9, "context": "A stronger guarantee can be achieved by enforcing local differential privacy (Kasiviswanathan et al., 2008; Duchi et al., 2014), where rather than adding noise to the final model, we noise the individual updates, which precludes the central server from making any definitive inference about a client.", "startOffset": 77, "endOffset": 127}, {"referenceID": 15, "context": "It is also possible to use secure multiparty computation to perform aggregation over multiple client updates, allowing local differential privacy to be achieved using much less random noise (Goryczka et al., 2013).", "startOffset": 190, "endOffset": 213}, {"referenceID": 1, "context": "In the convex setting, the problem of distributed optimization and estimation has received significant attention (Balcan et al., 2012; Fercoq et al., 2014; Shamir & Srebro, 2014), and some algorithms do focus specifically on communication efficiency (Zhang et al.", "startOffset": 113, "endOffset": 178}, {"referenceID": 11, "context": "In the convex setting, the problem of distributed optimization and estimation has received significant attention (Balcan et al., 2012; Fercoq et al., 2014; Shamir & Srebro, 2014), and some algorithms do focus specifically on communication efficiency (Zhang et al.", "startOffset": 113, "endOffset": 178}, {"referenceID": 34, "context": ", 2014; Shamir & Srebro, 2014), and some algorithms do focus specifically on communication efficiency (Zhang et al., 2013; Shamir et al., 2013; Yang, 2013; Ma et al., 2015; Zhang & Xiao, 2015).", "startOffset": 102, "endOffset": 192}, {"referenceID": 25, "context": ", 2014; Shamir & Srebro, 2014), and some algorithms do focus specifically on communication efficiency (Zhang et al., 2013; Shamir et al., 2013; Yang, 2013; Ma et al., 2015; Zhang & Xiao, 2015).", "startOffset": 102, "endOffset": 192}, {"referenceID": 22, "context": ", 2014; Shamir & Srebro, 2014), and some algorithms do focus specifically on communication efficiency (Zhang et al., 2013; Shamir et al., 2013; Yang, 2013; Ma et al., 2015; Zhang & Xiao, 2015).", "startOffset": 102, "endOffset": 192}, {"referenceID": 1, "context": "In the convex setting, the problem of distributed optimization and estimation has received significant attention (Balcan et al., 2012; Fercoq et al., 2014; Shamir & Srebro, 2014), and some algorithms do focus specifically on communication efficiency (Zhang et al., 2013; Shamir et al., 2013; Yang, 2013; Ma et al., 2015; Zhang & Xiao, 2015). In addition to assuming convexity, this existing work generally requires that the number of clients is much smaller than the number of examples per client, that the data is distributed across the clients in IID fashion, and that each node has an identical number of data points \u2014 all of these assumptions are violated in the federated optimization setting. Asynchronous distributed forms of SGD have also been applied to training neural networks, e.g., Dean et al. (2012), but these approaches require a prohibitive number of updates in the federated setting.", "startOffset": 114, "endOffset": 814}, {"referenceID": 33, "context": "This approach has been studied extensively in the convex case with IID data, and it is known that in the worst-case, the global model produced is no better than training a model on a single client (Zhang et al., 2012; Arjevani & Shamir, 2015).", "startOffset": 197, "endOffset": 242}, {"referenceID": 32, "context": "This approach has been studied extensively in the convex case with IID data, and it is known that in the worst-case, the global model produced is no better than training a model on a single client (Zhang et al., 2012; Arjevani & Shamir, 2015). Zinkevich et al. (2011) studies an averaging algorithm very similar to ours in the convex, balanced, IID setting.", "startOffset": 198, "endOffset": 268}, {"referenceID": 13, "context": "The recent multitude of successful applications of deep learning have almost exclusively relied on variants of stochastic gradient descent (SGD) as the optimization algorithm; in fact, many advances can be understood as adapting the structure of the model (and hence the loss function) to be more amenable to optimization by simple gradientbased methods (Goodfellow et al., 2016).", "startOffset": 354, "endOffset": 379}, {"referenceID": 13, "context": "Following the approach of Goodfellow et al. (2015), we see exactly this bad behavior when we average two MNIST models2 trained from different initial conditions (Figure 1, left).", "startOffset": 26, "endOffset": 51}, {"referenceID": 6, "context": "However, recent work indicates that in practice, the loss surfaces of sufficiently over-parameterized NNs are surprisingly well-behaved and in particular less prone to bad local minima than previously thought (Dauphin et al., 2014; Goodfellow et al., 2015; Choromanska et al., 2015).", "startOffset": 209, "endOffset": 282}, {"referenceID": 14, "context": "However, recent work indicates that in practice, the loss surfaces of sufficiently over-parameterized NNs are surprisingly well-behaved and in particular less prone to bad local minima than previously thought (Dauphin et al., 2014; Goodfellow et al., 2015; Choromanska et al., 2015).", "startOffset": 209, "endOffset": 282}, {"referenceID": 4, "context": "However, recent work indicates that in practice, the loss surfaces of sufficiently over-parameterized NNs are surprisingly well-behaved and in particular less prone to bad local minima than previously thought (Dauphin et al., 2014; Goodfellow et al., 2015; Choromanska et al., 2015).", "startOffset": 209, "endOffset": 282}, {"referenceID": 27, "context": "The success of dropout training also provides some intuition for the success of our model averaging scheme; dropout training can be interpreted as averaging models of different architectures which share parameters, and the inference-time scaling of the model parameters is analogous to the model averaging used in FedAvg (Srivastava et al., 2014).", "startOffset": 321, "endOffset": 346}, {"referenceID": 21, "context": "The first two are for the MNIST digit recognition task (LeCun et al., 1998):", "startOffset": 55, "endOffset": 75}, {"referenceID": 27, "context": "We conjecture that in addition to lowering communication costs, model averaging produces a regularization benefit similar to that achieved by dropout (Srivastava et al., 2014).", "startOffset": 150, "endOffset": 175}, {"referenceID": 8, "context": "Investigating the compatibility of our approach with other optimization algorithms such as AdaGrad (McMahan & Streeter, 2010; Duchi et al., 2011) and ADAM (Kingma & Ba, 2015), as well as with changes in model structure that can aid optimization, such as dropout (Srivastava et al.", "startOffset": 99, "endOffset": 145}, {"referenceID": 27, "context": ", 2011) and ADAM (Kingma & Ba, 2015), as well as with changes in model structure that can aid optimization, such as dropout (Srivastava et al., 2014) and batch-normalization (Ioffe & Szegedy, 2015), are another natural direction for future work.", "startOffset": 124, "endOffset": 149}], "year": 2016, "abstractText": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data-center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks that proves robust to the unbalanced and non-IID data distributions that naturally arise. This method allows high-quality models to be trained in relatively few rounds of communication, the principal constraint for federated learning. The key insight is that despite the non-convex loss functions we optimize, parameter averaging over updates from multiple clients produces surprisingly good results, for example decreasing the communication needed to train an LSTM language model by two orders of magnitude.", "creator": "LaTeX with hyperref package"}}}