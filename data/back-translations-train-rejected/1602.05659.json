{"id": "1602.05659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "Boost Picking: A Universal Method on Converting Supervised Classification to Semi-supervised Classification", "abstract": "This paper proposes a universal method, Boost Picking, to train supervised classification models mainly by un-labeled data. Boost Picking only adopts two weak classifiers to estimate and correct the error. It is theoretically proved that Boost Picking could train a supervised model mainly by un-labeled data as effectively as the same model trained by 100% labeled data, only if recalls of the two weak classifiers are all greater than zero and the sum of precisions is greater than one. Based on Boost Picking, we present \"Test along with Training (TawT)\" to improve the generalization of supervised models. Both Boost Picking and TawT are successfully tested in varied little data sets.", "histories": [["v1", "Thu, 18 Feb 2016 02:24:54 GMT  (851kb,D)", "http://arxiv.org/abs/1602.05659v1", "9 pages, 9 figures"], ["v2", "Mon, 29 Feb 2016 13:16:23 GMT  (841kb,D)", "http://arxiv.org/abs/1602.05659v2", "9 pages, 9 figures"], ["v3", "Sat, 12 Nov 2016 09:25:54 GMT  (0kb,I)", "http://arxiv.org/abs/1602.05659v3", "This paper has been withdraw by the author due to format error"]], "COMMENTS": "9 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["fuqiang liu", "fukun bi", "yiding yang", "liang chen"], "accepted": false, "id": "1602.05659"}, "pdf": {"name": "1602.05659.pdf", "metadata": {"source": "META", "title": "Boost Picking: A Universal Method on Converting Supervised Classification to Semi-supervised Classification", "authors": ["FUQIANG LIU", "FUKUN BI", "YIDING YANG", "LIANG CHEN"], "emails": ["FQLIU@OUTLOOK.COM", "BIFUKUN@NCUT.EDU.CN", "YANGYDI9G3@BIT.EDU.CN", "CHENL@BIT.EDU.CN"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "2. Prior Work", "text": "In this section, Kalal's work is presented. \u2212 \u2212 In TLD (Kalal et al., 2012), P-N Learning is used as an online learning method to learn the new form of the tracked object. \u2212 Two classifiers, called P-expert and N-expert, are used to evaluate and correct the error of the detector (a classifier). P-expert estimates the false negatives from the negative results of the detector, while N-expert estimates the false positives from the positive results. Boost Picking refers to this reason.Kalal has theoretically proven that P-N Learning could form a detector whose error converges to zero (Kalal et al., 2010; 2012). \u2212 Define \u2212 \u2212 d (k) as an error in the kest iteration, \u2212 d (k) can be classified as an error."}, {"heading": "3. Boost Picking", "text": "In this section, boost picking is described in detail, its foundation state is examined, and its efficiency is analyzed in theory."}, {"heading": "3.1. Description of Boost Picking", "text": "The structure of Boost Picking is marked as Fig 1. Suppose x is an example from a feature space X = two functions (Finig), and y is its corresponding label from a label space Y = {1, \u2212 1}, the training of Boost Picking is from a described sentence Xl with its label Ll and an undescribed sentence Xu. Assuming that Lu are the true labels of Xu (do not exist, in fact), define F: x \u2192 y as a supervised learning model labeled by 100%. (100%) Xu} with L (100%), Lu} with L (100%), Lu}. The goal of Boost Picking is an effective model f \u2192 F of Xl, Ll and Xu.First, the revised model is trained by labeled data Xl, which is called \"initialization.\" (1 \u00a9 in Fig 1), the model is initialized as f0.Second."}, {"heading": "3.2. Establishment Condition", "text": "This section describes the founding condition of Boost Picking. It includes: \u2022 Proving that Boost Picking could train monitored models as effectively as the same models trained by all labeled data, only if the eigenvalues of the transformation matrix (defined in Section 2) are all less than one. \u2022 Deriving that Boost Picking would only work effectively if the sum of two weak classifiers is precision greater than one and callbacks are all greater than zero. \u2022 Discuss the current situation."}, {"heading": "3.2.1. CONVERGENCE DERIVATION", "text": "The conclusion in the previous paper (Section 2) is based on the assumption that the worn model can be fully classified. (B) Under most conditions, this assumption cannot be established. (B) The conclusion that the eigenvalues of the transformation matrix M are smaller than one, however, is that both f and F are the same model and all their parameters are the same. (The only difference between f and F is the learning data set (Section 3.1). (Define XT is the data set in which the examples can be correctly classified in X100%, and XF is the data set in which the examples cannot be correctly classified. (B) The only difference between f and F is the learning data set (Section 3.1). (X) XT is the data set in which the examples can be correctly classified in X100%. (XF) and XF is the data set in which the examples cannot be correctly classified."}, {"heading": "3.2.2. FP FINDER AND FN FINDER DESIGN", "text": "This part theoretically analyzes how to design FN \u2212 \u2212 \u2212 \u2212 Finder and FP \u2212 Finder to ensure that the eigenvalues of M are all less than one and then f \u2192 F. Assuming that both are eigenvalues of the transformation matrix M, the expression of the two eigenvalues remembers them as equation (14).\u03bb1, \u03bb2 = (2 \u2212 R \u2212 \u2212 R +) \u00b1 270 (14), where P +, R \u2212 P \u2212 0, 1] (defined in Section 2), and4 = (R + \u2212 R \u2212) 2 + 4 (1 \u2212 P \u2212 P \u2212 P \u2212 P \u2212) 2 +) (1 \u2212 P \u2212 R \u2212 P \u2212) 2 \u2265 0 and (1 \u2212 P \u2212 P +) P + P + P + P \u2212 R + (1 \u2212 R \u2212 R \u2212) 2 (1 \u2212 P + R \u2212 R +) 2 (1 \u2212 R \u2212 R \u2212 R \u2212 R) 2 (1 \u2212 R \u2212 R \u2212 R) 2 (4) = 4 \u2212 P \u2212 P \u2212 P \u2212 P) \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P."}, {"heading": "3.2.3. INSTANTANEOUS SITUATION", "text": "Assuming that P + (k), P \u2212 (k) (19), R + (k), R \u2212 (k), R \u2212 (k) are the current values of accuracy and callbacks, the expression of P + is written as Equation (19). The ideal condition is that FP Finder and FN Finder in each generation fulfill Equation (19), {R + (k) R \u2212 (k) 6 = 0 P + (k) + P \u2212 (k) + P \u2212 (k) > 1 (20) Equation (20) is relatively strict compared to Equation (18). In practice, P + (k), R \u2212 (k) + P \u2212 (k) + P \u2212 (k) + P \u2212 (k) > 1 (20) Equation (20) is a guarantee that the Equation (18), P \u2212 P \u2212 P \u2212 (k), P \u2212 (k), R \u2212 (k), R \u2212 (k) and P \u2212 (k) is relatively long, that the Equation (K) and K are relative."}, {"heading": "3.3. Performance Analysis", "text": "This section discusses how to design FP finders and FN finders to ensure boost picking works efficiently. Two aspects are involved in the discussion: the first focuses on the entire iteration process and discusses shock; and the second focuses on single iteration and describes how to reduce d as much as possible within a generation."}, {"heading": "3.3.1. EFFICIENCY IMPROVEMENT FROM GLOBAL ASPECT", "text": "According to Section 3.2.3 d would be chopped off because P + (k) + P \u2212 (k) could not always be greater than one. Define K + = {k | P + (k) + P \u2212 (k) \u2264 1} and K \u2212 = {k | P + (k) + P \u2212 (k) > 1}. Suppose 4d (k) is the increase of d in the kth iteration when d \u2192 0, f \u00b2 k \u00b2 K \u2212 | 4d (k) | \u2212 conservation k \u00b2 K + | 4 d (k) | \u2265 \u2212 \u2192 d (0). Consequently, large K + has an obvious negative effect on the reduction of convergence. Therefore, FP Finder and FN Finder should fulfill the equation (20) as well as possible."}, {"heading": "3.3.2. EFFICIENCY IMPROVEMENT FROM ONE GENERATION", "text": "Based on the theory of dynamic system (Ogata, 2001), d converges faster to zero (R = > b = < < R = = smaller if the absolute values of eigenvalues are smaller. There is a negative correlation between the rate of error and (\u2212 \u2212 P \u2212 \u2212 \u2212 \u2212 P \u2212 \u2212 \u2212 P \u2212 1 |, | \u03bb2 |). It can be written as equation (21). (\u2212 R).4 d (\u2212 E). (\u2212 R).4 d). (\u2212 R). (\u2212 R).4 d). (\u2212 R). (\u2212 F). (F). (F)...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4. Test along with Training", "text": "This section describes a method for improving the generalization of supervised classification models by Boost Picking. Since Boost Picking can use unmarked data to train the supervised classification models before classifying the unmarked data in the test set, use this unmarked data to train the model. In fact, this data is classified during the Boost Picking process. Consequently, this method is called \"Test along with Training (TawT).\" Figure 6 illustrates the concept of TawT. In the traditional process, supervised learning models are trained by marked data in the training set and the unmarked data are classified in the test set. In the case of Boost Picking, the training set includes unmarked data. Based on Boost Picking, TawT crosses the boundary between training set and test set to improve the generalization. Based on Equation (12), the relationship between \"bias,\" \"variance\" and \"error\" (Geman, 1992; James, 2003) is assumed that the \"Generalization 7\" is true."}, {"heading": "5. Experiment", "text": "This section describes experiments to verify the functions of Boost Picking and TawT. These experiments involve logistic regression (lr), bpneural network (nn), support vector engine (svm) and adaboost (ada). In certain models, the parameters are the same in the traditionally monitored classification, Boost Picking and TawT. Two sets of data are used, heart scale and wine. For a dataset X with its L label set, it is divided into two parts, Xrest and Xtest. Xrest / Xtest means the ratio of the number of examples and its results is 3 / 1. Xtest is the constant test dataset for all experiments. And for all experiments, the measurement of model performance is f1 = 2. Precision and RecallPrecision + Recall."}, {"heading": "5.1. Baseline", "text": "This subsection illustrates that only small amounts of labeled data cannot train an effectively monitored model. Classic models are trained by different training sets Xtrain with different scales and the trained models are tested in Xtest. Xtrain, Xrest and Xtrain / Xrest range from 0.1 to 1. Fig. 9 shows the experiment. The horizontal axis represents Xtrain / Xrest. In addition, this experiment also determines the baseline or target for boost picking. According to Fig. 9, a larger Xtrain / Xrest generally leads to a larger f1 in the Xtest. We select the models trained by the whole Xrest as a baseline."}, {"heading": "5.2. FP Finder and FN Finder in Experiments", "text": "Here, a very bad FP finder and FN finder are designed to verify Equation (18). Calculate the center of the positive outputs and the examples that are farthest 20% from the center are considered false-positive. FP is designed in the same way as the above statement and FN is similarly designed. These two classifiers are very weak and their performance varies. In these experiments, the average sum of precisions is 1.06 and the average sum of callbacks is 0.12."}, {"heading": "5.3. Verification", "text": "In this subsection, the functions of Boost Picking and TawT are tested in practice, the results of which are presented in Fig. 8. For Boost Picking experiments, all Xrest models are used, but only examples of Xlabel have labels. Xlabel, Xrest, and Xlabel / Xrest range from 5% to 50%. For TawT experiments, all X including Xtest can be used to train models, and only Xlabel labels can be used. Corresponding Boost Picking experiments can also be used as Xlabel / Xrest. Generally, if Xlabel / Xrest is greater than 0.25, models can be trained near or above the corresponding baseline (Fig. 8). As a result, the use of Boost Picking can train a monitored classification model by 75% blank data, as well as the same model trained by 100% labeled data."}], "references": [{"title": "Beat the machine: Challenging workers to find the unknown unknowns", "author": ["Attenberg", "Josh", "Ipeirotis", "Panagiotis G", "Provost", "Foster J"], "venue": "Human Computation,", "citeRegEx": "Attenberg et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Attenberg et al\\.", "year": 2011}, {"title": "Self-trained lmt for semisupervised learning", "author": ["Fazakis", "Nikos", "Karlos", "Stamatis", "Kotsiantis", "Sotiris", "Sgarbas", "Kyriakos"], "venue": "Computational Intelligence and Neuroscience,", "citeRegEx": "Fazakis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fazakis et al\\.", "year": 2015}, {"title": "Imagenet: crowdsourcing, benchmarking & other cool things", "author": ["L. Fei-Fei"], "venue": "In CMU VASC Seminar,", "citeRegEx": "Fei.Fei,? \\Q2010\\E", "shortCiteRegEx": "Fei.Fei", "year": 2010}, {"title": "Neural networks and the bias/variance dilemma", "author": ["Geman", "Stuart", "Bienenstock", "Elie", "Doursat", "Ren\u00e9"], "venue": "Neural computation,", "citeRegEx": "Geman et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1992}, {"title": "Applied logistic regression", "author": ["Hosmer Jr.", "David W", "Lemeshow", "Stanley"], "venue": null, "citeRegEx": "Jr et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jr et al\\.", "year": 2004}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["Huang", "Gary B", "Ramesh", "Manu", "Berg", "Tamara", "Learned-Miller", "Erik"], "venue": "Technical Report 07-49,", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Variance and bias for general loss functions", "author": ["James", "Gareth M"], "venue": "Machine Learning,", "citeRegEx": "James and M.,? \\Q2003\\E", "shortCiteRegEx": "James and M.", "year": 2003}, {"title": "Pn learning: Bootstrapping binary classifiers by structural constraints", "author": ["Kalal", "Zdenek", "Matas", "Jiri", "Mikolajczyk", "Krystian"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kalal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kalal et al\\.", "year": 2010}, {"title": "Tracking-learning-detection. Pattern Analysis and Machine Intelligence", "author": ["Kalal", "Zdenek", "Mikolajczyk", "Krystian", "Matas", "Jiri"], "venue": "IEEE Transactions on,", "citeRegEx": "Kalal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalal et al\\.", "year": 2012}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Foundations of machine learning", "author": ["Mohri", "Mehryar", "Rostamizadeh", "Afshin", "Talwalkar", "Ameet"], "venue": "MIT press,", "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "Modern control engineering", "author": ["Ogata", "Katsuhiko"], "venue": "Prentice Hall PTR,", "citeRegEx": "Ogata and Katsuhiko.,? \\Q2001\\E", "shortCiteRegEx": "Ogata and Katsuhiko.", "year": 2001}, {"title": "Soft margins for adaboost", "author": ["R\u00e4tsch", "Gunnar", "Onoda", "Takashi", "M\u00fcller", "K-R"], "venue": "Machine learning,", "citeRegEx": "R\u00e4tsch et al\\.,? \\Q2001\\E", "shortCiteRegEx": "R\u00e4tsch et al\\.", "year": 2001}, {"title": "Least squares support vector machine classifiers", "author": ["Suykens", "Johan AK", "Vandewalle", "Joos"], "venue": "Neural processing letters,", "citeRegEx": "Suykens et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Suykens et al\\.", "year": 1999}, {"title": "Semi-supervised learning literature survey", "author": ["Zhu", "Xiaojin"], "venue": null, "citeRegEx": "Zhu and Xiaojin.,? \\Q2005\\E", "shortCiteRegEx": "Zhu and Xiaojin.", "year": 2005}, {"title": "Introduction to semi-supervised learning", "author": ["Zhu", "Xiaojin", "Goldberg", "Andrew B"], "venue": "Synthesis lectures on artificial intelligence and machine learning,", "citeRegEx": "Zhu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 10, "context": "Supervised learning is the machine learning task of inferring a function from labeled training data (Mohri et al., 2012).", "startOffset": 100, "endOffset": 120}, {"referenceID": 5, "context": "The achievements of supervised learning rely on labeled data bases, like LFW (Huang et al., 2007), ImageNet (Fei-Fei, 2010) and Microsoft COCO (Lin et al.", "startOffset": 77, "endOffset": 97}, {"referenceID": 2, "context": ", 2007), ImageNet (Fei-Fei, 2010) and Microsoft COCO (Lin et al.", "startOffset": 18, "endOffset": 33}, {"referenceID": 9, "context": ", 2007), ImageNet (Fei-Fei, 2010) and Microsoft COCO (Lin et al., 2014).", "startOffset": 53, "endOffset": 71}, {"referenceID": 0, "context": "Humans academically called \u201dcrowds\u201d (Attenberg et al., 2011; Burton et al., 2012), who label the data in practice, greatly contribute to the development of machine learning.", "startOffset": 36, "endOffset": 81}, {"referenceID": 8, "context": "The structure of Boost Picking refers to \u201dP-N Learning\u201d in \u201dTracking-Learning-Detection\u201d (Kalal et al., 2012).", "startOffset": 89, "endOffset": 109}, {"referenceID": 1, "context": "Different from previous semi-supervised learning works like (Zhu, 2005; Zhu & Goldberg, 2009; Fazakis et al., 2015), Boost Picking, a universal method on training traditional supervised models using un-labeled data, adopts no internal change and has no limitation on the supervised models.", "startOffset": 60, "endOffset": 115}, {"referenceID": 12, "context": "Logistic regression (Hosmer Jr & Lemeshow, 2004), neural network (Haykin & Network, 2004), support vector machine (SVM) (Suykens & Vandewalle, 1999) and Adaboost (R\u00e4tsch et al., 2001) are all tested with varied data sets in experiments (section 5).", "startOffset": 162, "endOffset": 183}, {"referenceID": 8, "context": "In TLD (Kalal et al., 2012), P-N Learning is an online learning method to learn the new form of the tracked object.", "startOffset": 7, "endOffset": 27}, {"referenceID": 7, "context": "Kalal theoretically proved that P-N Learning could train a detector whose error converges into zero (Kalal et al., 2010; 2012).", "startOffset": 100, "endOffset": 126}, {"referenceID": 8, "context": "The recursive equations are as equation (2) (Kalal et al., 2012).", "startOffset": 44, "endOffset": 64}, {"referenceID": 7, "context": "Kalal concludes that d will converge to zero only if eigenvalues of the transformation matrix M are all smaller than one (Kalal et al., 2010).", "startOffset": 121, "endOffset": 141}, {"referenceID": 3, "context": "Based on equation (12), when d \u2192 0, relationship among \u201dbias\u201d, \u201dvariance\u201d and \u201derror\u201d (Geman et al., 1992; James, 2003) could be simplified as Fig 7.", "startOffset": 86, "endOffset": 119}], "year": 2017, "abstractText": "This paper proposes a universal method, Boost Picking, to train supervised classification models mainly by un-labeled data. Boost Picking only adopts two weak classifiers to estimate and correct the error. It is theoretically proved that Boost Picking could train a supervised model mainly by un-labeled data as effectively as the same model trained by 100% labeled data, only if recalls of the two weak classifiers are all greater than zero and the sum of precisions is greater than one. Based on Boost Picking, we present \u201dTest along with Training (TawT)\u201d to improve the generalization of supervised models. Both Boost Picking and TawT are successfully tested in varied little data sets.", "creator": "LaTeX with hyperref package"}}}