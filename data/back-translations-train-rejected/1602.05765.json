{"id": "1602.05765", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning", "abstract": "Conceptual spaces are geometric representations of conceptual knowledge, in which entities correspond to points, natural properties correspond to convex regions, and the dimensions of the space correspond to salient features. While conceptual spaces enable elegant models of various cognitive phenomena, the lack of automated methods for constructing such representations have so far limited their application in artificial intelligence. To address this issue, we propose a method which learns a vector-space embedding of entities from Wikipedia and constrains this embedding such that entities of the same semantic type are located in some lower-dimensional subspace. We experimentally demonstrate the usefulness of these subspaces as (approximate) conceptual space representations by showing, among others, that important features can be modelled as directions and that natural properties tend to correspond to convex regions.", "histories": [["v1", "Thu, 18 Feb 2016 11:37:50 GMT  (21kb)", "http://arxiv.org/abs/1602.05765v1", null], ["v2", "Wed, 25 Oct 2017 13:48:21 GMT  (37kb)", "http://arxiv.org/abs/1602.05765v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["shoaib jameel", "steven schockaert"], "accepted": false, "id": "1602.05765"}, "pdf": {"name": "1602.05765.pdf", "metadata": {"source": "CRF", "title": "Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning", "authors": ["Shoaib Jameel", "Steven Schockaert"], "emails": ["JameelS1@cardiff.ac.uk", "SchockaertS1@cardiff.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.05 765v 1 [cs.A I] 1 8Fe b20 16"}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to move to another world in which they are able, in which they are able to move, and in which they are able to move."}, {"heading": "2 Related work", "text": "There are several existing models that construct a vector for each word by applying some form of matrix factorization to a term-term co-occurrence matrix. [Turney et al., 2013] attempts to find word vectors that can be used to predict the probability of seeing a context word, given an occurrence of the word being modeled, while the associated Continuous Word or Word Model (CBOW) focuses on the probability that the word will be modeled, given the occurrence of a context word.3The context-dependent nature of similarity is modeled in conceptual spaces by allowing dimensions to be remodeled according to the meaning of the respective property of the word."}, {"heading": "3 Description of the model", "text": "The goal is to learn a vector space that recognizes a series of entities E in which the same semantic types are located. So, it is the way in which the individual entities of the individual entities lean on each other. We also assume that a set of binary relationships is available, and a set of words that we provide. The models we propose have the following form: Jtext + (1 \u2212), encoding that entities e and f, which are available in relation to each entity, an entity e, a description of the entity that is available. The model we propose has the following form: Jtype + Jrel) + \u03b2Jreg (3), where we (0, 1) where the entities [0, 1] and \u03b2 [0, + 2] use the relative meaning of the various components of the model."}, {"heading": "4 Evaluation", "text": "In fact, it is such that most of us are in a position to behave in the way in which they do it, namely in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the manner in which they do it, in the manner in which they do it, in the manner in which they do it, in the manner in which they do it, in the manner in which they do it, in the manner in which they do it, in the manner in the manner in which they do it, in the manner in which they do it, in the manner in the way in which they do it, in the way in which they do it, in the way in the way in which they do it, in the way in which they do it, in the way they do it, in the way in the way in which they do it, in the way in which they do it, in the way they do it, in the way it, in which they do it, in which they do it, in the way it, in the way they do it, in which they, in the way it, in the way it, in the way it, in which they do it, in the way, in which they do it, in which they do it, in the way it, in the way it, in the way it, they, in the way it, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they,"}, {"heading": "5 Conclusions", "text": "We have proposed a new method for embedding entities in vector space, based on available semantic information (from Wikidata) and text descriptions (from Wikipedia). From a technical point of view, the most important innovation of our model is the use of the regulation of core norms to encode the requirement that entities of the same semantic type should be located in a low-dimensional subspace. Specifically, the regulation of core norms allows the model to automatically select the most appropriate number of dimensions for the respective type of subspace. From a practical point of view, our main motivation was to learn subspaces that are useful as approaches to conceptual spaces. To support this view, we have shown, among other things, that many numerical attributes can be faithfully modeled as directions and that the learned representations allow us to model the induction based on the distance to a centroid. 10http: / / nlp.stangedd.u / projects"}], "references": [{"title": "Distinguishing concepts and instances in wordnet", "author": ["Enrique Alfonseca", "Suresh Manandhar"], "venue": "Proceedings of the First International Conference of the Global WordNet Association,", "citeRegEx": "Alfonseca and Manandhar. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": "Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "Bordes et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Proceedings of the Annual Conference on Neural Information Processing Systems", "author": ["A. Bordes", "N. Usunier", "A. GarciaDuran", "J. Weston", "O. Yakhnenko. Translating embeddings for modeling multi-relational data"], "venue": "pages 2787\u20132795.", "citeRegEx": "Bordes et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "editors", "author": ["Antonio Chella. A cognitive architecture for music perception exploiting conceptual spaces. In Frank Zenker", "Peter Grdenfors"], "venue": "Applications of Conceptual Spaces, pages 187\u2013203. Springer International Publishing,", "citeRegEx": "Chella. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "The logic of plausible reasoning: A core theory", "author": ["A. Collins", "R. Michalski"], "venue": "Cognitive Science, 13(1):1\u201349", "citeRegEx": "Collins and Michalski. 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Inducing semantic relations from conceptual spaces: a data-driven approach to plausible reasoning", "author": ["J. Derrac", "S. Schockaert"], "venue": "Artificial Intelligence, pages 74\u2013105", "citeRegEx": "Derrac and Schockaert. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["X. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "[Dong et al.,", "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Vagueness: A conceptual spaces approach", "author": ["I. Douven", "L. Decock", "R. Dietz", "P. \u00c9gr\u00e9"], "venue": "Journal of Philosophical Logic, 42:137\u2013160", "citeRegEx": "Douven et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "PhD thesis", "author": ["Maryam Fazel. Matrix rank minimization with applications"], "venue": "Stanford University,", "citeRegEx": "Fazel. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "A theory of cognitive dissonance", "author": ["Leon Festinger"], "venue": "Stanford University Press,", "citeRegEx": "Festinger. 1957", "shortCiteRegEx": null, "year": 1957}, {"title": "Unifying conceptual spaces: Concept formation in musical creative systems", "author": ["J. Forth", "G. A Wiggins", "A. McLean"], "venue": "Minds and Machines, 20:503\u2013532", "citeRegEx": "Forth et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Conceptual Spaces: The Geometry of Thought", "author": ["P. G\u00e4rdenfors"], "venue": "MIT Press", "citeRegEx": "G\u00e4rdenfors. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Semantically smooth knowledge graph embedding", "author": ["S. Guo", "Q. Wang", "B. Wang", "L. Wang", "L. Guo"], "venue": "[Guo et al.,", "citeRegEx": "Guo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "Nuclear norm minimization via active subspace selection", "author": ["Hsieh", "Olsen", "2014] Cho-Jui Hsieh", "Peder Olsen"], "venue": null, "citeRegEx": "Hsieh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2014}, {"title": "pages 133\u2013142", "author": ["Thorsten Joachims. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery", "data mining"], "venue": "ACM,", "citeRegEx": "Joachims. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Connection Science", "author": ["Antonio Lieto", "Andrea Minieri", "Alberto Piana", "Daniele P. Radicioni. A knowledge-based system for prototypical reasoning"], "venue": "27:137\u2013152,", "citeRegEx": "Lieto et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu. Learning entity", "relation embeddings for knowledge graph completion"], "venue": "pages 2181\u20132187,", "citeRegEx": "Lin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al", "2013] Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "venue": "In Proceedings of the 27th Annual Conference on Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1532\u20131543,", "citeRegEx": "Pennington et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "SIAM review", "author": ["Benjamin Recht", "Maryam Fazel", "Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization"], "venue": "52:471\u2013501,", "citeRegEx": "Recht et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Cognitive Psychology", "author": ["Eleanor H Rosch. Natural categories"], "venue": "4(3):328\u2013350,", "citeRegEx": "Rosch. 1973", "shortCiteRegEx": null, "year": 1973}, {"title": "Artificial Intelligence", "author": ["Steven Schockaert", "Henri Prade. Interpolative", "extrapolative reasoning in propositional theories using qualitative knowledge about conceptual spaces"], "venue": "202:86\u2013131,", "citeRegEx": "Schockaert and Prade. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research, 37:141\u2013188", "citeRegEx": "Turney and Pantel. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge graph and text jointly embedding", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1591\u20131601", "citeRegEx": "Wang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "RC-NET: A general framework for incorporating knowledge into word representations", "author": ["Xu et al", "2014] C. Xu", "Y. Bai", "J. Bian", "B. Gao", "G. Wang", "X. Liu", "T.-Y. Liu"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "author": ["Huaping Zhong", "Jianwen Zhang", "Zhen Wang", "Hai Wan", "Zheng Chen. Aligning knowledge", "text embeddings by entity descriptions"], "venue": "pages 267\u2013272,", "citeRegEx": "Zhong et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "Humans are remarkably adept at overcoming such challenges [Collins and Michalski, 1989; Festinger, 1957].", "startOffset": 58, "endOffset": 104}, {"referenceID": 9, "context": "Humans are remarkably adept at overcoming such challenges [Collins and Michalski, 1989; Festinger, 1957].", "startOffset": 58, "endOffset": 104}, {"referenceID": 11, "context": "The solution offered by the theory of conceptual spaces [G\u00e4rdenfors, 2000] is to represent concepts as regions in a suitable metric space.", "startOffset": 56, "endOffset": 74}, {"referenceID": 20, "context": "It is furthermore posited that most natural properties will correspond to regions that are convex, in accordance with prototype theory [Rosch, 1973].", "startOffset": 135, "endOffset": 148}, {"referenceID": 11, "context": "Using conceptual space representations, many cognitive phenomena, including vagueness and induction, can be modelled in a natural way [G\u00e4rdenfors, 2000; Douven et al., 2013; Schockaert and Prade, 2013; Lieto et al., 2015].", "startOffset": 134, "endOffset": 221}, {"referenceID": 7, "context": "Using conceptual space representations, many cognitive phenomena, including vagueness and induction, can be modelled in a natural way [G\u00e4rdenfors, 2000; Douven et al., 2013; Schockaert and Prade, 2013; Lieto et al., 2015].", "startOffset": 134, "endOffset": 221}, {"referenceID": 21, "context": "Using conceptual space representations, many cognitive phenomena, including vagueness and induction, can be modelled in a natural way [G\u00e4rdenfors, 2000; Douven et al., 2013; Schockaert and Prade, 2013; Lieto et al., 2015].", "startOffset": 134, "endOffset": 221}, {"referenceID": 15, "context": "Using conceptual space representations, many cognitive phenomena, including vagueness and induction, can be modelled in a natural way [G\u00e4rdenfors, 2000; Douven et al., 2013; Schockaert and Prade, 2013; Lieto et al., 2015].", "startOffset": 134, "endOffset": 221}, {"referenceID": 10, "context": "For example, several authors have considered conceptual spaces for music perception [Forth et al., 2010; Chella, 2015].", "startOffset": 84, "endOffset": 118}, {"referenceID": 3, "context": "For example, several authors have considered conceptual spaces for music perception [Forth et al., 2010; Chella, 2015].", "startOffset": 84, "endOffset": 118}, {"referenceID": 18, "context": "Word embeddings [Mikolov et al., 2013; Pennington et al., 2014; Turney and Pantel, 2010] represent the meaning of words as points in a high-dimensional Euclidean space, and are in this sense reminiscent of conceptual spaces.", "startOffset": 16, "endOffset": 88}, {"referenceID": 22, "context": "Word embeddings [Mikolov et al., 2013; Pennington et al., 2014; Turney and Pantel, 2010] represent the meaning of words as points in a high-dimensional Euclidean space, and are in this sense reminiscent of conceptual spaces.", "startOffset": 16, "endOffset": 88}, {"referenceID": 5, "context": "The solution proposed in [Derrac and Schockaert, 2015] is to learn a different vector space representation for each semantic type (e.", "startOffset": 25, "endOffset": 54}, {"referenceID": 5, "context": "Third, MDS requires a distance matrix whose size is quadratic in the number of entities, which severely limits the scalability of the method from [Derrac and Schockaert, 2015].", "startOffset": 146, "endOffset": 175}, {"referenceID": 22, "context": "Several existing models construct a vector for each word by applying some form of matrix factorization to a term-term co-occurrence matrix; see [Turney and Pantel, 2010] for an overview of such approaches.", "startOffset": 144, "endOffset": 169}, {"referenceID": 18, "context": "In [Pennington et al., 2014], the authors analyze what characteristics of a word embedding model can explain this effect, and propose a new model, called GloVe, which is explicitly aimed at capturing linear regularities.", "startOffset": 3, "endOffset": 28}, {"referenceID": 6, "context": "Several authors have looked at the problem of automatically expanding such knowledge graphs [Dong et al., 2014].", "startOffset": 92, "endOffset": 111}, {"referenceID": 1, "context": "The idea of embedding knowledge graphs in a vector space was proposed in [Bordes et al., 2011].", "startOffset": 73, "endOffset": 94}, {"referenceID": 2, "context": "[Bordes et al., 2013] a simpler alternative, called TransE, was proposed, which represents reach relation as a vector and considers the following scoring function instead:", "startOffset": 0, "endOffset": 21}, {"referenceID": 23, "context": "However, as noted in [Wang et al., 2014], TransE is mostly suitable for one-to-one relations.", "startOffset": 21, "endOffset": 40}, {"referenceID": 16, "context": "The TransR model, introduced in [Lin et al., 2015], follows a similar strategy, but instead associates an m-dimensional vector rk and an m\u00d7 n matrix Mk with each relation, and uses the following scoring function:", "startOffset": 32, "endOffset": 50}, {"referenceID": 16, "context": "Finally, [Lin et al., 2015] also proposes a variant CTransR, in which each entities are clustered, and each relation can have a different representation for each cluster.", "startOffset": 9, "endOffset": 27}, {"referenceID": 12, "context": "One other approach that explicitly takes semantic type into account is [Guo et al., 2015].", "startOffset": 71, "endOffset": 89}, {"referenceID": 23, "context": "In [Wang et al., 2014] a model is proposed that combines word embedding with knowledge graph embedding.", "startOffset": 3, "endOffset": 22}, {"referenceID": 25, "context": "An improvement of this method was proposed in [Zhong et al., 2015], where the alignment is instead based on the text of the Wikipedia article of the entity.", "startOffset": 46, "endOffset": 66}, {"referenceID": 5, "context": "Following [Derrac and Schockaert, 2015], we therefore use word co-occurrence as a proxy for feature values.", "startOffset": 10, "endOffset": 39}, {"referenceID": 8, "context": "The relaxation suggested in [Fazel, 2002] is to minimize the nuclear norm \u2016M\u2016\u2217 instead (i.", "startOffset": 28, "endOffset": 41}, {"referenceID": 19, "context": "This technique was empirically shown to lead to low-rank matrix solutions in many applications, and is known to be equivalent to rank minimization in certain cases [Recht et al., 2010].", "startOffset": 164, "endOffset": 184}, {"referenceID": 0, "context": "7 only 7K were found to be instances in [Alfonseca and Manandhar, 2002]).", "startOffset": 40, "endOffset": 71}, {"referenceID": 18, "context": "Following [Pennington et al., 2014], we have used a window size of m = 10 (but without crossing sentence boundaries).", "startOffset": 10, "endOffset": 35}, {"referenceID": 23, "context": "In the case of Wikipedia, we adopted a fairly straightforward preprocessing strategy, as used in many other works such as [Wang et al., 2014].", "startOffset": 122, "endOffset": 141}, {"referenceID": 23, "context": "We consider three variants of this baseline: pTransEanch is the version proposed in [Wang et al., 2014], which uses anchor text for aligning word vectors and entity vectors; pTransEart is the improvement proposed in [Zhong et al.", "startOffset": 84, "endOffset": 103}, {"referenceID": 25, "context": ", 2014], which uses anchor text for aligning word vectors and entity vectors; pTransEart is the improvement proposed in [Zhong et al., 2015], which uses the words in the Wikipedia article de instead of anchor text (and a slightly different model); pTransEfull is a variant of pTransEart, which uses the bag of words representation We instead, as in our method.", "startOffset": 120, "endOffset": 140}, {"referenceID": 23, "context": "We used Bernoulli sampling for selecting negative examples (see [Wang et al., 2014]); we also evaluated uniform sampling (not shown), and found the results to be very similar to Bernoulli sampling but slightly worse.", "startOffset": 64, "endOffset": 83}, {"referenceID": 5, "context": "Finally, we have compared our method with the multi-dimensional scaling (MDS) based method from [Derrac and Schockaert, 2015], in which case we learn a separate vector space for every semantic type.", "startOffset": 96, "endOffset": 125}, {"referenceID": 14, "context": "From the training set, a direction is estimated using the SVMRank model8 [Joachims, 2002].", "startOffset": 73, "endOffset": 89}, {"referenceID": 5, "context": "The results in Table 2 compare our model against the MDS model from [Derrac and Schockaert, 2015] on a reduced set of 27 problem instances.", "startOffset": 68, "endOffset": 97}], "year": 2016, "abstractText": "Conceptual spaces are geometric representations of conceptual knowledge, in which entities correspond to points, natural properties correspond to convex regions, and the dimensions of the space correspond to salient features. While conceptual spaces enable elegant models of various cognitive phenomena, the lack of automated methods for constructing such representations have so far limited their application in artificial intelligence. To address this issue, we propose a method which learns a vector-space embedding of entities from Wikipedia and constrains this embedding such that entities of the same semantic type are located in some lower-dimensional subspace. We experimentally demonstrate the usefulness of these subspaces as (approximate) conceptual space representations by showing, among others, that important features can be modelled as directions and that natural properties tend to correspond to convex regions.", "creator": "LaTeX with hyperref package"}}}