{"id": "1602.05916", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning", "abstract": "We show a Talagrand-type of concentration inequality for Multi-Task Learning (MTL), using which we establish sharp excess risk bounds for MTL in terms of distribution- and data-dependent versions of the Local Rademacher Complexity (LRC). We also give a new bound on the LRC for strongly convex hypothesis classes, which applies not only to MTL but also to the standard i.i.d. setting. Combining both results, one can now easily derive fast-rate bounds on the excess risk for many prominent MTL methods, including---as we demonstrate---Schatten-norm, group-norm, and graph-regularized MTL. The derived bounds reflect a relationship akeen to a conservation law of asymptotic convergence rates. This very relationship allows for trading o? slower rates w.r.t. the number of tasks for faster rates with respect to the number of available samples per task, when compared to the rates obtained via a traditional, global Rademacher analysis.", "histories": [["v1", "Thu, 18 Feb 2016 19:13:23 GMT  (250kb)", "http://arxiv.org/abs/1602.05916v1", null], ["v2", "Thu, 9 Feb 2017 22:48:06 GMT  (267kb)", "http://arxiv.org/abs/1602.05916v2", "In this version, some arguments and results (of the previous version) have been corrected, or modified"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["niloofar yousefi", "yunwen lei", "marius kloft", "mansooreh mollaghasemi", "georgios anagnostopoulos"], "accepted": false, "id": "1602.05916"}, "pdf": {"name": "1602.05916.pdf", "metadata": {"source": "META", "title": "Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning", "authors": ["Niloofar Yousefi", "Yunwen Lei", "Marius Kloft", "Mansooreh Mollaghasemi", "Georgios Anagnastapolous"], "emails": ["niloofaryousefi@knights.ucf.edu,", "yunwen.lei@hotmail.com,", "kloft@hu-berlin.de,", "Mansooreh.Mollaghasemi@ucf.edu", "georgio@fit.edu"], "sections": [{"heading": null, "text": "This is just one example of simultaneous analysis of a collection of conceptual tasks, each of which has its own data. Such an approach can be beneficial for learning each task independently of each other when the tasks lack a sufficient body of observed data. MTL is achieved by collectively limiting the task hypotheses so that tasks regulate each other, facilitating the learning of others based on their intertask relatedness; this exchange mechanism is often referred to as information exchange. Pioneering work on MTL includes [16, 9, 2, 4]. Today, MTL frameworks are used in a variety of settings. Some recent examples include image segmentation, HIV screening [12], collaborative facial examinations [12], [15]."}], "references": [{"title": "Hierarchical kernel stick-breaking process for multi-task image analysis", "author": ["Qi An", "Chunping Wang", "Ivo Shterev", "Eric Wang", "Lawrence Carin", "David B Dunson"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Rie Kubota Ando", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Learning the graph of relations among multiple tasks", "author": ["Andreas Argyriou", "St\u00e9phan Cl\u00e9men\u00e7on", "Ruocong Zhang"], "venue": "ICML workshop on New Learning Frameworks and Models for Big Data,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Convex multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "An algorithm for transfer learning in a heterogeneous environment", "author": ["Andreas Argyriou", "Andreas Maurer", "Massimiliano Pontil"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "A spectral regularization framework for multi-task structure learning", "author": ["Andreas Argyriou", "Massimiliano Pontil", "Yiming Ying", "Charles A Micchelli"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Local rademacher complexities", "author": ["Peter L Bartlett", "Olivier Bousquet", "Shahar Mendelson"], "venue": "Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "A model of inductive bias learning", "author": ["Jonathan Baxter"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "A notion of task relatedness yielding provable multiple-task learning guarantees", "author": ["Shai Ben-David", "Reba Schuller Borbely"], "venue": "Machine learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Exploiting task relatedness for multiple task learning", "author": ["Shai Ben-David", "Reba Schuller"], "venue": "In Learning Theory and Kernel Machines,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Multi-task learning for hiv therapy screening", "author": ["Steffen Bickel", "Jasmina Bogojeska", "Thomas Lengauer", "Tobias Scheffer"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "A bennett concentration inequality and its application to suprema of empirical processes", "author": ["Olivier Bousquet"], "venue": "Comptes Rendus Mathematique,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Concentration inequalities for sub-additive functions using the entropy method", "author": ["Olivier Bousquet"], "venue": "Birkhuser Basel,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Transfer learning for collective link prediction in multiple heterogenous domains", "author": ["Bin Cao", "Nathan N Liu", "Qiang Yang"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Learning kernels using local rademacher complexity", "author": ["Corinna Cortes", "Marius Kloft", "Mehryar Mohri"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Algorithmic Learning Theory: 22nd International Conference, ALT", "author": ["Corinna Cortes", "Mehryar Mohri"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Domain adaptation and sample bias correction theory and algorithm for regression", "author": ["Corinna Cortes", "Mehryar Mohri"], "venue": "Theoretical Computer Science,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Multi-task feature learning", "author": ["A Evgeniou", "Massimiliano Pontil"], "venue": "Advances in neural information processing systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Regularization techniques for learning with matrices", "author": ["Sham M Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Zhuoliang Kang", "Kristen Grauman", "Fei Sha"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "The local rademacher complexity of lpnorm multiple kernel learning", "author": ["Marius Kloft", "Gilles Blanchard"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "On the convergence rate of lp-norm multiple kernel learning", "author": ["Marius Kloft", "Gilles Blanchard"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Empirical margin distributions and bounding the generalization error of combined classifiers", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": "Ann. Statist., 30(1):1\u201350,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Local rademacher complexities and oracle inequalities in risk minimization", "author": ["Vladimir Koltchinskii"], "venue": "Ann. Statist., 34(6):2593\u20132656,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Rademacher complexities and bounding the excess risk in active learning", "author": ["Vladimir Koltchinskii"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["Abhishek Kumar", "Hal Daume III"], "venue": "arXiv preprint arXiv:1206.6417,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Probability in Banach Spaces: isoperimetry and processes", "author": ["Michel Ledoux", "Michel Talagrand"], "venue": "Springer Science & Business Media,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Local rademacher complexity bounds based on covering numbers", "author": ["Yunwen Lei", "Lixin Ding", "Yingzhou Bi"], "venue": "[cs.AI],", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Multitask classification hypothesis space with improved generalization bounds", "author": ["Cong Li", "Michael Georgiopoulos", "Georgios C Anagnostopoulos"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Taking advantage of sparsity in multi-task learning", "author": ["K Lounici", "M Pontil", "AB Tsybakov", "SA Van De Geer"], "venue": "In COLT 2009-The 22nd Conference on Learning Theory,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Khintchine inequalities in cp (1 < p < \u221e)", "author": ["F. Lust-Piquard"], "venue": "COMPTES RENDUS DE L ACADEMIE DES SCIENCES SERIE I-MATHEMATIQUE,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1986}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In Proceedings of The 22nd Annual Conference on Learning Theory (COLT", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "Domain adaptation with multiple sources", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Multiple source adaptation and the r\u00c9nyi divergence", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "Robust domain adaptation", "author": ["Yishay Mansour", "Mariano Schain"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Bounds for linear multi-task learning", "author": ["Andreas Maurer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "The rademacher complexity of linear transformation classes. In Learning Theory, pages 65\u201378", "author": ["Andreas Maurer"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Algorithmic Learning Theory: 25th International Conference, ALT", "author": ["Andreas Maurer"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Excess risk bounds for multitask learning with trace norm regularization", "author": ["Andreas Maurer", "Massimiliano Pontil"], "venue": "In Conference on Learning Theory,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "The benefit of multitask representation learning", "author": ["Andreas Maurer", "Massimiliano Pontil", "Bernardino Romera-Paredes"], "venue": "arXiv preprint arXiv:1505.06279,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "On the performance of kernel classes", "author": ["Shahar Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2003}, {"title": "Local rademacher complexity: Sharper risk bounds with and without unlabeled samples", "author": ["Luca Oneto", "Alessandro Ghio", "Sandro Ridella", "Davide Anguita"], "venue": "Neural Networks,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Multi-task and lifelong learning of kernels", "author": ["Anastasia Pentina", "Shai Ben-David"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Lifelong learning with non-iid tasks", "author": ["Anastasia Pentina", "Christoph H Lampert"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "The khintchine inequalities and martingale expanding sphere of their action", "author": ["G Peshkir", "Albert Nikolaevich Shiryaev"], "venue": "Russian Mathematical Surveys,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1995}, {"title": "Trace norm regularization: Reformulations, algorithms, and multi-task learning", "author": ["Ting Kei Pong", "Paul Tseng", "Shuiwang Ji", "Jieping Ye"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Exploiting unrelated tasks in multi-task learning", "author": ["Bernardino Romera-Paredes", "Andreas Argyriou", "Nadia Berthouze", "Massimiliano Pontil"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "New concentration inequalities in product spaces", "author": ["Michel Talagrand"], "venue": "Inventiones mathematicae,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1996}, {"title": "Learning to learn: Introduction", "author": ["S Thrun"], "venue": "In In Learning To Learn,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1996}, {"title": "Localized complexities for transductive learning", "author": ["I. Tolstikhin", "G. Blanchard", "M. Kloft"], "venue": "In Proceedings of the 27th Conference on Learning Theory,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}, {"title": "Multitask learning for protein subcellular location prediction", "author": ["Qian Xu", "Sinno Jialin Pan", "Hannah Hong Xue", "Qiang Yang"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB),", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2011}, {"title": "Generalization bounds for domain adaptation", "author": ["Chao Zhang", "Lei Zhang", "Jieping Ye"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2012}, {"title": "Multi-task warped gaussian process for personalized age estimation", "author": ["Yu Zhang", "Dit-Yan Yeung"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "Pioneering works on MTL include [16, 9, 2, 4].", "startOffset": 32, "endOffset": 45}, {"referenceID": 8, "context": "Pioneering works on MTL include [16, 9, 2, 4].", "startOffset": 32, "endOffset": 45}, {"referenceID": 1, "context": "Pioneering works on MTL include [16, 9, 2, 4].", "startOffset": 32, "endOffset": 45}, {"referenceID": 3, "context": "Pioneering works on MTL include [16, 9, 2, 4].", "startOffset": 32, "endOffset": 45}, {"referenceID": 0, "context": "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.", "startOffset": 48, "endOffset": 51}, {"referenceID": 11, "context": "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.", "startOffset": 105, "endOffset": 109}, {"referenceID": 54, "context": "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.", "startOffset": 145, "endOffset": 149}, {"referenceID": 52, "context": "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.", "startOffset": 188, "endOffset": 192}, {"referenceID": 24, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 145, "endOffset": 149}, {"referenceID": 7, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 175, "endOffset": 178}, {"referenceID": 37, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 209, "endOffset": 213}, {"referenceID": 38, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 215, "endOffset": 219}, {"referenceID": 20, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 221, "endOffset": 225}, {"referenceID": 40, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 227, "endOffset": 231}, {"referenceID": 39, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 233, "endOffset": 237}, {"referenceID": 41, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 242, "endOffset": 246}, {"referenceID": 25, "context": "More recently, the seminal works in [26] and [7] introduced a more nuanced variant of these complexities, termed Local Rademacher Complexity (LRC) (as opposed to the original Global Rademacher Complexity (GRC)).", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "More recently, the seminal works in [26] and [7] introduced a more nuanced variant of these complexities, termed Local Rademacher Complexity (LRC) (as opposed to the original Global Rademacher Complexity (GRC)).", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "This new, modified function class complexity measure is attention-worthy, since, as shown in [7], a LRCs-based (local) analysis is capable of producing more rapidly-converging excess risk bounds, when compared to the ones obtained via a GRC (global) analysis.", "startOffset": 93, "endOffset": 96}, {"referenceID": 26, "context": "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].", "startOffset": 177, "endOffset": 181}, {"referenceID": 22, "context": "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].", "startOffset": 208, "endOffset": 212}, {"referenceID": 16, "context": "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].", "startOffset": 217, "endOffset": 221}, {"referenceID": 51, "context": "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].", "startOffset": 245, "endOffset": 249}, {"referenceID": 43, "context": "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].", "startOffset": 267, "endOffset": 271}, {"referenceID": 29, "context": "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].", "startOffset": 316, "endOffset": 320}, {"referenceID": 37, "context": "3 Previous Related Works Earlier works that investigate MTL generalization guarantees employing Rademacher averages include [38], which considers linear MTL frameworks for binary classification.", "startOffset": 124, "endOffset": 128}, {"referenceID": 38, "context": "Another study, [39], provides bounds for the empirical and expected Rademacher complexities of", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "In [21], the authors take advantage of the strongly-convex nature of certain matrix-norm regularizers to easily obtain generalization bounds for a variety of machine learning problems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 40, "context": "Moreover, [41] presents a global Rademacher complexity analysis leading to both data and distribution-dependent excess risk bounds of order O( \u221a log(n)/n) and non-vanishing w.", "startOffset": 10, "endOffset": 14}, {"referenceID": 39, "context": "Also, [40] examines the bounding of (global) Gaussian complexities of function classes that result from considering composite maps, as it is typical in MTL among other settings.", "startOffset": 6, "endOffset": 10}, {"referenceID": 41, "context": "More recently, [42] presents excess risk bounds for both MTL and Learning-To-Learn (LTL) settings and reveals conditions, under which MTL is more beneficial over learning tasks independently.", "startOffset": 15, "endOffset": 19}, {"referenceID": 38, "context": "The accompanying bounds are of order O(1/ \u221a nT ) and, compared to the results in [39, 41], it enjoys the advantage of vanishing as T \u2192 +\u221e.", "startOffset": 81, "endOffset": 89}, {"referenceID": 40, "context": "The accompanying bounds are of order O(1/ \u221a nT ) and, compared to the results in [39, 41], it enjoys the advantage of vanishing as T \u2192 +\u221e.", "startOffset": 81, "endOffset": 89}, {"referenceID": 50, "context": "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].", "startOffset": 83, "endOffset": 99}, {"referenceID": 10, "context": "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].", "startOffset": 83, "endOffset": 99}, {"referenceID": 9, "context": "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].", "startOffset": 83, "endOffset": 99}, {"referenceID": 45, "context": "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].", "startOffset": 83, "endOffset": 99}, {"referenceID": 44, "context": "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].", "startOffset": 104, "endOffset": 108}, {"referenceID": 33, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 82, "endOffset": 106}, {"referenceID": 35, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 82, "endOffset": 106}, {"referenceID": 34, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 82, "endOffset": 106}, {"referenceID": 17, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 82, "endOffset": 106}, {"referenceID": 53, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 82, "endOffset": 106}, {"referenceID": 36, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 82, "endOffset": 106}, {"referenceID": 18, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "2 Talagrand-Type Inequality for Multi-Task Learning The derivation of our LRC-based error bounds for MTL is founded on the following modified Talagrand\u2019s concentration inequality [13, 50] adapted to the context of MTL, showing that the uniform deviation between the true and empirical means in a vector-valued function class F can be dominated by the associated multitask Rademacher complexity plus a term involving the variance of functions in F .", "startOffset": 179, "endOffset": 187}, {"referenceID": 49, "context": "2 Talagrand-Type Inequality for Multi-Task Learning The derivation of our LRC-based error bounds for MTL is founded on the following modified Talagrand\u2019s concentration inequality [13, 50] adapted to the context of MTL, showing that the uniform deviation between the true and empirical means in a vector-valued function class F can be dominated by the associated multitask Rademacher complexity plus a term involving the variance of functions in F .", "startOffset": 179, "endOffset": 187}, {"referenceID": 37, "context": "In Theorem 1, the data from different tasks assumed to be mutually independent, which is typical in the MTL setting [38].", "startOffset": 116, "endOffset": 120}, {"referenceID": 28, "context": "Also, using Talagrand\u2019s Lemma [29], one can verify", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "3 of [7] to MTL function classes) to the function class H\u2217 F completes the proof.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "4 in [7], presents a data-dependent version of (5) replacing the Rademacher complexity in Corollary 3 with its empirical counterpart.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "4 in [7] and, therefore, can be found in the Appendix.", "startOffset": 5, "endOffset": 8}, {"referenceID": 38, "context": "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].", "startOffset": 228, "endOffset": 249}, {"referenceID": 19, "context": "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].", "startOffset": 228, "endOffset": 249}, {"referenceID": 5, "context": "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].", "startOffset": 228, "endOffset": 249}, {"referenceID": 3, "context": "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].", "startOffset": 228, "endOffset": 249}, {"referenceID": 30, "context": "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].", "startOffset": 228, "endOffset": 249}, {"referenceID": 2, "context": "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].", "startOffset": 228, "endOffset": 249}, {"referenceID": 19, "context": "3 Group Norm Regularized MTL We first consider the group norm regularized MTL capturing the inter-task relationships by the group norm \u2016W \u20162,q := (\u2211T t=1 \u2016wt\u2016 q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form Fq := { X 7\u2192 [\u3008w1, \u03c6(X1)\u3009 , .", "startOffset": 168, "endOffset": 183}, {"referenceID": 3, "context": "3 Group Norm Regularized MTL We first consider the group norm regularized MTL capturing the inter-task relationships by the group norm \u2016W \u20162,q := (\u2211T t=1 \u2016wt\u2016 q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form Fq := { X 7\u2192 [\u3008w1, \u03c6(X1)\u3009 , .", "startOffset": 168, "endOffset": 183}, {"referenceID": 31, "context": "3 Group Norm Regularized MTL We first consider the group norm regularized MTL capturing the inter-task relationships by the group norm \u2016W \u20162,q := (\u2211T t=1 \u2016wt\u2016 q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form Fq := { X 7\u2192 [\u3008w1, \u03c6(X1)\u3009 , .", "startOffset": 168, "endOffset": 183}, {"referenceID": 48, "context": "3 Group Norm Regularized MTL We first consider the group norm regularized MTL capturing the inter-task relationships by the group norm \u2016W \u20162,q := (\u2211T t=1 \u2016wt\u2016 q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form Fq := { X 7\u2192 [\u3008w1, \u03c6(X1)\u3009 , .", "startOffset": 168, "endOffset": 183}, {"referenceID": 5, "context": "Among p \u2265 1, a particular group-norm of independent interest is the sparsity-inducing group-norm achieved by q = 1 [6, 4], for which we can take \u03ba\u2217 = logT to get that R(F1, r) \u2264 \u221a \u221a \u221a \u221a 4 nT \u2225 \u2225 \u2225 ( \u221e \u2211", "startOffset": 115, "endOffset": 121}, {"referenceID": 3, "context": "Among p \u2265 1, a particular group-norm of independent interest is the sparsity-inducing group-norm achieved by q = 1 [6, 4], for which we can take \u03ba\u2217 = logT to get that R(F1, r) \u2264 \u221a \u221a \u221a \u221a 4 nT \u2225 \u2225 \u2225 ( \u221e \u2211", "startOffset": 115, "endOffset": 121}, {"referenceID": 5, "context": "4 Schatten Norm Regularized MTL [6] developed a spectral regularization framework for MTL where the Schatten p-norm \u2016W\u2016Sq := [ tr ( W W ) q 2 ] 1 q", "startOffset": 32, "endOffset": 35}, {"referenceID": 40, "context": "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].", "startOffset": 85, "endOffset": 93}, {"referenceID": 47, "context": "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].", "startOffset": 85, "endOffset": 93}, {"referenceID": 4, "context": "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].", "startOffset": 210, "endOffset": 221}, {"referenceID": 27, "context": "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].", "startOffset": 210, "endOffset": 221}, {"referenceID": 21, "context": "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].", "startOffset": 210, "endOffset": 221}, {"referenceID": 38, "context": "We consider the following graph regularized MTL [39]", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "Note that, due to the space limitation, the proofs of the results are provided only for the hypothesis space Fq with q \u2208 [1, 2] in (13).", "startOffset": 121, "endOffset": 127}, {"referenceID": 1, "context": "Note that, due to the space limitation, the proofs of the results are provided only for the hypothesis space Fq with q \u2208 [1, 2] in (13).", "startOffset": 121, "endOffset": 127}, {"referenceID": 0, "context": "However, for the group and LSq -Schatten norm (q \u2208 [1, 2]) regularized MTL, the proofs can be obtained in a very similar way.", "startOffset": 51, "endOffset": 57}, {"referenceID": 1, "context": "However, for the group and LSq -Schatten norm (q \u2208 [1, 2]) regularized MTL, the proofs can be obtained in a very similar way.", "startOffset": 51, "endOffset": 57}, {"referenceID": 6, "context": "4 in [7], Fq is a sub-root function.", "startOffset": 5, "endOffset": 8}, {"referenceID": 37, "context": "First, note that to obtain the GRC-based bounds, we apply Theorem 16 of [38], as we consider the same setting and assumptions for tasks\u2019 distributions as considered in this work.", "startOffset": 72, "endOffset": 76}, {"referenceID": 37, "context": "Theorem 16 (MTL excess risk bound based on GRC; Theorem 16 of [38] ).", "startOffset": 62, "endOffset": 66}, {"referenceID": 37, "context": "As it has been shown in [38], the proof of this theorem is based on using McDiarmid\u2019s inequality for Z defined in Theorem 1, and noticing that for the function class F with values in [\u2212b, b], it holds that |Z \u2212 Zs,j | \u2264 2b/nT .", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "Schatten-norm: \u2200q \u2208 [1, 2], P (l f\u0302 \u2212 lf\u2217) = O ( (R\u2032 maxq \u2217(q\u2217 \u2212 1)) 1 2 T\u2212 12n\u2212 12 ) .", "startOffset": 20, "endOffset": 26}, {"referenceID": 1, "context": "Schatten-norm: \u2200q \u2208 [1, 2], P (l f\u0302 \u2212 lf\u2217) = O ( (R\u2032 maxq \u2217(q\u2217 \u2212 1)) 1 2 T\u2212 12n\u2212 12 ) .", "startOffset": 20, "endOffset": 26}, {"referenceID": 0, "context": "Schatten-norm: \u2200q \u2208 [1, 2], P (l f\u0302 \u2212 lf\u2217) = O ( (R\u2032 maxq \u2217(q\u2217 \u2212 1)) 1 1+\u03b1T \u22121 1+\u03b1n \u2212\u03b1 1+\u03b1 ) .", "startOffset": 20, "endOffset": 26}, {"referenceID": 1, "context": "Schatten-norm: \u2200q \u2208 [1, 2], P (l f\u0302 \u2212 lf\u2217) = O ( (R\u2032 maxq \u2217(q\u2217 \u2212 1)) 1 1+\u03b1T \u22121 1+\u03b1n \u2212\u03b1 1+\u03b1 ) .", "startOffset": 20, "endOffset": 26}, {"referenceID": 40, "context": "2 Comparisons to Related Works Also, it would be interesting to compare our (global and local) results for the trace norm regularized MTL with the GRC-baesd excess risk bound provided in [41] wherein they apply a trace norm regularizer to capture the tasks\u2019 relatedness.", "startOffset": 187, "endOffset": 191}, {"referenceID": 40, "context": "(39) The intuition behind this assumption is interpreted as: assuming a common vector w for all tasks, the regularizer should not be a function of number of tasks [41].", "startOffset": 163, "endOffset": 167}, {"referenceID": 40, "context": "Given the task averaged covariance operator C := 1/T \u2211T t=1 Jt, the excess risk bound in [41] reads as (for the L-Lipschitz loss function l, and F with ranges in [\u2212b, b])", "startOffset": 89, "endOffset": 93}, {"referenceID": 40, "context": "[41]: P (l f\u0302 \u2212 lf\u2217) \u2264 2LR\u2032 max (\u221a \u03bbmax n + 5 \u221a ln(nT ) + 1 nT ) + \u221a bLx nT .", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Another interesting comparison can be performed between our bounds and the one introduced in [39] for a graph regularized hypothesis spaces similar to (22).", "startOffset": 93, "endOffset": 97}, {"referenceID": 38, "context": "[39] provides a bound on the empirical GRC, however, similar to the proof of Corollary 12, we can easily convert it to a distribution dependent GRC bound which in our notation reads as (assuming that \u2225 \u2225 \u2225D W \u2225 \u2225 \u2225 \u2264 \u221a TR\u2032\u2032 max)", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39]: P (l f\u0302 \u2212 lf\u2217) \u2264 2L \u221a n \u221a M\u03bbmaxR\u2032\u20322 max ( 1 \u03b4min + 1 T\u03b7 ) + \u221a bLx nT .", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "1 in [14]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 37, "context": "2) The first term in the right-hand side of the above inequality, EZ, can also be upper-bounded using the same approach as in Theorem 16 in [38].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "2 from [7] is essential component of the proof in Theorem 4.", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "4 in [7] it can be shown that \u03c8(r) defined in (A.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "4 in [7], it can be shown that with probability at least 1\u2212 4e\u2212x, \u03c8(r) \u2264 c1E\u03c3 \uf8ee", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "3 of [7], gives r\u2217 \u2264 r\u0302\u2217 which together with (A.", "startOffset": 5, "endOffset": 8}, {"referenceID": 20, "context": "Property 1 (Theorem 3 in [21]: Strong convexity/strong smoothness duality).", "startOffset": 25, "endOffset": 29}, {"referenceID": 46, "context": "Lemma 3 (Khintchine-Kahane Inequality [47]).", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "Lemma 4 (Rosenthal-Young Inequality; Lemma 3 of [24]).", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "Also, from Theorem 3 and Theorem 13 in [21], it can be shown that R(W ) = \u2016W \u201622,q is 1 q -strongly convex w.", "startOffset": 39, "endOffset": 43}, {"referenceID": 42, "context": "According to [43], it can be shown that there is a constant c such that if \u03bbt \u2265 1 nRmax , then for all r \u2265 1 n it holds R(F 1,RT \u2212 1 q ,1 , r) \u2265 \u221a c n \u2211\u221e j=1 min ( r, R2T\u2212 2 q \u03bbj1 ) , which with some algebra manipulations gives the desired result.", "startOffset": 13, "endOffset": 17}, {"referenceID": 32, "context": "Lemma 5 (Non-commutative Khintchine\u2019s inequality [33]).", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "where in A \u00a9, we assumed that the first term in the max argument is the largest Note that using Theorem 11 in [21], it can be shown that the regularization function R(W ) = \u2016W \u20162Sq with q \u2208 [1, 2] is (q \u2212 1)-strongly convex w.", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "where in A \u00a9, we assumed that the first term in the max argument is the largest Note that using Theorem 11 in [21], it can be shown that the regularization function R(W ) = \u2016W \u20162Sq with q \u2208 [1, 2] is (q \u2212 1)-strongly convex w.", "startOffset": 190, "endOffset": 196}, {"referenceID": 1, "context": "where in A \u00a9, we assumed that the first term in the max argument is the largest Note that using Theorem 11 in [21], it can be shown that the regularization function R(W ) = \u2016W \u20162Sq with q \u2208 [1, 2] is (q \u2212 1)-strongly convex w.", "startOffset": 190, "endOffset": 196}, {"referenceID": 0, "context": "Indeed, similar to the proof of Theorem 5, it can be shown that for the group norm with \u03ba = q \u2208 [1, 2], R(Fq) = EX,\u03c3 {", "startOffset": 96, "endOffset": 102}, {"referenceID": 1, "context": "Indeed, similar to the proof of Theorem 5, it can be shown that for the group norm with \u03ba = q \u2208 [1, 2], R(Fq) = EX,\u03c3 {", "startOffset": 96, "endOffset": 102}], "year": 2017, "abstractText": "We show a Talagrand-type of concentration inequality for MTL, using which we establish sharp excess risk bounds for Multi-Task Learning (MTL) in terms of distributionand data-dependent versions of the Local Rademacher Complexity (LRC). We also give a new bound on the LRC for strongly convex hypothesis classes, which applies not only to MTL but also to the standard i.i.d. setting. Combining both results, one can now easily derive fast-rate bounds on the excess risk for many prominent MTL methods, including\u2014as we demonstrate\u2014Schatten-norm, group-norm, and graph-regularized MTL. The derived bounds reflect a relationship akeen to a conservation law of asymptotic convergence rates. This very relationship allows for trading off slower rates w.r.t. the number of tasks for faster rates with respect to the number of available samples per task, when compared to the rates obtained via a traditional, global Rademacher analysis.", "creator": "LaTeX with hyperref package"}}}