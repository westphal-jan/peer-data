{"id": "1602.05944", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "The Interaction of Memory and Attention in Novel Word Generalization: A Computational Investigation", "abstract": "People exhibit a tendency to generalize a novel noun to the basic-level in a hierarchical taxonomy -- a cognitively salient category such as \"dog\" -- with the degree of generalization depending on the number and type of exemplars. Recently, a change in the presentation timing of exemplars has also been shown to have an effect, surprisingly reversing the prior observed pattern of basic-level generalization. We explore the precise mechanisms that could lead to such behavior by extending a computational model of word learning and word generalization to integrate cognitive processes of memory and attention. Our results show that the interaction of forgetting and attention to novelty, as well as sensitivity to both type and token frequencies of exemplars, enables the model to replicate the empirical results from different presentation timings. Our results reinforce the need to incorporate general cognitive processes within word learning models to better understand the range of observed behaviors in vocabulary acquisition.", "histories": [["v1", "Thu, 18 Feb 2016 20:53:26 GMT  (31kb)", "http://arxiv.org/abs/1602.05944v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["erin grant", "aida nematzadeh", "suzanne stevenson"], "accepted": false, "id": "1602.05944"}, "pdf": {"name": "1602.05944.pdf", "metadata": {"source": "CRF", "title": "The Interaction of Memory and Attention in Novel Word Generalization: A Computational Investigation", "authors": ["Erin Grant", "Aida Nematzadeh", "Suzanne Stevenson"], "emails": ["suzanne}@cs.toronto.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.05 944v 1 [cs.C L] 18 Feb 2016"}, {"heading": "Introduction", "text": "A number of computer models have successfully mimicked the behavior of children by studying the meaning of words from ambiguous input objects (e.g. Siskind, 1996; Yu & Ballard, 2007; Frank et al., 2007; Fazly, Alishahi, & Stevenson, 2010), but one challenge in terms of word meaning acquisition that has received less attention is that of novel word generalizations: i.e., all dogs of different breeds, or any kind of animal? This problem presents difficulties for the learner because the accumulated evidence may be consistent with more than one of these decisions. In this example, all dogs are also animals, and thus the meaning \"animal\" could also be consistent with all uses of the word dog.Xu and tensizing."}, {"heading": "Suspicious Coincidence: Data and Models", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "Our Computational Model", "text": "We start with the NGS15 model because it uses an incremental vocabulary learning framework that mimics a range of vocabulary acquisition behaviors (e.g. Fazly, Alishahi, & Stevenson, 2010; Fazly, Ahmadi-Fakhr, et al., 2010), which has recently been expanded to take into account the effects of memory and attention on text learning (Nematzadeh, Fazly, & Stevenson, 2012), which is a natural opportunity to integrate these processes into word generalization."}, {"heading": "Learned Meanings in the NGS15 Model", "text": "The NGS15 model is a cross-situational learner who follows weighted co-occurrences of words and semantic characteristics in his inputs, as in Fazly, Alishahi and Stevenson (2010). Thus, the input into the model is to reflect the naturalistic input a child is exposed to, which consists of linguistic inputs (the words a child hears) paired with non-linguistic data (the things a child perceives). An input pair is the set of words Ut and the set of semantic characteristics observed by St: Ut: {look, a, fep} St: {PERCEPTION, LOOK,... DALMATIAN, DOG, ANIMAL} The output of the model at any time t is a group of probabilities of meaning, Pt (fi | w j), for each characteristic fi | j, for each characteristic fi and each word w j, observed over time."}, {"heading": "The NGS15 Learning Algorithm", "text": "The input to the model is processed in an incremental two-step mapping frame: words and attributes occurring side by side are aligned in relation to the current probabilities of meaning, which are then updated with the new proofs regarding the strength of the association as follows. The alignment step for an input pair in time t, the alignment strength for each attribute is fi-St and word w-Ut: at (fi, w-j) = Pt-1 (fi | w-j) \u2211 w-1 (fi | w-1) (1) These alignment attributes are accumulated incrementally as: assoct (fi, w-j) = p-T at (fi, w-j) j. (2), where T occurred at all times where fi and w-j occurred together. In our model, when more than one instance of a characteristic occurs in time t, multiple instances of alignment for that feature and the word are recorded."}, {"heading": "Our Extensions to Integrate Memory and Attention", "text": "We adopt the general approach of Nematzadeh et al. (2012) because it makes the memory and attention function a function that we do not explicitly include in the intersitutional word-learning mechanism; the approach has been shown to take into account distance effects in word learning that are closely related to the presentation time factors considered by SPSS11. However, the methods only need to be expanded to adequately meet the needs of word generalization in the NGS15 model; we describe these extensions here.Modeling the effects of forgetting. To model the effect of memory, we use the associative formula of Nematzadeh et al. (2012), which implements \"forgetting\" by applying a decay factor to each alignment probability (cf. Eqn. 2 above): assoct (fi, w j j) = \"act\" (fi, w j j) (t) (\"sum \u2212 dat 1\" in the hierarchy of time (4)."}, {"heading": "Methodology", "text": "We follow the methods of NGS15, adapted where for our advanced model on the SPSS11 data.3Training the Model. We use a taxonomy with three levels that correspond to the subordinate, basic and superordinate categories of animals. This results in four feature groups, one per category level plus an \"instance\" group to distinguish several objects of the same subordinate category. See Figure 2: In each Ut-St input pair, Ut consists of the novel word, and St is a set of four features (one per feature group) that represent a unique instance of the same subordinate category in all educational studies; for example: {fep} St: {INSTANCE1, DALMATIAN, DOG} In the 1 example condition, training consists of just one such code and data we are at https: / / github.com / eringrant / novel word generalizations."}, {"heading": "Model Results and Discussion", "text": "In fact, you will be able to move to another world, you will have to move to another world, you will have to move to another world, you will have to move to another world, you will have to move to another world, you will have to move to another world, you will have to move to another world, you will be able to create a new world, you will be able to create a new world, you will be able to create a new world, you will be able to create a new world, you will be able to create a new world, you will be able to create a new world."}, {"heading": "Conclusions and Future Work", "text": "We therefore propose a unified model of word learning that takes into account the different observed patterns of novel word generalization - in particular, the suspicious random effect (Xu & Tenenbaum, 2007) and its reversal under different presentation conditions (Spencer et al., 2011). We extend the Nematzadeh et al. (2015) model to include a novel integration of the general cognitive mechanisms of memory and attention, and show that the success of our model is the result of the interaction of forgetting and attention to the novelty of word characteristics. Our approach builds on the earlier NGS15 model by highlighting the importance of type and symbol frequency patterns in the input to capture interesting generalization effects."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "People exhibit a tendency to generalize a novel noun to the basic-level in a hierarchical taxonomy \u2013 a cognitively salient category such as \u201cdog\u201d \u2013 with the degree of generalization depending on the number and type of exemplars. Recently, a change in the presentation timing of exemplars has also been shown to have an effect, surprisingly reversing the prior observed pattern of basic-level generalization. We explore the precise mechanisms that could lead to such behavior by extending a computational model of word learning and word generalization to integrate cognitive processes of memory and attention. Our results show that the interaction of forgetting and attention to novelty, as well as sensitivity to both type and token frequencies of exemplars, enables the model to replicate the empirical results from different presentation timings. Our results reinforce the need to incorporate general cognitive processes within word learning models to better understand the range of observed behaviors in vocabulary acquisition.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}