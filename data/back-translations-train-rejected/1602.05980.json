{"id": "1602.05980", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "Revise Saturated Activation Functions", "abstract": "It has been generally believed that training deep neural networks is hard with saturated activation functions, including Sigmoid and Tanh. Recent works shows that deep Tanh networks are able to converge with careful model initialization while deep Sigmoid networks still fail. In this paper, we propose a re-scaled Sigmoid function which is able to maintain the gradient in a stable scale. In addition, we break the symmetry of Tanh by penalizing the negative part. Our preliminary results on deep convolution networks shown that, even without stabilization technologies such as batch normalization and sophisticated initialization, the \"re-scaled Sigmoid\" converges to local optimality robustly. Furthermore the \"leaky Tanh\" is comparable or even outperforms the state-of-the-art non-saturated activation functions such as ReLU and leaky ReLU.", "histories": [["v1", "Thu, 18 Feb 2016 21:26:53 GMT  (30kb)", "http://arxiv.org/abs/1602.05980v1", null], ["v2", "Mon, 2 May 2016 08:56:19 GMT  (56kb,D)", "http://arxiv.org/abs/1602.05980v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bing xu", "ruitong huang", "mu li"], "accepted": false, "id": "1602.05980"}, "pdf": {"name": "1602.05980.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["antinucleon@gmail.com", "rtonghuang@gmail.com", "muli@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.05 980v 1 [cs.L G] 18 Feb 2016 Underreview for Workshop track - ICLR 2016"}, {"heading": "1 INTRODUCTION", "text": "Sigmoid and Tanh are among the most notable saturated activation functions in neural network literature. Careful LSUV initialization (Mishkin & Matas, 2015) allows the deep Tanh network to converge to a local optimality through random initialization, while the deep Sigmoid network fails with LSUV initialization; a publicly accepted reason for this failure is the disappearance (and / or explosion) that occurs with saturated activation functions. Based on this explanation, layer-by-layer pretraining (Hinton & Salakhutdinov, 2006; Bengio et al., 2007) or batch normalization (Ioffe & Szegedy, 2015) can be used as an efficient way to address this saturation problem."}, {"heading": "2 UNDERSTANDING DIFFICULTY OF TRAINING DEEP SIGMOID NETWORK", "text": "In this section, we analyze the behavior of the deep sigmoid network based on the idea of popular initialization (Glorot & Bengio, 2010; He et al., 2015; Mishkin & Matas, 2015) that the gradient and output variance of each layer can be better maintained on a stable scale (at least for the first few iterations). Then, we propose our method to fix the detected problem in deep sigmoid. \u2212 \u2212 Assuming that for the l-th layer in the neural network, the input dimension and output are the same, nl. Then, with the activation function f, in a forward step havex (l) = f (l \u2212 1) \u2212 \u2212 y (l) = W (l) x (l) x (l) + b (l) (l) (l), where y (l) (2) is the output, x (l), x (l) is the current input, W (l) is the weight matrix, and l (l) is the term."}, {"heading": "3 SATURATED ACTIVATION FUNCTION WITH LEAKY", "text": "In the case of the use of unsaturated activation functions, it has been reported that the leak in the negative part of the activation is a way to improve network performance (Xu et al., 2015; Clevert et al., 2015).In this section, we test the same idea on the Tanh function as follows: Leak (x) = {tanh (x) if x > 0 a \u00b7 tanh (x) if x < 0 where a leak point [0, 12] (11) We experiment with ReLU, Leaky ReLU (with a = 0.25), Sigmoid, Sigmoid *, Tanh and Leaky Tanh on CIFAR-100 with 33-layer Inception Network (Ioffe & Szegedy, 2015), but removed all batch normalization layers (with a = 0.25), Sigmoid, Sigmoid, Sigmoid *, Tanh and Leaky Tanh on CIFAR-100 with 33-layer Inception Network (IFAR-100, 2015), but removed all batch normalization layers (with a = 0.25), Tanh, Sigmoid, Sigmoid, Sigmoid *, and Leaky Tanh on CIFAR-100 with 33-layer Inception Network (IFAR-100)."}, {"heading": "4 CONCLUSION & FUTURE WORK", "text": "The result of this work is twofold: First, we try to explain and correct the failure of training a deep sigmoid network based on the idea of the work (Glorot & Bengio, 2010); second, we try to investigate the differences in network performance between using saturated activation and using unsaturated activation functions in order to train the deep sigmoid network; and third, we suggest that using the leaky trick, the saturation of the activation function is comparable to ReLU and Leaky ReLU. There are still many open questions that require further investigation: 1. How to efficiently determine different learning rates for different layers in a very deep neural network; 2. How do the positive part (an [0, + \u221e) and the negative part (an (\u2212 \u00b2, 0]) of the activation function affect the performance of the network?"}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank NVIDIA's GPU donation."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["Tianqi Chen", "Mu Li", "Yutian Li", "Min Lin", "Naiyan Wang", "Minjie Wang", "Tianjun Xiao", "Bing Xu", "Chiyuan Zhang", "Zheng Zhang"], "venue": "arXiv preprint arXiv:1512.01274,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "All you need is a good init", "author": ["Dmytro Mishkin", "Jiri Matas"], "venue": "arXiv preprint arXiv:1511.06422,", "citeRegEx": "Mishkin and Matas.,? \\Q2015\\E", "shortCiteRegEx": "Mishkin and Matas.", "year": 2015}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["Bing Xu", "Naiyan Wang", "Tianqi Chen", "Mu Li"], "venue": "arXiv preprint arXiv:1505.00853,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Based on the explanation, Layer-wise pretrain (Hinton & Salakhutdinov, 2006; Bengio et al., 2007) or Batch Normalization (Ioffe & Szegedy, 2015) can be used as an efficient way of tackling this saturation problem.", "startOffset": 46, "endOffset": 97}, {"referenceID": 1, "context": "All the networks in the paper are trained by using MXNet (Chen et al., 2015).", "startOffset": 57, "endOffset": 76}, {"referenceID": 4, "context": "In this section we analyze the behavior of deep Sigmoid network based on the idea of popular initialization (Glorot & Bengio, 2010; He et al., 2015; Mishkin & Matas, 2015) that the variance of the gradient and the output of each layer is better to be maintained in a stable scale (for at least the first few iterations).", "startOffset": 108, "endOffset": 171}, {"referenceID": 8, "context": "In the case of using non-saturated activation functions, leaky in negative part of activation has been reported as a way to improve the network performance (Xu et al., 2015; Clevert et al., 2015).", "startOffset": 156, "endOffset": 195}, {"referenceID": 2, "context": "In the case of using non-saturated activation functions, leaky in negative part of activation has been reported as a way to improve the network performance (Xu et al., 2015; Clevert et al., 2015).", "startOffset": 156, "endOffset": 195}], "year": 2017, "abstractText": "It has been generally believed that training deep neural networks is hard with saturated activation functions, including Sigmoid and Tanh. Recent works (Mishkin & Matas, 2015) shown that deep Tanh networks are able to converge with careful model initialization while deep Sigmoid networks still fail. In this paper, we propose a re-scaled Sigmoid function which is able to maintain the gradient in a stable scale. In addition, we break the symmetry of Tanh by penalizing the negative part. Our preliminary results on deep convolution networks shown that, even without stabilization technologies such as batch normalization and sophisticated initialization, the \u201cre-scaled Sigmoid\u201d converges to local optimality robustly. Furthermore the \u201cleaky Tanh\u201d is comparable or even outperforms the state-of-the-art non-saturated activation functions such as ReLU and leaky ReLU.", "creator": "LaTeX with hyperref package"}}}