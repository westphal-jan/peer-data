{"id": "1602.06023", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2016", "title": "Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond", "abstract": "In this work, we cast text summarization as a sequence-to-sequence problem and apply the attentional encoder-decoder RNN that has been shown to be successful for Machine Translation (Bahdanau et al. (2014)). Our experiments show that the proposed architecture significantly outperforms the state-of-the art model of Rush et al. (2015) on the Gigaword dataset without any additional tuning. We also propose additional extensions to the standard architecture, which we show contribute to further improvement in performance.", "histories": [["v1", "Fri, 19 Feb 2016 02:04:18 GMT  (34kb)", "http://arxiv.org/abs/1602.06023v1", "4 pages with references"], ["v2", "Mon, 11 Apr 2016 22:50:03 GMT  (163kb,D)", "http://arxiv.org/abs/1602.06023v2", "9 pages with references"], ["v3", "Sat, 23 Apr 2016 02:38:01 GMT  (163kb,D)", "http://arxiv.org/abs/1602.06023v3", "8 pages plus references"], ["v4", "Wed, 10 Aug 2016 22:56:10 GMT  (314kb,D)", "http://arxiv.org/abs/1602.06023v4", null], ["v5", "Fri, 26 Aug 2016 16:13:13 GMT  (314kb,D)", "http://arxiv.org/abs/1602.06023v5", null]], "COMMENTS": "4 pages with references", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ramesh nallapati", "bowen zhou", "cicero nogueira dos santos", "caglar gulcehre", "bing xiang"], "accepted": false, "id": "1602.06023"}, "pdf": {"name": "1602.06023.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["nallapati@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.06 023v 1 [cs.C L] 19 Feb 2016"}, {"heading": "1 INTRODUCTION AND RELATED WORK", "text": "In this work, we focus on the task of summarizing text, which of course can also be thought of as mapping an input sequence of words in a source document called a summary. (In the context of sequence-to-sequence models, a very relevant model for our task is the attentive RNN encoder decoder models developed in Bahdanau et al. (2014), which have produced a state-of-the-art performance in machine translation. (MT is also a task that maps one key sequence in another.)"}, {"heading": "2 MODELS, EXPERIMENTS AND RESULTS", "text": "This year it is more than ever before."}, {"heading": "3 CONCLUSION", "text": "Due to space constraints, we are unable to present representative summaries from our models, but we note that even if they do not match the gold summaries, they are surprisingly good and in most cases would easily represent a man-made summary. Our results clearly show that the sequence-to-sequence models are extremely promising for the summary; some of the other lessons we have learned from our experiments are: (i) the LVT trick is very useful for the summary, as it improves training speed without sacrificing performance; (ii) traditional methods such as vocabulary expansion and syntax-based features can also enhance the performance of deep learning-based models. As part of our future work, we plan to explore ways to effectively generate rare words in the summary, which is a glaring weakness in the existing models.5 Based on the correspondence with the authors of this work."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Endto-end attention-based large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio"], "venue": "CoRR, abs/1508.04395,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Headline generation based on statistical translation", "author": ["Michele Banko", "Vibhu O. Mittal", "Michael J Witbrock"], "venue": "In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Banko et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2000}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Erkan and Radev.,? \\Q2004\\E", "shortCiteRegEx": "Erkan and Radev.", "year": 2004}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "CoRR, abs/1412.2007,", "citeRegEx": "Jean et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky"], "venue": "CoRR, abs/1506.01057,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "CoRR, abs/1310.4546,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "CoRR, abs/1509.00685,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Sequence to sequence - video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeff Donahue", "Raymond J. Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "CoRR, abs/1505.00487,", "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "CoRR, abs/1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "In this work, we cast text summarization as a sequence-to-sequence problem and apply the attentional encoder-decoder RNN that has been shown to be successful for Machine Translation (Bahdanau et al. (2014)).", "startOffset": 183, "endOffset": 206}, {"referenceID": 0, "context": "In this work, we cast text summarization as a sequence-to-sequence problem and apply the attentional encoder-decoder RNN that has been shown to be successful for Machine Translation (Bahdanau et al. (2014)). Our experiments show that the proposed architecture significantly outperforms the state-of-the art model of Rush et al. (2015) on the Gigaword dataset without any additional tuning.", "startOffset": 183, "endOffset": 335}, {"referenceID": 0, "context": "Deep learning based sequence-to-sequence models have been successful in many problems such as machine translation (Bahdanau et al. (2014)), speech recognition (Bahdanau et al.", "startOffset": 115, "endOffset": 138}, {"referenceID": 0, "context": "Deep learning based sequence-to-sequence models have been successful in many problems such as machine translation (Bahdanau et al. (2014)), speech recognition (Bahdanau et al. (2015)) and video captioning (Venugopalan et al.", "startOffset": 115, "endOffset": 183}, {"referenceID": 0, "context": "Deep learning based sequence-to-sequence models have been successful in many problems such as machine translation (Bahdanau et al. (2014)), speech recognition (Bahdanau et al. (2015)) and video captioning (Venugopalan et al. (2015)).", "startOffset": 115, "endOffset": 232}, {"referenceID": 0, "context": "Deep learning based sequence-to-sequence models have been successful in many problems such as machine translation (Bahdanau et al. (2014)), speech recognition (Bahdanau et al. (2015)) and video captioning (Venugopalan et al. (2015)). In this work, we focus on the task of text summarization, which can also be naturally thought of as mapping an input sequence of words in a source document to a target sequence of words called summary. In the framework of sequence-to-sequence models, a very relevant model to our task is the attentional RNN encoder-decoder model proposed in Bahdanau et al. (2014), which has produced state-of-the-art performance in machine translation (MT).", "startOffset": 115, "endOffset": 599}, {"referenceID": 2, "context": "There has also been some work on abstractive summarization using traditional machine translation based models (Banko et al. (2000)).", "startOffset": 111, "endOffset": 131}, {"referenceID": 2, "context": "There has also been some work on abstractive summarization using traditional machine translation based models (Banko et al. (2000)). In the framework of deep learning, the closest to our model is the recent work of Rush et al. (2015), in which, the authors use convolutional models to encode the source and a context-sensitive attentional feed-forward neural network to generate the summary, and they produced state-of-the-art results on the Gigaword and DUC datasets.", "startOffset": 111, "endOffset": 234}, {"referenceID": 8, "context": "In this work, we used the annotated Gigaword corpus as described in Rush et al. (2015). We used the scripts made available by the authors of this work1 to preprocess the data, which resulted in about 3.", "startOffset": 68, "endOffset": 87}, {"referenceID": 6, "context": "Training: For all the models we discuss below, we used 100-dimension word-embeddings pretrained by the word2vec algorithm (Mikolov et al. (2013)) on a separate dataset comprising news stories, and we allowed the embeddings to be updated during training.", "startOffset": 123, "endOffset": 145}, {"referenceID": 3, "context": "The encoder consists of a bidirectional GRU-RNN (Chung et al. (2014)), each with a hidden state dimension of 200.", "startOffset": 49, "endOffset": 69}, {"referenceID": 3, "context": "The encoder consists of a bidirectional GRU-RNN (Chung et al. (2014)), each with a hidden state dimension of 200. The decoder consists of a uni-directional GRU-RNN with the same hidden-state size, an attention mechanism over the source-hidden states and a soft-max layer over target vocabulary to generate words. We kept the source and target vocabularies separate for computational efficiency, since the target vocabulary is much smaller. When we used only the first sentence of the document as the source, as done in Rush et al. (2015), the encoder vocabulary size was 119,505 and that of the decoder stood at 68,885.", "startOffset": 49, "endOffset": 538}, {"referenceID": 3, "context": "The encoder consists of a bidirectional GRU-RNN (Chung et al. (2014)), each with a hidden state dimension of 200. The decoder consists of a uni-directional GRU-RNN with the same hidden-state size, an attention mechanism over the source-hidden states and a soft-max layer over target vocabulary to generate words. We kept the source and target vocabularies separate for computational efficiency, since the target vocabulary is much smaller. When we used only the first sentence of the document as the source, as done in Rush et al. (2015), the encoder vocabulary size was 119,505 and that of the decoder stood at 68,885. We used Adadelta (Zeiler (2012)) for training, with an initial learning rate of 0.", "startOffset": 49, "endOffset": 652}, {"referenceID": 8, "context": "4 Similar to Rush et al. (2015), we also compare the performance of our best performing model on a randomly sampled subset of 2,000 examples from the test set (which is not identical to their sample).", "startOffset": 13, "endOffset": 32}, {"referenceID": 5, "context": "Model #2: words-lvt2k-1sent: In this variant, we use the large vocabulary trick (LVT) described in Jean et al. (2014). In our experiments, we restricted the decoder-vocabulary of each batch to words in the source documents of that batch and added the most frequent words in the target dictionary on top of these, until the vocabulary size reached a fixed size (we found 2,000 to be good enough in our validation experiments).", "startOffset": 99, "endOffset": 118}, {"referenceID": 6, "context": "Model #5: words-lvt2k-2sent-hier: Since we used two sentences from source document, we implemented a hierarchical encoder with a second bi-directional RNN layer running at the sentence-level as described in Li et al. (2015), while retaining a single layer decoder with its attention operating over the top layer of the encoder.", "startOffset": 207, "endOffset": 224}, {"referenceID": 8, "context": "Full length Recall-only on test set 11 SOTA (Rush et al. (2015)) 31.", "startOffset": 45, "endOffset": 64}, {"referenceID": 8, "context": "The work of Rush et al. (2015), reported recall-only from full-length version of Rouge.", "startOffset": 12, "endOffset": 31}, {"referenceID": 8, "context": "The work of Rush et al. (2015), reported recall-only from full-length version of Rouge.5 In this metric, performance depends on the maximum allowed length of the system summary.6 In our work, we used a maximum length of 30 as specified earlier, and we found that the average system summary length from all our models (7.8 to 8.3) agrees very closely with that of the ground truth on the validation set (about 8.7 words), without any specific tuning. We report the full length Recallonly numbers of our Models #1 and #8 in rows #12 and #13 in Table 1. On this metric, contrary to F1, our baseline Model #1 outperforms Model #8, as the former is more recall oriented. We believe this can be adjusted by fine-tuning the maximum length criterion of the decoder, which we did not do for this work. More importantly, both models significantly outperform the state of the art model of Rush et al. (2015), displayed in row #11.", "startOffset": 12, "endOffset": 897}], "year": 2016, "abstractText": "In this work, we cast text summarization as a sequence-to-sequence problem and apply the attentional encoder-decoder RNN that has been shown to be successful for Machine Translation (Bahdanau et al. (2014)). Our experiments show that the proposed architecture significantly outperforms the state-of-the art model of Rush et al. (2015) on the Gigaword dataset without any additional tuning. We also propose additional extensions to the standard architecture, which we show contribute to further improvement in performance.", "creator": "LaTeX with hyperref package"}}}