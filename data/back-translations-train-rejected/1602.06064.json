{"id": "1602.06064", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2016", "title": "On Training Bi-directional Neural Network Language Model with Noise Contrastive Estimation", "abstract": "We propose to train bi-directional neural network language model(NNLM) with noise contrastive estimation(NCE). Experiments are conducted on a rescore task on the PTB data set. It is shown that NCE-trained bi-directional NNLM outperformed the one trained by conventional maximum likelihood training. But still(regretfully), it did not out-perform the baseline uni-directional NNLM.", "histories": [["v1", "Fri, 19 Feb 2016 07:27:49 GMT  (453kb,D)", "http://arxiv.org/abs/1602.06064v1", null], ["v2", "Mon, 22 Feb 2016 02:51:34 GMT  (458kb,D)", "http://arxiv.org/abs/1602.06064v2", null], ["v3", "Thu, 25 Feb 2016 02:00:23 GMT  (458kb,D)", "http://arxiv.org/abs/1602.06064v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tianxing he", "yu zhang", "jasha droppo", "kai yu"], "accepted": false, "id": "1602.06064"}, "pdf": {"name": "1602.06064.pdf", "metadata": {"source": "CRF", "title": "On Training Bi-directional Neural Network Language Model with Noise Contrastive Estimation", "authors": ["Tianxing He", "Yu Zhang"], "emails": ["cloudygoose@sjtu.edu.cn", "yzhang87@csail.mit.edu", "jdroppo@microsoft.com", "kai.yu@sjtu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year, as never before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a city, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country"}, {"heading": "2 Motivation", "text": "Statistical language models assign a probability P (W) to a given proposition. BI = < w1, w2,..., wn >, which can be broken down into a product of probabilities at the word level, applying the rule of conditional probability: P (W) = iP (wi | w1.. i \u2212 1) (1) Language models that use this formulation to predict the probability distribution of the next word based on its previous words (history). Since the prediction depends only on historical information, this type of model is referred to in this paper as the uni-directional language model. All types of language models mentioned in Section 1 fall into this category, but note that Shotspan models such as N-gram use the \"Markov chain assumption P (wi | w1.. i \u2212 1) \u2248 P (wi | wi \u2212 N.. i \u2212 1) to alleviate the data-economy problem."}, {"heading": "3 Formulation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Model Formulation", "text": "In this work, P (W) is the product of word-level scores (similar to BI = GRi-directional LM = 1vt i) and a learned normalization scalar c, required by the NCE framework to ensure normalization: f \u2032 (W) = \u043fifi (W) PNCE (W) = f \u2032 (W) exp (c) (3) where fi is the scoring formula given by a bidirectional neural network on each word index. And, the \"NCE\" superscript for PNCE (W) serves to induce normalization through NCE training. In this work, the same bidirectional neural network structure used in [18, 20] is applied and is shown in Figure 1 \u2212 and formulated below (we are aware that other variants of BI-RNN exist [21], but they are not fundamentally different in relation to this work): vi = Wxhxi \u2192 \u2212 1g (h \u2212 h \u2212 h \u2212 h \u2212 h \u2212 h)."}, {"heading": "3.2 Training of bi-directional NNLM", "text": "As emphasized in section 2, the MLE framework is not suitable for the formation of bidirectional NNLM noise, nevertheless, the MLE training is attempted as a basic experiment in this thesis. If the data distribution is called Pdata (W), the MLE target function is formulated as follows: JMLE (\u03b8) = EPdata (W) [logf \u2032 \u03b8 (W)] (8) Note that here the normalization scalar c is not present in the model. In this thesis, the contrasting estimate of noise [19] is applied to perform the bidirectional NNLM training. NCE introduces a sound distribution Pnoise (W) into the training and inserts a \"to be learned\" normalization scale c into the model, and its basic idea is that instead of maximizing the probability of the data batch sequences, the model is asked to discriminate samples from the data distribution PPNP training."}, {"heading": "4 Training and Implementation details", "text": "The training process is very similar to [12], but for the bidirectional NNLM training at the set level several changes need to be made. As the NN training in this work takes place at the set level, the data (consisting of real data samples and noise model samples) are processed in blocks, as shown in Figure 2. In addition, a batch of data streams is processed jointly to use the computing power of the GPU. It is relatively easy to implement this training process of BI-RNN using neural network training tools such as CNTK [22]. In this work, the piece size is set to 90 (which is greater than the longest set in the ptb dataset) and the batch size to 64. A validation-based learning strategy is set so that the learning rate is initially set to a large value and an improvement of 0.6 is achieved with semi-directional use of the ILG rate if no significant validation on the M-base is observed."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets", "text": "In this section, the results of the experiments to test the performance of the proposed bidirectional NNLM trained by NCE are evaluated. As the training process is very time-consuming when the noise ratio k is high (in our training framework it will cost at least k times the time to train the UNIGRULM model), we limit our experiments to the Penn Treebank part (PTB) of the WSJ corpus, which is publicly available and widely used in the LM community. There are 930k tokens, 82k tokens for training, validation and testing, and the vocabulary size is 10k.Furthermore, since there is no guarantee that the trained model will be properly normalized, the evaluation of perplexity (PPPL), 82k tokens for training, validation and testing, and vocabulary size are evaluated."}, {"heading": "5.2 Pseudo-PPL Test", "text": "Although confusion cannot be used to evaluate bi-directional NNLM, it is still interesting which PPL the trained model will assign to the test sets. In addition to the original test set for the PTB data, two additional texts are generated, one is sentences taken from the 4-GRAM base model (referred to as 4gram text), the other is sentences taken from a completely uniform distribution (denoted2This test set and the scripts for rendering the N-gram baseline are available at https: / / cloudygoose @ bitbucket.org / cloudygoose / ptb-robust-datagen.gitas uniform text) All three sets have about 4,000 sets. A well-educated LM is expected to assign the lowest PPL to the first group, relatively low PPL to the second group and very high (bad) PPL to the last group. Results are shown in Table 2.It is expected that the BI-GRLE line is trained at the base level relative to the NBI-GRLE 3."}, {"heading": "5.3 Evaluation on the ptb-rescore task", "text": "In this section, Accuracy Results is presented on the ptb-rescore task. Three models are trained to be baseline models: 4-GRAM, UNI-GRULM case, and BI-GRULM coached by MLE. Note that, unless otherwise mentioned, all GRULMs at work has 300 neurons on the hidden layer and only one layer (in the BI-GRULM case, one layer means one layer layer and one reverse layer) is used. This setting is chosen for the reason that adding more neurons or more layers does not mean any significant on the baseline PPPL for the UNI-GRULM model."}, {"heading": "6 Related work", "text": "In [20] bidirectional LSTMLM is trained with MLE and tested by rescoring LM in an ASR task. However, there is no improvement compared to the unidirectional base model. On the other hand, NCE has been used in unidirectional LM training for both FNNLM [25] and RNNLM [26], the main objective being to speed up the training and evaluation of these two models, since the final Softmax operation on the initial layer is no longer necessary in NCE training."}, {"heading": "7 Conclusion", "text": "It is shown that NCEtrained bi-directional NNLM performs better than conventional maximum probability training, but it has not surpassed unidirectional NNLM. Perhaps the main reason for this is that the sampling space at the record level is too sparse for our sampling to cover."}], "references": [{"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F. Chen", "Joshua Goodman"], "venue": "Proc. ACL. 1996, pp. 310\u2013318, Association for Computational Linguistics.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal OF Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "Computer Speech Language, vol. 21, no. 3, pp. 492\u2013518, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio"], "venue": "AISTATS, 2005, pp. 246\u2013252.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Improved neural network based language modelling andadaptation", "author": ["J. Park", "X. Liu", "M.J.F. Gales", "P.C. Woodland"], "venue": "Proc. InterSpeech, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "Proc. ICML, 2007, pp. 641\u2013648. 7", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafiat", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur"], "venue": "Proc. InterSpeech, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Comparison of feedforward and recurrent neural network language models", "author": ["Martin Sundermeyer", "Ilya Oparin", "Ben Freiberg", "Ralf Schlter", "Hermann Ney"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schluter", "Hermann Ney"], "venue": "Proc. InterSpeech, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Cache based recurrent neural network language model inference for first pass speech recognition", "author": ["Zhiheng Huang", "Geoffrey Zweig", "Benoit Dumoulin"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient lattice rescoring using recurrent neural network language models", "author": ["X. Liu", "Y. Wang", "X. Chen", "M.J.F. Gales", "P.C. Woodland"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient gpu-based training of recurrent neural network language models using spliced sentence bunch", "author": ["X. Chen", "Y. Wang", "X. Liu", "M.J.F. Gales", "P.C. Woodland"], "venue": "Proc. InterSpeech, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "Jrgen Schmidhuber"], "venue": "2001.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "CoRR, vol. abs/1409.2329, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR, vol. abs/1412.3555, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Feedforward sequential memory neural networks without recurrent feedback", "author": ["Shiliang Zhang", "Hui Jiang", "Si Wei", "Li-Rong Dai"], "venue": "CoRR, vol. abs/1510.02693, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, 2014, pp. 1764\u20131772.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Michael U. Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "J. Mach. Learn. Res., vol. 13, no. 1, pp. 307\u2013361, Feb. 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Bidirectional recurrent neural network language models for automatic speech recognition", "author": ["Ebru Arisoy1", "Abhinav Sethy", "Bhuvana Ramabhadran", "Stanley Chen"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Bidirectional recurrent neural networks as generative models - reconstructing gaps in time series", "author": ["Mathias Berglund", "Tapani Raiko", "Mikko Honkala", "Leo K\u00e4rkk\u00e4inen", "Akos Vetek", "Juha Karhunen"], "venue": "CoRR, vol. abs/1504.01575, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "Proceedings International Conference on Spoken Language Processing, November 2002, pp. 257\u2013286.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh"], "venue": "Proceedings of the 29th International Conference on Machine Learning, 2012, pp. 1751\u20131758.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural network language model training with noise contrastive estimation for speech recognition", "author": ["M.J.F. Gales X. Chen", "X. Liu", "P.C.Woodland"], "venue": "Proc. ICASSP, 2015. 8", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Various smoothing techniques[1] are proposed to address this issue but the improvements have been limited.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "Recently, neural network based language models have attracted great interest due to its effective encoding of word context history [2, 3, 4, 5].", "startOffset": 131, "endOffset": 143}, {"referenceID": 2, "context": "Recently, neural network based language models have attracted great interest due to its effective encoding of word context history [2, 3, 4, 5].", "startOffset": 131, "endOffset": 143}, {"referenceID": 3, "context": "Recently, neural network based language models have attracted great interest due to its effective encoding of word context history [2, 3, 4, 5].", "startOffset": 131, "endOffset": 143}, {"referenceID": 4, "context": "Recently, neural network based language models have attracted great interest due to its effective encoding of word context history [2, 3, 4, 5].", "startOffset": 131, "endOffset": 143}, {"referenceID": 1, "context": "With the continuous context representation, feed-forward neural network language models (FNNLM)[2, 3, 4, 5, 6], have achieved both better perplexity(PPL) and better word error rate (WER) when embedded into a realworld system.", "startOffset": 95, "endOffset": 110}, {"referenceID": 2, "context": "With the continuous context representation, feed-forward neural network language models (FNNLM)[2, 3, 4, 5, 6], have achieved both better perplexity(PPL) and better word error rate (WER) when embedded into a realworld system.", "startOffset": 95, "endOffset": 110}, {"referenceID": 3, "context": "With the continuous context representation, feed-forward neural network language models (FNNLM)[2, 3, 4, 5, 6], have achieved both better perplexity(PPL) and better word error rate (WER) when embedded into a realworld system.", "startOffset": 95, "endOffset": 110}, {"referenceID": 4, "context": "With the continuous context representation, feed-forward neural network language models (FNNLM)[2, 3, 4, 5, 6], have achieved both better perplexity(PPL) and better word error rate (WER) when embedded into a realworld system.", "startOffset": 95, "endOffset": 110}, {"referenceID": 5, "context": "With the continuous context representation, feed-forward neural network language models (FNNLM)[2, 3, 4, 5, 6], have achieved both better perplexity(PPL) and better word error rate (WER) when embedded into a realworld system.", "startOffset": 95, "endOffset": 110}, {"referenceID": 6, "context": "It has achieved significant performance gain on perplexity and word error rate (WER) performance on various data sets [7, 8, 9, 10, 11, 12], out-performing traditional back-off n-gram models and FNNLMs.", "startOffset": 118, "endOffset": 139}, {"referenceID": 7, "context": "It has achieved significant performance gain on perplexity and word error rate (WER) performance on various data sets [7, 8, 9, 10, 11, 12], out-performing traditional back-off n-gram models and FNNLMs.", "startOffset": 118, "endOffset": 139}, {"referenceID": 8, "context": "It has achieved significant performance gain on perplexity and word error rate (WER) performance on various data sets [7, 8, 9, 10, 11, 12], out-performing traditional back-off n-gram models and FNNLMs.", "startOffset": 118, "endOffset": 139}, {"referenceID": 9, "context": "It has achieved significant performance gain on perplexity and word error rate (WER) performance on various data sets [7, 8, 9, 10, 11, 12], out-performing traditional back-off n-gram models and FNNLMs.", "startOffset": 118, "endOffset": 139}, {"referenceID": 10, "context": "It has achieved significant performance gain on perplexity and word error rate (WER) performance on various data sets [7, 8, 9, 10, 11, 12], out-performing traditional back-off n-gram models and FNNLMs.", "startOffset": 118, "endOffset": 139}, {"referenceID": 11, "context": "It has achieved significant performance gain on perplexity and word error rate (WER) performance on various data sets [7, 8, 9, 10, 11, 12], out-performing traditional back-off n-gram models and FNNLMs.", "startOffset": 118, "endOffset": 139}, {"referenceID": 12, "context": "However, RNN training generally suffered from the \u201cvanishing gradient\u201d problem[13]:the gradient flow will decay sharply through a non-linear operation.", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "The LSTM[14] structure alleviates this problem by introducing a \u201cmemory cell\u201d structure which allows the gradient to travel without being squashed by a non-linear operation.", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "into RNNLM[9], LSTMLM is able to remember longer context information and gains more performance gain.", "startOffset": 10, "endOffset": 13}, {"referenceID": 14, "context": "It has also been shown that the dropout [15] can be used to regularize the LSTMLM.", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "Inspired by its success, several variants of LSTM have been proposed, recently the gated recurrent unit(GRU)[16] is gaining increasing popularity becuase it has matching performance with LSTM but has simpler structure.", "startOffset": 108, "endOffset": 112}, {"referenceID": 16, "context": "More recently, [17] has proposed to introduce the concept of memory into NNLM.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "In literature, very few attempts have been made to train a proper bi-directional neural network language model, even though bi-drectional NN has already been successfully applied to other fields[18].", "startOffset": 194, "endOffset": 198}, {"referenceID": 8, "context": "And recent years of research effort in the field of neural network language model has been focused on getting a better representation of history context using sophisticated recurrent neural network structures like LSTM[9].", "startOffset": 218, "endOffset": 221}, {"referenceID": 18, "context": "In this work, noise contrastive estimation(NCE)[19] is used to train a bi-directional neural network based LM, one big advantage of NCE over MLE is that it doesn\u2019t require the model to be self-normalized.", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "In this work, the same bi-directional neural network structure that has been used in [18, 20] is applied, and is shown in figure 1 and formulated below(we are aware that other variants of BI-RNN exist[21], but they are not fundamentally different with regard to this work):", "startOffset": 85, "endOffset": 93}, {"referenceID": 19, "context": "In this work, the same bi-directional neural network structure that has been used in [18, 20] is applied, and is shown in figure 1 and formulated below(we are aware that other variants of BI-RNN exist[21], but they are not fundamentally different with regard to this work):", "startOffset": 85, "endOffset": 93}, {"referenceID": 20, "context": "In this work, the same bi-directional neural network structure that has been used in [18, 20] is applied, and is shown in figure 1 and formulated below(we are aware that other variants of BI-RNN exist[21], but they are not fundamentally different with regard to this work):", "startOffset": 200, "endOffset": 204}, {"referenceID": 15, "context": "In this work, gated recurrent unit is used as the recurrent structure ht = g(ht\u22121, vt) because it is faster, causes less memory and has matching performance with the LSTM structure[16].", "startOffset": 180, "endOffset": 184}, {"referenceID": 18, "context": "In this work, noise contrastive estimation[19] is applied to train the bi-directional NNLM.", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": "The training process is very similar to [12], but several changes need to be made for the sentence-level bi-directional NNLM training.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "Finally, the SRILM[23] Toolkit is used for N-gram LM training in this work.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "In [20], bi-directional LSTMLM is trained with MLE and tested by LM rescoring in an ASR task.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "On the other hand, NCE has been used in uni-directional LM training both for FNNLM[25] and RNNLM[26], the main goal was to speed-up the training and evaluation of these two models because under NCE training the final softmax operation on the output layer is no longer necessary.", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "On the other hand, NCE has been used in uni-directional LM training both for FNNLM[25] and RNNLM[26], the main goal was to speed-up the training and evaluation of these two models because under NCE training the final softmax operation on the output layer is no longer necessary.", "startOffset": 96, "endOffset": 100}], "year": 2017, "abstractText": "We propose to train bi-directional neural network language model(NNLM) with noise contrastive estimation(NCE). Experiments are conducted on a rescore task on the PTB data set. It is shown that NCE-trained bi-directional NNLM outperformed the one trained by conventional maximum likelihood training. But still(regretfully), it did not out-perform the baseline uni-directional NNLM.", "creator": "LaTeX with hyperref package"}}}