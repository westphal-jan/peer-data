{"id": "1602.06183", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2016", "title": "Node-By-Node Greedy Deep Learning for Interpretable Features", "abstract": "Multilayer networks have seen a resurgence under the umbrella of deep learning. Current deep learning algorithms train the layers of the network sequentially, improving algorithmic performance as well as providing some regularization. We present a new training algorithm for deep networks which trains \\emph{each node in the network} sequentially. Our algorithm is orders of magnitude faster, creates more interpretable internal representations at the node level, while not sacrificing on the ultimate out-of-sample performance.", "histories": [["v1", "Fri, 19 Feb 2016 15:36:38 GMT  (1172kb,D)", "http://arxiv.org/abs/1602.06183v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ke wu", "malik magdon-ismail"], "accepted": false, "id": "1602.06183"}, "pdf": {"name": "1602.06183.pdf", "metadata": {"source": "CRF", "title": "Node-By-Node Greedy Deep Learning for Interpretable Features", "authors": ["Ke Wu", "Malik Magdon-Ismail"], "emails": ["WUK3@RPI.EDU", "MAGDON@GMAIL.COM"], "sections": [{"heading": "1. Introduction", "text": "(...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...).). (...). (...). (...).). (...).). (...). (...).). (...). (...). (...).). (...).).). (...).). (...). (...). (...).).).).). (...). (...).). (...).).). (). ().).)."}, {"heading": "2. Greedy Node-by-Node Deep Learning", "text": "This year it is more than ever before."}, {"heading": "3. Results and Discussion", "text": "We use a variety of data sets to compare our new GN (unattended) and GCN (supervised) algorithms with some standard deep network training algorithms. Our goal is to show that \u2022 features of GN and GCN are more interpretable. \u2022 The classification performance of our algorithms is comparable to layer-by-layer training, although they are orders of magnitude more efficient."}, {"heading": "3.1. Quality of Features", "text": "First, we appeal to the results of the USPS digit data in Figures 2 and 4 to illustrate the qualitative difference between our greedy knot characteristics and the characteristics generated by the standard layer-by-layer pre-training. If you learn all the characteristics in a layer at the same time, there are several local minima that do not correspond to the natural intuitive characteristics that might be present. Since all the nodes are trained at the same time, several characteristics can be mixed within a knot and this mixing can be compensated for in other nodes, resulting in somewhat counterproductive characteristics such as those achieved in Figures 2 and 4 for the layer-by-knot approach. Node-by-knot-knot-knot approaches do not seem to suffer from this problem, as a simple visual inspection of the characteristics implemented at each knot results in recognizable digits. It is clear that our new algorithms find more interpretable characteristics."}, {"heading": "3.2. Efficiency and Classification Performance", "text": "We have argued that our algorithms are more efficient than layer-by-layer pre-training. The rationale is that training is distributed at each level by dividing the data. First, we have compared the number of iterations in the practice we are in. We have 500 iterations that we are trying to test in practice."}, {"heading": "4. Conclusion and Future Work", "text": "Our goal was to develop a greedy node for a deep network algorithm that sequences in each layer of a deep network (similar to PCA in the linear environment, but different because we split data between features).Our two novel deep learning algorithms are based on the idea of simulating a human's learning process at the same time by using a portion of the data to learn each feature, in contrast to the classic deep learning algorithms that occur in one layer for all objects at the same time (pulling all nodes in a layer at the same time)."}], "references": [{"title": "Learning From Data: A Short Course", "author": ["Abu-Mostafa", "Yaser", "Magdon-Ismail", "Malik", "Lin", "Hsuan-Tien"], "venue": "amlbook.com,", "citeRegEx": "Abu.Mostafa et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abu.Mostafa et al\\.", "year": 2012}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Bengio", "Yoshua"], "venue": "Unsup. and Transfer Learning Challenges in ML,", "citeRegEx": "Bengio and Yoshua.,? \\Q2012\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2012}, {"title": "Sparse feature learning for deep belief networks", "author": ["Boureau", "Y-lan", "Cun", "Yann L"], "venue": "In NIPS, pp", "citeRegEx": "Boureau et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2008}, {"title": "Deep learning with nonparametric clustering", "author": ["Chen", "Gang"], "venue": "arXiv preprint arXiv:1501.03084,", "citeRegEx": "Chen and Gang.,? \\Q2015\\E", "shortCiteRegEx": "Chen and Gang.", "year": 2015}, {"title": "Learning feature representations with k-means", "author": ["Coates", "Adam", "Ng", "Andrew Y"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Coates et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2012}, {"title": "Principal components analysis of images via back propagation", "author": ["Cottrell", "Garrison", "Munro", "Paul"], "venue": "In Proc. SPIE 1001,", "citeRegEx": "Cottrell et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Cottrell et al\\.", "year": 1988}, {"title": "New types of deep neural network learning for speech recognition and related applications: An overview", "author": ["Deng", "Li", "Hinton", "Geoffrey", "Kingsbury", "Brian"], "venue": "In Proc. ICASSP,", "citeRegEx": "Deng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2013}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Manzagol", "Pierre-Antoine", "Vincent", "Pascal", "Samy"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Neural networks learning improvement using the k-means clustering algorithm to detect network intrusions", "author": ["KM Faraoun", "A. Boukelif"], "venue": "INFOCOMP Journal of Computer Science,", "citeRegEx": "Faraoun and Boukelif,? \\Q2006\\E", "shortCiteRegEx": "Faraoun and Boukelif", "year": 2006}, {"title": "Local minima and plateaus in hierarchical structures of multilayer perceptrons", "author": ["Fukumizu", "Kenji", "Amari", "Shun-ichi"], "venue": "Neural Networks,", "citeRegEx": "Fukumizu et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2000}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In Proc. ICML, pp", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "On the problem of local minima in backpropagation", "author": ["Gori", "Marco", "Tesi", "Alberto"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gori et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Gori et al\\.", "year": 1992}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey", "Osindero", "Simon", "Teh", "Yee-Whye"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Learning phenomena in networks of adaptive switching circuits", "author": ["Hoff Jr.", "ME"], "venue": "PhD thesis,", "citeRegEx": "Jr and ME.,? \\Q1962\\E", "shortCiteRegEx": "Jr and ME.", "year": 1962}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Hornik", "Kurt", "Stinchcombe", "Maxwell", "White", "Halbert"], "venue": "Neural networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["LeCun", "Yann", "Bengio", "Yoshua"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "LeCun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1995}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Lee", "Honglak", "Grosse", "Roger", "Ranganath", "Rajesh", "Ng", "Andrew Y"], "venue": "In Proc. ICML,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["Lee", "Honglak", "Pham", "Peter", "Largman", "Yan", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Deep learning via hessian-free optimization", "author": ["Martens", "James"], "venue": "In Proc. ICML, pp", "citeRegEx": "Martens and James.,? \\Q2010\\E", "shortCiteRegEx": "Martens and James.", "year": 2010}, {"title": "On optimization methods for deep learning", "author": ["Ngiam", "Jiquan", "Coates", "Adam", "Lahiri", "Ahbik", "Prochnow", "Bobby", "Le", "Quoc V", "Ng", "Andrew Y"], "venue": "In Proc. ICML,", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["Poultney", "Christopher", "Chopra", "Sumit", "Cun", "Yann L"], "venue": "In NIPS,", "citeRegEx": "Poultney et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poultney et al\\.", "year": 2006}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain", "author": ["Rosenblatt", "Frank"], "venue": "Psychological review,", "citeRegEx": "Rosenblatt and Frank.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt and Frank.", "year": 1958}, {"title": "Learning internal representation by back propagation. Parallel distributed processing: exploration in the microstructure", "author": ["DE Rumelhart", "GE Hinton", "Williams", "RJ"], "venue": "of cognition,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "The nature of statistical learning theory", "author": ["Vapnik", "Vladimir"], "venue": null, "citeRegEx": "Vapnik and Vladimir.,? \\Q2000\\E", "shortCiteRegEx": "Vapnik and Vladimir.", "year": 2000}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proc. ICML, pp", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": null, "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Deep learning via semi-supervised embedding", "author": ["Weston", "Jason", "Ratle", "Fr\u00e9d\u00e9ric", "Mobahi", "Hossein", "Collobert", "Ronan"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Weston et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2012}, {"title": "Adaptive switching circuits", "author": ["B. Widrow"], "venue": "In IRE WESCON Convention Record, pp", "citeRegEx": "Widrow,? \\Q1960\\E", "shortCiteRegEx": "Widrow", "year": 1960}, {"title": "Deep learning and its applications to signal and information processing", "author": ["Yu", "Dong", "Deng", "Li"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 28, "context": "Multilayer neural networks have gone through ups and downs since their arrival in (Rosenblatt, 1958; Widrow, 1960; Hoff Jr, 1962).", "startOffset": 82, "endOffset": 129}, {"referenceID": 0, "context": "Assume a standard machine learning from data setup (Abu-Mostafa et al., 2012), withN datapoints (x1, y1), .", "startOffset": 51, "endOffset": 77}, {"referenceID": 23, "context": "Backpropagation which trains all the weights simultaneously, allowing for maximum flexibility, was the popular approach to training a deep network (Rumelhart et al., 1986).", "startOffset": 147, "endOffset": 171}, {"referenceID": 7, "context": "The greedy layer-by-layer pre-training is significantly more efficient than full backpropagation, and appears to be better at avoiding bad local minima (Erhan et al., 2010).", "startOffset": 152, "endOffset": 172}, {"referenceID": 23, "context": "There is no doubt softening the hard threshold to a sigmoid and the arrival of a new efficient training algorithm, backpropagation (Rumelhart et al., 1986), was a huge part of the resurgence of neural networks in the 1980s/1990s.", "startOffset": 131, "endOffset": 155}, {"referenceID": 27, "context": "To help motivate our approach, it helps to start back at the very beginning of neural networks, with Rosenblatt (1958) and Widrow (1960). They introduced the adaline, the adaptive linear (hard threshold element), and the combination of multiple elements came in Hoff Jr (1962), the madaline (the precursor to the multilayer perceptron).", "startOffset": 123, "endOffset": 137}, {"referenceID": 27, "context": "To help motivate our approach, it helps to start back at the very beginning of neural networks, with Rosenblatt (1958) and Widrow (1960). They introduced the adaline, the adaptive linear (hard threshold element), and the combination of multiple elements came in Hoff Jr (1962), the madaline (the precursor to the multilayer perceptron).", "startOffset": 123, "endOffset": 277}, {"referenceID": 14, "context": "due to the facts that multilayer feedforward networks were still hard to train iteratively due to convergence issues and multiple local minima (Gori & Tesi, 1992; Fukumizu & Amari, 2000), are extremely powerful (Hornik et al., 1989) and easy to overfit to data.", "startOffset": 211, "endOffset": 232}, {"referenceID": 12, "context": "Multi-layer (deep) neural networks are back in the guise of deep learning/deep networks, and again because of a leap in the methods used to train the network (Hinton et al., 2006).", "startOffset": 158, "endOffset": 179}, {"referenceID": 12, "context": "In so doing, training becomes manageable (Hinton et al., 2006; Bengio et al., 2007), the local minima problem when training a single layer is significantly diminished as compared to the whole network and the restriction to layer by layer learning reigns in the power of the network, helping with regularizing it(Erhan et al.", "startOffset": 41, "endOffset": 83}, {"referenceID": 7, "context": ", 2007), the local minima problem when training a single layer is significantly diminished as compared to the whole network and the restriction to layer by layer learning reigns in the power of the network, helping with regularizing it(Erhan et al., 2010).", "startOffset": 235, "endOffset": 255}, {"referenceID": 7, "context": "A side benefit has also emerged, which is that each layer successively has the potential to learn hierarchical representations (Erhan et al., 2010; Lee et al., 2009a).", "startOffset": 127, "endOffset": 166}, {"referenceID": 10, "context": "As a result of these algorithmic advances, deep networks have found a host of modern applications, ranging from sentiment classification (Glorot et al., 2011), to audio (Lee et al.", "startOffset": 137, "endOffset": 158}, {"referenceID": 6, "context": ", 2009b), to signal and information processing (Yu & Deng, 2011), to speech (Deng et al., 2013), and even to the unsupervised and transfer settings (Bengio, 2012).", "startOffset": 76, "endOffset": 95}, {"referenceID": 20, "context": "Optimization of such deep networks is also an active area, for example (Ngiam et al., 2011; Martens, 2010).", "startOffset": 71, "endOffset": 106}, {"referenceID": 12, "context": "Besides the original deep belief network (Hinton et al., 2006) and autoencoder (Bengio et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 26, "context": ", 2007), the stacked denoising autoencoder (Vincent et al., 2010; 2008) has been widely used as a variant to the classic autoencoder, where corrupted data is used in pre-training.", "startOffset": 43, "endOffset": 71}, {"referenceID": 2, "context": "Sparse encoding (Boureau et al., 2008; Poultney et al., 2006) has also been used to prevent the system from activating the same subset of nodes constantly (Poultney et al.", "startOffset": 16, "endOffset": 61}, {"referenceID": 21, "context": "Sparse encoding (Boureau et al., 2008; Poultney et al., 2006) has also been used to prevent the system from activating the same subset of nodes constantly (Poultney et al.", "startOffset": 16, "endOffset": 61}, {"referenceID": 21, "context": ", 2006) has also been used to prevent the system from activating the same subset of nodes constantly (Poultney et al., 2006).", "startOffset": 101, "endOffset": 124}, {"referenceID": 15, "context": "For images, convolutional neural networks (LeCun & Bengio, 1995) and the deep convolutional neural network (Krizhevsky et al., 2012) are widely used, however the computation cost is significantly more, and the feature learning is based on the local filter defined by the modeler.", "startOffset": 107, "endOffset": 132}, {"referenceID": 15, "context": "For images, convolutional neural networks (LeCun & Bengio, 1995) and the deep convolutional neural network (Krizhevsky et al., 2012) are widely used, however the computation cost is significantly more, and the feature learning is based on the local filter defined by the modeler. Our methods apply to a general deep network. Our algorithms can be roughly seen as simultaneously clustering the data while extracting the features. Deep networks have been used for unsupervised clustering (Chen, 2015) and clustering has been used in classic deep networks by Weston et al. (2012) which are targeted for semi-supervised learning.", "startOffset": 108, "endOffset": 577}], "year": 2016, "abstractText": "Multilayer networks have seen a resurgence under the umbrella of deep learning. Current deep learning algorithms train the layers of the network sequentially, improving algorithmic performance as well as providing some regularization. We present a new training algorithm for deep networks which trains each node in the network sequentially. Our algorithm is orders of magnitude faster, creates more interpretable internal representations at the node level, while not sacrificing on the ultimate out-of-sample performance.", "creator": "LaTeX with hyperref package"}}}