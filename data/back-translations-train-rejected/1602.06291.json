{"id": "1602.06291", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2016", "title": "Contextual LSTM (CLSTM) models for Large scale NLP tasks", "abstract": "Documents exhibit sequential structure at multiple levels of abstraction (e.g., sentences, paragraphs, sections). These abstractions constitute a natural hierarchy for representing the context in which to infer the meaning of words and larger fragments of text. In this paper, we present CLSTM (Contextual LSTM), an extension of the recurrent neural network LSTM (Long-Short Term Memory) model, where we incorporate contextual features (e.g., topics) into the model. We evaluate CLSTM on three specific NLP tasks: word prediction, next sentence selection, and sentence topic prediction. Results from experiments run on two corpora, English documents in Wikipedia and a subset of articles from a recent snapshot of English Google News, indicate that using both words and topics as features improves performance of the CLSTM models over baseline LSTM models for these tasks. For example on the next sentence selection task, we get relative accuracy improvements of 21% for the Wikipedia dataset and 18% for the Google News dataset. This clearly demonstrates the significant benefit of using context appropriately in natural language (NL) tasks. This has implications for a wide variety of NL applications like question answering, sentence completion, paraphrase generation, and next utterance prediction in dialog systems.", "histories": [["v1", "Fri, 19 Feb 2016 20:52:08 GMT  (6420kb,D)", "http://arxiv.org/abs/1602.06291v1", null], ["v2", "Tue, 31 May 2016 17:19:09 GMT  (6816kb,D)", "http://arxiv.org/abs/1602.06291v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shalini ghosh", "oriol vinyals", "brian strope", "scott roy", "tom dean", "larry heck"], "accepted": false, "id": "1602.06291"}, "pdf": {"name": "1602.06291.pdf", "metadata": {"source": "CRF", "title": "Contextual LSTM (CLSTM) models for Large scale NLP tasks", "authors": ["Shalini Ghosh", "Oriol Vinyals", "Brian Strope", "Scott Roy", "Larry Heck"], "emails": ["shalini@csl.sri.com", "vinyals@google.com", "bps@google.com", "hsr@google.com", "tld@google.com", "larryheck@google.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "In fact, most people are able to move to another world in which they are in the position in which they find themselves."}, {"heading": "2. RELATED WORK", "text": "In fact, it is such that most of them will be able to go into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they"}, {"heading": "3. BACKGROUND", "text": "The LSTM model has multiple LSTM cells, with each LSTM cell modelling the digital memory in a neural network. It has gates that allow the LSTM to store information and access it over time. For example, the input / output gate controls the input / output of the cell, while the forge gate controls the state of the cell. The following equations represent the operations of the various components of the LSTM cell [17] 1: it = \u03c3 (Wxixt + Whiht \u2212 1 + Wcict \u2212 1 + bi) ft = \u03c3 (Wxfxt + Whfht \u2212 1 + Wcfct \u2212 1 + bf) ct = ftct \u2212 1 + it tanh (Wxcxt + Whcht \u2212 1 + bc) ot tanh (ct) (1), where the input is the gate, bf and the input \u2212 1 and \u2212 Gb, respectively."}, {"heading": "4. WORD PREDICTION", "text": "Of the three different tasks outlined in Section 1, we will first focus on the word prediction task, where the goal is to predict the next word in a sentence in which the words and context (captured via the subject) were seen before. Let si be the i-th sentence in a sentence sequence, wi, j be the j word of the sentence si, ni be the number of words in si and wi, j. wi, k enter the word sequence from word j to word k in sentence ian. Note that sentence si of the word sequence wi, 0,... wi, ni \u2212 1. Let T be the random variable that denotes the subject - it is calculated on the basis of a specific word sequence ranging from the first word of the word sequence (w0,0) to the current word (wi, j). This topic can be based on the current sentence segment (i.e. T = Topic, wi \u2212 1)."}, {"heading": "4.1 Model", "text": "The word prediction LxfM model was implemented in the large-scale Distributed DistBelief framework = bold term = 1. The model takes words that are in 1-hot encoding from the input, converts them into an embedding vector and consumes the word vectors at a time. The model is trained to predict the next word as a sequence of words is already being seen. The core algorithm used to train the LSTM parameters is BPTT [44], using a softmax layer that truth.In order to customize the LSTM cell that takes the words to a CLSTM cell that enters both words and topics, we modify the equations 1 to add the topic vector T to the entrance gate, 1We present the LSTM equations over here as we will show below how we have modified these equations to integrate the STM cell themes into the STM cell."}, {"heading": "4.2 HTM: Supervised Topic Labels", "text": "In fact, it is such that it is a matter of a way in which people are able to put themselves into the world, in which they are able to understand the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "5. NEXT SENTENCE SELECTION", "text": "Next, we will focus on the next sentence evaluation task, where we are given a sequence of sentences and the goal is to find the most likely next sentence out of a series of candidate sentences. An example of this task is in Figure 4. The task can be described as follows: In a model with the parameters \u044b, a sequence of p \u2212 1 sentences s0... sp \u2212 2 (with the corresponding topics T0... Tp \u2212 2), you will find the most likely next sentence sp \u2212 1 out of a sequence of candidates for the next sentence S, so that: sp \u2212 1 = arg max s-SP (s | s0.... sp \u2212 2, T0.. Tp \u2212 2, \u043a)."}, {"heading": "5.1 Problem Instantiation", "text": "Suppose we get a series of sequences in which each sequence consists of 4 sentences (i.e., we look at p = 4). If we give each sequence as Si = < AiBiCiDi > and the sequence set as {S1,..., Sk}. If we give the prefix AiBiCi of the sequence Si as context (which we will call Contexti), we consider the task of correctly identifying the next sentence as Di from a candidate set of sentences: {D0, D1,..., Dk \u2212 1}. For each sequence Si we calculate the accuracy of correctly identifying the next sentence. The accuracy of the model in determining the correct next sentence is calculated by the sequence set {S1,.., Sk}."}, {"heading": "5.2 Approach", "text": "Considering the context Contexti, the models find the Di under the set {D0... Dk \u2212 1}, which yields the maximum (normalized) value defined as follows: \u0435i, score = P (Di | Contexti) 1 k \u2211 k \u2212 1 j = 0 P (Di | Contextj) (5) In the above value, the conditional probability terms are estimated using the conclusions from the LSTM and CLSTM models (as shown in Figure 5). In the numerator, the probability of the word sequence in Di is estimated taking into account the prefix context Contexti by inferring a model whose state is already determined by the sequence AiBiCi (as shown in Figure 5). The normalization term 1 k \u2212 1 j = 0 P (Di | Contextj) in the denominator of Equation 5 is calculated the number of points P (Di)."}, {"heading": "5.3 Model", "text": "The CLSTM model uses words from Ai, Bi, Ci, and Di to predict the words from Di. The CLSTM model uses words from Ai, Bi, Ci, and topics from Ai, Bi, Ci, and Di to predict the words from Di. Note that in this case, we can use the topic Di, as all the next sentences of the candidate are entered into the next sentence evaluation task. With 1024 hidden units, the perplexity of the LSTM base model is 27.66 after model training convergence, while the perplexity of the CLSTM model is 24.81 at convergence. This relative gain of 10.3% on an intrinsic evaluation scale (such as perplexity) was the basis for the confidence to expect good performance when using this CLSTM model for the next sentence evaluation task."}, {"heading": "5.4 Experimental Results", "text": "We divided the test data set into 100 non-overlapping subsets. To create the data set for scoring the next set, we did the following: (a) sample 50 sentence sequences < AiBiCiDi > from 50 separate paragraphs, randomly selected from a subset of the test set - we call this a block; (b) consider 100 such blocks in the next sentence scoring data set. Thus, there are a total of 5000 sentence sequences in the final data set. For each sequence, the model must select the best next set Di from 50 competing next sentences in the block. The average accuracy of the LSTM base model on this data set is 52%, while the average accuracy of the CLSTM model using word + sentence subject attributes is 63% (as shown in Table 3)."}, {"heading": "5.5 Error Analysis", "text": "Figures 6-8 analyse various types of errors in the LSTM and CLSTM models using samples from the test data set."}, {"heading": "6. SENTENCE TOPIC PREDICTION", "text": "The last task we are considering is the following: when we get the words and the topic of the current sentence, can we predict the topic of the next sentence? This is an interesting problem for dialog systems, where we ask the question: can we predict the topic of their next utterance given the utterance of a speaker? This can be used in various applications in dialog systems, for example, in intention modeling. The problem of predicting the sentence topic can be formulated as follows: When we get a model with the parameters \u03b7, words in sentence si and the corresponding topic Ti, we find the next sentence topic Ti + 1, which maximizes the following probability - P (Ti + 1 | si, Ti, \u0442). Note that in this case, we train a model to predict the topic goal instead of the common word / topic goal, since we have found empirically that the formation of a model with a common goal has less accuracy in predicting the topic compared to a topic that only tries to predict the model that yielded."}, {"heading": "6.1 Model", "text": "For the task of sentence topic prediction, we found through ablation experiments that the unrolled model architecture, in which each sentence in a paragraph is modeled by a separate LSTM model, performs better than the rolled-up model architecture used for word prediction (as shown in Figure 2), in which the sentences in a paragraph are entered into a single LSTM."}, {"heading": "6.2 Experiments", "text": "In our experiments, we used the output of HTM as a theme for each sentence. Ideally, we associated each sentence with a \"monitored theme\" (e.g. monitoring by human advisors). Due to the difficulty of obtaining such human ratings on a scale, we used the HTM model to find themes for the sentences. Note that the HTM model is trained on human ratings. We also trained two base models on this record. The Word model uses the words of the current sentence to predict the theme of the next sentence - it determines how well we can predict the theme of the next sentence, considering the words of the current sentence. We also trained another base model, SentTopic, which uses the sentence theme of the current sentence to predict the theme of the next sentence - the performance of this model gives us an idea of the inherent difficulty of the task of predicting the topic. We also trained a CLSTM model (Word + Sentic) to predict the theme of both the topic and the next sentence."}, {"heading": "6.3 Comparison to BOW-DNN baseline", "text": "For the task of predicting sentence topics, we compared the CLSTM model with a Bag-of-Words Deep Neural Network (BOW-DNN) baseline [2]. The BOW-DNN model extracts word bags from the input text, and a DNN layer is used to extract parent characteristics from the word bags. In this experiment, the task we created was slightly different to allow for a more direct comparison, the goal being to predict the topic of the next sentence by giving words from the next sentence. BOW-DNN model was trained only on word characteristics and received a test sentence perplexity of 16.5 in predicting the sentence topic. The CLSTM model, which was trained on word and theme characteristics, achieved a confusion of 15.3 on the same sentence using 1024 hidden units, outperforming the BOW-DNN model by 7.3%."}, {"heading": "6.4 Using Unsupervised Topic Signals", "text": "In our experiments with thematic characteristics, we have looked at previously reviewed topic categories that come from an external source (namely HTM). One question arises: If we do not use external topics to summarize the long-term context, would we get improvements in performance with unmonitored topic signals? To answer this question, we experimented with \"thought embedding,\" which is intrinsically generated from the previous context. Here, the mental embedding from the previous LSTM is used as a topic in the current LSTM (as shown in Figure 9) when we make predictions about the topic of the next sentence - we call this context-based thought embedding of the \"thought vector.\" 3In our approach, the thought vector from the LSTM coding of the sentence n \u2212 1 is shown to compare the topic's origin with the next sentence."}, {"heading": "7. RESULTS ON GOOGLE NEWS DATA", "text": "We also conducted experiments on a sample of documents taken from a recent (2015 / 07 / 06) snapshot of the internal English corpus of Google news.4 This subset included 4.3 million documents divided into train, test and validation datasets. Some relevant statistics of the records are in Table 6. We filtered out words that occurred less than 100 times, giving us a vocabulary of 100 K terms. We trained the LSTM and CLSTM models for the various tasks with 1024 hidden units each. Here are the most important results: 1) Word prediction task: LSTM used only words as characteristics had a perplexity of 37. CLSTM improved over LSTM by about 2%, with words, sentence segment topics and paragraph set topics.2) Next sentence selection task: LSTM gave an accuracy of about 39%. CLSTM had an accuracy of about 46%, an improvement over the average."}, {"heading": "8. CONCLUSIONS", "text": "4Note that this snapshot of Google News is internal to Google and independent of the One Billion Word benchmark [5].We have shown how the use of context-based features in a CLSTM model can be beneficial for various NLP tasks such as word prediction, next sentence selection, and topic prediction. In the word prediction task, CLSTM improves the state-of-the-art LSTM by 2-3% in terms of perplexity; in the next sentence selection task, CLSTM improves the accuracy on average by about 20%; while CLSTM improves the state-of-the-art LSTM by about 10% in the topic prediction task (and improves BOW-DNN by about 7%).These increases are all quite significant and we are achieving similar gains in the Google News dataset (Section 7), which shows the generalisability of our approach."}, {"heading": "9. FUTURE WORK", "text": "Our initial experiments on the use of unattended thought vectors to capture far-reaching correlations in CLSTM models yielded promising results. A natural extension of the thought vector model in Figure 9 is a model that has a connection between the hidden layers to model the \"continuity of thought.\" Figure 10 shows such a hierarchical LSTM model (HLSTM) that has a two-tier hierarchy: a lower-level LSTM for modeling the words in a sentence and a higher-level LSTM for modeling the sentences in a paragraph. The thought vector connection from the LSTM cell in layer n to the LSTM cells in layer n \u2212 1 (corresponding to the next sentence) allows concepts from the previous context to advance, whereby the \"thought vector\" of a sentence is able to affect the next sentence's points of thought, allowing the connection between the two."}, {"heading": "10. ACKNOWLEDGMENTS", "text": "We would like to thank Louis Shao and Yun-hsuan Sung for their help in conducting some experiments and Ray Kurzweil, Geoffrey Hinton, Dan Bikel, Lukasz Kaiser and Javier Snaider for their useful feedback on this work."}, {"heading": "11. REFERENCES", "text": "In ICASSP, 2012. [2] Yalong Bai, Wei Yu, Tianjun Xiao, Chang Xu, Kuiyuan Yang, Wei-Ying Ma, and Tiejun Zhao. Bag-of-words based deep neural network for image retrieval. In Proc. of ACM Intl. Conf. on Multimedia, 2014. [3] Regina Barzilay and Lillian Lee. Catching the drift: Probabilistic content models, with applications to generation and summarization. In HLT-NAACL, 2004. [4] Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. Large language models in translation."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition", "author": ["Ossama Abdel-Hamid", "Abdel rahman Mohamed", "Hui Jiang", "Gerald Penn"], "venue": "In ICASSP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Bag-of-words based deep neural network for image retrieval", "author": ["Yalong Bai", "Wei Yu", "Tianjun Xiao", "Chang Xu", "Kuiyuan Yang", "Wei-Ying Ma", "Tiejun Zhao"], "venue": "In Proc. of ACM Intl. Conf. on Multimedia,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Catching the drift: Probabilistic content models, with applications to generation and summarization", "author": ["Regina Barzilay", "Lillian Lee"], "venue": "In HLT-NAACL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Large language models in machine translation", "author": ["Thorsten Brants", "Ashok C. Popat", "Peng Xu", "Franz J. Och", "Jeffrey Dean"], "venue": "In EMNLP,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn"], "venue": "CoRR, abs/1312.3005,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Merri\u00ebnboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "CoRR, arXiv:406.1078", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Document embedding with paragraph vectors", "author": ["Andrew M Dai", "Christopher Olah", "Quoc V Le", "Greg S Corrado"], "venue": "NIPS Deep Learning Workshop,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg S. Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc V. Le", "Mark Z. Mao", "Marc\u2019Aurelio Ranzato", "Andrew Senior", "Paul Tucker", "Ke Yang", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Learning invariant features using inertial priors", "author": ["Thomas Dean"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["Salah El Hihi", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Sequence labelling in structured domains with hierarchical recurrent neural networks", "author": ["Santiago Fern\u00e1ndez", "Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In IJCAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "The hierarchical hidden Markov model: Analysis and applications", "author": ["Shai Fine", "Yoram Singer", "Naftali Tishby"], "venue": "Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["Felix A. Gers", "Nicol N. Schraudolph", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A.-R. Mohamed"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding, pages 273\u2013278", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["Alex Graves"], "venue": "Diploma thesis. Technische Universita\u0308t Mu\u0308nchen,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-Rahman Mohamed", "Geoffrey  Hinton"], "venue": "CoRR, arXiv:1303.5778,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional LSTM networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In IJCNN,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In CIKM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Document context language models", "author": ["Yangfeng Ji", "Trevor Cohn", "Lingpeng Kong", "Chris Dyer", "Jacob Eisenstein"], "venue": "CoRR, abs/1511.03962,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "CoRR, abs/1506.06726", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "How to Create a Mind: The Secret of Human Thought Revealed", "author": ["Ray Kurzweil"], "venue": "Penguin Books, NY, USA,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tom\u00e0s Mikolov"], "venue": "CoRR, abs/1405.4053v2,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Hierarchical Bayesian inference in the visual cortex", "author": ["Tai Sing Lee", "David Mumford"], "venue": "Journal of the Optical Society of America,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li"], "venue": "In EMNLP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "A generative model for parsing natural language to meaning representations", "author": ["Wei Lu", "Hwee Tou Ng", "Wee Sun Lee", "Luke S. Zettlemoyer"], "venue": "In EMNLP,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Foundations of  Statistical Natural Language Processing", "author": ["Chris Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1999}, {"title": "Context dependent recurrent neural network language model", "author": ["T. Mikolov", "G. Zweig"], "venue": "SLT Workshop", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["Kevin P. Murphy"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Figure-ground segmentation using a hierarchical conditional random field", "author": ["Jordan Reynolds", "Kevin Murphy"], "venue": "In Fourth Canadian Conference on Computer and Robot Vision,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "Andrew Senior", "Francoise Beaufays"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "A learning based hierarchical model for vessel segmentation", "author": ["Richard Socher", "Adrian Barbu", "Dorin Comaniciu"], "venue": "In IEEE International Symposium on Biomedical Imaging: From Nano to Macro,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In INTERSPEECH,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Training Recurrent Neural Networks", "author": ["Ilya Sutskever"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "CoRR, arXiv:1409.3215,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "networks. CoRR,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In CVPR 2015,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Paul J. Werbos"], "venue": "Neural Networks,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1988}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "CoRR, abs/1502.03044,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Long short-term memory over tree structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "CoRR, abs/1503.04881,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Modeling language and cognition with deep unsupervised learning: A tutorial overview", "author": ["Marco Zorzi", "Alberto Testolin", "Ivilin P. Stoianov"], "venue": "Frontiers in Psychology,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "Capturing this hierarchical sequential structure in a language model (LM) [30] can potentially give the model more predictive accuracy, as we have seen in previous work [12, 13, 25, 33, 47].", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "Capturing this hierarchical sequential structure in a language model (LM) [30] can potentially give the model more predictive accuracy, as we have seen in previous work [12, 13, 25, 33, 47].", "startOffset": 169, "endOffset": 189}, {"referenceID": 10, "context": "Capturing this hierarchical sequential structure in a language model (LM) [30] can potentially give the model more predictive accuracy, as we have seen in previous work [12, 13, 25, 33, 47].", "startOffset": 169, "endOffset": 189}, {"referenceID": 21, "context": "Capturing this hierarchical sequential structure in a language model (LM) [30] can potentially give the model more predictive accuracy, as we have seen in previous work [12, 13, 25, 33, 47].", "startOffset": 169, "endOffset": 189}, {"referenceID": 29, "context": "Capturing this hierarchical sequential structure in a language model (LM) [30] can potentially give the model more predictive accuracy, as we have seen in previous work [12, 13, 25, 33, 47].", "startOffset": 169, "endOffset": 189}, {"referenceID": 41, "context": "Capturing this hierarchical sequential structure in a language model (LM) [30] can potentially give the model more predictive accuracy, as we have seen in previous work [12, 13, 25, 33, 47].", "startOffset": 169, "endOffset": 189}, {"referenceID": 17, "context": "In this paper, we consider Long-Short Term Memory (LSTM) models [20], a specific kind of Recurrent Neural Networks (RNNs).", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 13, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 16, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 32, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 35, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 36, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 38, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 40, "context": "The LSTM model and its different variants have achieved impressive performance in different sequence learning problems in speech, image, music and text analysis [15, 16, 19, 36, 39, 40, 42, 43, 45], where it is useful in capturing long-range dependencies in sequences.", "startOffset": 161, "endOffset": 197}, {"referenceID": 5, "context": "LSTMs substantially improve our ability to handle long-range dependencies, though they still have some limitations in this regard [6, 12].", "startOffset": 130, "endOffset": 137}, {"referenceID": 9, "context": "LSTMs substantially improve our ability to handle long-range dependencies, though they still have some limitations in this regard [6, 12].", "startOffset": 130, "endOffset": 137}, {"referenceID": 28, "context": "[32], and in particular the variant using LSTMs was introduced by Sundermeyer et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[38].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[31] proposed a conditional RNN-LM for adding context \u2014 we extend this approach of using context in RNNLMs to LSTMs, train the LSTM models on large-scale data, and propose new tasks beyond next work prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "These include models that capture the content structure using Hidden Markov Models (HMMs) [3], or semantic parsing techniques to identify the underlying meanings in text segments [29].", "startOffset": 90, "endOffset": 93}, {"referenceID": 25, "context": "These include models that capture the content structure using Hidden Markov Models (HMMs) [3], or semantic parsing techniques to identify the underlying meanings in text segments [29].", "startOffset": 179, "endOffset": 183}, {"referenceID": 8, "context": "Hierarchical models have been used successfully in many applications, including hierarchical Bayesian models [10, 27], hierarchical probabilistic models [37], hierarchical HMMs [14] and hierarchical CRFs [35].", "startOffset": 109, "endOffset": 117}, {"referenceID": 23, "context": "Hierarchical models have been used successfully in many applications, including hierarchical Bayesian models [10, 27], hierarchical probabilistic models [37], hierarchical HMMs [14] and hierarchical CRFs [35].", "startOffset": 109, "endOffset": 117}, {"referenceID": 33, "context": "Hierarchical models have been used successfully in many applications, including hierarchical Bayesian models [10, 27], hierarchical probabilistic models [37], hierarchical HMMs [14] and hierarchical CRFs [35].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "Hierarchical models have been used successfully in many applications, including hierarchical Bayesian models [10, 27], hierarchical probabilistic models [37], hierarchical HMMs [14] and hierarchical CRFs [35].", "startOffset": 177, "endOffset": 181}, {"referenceID": 31, "context": "Hierarchical models have been used successfully in many applications, including hierarchical Bayesian models [10, 27], hierarchical probabilistic models [37], hierarchical HMMs [14] and hierarchical CRFs [35].", "startOffset": 204, "endOffset": 208}, {"referenceID": 28, "context": "[32], and the variant using LSTMs was introduced by Sundermeyer et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[38] \u2013 in this paper, we work with LSTM-based LMs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[31] proposed a conditional RNN-LM for adding context \u2014 we extend this approach of using context in RNNLMs to LSTMs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Recent advances in deep learning can model hierarchical structure using deep belief networks [21, 47, 48], especially using a hierarchical recurrent neural network (RNN) framework.", "startOffset": 93, "endOffset": 105}, {"referenceID": 41, "context": "Recent advances in deep learning can model hierarchical structure using deep belief networks [21, 47, 48], especially using a hierarchical recurrent neural network (RNN) framework.", "startOffset": 93, "endOffset": 105}, {"referenceID": 42, "context": "Recent advances in deep learning can model hierarchical structure using deep belief networks [21, 47, 48], especially using a hierarchical recurrent neural network (RNN) framework.", "startOffset": 93, "endOffset": 105}, {"referenceID": 15, "context": "Connectionist Temporal Classification or CTC [18] does not explicitly segment the input in the hidden layer \u2013 it instead uses a forwardbackward algorithm to sum over all possible segments, and determines the normalized probability of the target sequence given the input sequence.", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "Other approaches include a hybrid NN-HMM model [1], where the temporal dependency is handled by an HMM and the dependency between adjacent frames is handled by a neural net (NN).", "startOffset": 47, "endOffset": 50}, {"referenceID": 6, "context": "Paragraph vectors [8, 26] propose an unsupervised algorithm that learns a latent variable from a sample of words from the context of a word, and uses the learned latent context representation as an auxiliary input to an underlying skip-gram or Continuous Bag-of-words (CBOW) model.", "startOffset": 18, "endOffset": 25}, {"referenceID": 22, "context": "Paragraph vectors [8, 26] propose an unsupervised algorithm that learns a latent variable from a sample of words from the context of a word, and uses the learned latent context representation as an auxiliary input to an underlying skip-gram or Continuous Bag-of-words (CBOW) model.", "startOffset": 18, "endOffset": 25}, {"referenceID": 27, "context": "Another model that uses the context of a word infers the Latent Dirichlet Allocation (LDA) topics of the context before a word and uses those to modify a RNN model predicting the word [31].", "startOffset": 184, "endOffset": 188}, {"referenceID": 37, "context": "Tree-structured LSTMs [41, 47] extend chain-structured LSTMs to the tree structure and propose a principled approach of considering long-distance interaction over hierarchies, e.", "startOffset": 22, "endOffset": 30}, {"referenceID": 41, "context": "Tree-structured LSTMs [41, 47] extend chain-structured LSTMs to the tree structure and propose a principled approach of considering long-distance interaction over hierarchies, e.", "startOffset": 22, "endOffset": 30}, {"referenceID": 20, "context": "Skip thought vectors have also been used to train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage [23].", "startOffset": 149, "endOffset": 153}, {"referenceID": 19, "context": "Other related work include Document Context Language models [22], where the authors have multi-level recurrent neural network language models that incorporate context from within a sentence and from previous sentences.", "startOffset": 60, "endOffset": 64}, {"referenceID": 24, "context": "[28] use a hierarchical RNN structure for document-level as well as sentence-level modeling \u2013 they evaluate their models using word prediction perplexity, as well as an approach of coherence evaluation by trying to predict sentence-level ordering in a document.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The following equations represent the operations of the different components of the LSTM cell [17]:", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "The word-prediction LSTM model was implemented in the large-scale distributed DistBelief framework [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 39, "context": "The core algorithm used to train the LSTM parameters is BPTT [44], using a softmax layer that uses the id of the next word as the ground truth.", "startOffset": 61, "endOffset": 65}, {"referenceID": 7, "context": "Figure 2 shows the implementation of the CLSTM model in the DistBelief framework, where the PSEmbedding Layer maps the 1-hot encoded input to a dense encoding and is also learned as part of the LSTM training [9].", "startOffset": 208, "endOffset": 211}, {"referenceID": 30, "context": "For the word prediction task we use HTM, a hierarchical topic model for supervised classification of text into a hierarchy of topic categories, based on the Google Rephil large-scale clustering tool [34].", "startOffset": 199, "endOffset": 203}, {"referenceID": 3, "context": "Note that we also trained a distributed n-gram model with \u201cstupid backoff\u201dsmoothing [4] on the Wikipedia dataset, and it gave a perplexity of \u224880 on the validation set.", "startOffset": 84, "endOffset": 87}, {"referenceID": 27, "context": "[31]), since we didn\u2019t have access to implementations of these approaches that can scale to the vocabulary sizes (\u2248 100K) and dataset sizes we worked with (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "For the task of sentence topic prediction, we also compared the CLSTM model to a Bag-of-Words Deep Neural Network (BOW-DNN) baseline [2].", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "Note that this snapshot from Google News is internal to Google, and is separate from the One Billion Word benchmark [5].", "startOffset": 116, "endOffset": 119}], "year": 2016, "abstractText": "Documents exhibit sequential structure at multiple levels of abstraction (e.g., sentences, paragraphs, sections). These abstractions constitute a natural hierarchy for representing the context in which to infer the meaning of words and larger fragments of text. In this paper, we present CLSTM (Contextual LSTM), an extension of the recurrent neural network LSTM (Long-Short Term Memory) model, where we incorporate contextual features (e.g., topics) into the model. We evaluate CLSTM on three specific NLP tasks: word prediction, next sentence selection, and sentence topic prediction. Results from experiments run on two corpora, English documents in Wikipedia and a subset of articles from a recent snapshot of English Google News, indicate that using both words and topics as features improves performance of the CLSTM models over baseline LSTM models for these tasks. For example on the next sentence selection task, we get relative accuracy improvements of 21% for the Wikipedia dataset and 18% for the Google News dataset. This clearly demonstrates the significant benefit of using context appropriately in natural language (NL) tasks. This has implications for a wide variety of NL applications like question answering, sentence completion, paraphrase generation, and next utterance prediction in dialog systems.", "creator": "LaTeX with hyperref package"}}}