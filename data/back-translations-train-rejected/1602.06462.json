{"id": "1602.06462", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2016", "title": "The Singularity May Never Be Near", "abstract": "There is both much optimism and pessimism around artificial intelligence (AI) today. The optimists are investing millions of dollars, and even in some cases billions of dollars into AI. The pessimists, on the other hand, predict that AI will end many things: jobs, warfare, and even the human race. Both the optimists and the pessimists often appeal to the idea of a technological singularity, a point in time where machine intelligence starts to run away, and a new, more intelligent species starts to inhabit the earth. If the optimists are right, this will be a moment that fundamentally changes our economy and our society. If the pessimists are right, this will be a moment that also fundamentally changes our economy and our society. It is therefore very worthwhile spending some time deciding if either of them might be right.", "histories": [["v1", "Sat, 20 Feb 2016 21:09:07 GMT  (13kb)", "http://arxiv.org/abs/1602.06462v1", "Under review"]], "COMMENTS": "Under review", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["toby walsh"], "accepted": false, "id": "1602.06462"}, "pdf": {"name": "1602.06462.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Toby Walsh"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 2.06 462v 1 [cs.A I] 2 0Fe b20 16The Singularity May Never Be NearToby Walsh University of New South Wales and Data61 (formerly NICTA) Sydney, Australia"}, {"heading": "Introduction", "text": "Optimists invest millions of dollars, and in some cases even billions of dollars, in artificial intelligence. Pessimists, on the other hand, predict that artificial intelligence will end many things: jobs, warfare, and even humanity. Optimists and pessimists alike often invoke the idea of technological singularity, a point in time when machine intelligence begins to disappear and a new, smarter \"species\" begins to inhabit the Earth. If the optimists are right, this will be a moment that fundamentally changes our economy and our society. If the pessimists are right, this will be a moment that also fundamentally changes our economy and our society. So it is worth spending some time deciding whether one of them might be right."}, {"heading": "The History of the Singularity", "text": "The idea is that people who are able to realize themselves are also able to realize themselves, not only people, but also people who are able to realize themselves, people who are able to realize themselves, people who are able to realize themselves, not only people who are able to realize themselves, but also people who are able to realize themselves, people who are able to realize themselves, not only people who are able to realize themselves, but also people who are able to realize themselves, not only people who are able to realize themselves, but also people who are able to realize themselves, not only people who are able to realize themselves, but also people who are able to realize themselves."}, {"heading": "The \u201cFast Thinking Dog\u201d argument", "text": "One of the arguments put forward by advocates of technological singularity is that silicon has a distinct speed advantage over the humidity of our brain, and that advantage doubles about every two years, according to Moore's Law. Unfortunately, speed does not bring increased intelligence. To adapt an idea by Vernor Vinge (Vinge 1993), it is still unlikely that a fast-thinking dog will play chess. Steven Pinker put it eloquently: \"There is not the slightest reason to believe in a coming singularity. The fact that you can imagine a future in your imagination is not proof that it is probable or even possible. Look at dominated cities, jetpack commuter cities, underwater cities, mile-high buildings, and nuclear-powered automobiles are all staples of futuristic fantasies when I was a kid who never arrived."}, {"heading": "The \u201cAnthropcentric\u201d argument", "text": "Many descriptions of technological singularity assume that human intelligence is a specific point to pass, a kind of \"tipping point.\" For example, Nick Bostrom writes, \"Artificial intelligence at the human level quickly leads to artificial intelligence at the superhuman level.... The interval in which machines and humans are roughly aligned is likely to be short. Shortly thereafter, man will be intellectually incapable of competing with artificial minds.\" (Bostrom 2002) Human intelligence is a point in a broad spectrum that takes us from cockroach to mouse to man. In fact, it might be better to say that it is a distribution of probabilities than a single point. In arguments such as human intelligence, which must be crossed before growth begins. Is it a kind of average intelligence? Or the intelligence of the smartest man ever? If there is one thing we should have learned from the history of science, it is that we do not like to believe it is special."}, {"heading": "The \u201cMeta-intelligence\u201d argument", "text": "One of the strongest arguments against the idea of a technological singularity, in my view, is that it confuses intelligence with the ability to improve your intelligence to accomplish a task. David Chalmers writes in an otherwise careful analysis of the idea of a technological singularity: \"However, if we create an AI through machine learning, it is likely that soon after that we will be able to improve the learning algorithm and expand the learning process, leading to AI +.\" (Chalmers 2010) Here, AI is a system with human intelligence and AI + is a system that is more intelligent than the most intelligent human. But why should it be likely that soon after that we can improve the learning algorithm that leads to AI +? Advances in machine learning of algorithms are neither particularly fast nor easy. Indeed, machine learning is probably a significant component of any human AI system that we could build in the future, if it will only be painful to be to be able to encode its knowledge and competence differently."}, {"heading": "The \u201cDiminishing returns\u201d argument", "text": "The idea of technological singularity usually assumes that improvements in intelligence will be a relatively constant multiplier, with each generation improving a fraction better than the last. However, the performance of most of our AI systems so far has been that of a declining rate of return. Often, there are many low-hanging fruits at the beginning, but afterwards, we encounter great difficulties in improving ourselves. This explains the overly optimistic claims of many of the early AI researchers. While an AI system can improve itself infinitely often, the extent to which its overall intelligence changes may be limited. If, for example, each generation improves by only half of the last change, the system will never go beyond a doubling of its overall intelligence. Declining returns may also not stem from the difficulty of improving our AI algorithms, but from the difficulty of its subject rapidly increasing. Paul Allen, Microsoft's co-founder, calls this the \"complexity brake.\""}, {"heading": "The \u201cLimits of intelligence\u201d argument", "text": "There are many basic boundaries within the Universe, some of which are physical. You cannot accelerate beyond the speed of light, you cannot know the position or momentum with absolute accuracy, you cannot know when the radioactive decay of an atom is certain to occur, any thinking machine we could build will be limited by these physical laws. Of course, if this machine is electronic or even quantum in nature, these boundaries will probably be much greater than the biological and chemical boundaries of our human brain. There are also more empirical laws that can be observed from complex systems. Dunbar's number, for example, is the observed correlation between the brain size of primates and the average social group size, which sets a boundary between 100 and 250 stable relationships for human social groups. Intelligence is also a complex phenomenon and may also have such boundaries as result from this complexity."}, {"heading": "The \u201cComputational complexity\u201d argument", "text": "Suppose we build AI systems with computers that obey our traditional computing models, and even exponential improvements are not up to the task of computing complexity, for example, exponential power growth is not enough to run super-exponential algorithms, and no increase in performance will make undecidable problems decidable. Computational complexity could be one of the basic limits discussed in the previous argument, so if we use machines that go beyond our traditional computing models, we are likely to encounter many problems where computing complexity fundamentally limits performance. Of course, a lot of computing complexity is about the worst case, and much of AI is about solving problems in practice with heuristics that are, at worst, mathematically insolvable. However, there are fundamental limits to the quality of this heuristics, and there will be classes of problems that even superhuman intelligence cannot solve well, even remotely."}, {"heading": "Conclusions", "text": "I have argued that there are many reasons why we may never experience a technological signaling effect, but even without technological singularity, we may end up with machines that have superhuman levels of intelligence, and we may have to program much of it ourselves painfully. If so, the effects of artificial intelligence on our economy and society may be less dramatic than either the pessimists or optimists predicted. Nonetheless, we should start planning the impact that artificial intelligence will have on society. Even without technological singularity, artificial intelligence is likely to have a major impact on the nature of work."}], "references": [{"title": "The singularity isn\u2019t near", "author": ["Allen", "P. Greaves 2011] Allen", "M. Greaves"], "venue": "MIT Technology Review", "citeRegEx": "Allen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Allen et al\\.", "year": 2011}], "referenceMentions": [], "year": 2016, "abstractText": "There is both much optimism and pessimism around artificial intelligence (AI) today. The optimists are investing millions of dollars, and even in some cases billions of dollars into AI. The pessimists, on the other hand, predict that AI will end many things: jobs, warfare, and even the human race. Both the optimists and the pessimists often appeal to the idea of a technological singularity, a point in time where machine intelligence starts to run away, and a new, more intelligent \u201cspecies\u201d starts to inhabit the earth. If the optimists are right, this will be a moment that fundamentally changes our economy and our society. If the pessimists are right, this will be a moment that also fundamentally changes our economy and our society. It is therefore very worthwhile spending some time deciding if either of them might be right.", "creator": "LaTeX with hyperref package"}}}