{"id": "1602.06489", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2016", "title": "Distributed Private Online Learning for Social Big Data Computing over Data Center Networks", "abstract": "With the rapid growth of Internet technologies, cloud computing and social networks have become ubiquitous. An increasing number of people participate in social networks and massive online social data are obtained. In order to exploit knowledge from copious amounts of data obtained and predict social behavior of users, we urge to realize data mining in social networks. Almost all online websites use cloud services to effectively process the large scale of social data, which are gathered from distributed data centers. These data are so large-scale, high-dimension and widely distributed that we propose a distributed sparse online algorithm to handle them. Additionally, privacy-protection is an important point in social networks. We should not compromise the privacy of individuals in networks, while these social data are being learned for data mining. Thus we also consider the privacy problem in this article. Our simulations shows that the appropriate sparsity of data would enhance the performance of our algorithm and the privacy-preserving method does not significantly hurt the performance of the proposed algorithm.", "histories": [["v1", "Sun, 21 Feb 2016 02:32:25 GMT  (988kb)", "http://arxiv.org/abs/1602.06489v1", "ICC2016"]], "COMMENTS": "ICC2016", "reviews": [], "SUBJECTS": "cs.DC cs.LG cs.SI", "authors": ["chencheng li", "pan zhou", "yingxue zhou", "kaigui bian", "tao jiang", "susanto rahardja"], "accepted": false, "id": "1602.06489"}, "pdf": {"name": "1602.06489.pdf", "metadata": {"source": "CRF", "title": "Distributed Private Online Learning for Social Big Data Computing over Data Center Networks", "authors": ["Chencheng Li", "Pan Zhou", "Yingxue Zhou", "Kaigui Bian"], "emails": ["lichencheng@hust.edu.cn,", "panzhou@hust.edu.cn,"], "sections": [{"heading": null, "text": "In this case, it is a purely mental game, which is about finding a solution, how it can be found, how it can be found."}, {"heading": "II. SYSTEM MODEL", "text": "This section presents the system model and our private online learning algorithm. Consider a social network where all online users are served on cloud platforms, for example Fig.1. These users operate on their own personal site and the social data generated is collected and transferred to the nearest data centre on the cloud, just as shown in Fig.1, all data is collected from data centres marked A \u2192 G. Because of the huge network, many data centres are scattered across the world. As there are too many data centres and most of them are scattered around the world, a data centre can never communicate with all the other centres. To achieve better economic benefits, each data centre should only have good knowledge of all the data it has, and therefore data centres should share information with each other. As there are too many data centres and most of them are scattered around the world, a data centre can never communicate with all the other centres."}, {"heading": "A. Communication Graph", "text": "For our online social learning network, we call the communication matrix A and leave aij the (i, j) th element of A. In the system, aij is the weight of the learning parameter that the i-th cloud node transmits to the j-th. aij (t) > 0 means that there is communication between the i-th and the j-th node in round t, while aij (t) = 0 means non-communication between them. To achieve global convergence, we proceed from some assumptions to A. Assumption 1. For an arbitrary node i, there is a minimum optimization in round t byGi = {(i, j): aij > 0}, (1) where aij > 0 is heading (i, j)."}, {"heading": "B. Sparse Online Learning", "text": "To find the factors that have the most to do with predicting behavior, we must aggressively zero out the irrelevant dimensions. \u2212 Lasso [14] is a famous method of producing some coefficients that are exactly 0. (Lasso cannot be used directly in the algorithm, we combine it with an online mirror descent (see algorithm 1), which is a special online learning algorithm. For convenient analysis, we will next make some assumptions about the mathematical model of the online learning system in the social network. (We assume that we have m data centers in the social network. \u2212 Each data center collects massive social data every minute and processes it in cloud computing. For the data generated from social networks, we use x to name the social data of the individual."}, {"heading": "C. Differential Privacy", "text": "Dwork [15] first proposed the definition of a differential privacy that would enable a Data Miner to publish some statistics from its database without revealing sensitive information about a particular value itself. In this essay, we realize output disturbances by adding a random noise denoted by \u03b4, which prevents some malicious Data Miners from stealing sensitive information (e.g. birthday and contact information). Based on the parameters defined above, we give the following definition. Definition 1. Suppose A denotes our differentiated private online learning algorithm. Let X = < x1, x2,..., xT > be a sequence of social data taken from the local data center of any node. Suppose A > is a sequence of node T results and vice versa."}, {"heading": "D. Private Distributed Online Learning Algorithm", "text": "We present a private distributed online learning algorithm for cloud-based social networks. Specifically, each cloud computing node propagates the parameter with noise added to adjacent nodes. After obtaining the parameters from others, each node calculates a weight average of the received and its old parameters. Subsequently, each node updates the parameter based on a general online mirror descent and induces sparseness using lasso. The algorithm is summarized in algorithm 1. Note that wit denotes the parameter of the i-th cloud node at a certain time. t = 1,..., T are a series of \u03b2-strongly convex functions. \u2212 Algorithm 1 Private Distributed Online Learning1: Input: Cost functions f (w): = (w, x i t, y i t), i [1, m] and t: it [1, t] and t [1, T] [1, T], double Git; double Gization \u00b7 (x): (m)."}, {"heading": "III. PRIVACY ANALYSIS", "text": "In step 11 of algorithm 1 \u03b8 is the exchanged parameter to which we add random noise. Adding noise causes \u03b8 to be disturbed, so that someone else cannot degrade the privacy of the individual using an exact parameter. Remember, DL is mathematically defined in definition 1, which aims to weaken the significant difference between A (X) and A (X). Only by satisfying the inequality (4) can we ensure the privacy of social data in each data center."}, {"heading": "A. Adding Noise", "text": "Since we add noise to disguise the difference between two datasets that differ at most in one point, the sensitivity should be known. Dwork [15] suggested that the size of the noise depends on the largest change that a single entry in the data source could have on the output of algorithm 1; this value is called the sensitivity of algorithm 1 in the defined definition 2 (sensitivity). Based on definition 1, for each X and X \"that differ in exactly one entry, we define the sensitivity of algorithm 1 in the tenth round asS (t) = sup X, X\" A (X) \u2212 A (X \") \u2022 Knowledge in Lemma 1. Assuming 1 that the L1 sensitivity of the parameter Progorithm 1 is defined as (5), we determine the sensitivity of algorithm 1 in the t-th round asS (t) = sup,\" X. \""}, {"heading": "B. Guaranteeing \u01eb-Differentially Private", "text": "In our system model as an independent cloud node, every data center should protect privacy at all times. (If a malicious user invades a data center, that \"bad kid\" may receive some information about the social data of other users stored in other data centers on the network.) Therefore, any data transmitted by DL (i.e., satisfy (4)) should be processed. (Quoting from Fig.1, we add random noise to any communication on the data center network. (After describing the method and extent of the additional noise, we will next demonstrate how we guarantee privacy each time. (First, we demonstrate the preservation of privacy at all times.) In the fourth round, the i-th cloud node product of A, and then it is private to varying degrees. Evidence. (Let us know that it is privacy.) Let us know that it is the shared privacy (that it is shared privacy)."}, {"heading": "IV. UTILITY ANALYSIS", "text": "The regret of our online learning algorithms is a sum of errors made by all data centres during the learning and prediction process. If social websites make personalised recommendations (e.g. songs, videos and messages, etc.) to users, not all recommendations make sense for individuals. However, we would like the system that works and collects more social data to make the predictions used for the recommendation more accurate, which means that the regret should have a limit. Therefore, lower remorse indicates better and faster online learning algorithms. We specify the definition of \"remorse.\" Definition 3. We propose algorithms that are used for social websites via data centres. Then we measure the regret of the algorithms asR = T = 1 m = 1 m es (wt) \u2212 min WT = 1 m es."}, {"heading": "V. SIMULATIONS", "text": "In this section, we run four simulations: the first is to study privacy and predictive performance compromises; the second is to find out whether the topology of social networks has a major impact on performance; the third is to examine sparseness and performance compromises; the last is to analyze performance compromises between the number of data centres and accuracy; all simulations are run on real large-scale and high-dimensional social data; for our implementations, we have the loss of hinges f it (w) = max (1 \u2212 yit w, xit), where {(xit, y t) the data is only available to the i-th data centre; for persuasion, we use 100,000 social data to experiment, and the dimension of the data is 10,000; since the data being tested is real social data, we should anticipate the data; each dimension in vectors is converted into a specific numerical interval."}, {"heading": "VI. CONCLUSION", "text": "The Internet has entered the big data era, and social networks face huge amounts of data to handle. In response to these challenges, we proposed a private, distributed online learning algorithm for social big data over data center networks. We demonstrated that a higher level of data protection leads to weaker use of the system, and the corresponding scarcity increases the performance of online learning for high-dimensional data. In addition, there must be delays in social networks that we have not considered. Therefore, we hope that online learning can be presented with delay in future work."}, {"heading": "ACKNOWLEDGMENT", "text": "This research is supported by the National Science Foundation of China with grant 61401169."}], "references": [{"title": "A survey of cloud computing and social networks", "author": ["S.P. Ahuja", "B. Moore"], "venue": "Network and Communication Technologies,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Analysis of an investment social network", "author": ["A. Dusenbery", "K. Nguyen", "D. Tran"], "venue": "ICC. IEEE, 2012, pp. 2087-2092.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding social networks from a multiagent perspective", "author": ["Y. Jiang", "J. Jiang"], "venue": "IEEE Transactions on Parallel and Distributed Systems, vol. 25, no. 10, pp. 2743-2759, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Dual averaging for distributed optimization", "author": ["J.C. Duchi", "A. Agarwal", "M.J. Wainwright"], "venue": "Allerton Conference on Communication, Control, and Computing (Allerton). IEEE, 2012, pp. 1564-1565.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed subgradient methods for multiagent optimization", "author": ["A. Nedic", "A. Ozdaglar"], "venue": "IEEE Transactions on Automatic Control, vol. 54, no. 1, pp. 48-61, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Distributed stochastic subgradient projection algorithms for convex optimization", "author": ["S. Ram", "A. Nedic", "V. Veeravalli"], "venue": "Journal of optimization theory and applications, vol. 147, no. 3, pp. 516-545, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning, vol. 4, no. 2, pp. 107- 194, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Large scale distributed sparse precision estimation", "author": ["H. Wang", "A. Banerjee", "C.-J. Hsieh", "P.K. Ravikumar", "I.S. Dhillon"], "venue": "In NIPS, 2013, pp. 584-592.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "A framework of sparse online learning and its applications", "author": ["D. Wang", "P. Wu", "P. Zhao", "S.C. Hoi"], "venue": "arXiv preprint arXiv:1507.07146, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic methods for L1- regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 1865-1892, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1865}, {"title": "Sparse online learning via truncated gradient", "author": ["J. Langford", "L. Li", "T. Zhang"], "venue": "Journal of Machine Learning Research, 2009, pp. 777-801.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 2543-2596, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Composite objective mirror descent", "author": ["J.C. Duchi", "S. Shalev-Shwartz", "Y. Singer", "A. Tewari"], "venue": "COLT. Citeseer, 2010, pp. 14-26.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 267-288, 1996.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Differential privacy", "author": ["C. Dwork"], "venue": "Proceedings of the 33rd ICALP. Springer-Verlag, 2006, pp. 1-12.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Privacy integrated queries: an extensible platform for privacy-preserving data analysis", "author": ["F.D. McSherry"], "venue": "Proceedings of the SIGMOD International Conference on Management of data. ACM, 2009, pp. 19- 30.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "ICML, pp. 928-936, 2003.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "A social network is referred as a structure of \u201cInternet users\u201d interconnected through a variety of relations [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "They can rent cloud computing services from other third part due to their actual needs and scale up and down at any time without taking additional cost in infrastructure [2].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": "Naturally, for social data analysis in cloud, a distributed online learning algorithm is needed to handle the massive social data in distributed scenarios [3].", "startOffset": 155, "endOffset": 158}, {"referenceID": 3, "context": "In theory, this approach is a distributed optimization technology and many researches [4]\u2013[6] have been devoted to it.", "startOffset": 86, "endOffset": 89}, {"referenceID": 5, "context": "In theory, this approach is a distributed optimization technology and many researches [4]\u2013[6] have been devoted to it.", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "To estimate the utility of the proposed model, we use the notion \u201cregret\u201d [7] in online learning (see Definition 3).", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "In this paper, we introduce two classical groups of effective methods for sparse online learning [8]\u2013[10].", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "In this paper, we introduce two classical groups of effective methods for sparse online learning [8]\u2013[10].", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": ", [11]) induces sparsity in the weights of online learning algorithms via truncated gradient.", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "The second group studies on sparse online learning follows the dual averaging algorithm [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "In this paper, we will exploit online mirror descent [13] and Lasso-L1 norm [14] to make the parameter updated in algorithm sparse.", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "In this paper, we will exploit online mirror descent [13] and Lasso-L1 norm [14] to make the parameter updated in algorithm sparse.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "Finally, we choose the \u201cdifferential privacy\u201d [15] technology to guarantee the safety of data centers in cloud.", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": ", [4]\u2013[6]) about distributed optimization.", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": ", [4]\u2013[6]) about distributed optimization.", "startOffset": 6, "endOffset": 9}, {"referenceID": 13, "context": "Lasso [14] is a famous method to produce some coefficients that are exactly 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "Dwork [15] first proposed the definition of differential privacy which makes a data miner be able to release some statistic of its database without revealing sensitive information about a particular value itself.", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "Dwork [15] proposed that the magnitude of the noise depends on the largest change that a single entry in data source could", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "McSherry [16] has proposed that the privacy guarantee does not degrade across rounds as the samples used in the rounds are disjoint.", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "For details of proof of Theorem 1, readers are advised to [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 8, "context": "[9], we know", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "According to Theorem 2, the regret bound becomes the classical square root regret O( \u221a T ) [17], which means less mistakes are made in social recommendations as the algorithm runs.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "Each data point is labeled with a value into [0, 1] according to its classification attribute.", "startOffset": 45, "endOffset": 51}], "year": 2016, "abstractText": "With the rapid growth of Internet technologies, cloud computing and social networks have become ubiquitous. An increasing number of people participate in social networks and massive online social data are obtained. In order to exploit knowledge from copious amounts of data obtained and predict social behavior of users, we urge to realize data mining in social networks. Almost all online websites use cloud services to effectively process the large scale of social data, which are gathered from distributed data centers. These data are so largescale, high-dimension and widely distributed that we propose a distributed sparse online algorithm to handle them. Additionally, privacy-protection is an important point in social networks. We should not compromise the privacy of individuals in networks, while these social data are being learned for data mining. Thus we also consider the privacy problem in this article. Our simulations shows that the appropriate sparsity of data would enhance the performance of our algorithm and the privacy-preserving method does not significantly hurt the performance of the proposed algorithm.", "creator": "LaTeX with hyperref package"}}}