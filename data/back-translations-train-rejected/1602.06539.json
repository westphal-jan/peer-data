{"id": "1602.06539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2016", "title": "Determining the best attributes for surveillance video keywords generation", "abstract": "Automatic video keyword generation is one of the key ingredients in reducing the burden of security officers in analyzing surveillance videos. Keywords or attributes are generally chosen manually based on expert knowledge of surveillance. Most existing works primarily aim at either supervised learning approaches relying on extensive manual labelling or hierarchical probabilistic models that assume the features are extracted using the bag-of-words approach; thus limiting the utilization of the other features. To address this, we turn our attention to automatic attribute discovery approaches. However, it is not clear which automatic discovery approach can discover the most meaningful attributes. Furthermore, little research has been done on how to compare and choose the best automatic attribute discovery methods. In this paper, we propose a novel approach, based on the shared structure exhibited amongst meaningful attributes, that enables us to compare between different automatic attribute discovery approaches.We then validate our approach by comparing various attribute discovery methods such as PiCoDeS on two attribute datasets. The evaluation shows that our approach is able to select the automatic discovery approach that discovers the most meaningful attributes. We then employ the best discovery approach to generate keywords for videos recorded from a surveillance system. This work shows it is possible to massively reduce the amount of manual work in generating video keywords without limiting ourselves to a particular video feature descriptor.", "histories": [["v1", "Sun, 21 Feb 2016 15:08:51 GMT  (2392kb,D)", "http://arxiv.org/abs/1602.06539v1", "7 pages, ISBA 2016. arXiv admin note: text overlap witharXiv:1602.01940"]], "COMMENTS": "7 pages, ISBA 2016. arXiv admin note: text overlap witharXiv:1602.01940", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["liangchen liu", "arnold wiliem", "shaokang chen", "kun zhao", "brian c lovell"], "accepted": false, "id": "1602.06539"}, "pdf": {"name": "1602.06539.pdf", "metadata": {"source": "CRF", "title": "Determining the best attributes for surveillance video keywords generation", "authors": ["Liangchen Liu", "Arnold Wiliem", "Shaokang Chen", "Kun Zhao", "Brian C. Lovell"], "emails": ["l.liu9@uq.edu.au", "a.wiliem@uq.edu.au", "shaokangchenuq@gmail.com", "k.zhao1@uq.edu.au", "lovell@itee.uq.edu.au"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2. Related Works", "text": "Recently, several methods have been proposed that deal with extracting video passwords and descriptions [26, 27, 31, 29, 35, 5, 4]. For example, Rohrbach et al. [27] proposed generating a rich semantic representation of visual content such as object and activity labels, using the conditional random field (CRF) to model all visual input components. In [26], they expanded their work to a three-step video description scheme, and then applied a machine translation scheme to generate natural language using semantic representation as sources. Unfortunately, this model cannot be used to solve our problem due to the extensive manual labeling work. To this end, some researchers rely on hierarchical probability models. Wang et al. [31] and Varadarajan et al. [29] use LDA and pLSA to perform unattended activity analyses."}, {"heading": "3. Selecting the attribute discovery method", "text": "First, we describe the manifold space in which the attributes are located, and then use this representation to select the method for discovering attributes that will discover the most significant attributes. Technically, we measure the meaningfulness of a number of discovered attributes."}, {"heading": "3.1. The manifold of decision boundaries", "text": "??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "3.2. Distance from the Meaningful Subspace", "text": "Considering a set of images X and a set of discovered attributes, D = {zk} Kk = 1, zk \u00b2 J = 1, zk \u00b2 N, here is our goal to define the removal of attributes from the various image datasets such as [2, 19, 20]. These attributes are considered human annotators that they have labeled via the Amazon Mechanical Turk (AMT). We define this set of meaningful attributes as S = {hj} YY = 1} N. Since meaningful attributes are assumed to be a common structure, we might assume that a meaningful attribute mustbe can be described in the other meaningful attributes. For instance, a set of attributes of primary colors red, green, and blue could be used to reconstruct the primary structure."}, {"heading": "4. Generating keywords using discovered attributes", "text": "Despite this manual process, we argue that the manual process of naming meaningful attributes is much easier and faster than the manual process of labeling images / videos to train attribute characteristics. You can name an attribute by first extracting the attribute characteristics from a given set of images. As mentioned above, each attribute splits any group of images / videos into two groups: the group of images in which the visual attribute exists (the positive class) and the group of images / videos in which the visual attribute is missing (the negative class). Some attributes may have similar names, in which case these attributes are considered duplicate and thus merged."}, {"heading": "5. Experiment", "text": "In this section, we validate our proposed approach and evaluate the accuracy of the keywords extracted from the best discovered method of describing videos. In the first part, we evaluate the ability of our approach to measure the meaningfulness of a set of attributes, and then we use our proposed approach to evaluate the meaningfulness of attributes using various automatic methods for identifying attributes such as PiCoDeS [1], as well as hash methods such as Spectral Hashing (SPH) [32] and Locality Sensitivity Hashing (LSH) [15]. In this case, two datasets are used: (1) a-Pascal a-Yahoo dataset (ApAy) [9]; (2) SUN Attribute Dataset (ASUN) [21]. In the second part of our experiment, we apply the best method for recognizing attributes from a monitoring dataset. In this setting, we use UT Tower aerial datasets (UT8)."}, {"heading": "5.1. Datasets and experiment setup", "text": "In fact, most of them will be able to play by the rules that they need for their work, and they will be able to play by the rules that they need for their work."}, {"heading": "5.2. Attribute meaningfulness evaluation", "text": "In this experiment, our goal is to verify that the proposed approach actually measures the meaningfulness of the discovered attributes. One of the most important assumptions in our proposal is that the meaningfulness is reflected in the distance between the meaningful subspace and the given attribute set. That is, if the distance is long, then it is assumed that the attribute set is less significant, and vice versa. To evaluate this assumption, we create two sets of attributes, meaningful and non-meaningful attributes, and observe their distances to the meaningful substances. For the meaningful attribute, we set the attributes from AMT provided in each data set. More specifically, given the manually designated attribute set S, we divide the set into two subsets S2 = S. Following the attributes used in Section 3, we use S1 to represent the Meaningful Subspace and consider S2 as a set of discovered attributes (i.e., SD = 2)."}, {"heading": "5.3. Generating video keywords using discovered", "text": "In this experiment, we will follow the strategy proposed in Section 4. Here, we ask experts to perform the attribute naming task for the three attribute discovery methods such as PiCoDeS, SH and LSH configured to discover 16 attributes on the UTTower video records. Then, we will use the attributes mentioned as keywords. To make our work reproducible, our experimental results will be published online1 after this thesis. Note that we will only consider the attributes that can be named by experts. This means that any attribute that cannot be considered a valid keyword. After performing this task, we will find that there are 9 attributes for PiCoDeS, 8 attributes for SH and 3 attributes for LSH that can be named. These results indicate that our proposed approach is able to guide us in selecting the best attribute discovery method."}, {"heading": "6. Conclusion", "text": "Since there have been numerous approaches to attribute recognition in the literature, we developed a selection method based on the common structure between significant attributes, which allows us to compare effectiveness between different automatic approaches to attribute recognition. In particular, we developed a distance function that measures the meaningfulness of a number of attributes discovered. Finally, we demonstrated how the detected attributes were used to discover the most likely meaningful attributes. Then, we validated our approach using two attribute datasets. The results showed that our approach is capable of determining which automatic method for attribute recognition can generate the most meaningful keywords or attributes. Finally, we showed how the detected attributes were used to generate keywords for videos recorded from a monitoring system. The proposed approach suggests that it is possible to reduce the amount of manual work involved in video generation to a limited number of keywords, without restricting the number of functions."}], "references": [{"title": "Picodes: Learning a compact code for novel-category recognition", "author": ["A. Bergamo", "L. Torresani", "A.W. Fitzgibbon"], "venue": "NIPS,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Simultaneous active learning of classifiers & attributes via relative feedback", "author": ["A. Biswas", "D. Parikh"], "venue": "CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Face recognition based on image sets", "author": ["H. Cevikalp", "B. Triggs"], "venue": "CVPR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic concept discovery for large-scale zero-shot event detection", "author": ["X. Chang", "Y. Yang", "A.G. Hauptmann", "E.P. Xing", "Y.- L. Yu"], "venue": "In IJCAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Dynamic concept composition for zero-example event detection", "author": ["X. Chang", "Y. Yang", "G. Long", "C. Zhang", "A.G. Hauptmann"], "venue": "AAAI,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Complex event detection using semantic saliency and nearly-isotonic svm", "author": ["X. Chang", "Y. Yang", "E.P. Xing", "Y.-L. Yu"], "venue": "ICML,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Searching persuasively: Joint event detection and evidence recounting with limited supervision", "author": ["X. Chang", "Y.-L. Yu", "Y. Yang", "A.G. Hauptmann"], "venue": "ACM Conference on Multimedia (MM),", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Ut-tower dataset: Aerial View Activity Classification Challenge", "author": ["C.-C. Chen", "M.S. Ryoo", "J.K. Aggarwal"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "CVPR,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "CVPR,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Semanticbased surveillance video retrieval", "author": ["W. Hu", "D. Xie", "Z. Fu", "W. Zeng", "S. Maybank"], "venue": "Image Processing, IEEE Transactions on, 16(4):1168\u20131181,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Recognizing complex events using large margin joint low-level event model", "author": ["H. Izadinia", "M. Shah"], "venue": "ECCV.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "A survey on behavior analysis in video surveillance for homeland security applications", "author": ["T. Ko"], "venue": "Applied Imagery Pattern Recognition Workshop, 2008. AIPR \u201908. 37th IEEE,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Attributebased classification for zero-shot learning of object categories", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 99:1,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Mining of Massive Datasets", "author": ["J. Leskovec", "A. Rajaraman", "J. Ullman"], "venue": "Cambridge university press,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "A novel separating strategy for face hallucination", "author": ["L. Liu", "W. Li", "S. Tang", "W. Gong"], "venue": "ICIP,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Anomaly detection in crowded scenes", "author": ["V. Mahadevan", "W. Li", "V. Bhalodia", "N. Vasconcelos"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Interactive exploration of surveillance video through action shot summarization and trajectory visualization", "author": ["A.H. Meghdadi", "P. Irani"], "venue": "Visualization and Computer Graphics, IEEE Transactions on, 19(12):2119\u20132128,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Interactive discovery of taskspecific nameable attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "Workshop on Fine-Grained Visual Categorization, CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Interactively building a discriminative vocabulary of nameable attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "CVPR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Sun attribute database: Discovering, annotating, and recognizing scene attributes", "author": ["G. Patterson", "J. Hays"], "venue": "CVPR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Large-scale image retrieval with compressed fisher vectors", "author": ["F. Perronnin", "Y. Liu", "J. S\u00e1nchez", "H. Poirier"], "venue": "CVPR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Clustered synopsis of surveillance video", "author": ["Y. Pritch", "S. Ratovitch", "A. Hendel", "S. Peleg"], "venue": "AVSS, pages 195\u2013200,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Video event understanding using natural language descriptions", "author": ["V. Ramanathan", "P. Liang", "L. Fei-Fei"], "venue": "ICCV,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Attribute discovery via predictable discriminative binary codes", "author": ["M. Rastegari", "A. Farhadi", "D. Forsyth"], "venue": "ECCV.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Coherent multi-sentence video description with variable level of detail", "author": ["A. Rohrbach", "M. Rohrbach", "W. Qiu", "A. Friedrich", "M. Pinkal", "B. Schiele"], "venue": "pages 184\u2013195,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "Q. Wei", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "ICCV,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Spatiotemporal covariance descriptors for action and gesture recognition", "author": ["A. Sanin", "C. Sanderson", "M. Harandi", "B. Lovell"], "venue": "WACV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "A sequential topic model for mining recurrent activities from long term video logs", "author": ["J. Varadarajan", "R. Emonet", "J.-M. Odobez"], "venue": "International journal of computer vision, 103(1):100\u2013126,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(3):480\u2013492, March", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised activity perception in crowded and complicated scenes using hierarchical bayesian models", "author": ["X. Wang", "X. Ma", "W.E.L. Grimson"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(3):539\u2013555,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "NIPS,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Discovering discriminative cell attributes for hep-2 specimen image classification", "author": ["A. Wiliem", "P. Hobson", "B.C. Lovell"], "venue": "WACV,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba"], "venue": "CVPR,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Discovery of shared semantic spaces for multi-scene video query and summarization", "author": ["X. Xu", "T. Hospedales", "S. Gong"], "venue": "arXiv preprint arXiv:1507.07458,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Kernelised orthonormal random projection on grassmann manifolds with applications to action and gait-based gender recognition", "author": ["K. Zhao", "A. Wiliem", "B. Lovell"], "venue": "Identity, Security and Behavior Analysis (ISBA), 2015 IEEE International Conference on,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "For example, they can be used to detect anomalous events to alert security officers [13].", "startOffset": 84, "endOffset": 88}, {"referenceID": 22, "context": "This makes finding critical information in surveillance video as challenging as finding the proverbial needle in a haystack [23].", "startOffset": 124, "endOffset": 128}, {"referenceID": 27, "context": "Some examples of the current works are: action recognition [28], face hallucination [16], anomaly detection [17], video description [18] and video complex event detection [7, 6].", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "Some examples of the current works are: action recognition [28], face hallucination [16], anomaly detection [17], video description [18] and video complex event detection [7, 6].", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "Some examples of the current works are: action recognition [28], face hallucination [16], anomaly detection [17], video description [18] and video complex event detection [7, 6].", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "Some examples of the current works are: action recognition [28], face hallucination [16], anomaly detection [17], video description [18] and video complex event detection [7, 6].", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "Some examples of the current works are: action recognition [28], face hallucination [16], anomaly detection [17], video description [18] and video complex event detection [7, 6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 5, "context": "Some examples of the current works are: action recognition [28], face hallucination [16], anomaly detection [17], video description [18] and video complex event detection [7, 6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 26, "context": "Keywords are important ingredients in generating textual descriptions [27].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "Unfortunately, existing approaches still require a great deal of manual labelling before the systems can be used to generate the keywords/description [11].", "startOffset": 150, "endOffset": 154}, {"referenceID": 11, "context": "in [12] uses extensive spatio temporal annotations to train action and role models for action recognition.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "One feasible way to circumvent this is to employ latent hierarchical probabilistic models such as probabilistic Latent Semantic Analysis (pLSA) [29] or Latent Dirichlet Allocation (LDA) [31].", "startOffset": 144, "endOffset": 148}, {"referenceID": 30, "context": "One feasible way to circumvent this is to employ latent hierarchical probabilistic models such as probabilistic Latent Semantic Analysis (pLSA) [29] or Latent Dirichlet Allocation (LDA) [31].", "startOffset": 186, "endOffset": 190}, {"referenceID": 0, "context": "More specifically, several attribute discovery methods such as PiCoDeS [1] and Spectral Hashing [32] can be employed.", "startOffset": 71, "endOffset": 74}, {"referenceID": 31, "context": "More specifically, several attribute discovery methods such as PiCoDeS [1] and Spectral Hashing [32] can be employed.", "startOffset": 96, "endOffset": 100}, {"referenceID": 0, "context": "In practice, we can represent the binary features as [1 1 0].", "startOffset": 53, "endOffset": 60}, {"referenceID": 0, "context": "In practice, we can represent the binary features as [1 1 0].", "startOffset": 53, "endOffset": 60}, {"referenceID": 8, "context": "The attribute features trained in one domain can be reused for another domain with minimum manual work [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 13, "context": "As such, a system can be potentially trained to recognize unseen events [14].", "startOffset": 72, "endOffset": 76}, {"referenceID": 26, "context": "Visual attributes have shown promising results in many works which deal with video related tasks [27, 24] as well as in some novel problems such as the zero shot learning problem [14].", "startOffset": 97, "endOffset": 105}, {"referenceID": 23, "context": "Visual attributes have shown promising results in many works which deal with video related tasks [27, 24] as well as in some novel problems such as the zero shot learning problem [14].", "startOffset": 97, "endOffset": 105}, {"referenceID": 13, "context": "Visual attributes have shown promising results in many works which deal with video related tasks [27, 24] as well as in some novel problems such as the zero shot learning problem [14].", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "To that end, some researchers have turned their attention to automatic attribute discovery methods [1, 25, 33].", "startOffset": 99, "endOffset": 110}, {"referenceID": 24, "context": "To that end, some researchers have turned their attention to automatic attribute discovery methods [1, 25, 33].", "startOffset": 99, "endOffset": 110}, {"referenceID": 32, "context": "To that end, some researchers have turned their attention to automatic attribute discovery methods [1, 25, 33].", "startOffset": 99, "endOffset": 110}, {"referenceID": 9, "context": "We note that these approaches are also closely related to hashing approaches [10, 15, 32].", "startOffset": 77, "endOffset": 89}, {"referenceID": 14, "context": "We note that these approaches are also closely related to hashing approaches [10, 15, 32].", "startOffset": 77, "endOffset": 89}, {"referenceID": 31, "context": "We note that these approaches are also closely related to hashing approaches [10, 15, 32].", "startOffset": 77, "endOffset": 89}, {"referenceID": 18, "context": "The intuition of our approach comes from a speculation proposed in [19, 20].", "startOffset": 67, "endOffset": 75}, {"referenceID": 19, "context": "The intuition of our approach comes from a speculation proposed in [19, 20].", "startOffset": 67, "endOffset": 75}, {"referenceID": 25, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 26, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 30, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 28, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 34, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 4, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 3, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 26, "context": "[27] proposed to generate a rich semantic representation of the visual content such as object and activity labels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "In [26], they extended their work to a threelevel-of-detail video description scheme.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "[31] and Varadarajan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] employ LDA and pLSA respectively to perform unsupervised activity analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "This means, more powerful features such as Fisher vectors [22] cannot be used directly.", "startOffset": 58, "endOffset": 62}, {"referenceID": 34, "context": "[35] develop a novel distributed multiple-scene global understanding framework that clusters surveillance scenes by their ability to explain each others behaviours.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Hence, in this case, we assume that all attributes lie on a manifold of decision boundaries [20].", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "One possible solution is to use previously human labelled attributes in various image datasets such as [2, 19, 20].", "startOffset": 103, "endOffset": 114}, {"referenceID": 18, "context": "One possible solution is to use previously human labelled attributes in various image datasets such as [2, 19, 20].", "startOffset": 103, "endOffset": 114}, {"referenceID": 19, "context": "One possible solution is to use previously human labelled attributes in various image datasets such as [2, 19, 20].", "startOffset": 103, "endOffset": 114}, {"referenceID": 2, "context": "One possible way to address this is to add the convex hull regularization which has been shown in [3] to induce sparsity.", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "The above optimization problem could be solved using the method proposed in [3].", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "Then, we use our proposed approach to evaluate attribute meaningfulness on the attribute sets generated from various automatic attribute discovery methods such as PiCoDeS [1] as well as the hashing methods such as Spectral Hashing (SPH) [32] and Locality Sensitivity Hashing (LSH) [15].", "startOffset": 171, "endOffset": 174}, {"referenceID": 31, "context": "Then, we use our proposed approach to evaluate attribute meaningfulness on the attribute sets generated from various automatic attribute discovery methods such as PiCoDeS [1] as well as the hashing methods such as Spectral Hashing (SPH) [32] and Locality Sensitivity Hashing (LSH) [15].", "startOffset": 237, "endOffset": 241}, {"referenceID": 14, "context": "Then, we use our proposed approach to evaluate attribute meaningfulness on the attribute sets generated from various automatic attribute discovery methods such as PiCoDeS [1] as well as the hashing methods such as Spectral Hashing (SPH) [32] and Locality Sensitivity Hashing (LSH) [15].", "startOffset": 281, "endOffset": 285}, {"referenceID": 8, "context": "For this case, two datasets will be utilized: (1) a-Pascal a-Yahoo dataset (ApAy) [9]; (2) SUN Attribute dataset (ASUN) [21].", "startOffset": 82, "endOffset": 85}, {"referenceID": 20, "context": "For this case, two datasets will be utilized: (1) a-Pascal a-Yahoo dataset (ApAy) [9]; (2) SUN Attribute dataset (ASUN) [21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 7, "context": "In this setting, we utilize the UT Tower aerial view dataset (UTTower) [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "a-Pascal a-Yahoo dataset (ApAy) [9] \u2014 comprises two sources: a-Pascal and a-Yahoo.", "startOffset": 32, "endOffset": 35}, {"referenceID": 20, "context": "SUN Attribute dataset (ASUN) [21] \u2014 ASUN is a finegrained scene classification dataset consisting of 717 categories (20 images per category) and 14,340 images in total with 102 attributes.", "startOffset": 29, "endOffset": 33}, {"referenceID": 33, "context": "There are four types of features provided in this dataset: (1) GIST; (2) HOG; (3) selfsimilarity and (4) geometric context color histograms (See [34] for feature and kernel details).", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "For the first experiment, we apply the following preprocessing described in [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 29, "context": "We first lift each feature into a higher-dimensional space approximating the histogram intersection kernel by using the explicit feature maps proposed by Vedaldi and Zisserman [30].", "startOffset": 176, "endOffset": 180}, {"referenceID": 0, "context": "This effectively allows us to apply linear classifiers in the explicit kernel space [1].", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "UT Tower aerial view activity classification dataset (UTTower) [8] \u2014 consists of 108 low-resolution video sequences from 9 types of actions.", "startOffset": 63, "endOffset": 66}, {"referenceID": 35, "context": "For the second experiment, we use manifold feature proposed in [36] to extract visual information from the surveillance videos in the dataset.", "startOffset": 63, "endOffset": 67}], "year": 2016, "abstractText": "Automatic video keyword generation is one of the key ingredients in reducing the burden of security officers in analyzing surveillance videos.", "creator": "LaTeX with hyperref package"}}}