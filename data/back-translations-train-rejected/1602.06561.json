{"id": "1602.06561", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2016", "title": "Deep Learning in Finance", "abstract": "We explore the use of deep learning hierarchical models for problems in financial prediction and classification. Financial prediction problems -- such as those presented in designing and pricing securities, constructing portfolios, and risk management -- often involve large data sets with complex data interactions that currently are difficult or impossible to specify in a full economic model. Applying deep learning methods to these problems can produce more useful results than standard methods in finance. In particular, deep learning can detect and exploit interactions in the data that are, at least currently, invisible to any existing financial economic theory.", "histories": [["v1", "Sun, 21 Feb 2016 18:19:56 GMT  (476kb,D)", "http://arxiv.org/abs/1602.06561v1", "20 Pages, 5 Figures"], ["v2", "Tue, 23 Feb 2016 19:14:51 GMT  (475kb,D)", "http://arxiv.org/abs/1602.06561v2", "20 Pages, 5 Figures"]], "COMMENTS": "20 Pages, 5 Figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["j b heaton", "n g polson", "j h witte"], "accepted": false, "id": "1602.06561"}, "pdf": {"name": "1602.06561.pdf", "metadata": {"source": "CRF", "title": "Deep Learning in Finance", "authors": ["J. B. Heaton", "N. G. Polson", "J. H. Witte"], "emails": [], "sections": [{"heading": null, "text": "Financial prediction problems - such as those encountered in the design and pricing of securities, portfolio creation and risk management - often involve large sets of data with complex data interactions that are currently difficult or impossible to specify in a complete economic model. Applying deep learning methods to these problems can lead to more useful results than standard methods in finance. In particular, deep learning can detect and exploit interactions in the data that are, at least at present, invisible to any existing financial theory.Keywords: deep learning, machine learning, big data, artificial intelligence, LSTM models, finance, asset pricing, volatility. Bartlit Beck Herman Palenchar & Scott LLP and GreyMaths Inc. \u2020 Booth School of Business, University of Chicago, and GreyMaths Inc., University of Oxford, and GreyMaths Inc.ar Xiv: 160 2.06 56cv s.1L [21 Feb]"}, {"heading": "1 Introduction", "text": "Financial prediction problems are of great practical and theoretical interest, and they are also quite discouraging. Theory suggests that much of the information relevant to financial prediction problems can be disseminated through available economic and other data, an idea also supported by the many different data sources that different market participants look for clues to future price movements. Dealing with this variety of data sources is difficult, and the collection of potentially relevant data is very extensive, while the significance of the data and the potentially complex nonlinear interactions in the data are not well specified by financial economics theory. In practice, this leads to a wealth of prediction models, many of which have little theoretical justification and are subject to excessive and poor prediction performance from the sample. What is required is a method capable of learning these complex features of data inputs that can lead to good predictions of target production variables (such as an investment model or a portfolio problem) that are easily incorporated into the traditional prediction variabled.3."}, {"heading": "2 Deep Learning", "text": "We start with the introduction of the general theoretical deep learning framework as well as several specifications."}, {"heading": "2.1 Architecture", "text": "Basic machine learning is a form of machine learning. Machine learning uses data to learn a model, and then uses the trained model to make predictions from new data. Basic machine learning problem is to find a predictor for an output Y = F (X) in which the input space is high dimensional, and we write a model in which X = (X1,.) and a predictor is called input-output mapping Y = F (X). Output T can be continuous, discrete as in classification, or mixed. For example, in a classification problem we must learn a figure F: X \u2192 Y where Y {1,.,., K} indexes categories.as a form of machine learning to make deep traces a model on data, but is differentiated by passing learned characteristics of data through different levels of abstraction."}, {"heading": "2.2 Training a Deep Architecture", "text": "Building a deep learner requires a series of steps. It is common to divide the data set into three subsets, namely training, validation, and testing; the training set is used to adjust the weights of the network; the validation set is used to minimize overmatch; and refers to the architecture design (also known as model selection); and finally, the test is used to confirm a learner's actual predictive power. Once the activation features, size, and depth of the learning routine have been selected, we need to solve the training problem of finding a learner."}, {"heading": "W\u0302 = (W\u03020, . . . , W\u0302L) and b\u0302 = (b\u03020, . . . , b\u0302L)", "text": "To do this, we need a training dataset D = {Y (i), X (i)} Ti = 1 of the input-output pairs and a loss function L (Y, Y) at the level of the output signal. In its simplest form, we solve minW, b 1T, i = 1 L (Yi, Y, W, b (Xi)). (1) Frequently, the L2 standard is chosen as the error measure for a traditional problem with the fewest squares, and when we then weaken the loss function L (Yi, Y, Xi), our target function (1) becomes the mean square error (MSE) via the training datasets D = {Y (i), X (i), Y (i), Y standard, the arithmetic task."}, {"heading": "2.2.1 Probabilistic Interpretation", "text": "In a traditional probability setting, we could consider output Y as a random variable generated by a probability model p (Y | Y W, b (X)), in which the conditioning is on the predictor Y (X). The corresponding loss function is then L (Y, Y) = \u2212 log p (Y | Y W, b (X))), namely the negative log probability. If, for example, we predict the probability of a default, we have a multinomial logistic regression model that leads to a cross-entropy loss function. In multivariate normal models in particular (which include many financial time series), the L2 standard becomes a suitable error measurement. As a matter of probability, the regulatory term (W, b) can be regarded as a negative log prior distribution over parameters, namely p \u2212 log p (W, b), p \u2212 eridence (W, Bayerib), p (W, b)."}, {"heading": "2.2.2 Cross Validation", "text": "Cross-validation is a technique by which we divide our training data into complementary subsets, and then perform analysis and validation on different sets, with the aim of reducing overfits and increasing performance outside the sample. In particular, if we train our training data in time series, we can split our training data into separate periods of equal length, which is particularly desirable in financial applications where reliable time-consistent predictors are difficult to obtain and require extensive training and testing. Cross-validation also provides a tool to determine what degree of regulation leads to a good generalization (i.e. prediction), which is the classic compromise of variance distortion. A major advantage of cross-validation (compared to conventional statistical indicators such as T ratios and p-values) is that it also allows us to assess the size and depth of the hidden layers, i.e., to resolve the problem of model selection by selecting this model ability and NL for pragmatic estimation problems."}, {"heading": "2.2.3 Back-propagation", "text": "The common numerical approach to solving (2) is a form of stochastic gradient descent that adapts to a deep learning environment and is commonly referred to as backpropagation. A limitation of backpropagation in this context is the multimodality of the system to be solved (and the resulting slow convergence properties), which is the main reason why deep learning methods depend heavily on the availability of large computing power. One of the advantages of using a deep network is that derived first-order information is directly available. Tensor libraries are available that directly calculate this optimization using the chain rule over the entire training dataset. For ultra-large datasets, we use mini-batches and stochastic gradient declines (SGD) to perform this optimization, see LeCun et al. (2012) An active area of research is the use of this information within a customized MCMC algorithm, which enables a high-grade gradient ascent (SD)."}, {"heading": "2.3 Predictive Performance", "text": "There are two key training problems that can be addressed using the predictive power of an architecture. (i) How much regulation is to be added to the loss function? (i) As already indicated, one approach is to use cross-validation and teach the algorithm to calibrate itself on a training data. (ii) An independent hold-out data set is kept separately to perform an out-of-sample measurement of training success in a second step. (ii) A more difficult problem is to train the size and depth of each layer of the architecture, i.e. to select the level of regulation to optimize out-of-sample loss. Another approach is to use Stein's unbiased risk estimator (SURE). (ii) A more difficult problem is to determine the size and depth of each layer of the architecture, i.e., L and N = (N1, NL). This is known as a selection problem."}, {"heading": "2.4 Dropout for Model Selection", "text": "In a simple model with a hidden layer we replace the networkY (l) i = f (l)? X (l) i = W (l) i (l) i (l) i, with the dropout architecturD (l) i (l) i, Y (l) i = F (l) i (l) i, Z (l) i = W (l) i (l) i (l) i (l) i, B (l) i (l) i."}, {"heading": "2.5 Auto-encoder", "text": "\"For the first time in my life, I have been able to do this for the first time in my life, and I have been able to do it for the first time. I have been able to do it for the first time in my life, and I have been able to do it for the second time. I have been able to do it for the first time in my life, and I have been able to do it for the second time. I have been able to do it for the first time in my life, and I have been able to do it for the last two years, and I have been able to do it for the first time, and I have been able to do it for the second time, and I have been able to do it for the first time, and I have been able to do it for the second time, and I have been able to do it for the first time, and I have been able to do it for the first time.\""}, {"heading": "2.6 Long Short Term Memory Models (LSTMs)", "text": "It is the first time that in a country where most people are able to put themselves in another world, in which they are able to put themselves in another world, in which they are able to create a new world, in which they are able to create a new world, in which they are able to create a new world, in which they are able to create a new world, in which they are able to create a new world, in which they are able to create a new world, in which they are able to create a new world, in which they are able to create a new world, in which they are able to create a new world, in which they are able to create a new world, in which they are able to create a new world, in which they are able to create a new world, in which they create a new world, in which they want to create a new world, in which they want to create a new world, in which they want to create a new world, in which they want to create a new world, in which they want to create a new world, in which they want to create a new world, in which they want to create a new world, in which they want to create a new world, in which they want to create a new world, in which they want to create a new world, in which they want to create a new world in which they want to create a new world, in which they want to create a new world, in which they want to create a new world in which they want to create a new world, in which they want to create a new world in which they want to create a new world, in which they want to create a new world, in which they want to create a new, in which they want to create a new world in which they want to create a new world, in which they want to create a new world in which they want to create a new world in which they want to create a new world, in which they want to create a new world in which they want to create a new world, in which they want to create a new, in which they want to create a new world in which they want to create a new world in which they want to create a new, in which they want to create a new world in which they want to create a new world in which they want to create a new world, in which they want to create a new world in which they want to create they want to create a new"}, {"heading": "3 Finance Applications", "text": "For financial applications, see Fama and French (1992, 2008), Engle (1982), Campbell, Lo and MacKinley (1997), Singleton (2006), and Daniel and Titman (2006). Hutchison, Lo and Poggio (1994) offer a shallow learner for option pricing."}, {"heading": "3.1 Deep Factor Models versus Shallow Factor Models", "text": "Almost all shallow data reduction techniques can be considered to consist of a low-dimensional auxiliary variable Z = > a prediction rule defined by a set of functions. Y = fW1, b11 (W2X + b2)) = fW1, b11 (Z), where Z: = f2 (W2X + b2).In this formulation we also recognize the previously introduced deep learning structure (2,1).The problem of high-dimensional data reduction in general is to find the Z variable and correctly estimate the level functions (f1, f2).In the layers we want to uncover the low-dimensional Z structure in a way that does not disregard information about the production Y. Principal component analysis (PCA), reduced rank regression (RRRR), linear discrimination analysis (LDA), project tracking regression (PPR), and logistic regression are all learners."}, {"heading": "3.2 Default Probabilities", "text": "Another area of application of deep learning is the credit risk analysis. The aim of a deep learning model is the representation of a high-dimensional input area (max.). In image processing, one can imagine that the layers represent objects first, then parts of objects (faces), then edges and finally pixels. A similar function card can be found for the creditworthiness of companies (max.). We can combine the return data of financial investments with text data (income calls) and accounting data (book values, etc.) to get a picture of the health of a company. Specifically, we assume that our observations Yi represent a multi-class 1-of-K indicator vector. We set classes using Y = k for 1 \u2264 k \u2264 k \u2264 K. In extreme cases, the K classes could correspond with bond gradations.In extreme cases, we could have an indicator Y, the Yi value (1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,"}, {"heading": "3.3 Event Studies", "text": "We have a high-dimensional input / covariable given by X = {Xt} Tt = 1 and Xt-RN \u00b7 M. We can build a deep learner for analyzing events as follows: If we embed a series of input events in X = (X1,.., Xn), we can use a weight matrix W1-Rl to extract the l-possible events, for example l = 4 for profit announcements during the year and n = 252 for the number of trading days. We are now constructing a hidden factorZj = W > 1 Xj \u2212 l + 1 so that we can measure the effect of l previous events on today's return. We could use an activation function for maximum merge if we believe that the effect is based solely on the largest value (i.e. maxZj), in which case the model disregards the largest and all other factors."}, {"heading": "4 Example: Smart Indexing", "text": "In fact, the situation is that most people are able to survive on their own and that they are able to survive on their own. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "5 Conclusion", "text": "Deep learning provides a general framework for the use of large data sets to optimize predictive power. Deep learning is therefore well suited to many financial problems, both practical and theoretical. Our example of smart indexing in Section 4 is just one way to implement deep learning models in finance, and many other applications remain for development. At the same time, deep learning is likely to present significant challenges to current thinking in finance, particularly the concept of market efficiency. As it can model complex nonlinearities in the data, deep learning may be able to value assets within arbitrarily small price errors. Will this mean that markets are informationally efficient, or will new tests of market efficiency be needed? Overall, theoretical models built on existing axiomatic foundations are unlikely to compete with the predictive power of models."}], "references": [{"title": "Statistical modeling: the two cultures (with comments and a rejoinder by the author)", "author": ["L. Breiman"], "venue": "Statistical Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "A", "author": ["J.Y. Campbell"], "venue": "W. Lo and A. C. MacKinley: The econonmetrics of financial markets. Princeton University Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Titman: Market reactions to tangible and intangible information", "author": ["S.K. Daniel"], "venue": "Journal of Finance,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "R", "author": ["J. Dean", "G. Corrado"], "venue": "Monga, et al.: Large scale distributed deep networks, Advances in Neural Information Processing Systems, pp. 1223-1231", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "On non-linear functions of linear combinations", "author": ["P. Diaconis", "M. Shahshahani"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1984}, {"title": "Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation, Econometrika", "author": ["R. Engle"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1982}, {"title": "The cross-section of expected stock returns", "author": ["E.F. Fama", "K.R. French"], "venue": "Journal of Finance,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "There exists a neural network that does not make avoidable mistakes", "author": ["A.R. Gallant", "H. White"], "venue": "IEEE International Conference on Neural Networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1988}, {"title": "and J", "author": ["T. Hastie", "R. Tibshirani"], "venue": "Friedman: The elements of statistical learning, Vol 2", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, Vol", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "and H", "author": ["K. Hornik", "M. Stinchcombe"], "venue": "White: Multilayer feedforward networks are universal 19 approximators, Neural networks, Vol. 2(5), pp. 359-366", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1989}, {"title": "and T", "author": ["J.M. Hutchinson", "A.W. Lo"], "venue": "Poggio: A Nonparametric approach to pricing and hedging derivative securities via learning networks, Journal of Finance, Vol. 48(3), pp. 851-889", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1994}, {"title": "The representation of continuous functions of many variables by superposition of continuous functions of one variable and addition, Dokl", "author": ["A. Kolmogorov"], "venue": "Akad. Nauk SSSR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1957}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "Science, Vol. 3560,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "G", "author": ["Y.A. LeCun", "L. Bottou"], "venue": "B. Orr, and K.-R. Muller: Efficient backprop, Neural networks: Tricks of the trade, pp. 948", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian Model Assessment in Factor Analysis", "author": ["H.F. Lopes", "M. West"], "venue": "Statistica Sinica,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Lorentz: The 13th problem of Hilbert", "author": ["G. G"], "venue": "Proceedings of Symposia in Pure Mathematics, American Mathematical Society,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1976}, {"title": "Girosi: Networks for approximation and learning", "author": ["F.T. Poggio"], "venue": "Proceedings of the IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1990}, {"title": "and B", "author": ["N.G. Polson", "J.G. Scott"], "venue": "T. Willard: Proximal algorithms in statistics and machine learning, Statistical Science, 30, 559-581", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "and M", "author": ["N.G. Polson", "B.T. Willard"], "venue": "Heidari: A statistical theory for Deep Learning", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Ripley: Pattern recognition and neural networks", "author": ["D. B"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1996}, {"title": "Singelton: Empirical Dynamic Asset Pricing", "author": ["J. K"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Causal inference from observational data: a review of end and means, Journal of the Royal Statistical Society, Series A (General)", "author": ["H. Wold"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1956}], "referenceMentions": [], "year": 2016, "abstractText": "We explore the use of deep learning hierarchical models for problems in financial prediction and classification. Financial prediction problems \u2013 such as those presented in designing and pricing securities, constructing portfolios, and risk management \u2013 often involve large data sets with complex data interactions that currently are difficult or impossible to specify in a full economic model. Applying deep learning methods to these problems can produce more useful results than standard methods in finance. In particular, deep learning can detect and exploit interactions in the data that are, at least currently, invisible to any existing financial economic theory.", "creator": "LaTeX with hyperref package"}}}