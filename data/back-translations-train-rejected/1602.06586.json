{"id": "1602.06586", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2016", "title": "Recovering Structured Probability Matrices", "abstract": "We consider the problem of accurately recovering a matrix B of size M by M , which represents a probability distribution over M^2 outcomes, given access to an observed matrix of \"counts\" generated by taking independent samples from the distribution B. How can structural properties of the underlying matrix B be leveraged to yield computationally efficient and information theoretically optimal reconstruction algorithms? When can accurate reconstruction be accomplished in the sparse data regime? This basic problem lies at the core of a number of questions that are currently being considered by different communities, including community detection in sparse random graphs, learning structured models such as topic models or hidden Markov models, and the efforts from the natural language processing community to compute \"word embeddings\".", "histories": [["v1", "Sun, 21 Feb 2016 21:47:45 GMT  (542kb,D)", "http://arxiv.org/abs/1602.06586v1", null], ["v2", "Mon, 29 Feb 2016 22:20:50 GMT  (542kb,D)", "http://arxiv.org/abs/1602.06586v2", null], ["v3", "Sat, 9 Apr 2016 06:59:37 GMT  (518kb,D)", "http://arxiv.org/abs/1602.06586v3", null], ["v4", "Fri, 15 Apr 2016 04:58:58 GMT  (518kb,D)", "http://arxiv.org/abs/1602.06586v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qingqing huang", "sham m kakade", "weihao kong", "gregory valiant"], "accepted": false, "id": "1602.06586"}, "pdf": {"name": "1602.06586.pdf", "metadata": {"source": "CRF", "title": "Recovering Structured Probability Matrices", "authors": ["Qingqing Huang", "Sham M. Kakade", "Weihao Kong", "Gregory Valiant"], "emails": ["qqh@mit.edu.", "sham@cs.washington.edu", "kweihao@gmail.com", "valiant@stanford.edu."], "sections": [{"heading": null, "text": "Our results relate to the setting in which B has a particular Rank 2 structure. To this setting, we propose an efficient (and practicable) algorithm that accurately restores the underlying M \u00d7 M matrix using Area Samples. This result can easily be applied to Area Sample Algorithms to learn two-topic topic topic models about Size M dictionaries and hidden Markov models with two hidden states and observational distributions based on M elements. These linear sample complexities are optimal to the point of constant factors in an extremely strong sense: even testing basic properties of the underlying matrix (such as whether it has Rank 1 or 2) requires Area Samples. In addition, we offer an even stronger lower limit distinguishing whether a sequence of observations has been drawn from the uniform distribution via M observations or is generated by an HMM observation."}, {"heading": "1 Introduction", "text": "This year, it has come to the point where it is only a matter of time before a solution is found, in which a solution is found."}, {"heading": "1.1 Problem Formulation", "text": "Suppose our index is the sentence M = {1,., M} of the M-words and that there is an underlying low probability matrix B, size M \u00b7 M, with the following structure: B = DWD >, where D = [p, q]. (1) Here, D = RM \u00b7 2 + is the probability matrix supported by two M-dimensional probability vectors p, q, when used on the standard (M \u2212 1) -simplex. Also W is the 2 \u2022 2 mixed matrix, which is a satisfactory matrix that can be used by two M-dimensional probability vectors p, q \u00b2, which is supported on the standard (M \u2212 1) -simplex. Note that we define the k Bi, k = wpp + wqq. Define the covariance matrix of any probability P as: [Cov (P)] i, j = W1,2 + W2."}, {"heading": "1.2 Main Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.2.1 Recovering Rank-2 Probability Matrices", "text": "The separation assumptions guarantee that the rank 2 matrix B is well conditioned. Furthermore, this assumption also has natural interpretations in each of the various application problems (up to that of community probability, theme modeling, and HMMs).All notations of our order are in relation to the vocabulary size M, which is asymptotically large. We also say that a statement is \"highly probable\" if the error probability of the statement is inverse poly in M; and we say that a statement is true \"with great probability\" if the error probability of some small constant B, which can easily be increased to very small probabilities with repetitions. Assumption 1 (1): We assume that W is symmetrical where wL = wR = w (all our results extend to the asymmetrical case)."}, {"heading": "1.2.2 Topic Models and Hidden Markov Models", "text": "One of the main motivations for considering the specific structure 2 on the underlying matrix B is that this structure includes the structure of the matrix of expected bigrams generated by both 2-topic models and two state HMMs. We now make these connections explicitly. The process of drawing a bigram (i, j) consists of the first randomized selection of one of the two \"topics\" models according to the mixture weights and the drawing of two independent words from the word distribution corresponding to the chosen topic."}, {"heading": "1.2.3 Testing vs. Learning", "text": "The above theories and corrections are in an extremely strong sense. They are suitable for the topic model as well as for the HMM settings."}, {"heading": "1.3 Related Work", "text": "As already mentioned, the general problem of reconstructing an underlying matrix of probabilities given access to a number matrix drawn according to the corresponding distribution is at the core of questions actively pursued by several different communities. We briefly describe these questions and their relationship to current work. Community Detection. With the increasing prevalence of large social networks, there has been a flood of activity models from the algorithms and probability communities that both models have structured random graphs and understand how (and when it is possible) to examine a graph and deduce the underlying structures that may have led to the observed graph. One of the best studied community models is the stochastic block model [27], which parameterizes this model by a number of individuals, M, and two probabilities. The model postulates that the M individuals are divided into two equally large \"communities,\" and such a random model is the following community of each of them: the pair of individuals is the same."}, {"heading": "2 Outline of our estimation algorithm", "text": "This year, the number of ageing and ageing people has plummeted in comparison with the ageing people in comparison with the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people in relation to the ageing people."}, {"heading": "3 Algorithm Phase I, achieving constant 0 accuracy", "text": "In this section, we outline the evidence for the marginal probability. (5) In our discussion of the Phase I algorithms, we assume that d0 = 0 = 0 (1) will be a fixed big constant. (5) In our discussion of the Phase I algorithms, we assume that d0 = 1 (1) will be a fixed big constant. (5) In the analysis of the underlying probability matrix B with B (2) a marginal vector ratification as well as the dictatorial separation vector ratification up to constant accuracy in \"1 norm.\" We denote the estimates of the underlying probability matrix B with B (1) > + 3 (1) a large probability in \".1) we can estimate the constant 1 (1) standard accuracy in\" 1 norm. \"We show that we can also show the constant accuracy of B (1) in\" 1 norm \"a large probability that it is easy to estimate the marginal."}, {"heading": "3.1 Binning according to empirical marginal distribution", "text": "Instead of dealing with the empirical count, I focused on its diagonal and analyzed the spectral concentrations that are limited to the individual areas. (...) We have to focus on the fact that we can estimate the individual areas of the separation vector. (...) We have to be prepared for the diagonal-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector"}, {"heading": "3.2 Estimate \u2206 restricted to the heaviest empirical bin", "text": "Firstly, we are showing that the empirical marginalisation that we are able to hide, to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be in the position in which they are able to be in, in the position in which they are able to be in, in the position in which they are able to be in, in the position in which they are able to be in, in the position in which they are able to be in."}, {"heading": "3.4 Estimate \u2206 restricted to the lightest empirical bin.", "text": "Claim 3.7 (Estimated separation limited to the lightest trash can): Setting the value \"I\" 0 = 0 only results in a \"1 error of a small constant,\" simply because the total number of words in the lightest trash can most likely be limited by a small constant: \"I\" 0 \"1\" 0 \"0\" 1 \"0\" 1 \"0\" 1 \"0\" 1 \"1\" 0 \"1\" W 0 \"0\" M + e \"\u2212 d0\" / 2 \"= O (0), using the assumption that d0\" 1 / 40. \""}, {"heading": "3.5 Stitching the segments of \u2206\u0302 to reconstruct an estimation of the matrix", "text": "Given vk for all k as an estimate for \"I + k\" s to \"flips.\" Fix k \"to a single good bin (with large bin marginal and large separation). Divide the words into two groups I + k\" s to \"flips.\" For all other good bins k \"we define I + k and I \u2212 k. The next assertion shows how to define the relative character Flip from vk\" and vk.Claim 3.8 (pairs of containers for fixing flips characters). For all other good bins k \"K\" we define similarly I + k and I \u2212 k. The next assertion shows how to compare the relative character Flip from vk \"and vk.Claim of containers for fixing flips characters. For all good bins k\" G, \"we can define the character Flip from\" and I \u2212 k."}, {"heading": "4 Algorithm Phase II, achieving arbitrary accuracy", "text": "Under the assumptions for separating the two groups, we refine the estimate to achieve arbitrary accuracy in phase II. In this section, we review the steps of algorithm 2 and show the correctness of theorem 2.2."}, {"heading": "4.1 Construct an anchor partition", "text": "Imagine we have a way to integrate the M-words into a new vocabulary that has a constant number of superwords, and similarly we define the marginal vector A and the delimiter vector A over the superwords. The new probability matrix (constant size) then corresponds to our sum over the rows / columns of matrix B. If we group the words so that \"A\" is still defined over the superwords, then we can estimate the constant vectors of matrix A and A over the arbitrary accuracy (as M 1). Note: Accurate estimates of \"A\" and \"A\" give us rough \"global information about the true\" and \"I.\" Now we add the empirical BN over the lines accordingly and leave the columns intact, it is easy to see that the expected factorization is given by us."}, {"heading": "4.2 Estimate the anchor matrix", "text": "Let us now consider the grouping of the words into two superwords according to the anchor partition we constructed. To estimate the anchor matrix, we need only estimate two scalar variables precisely: DA = [\u03c1A, \u0445 A 1 \u2212 \u03c1A] as anchor matrix. If we apply the standard concentration limit, we can argue that we have a high probability of using the two scalar variables \u03c1A and \u0432 A [BN] i, j \u00b2 A [BN] i, j \u00b2 i [BN] i, j \u00b2 i, j \u00b2 Ac [BN] i, j \u00b2 i, j \u00b2 i, j \u00b2 Ac [BN] i \u2212 DAD > A \u00b2 < O (1 \u00b0 N). In addition, we have an anchor partition because (A, Ac) is an anchor partition."}, {"heading": "4.3 Use anchor matrix to estimate dictionary", "text": "Given an anchor partition of the vocabulary (A, Ac) and given the exact anchor matrix DA, which has the condition number 1, refining the estimate of 2i and 3i for each i is very simple and achieves an optimal rate.Lemma 4.6 (Estimate 1 and 2 in '2 distance).We have this with a probability of at least 1 \u2212 3, 3 \u2212 4, 5 \u2212 5, 5 \u2212 5, 5 \u2212 5 < 5 \u00b2 / 5 (Estimate 2 and 5 in' 1 distance).With a high probability, we can estimate 1 and 1 in 1 distance: 1, 1 < 5, 5, 5, 1 < 5, 1 < 5, 1 < 5, 1 < 5."}, {"heading": "5 Sample complexity lower bounds for estimation VS testing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Lower bound for estimating probabilities", "text": "We reduce the estimation problem to the common recognition for a certain set of model parameters. Let us consider the following theme model with equal mixed weights, i.e. w = wc = 1 / 2. For a certain constant C = 1 = 1) the two word distributions are divided by: p = [1 + C \u00b2 M,.., 1 + C \u00b2 M, 1 \u2212 C \u00b2 M,.., 1 \u2212 C \u00b2 M], q = [1 \u2212 C \u00b2 M,.., 1 \u2212 C \u00b2 M, 1 + C \u00b2 M,.,.. (1 + C \u00b2 M).The expectation of the sum of the samples is determined by E [BN] = N 2 (pp > + qq >) = N M2 [1 + C2 = 1 \u2212 C2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "5.2 Lower bound for testing property of HMMs", "text": "In this section, we analyze the theoretical lower limit to accomplish the task of testing whether a sequence of observations is actually generated by a 2-state HMM = 1. This problem is closely related to the problem of the higher probability (given in our general setup in the main text) of whether the underlying probability matrix of the observed sample is actually of rank 1 or rank 2. Note that in the context of HMM, this would be a stronger lower limit, since we allow an estimator to have more information about the sequence of successive observations, rather than just bigram numbers. Theorem 5.1 (restored to Theorem 1.6) Consider a sequence of N observations from a HMM with two states {+, \u2212} and emission distributions p, which are based on M elements. For asymptotically large M, using a sequence of N = O (M) observations, it is theoretically impossible to distinguish the case that the two emission distributions are separate from each other."}, {"heading": "A Proofs for Algorithm Phase I (Section 3.1, 3.2, 3.4)", "text": "The Evidence. (to Lemma 3.1 \u2212 \u2212 \u2212 g) We analyze how accurate the empirical average probability is. (to Lemma 3.1 \u2212 \u2212 g) We analyze how accurate the empirical average probability is. (to Lemma 3.1 \u2212 g) We analyze how accurate the empirical average probability is. (to Lemma 3.1 \u2212 g) We analyze how accurate the empirical average probability is. (to Lemma 3.1 \u2212 g) We analyze how accurate the empirical average probability is. (c) We analyze how high the empirical probability is. (c) We analyze how high the empirical probability is. (c) We analyze how high the empirical probability is. (c) We analyze the empirical average probability."}, {"heading": "B Proofs for Algorithm Phase I (Section 3.3)", "text": "Proof. (to Lemma 3.4 (Spillover from much heavy bins is small in all bins) < < p > p > p > p > p > p > p > p > p > p > p > p > p < p > p > p (n) p > p > p > p (n) p > p > p > p (n) p > p < p > p < p (n) p > p > p (n) p > p (n) p (n) p < p (n) p < p (n) p (n < p) p (n < p) p (n < p) p (p) p (p) p; p (n < p) p (p) p) p (p) p (p) p (n < p) p (p) p (n < p) p (p) p (n < p) p (n < p) p (p) p (n < p) p (p) p (n < p) p (n < p) p (n) p (p) p (n < p) p (p) p (n < p) p (p) p (p) p (n < p) p (p) p (n < p) p (p) p (p) p) p (p) p (p) p (p) p (p) p (p) p) p (p) p (p) p) p (p) p (p) p) p (p) p) p (n < p) p) p (p) p (p) p) p (p) p) p (p) p (p) p) p (p) p) p) p (p) p) p (p) p) p) p (p) p) p) p (n &ltn < p) p) p) p (p) p) p) p (p) p) p (p) p) p) p (p) p) p (p) p) p) p (p) p) p) p (p) p)"}, {"heading": "C Proofs for Algorithm Phase II (Section 4)", "text": "The Evidence. (to Lemma 4.2 (Sufficient condition for building an anchor partition)) (1) First, we show that if for any constant c = 1), a set of words A = 1 (1), a set of words A = 1 (1), a set of words A = (1), (40), then (A, [M]\\ A), a set of words A = 1 (1), a set of words A = 1 (1), a set of words A = (1), a (1), a (2), a (2), a (2), a (2), a (2), a (2), a (2), a (2), a (2), a (2), a (2), a (2), a (2), a (2), a (2), a (2), a (2), a (2), a (2), a, a (2), a (2), a, a (2), a (2), a, a (2), a (2), a, a (2), a, a (2), a (2), a, a (2), a, a (2), a, a (2), a, a (1, a (2), a, a (2)."}, {"heading": "D Proofs for HMM testing lower bound (Section 5.2)", "text": "Proof. (to Theorem 5.1) TV (Pr1 (G N 1), Pr2 (G N 1), Pr2 (G N 1), Pr1 (GN1, A), Pr2 (GN1, A), Pr2 (GN1, A), Pr1 (GN1, A), Pr1 (GN1, A), Pr1 (GN1, A), Pr1 (M), Pr1 (M), Pr1 (M), Pr1 (M), Pr1 (M), Pr1 (G), Pr2 (G), Pr2 (G), Pr2 (G), Pr2 (G), Pr2 (G), Pr2 (G), Pr2 (G), Pr2 (G), Pr2 (G), Pr2 (G), Pr2 (G), G (G), G (G), G (G), G (G), G (G, G), G (G), G (G), G (G), G (G, G), G (G), G (G (G), G (G), G (G, G (G), G (G), G (G, G (G), G (G), G (G (G), G (G, G, G), Pr2 (G (G), Pr2 (G), Pr2 (G (G), Pr2 (G), Pr2 (G), Pr2 (G (G), Pr2 (G), Pr2 (G (G), Pr2 (G), Pr2 (G (G), Pr2 (G), Pr2 (A (G), Pr2 (G), Pr2 (G), Pr2 (G (G), Pr2 (G), Pr2 (G (G), Pr1 (G), Pr1 (G), Pr1 (G (G), Pr2 (G), Pr1 (G), Pr2 (G), Pr1 (G (G), Pr1 (G), Pr1 (G), Pr1 (G), Pr1 (G), Pr1 (G), Pr1 (G), Pr"}, {"heading": "E Analyze truncated SVD", "text": "The reason why the truncated SVD does not focus on the optimal rate is as follows: What the truncated SVD actually optimizes is the spectral distance from the estimator to the empirical average (minimizing B-2NBN), but not to the expected matrix B. It is only \"optimal\" in a very special constellation, for example, when (1NBN -B) the probability of occurrence i.e. Gaussian. In the asymptotic regime it is actually true that under mild conditions all sampling noise converges to i.d Gaussian. However, in the sparse regime in which N = 2 (M), the sampling noise from the probability matrix is very different from the additive Gaussian noise.Claim E.1 (Truncated SVD has sample complexity super linear)."}, {"heading": "F Auxiliary Lemmas", "text": "Lemma F. 1 (Wedin's theorem applied to rank-1 matrix) = < Denote symmetrical matrix X = vv > + E. Let v \u00b2 v \u00b2 > rank-1 = truncated SVD of X. There is a positive universal constant C like this. < C \u00b2 E (F \u2212 V + V \u00b2) \u2264 {C \u00b2 E \u00b2 if ranking-1 > C \u00b2 E \u00b2 if ranking-2 < C \u00b2 E \u00b2.Lemma F. 2 (Chernoff Bound for Poisson variables). Pr (Poi (f \u2212 E) n \u00b2 e \u00b2, for x \u2212 x \u00b2 E, for x \u00b2 \u00b2 \u00b2, Pr \u00b2 E \u00b2, for Pr \u00b2 \u00b2, for Pr \u00b2 s \u00b2, for Pr \u00b2 s \u00b2, for Pr \u00b2 s \u00b2, for ranking-1 < C \u00b2 E (Chernoff Bound for Poisson variables).Pr (Poi (Poi sp.)."}], "references": [{"title": "Exact recovery in the stochastic block model", "author": ["Emmanuel Abbe", "Afonso S Bandeira", "Georgina Hall"], "venue": "arXiv preprint arXiv:1405.3267,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Community detection in general stochastic block models: fundamental limits and efficient recovery algorithms", "author": ["Emmanuel Abbe", "Colin Sandon"], "venue": "arXiv preprint arXiv:1503.00609,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Competitive closeness testing", "author": ["J. Acharya", "H. Das", "A. Jafarpour", "A. Orlitsky", "S. Pan"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Competitive classification and closeness testing", "author": ["J. Acharya", "H. Das", "A. Jafarpour", "A. Orlitsky", "S. Pan"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "A spectral algorithm for latent dirichlet allocation", "author": ["Anima Anandkumar", "Yi kai Liu", "Daniel J. Hsu", "Dean P Foster", "Sham M Kakade"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M. Kakade", "Matus Telgarsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Computing a nonnegative matrix factorization\u2013provably", "author": ["Sanjeev Arora", "Rong Ge", "Ravindran Kannan", "Ankur Moitra"], "venue": "In Proceedings of the forty-fourth annual ACM symposium on Theory of computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Learning topic models\u2013going beyond svd", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "arXiv preprint arXiv:1502.03520,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "CoRR, abs/1502.03520,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Testing closeness of discrete distributions", "author": ["T. Batu", "L. Fortnow", "R. Rubinfeld", "W.D. Smith", "P. White"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Sublinear algorithms for testing monotone and unimodal distributions", "author": ["T. Batu", "R. Kumar", "R. Rubinfeld"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Smoothed analysis of tensor decompositions", "author": ["Aditya Bhaskara", "Moses Charikar", "Ankur Moitra", "Aravindan Vijayaraghavan"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Testing closeness with unequal sized samples", "author": ["B. Bhattacharya", "G. Valiant"], "venue": "In Neural Information Processing Systems (NIPS) (to appear),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Estimating a density under order restrictions: Nonasymptotic minimax risk", "author": ["L. Birge"], "venue": "Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1987}, {"title": "Full reconstruction of Markov models on evolutionary trees: Identifiability and consistency", "author": ["J.T. Chang"], "venue": "Mathematical Biosciences,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}, {"title": "Solving random quadratic systems of equations is nearly as easy as solving linear systems", "author": ["Yuxin Chen", "Emmanuel J Candes"], "venue": "arXiv preprint arXiv:1505.05114,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Stochastic block model and community detection in the sparse graphs: A spectral algorithm with optimal rate of recovery", "author": ["Peter Chin", "Anup Rao", "Van Vu"], "venue": "arXiv preprint arXiv:1501.05021,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "In Foundations of Computer Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Testing k-modal distributions: optimal algorithms via reductions", "author": ["C. Daskalakis", "I. Diakonikolas", "R. Servedio", "G. Valiant", "P. Valiant"], "venue": "In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Spectral techniques applied to sparse random graphs", "author": ["Uriel Feige", "Eran Ofek"], "venue": "Random Structures & Algorithms,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "On the second eigenvalue of random regular graphs", "author": ["Joel Friedman", "Jeff Kahn", "Endre Szemeredi"], "venue": "In Proceedings of the twenty-first annual ACM symposium on Theory of computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1989}, {"title": "Learning mixtures of gaussians in high dimensions", "author": ["Rong Ge", "Qingqing Huang", "Sham M. Kakade"], "venue": "In Proceedings of the Symposium on Theory of Computing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Upper bounds on poisson tail probabilities", "author": ["Peter W Glynn"], "venue": "Operations research letters,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1987}, {"title": "Streaming and sublinear approximation of entropy and information distances", "author": ["S. Guha", "A. McGregor", "S. Venkatasubramanian"], "venue": "In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Stochastic blockmodels: First steps", "author": ["Paul W Holland", "Kathryn Blackmond Laskey", "Samuel Leinhardt"], "venue": "Social networks,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1983}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M Kakade"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["Daniel Hsu", "Sham M Kakade", "Tong Zhang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Estimation of a discrete monotone density", "author": ["H.K. Jankowski", "J.A. Wellner"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Efficiently learning mixtures of two gaussians", "author": ["Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant"], "venue": "In Proceedings of the 42nd ACM symposium on Theory of computing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Spectral redemption in clustering sparse networks", "author": ["Florent Krzakala", "Cristopher Moore", "Elchanan Mossel", "Joe Neeman", "Allan Sly", "Lenka Zdeborov\u00e1", "Pan Zhang"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Sparse random graphs: regularization and concentration of the laplacian", "author": ["Can M Le", "Elizaveta Levina", "Roman Vershynin"], "venue": "arXiv preprint arXiv:1502.03049,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Concentration and regularization of random graphs", "author": ["Can M Le", "Roman Vershynin"], "venue": "arXiv preprint arXiv:1506.00669,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Community detection thresholds and the weak ramanujan property", "author": ["Laurent Massouli\u00e9"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Learning nonsingular phylogenies and hidden Markov models", "author": ["E. Mossel", "S. Roch"], "venue": "Annals of Applied Probability,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Stochastic block models and reconstruction", "author": ["Elchanan Mossel", "Joe Neeman", "Allan Sly"], "venue": "arXiv preprint arXiv:1202.1499,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "Consistency thresholds for binary symmetric block models", "author": ["Elchanan Mossel", "Joe Neeman", "Allan Sly"], "venue": "arXiv preprint arXiv:1407.1591,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Optimal algorithms for testing closeness of discrete distributions", "author": ["S. on Chan", "I. Diakonikolas", "G. Valiant", "P. Valiant"], "venue": "In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Estimating entropy on m bins given fewer than m samples", "author": ["L. Paninski"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2004}, {"title": "Strong lower bounds for approximating distribution support size and the distinct elements problem", "author": ["S. Raskhodnikova", "D. Ron", "A. Shpilka", "A. Smith"], "venue": "SIAM Journal on Computing,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["Karl Stratos", "Michael Collins", "Daniel Hsu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "A spectral algorithm for learning class-based n-gram models of natural language", "author": ["Karl Stratos", "Michael Collins Do-Kyum Kim", "Daniel Hsu"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2014}, {"title": "Estimating the unseen: an n/ log n-sample estimator for entropy and support size, shown optimal via new clts", "author": ["G. Valiant", "P. Valiant"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "The power of linear estimators", "author": ["G. Valiant", "P. Valiant"], "venue": "In Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}, {"title": "Estimating the unseen: improved estimators for entropy and other properties", "author": ["G. Valiant", "P. Valiant"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2013}, {"title": "An automatic inequality prover and instance optimal identity testing", "author": ["G. Valiant", "P. Valiant"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "A spectral algorithm for learning mixture models", "author": ["Santosh Vempala", "Grant Wang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2004}], "referenceMentions": [{"referenceID": 5, "context": "However, one can recover the model parameters using sampled trigram sequences; this last step is straightforward (and sample efficient as it uses only an additional \u03a9(1/ 2) trigrams) when given an accurate estimate of B (see [6] for the moment structure in the trigrams).", "startOffset": 225, "endOffset": 228}, {"referenceID": 40, "context": "In the case of 2-topic models, the community detection lower bounds [41][32][52] imply that \u0398(M) bigrams are necessary to even distinguish between the case that the underlying model is simply the uniform distribution over bigrams versus the case of a 2-topic model in which each topic corresponds to a uniform distribution over disjoint subsets of M/2 words.", "startOffset": 68, "endOffset": 72}, {"referenceID": 31, "context": "In the case of 2-topic models, the community detection lower bounds [41][32][52] imply that \u0398(M) bigrams are necessary to even distinguish between the case that the underlying model is simply the uniform distribution over bigrams versus the case of a 2-topic model in which each topic corresponds to a uniform distribution over disjoint subsets of M/2 words.", "startOffset": 72, "endOffset": 76}, {"referenceID": 26, "context": "One of the most well studied community models is the stochastic block model [27].", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "[22, 40, 32, 33]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 39, "context": "[22, 40, 32, 33]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 31, "context": "[22, 40, 32, 33]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 32, "context": "[22, 40, 32, 33]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 40, "context": "In the past year, for both the exact recovery problem and the detection problem, the exact tradeoffs between \u03b1, \u03b2, and M were established, down to subconstant factors [41, 1, 36].", "startOffset": 167, "endOffset": 178}, {"referenceID": 0, "context": "In the past year, for both the exact recovery problem and the detection problem, the exact tradeoffs between \u03b1, \u03b2, and M were established, down to subconstant factors [41, 1, 36].", "startOffset": 167, "endOffset": 178}, {"referenceID": 35, "context": "In the past year, for both the exact recovery problem and the detection problem, the exact tradeoffs between \u03b1, \u03b2, and M were established, down to subconstant factors [41, 1, 36].", "startOffset": 167, "endOffset": 178}, {"referenceID": 18, "context": "[19, 2]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 1, "context": "[19, 2]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 36, "context": "On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on \u201cword embeddings\u201d [37, 35, 46, 9].", "startOffset": 152, "endOffset": 167}, {"referenceID": 34, "context": "On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on \u201cword embeddings\u201d [37, 35, 46, 9].", "startOffset": 152, "endOffset": 167}, {"referenceID": 45, "context": "On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on \u201cword embeddings\u201d [37, 35, 46, 9].", "startOffset": 152, "endOffset": 167}, {"referenceID": 8, "context": "On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on \u201cword embeddings\u201d [37, 35, 46, 9].", "startOffset": 152, "endOffset": 167}, {"referenceID": 44, "context": "see [45])\u2014this is especially noticeable in the relatively poor quality of the embeddings for relatively rare words.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "The recent theoretical work [10] sheds some light on why current approaches are so successful, yet the following question largely remains: Is there a more accurate way to recover the best rank-d approximation of the underlying matrix than simply computing the best rank-d approximation for the (noisy) matrix of empirical counts? Efficient Algorithms for Latent Variable Models.", "startOffset": 28, "endOffset": 32}, {"referenceID": 28, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 49, "endOffset": 61}, {"referenceID": 38, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 49, "endOffset": 61}, {"referenceID": 16, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 49, "endOffset": 61}, {"referenceID": 7, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 93, "endOffset": 103}, {"referenceID": 6, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 93, "endOffset": 103}, {"referenceID": 13, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 93, "endOffset": 103}, {"referenceID": 19, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 50, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 37, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 12, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 27, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 30, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 23, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 4, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 287, "endOffset": 290}, {"referenceID": 5, "context": "A number of these methods essentially can be phrased as solving an inverse moments problem, and the work in [6] provides a unifying viewpoint for computationally efficient estimation for many of these models under a tensor decomposition perspective.", "startOffset": 108, "endOffset": 111}, {"referenceID": 42, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 90, "endOffset": 106}, {"referenceID": 25, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 90, "endOffset": 106}, {"referenceID": 46, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 90, "endOffset": 106}, {"referenceID": 48, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 90, "endOffset": 106}, {"referenceID": 43, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 121, "endOffset": 129}, {"referenceID": 46, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 121, "endOffset": 129}, {"referenceID": 46, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 162, "endOffset": 174}, {"referenceID": 48, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 162, "endOffset": 174}, {"referenceID": 47, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 162, "endOffset": 174}, {"referenceID": 10, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 289, "endOffset": 305}, {"referenceID": 41, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 289, "endOffset": 305}, {"referenceID": 49, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 289, "endOffset": 305}, {"referenceID": 14, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 289, "endOffset": 305}, {"referenceID": 2, "context": "[3, 4, 50], with stronger information theoretic optimality guarantees.", "startOffset": 0, "endOffset": 10}, {"referenceID": 3, "context": "[3, 4, 50], with stronger information theoretic optimality guarantees.", "startOffset": 0, "endOffset": 10}, {"referenceID": 49, "context": "[3, 4, 50], with stronger information theoretic optimality guarantees.", "startOffset": 0, "endOffset": 10}, {"referenceID": 15, "context": "where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21].", "startOffset": 109, "endOffset": 125}, {"referenceID": 11, "context": "where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21].", "startOffset": 109, "endOffset": 125}, {"referenceID": 29, "context": "where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21].", "startOffset": 109, "endOffset": 125}, {"referenceID": 20, "context": "where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21].", "startOffset": 109, "endOffset": 125}, {"referenceID": 22, "context": "This simple idea was first introduced by [23], and followed by analysis works in [22] and many others.", "startOffset": 41, "endOffset": 45}, {"referenceID": 21, "context": "This simple idea was first introduced by [23], and followed by analysis works in [22] and many others.", "startOffset": 81, "endOffset": 85}, {"referenceID": 32, "context": "Recently in [33] and [34] the authors provided clean and clever proofs to show that any such \u201cregularization\u201d essentially leads to better spectral concentration for the adjacency matrix of random graphs whose row/column sums are roughly uniform in expectation.", "startOffset": 12, "endOffset": 16}, {"referenceID": 33, "context": "Recently in [33] and [34] the authors provided clean and clever proofs to show that any such \u201cregularization\u201d essentially leads to better spectral concentration for the adjacency matrix of random graphs whose row/column sums are roughly uniform in expectation.", "startOffset": 21, "endOffset": 25}, {"referenceID": 33, "context": "When directly applied to the empirical bins with such spillover, the existing results of \u201cregularization\u201d in [34] do not lead to the desired concentration result.", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "For example, in a recent paper [19] on community detection, after obtaining a crude classification of nodes using spectral algorithm, one round of a \u201ccorrection\u201d routine is applied to each node based on its connections to the graph partition given by the first round.", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "Another example is given in [18] in the context of solving random quadratic equations, where local refinement of the solution follows the spectral method initialization.", "startOffset": 28, "endOffset": 32}, {"referenceID": 33, "context": "Then we leverage the clever proof techniques from [34] to show that if spillover effect is small, regularized truncated SVD can be applied to estimate the entries of \u2206 restricted to each bin.", "startOffset": 50, "endOffset": 54}, {"referenceID": 33, "context": "For block A1 whose rows and columns all correspond to the \u201cgood words\u201d with roughly uniform marginals, we show its concentration by applying the result in [34].", "startOffset": 155, "endOffset": 159}], "year": 2017, "abstractText": "We consider the problem of accurately recovering a matrix B of size M \u00d7M , which represents a probability distribution overM outcomes, given access to an observed matrix of \u201ccounts\u201d generated by taking independent samples from the distribution B. How can structural properties of the underlying matrix B be leveraged to yield computationally efficient and information theoretically optimal reconstruction algorithms? When can accurate reconstruction be accomplished in the sparse data regime? This basic problem lies at the core of a number of questions that are currently being considered by different communities, including community detection in sparse random graphs, learning structured models such as topic models or hidden Markov models, and the efforts from the natural language processing community to compute \u201cword embeddings\u201d. Many aspects of this problem\u2014both in terms of learning and property testing/estimation and on both the algorithmic and information theoretic sides\u2014remain open. Our results apply to the setting where B has a particular rank 2 structure. For this setting, we propose an efficient (and practically viable) algorithm that accurately recovers the underlying M \u00d7 M matrix using \u0398(M) samples. This result easily translates to \u0398(M) sample algorithms for learning topic models with two topics over dictionaries of size M , and learning hidden Markov Models with two hidden states and observation distributions supported on M elements. These linear sample complexities are optimal, up to constant factors, in an extremely strong sense: even testing basic properties of the underlying matrix (such as whether it has rank 1 or 2) requires \u03a9(M) samples. Furthermore, we provide an even stronger lower bound where distinguishing whether a sequence of observations were drawn from the uniform distribution over M observations versus being generated by an HMM with two hidden states requires \u03a9(M) observations. This precludes sublinear-sample hypothesis tests for basic properties, such as identity or uniformity, as well as sublinear sample estimators for quantities such as the entropy rate of HMMs. This impossibility of sublinear-sample property testing in these settings is intriguing and underscores the significant differences between these structured settings and the standard setting of drawing i.i.d samples from an unstructured distribution of support size M . \u2217MIT. Email: qqh@mit.edu. \u2020University of Washington. Email: sham@cs.washington.edu \u2021Stanford University. Email: kweihao@gmail.com \u00a7Stanford University. Email: valiant@stanford.edu. Gregory and Weihao\u2019s contributions were supported by NSF CAREER Award CCF-1351108, and a research grant from the Okawa Foundation. ar X iv :1 60 2. 06 58 6v 1 [ cs .L G ] 2 1 Fe b 20 16", "creator": "LaTeX with hyperref package"}}}