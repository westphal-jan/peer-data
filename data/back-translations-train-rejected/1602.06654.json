{"id": "1602.06654", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "Structured Learning of Binary Codes with Column Generation", "abstract": "Hashing methods aim to learn a set of hash functions which map the original features to compact binary codes with similarity preserving in the Hamming space. Hashing has proven a valuable tool for large-scale information retrieval. We propose a column generation based binary code learning framework for data-dependent hash function learning. Given a set of triplets that encode the pairwise similarity comparison information, our column generation based method learns hash functions that preserve the relative comparison relations within the large-margin learning framework. Our method iteratively learns the best hash functions during the column generation procedure. Existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest---multivariate performance measures such as the AUC and NDCG. Our column generation based method can be further generalized from the triplet loss to a general structured learning based framework that allows one to directly optimize multivariate performance measures. For optimizing general ranking measures, the resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. We use a combination of column generation and cutting-plane techniques to solve the optimization problem. To speed-up the training we further explore stage-wise training and propose to use a simplified NDCG loss for efficient inference. We demonstrate the generality of our method by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.", "histories": [["v1", "Mon, 22 Feb 2016 06:02:25 GMT  (318kb,D)", "http://arxiv.org/abs/1602.06654v1", "20 pages"]], "COMMENTS": "20 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["guosheng lin", "fayao liu", "chunhua shen", "jianxin wu", "heng tao shen"], "accepted": false, "id": "1602.06654"}, "pdf": {"name": "1602.06654.pdf", "metadata": {"source": "CRF", "title": "Structured Learning of Binary Codes with Column Generation", "authors": ["Guosheng Lin", "Fayao Liu", "Chunhua Shen", "Jianxin Wu", "Heng Tao Shen"], "emails": [], "sections": [{"heading": null, "text": "Existing hash methods optimize using simple objectives such as reconstruction errors or laplactic loss functions in charts instead of interesting performance assessment criteria - multivariate performance measures such as AUC and NDCG. Our column generation-based method can be further generalized from triplet loss to a general structured learning framework that allows multivariate performance measures to be directly optimized. To optimize general ranking measures, the resulting optimization problem may involve exponentially or infinitely many variables and constraints, which is more difficult than standard structured output learning. We use a combination of column generation and cutting-level techniques to solve the optimization problem. To accelerate the training, we continue exploring step-by-step training and propose using simplified NDCG loss for efficient conclusions. We demonstrate the universality of our hash method by demonstrating it and applying some highly valid retrieval methods."}, {"heading": "1 Introduction", "text": "In fact, most people who have chosen such a path are able to walk it, to walk it and to walk it. It's not that they want to walk it. It's that they want to walk it. It's that they want to walk it. It's that they want to walk it. It's that they want to walk it. It's that they want to walk it. It's that they want to walk it. It's that they want to walk it. It's that they want to walk it. It's that they want to walk it. It's that they want to walk it."}, {"heading": "2 Related work", "text": "This year, it has reached the point where there is only one person who is able to establish himself in the region."}, {"heading": "3 Hashing for optimizing the triplet loss", "text": "We refer to this approach as CGHash = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "3.3.1 Hashing with logistic loss", "text": "It has been shown in (24) that the formulation of the square hinge loss is an example of the general formulation in (23) with smooth convex loss functions. At this point, we describe the logistic loss as another example of the general formulation. The learning algorithm is similar to the case of the use of the square hinge loss described above. (32) The general result for the smooth convex loss function can be applied here. (33) s.t. (i, j, k): p (i, p, k) = w > output problem (i, j \u2212 k), w > tlog (\u2212 \u03c1 (i, j, k))))) (33) s.t. The initial optimization problem can be described as follows: min w, p (i, j, k) = w > output problem (i, j \u2212 k), w > tlog (\u2212 \u03c1 (i, j, k), k).The corresponding problem can be written k, k, k, k (k)."}, {"heading": "3.3.2 Hashing with `\u221e norm regularization", "text": "The proposed method is flexible in that it is easy to integrate different types of regularizations. At this point, we will discuss the \"dual-norm regularization\" as an example. For general konvexe losses, the optimization can be written as follows: min w, ay, w, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "4 Hashing for optimizing ranking loss", "text": "Our column generation approach CGHash should be larger than any other \"incorrect\" model, in which the overall ranking loss is optimized, which is more complex than the simple triplet loss. This extension is a structured learning-based approach to binary code learning, so we refer to this extension as StructHash in this thesis. Before describing details of StructHash, we first present a preliminary technique that applies a set of input-output pairs to optimize ranking losses. The discriminatory function for structured output predictions is F (x, y): X \u00d7 Y 7 \u2192 R, which measures the compatibility of the input-output pair (x, y). Structured SVM forces that the score of the \"correct\" model y should be higher than any other \"incorporated output model."}, {"heading": "4.5.1 Stage-wise training", "text": "In forming a new party capable of establishing itself, we must play by the rules we have adopted."}, {"heading": "4.5.2 Optimizing Simplified NDCG (SNDCG) score", "text": "As discussed before, we need to solve the maximization conclusion in (50) to find the most violated limitations. The computational complexity to solve this consequence problem mainly depends on the definition of \u2206 (y, y \u2032) in (58), from which some examples in paragraph 4.4 are discussed. Normally, when using position-sensitive loss functions, such as MAP, NDCG, the maximization conclusion is solved with great effort [40, 41], which could limit their application to large-scale learning. Inspired by the efficient metric learning method in [27], we discuss here a form of position-sensitive ranking losses that are able to draw quick conclusions. Basically, we construct a simplified NDCG Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score"}, {"heading": "5 Experiments", "text": "We evaluate the proposed method CGHash for optimizing triplet loss and the more general methods used in Sec. 5.2 we evaluate the more efficient models used in Sec. 4.5.9 datasets here for evaluation, including one UCI dataset: ISOLET, 4 image datasets: CIFAR103, STL104, MNIST, USPS, and another 4 large image datasets: Tiny-580K [21], Flickr-1M5, SIFT-1M6 [8] and GIST1M6. CIFAR10 is a subset of 80-million tiny images and STL10 is a subset of the image network."}, {"heading": "5.2.1 Training on large-scale datasets", "text": "Furthermore, we evaluate the more efficient models, i.e. the StructH-NDCG stage and the StructH-SNDCG stage, on two large datasets, namely the TINY-580K and the SIFT-1M. Results are presented in Table 3. As can be seen, the StructHash with incremental training exceeds the original StructHash model. Given the fact that the Stage-wise StructHash uses unweighted hammering distance, this may indicate that the learned hash functions are more important than the weights. We also find that the optimization of the SNDCG loss with incremental training is on par with the optimization of the original NDCG loss."}, {"heading": "5.2.2 Computational complexity", "text": "To show the scalability of the two more efficient extensions of StructHash, we show the training time by varying the number of training examples in Fig. 7. We report on the training time of learning 32-bit hash functions. We also compare StructHash with the original StructHash in the left diagram of Fig. 7. As we can see, the gradual training brings orders of magnitude of acceleration compared to the original StructHash model. The right diagram in Fig. 7 compares the use of simplified NDCG losses with the original NDCG loss, and the significantly simplified NDCG loss is significantly more efficient."}, {"heading": "6 Conclusion", "text": "We have developed a flexible, column-generation-based hashing framework that is capable of optimizing general multivariate ranking measures as well as treble loss. We have shown that column generation is capable of learning high-quality binary codes for monitored hashing. It is expected that the proposed method for optimizing ranking losses usually outperforms comparable hashing approaches by optimizing the required loss function more directly. It is expected that the success of the approach will lead to a number of new hashing-based applications with task-specific goals. We also present two enhancements to the Learning Framework for Efficient Learning based on step-by-step training and the use of the proposed simplified NDCG for efficient ranking conclusions."}], "references": [{"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "Proc. Adv. Neural Info. Process. Syst.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "In Proc. Adv. Neural Info. Process. Syst.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "A general two-step approach to learning-based hashing", "author": ["G. Lin", "C. Shen", "D. Suter", "A. van den Hengel"], "venue": "In Proc. Int. Conf. Comp. Vis., Sydney, Australia,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Inductive hashing on manifolds", "author": ["F. Shen", "C. Shen", "Q. Shi", "A. van den Hengel", "Z. Tang"], "venue": "In Proc. Int. Conf. Comp. Vis. & Patt. Recogn., Oregon,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.F. Chang"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Fast supervised hashing with decision trees for high-dimensional data", "author": ["G. Lin", "C. Shen", "Q. Shi", "A. van den Hengel", "D. Suter"], "venue": "In Proc. Int. Conf. Comp. Vis. & Patt. Recogn.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Small codes and large image databases for recognition", "author": ["A. Torralba", "R. Fergus", "Y. Weiss"], "venue": "In Proc. Int. Conf. Comp. Vis. & Patt. Recogn.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Semi-supervised hashing for large scale search", "author": ["J. Wang", "S. Kumar", "S.F. Chang"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "LDAHash: Improved matching with smaller descriptors", "author": ["C. Strecha", "A. Bronstein", "M. Bronstein", "P. Fua"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Fast, accurate detection of 100,000 object classes on a single machine", "author": ["T. Dean", "M.A. Ruzon", "M. Segal", "J. Shlens", "S. Vijayanarasimhan", "J. Yagnik"], "venue": "In Proc. Int. Conf. Comp. Vis. & Patt. Recogn.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Learning a distance metric from relative comparisons", "author": ["M. Schultz", "T. Joachims"], "venue": "In Proc. Adv. Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Positive semidefinite metric learning using boosting-like algorithms", "author": ["C. Shen", "J. Kim", "L. Wang", "A. van den Hengel"], "venue": "J. Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Metric learning to rank", "author": ["B. McFee", "G.R.G. Lanckriet"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "IR evaluation methods for retrieving highly relevant documents", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "In Proc. ACM Conf. SIGIR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Extensions to self-taught hashing: kernelisation and supervision", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "In Proc. ACM Conf. SIGIR Workshop,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.G. Jiang", "S.F. Chang"], "venue": "In Proc. Int. Conf. Comp. Vis. & Patt. Recogn.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Learning hash functions using column generation", "author": ["X. Li", "G. Lin", "C. Shen", "A. van den Hengel", "Anthony Dick"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Optimizing ranking measures for compact binary code learning", "author": ["G. Lin", "C. Shen", "J. Wu"], "venue": "In Proc. Eur. Conf. Comp. Vis.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In Proc. Int. Conf. Very Large Data Bases,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Iterative quantization: a procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Kernelized locality-sensitive hashing", "author": ["B. Kulis", "K. Grauman"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Minimal loss hashing for compact binary codes", "author": ["Mohammad Norouzi", "David J. Fleet"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Hamming distance metric learning", "author": ["Mohammad Norouzi", "David J. Fleet", "Ruslan Salakhutdinov"], "venue": "In Proc. Adv. Neural Info. Process. Syst.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Ranking preserving hashing for fast similarity search", "author": ["Qifan Wang", "Zhiwei Zhang", "Luo Si"], "venue": "In Proc. Int. Joint Conf. Artif. Intelli.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Online learning in the embedded manifold of low-rank matrices", "author": ["Uri Shalit", "Daphna Weinshall", "Gal Chechik"], "venue": "Journal of Mach. Learn. Res.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Efficient learning of mahalanobis metrics for ranking", "author": ["Daryl Lim", "Gert Lanckriet"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "Machine learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Linear programming boosting via column generation", "author": ["Ayhan Demiriz", "Kristin P. Bennett", "John Shawe-Taylor"], "venue": "Mach. Learn.,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "On the dual formulation of boosting algorithms", "author": ["Chunhua Shen", "Hanxi Li"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "StructBoost: Boosting methods for predicting structured output variables", "author": ["C. Shen", "G. Lin", "A. van den Hengel"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Multidimensional spectral hashing", "author": ["Y. Weiss", "R. Fergus", "A. Torralba"], "venue": "In Proc. Eur. Conf. Comp. Vis.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "P. Lu", "J. Nocedal"], "venue": "ACM T. Math. Softw.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1997}, {"title": "Linear programming boosting via column generation", "author": ["A. Demiriz", "K.P. Bennett", "J. Shawe-Taylor"], "venue": "Mach. Learn.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2002}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2004}, {"title": "The cutting-plane method for solving convex programs", "author": ["J.E. Jr. Kelley"], "venue": "J. Society for Industrial & Applied Math.,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1960}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "In Proc. ACM Knowledge Discovery & Data Mining,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "On the dual formulation of boosting algorithms", "author": ["C. Shen", "H. Li"], "venue": "IEEE Trans. Patt. Anal. & Mach. Intelli.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "The MOSEK optimization toolbox for MATLAB manual", "author": ["MOSEK ApS"], "venue": "Version 7.1 (Revision", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Structured learning for non-smooth ranking losses", "author": ["S. Chakrabarti", "R. Khanna", "U. Sawant", "C. Bhattacharyya"], "venue": "In Proc. ACM Knowledge Discovery & Data Mining,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "A support vector method for optimizing average precision", "author": ["Y. Yue", "T. Finley", "F. Radlinski", "T. Joachims"], "venue": "In Proc. ACM Conf. SIGIR,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2007}, {"title": "Spherical hashing", "author": ["J. Heo", "Y. Lee", "J. He", "S. Chang", "S. Yoon"], "venue": "In Proc. Int. Conf. Comp. Vis. & Patt. Recogn.,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "One of the simplest, and most effective means of improving the scale and efficiency of an application has been to use hashing to pre-process the data [1, 2, 3, 4, 5, 6].", "startOffset": 150, "endOffset": 168}, {"referenceID": 1, "context": "One of the simplest, and most effective means of improving the scale and efficiency of an application has been to use hashing to pre-process the data [1, 2, 3, 4, 5, 6].", "startOffset": 150, "endOffset": 168}, {"referenceID": 2, "context": "One of the simplest, and most effective means of improving the scale and efficiency of an application has been to use hashing to pre-process the data [1, 2, 3, 4, 5, 6].", "startOffset": 150, "endOffset": 168}, {"referenceID": 3, "context": "One of the simplest, and most effective means of improving the scale and efficiency of an application has been to use hashing to pre-process the data [1, 2, 3, 4, 5, 6].", "startOffset": 150, "endOffset": 168}, {"referenceID": 4, "context": "One of the simplest, and most effective means of improving the scale and efficiency of an application has been to use hashing to pre-process the data [1, 2, 3, 4, 5, 6].", "startOffset": 150, "endOffset": 168}, {"referenceID": 5, "context": "One of the simplest, and most effective means of improving the scale and efficiency of an application has been to use hashing to pre-process the data [1, 2, 3, 4, 5, 6].", "startOffset": 150, "endOffset": 168}, {"referenceID": 6, "context": "Applications include image retrieval [7, 8], image matching [9], large-scale object detection [10], etc.", "startOffset": 37, "endOffset": 43}, {"referenceID": 7, "context": "Applications include image retrieval [7, 8], image matching [9], large-scale object detection [10], etc.", "startOffset": 37, "endOffset": 43}, {"referenceID": 8, "context": "Applications include image retrieval [7, 8], image matching [9], large-scale object detection [10], etc.", "startOffset": 60, "endOffset": 63}, {"referenceID": 9, "context": "Applications include image retrieval [7, 8], image matching [9], large-scale object detection [10], etc.", "startOffset": 94, "endOffset": 98}, {"referenceID": 10, "context": "This type of relative similarity comparisons have been successfully applied to learn quadratic distance metrics [11, 12].", "startOffset": 112, "endOffset": 120}, {"referenceID": 11, "context": "This type of relative similarity comparisons have been successfully applied to learn quadratic distance metrics [11, 12].", "startOffset": 112, "endOffset": 120}, {"referenceID": 12, "context": "For example, information retrieval and ranking criteria [13] such as the Area Under the ROC Curve (AUC) [14], Normalized Discounted Cumulative Gain (NDCG) [15], Precision-at-K, Precision-Recall and Mean Average Precision (mAP) have been widely adopted to evaluate the success of hashing methods.", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "For example, information retrieval and ranking criteria [13] such as the Area Under the ROC Curve (AUC) [14], Normalized Discounted Cumulative Gain (NDCG) [15], Precision-at-K, Precision-Recall and Mean Average Precision (mAP) have been widely adopted to evaluate the success of hashing methods.", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "For example, information retrieval and ranking criteria [13] such as the Area Under the ROC Curve (AUC) [14], Normalized Discounted Cumulative Gain (NDCG) [15], Precision-at-K, Precision-Recall and Mean Average Precision (mAP) have been widely adopted to evaluate the success of hashing methods.", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": ", binary reconstruction embedding hashing [1]), the graph Laplacian related loss [2, 5, 16], or the pairwise similarity loss [17].", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": ", binary reconstruction embedding hashing [1]), the graph Laplacian related loss [2, 5, 16], or the pairwise similarity loss [17].", "startOffset": 81, "endOffset": 91}, {"referenceID": 4, "context": ", binary reconstruction embedding hashing [1]), the graph Laplacian related loss [2, 5, 16], or the pairwise similarity loss [17].", "startOffset": 81, "endOffset": 91}, {"referenceID": 15, "context": ", binary reconstruction embedding hashing [1]), the graph Laplacian related loss [2, 5, 16], or the pairwise similarity loss [17].", "startOffset": 81, "endOffset": 91}, {"referenceID": 16, "context": ", binary reconstruction embedding hashing [1]), the graph Laplacian related loss [2, 5, 16], or the pairwise similarity loss [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "Preliminary results of our work appeared in [18] and [19].", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "Preliminary results of our work appeared in [18] and [19].", "startOffset": 53, "endOffset": 57}, {"referenceID": 19, "context": "Examples fall into this category are locality sensitive hashing (LSH) [20], spectral hashing (SPH) [2], anchor graph hashing (AGH) [5], iterative quantization hashing (ITQ) [21].", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "Examples fall into this category are locality sensitive hashing (LSH) [20], spectral hashing (SPH) [2], anchor graph hashing (AGH) [5], iterative quantization hashing (ITQ) [21].", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "Examples fall into this category are locality sensitive hashing (LSH) [20], spectral hashing (SPH) [2], anchor graph hashing (AGH) [5], iterative quantization hashing (ITQ) [21].", "startOffset": 131, "endOffset": 134}, {"referenceID": 20, "context": "Examples fall into this category are locality sensitive hashing (LSH) [20], spectral hashing (SPH) [2], anchor graph hashing (AGH) [5], iterative quantization hashing (ITQ) [21].", "startOffset": 173, "endOffset": 177}, {"referenceID": 19, "context": "Specifically, LSH [20] uses random projection to generate binary codes; SPH [2] aims to preserve the neighbourhood relation by optimizing the Laplacian affinity; AGH [5] makes the original SPH much more scalable; ITH [21] first performs linear dimensionality reduction and then conduct binary quantization in the resulting space.", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "Specifically, LSH [20] uses random projection to generate binary codes; SPH [2] aims to preserve the neighbourhood relation by optimizing the Laplacian affinity; AGH [5] makes the original SPH much more scalable; ITH [21] first performs linear dimensionality reduction and then conduct binary quantization in the resulting space.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "Specifically, LSH [20] uses random projection to generate binary codes; SPH [2] aims to preserve the neighbourhood relation by optimizing the Laplacian affinity; AGH [5] makes the original SPH much more scalable; ITH [21] first performs linear dimensionality reduction and then conduct binary quantization in the resulting space.", "startOffset": 166, "endOffset": 169}, {"referenceID": 20, "context": "Specifically, LSH [20] uses random projection to generate binary codes; SPH [2] aims to preserve the neighbourhood relation by optimizing the Laplacian affinity; AGH [5] makes the original SPH much more scalable; ITH [21] first performs linear dimensionality reduction and then conduct binary quantization in the resulting space.", "startOffset": 217, "endOffset": 221}, {"referenceID": 0, "context": "Binary reconstruction embedding (BRE) [1] aims to minimize the expected distances; semi-supervised sequential projection learning hashing (SPLH) [8] enforces the smoothness of similar data points and the separability of dissimilar data points; kernelized LSH, proposed by Kulis and Grauman [22], randomly samples training data as support vectors, and randomly draws the dual coefficients from a Gaussian distribution.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "Binary reconstruction embedding (BRE) [1] aims to minimize the expected distances; semi-supervised sequential projection learning hashing (SPLH) [8] enforces the smoothness of similar data points and the separability of dissimilar data points; kernelized LSH, proposed by Kulis and Grauman [22], randomly samples training data as support vectors, and randomly draws the dual coefficients from a Gaussian distribution.", "startOffset": 145, "endOffset": 148}, {"referenceID": 21, "context": "Binary reconstruction embedding (BRE) [1] aims to minimize the expected distances; semi-supervised sequential projection learning hashing (SPLH) [8] enforces the smoothness of similar data points and the separability of dissimilar data points; kernelized LSH, proposed by Kulis and Grauman [22], randomly samples training data as support vectors, and randomly draws the dual coefficients from a Gaussian distribution.", "startOffset": 290, "endOffset": 294}, {"referenceID": 16, "context": "[17] extended kernelized LSH to kernelized supervised hashing (KSH).", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3, 6] present a general two step framework for hashing learning.", "startOffset": 0, "endOffset": 6}, {"referenceID": 5, "context": "[3, 6] present a general two step framework for hashing learning.", "startOffset": 0, "endOffset": 6}, {"referenceID": 22, "context": "In [23], Norouzi et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "They further generalize the method in [23] to optimize a triplet ranking loss designed to preserve relative similarities [24].", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "They further generalize the method in [23] to optimize a triplet ranking loss designed to preserve relative similarities [24].", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "The hashing method in [25] proposes to optimizes the NDCG ranking loss with a gradient decent method, which comes out after the publication of our preliminary version of StructHash in [19].", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "The hashing method in [25] proposes to optimizes the NDCG ranking loss with a gradient decent method, which comes out after the publication of our preliminary version of StructHash in [19].", "startOffset": 184, "endOffset": 188}, {"referenceID": 12, "context": "Learning to rank Our method is primarily inspired by recent advances in metric learning for ranking [13, 26, 27].", "startOffset": 100, "endOffset": 112}, {"referenceID": 25, "context": "Learning to rank Our method is primarily inspired by recent advances in metric learning for ranking [13, 26, 27].", "startOffset": 100, "endOffset": 112}, {"referenceID": 26, "context": "Learning to rank Our method is primarily inspired by recent advances in metric learning for ranking [13, 26, 27].", "startOffset": 100, "endOffset": 112}, {"referenceID": 12, "context": "In [13], McFee et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "[26] propose a scalable method for optimizing a ranking loss, though they only consider the Area Under the ROC Curve (AUC) loss.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "In [27], Lim et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": ", the Weighted Approximate Pairwise Ranking (WARP) loss [28].", "startOffset": 56, "endOffset": 60}, {"referenceID": 28, "context": "Column generation Column generation is widely applied in boosting methods [29, 30, 31].", "startOffset": 74, "endOffset": 86}, {"referenceID": 29, "context": "Column generation Column generation is widely applied in boosting methods [29, 30, 31].", "startOffset": 74, "endOffset": 86}, {"referenceID": 30, "context": "Column generation Column generation is widely applied in boosting methods [29, 30, 31].", "startOffset": 74, "endOffset": 86}, {"referenceID": 28, "context": "LPBoost [29] is a linear programming boosting method that iteratively learn weak classifiers to form a strong classifier.", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "StructBoost [31] provides a general structured learning framework using column generation for structured prediction problems.", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "Such weighted hamming distance is used in multi-dimension spectral hashing [32].", "startOffset": 75, "endOffset": 79}, {"referenceID": 32, "context": ", using LBFGS-B [33].", "startOffset": 16, "endOffset": 20}, {"referenceID": 33, "context": "LPBoost [34] applies this technique to design boosting algorithms.", "startOffset": 8, "endOffset": 12}, {"referenceID": 32, "context": "2 for r = 1 to m do 3 find a new hash function hr(\u00b7) by solving the subproblem: (17); 4 add hr(\u00b7) to the working set of hash functions; 5 solve the primal problem in (11) for w (using LBFGS-B[33]), and calculate the dual solution \u03bc by (18);", "startOffset": 191, "endOffset": 195}, {"referenceID": 32, "context": "LBFGS-B solver [33].", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "Alternatively, we can employ the spectral relaxation method [5] which drops the sign function and solves a generalized eigenvalue problem to obtain a solution for initialization.", "startOffset": 60, "endOffset": 63}, {"referenceID": 34, "context": "With the definition of Fenchel conjugate [35]: f(z) := sup x\u2208domf x>z\u2212 f(x) ( here f(\u00b7) is the Fenchel conjugate of the function f(\u00b7) ), we have the following dual objective:", "startOffset": 41, "endOffset": 45}, {"referenceID": 35, "context": "The cutting-plane method [36] is commonly employed, which allows to maintain a small working-set of constraints and obtain an approximate solution of the original problem up to a pre-set precision.", "startOffset": 25, "endOffset": 29}, {"referenceID": 36, "context": "To speed up, the 1-slack reformulation is proposed [37].", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "These evaluation measures can be optimized directly as label loss \u2206 [13, 14].", "startOffset": 68, "endOffset": 76}, {"referenceID": 13, "context": "These evaluation measures can be optimized directly as label loss \u2206 [13, 14].", "startOffset": 68, "endOffset": 76}, {"referenceID": 33, "context": "This is aligned with boosting methods [34, 38], and enables us to learn hash functions efficiently.", "startOffset": 38, "endOffset": 46}, {"referenceID": 37, "context": "This is aligned with boosting methods [34, 38], and enables us to learn hash functions efficiently.", "startOffset": 38, "endOffset": 46}, {"referenceID": 36, "context": "Inspired by [37], we first derive the 1-slack formulation of the original n-slack formulation (46):", "startOffset": 12, "endOffset": 16}, {"referenceID": 36, "context": "As in [37], cutting-plane methods can be used to solve the 1-slack primal problem (48) efficiently.", "startOffset": 6, "endOffset": 10}, {"referenceID": 38, "context": ", using MOSEK [39]) on current working set W ; 5 for i = 1, .", "startOffset": 14, "endOffset": 18}, {"referenceID": 32, "context": ", LBFGS [33]) for learning the parameters of a hash function.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "We apply the spectral relaxation [5] to obtain an initial point for solving (55), which drops the sign(\u00b7) function and solves a generalized eigenvalue problem.", "startOffset": 33, "endOffset": 36}, {"referenceID": 12, "context": "Following [13], we define the loss function over two rankings \u2206 \u2208 [0 1] as: \u2206(y,y) = 1\u2212 score(y,y).", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "Following [13], we define the loss function over two rankings \u2206 \u2208 [0 1] as: \u2206(y,y) = 1\u2212 score(y,y).", "startOffset": 66, "endOffset": 71}, {"referenceID": 13, "context": "For using this AUC loss, the maximization inference in (50) can be solved efficiently by sorting the distances of data pairs, as described in [14].", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "Normalized Discounted Cumulative Gain [15] is to measure the ranking quality of the first K returned neighbours.", "startOffset": 38, "endOffset": 42}, {"referenceID": 39, "context": "A dynamic programming algorithm is proposed in [40] for solving the maximization inference in (50).", "startOffset": 47, "endOffset": 51}, {"referenceID": 28, "context": "In column generation based totallycorrective boosting methods [29, 30, 31], all the hash function weights w are updated during each column generation iteration.", "startOffset": 62, "endOffset": 74}, {"referenceID": 29, "context": "In column generation based totallycorrective boosting methods [29, 30, 31], all the hash function weights w are updated during each column generation iteration.", "startOffset": 62, "endOffset": 74}, {"referenceID": 30, "context": "In column generation based totallycorrective boosting methods [29, 30, 31], all the hash function weights w are updated during each column generation iteration.", "startOffset": 62, "endOffset": 74}, {"referenceID": 39, "context": "Usually when using position sensitive loss functions, such as mAP, NDCG, it is computational expensive to solve the maximization inference [40, 41], which might limit its application on large-scale learning.", "startOffset": 139, "endOffset": 147}, {"referenceID": 40, "context": "Usually when using position sensitive loss functions, such as mAP, NDCG, it is computational expensive to solve the maximization inference [40, 41], which might limit its application on large-scale learning.", "startOffset": 139, "endOffset": 147}, {"referenceID": 26, "context": "Inspired by the efficient metric learning method in [27], here we discuss a form of positionsensitive ranking loss which is capable for fast inference.", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "Nine datasets are used here for evaluation, including one UCI dataset: ISOLET, 4 image datasets: CIFAR10, STL10, MNIST, USPS, and another 4 large image datasets: Tiny-580K [21], Flickr-1M, SIFT-1M [8] and GIST1M.", "startOffset": 172, "endOffset": 176}, {"referenceID": 7, "context": "Nine datasets are used here for evaluation, including one UCI dataset: ISOLET, 4 image datasets: CIFAR10, STL10, MNIST, USPS, and another 4 large image datasets: Tiny-580K [21], Flickr-1M, SIFT-1M [8] and GIST1M.", "startOffset": 197, "endOffset": 200}, {"referenceID": 0, "context": "For the hashing performance evaluation, we follow the common setting in many supervised methods [1, 17].", "startOffset": 96, "endOffset": 103}, {"referenceID": 16, "context": "For the hashing performance evaluation, we follow the common setting in many supervised methods [1, 17].", "startOffset": 96, "endOffset": 103}, {"referenceID": 7, "context": "For large datasets: Flickr-1M, SIFT-1M, GIST-1M and Tiny-580K, the semantic ground truth is defined according to the `2 distance [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 0, "context": "To evaluate the performance of compact bits, the maximum bit length is set to 64, as similar to the evaluation settings in other supervised hashing methods [1].", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "Thus we mainly compare with 3 supervised methods: supervised binary reconstructive embeddings (BREs) [1], supervised self-taught hashing (STHs) [16], semi-supervised sequential projection learning hashing (SPLH) [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 15, "context": "Thus we mainly compare with 3 supervised methods: supervised binary reconstructive embeddings (BREs) [1], supervised self-taught hashing (STHs) [16], semi-supervised sequential projection learning hashing (SPLH) [8].", "startOffset": 144, "endOffset": 148}, {"referenceID": 7, "context": "Thus we mainly compare with 3 supervised methods: supervised binary reconstructive embeddings (BREs) [1], supervised self-taught hashing (STHs) [16], semi-supervised sequential projection learning hashing (SPLH) [8].", "startOffset": 212, "endOffset": 215}, {"referenceID": 19, "context": "We also run some unsupervised methods for comparisons: locality-sensitive hashing (LSH) [20], anchor graph hashing (AGH) [5], spherical hashing (SPHER) [42], multi-dimension spectral hashing (MDSH) [32], and iterative quantization (ITQ) [21].", "startOffset": 88, "endOffset": 92}, {"referenceID": 4, "context": "We also run some unsupervised methods for comparisons: locality-sensitive hashing (LSH) [20], anchor graph hashing (AGH) [5], spherical hashing (SPHER) [42], multi-dimension spectral hashing (MDSH) [32], and iterative quantization (ITQ) [21].", "startOffset": 121, "endOffset": 124}, {"referenceID": 41, "context": "We also run some unsupervised methods for comparisons: locality-sensitive hashing (LSH) [20], anchor graph hashing (AGH) [5], spherical hashing (SPHER) [42], multi-dimension spectral hashing (MDSH) [32], and iterative quantization (ITQ) [21].", "startOffset": 152, "endOffset": 156}, {"referenceID": 31, "context": "We also run some unsupervised methods for comparisons: locality-sensitive hashing (LSH) [20], anchor graph hashing (AGH) [5], spherical hashing (SPHER) [42], multi-dimension spectral hashing (MDSH) [32], and iterative quantization (ITQ) [21].", "startOffset": 198, "endOffset": 202}, {"referenceID": 20, "context": "We also run some unsupervised methods for comparisons: locality-sensitive hashing (LSH) [20], anchor graph hashing (AGH) [5], spherical hashing (SPHER) [42], multi-dimension spectral hashing (MDSH) [32], and iterative quantization (ITQ) [21].", "startOffset": 237, "endOffset": 241}, {"referenceID": 21, "context": "Applying the kernel technique in KLSH [22] and KSH [17] further improves the performance of our method.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "Applying the kernel technique in KLSH [22] and KSH [17] further improves the performance of our method.", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "As describe in [17], we perform a pre-processing step to generate the kernel mapping features: we randomly select a number of support vectors (300) then compute the kernel response on data points as input features.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "3 Comparison on large datasets of our kernel StructHash (StructHash-Kernel) with our non-kernel StructHash and the relevant method KSH [17].", "startOffset": 135, "endOffset": 139}], "year": 2016, "abstractText": "Hashing methods aim to learn a set of hash functions which map the original features to compact binary codes with similarity preserving in the Hamming space. Hashing has proven a valuable tool for large-scale information retrieval. We propose a column generation based binary code learning framework for data-dependent hash function learning. Given a set of triplets that encode the pairwise similarity comparison information, our column generation based method learns hash functions that preserve the relative comparison relations within the large-margin learning framework. Our method iteratively learns the best hash functions during the column generation procedure. Existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest\u2014multivariate performance measures such as the AUC and NDCG. Our column generation based method can be further generalized from the triplet loss to a general structured learning based framework that allows one to directly optimize multivariate performance measures. For optimizing general ranking measures, the resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. We use a combination of column generation and cutting-plane techniques to solve the optimization problem. To speed-up the training we further explore stage-wise training and propose to use a simplified NDCG loss for efficient inference. We demonstrate the generality of our method by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods.", "creator": "LaTeX with hyperref package"}}}