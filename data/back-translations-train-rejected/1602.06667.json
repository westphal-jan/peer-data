{"id": "1602.06667", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "A Motion Planning Strategy for the Active Vision-Based Mapping of Ground-Level Structures", "abstract": "This paper presents a system capable of autonomously mapping the visible part of a bounded three-dimensional structure using a mobile ground robot equipped with a depth sensor. We describe motion planning strategies to determine appropriate successive viewpoints and attempt to fill holes automatically in a point cloud produced by the sensing and perception layer. We develop a local motion planner using potential fields to maintain a desired distance from the structure. The emphasis is on accurately reconstructing a 3D model of a structure of moderate size rather than mapping large open environments, with applications for example in architecture, construction and inspection. The proposed algorithms do not require any initialization in the form of a mesh model or a bounding box. We compare via simulations the performance of our policies to the classic frontier based exploration algorithm. We illustrate the efficacy of our approach for different structure sizes, levels of localization accuracy and range of the depth sensor.", "histories": [["v1", "Mon, 22 Feb 2016 07:05:49 GMT  (11682kb,D)", "https://arxiv.org/abs/1602.06667v1", null], ["v2", "Wed, 1 Mar 2017 17:27:45 GMT  (7599kb,D)", "http://arxiv.org/abs/1602.06667v2", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.SY", "authors": ["s r manikandasriram", "r\\'e phu-van nguyen", "jerome le ny"], "accepted": false, "id": "1602.06667"}, "pdf": {"name": "1602.06667.pdf", "metadata": {"source": "CRF", "title": "A Motion Planning Strategy for the Active Vision-Based Mapping of Ground-Level Structures", "authors": ["S. R. Manikandasriram", "Andr\u00e9 Phu-Van"], "emails": [], "sections": [{"heading": null, "text": "The aim of this work is to automate the process of building a 3D model of an interest structure that is as complete as possible, using a mobile camera or depth sensor in which there is no prior information about that structure. Considering that increasingly robust solutions for visual simulation and the mapping problem (vSLAM) are available, the central challenge we are tackling here is to develop motion planning strategies to control the trajectory of the sensor in a way that improves mapping performance. In certain scenarios, we aim at an external absolute positioning system, such as mapping certain interiors where GPS signals are blocked. In this case, it is often important to reconstruct previously seen locations relatively quickly to avoid excessive drift in the total accounting of the localization system. Our system works by first determining the boundaries of the structure before attempting to fill the holes in the constructed model."}, {"heading": "II. PROBLEM STATEMENT AND ASSUMPTIONS", "text": "In fact, most of us will be able to put ourselves in another world, in another world, in another world, in another world, in another world, in another world, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves."}, {"heading": "III. PERIMETER EXPLORATION", "text": "In this section we present a method for autonomously determining the boundaries of an unknown structure. Starting with assumptions 2 and 3, zg = 0 and zg = Hmax are limiting horizontal planes for the model. The remaining problem is determining the extent of the structure in the xgyg plane. To do this, the robot moves clockwise around the structure by determining a discrete sequence of successive targets or waypoints online. It tries to keep the optical axis of the depth sensor approximately perpendicular to the structure, maximizing the depth resolution with which a certain part of the structure is captured and increasing the density of the captured points. It also tries to keep the camera center Oc on a smooth path at a fixed distance D from the structure."}, {"heading": "A. Determination of the next goal", "text": "The pseudo-code for determining the next position and orientation of the camera in our perimeter exploration algorithm is shown in Algorithm 1. \u2212 The algorithm takes as input the current point cloud produced by the camera in its FoR. \u2212 For its implementation we rely on the Point Cloud Library (PCL) [35]. First, the ground level is computed with a simple algorithm 1 algorithm for calculating the next target for the camera using the current point cloud in camera FoR.1: Function COMPUTENEXTGOAL (cloud full) 2: Cloud 2: Cloud 2: Cloud Slics 1: Cloud Slics (cloud full) 3: Cloud Slics (cloud) 4: PC PCLcompute3Dcentroid (cloud disk) 5: [v1, v2, v3, v3, v3, v3, v3, v3]."}, {"heading": "B. Local path planning to the next goal", "text": "To move the camera center to g and keep it approximately at the desired distance, D = \u03b2 \u03b2 = j is occupied by the cell. We use a local path planner based on potential fields [37], [38]. A potential function encoding the structure as obstacles in the neighborhood of the camera, as well as the target g, is centered on the current position of the robot in the form of a cost map on a local 2D grid of size 2D \u00b7 2D, see Fig. 6. Assumption 2 guarantees that all occupied cells in this cost map designate the structure itself. For k-occupied cells centered on the path {xj} kj = 1, the potential function N (x) is defined at the current position of the camera, see Fig. \u2212 g. 2 + k."}, {"heading": "C. Replanning due to the structure interferring", "text": "Prerequisite 2 guarantees that the robot can move around the structure sufficiently freely, but this does not prevent the structure itself from disturbing the path described above. Consider the situation in Fig. 7a: The wall in front of the robot does not fall into the FOV of the camera due to the limited horizontal viewing angle, but the robot should not approach this wall closer than a distance D. Thus, if the robot detects obstacles in its front D neighbourhood, it is stopped at its current position and the yaw movement of the camera is used to scan forward and look into the new section of the structure. Specifically, as shown in Fig. 8, we use the cost map of the previous sub-section to rotate the camera along the direction from the robot centre or to the first occupied cell in the D neighbourhood of the robot, and the next target is then recalculated using the newly recorded point cloud."}, {"heading": "D. End of the PE phase", "text": "The end of the PE phase corresponds to the robot closing a loop around the structure. Therefore, it is necessary for the vSLAM module to detect a loop closure based on the captured images, i.e. detect that the robot has returned to a known point. Until this condition is met, the robot continues on the PE path. This task is not necessarily easy due to localization errors, especially the drift that accumulates in dead ends such as the visual odometry of the vSLAM module or the wheel odometry system. If present, absolute position sensors, such as a GPS receiver, can indirectly contribute to improving loop closure detection in the event of field operations. It is also possible to use the measurements of a compass to determine when the robot is traveling along an edge of the structure with the same orientation as the starting edge, and focus the search for a loop closure along these edges."}, {"heading": "IV. COMPLETING THE MODEL: CAVITY EXPLORATION", "text": "As mentioned in Section II, these holes could be due to sensor constraints or local occlusions caused by small irregularities in the structure itself, and should be filled with a more suitable accessible space by means of a platform, so we will not look at them any further. Type II errors, hereinafter referred to as cavities, correspond to regions skipped during the PE phase, particularly due to the situation described in Figure 7. These cavities are filled during the CE phase, during which the robot is allowed to move closer to the structure, although this means that in some places the model is not necessarily reconstructed up to a Hmax height."}, {"heading": "A. Cavity Entrances", "text": "In this subsection, we describe an algorithm for determining the positions of the inputs displayed in yellow in the model. Furthermore, this algorithm is used by the CE strategy. We use a voxel-based 3D occupation grid constructed by the global point cloud and held in a hierarchical tree structure. [32] For this, we need the vSLAM module to determine the order of the point clouds and the associated estimated camera positions used in mounting the current model. All voxels in the occupation grid that are not designated as free or occupied are referred to as unknown. With the constructed OctoMap, we compose a series of voxels whose definition is adapted by [9]."}, {"heading": "B. Cavity Exploration", "text": "We explore each detected cavity with a motion similar to the PE phase, maintaining the 7 structure on the right at a distance. [\u03b4, D], which is determined online based on the available distance in the cavity and is the minimum distance for the mapping module. To do this, we need a starting point for each cavity entrance and an algorithm for calculation. The starting point is chosen from the series of camera poses returned by the vSLAM module during the PE phase, so that the centroid of the cavity entrance lies within the view frustum. Additionally, the centroid structure should not be captured by the structure from the camera position. From these camera positions, the one with the earliest time stamp is chosen as the starting point, see Fig. 11. The time stamps of the starting points of the inputs of the cavities are used to sort them."}, {"heading": "V. COVERAGE ANALYSIS", "text": "To simplify the discussion, we focus on the case of simple structures consisting of vertical walls that may have hanging structures under which the mobile robot is able to keep the robot at a distance D from M, in other words, the path of the robot remains at the plane zg = hc, see Fig. 4.First, we analyze the structure on its right side. Note that MD keeps the structure on its right side and a closed disk of the robot at a distance D from M, in other words, the path of the robot on the border remains defined in Section III-B. Note that MD keeps the structure on its right side. Note that MD keeps the structure on the right side of M and a closed disk of the robot D.Lemma 1: The path followed by the robot during the PE phase cannot overlap itself, except at the starting point Og.Proof: During the PE phase, the robot keeps the structure at a distance while it moves forward."}, {"heading": "VI. SIMULATION RESULTS", "text": "We illustrate the behavior of our strategies through 3D simulations of various structure sizes, camera ranges and accuracy levels of robot localization. We implement our motion planning policies using the Robot Operating System (ROS) Navigation Stack supported by many mobile ground robots [40]. All simulations are performed using the Gazebo Simulator [27], the vSLAM algorithm used is RTAB Map [6]. Simulations are performed using publicly available models of a Clearpath Husky A200 robot and a Kinect depth sensor whose reach can be varied [41], see Fig. 2. An UR5 robot arm is used to carry the sensor, but only yaw movements of the arm are allowed, as described in Section II. To illustrate, we consider artificial structures from 9short wall-like segments. We refer to the structure used in most of the previous illustrations as the Small-Mapping model, but the Small-Maker model has also been used as the Small-Mapping model."}, {"heading": "A. Structure Size and Camera Range", "text": "The relative size of the structure in relation to the range of the camera influences the trajectory determined by our algorithms. Fig. 14 shows simulation results for 4 scenarios. At a camera range of 4.5 m, the Large model is fully mapped at the end of the PE phase. In the Small model, a cavity remains, which is subsequently explored during the CE phase. Increasing the camera range to a whopping 12 m makes it possible to map the Small structure even at the end of the PE phase, see Fig. 14b. Fig. 14d shows that the robot, which follows our guidelines, is capable of mapping large structures with several cavities of different sizes. Table I lists the route lengths obtained for the various test cases."}, {"heading": "B. Localization accuracy", "text": "The Husky robot combines data from an inertial measurement unit (IMU), a standard GPS receiver, and wheel odometry to achieve a relatively small localization error overall. To evaluate the effects of localization accuracy on our algorithms, we simulate the effects of large wheel slides by introducing an additive Gaussian noise with a variance equal to k (vx + \u03c9z) / 2 for each of the wheel encoder measurements, where vx is the linear speed of the robot, \u03c9z is its yaw rate, and k is a proportionality constant, hereinafter referred to as the noise level. Increasing k leads to poorer orientation of the point clouds, but all parts of the structure, with the exception of the horizontal surfaces, are still captured in the reconstructed model, see Fig. 15. Note that our guidelines calculate the nearest waypoint at discrete times and therefore assume that the gradient in the next waypoint between the camera is sufficient to reach the wavelength point."}, {"heading": "C. Comparing with Frontier-based Exploration", "text": "The Frontier Exploration [44] package available in ROS relies on a 2D LIDAR to build a allocation grid used to calculate the boundaries. It requires the user to define a 2D polygon that surrounds the structure, and the algorithm then explores until there are no further boundaries within the custom polygon. Compared to the FBE algorithm, our algorithms 1) are not needed to obtain a custom coverage of the structure (during the PE phase), ensuring that all parts are assigned up to a height of Hmax; 3) forms the structure while remaining on the right, which can be important to understand the robot's behavior."}, {"heading": "VII. REAL-WORLD EXPERIMENT", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "VIII. CONCLUSIONS", "text": "This paper presents novel motion planning guidelines that allow a mobile ground robot carrying a camera or depth sensor to autonomously explore the visible part of a limited three-dimensional structure. The proposed guidelines do not require prior information about the size or geometry of the structure. Combined with state-of-the-art vSLAM systems, our strategies are able to achieve high coverage in the reconstructed model given the physical limitations of the platform. We demonstrate the effectiveness of our approach using 3D simulations for different structure sizes, camera range and localization accuracy, and we have tested our system in real-world experiments. In addition, a comparison of our policies with classical boundary-based exploration algorithms clearly shows improvements in performance for a realistic structure like a house."}], "references": [{"title": "Autonomous inspection of underwater structures", "author": ["M. Jacobi"], "venue": "Robotics and Autonomous Systems, vol. 67, no. C, pp. 80\u201386, May 2015, special issue on Advances in Autonomous Underwater Robotics.  12", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Detailed 3D reconstruction of large-scale heritage sites with integrated techniques", "author": ["S.F. El-Hakim", "J.A. Beraldin", "M. Picard", "G. Godin"], "venue": "IEEE Computer Graphics and Applications, vol. 24, no. 3, pp. 21\u201329, May-June 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Automated progress tracking using 4D schedule and 3D sensing technologies", "author": ["Y. Turkan", "F. Bosche", "C.T. Haas", "R. Haas"], "venue": "Automation in Construction, vol. 22, pp. 414\u2013421, March 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Progressive 3D reconstruction of infrastructure with videogrammetry", "author": ["I. Brilakis", "H. Fathib", "A. Rashidi"], "venue": "Automation in Construction, vol. 20, no. 7, pp. 884\u2013895, November 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A framework for model-driven acquisition and analytics of visual data using UAVs for automated construction progress monitoring", "author": ["J. Lin", "K. Han", "M. Golparvar-Fard"], "venue": "Computing in Civil Engineering, Austin, Texas, June 2015, pp. 156\u2013164.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Online Global Loop Closure Detection for Large-Scale Multi-Session Graph-Based SLAM", "author": ["M. Labbe", "F. Michaud"], "venue": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Chicago, IL, September 2014, pp. 2661\u20132666.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "3-D mapping with an RGB-D camera", "author": ["F. Endres", "J. Hess", "J. Sturm", "D. Cremers", "W. Burgard"], "venue": "IEEE Transactions on Robotics, vol. 30, no. 1, pp. 177\u2013187, February 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Active perception", "author": ["R. Bajcsy"], "venue": "Proceedings of the IEEE, vol. 76, no. 8, pp. 966\u20131005, August 1988.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1988}, {"title": "A frontier-based approach for autonomous exploration", "author": ["B. Yamauchi"], "venue": "Proceedings of the 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation, ser. CIRA \u201997. Washington, DC, USA: IEEE Computer Society, 1997, pp. 146\u2013151.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Efficient next-bestscan planning for autonomous 3D surface reconstruction of unknown objects", "author": ["S. Kriegel", "C. Rink", "T. Bodenm\u00fcller", "M. Suppa"], "venue": "Journal of Real-Time Image Processing, pp. 611\u2013631, December 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Autonomous generation of complete 3D object models using next best view manipulation planning", "author": ["M. Krainin", "B. Curless", "D. Fox"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Shanghai, China, May 2011, pp. 5031\u20135037.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Structural inspection path planning via iterative viewpoint resampling with application to aerial robotics", "author": ["A. Bircher", "K. Alexis", "M. Burri", "P. Oettershagen", "S. Omari", "T. Mantel", "R. Siegwart"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Seattle, Washington, May 2015, pp. 6423\u20136430.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Sampling-based coverage path planning for inspection of complex structures", "author": ["B. Englot", "F.S. Hover"], "venue": "International Conference on Automated Planning and Scheduling (ICAPS), Sau Paulo, Brazil, June 2012, pp. 29\u201337.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Autonomous exploration for infrastructure modeling with a micro aerial vehicle", "author": ["L. Yoder", "S. Scherer"], "venue": "Proceedings of the Field and Service Robotics Conference, June 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Navigating a miniature crawler robot for engineered structure inspection", "author": ["W. Sheng", "H. Chen", "N. Xi"], "venue": "IEEE Transactions on Automation Science and Engineering, vol. 5, no. 2, pp. 368\u2013373, April 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "A terrain-covering algorithm for an AUV", "author": ["S. Hert", "S. Tiwari", "V. Lumelsky"], "venue": "Autonomous Robots, vol. 3, no. 2, pp. 91\u2013119, June 1996.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1996}, {"title": "Morse decompositions for coverage tasks", "author": ["E.U. Acar", "H. Choset", "A.A. Rizzi", "P.N. Atkar", "D. Hull"], "venue": "The International Journal of Robotics Research, vol. 21, no. 4, pp. 331\u2013344, 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Sensor-based coverage of unknown environments: Incremental construction of Morse decompositions", "author": ["E.U. Acar", "H. Choset"], "venue": "The International Journal of Robotics Research, vol. 21, no. 4, pp. 345\u2013 366, 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "A robotic crack inspection and mapping system for bridge deck maintenance", "author": ["R. Lim", "H.M. La", "W. Sheng"], "venue": "IEEE Transactions on Automation Science and Engineering, vol. 11, no. 2, pp. 367\u2013378, April 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling the world from internet photo collections", "author": ["N. Snavely", "S.M. Seitz", "R. Szeliski"], "venue": "International Journal of Computer Vision, vol. 80, no. 2, pp. 189\u2013210, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Building Rome on a cloudless day", "author": ["J.-M. Frahm", "P. Fite-Georgel", "D. Gallup", "T. Johnson", "R. Raguram", "C. Wu", "Y.-H. Jen", "E. Dunn", "B. Clipp", "S. Lazebnik", "M. Pollefeys"], "venue": "Proceedings of the 11th European Conference on Computer Vision: Part IV, ser. ECCV\u201910. Berlin, Heidelberg: Springer-Verlag, 2010, pp. 368\u2013381.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "VisualSFM: A visual structure from motion system", "author": ["C. Wu"], "venue": "http: //ccwu.me/vsfm/", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Accurate, dense, and robust multiview stereopsis", "author": ["Y. Furukawa", "J. Ponce"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 8, pp. 1362\u20131376, Aug 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Building with drones: Accurate 3D facade reconstruction using MAVs", "author": ["S. Daftry", "C. Hoppe", "H. Bischof"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Seattle, WA, May 2015, pp. 3487\u20133494.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Photocity: Training experts at large-scale image acquisition through a competitive game", "author": ["K. Tuite", "N. Snavely", "D.-y. Hsiao", "N. Tabing", "Z. Popovic"], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2011, pp. 1383\u20131392. [Online]. Available: http://doi.acm.org/10.1145/1978942.1979146", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "g2o: A general framework for graph optimization", "author": ["R. Kuemmerle", "G. Grisetti", "H. Strasdat", "K. Konolige", "W. Burgard"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Shanghai, China, May 2011, pp. 3607\u20133613.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Choosing where to go: Complete 3D exploration with stereo", "author": ["R. Shade", "P. Newman"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Shanghai, China, May 2011, pp. 2806\u20132811.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Autonomous indoor 3D exploration with a micro-aerial vehicle", "author": ["S. Shen", "N. Michael", "V. Kumar"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Seattle, Washington, May 2012, pp. 9\u201315.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient visual exploration and coverage with a micro aerial vehicle in unknown environments", "author": ["L. Heng", "A. Gotovos", "A. Krause", "M. Pollefeys"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Seattle, Washington, May 2015, pp. 1071\u20131078.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Decentralized active information acquisition: Theory and application to multi-robot SLAM", "author": ["N. Atanasov", "J. Le Ny", "K. Daniilidis", "G.J. Pappas"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Seattle, Washington, May 2015, pp. 4775\u20134782.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "OctoMap: An efficient probabilistic 3D mapping framework based on octrees", "author": ["A. Hornung", "K.M. Wurm", "M. Bennewitz", "C. Stachniss", "W. Burgard"], "venue": "Autonomous Robots, vol. 34, no. 3, pp. 189\u2013206, April 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Self-supervised learning to visually detect terrain surfaces for autonomous robots operating in forested terrain", "author": ["S. Zhou", "J. Xi", "M.W. McDaniel", "T. Nishihata", "P. Salesses", "K. Iagnemma"], "venue": "Journal of Field Robotics, vol. 29, no. 2, pp. 277\u2013297, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "A frontier-void-based approach for autonomous exploration in 3D", "author": ["C. Dornhege", "A. Kleiner"], "venue": "Advanced Robotics, vol. 27, pp. 459\u2013468, 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "3D is here: Point cloud library (PCL)", "author": ["R.B. Rusu", "S. Cousins"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Shanghai, China, May 2011, pp. 1\u20134.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimating surface normals in noisy point cloud data", "author": ["N.J. Mitra", "A. Nguyen", "L. Guibas"], "venue": "International Journal of Computational Geometry & Applications, vol. 14, no. 04n05, pp. 261\u2013276, 2004.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Real-time obstacle avoidance for manipulators and mobile robots", "author": ["O. Khatib"], "venue": "The International Journal of Robotics Research, vol. 5, no. 1, pp. 90\u201398, 1986.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1986}, {"title": "Principles of Robot Motion: Theory, Algorithms, and Implementations", "author": ["H. Choset", "K.M. Lynch", "S. Hutchinson", "G.A. Kantor", "W. Burgard", "L.E. Kavraki", "S. Thrun"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "Discontinuous dynamical systems", "author": ["J. Cortes"], "venue": "IEEE Control Systems Magazine, vol. 28, no. 3, pp. 36\u201373, June 2008.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient variants of the ICP algorithm", "author": ["S. Rusinkiewicz", "M. Levoy"], "venue": "Proceedings of the Third International Conference on 3-D Digital Imaging and Modeling, 2001, Quebec City, Canada, May 2001, pp. 145\u2013 152.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2001}, {"title": "A generalized extended Kalman filter implementation for the robot operating system", "author": ["T. Moore", "D. Stouch"], "venue": "Proceedings of the 13th International Conference on Intelligent Autonomous Systems (IAS-13). Springer, July 2014.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "sites [1], [2].", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "sites [1], [2].", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": ", positioning systems, stationary 3D laser scanners [3], high resolution video cameras [4], or still cameras carried by UAVs [5].", "startOffset": 52, "endOffset": 55}, {"referenceID": 3, "context": ", positioning systems, stationary 3D laser scanners [3], high resolution video cameras [4], or still cameras carried by UAVs [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": ", positioning systems, stationary 3D laser scanners [3], high resolution video cameras [4], or still cameras carried by UAVs [5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "This is a widely researched problem called Visual Simultaneous Localization and Mapping (vSLAM) or real-time Structure from Motion (SfM), for which several open source packages offer increasingly accurate and efficient solutions [6], [7].", "startOffset": 229, "endOffset": 232}, {"referenceID": 6, "context": "This is a widely researched problem called Visual Simultaneous Localization and Mapping (vSLAM) or real-time Structure from Motion (SfM), for which several open source packages offer increasingly accurate and efficient solutions [6], [7].", "startOffset": 234, "endOffset": 237}, {"referenceID": 7, "context": "The second problem relates to active sensing [8], as we need motion planning strategies that can guide a mobile sensor to explore the structure of interest.", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "For mapping, monitoring or inspection applications, certain classical strategies such as frontier-based exploration algorithms [9], which guide the robot to previously unexplored regions irrespective of whether it is part of the structure of interest or not, are not necessarily well adapted.", "startOffset": 127, "endOffset": 130}, {"referenceID": 9, "context": "Some recent work considers the problem of reconstructing a 3D model of arbitrary objects by moving a depth sensor relative to the object using different forms of next best view planning algorithms [10], [11].", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "Some recent work considers the problem of reconstructing a 3D model of arbitrary objects by moving a depth sensor relative to the object using different forms of next best view planning algorithms [10], [11].", "startOffset": 203, "endOffset": 207}, {"referenceID": 4, "context": "The related problem of automated inspection deals with large structures such as tall buildings [5] and ship hulls.", "startOffset": 95, "endOffset": 98}, {"referenceID": 11, "context": "[12] assume that a prior 3D mesh of the structure to inspect is available and compute a short path connecting viewpoints that together are guaranteed", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In [13], Englot et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Yoder and Scherer [14] also assume a bounding box and develop an algorithm combining next best view planning and frontierbased exploration to encourage coverage of the structure.", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "[15] use a prior CAD model of an aircraft to plan a path for a robotic crawler such that it inspects all the rivets on the surface of the aircraft.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": ", [16]\u2013[19] and the references therein, which has traditionally focused on developing algorithms ensuring that a mobile robot passes over all points in a 2D environment, assuming a sufficiently accurate localization system.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ", [16]\u2013[19] and the references therein, which has traditionally focused on developing algorithms ensuring that a mobile robot passes over all points in a 2D environment, assuming a sufficiently accurate localization system.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "In computer vision and photogrammetry, SfM techniques aim at building a 3D model of a scene from a large number of images [20]\u2013[23], but most of this work focuses on batch post-processing and typically assumes a given dataset, whereas here our focus is essentially on how to acquire an appropriate set of images.", "startOffset": 122, "endOffset": 126}, {"referenceID": 22, "context": "In computer vision and photogrammetry, SfM techniques aim at building a 3D model of a scene from a large number of images [20]\u2013[23], but most of this work focuses on batch post-processing and typically assumes a given dataset, whereas here our focus is essentially on how to acquire an appropriate set of images.", "startOffset": 127, "endOffset": 131}, {"referenceID": 23, "context": "[24], which presents an interactive real-time SfM system providing online feedback to the user taking pictures, alerting him or her when a new picture cannot be properly integrated in the model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] develop a competitive game where players are encouraged to take pictures that help build complete 3D models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We emphasize that we do not discuss in details the task of actually building a model from a collection of pictures or depth maps, which can be executed by one of the available vSLAM or real-time SfM systems, such as the Real-Time Appearance Based Mapping package (RTAB-Map) [6] that we use in our simulations.", "startOffset": 274, "endOffset": 277}, {"referenceID": 25, "context": "Which package we use for model reconstruction has little influence on our algorithms, for example any vSLAM system based on pose-graph optimization [26] could be used.", "startOffset": 148, "endOffset": 152}, {"referenceID": 26, "context": "Finally, another line of work in informative path planning relates to autonomous exploration and coverage of relatively large environments, using variants of frontier based exploration algorithms for example [28]\u2013[31].", "startOffset": 208, "endOffset": 212}, {"referenceID": 29, "context": "Finally, another line of work in informative path planning relates to autonomous exploration and coverage of relatively large environments, using variants of frontier based exploration algorithms for example [28]\u2013[31].", "startOffset": 213, "endOffset": 217}, {"referenceID": 5, "context": "in a coordinate frame in real-time using available SLAM algorithms, such as RTAB-Map [6] or RGBD-SLAM [7], and post-processing then allows us to build a dense 3D model or a 3D occupancy grid stored in an OctoMap [32].", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "in a coordinate frame in real-time using available SLAM algorithms, such as RTAB-Map [6] or RGBD-SLAM [7], and post-processing then allows us to build a dense 3D model or a 3D occupancy grid stored in an OctoMap [32].", "startOffset": 102, "endOffset": 105}, {"referenceID": 30, "context": "in a coordinate frame in real-time using available SLAM algorithms, such as RTAB-Map [6] or RGBD-SLAM [7], and post-processing then allows us to build a dense 3D model or a 3D occupancy grid stored in an OctoMap [32].", "startOffset": 212, "endOffset": 216}, {"referenceID": 31, "context": "Assumption 3 could be removed by using recent classification systems that can differentiate between ground and non-ground regions [33] to pre-process the point clouds before sending them to our system.", "startOffset": 130, "endOffset": 134}, {"referenceID": 9, "context": "robotic arm carrying the sensor [10], [34], as well as targeted computer vision techniques [23].", "startOffset": 32, "endOffset": 36}, {"referenceID": 32, "context": "robotic arm carrying the sensor [10], [34], as well as targeted computer vision techniques [23].", "startOffset": 38, "endOffset": 42}, {"referenceID": 22, "context": "robotic arm carrying the sensor [10], [34], as well as targeted computer vision techniques [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 33, "context": "For its implementation we rely on the Point Cloud Library (PCL) [35].", "startOffset": 64, "endOffset": 68}, {"referenceID": 34, "context": "On line 5, following [36], we compute via Principal Component Analysis (PCA) the normal direction to that plane \u03a0 which best fits S.", "startOffset": 21, "endOffset": 25}, {"referenceID": 35, "context": "along the way, we use a local path planner based on potential fields [37], [38].", "startOffset": 69, "endOffset": 73}, {"referenceID": 36, "context": "along the way, we use a local path planner based on potential fields [37], [38].", "startOffset": 75, "endOffset": 79}, {"referenceID": 37, "context": "The path will then slide on \u2202MD until it reaches its goal [39].", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "We use a voxel based 3D occupancy grid constructed from the global point cloud, and maintained in a hierarchical tree data structure by the OctoMap [32] library.", "startOffset": 148, "endOffset": 152}, {"referenceID": 8, "context": "Using the constructed OctoMap, we compute a set of frontier voxels, whose definition is adapted from [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 34, "context": "We can ignore them by only considering frontier voxels for which the normal vector n, computed using the nearby frontier voxels [36], makes a sufficiently small angle with the horizontal plane.", "startOffset": 128, "endOffset": 132}, {"referenceID": 33, "context": "The cavity entrance voxels are clustered using an Euclidean clustering algorithm from PCL [35] and each cluster is referred to as a cavity entrance.", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "The vSLAM algorithm used is RTAB-Map [6].", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "We also illustrate the effectiveness of our policy for a realistic model of a house, and compare its performance with that of the classic Frontier-Based Exploration (FBE) algorithm [9].", "startOffset": 181, "endOffset": 184}, {"referenceID": 38, "context": "First, we register Ck to CR using an Iterative Closest Point (ICP) algorithm [43].", "startOffset": 77, "endOffset": 81}, {"referenceID": 39, "context": "For odometry we use the extended Kalman filter (EKF) from [45] with IMU and wheel odometry data as inputs.", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "The output of the EKF is sent to RTAB-Map [6] for mapping and localization purposes.", "startOffset": 42, "endOffset": 45}], "year": 2017, "abstractText": "This paper presents a strategy to guide a mobile ground robot equipped with a camera or depth sensor, in order to autonomously map the visible part of a bounded threedimensional structure. We describe motion planning algorithms that determine appropriate successive viewpoints and attempt to fill holes automatically in a point cloud produced by the sensing and perception layer. The emphasis is on accurately reconstructing a 3D model of a structure of moderate size rather than mapping large open environments, with applications for example in architecture, construction and inspection. The proposed algorithms do not require any initialization in the form of a mesh model or a bounding box, and the paths generated are well adapted to situations where the vision sensor is used simultaneously for mapping and for localizing the robot, in the absence of additional absolute positioning system. We analyze the coverage properties of our policy, and compare its performance to the classic frontier based exploration algorithm. We illustrate its efficacy for different structure sizes, levels of localization accuracy and range of the depth sensor, and validate our design on a realworld experiment. Note to Practitioners\u2014 The objective of this work is to automate the process of building a 3D model of a structure of interest that is as complete as possible, using a mobile camera or depth sensor, in the absence of any prior information about this structure. Given that increasingly robust solutions for the Visual Simultaneous Localization and Mapping problem (vSLAM) are now readily available, the key challenge that we address here is to develop motion planning policies to control the trajectory of the sensor in a way that improves the mapping performance. We target in particular scenarios were no external absolute positioning system is available, such as mapping certain indoor environments where GPS signals are blocked. In this case, it is often important to revisit previously seen locations relatively quickly, in order to avoid excessive drift in the dead-reckoning localization system. Our system works by first determining the boundaries of the structure, before attempting to fill the holes in the constructed model. Its performance is illustrated through simulations and a real-world experiment performed with a depth sensor carried by a mobile manipulator.", "creator": "LaTeX with hyperref package"}}}