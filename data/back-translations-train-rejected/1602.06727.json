{"id": "1602.06727", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "Improving Trajectory Modelling for DNN-based Speech Synthesis by using Stacked Bottleneck Features and Minimum Generation Error Training", "abstract": "We propose two novel techniques --- stacking bottleneck features and minimum trajectory error training criterion --- to improve the performance of deep neural network (DNN)-based speech synthesis. The techniques address the related issues of frame-by-frame independence and ignorance of the relationship between static and dynamic features, within current typical DNN-based synthesis frameworks. Stacking bottleneck features, which are an acoustically--informed linguistic representation, provides an efficient way to include more detailed linguistic context at the input. The proposed minimum trajectory error training criterion minimises overall output trajectory error across an utterance, rather than minimising the error per frame independently, and thus takes into account the interaction between static and dynamic features. The two techniques can be easily combined to further improve performance. We present both objective and subjective results that demonstrate the effectiveness of the proposed techniques. The subjective results show that combining the two techniques leads to significantly more natural synthetic speech than from conventional DNN or long short-term memory (LSTM) recurrent neural network (RNN) systems.", "histories": [["v1", "Mon, 22 Feb 2016 11:11:04 GMT  (192kb,D)", "https://arxiv.org/abs/1602.06727v1", "submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing 2016 (Under second round review)"], ["v2", "Mon, 4 Apr 2016 11:18:07 GMT  (199kb,D)", "http://arxiv.org/abs/1602.06727v2", "submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing 2016 (AQ)"], ["v3", "Tue, 5 Apr 2016 11:31:02 GMT  (199kb,D)", "http://arxiv.org/abs/1602.06727v3", "submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing 2016 (AQ)"]], "COMMENTS": "submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing 2016 (Under second round review)", "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.NE", "authors": ["zhizheng wu", "simon king"], "accepted": false, "id": "1602.06727"}, "pdf": {"name": "1602.06727.pdf", "metadata": {"source": "CRF", "title": "Improving Trajectory Modelling for DNN-based Speech Synthesis by using Stacked Bottleneck Features and Minimum Generation Error Training", "authors": ["Zhizheng Wu"], "emails": ["mon.king}@ed.ac.uk"], "sections": [{"heading": null, "text": "INTRODUCTIONStatistical Parametric Language Synthesis (SPSS) [1] has evolved particularly rapidly over the last decade, as evidenced by the annual Blizzard challenges [2], and is capable of producing highly intelligible synthetic language with acceptable naturalness. Although it offers greater flexibility than the other common unit selection technique [3], the naturalness of the language generated by SPSS is still too low. There are many underlying factors, and acoustic modelling is a key factor, as discussed in [1].The task of modelling the complex relationship between linguistic representations resulting from text input and acoustic characteristics computed from speech waveforms is, of course, very difficult. In this paper, we propose two novel techniques to improve this acoustic modelling, both aimed at improving the modelling of natural language temporal characteristics."}, {"heading": "A. Related work", "text": "This year it is more than ever before."}, {"heading": "B. Contributions of this work", "text": "In fact, most of them are able to survive on their own."}, {"heading": "II. PROBLEM STATEMENT", "text": "To put our proposed methods into context, we will briefly consider DNN-based speech synthesis and discuss the limitations of typical DNNs for speech synthesis that address our proposed methods."}, {"heading": "A. DNN-based speech synthesis", "text": "DNN-based speech synthesis includes offline training and runtime generation phases. During the training, a DNN learns the complex relationship between input linguistic features xt and the corresponding output acoustic characteristics ot: ot = F (xt) + e, (1) where F (\u00b7) is the mapping function realized by the trained DNN, and e is the modeling error. Normally, the acoustic characteristics of the acoustic characteristics will not consist of static characteristics ct, also called vocoder parameters, and the corresponding dynamic characteristics realized by the trained DNN. (2) Dynamic characteristics are used as constraints to generate smooth parameters during generation. Dynamic characteristics are calculated from the sequence of static characteristics. Hence, a sequence of observed acoustic characteristics, was published in [34]."}, {"heading": "B. Limitations", "text": "Although DNN's significant improvements over HMMs for speech synthesis have been reported, as we have examined in Section I-A, there are at least two limitations in current DNN implementations: 1) Frame-by-frame independence: The acoustic characteristics of each frame are predicted based on the linguistic characteristics of that frame, without any contextual limitations other than those encoded in the linguistic characteristics. The acoustic context that is so important for the language is not explicitly taken into account during the training or in the reproductive step of the generation.2) Neglect of the relationship between static and dynamic characteristics: During the generation process, dynamic characteristics are used by the MLPG algorithm to create smooth parameter gradients. However, it is based on potentially inconsistent static and dynamic characteristics predicted by the DNN. It should be beneficial to incorporate the dynamic characteristics constraints of the training during the IV, which we will now incorporate the two techniques into the IV."}, {"heading": "III. STACKED BOTTLENECK FEATURES", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "IV. MINIMUM GENERATION ERROR TRAINING CRITERION", "text": "The second contribution of this paper is a novel training criterion, whereas conventional DNNs treat the dynamic characteristics of SPSS no differently from static characteristics. The basic idea of the MGE criterion is to minimize the error of the output after MLPG; i.e., the vocoder parameter trajectories that are actually used to reconstruct the speech waveform to minimize the error of the output after MLPG; i.e., the vocoder parameter trajectories used to reconstruct the speech waveform."}, {"heading": "V. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Experimental setups", "text": "In fact, it is so that most of them are able to keep to the rules which they have imposed on themselves. (...) It is not so that they are able to understand the rules. (...) It is not so that they keep to the rules. (...) It is not so that they keep to the rules. (...) It is as if they keep to the rules. (...). \"\" It is as if they keep to the rules. \"(...)\" It is not as if they keep to the rules. \"(...).\" (...). \"(...).\" (...). \"(.\" (.). \"(.).\" (.). \"(.).\" (.). (.). (.). (.). (.). (.). \"(.). (.).\" (.). \"(...).\" (. \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.\" (. \"(.).\" (.). \"(.).). (.\" (.). \"(.).\" (. \"(.). (.). (.\" (.).). (. \"(.). (. (.). (.).\" (.). \"(.). (. (.). (.).). (.).\" (. (.). \"(. (.).).\" (.).). \"(.).\" (. (.).). (. (.).). \"(. (.).). (.).). (. (."}, {"heading": "B. Objective evaluation", "text": "This year, more than ever before in the history of the city, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country,"}, {"heading": "C. Subjective evaluation", "text": "In fact, it is the case that we are able to get to grips with the problems mentioned in order to solve them."}, {"heading": "VI. DISCUSSION", "text": "Our experimental results confirm the effectiveness of both proposed techniques as ways to incorporate contextual constraints into neural network-based speech synthesis. Although LSTM-based recurring neural networks can be an elegant way to incorporate time constraints, there are at least two reasons to choose our proposed framework to combine stacked bottlenecks with MGE training.First, the proposed framework can very easily draw on data outside the scope: the bottleneck network can be trained with relatively poor speech quality from multiple speakers. Since thousands of hours of voice data are available for the training of speech recognition systems, this data could also be used to train the bottleneck network.Using such large amounts of data could further improve the results presented here, using dozens of hours of data to train the bottleneck network.Second, although the total training time of the proposed framework is close to the time of the LSTM-based system's proposed 14x2 seconds, the complexity of the proposed framework was lower than the time of the 14x2 seconds."}, {"heading": "VII. CONCLUSION", "text": "We propose two techniques to improve the performance of DNN-based speech synthesis: stacked bottlenecks and minimum error training; the two techniques can easily be combined into a single DNN speech synthesis framework; this novel framework allows us to benefit from additional extracurricular data to improve synthesis performance; and b) incorporate contextual limitations without greatly increasing computer complexity in synthesis time. Both objective and subjective results confirm the effectiveness of the proposed system with respect to DNN and LSTM baselines. To summarize the main findings: \u2022 Stacking bottlenecks provides an effective way to include contextual limitations."}], "references": [{"title": "Statistical parametric speech synthesis", "author": ["H. Zen", "K. Tokuda", "A.W. Black"], "venue": "Speech Communication, vol. 51, no. 11, pp. 1039\u20131064, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Measuring a decade of progress in text-to-speech", "author": ["S. King"], "venue": "Loquens, vol. 1, no. 1, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Unit selection in a concatenative speech synthesis system using a large speech database", "author": ["A.J. Hunt", "A.W. Black"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), vol. 1, 1996, pp. 373\u2013376.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Minimum generation error training for HMM-based speech synthesis", "author": ["Y.-J. Wu", "R.-H. Wang"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2006, pp. 89\u201392.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Reformulating the HMM as a trajectory model by imposing explicit relationships between static and dynamic feature vector sequences", "author": ["H. Zen", "K. Tokuda", "T. Kitamura"], "venue": "Computer Speech & Language, vol. 21, no. 1, pp. 153\u2013173, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "A speech parameter generation algorithm considering global variance for HMM-based speech synthesis", "author": ["T. Toda", "K. Tokuda"], "venue": "IEICE Transactions on Information and Systems, vol. 90, no. 5, pp. 816\u2013824, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "A postfilter to modify the modulation spectrum in HMM-based speech synthesis", "author": ["S. Takamichi", "T. Toda", "G. Neubig", "S. Sakti", "S. Nakamura"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2014, pp. 290\u2013294.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Attributing modelling errors in HMM synthesis by stepping gradually from natural to modelled speech", "author": ["T. Merritt", "J. Latorre", "S. King"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015, pp. 4220\u20134224.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning for acoustic modeling in parametric speech generation: A systematic review of existing techniques and future trends", "author": ["Z.-H. Ling", "S.-Y. Kang", "H. Zen", "A. Senior", "M. Schuster", "X.-J. Qian", "H.M. Meng", "L. Deng"], "venue": "IEEE Signal Processing Magazine, vol. 32, no. 3, pp. 35\u201352, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "A high quality text-to-speech system composed of multiple neural networks", "author": ["O. Karaali", "G. Corrigan", "N. Massey", "C. Miller", "O. Schnurr", "A. Mackie"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), vol. 2, 1998, pp. 1237\u20131240.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Speech synthesis with artificial neural networks", "author": ["T. Weijters", "J. Thole"], "venue": "Proc. Int. Conf. on Neural Networks, 1993, pp. 1764\u20131769.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "LSP speech synthesis using backpropagation networks", "author": ["G. Cawley", "P. Noakes"], "venue": "Proc. Third Int. Conf. on Artificial Neural Networks, 1993, pp. 291\u2013294.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "Speech synthesis using artificial neural networks trained on cepstral coefficients.", "author": ["C. Tuerk", "T. Robinson"], "venue": "in Proc. European Conference on Speech Communication and Technology (Eurospeech),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "A neural-network-based model of segmental duration for speech synthesis", "author": ["M. Riedi"], "venue": "Proc. European Conference on Speech Communication and Technology (Eurospeech), 1995, pp. 599\u2013602.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Modeling spectral envelopes using Restricted Boltzmann Machines and Deep Belief Networks for statistical parametric speech synthesis", "author": ["Z.-H. Ling", "L. Deng", "D. Yu"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 10, pp. 2129\u20132139, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-distribution deep belief network for speech synthesis", "author": ["S. Kang", "X. Qian", "H. Meng"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013, pp. 8012\u20138016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Statistical parametric speech synthesis using weighted multi-distribution deep belief network", "author": ["S. Kang", "H. Meng"], "venue": "Proc. Interspeech, 2014, pp. 1959\u20131963.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis", "author": ["H. Zen", "A. Senior"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2014, pp. 3844\u20133848.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Modelling acoustic feature dependencies with artificial neural networks: Trajectory-rnade", "author": ["B. Uria", "I. Murray", "S. Renals", "C. Valentini"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015, pp. 4465\u20134469.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "A deep generative architecture for postfiltering in statistical parametric speech synthesis", "author": ["L.-H. Chen", "T. Raitio", "C. Valentini-Botinhao", "Z.-H. Ling", "J. Yamagishi"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 23, no. 11, pp. 2003\u20132014, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Statistical parametric speech synthesis using deep neural networks", "author": ["H. Zen", "A. Senior", "M. Schuster"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013, pp. 7962\u2013 7966.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining a vector space representation of linguistic context with a deep neural network for text-to-speech synthesis", "author": ["H. Lu", "S. King", "O. Watts"], "venue": "Proc. the 8th ISCA Speech Synthesis Workshop (SSW), pp. 281\u2013285, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "On the training aspects of deep neural network (DNN) for parametric TTS synthesis", "author": ["Y. Qian", "Y. Fan", "W. Hu", "F.K. Soong"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2014, pp. 3829\u20133833.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis", "author": ["Z. Wu", "C. Valentini-Botinhao", "O. Watts", "S. King"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015, pp. 4460\u20134464.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "From hmms to dnns: where do the improvements come from?", "author": ["O. Watts", "G.E. Henter", "T. Merritt", "Z. Wu", "S. King"], "venue": "in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "A reading list of recent advances in speech synthesis", "author": ["S. King"], "venue": "Proc. ICPhS, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards minimum perceptual error training for DNN-based speech synthesis", "author": ["C. Valentini-Botinhao", "Z. Wu", "S. King"], "venue": "Proc. Interspeech, 2015, pp. 869\u2013873.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Fusion of multiple parameterisations for DNN-based sinusoidal speech synthesis with multi-task learning", "author": ["Q. Hu", "Z. Wu", "K. Richmond", "J. Yamagishi", "Y. Stylianou", "R. Maia"], "venue": "Proc. Interspeech, 2015, pp. 854\u2013 858.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "TTS synthesis with bidirectional LSTM based recurrent neural networks", "author": ["Y. Fan", "Y. Qian", "F. Xie", "F.K. Soong"], "venue": "Proc. Interspeech, 2014, pp. 1964\u20131968.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis", "author": ["H. Zen", "H. Sak"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015, pp. 4470\u20134474.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigating gated recurrent neural networks for speech synthesis", "author": ["Z. Wu", "S. King"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "An hmm-based speech synthesis system applied to english", "author": ["K. Tokuda", "H. Zen", "A.W. Black"], "venue": "Proc. the 2002 IEEE Workshop on Speech Synthesis, 2002, pp. 227\u2013230.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Minimum trajectory error training for deep neural networks, combined with stacked bottleneck features", "author": ["Z. Wu", "S. King"], "venue": "Proc. Interspeech, 2015, pp. 309\u2013313.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence generation error (SGE) minimization based deep neural networks training for text-tospeech synthesis", "author": ["Y. Fan", "Y. Qian", "F.K. Soong", "L. He"], "venue": "Proc. Interspeech, 2015, pp. 864\u2013868.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence error (SE) minimization training of neural network for voice conversion", "author": ["F.-L. Xie", "Y. Qian", "Y. Fan", "F.K. Soong", "H. Li"], "venue": "Proc. Interspeech, 2014, pp. 2283\u20132287.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech parameter generation algorithms for HMM-based speech synthesis", "author": ["K. Tokuda", "T. Yoshimura", "T. Masuko", "T. Kobayashi", "T. Kitamura"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), vol. 3, 2000, pp. 1315\u20131318.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, vol. 323, no. 6088, pp. 533\u2013536, 1986.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1986}, {"title": "A practical guide to training restricted Boltzmann machines", "author": ["G. Hinton"], "venue": "University of Toronto, Tech. Rep., 2010.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "A perceptual study of acceleration parameters in HMM-based TTS", "author": ["Y. Chen", "Z.-J. Yan", "F.K. Soong"], "venue": "Proc. Interspeech, 2010, pp. 426\u2013 429.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "The effect of neural networks in statistical parametric speech synthesis", "author": ["K. Hashimoto", "K. Oura", "Y. Nankaku", "K. Tokuda"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2015, pp. 4455\u20134459.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimizing bottle-neck features for LVCSR", "author": ["F. Gr\u00e9zl", "P. Fousek"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2008, pp. 4729\u20134732.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "Improved bottleneck features using pretrained deep neural networks", "author": ["D. Yu", "M.L. Seltzer"], "venue": "Proc. Interspeech, 2011, pp. 237\u2013240.  SUBMITTED TO IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING 2016  11", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Auto-encoder bottleneck features using deep belief networks", "author": ["T.N. Sainath", "B. Kingsbury", "B. Ramabhadran"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2012, pp. 4153\u2013 4156.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Restructuring speech representations using a pitch-adaptive time\u2013frequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds", "author": ["H. Kawahara", "I. Masuda-Katsuse", "A. de Cheveign\u00e9"], "venue": "Speech communication, vol. 27, no. 3, pp. 187\u2013207, 1999.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1999}, {"title": "The voice bank corpus: Design, collection and data analysis of a large regional accent speech database", "author": ["C. Veaux", "J. Yamagishi", "S. King"], "venue": "Int. Conf. Oriental COCOSDA, 2013, pp. 1\u20134.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "The design for the Wall Street Journalbased CSR corpus", "author": ["D.B. Paul", "J.M. Baker"], "venue": "the Workshop on Speech and Natural Language, 1992, pp. 357\u2013362.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1992}, {"title": "Deep neural network context embeddings for model selection in rich-context HMM synthesis", "author": ["T. Merritt", "J. Yamagishi", "Z. Wu", "O. Watts", "S. King"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural network-guided unit selection synthesis", "author": ["T. Merritt", "R.A. Clark", "Z. Wu", "J. Yamagishi", "S. King"], "venue": "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2016.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Statistical parametric speech synthesis (SPSS) [1] has advanced particularly rapidly in the last decade, as seen across the annual Blizzard Challenges [2], and can produce highlyintelligible synthesised speech with acceptable naturalness.", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "Statistical parametric speech synthesis (SPSS) [1] has advanced particularly rapidly in the last decade, as seen across the annual Blizzard Challenges [2], and can produce highlyintelligible synthesised speech with acceptable naturalness.", "startOffset": 151, "endOffset": 154}, {"referenceID": 2, "context": "However, although it offers greater flexibility than the other mainstream technique of unit selection [3], the naturalness of speech generated by SPSS is still too low.", "startOffset": 102, "endOffset": 105}, {"referenceID": 0, "context": "There are many factors that underlie this, and acoustic modelling is a key one, as discussed in [1].", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "In [4], a minimum generation error training criterion was proposed to address an inconsistency between training and generation criteria, and the lack of interaction between static and dynamic features during training.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [5], the so-called trajectory HMM was proposed to explicitly model the relationships between static and dynamic features.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "As a complement to improving the acoustic model itself, enhancement techniques such as global variance [6] and modulation spectrum enhancement [7] aim to mitigate the lack of variation in generated parameter trajectories that results from using an incorrect acoustic model.", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "As a complement to improving the acoustic model itself, enhancement techniques such as global variance [6] and modulation spectrum enhancement [7] aim to mitigate the lack of variation in generated parameter trajectories that results from using an incorrect acoustic model.", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "However, none of the above techniques address what is perhaps the most fundamental problem of HMM-based speech synthesis: across-context averaging via decision tree clustering, which has been identified as a major contributing factor to reduced naturalness [8].", "startOffset": 257, "endOffset": 260}, {"referenceID": 8, "context": "More recently, following on from successes in automatic speech recognition [9], artificial neural networks have reemerged as acoustic models for SPSS [10].", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "More recently, following on from successes in automatic speech recognition [9], artificial neural networks have reemerged as acoustic models for SPSS [10].", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "By the 1990s, artificial neural networks had already been employed as feature extractors from text input to produce linguistic features [11], as acoustic models to map linguistic features to vocoder parameters [12], [13], [14], and to predict segment durations [15].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "By the 1990s, artificial neural networks had already been employed as feature extractors from text input to produce linguistic features [11], as acoustic models to map linguistic features to vocoder parameters [12], [13], [14], and to predict segment durations [15].", "startOffset": 210, "endOffset": 214}, {"referenceID": 12, "context": "By the 1990s, artificial neural networks had already been employed as feature extractors from text input to produce linguistic features [11], as acoustic models to map linguistic features to vocoder parameters [12], [13], [14], and to predict segment durations [15].", "startOffset": 216, "endOffset": 220}, {"referenceID": 13, "context": "By the 1990s, artificial neural networks had already been employed as feature extractors from text input to produce linguistic features [11], as acoustic models to map linguistic features to vocoder parameters [12], [13], [14], and to predict segment durations [15].", "startOffset": 222, "endOffset": 226}, {"referenceID": 14, "context": "By the 1990s, artificial neural networks had already been employed as feature extractors from text input to produce linguistic features [11], as acoustic models to map linguistic features to vocoder parameters [12], [13], [14], and to predict segment durations [15].", "startOffset": 261, "endOffset": 265}, {"referenceID": 15, "context": "One prominent theme in more recent studies is the use of neural architectures to replace Gaussian mixture models (GMMs) associated with leaf nodes of decision trees, such as the restricted Boltzmann machines (RBMs) in [16], where", "startOffset": 218, "endOffset": 222}, {"referenceID": 16, "context": "In [17], [18], a deep belief network (DBN) was employed as a deep generative model of the joint probability distribution between linguistic and acoustic features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [17], [18], a deep belief network (DBN) was employed as a deep generative model of the joint probability distribution between linguistic and acoustic features.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "applied to SPSS include the use of deep mixture density networks to predict probability density functions over acoustic features given the corresponding linguistic features [19] and a trajectory real-valued neural autoregressive density estimator to model acoustic parameter trajectories as well as acrossfeature dependencies [20].", "startOffset": 173, "endOffset": 177}, {"referenceID": 19, "context": "applied to SPSS include the use of deep mixture density networks to predict probability density functions over acoustic features given the corresponding linguistic features [19] and a trajectory real-valued neural autoregressive density estimator to model acoustic parameter trajectories as well as acrossfeature dependencies [20].", "startOffset": 326, "endOffset": 330}, {"referenceID": 20, "context": "plied to enhancement too, such as the deep generative model in [21] acting as a post-filter to enhance the quality of speech synthesised from an HMM-based system.", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "The most popular way to use neural networks in SPSS is with a deep feed-forward neural network (DNN) as a conditional model to map linguistic features to vocoder parameters directly [22], [23], [24], [25], [26].", "startOffset": 182, "endOffset": 186}, {"referenceID": 22, "context": "The most popular way to use neural networks in SPSS is with a deep feed-forward neural network (DNN) as a conditional model to map linguistic features to vocoder parameters directly [22], [23], [24], [25], [26].", "startOffset": 188, "endOffset": 192}, {"referenceID": 23, "context": "The most popular way to use neural networks in SPSS is with a deep feed-forward neural network (DNN) as a conditional model to map linguistic features to vocoder parameters directly [22], [23], [24], [25], [26].", "startOffset": 194, "endOffset": 198}, {"referenceID": 24, "context": "The most popular way to use neural networks in SPSS is with a deep feed-forward neural network (DNN) as a conditional model to map linguistic features to vocoder parameters directly [22], [23], [24], [25], [26].", "startOffset": 200, "endOffset": 204}, {"referenceID": 25, "context": "The most popular way to use neural networks in SPSS is with a deep feed-forward neural network (DNN) as a conditional model to map linguistic features to vocoder parameters directly [22], [23], [24], [25], [26].", "startOffset": 206, "endOffset": 210}, {"referenceID": 21, "context": "This can be viewed as replacing the decision tree used in HMM-based speech synthesis with a more powerful regression model [22], [27].", "startOffset": 123, "endOffset": 127}, {"referenceID": 26, "context": "This can be viewed as replacing the decision tree used in HMM-based speech synthesis with a more powerful regression model [22], [27].", "startOffset": 129, "endOffset": 133}, {"referenceID": 27, "context": ", the spectrum [28]), and the availability of techniques such as multi-task learning [25], [29].", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": ", the spectrum [28]), and the availability of techniques such as multi-task learning [25], [29].", "startOffset": 85, "endOffset": 89}, {"referenceID": 28, "context": ", the spectrum [28]), and the availability of techniques such as multi-task learning [25], [29].", "startOffset": 91, "endOffset": 95}, {"referenceID": 29, "context": "One way to model contextual constraints is proposed in [30]: a bidirectional long short-term memory (LSTM)-based recurrent neural network (RNN) to map a sequence of linguistic features to the corresponding sequence of acoustic features.", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "An LSTM with a recurrent output layer is proposed in [31] to smooth acoustic features across consecutive frames.", "startOffset": 53, "endOffset": 57}, {"referenceID": 31, "context": "A systematic investigation on the architectures of gated recurrent neural network can be found in [32].", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "The input to this network is the usual set of linguistic features [33], and the output is some representation of the corresponding speech signal (e.", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "1Preliminary results were published in [25].", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "Second, we apply a sequential training criterion \u2013 minimum generation error (MGE) \u2014 for DNNs2, which is inspired by minimum generation error for HMM-based speech synthesis [4] and sequence error minimisation for voice conversion [36].", "startOffset": 172, "endOffset": 175}, {"referenceID": 35, "context": "Second, we apply a sequential training criterion \u2013 minimum generation error (MGE) \u2014 for DNNs2, which is inspired by minimum generation error for HMM-based speech synthesis [4] and sequence error minimisation for voice conversion [36].", "startOffset": 229, "endOffset": 233}, {"referenceID": 33, "context": "2Preliminary results were published in [34].", "startOffset": 39, "endOffset": 43}, {"referenceID": 34, "context": "[35], and published at the same time as [34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35], and published at the same time as [34].", "startOffset": 40, "endOffset": 44}, {"referenceID": 36, "context": "Details can be found in [37].", "startOffset": 24, "endOffset": 28}, {"referenceID": 37, "context": "To minimise this error, the classic gradient descent algorithm back-propagation [38] is typically used.", "startOffset": 80, "endOffset": 84}, {"referenceID": 38, "context": "In practice, a mini-batch gradient descent method is usually applied, for faster convergence and more stable behaviour [39]; this is possible because the error at each frame can be calculated independently.", "startOffset": 119, "endOffset": 123}, {"referenceID": 36, "context": "To generate smooth parameter trajectories, the maximum likelihood parameter generation (MLPG) algorithm [37] is used, to take the dynamic feature constraints into account.", "startOffset": 104, "endOffset": 108}, {"referenceID": 39, "context": "Using MLPG is important for good quality synthesised speech [40], [41].", "startOffset": 60, "endOffset": 64}, {"referenceID": 40, "context": "Using MLPG is important for good quality synthesised speech [40], [41].", "startOffset": 66, "endOffset": 70}, {"referenceID": 41, "context": "Bottleneck features have been extensively employed in automatic speech recognition (ASR) as a compact representation of acoustic features [42], [43], [44].", "startOffset": 138, "endOffset": 142}, {"referenceID": 42, "context": "Bottleneck features have been extensively employed in automatic speech recognition (ASR) as a compact representation of acoustic features [42], [43], [44].", "startOffset": 144, "endOffset": 148}, {"referenceID": 43, "context": "Bottleneck features have been extensively employed in automatic speech recognition (ASR) as a compact representation of acoustic features [42], [43], [44].", "startOffset": 150, "endOffset": 154}, {"referenceID": 44, "context": "We used the STRAIGHT vocoder [45] to extract vocoder parameters \u2014 60-dimensional Mel-Cepstral Coefficients (MCCs), 25 band aperiodicities (BAPs), and log-scale fundamental frequency (logF0 at a 5ms frame step, and we employed the same vocoder to reconstruct speech waveforms during synthesis.", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "As reported in our previous work [25] and other previous studies [24], [41], DNN-based systems are generally significantly better than HMM-based ones, and therefore we only included DNN and LSTM baselines, and no HMM systems.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "As reported in our previous work [25] and other previous studies [24], [41], DNN-based systems are generally significantly better than HMM-based ones, and therefore we only included DNN and LSTM baselines, and no HMM systems.", "startOffset": 65, "endOffset": 69}, {"referenceID": 40, "context": "As reported in our previous work [25] and other previous studies [24], [41], DNN-based systems are generally significantly better than HMM-based ones, and therefore we only included DNN and LSTM baselines, and no HMM systems.", "startOffset": 71, "endOffset": 75}, {"referenceID": 29, "context": "\u2022 LSTM: a second baseline system based on a long shortterm memory (LSTM) network with three feed-forward lower hidden layers each of 1024 units with tangent activation functions (intended to extract features, as suggested in [30]) plus one LSTM layer with 768 units on top of these feed-forward layers, and finally a linear regression output layer.", "startOffset": 225, "endOffset": 229}, {"referenceID": 45, "context": "\u2022 BN-DNN-VB: same as BN-DNN system, except using a different database (the voice bank database [46]) to train the bottleneck network.", "startOffset": 95, "endOffset": 99}, {"referenceID": 46, "context": "\u2022 BN-DNN-WSJ: same as BN-DNN-MFC, except using the Wall Street Journal (WSJ0+WSJ1) database [47] to train the bottleneck network.", "startOffset": 92, "endOffset": 96}, {"referenceID": 38, "context": "This phenomenon has been reported in [39].", "startOffset": 37, "endOffset": 41}, {"referenceID": 33, "context": "Similar convergence properties were also observed in the MGE-BNDNN system, and is consistent with that reported in our previous work [34].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "Between the two baselines, LSTM is consistently better than DNN under all objective measures, consistent with [30].", "startOffset": 110, "endOffset": 114}, {"referenceID": 33, "context": "Overall, these preference tests demonstrate the effectiveness of the proposed MGE criterion, and reconfirm our findings reported earlier in [34].", "startOffset": 140, "endOffset": 144}, {"referenceID": 47, "context": "Our preliminary results show that the proposed bottleneck features can also be used to guide rich-context model selection [48], and waveform unit", "startOffset": 122, "endOffset": 126}, {"referenceID": 48, "context": "selection [49].", "startOffset": 10, "endOffset": 14}], "year": 2016, "abstractText": "We propose two novel techniques \u2014 stacking bottleneck features and minimum generation error training criterion \u2014 to improve the performance of deep neural network (DNN)based speech synthesis. The techniques address the related issues of frame-by-frame independence and ignorance of the relationship between static and dynamic features, within current typical DNNbased synthesis frameworks. Stacking bottleneck features, which are an acoustically\u2013informed linguistic representation, provides an efficient way to include more detailed linguistic context at the input. The minimum generation error training criterion minimises overall output trajectory error across an utterance, rather than minimising the error per frame independently, and thus takes into account the interaction between static and dynamic features. The two techniques can be easily combined to further improve performance. We present both objective and subjective results that demonstrate the effectiveness of the proposed techniques. The subjective results show that combining the two techniques leads to significantly more natural synthetic speech than from conventional DNN or long short-term memory (LSTM) recurrent neural network (RNN) systems.", "creator": "LaTeX with hyperref package"}}}