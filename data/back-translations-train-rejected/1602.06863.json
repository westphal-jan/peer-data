{"id": "1602.06863", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "Higher-Order Low-Rank Regression", "abstract": "This paper proposes an efficient algorithm (HOLRR) to handle regression tasks where the outputs have a tensor structure. We formulate the regression problem as the minimization of a least square criterion under a multilinear rank constraint, a difficult non convex problem. HOLRR computes efficiently an approximate solution of this problem, with solid theoretical guarantees. A kernel extension is also presented. Experiments on synthetic and real data show that HOLRR outperforms multivariate and multilinear regression methods and is considerably faster than existing tensor methods.", "histories": [["v1", "Mon, 22 Feb 2016 17:21:11 GMT  (4395kb,D)", "http://arxiv.org/abs/1602.06863v1", "submitted to ICML 2016"]], "COMMENTS": "submitted to ICML 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["guillaume rabusseau", "hachem kadri"], "accepted": false, "id": "1602.06863"}, "pdf": {"name": "1602.06863.pdf", "metadata": {"source": "CRF", "title": "Higher-Order Low-Rank Regression", "authors": ["Guillaume Rabusseau", "Hachem Kadri"], "emails": ["guillaume.rabusseau@lif.univ-mrs.fr."], "sections": [{"heading": null, "text": "We formulate the regression problem as minimizing a criterion with the least square under a multilinear ranking constraint, a difficult non-convex problem. HOLRR efficiently calculates an approximate solution to this problem with solid theoretical guarantees. Furthermore, a kernel extension is presented. Experiments with synthetic and real data show that HOLRR exceeds multivariate and multilinear regression methods and is significantly faster than existing tensor methods."}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Preliminaries", "text": "We begin with the introduction of some notations. For each integer k we use [k] to denote the set of integers from 1 to k. We use bold lowercase letters for vectors (e.g. v-Rd1), bold uppercase letters for matrices (e.g. M-Rd1-d2), and bold calligraphic letters for higher order tensors (e.g. T-Rd1-d2-d3). The identity matrix is written as I. The ith line (or column) of a matrix M is denoted by Mi,: (or M-Tensor). This notation is easily extended to discs of a tensor. If v-Rd1 and v-Rd2 are used, we use v-v-Rd1-d2 to denote the Kronecker product between vectors and its simple extension to matrices and tensors."}, {"heading": "2.1 Tensors and Tucker Decomposition", "text": "Let us first remember some basic definitions of tensor S and T (of the same size) defined in Kolda & Bader (2009)."}, {"heading": "2.2 Low-Rank Regression", "text": "To solve this problem, the ordinary smallest quadratic approach must point to a linear dependence between input and output data and the search for a matrix W that minimizes the quadritic error of the quadritic error. (R) The quadritic error of the quadritic error XW \u2212 Y, which indicates the quadritic errors of the quadritic error XW \u2212 Y, which indicates the quadritic errors of the quadritic error XW \u2212 Y, which indicates the input and output matrices matrices. To prevent numerical instabilities from avoiding complete regulation, (i.e.) that the quadritic error is the objective function that leads to the regulated least squared quartiles (RLS)."}, {"heading": "3 Low-Rank Regression for Tensor-Valued Functions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Problem Formulation", "text": "We consider a multivariate regression task where the response has a tensory structure. (Ld0) We assume that vectorizing this relationship leads to vec (f) = W > (1) x showing that this model is equivalent to the standard multivariate linear models. (One way to tackle this linear regression task would be to perform a standard multivariate linear regression. (One way to tackle this linear regression task.)"}, {"heading": "3.2 Higher-Order Low-Rank Regression", "text": "We suggest an efficient algorithm to address the problem (5). \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "3.3 Theoretical Analysis", "text": "The complexity analysis compares the computational complexity of the LRR and HOLRR problem with the problem of \"X.\" For both algorithms, the computational costs of matrix multiplications are asymptotically the same (dominated by the products X > X and Y (1) Y > (1). Therefore, we focus our analysis on the computational costs of matrix inversions and abbreviated unit value decompositions. For both methods, the inversion of the matrix X > X + 2) Y into O (d0) 3) is feasible. The LRR method must include the R-dominant eigenvectors of a matrix of the same size as Y (1) Y (see Section 2.2), which can be performed in O (d1d2)."}, {"heading": "4 HOLRR Kernel Extension", "text": "In this section, we provide a kernel-based version of the HOLRR algorithm = > > Property of the algorithm is as follows: \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "5 Experiments", "text": "We present and analyze the experimental results of a regression task on synthetic data, an image reconstruction task, and a meteorological prediction task on real data.1 We compare the predictive accuracy of HOLRR with the following methods: - RLS: Regularized least squares. - LRR: Low-rank regression (see Section 2.2). - ADMM: a multilinear approach based on the tensor trace norm regularization introduced in (Gandy et al., 2011) and (Romera-Paredes et al., 2013). - MLMT-NC: a nonconvex approach proposed in (Romera-Paredes et al., 2013) in the context of multilinear multitask learning processes. For experiments with kernel algorithms, we use the readily available nucleated RLS and the LRR kernel extension, which is proposed in (Muku & MT) and are only available for MT (Zheru & MT)."}, {"heading": "5.1 Synthetic Data", "text": "We generate both linear and non-linear data. Linear data are derived from model Y = W \u2022 1 x + E, where W \u0435R10 \u00b7 10 \u00b7 10 \u00b7 10 is a randomly drawn tensor of multilinear rank (6, 4, 4, 8), x R10 from N (0, I) and each component of error tensor E from N (0, 0.1). Non-linear data is derived from model Y = W \u2022 1 (x x) + E, where W R25 \u00d7 10 \u00d7 10 is a randomly drawn tensor of rank (5, 6, 4, 2) and x R5 and E are generated as above. Hyperparameters for all algorithms are selected by triple cross-validation on the training data. These experiments were performed for different sizes of the training data set, 20 studies were performed for each size. Average RMSEs on a test set of size 100 for the 20 studies are significantly better represented in the figure R."}, {"heading": "5.2 Image Reconstruction from Noisy Measurements", "text": "To give an illustrative intuition about the differences between matrix and multilinear rank regularization, we generate data from the model Y = W \u2022 1 x + E, where the tensor W is a color image of size m \u00b7 n encoded with three color channels RGB. We consider two different tasks that depend on the input dimension: (i) W-R3 \u00b7 m \u00b7 n, x-R3 and (ii) W-Rn \u00b7 m \u00b7 3, x-Rn. In both tasks, the components of X and E are drawn from N (0, 1) and the regression tensor W is drawn from a training set of size 200.This experiment allows us to visualize the tensors returned by the RLS, LRR and HOLRR algorithms. The results are shown in Figure 2 for three images: a green cross (of size 50 x 50), a thumbnail of RRD RD RD RD and a square formed."}, {"heading": "5.3 Real Data", "text": "We compare our algorithm with other methods for the task of meteorological prediction. We collected data from the UK Meteorological Bureau 2: monthly average measurements of 5 variables at 16 stations across the UK between 1960 and 2000. The prediction task is to predict the values of the 5 variables in the 16 stations from their values in the previous months. We use the values of all variables of the last 2 months as covariants and consider the tasks of predicting the values of all variables for the next k months, the output tensors are therefore in Rk \u00d7 16 \u00d7 5. We randomly divide the available data into a training set of size N, a test set of size 50 and a validation set of size 20, because all methods are selected hyperparameters according to their performance on the validation set. The average test RMSE over 10 runs of this experiment for different values of N and k is shown in Table 2. We see that the HOLRR as a whole has better prediction accuracy than the other methods on the validation set."}, {"heading": "6 Conclusion", "text": "We proposed a low-ranking multilinear regression model for tensor-structured output data, developed a fast and efficient algorithm to solve the multilinear-scale minimization problem, and provided theoretical approximation guarantees. Experimental results showed that capturing low-ranking in output data can help improve tensor regression performance.2http: / / www.metoffice.gov.uk / public / weather / climate-historical /"}], "references": [{"title": "Estimating linear restrictions on regression coefficients for multivariate normal distributions", "author": ["T.W. Anderson"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Anderson,? \\Q1951\\E", "shortCiteRegEx": "Anderson", "year": 1951}, {"title": "Fast multivariate spatio-temporal analysis via low rank tensor learning", "author": ["M.T. Bahadori", "Q.R. Yu", "Y. Liu"], "venue": "In NIPS", "citeRegEx": "Bahadori et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahadori et al\\.", "year": 2014}, {"title": "Nonnegative Matrix and Tensor Factorizations. Applications to Exploratory Multi-way Data Analysis and Blind Source Separation", "author": ["A. Cichocki", "R. Zdunek", "A.H. Phan", "S.I. Amari"], "venue": null, "citeRegEx": "Cichocki et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2009}, {"title": "Tensor completion and low-n-rank tensor recovery via convex optimization", "author": ["Gandy", "Silvia", "Recht", "Benjamin", "Yamada", "Isao"], "venue": "Inverse Problems,", "citeRegEx": "Gandy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gandy et al\\.", "year": 2011}, {"title": "Reduced-rank regression for the multivariate linear model", "author": ["A.J. Izenman"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Izenman,? \\Q1975\\E", "shortCiteRegEx": "Izenman", "year": 1975}, {"title": "Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning", "author": ["A.J. Izenman"], "venue": null, "citeRegEx": "Izenman,? \\Q2008\\E", "shortCiteRegEx": "Izenman", "year": 2008}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM review,", "citeRegEx": "Kolda and Bader,? \\Q2009\\E", "shortCiteRegEx": "Kolda and Bader", "year": 2009}, {"title": "Multilinear Subspace Learning: Dimensionality Reduction of Multidimensional Data", "author": ["H. Lu", "K.N. Plataniotis", "A. Venetsanopoulos"], "venue": null, "citeRegEx": "Lu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2013}, {"title": "Reduced rank ridge regression and its kernel extensions", "author": ["A. Mukherjee", "J. Zhu"], "venue": "Statistical analysis and data mining,", "citeRegEx": "Mukherjee and Zhu,? \\Q2011\\E", "shortCiteRegEx": "Mukherjee and Zhu", "year": 2011}, {"title": "Multivariate reduced-rank regression: theory and applications", "author": ["G.C. Reinsel", "R.P. Velu"], "venue": "Lecture Notes in Statistics. Springer,", "citeRegEx": "Reinsel and Velu,? \\Q1998\\E", "shortCiteRegEx": "Reinsel and Velu", "year": 1998}, {"title": "Multilinear multitask learning", "author": ["B. Romera-Paredes", "M.H. Aung", "N. Bianchi-Berthouze", "M. Pontil"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Romera.Paredes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Romera.Paredes et al\\.", "year": 2013}, {"title": "Learning with tensors: a framework based on convex optimization and spectral regularization", "author": ["M. Signoretto", "Q.T. Dinh", "L. De Lathauwer", "J.K. Suykens"], "venue": "Machine Learning,", "citeRegEx": "Signoretto et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Signoretto et al\\.", "year": 2013}, {"title": "Tensor regression with applications in neuroimaging data analysis", "author": ["H. Zhou", "L. Li", "H. Zhu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "Data with a natural tensor structure is encountered in many scientific areas including neuroimaging (Zhou et al., 2013), signal processing (Cichocki et al.", "startOffset": 100, "endOffset": 119}, {"referenceID": 2, "context": ", 2013), signal processing (Cichocki et al., 2009), spatio-temporal analysis (Bahadori et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 1, "context": ", 2009), spatio-temporal analysis (Bahadori et al., 2014) and computer vision (Lu et al.", "startOffset": 34, "endOffset": 57}, {"referenceID": 7, "context": ", 2014) and computer vision (Lu et al., 2013).", "startOffset": 28, "endOffset": 45}, {"referenceID": 1, "context": ", 2009), spatio-temporal analysis (Bahadori et al., 2014) and computer vision (Lu et al., 2013). Extending multivariate regression methods to tensors is one of the challenging task in this area. Most existing works extend linear models to the multilinear setting and focus on the tensor structure of the input data (e.g. Signoretto et al. (2013)).", "startOffset": 35, "endOffset": 346}, {"referenceID": 9, "context": "In the context of multi-task learning, Romera-Paredes et al. (2013) have proposed a linear model using a tensor-rank penalization of a least squares criterion to take into account the multi-modal interactions between tasks.", "startOffset": 39, "endOffset": 68}, {"referenceID": 1, "context": "Bahadori et al. (2014) have proposed a greedy algorithm to solve a low-rank tensor learning problem in the context of multivariate spatio-temporal data analysis.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "The rank constraint in (3) was first proposed in (Anderson, 1951), whereas the term reduced-rank regression was introduced in (Izenman, 1975).", "startOffset": 49, "endOffset": 65}, {"referenceID": 4, "context": "The rank constraint in (3) was first proposed in (Anderson, 1951), whereas the term reduced-rank regression was introduced in (Izenman, 1975).", "startOffset": 126, "endOffset": 141}, {"referenceID": 0, "context": "The rank constraint in (3) was first proposed in (Anderson, 1951), whereas the term reduced-rank regression was introduced in (Izenman, 1975). Adding a ridge regularization to the rank penalized problem was proposed in Mukherjee & Zhu (2011). In the rest of the paper we will refer to this approach as lowrank regression (LRR).", "startOffset": 50, "endOffset": 242}, {"referenceID": 0, "context": "The rank constraint in (3) was first proposed in (Anderson, 1951), whereas the term reduced-rank regression was introduced in (Izenman, 1975). Adding a ridge regularization to the rank penalized problem was proposed in Mukherjee & Zhu (2011). In the rest of the paper we will refer to this approach as lowrank regression (LRR). The solution of minimization problem (3) is given by projecting the RLS solution onto the space spanned by the top R eigenvectors of Y>PY where P is the orthogonal projection matrix onto the column space of X, that is WLRR = WRLS\u03a0 where \u03a0 is the matrix of the aforementioned projection. For more description and discussion of reduced-rank regression, we refer the reader to the books of Reinsel & Velu (1998) and Izenman (2008).", "startOffset": 50, "endOffset": 737}, {"referenceID": 0, "context": "The rank constraint in (3) was first proposed in (Anderson, 1951), whereas the term reduced-rank regression was introduced in (Izenman, 1975). Adding a ridge regularization to the rank penalized problem was proposed in Mukherjee & Zhu (2011). In the rest of the paper we will refer to this approach as lowrank regression (LRR). The solution of minimization problem (3) is given by projecting the RLS solution onto the space spanned by the top R eigenvectors of Y>PY where P is the orthogonal projection matrix onto the column space of X, that is WLRR = WRLS\u03a0 where \u03a0 is the matrix of the aforementioned projection. For more description and discussion of reduced-rank regression, we refer the reader to the books of Reinsel & Velu (1998) and Izenman (2008).", "startOffset": 50, "endOffset": 756}, {"referenceID": 3, "context": "- ADMM: a multilinear approach based on tensor trace norm regularization introduced in (Gandy et al., 2011) and (Romera-Paredes et al.", "startOffset": 87, "endOffset": 107}, {"referenceID": 10, "context": ", 2011) and (Romera-Paredes et al., 2013).", "startOffset": 12, "endOffset": 41}, {"referenceID": 10, "context": "- MLMT-NC: a nonconvex approach proposed in (Romera-Paredes et al., 2013) in the context of multilinear multitask learning.", "startOffset": 44, "endOffset": 73}], "year": 2016, "abstractText": "This paper proposes an efficient algorithm (HOLRR) to handle regression tasks where the outputs have a tensor structure. We formulate the regression problem as the minimization of a least square criterion under a multilinear rank constraint, a difficult non convex problem. HOLRR computes efficiently an approximate solution of this problem, with solid theoretical guarantees. A kernel extension is also presented. Experiments on synthetic and real data show that HOLRR outperforms multivariate and multilinear regression methods and is considerably faster than existing tensor methods.", "creator": "LaTeX with hyperref package"}}}