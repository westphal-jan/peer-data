{"id": "1602.07017", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2016", "title": "A survey of sparse representation: algorithms and applications", "abstract": "Sparse representation has attracted much attention from researchers in fields of signal processing, image processing, computer vision and pattern recognition. Sparse representation also has a good reputation in both theoretical research and practical applications. Many different algorithms have been proposed for sparse representation. The main purpose of this article is to provide a comprehensive study and an updated review on sparse representation and to supply a guidance for researchers. The taxonomy of sparse representation methods can be studied from various viewpoints. For example, in terms of different norm minimizations used in sparsity constraints, the methods can be roughly categorized into five groups: sparse representation with $l_0$-norm minimization, sparse representation with $l_p$-norm (0$&lt;$p$&lt;$1) minimization, sparse representation with $l_1$-norm minimization and sparse representation with $l_{2,1}$-norm minimization. In this paper, a comprehensive overview of sparse representation is provided. The available sparse representation algorithms can also be empirically categorized into four groups: greedy strategy approximation, constrained optimization, proximity algorithm-based optimization, and homotopy algorithm-based sparse representation. The rationales of different algorithms in each category are analyzed and a wide range of sparse representation applications are summarized, which could sufficiently reveal the potential nature of the sparse representation theory. Specifically, an experimentally comparative study of these sparse representation algorithms was presented. The Matlab code used in this paper can be available at:", "histories": [["v1", "Tue, 23 Feb 2016 02:44:53 GMT  (3200kb)", "http://arxiv.org/abs/1602.07017v1", "Published on IEEE Access, Vol. 3, pp. 490-530, 2015"]], "COMMENTS": "Published on IEEE Access, Vol. 3, pp. 490-530, 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["zheng zhang", "yong xu", "jian yang", "xuelong li", "david zhang"], "accepted": false, "id": "1602.07017"}, "pdf": {"name": "1602.07017.pdf", "metadata": {"source": "CRF", "title": "A survey of sparse representation: algorithms and applications", "authors": ["Zheng Zhang"], "emails": ["(yongxu@ymail.com).", "yongxu@ymail.com)."], "sections": [{"heading": null, "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"in which he deals with the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" the \"New York,\" the \"the\" the \"the\" New York, \"the\" the \"the\" New York, \"the\" the \"the\" New York, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"New York,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" New York, \"the\" the \"the\" the \"the\" the \"New York,\" the \"the\" the \"the\" the \""}, {"heading": "A. Categorization of sparse representation techniques", "text": "This year, we are in a position to find a new home in a country where most people live in poverty, where they will no longer find themselves."}, {"heading": "B. Motivation and objectives", "text": "This paper provides an overview of sparse representation methods and an overview of available sparse representation algorithms from the perspective of mathematical and theoretical optimization.This paper aims to provide the foundations of the study on sparse representation and aims to give a good start to newcomers to computer vision and pattern recognition communities interested in sparse representation methods and their related fields.Extensive state-of-art sparse representation methods are summarized and the ideas, algorithms and broad applications of sparse representation are comprehensively presented. In particular, it focuses on the introduction of a recent review of existing literature and presents some insights into studies of recent sparse representation methods. Furthermore, the existing sparse representation methods are divided into different categories. Subsequently, the corresponding typical algorithms are presented in different categories and their distinctness explicitly shown in smarse smars."}, {"heading": "II. FUNDAMENTALS AND PRELIMINARY CONCEPTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Notations", "text": "In this essay, vectors are denoted by lowercase letters with bold obverse, e.g. x. Matrices are denoted by uppercase letters, e.g. X and their elements are denoted by indices such as Xi. In this essay, all data is of real value only. Let's assume that the sample comes from space Rd, and therefore all samples are linked to a matrix called D-Rd-n. If a sample can be roughly represented by a linear combination of dictionary D, and the number of samples is greater than the dimension of the samples in D, i.e. n > d, dictionary D is denoted as a complete dictionary. A signal should be compressible if it is a sparse signal in the original or transformed domain, if there is no information or energy loss during the transformation process. \"sparse\" or \"sparsity\" of a vector means that some elements of the vector are zero. We use a linear signal in the initial or transformed domain, if there is no information or energy loss during the transformation process."}, {"heading": "B. Basic background", "text": "The question for the \"why,\" according to the question, \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \",\" \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\" \",\", \"\", \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\", \",\", \"\", \",\", \"\", \"\", \"\", \"\", \",\" \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\" \",\", \"\", \"\", \"\", \"\", \"\", \"\" \",\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\", \"\" \"\", \"\" \"\", \"\" \"\", \",\" \"\" \",\" \"\" \",\" \"\" \"\" \"\", \",\" \"\" \",\" \"\" \"\", \"\" \"\" \"\", \"\" \"\", \"\" \"\" \",\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\" \",\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\""}, {"heading": "III. SPARSE REPRESENTATION PROBLEM WITH DIFFERENT NORM REGULARIZATIONS", "text": "In this section, frugal representation is summarized and divided into different categories in terms of the norm regularizations used. The general framework of frugal representation is to use the linear combination of some samples or \"atoms\" to represent the sample, but the coefficients of representation of these samples or \"atoms\" can be largely dominated. [32-35] In terms of the different standards used in the optimizers, the representation techniques can be roughly divided into five general categories: frugal representation with l0-norm minimization [36, 37] frugal representation with lp-norm (0 < p < 1) minimized, frugal representation with l0-norm minimization [36, 37] frugal representation with lp-norm."}, {"heading": "IV. GREEDY STRATEGY APPROXIMATION", "text": "Greedy algorithms date back to the 1950s. The basic idea of the greedy strategy [7, 23] is to determine the position on the basis of the relationship between atomic and probe sample and then use the smallest square to evaluate the amplitude value. Greedy algorithms can obtain the locally optimized solution to solve the problem at any step. However, the greedy algorithm can always produce the global optimal solution or an approximate total solution [7, 23]. Solving the sparse representation with the l0 standard regularization, i.e. problem III.3, is a NP-hard problem [20, 56]. Greedy strategy offers a special way to achieve an approximate meagre representation solution. In fact, the greedy strategy cannot directly solve the optimization problem and only seeks an approximate solution to problem III.3."}, {"heading": "A. Matching pursuit algorithm", "text": "The Matching Purchase (MP) algorithm [63] is the earliest and most representative method to approximate the problem III.3 or II.4. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &"}, {"heading": "B. Orthogonal matching pursuit algorithm", "text": "Orthogonal Matching Purchase (OMP) Value Value Value Value Value-Value Value-Value Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-Value-"}, {"heading": "C. Series of matching pursuit algorithms", "text": "These algorithms are typically greedy iterative algorithms. The earliest algorithms were Matching Tracking (MP) and Orthogonal Matching Tracking (OMP). The basic idea of the MP algorithm is to select the best matching atom from the complete dictionary to construct sparse approximation during each iteration, calculate the signal representation, and then select the best matching atom until the stop criterion of iteration is met. Many other greedy algorithms based on the MP and OMP algorithm, such as the efficient orthogonal matching algorithm [65] were subsequently proposed to improve the tracking algorithm. Needell et algorithm proposed a regulated version of Orthogonal Matching Tracking (ROMP)."}, {"heading": "V. CONSTRAINED OPTIMIZATION STRATEGY", "text": "In this section, some typical methods that apply the restricted optimization strategy to solve the original unrestricted, non-smooth problem are presented by reformulation as a smoothly differentiated, restricted optimization problem. These methods take advantage of the restricted optimization method with efficient convergence to obtain the sparse solution. Furthermore, the restricted optimization strategy emphasizes the equivalent conversion of the original unrestricted, non-smooth problem to problem II.12 and uses the newly formulated restricted problem to obtain a sparse representational solution."}, {"heading": "A. Gradient Projection Sparse Reconstruction", "text": "The basic idea of the sequence projection sparse representation is to find a limited problem in which each value of \u03b1 can be divided into its positive and negative parts. Vectors \u03b1 + and \u03b1 \u2212 are introduced to denote the positive and negative coefficients of \u03b1, which are defined as (x) + = max {0, x} The sparse representation solution can be formulated as follows: \u03b1 = \u03b1 + \u2212 \u03b1 \u2212 \u03b1, \u03b1 \u2212 \u03b1 0 (V.1), where the operator (\u00b7) + denotes the positive part operator, which is defined as (x) + = max {0, x}."}, {"heading": "C. Alternating direction method (ADM) based sparse representation strategy", "text": "This section shows how the ADM [43] is used to solve primary and dual problems in III.12. (Firstly, an auxiliary variable is introduced to solve the problem in III.12 in a restricted problem with the form of the problem V.22 \u2212 \u2212 Subsequently, the alternative direction method is used to solve the partial problems of the problem V.22 (V.22) The optimization problem of augmented Lagrangian function of the problem V.22 is equivalent to a restricted problem V.22 \u2212 \u2212 s, which results in a limited problem margmin solution, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s,"}, {"heading": "VI. PROXIMITY ALGORITHM BASED OPTIMIZATION STRATEGY", "text": "This section discusses the methods that use the proximity algorithm to solve problems with limited convex optimization, which are much more efficient in computational terms than the original problem. The proximity algorithm is often used to solve problems with uneven, limited convex optimization [28]. In addition, the general problem of sparse representation with l1-normal regulation is an uneven convex optimization problem that can be effectively addressed by using the proximal algorithm. Replace a simple problem with limited optimization ismin (x). (VI.1) The general framework for solving the problem with limited convex optimization is a problem that can be effectively solved by using the proximal algorithm."}, {"heading": "A. Soft thresholding or shrinkage operator", "text": "First, a simple form of problem III.12 is introduced, which has a closed solution and is formulated as follows: \u03b1 (VI.6) 2 (VI.6), where \u03b1 (VI.6) is the optimal solution to problem VI.6, and then there are the following conclusions: (1), if it is a problem > 0, then h (\u03b1) = 12 (VI.2 (VI.6)), and its derivative is h (VI.6), if it is a problem > 0, then h (\u03b1) = 12 (VI.2), and its derivative is h (VI.2), then the derivative is h (VI.6) = \u03b1 (VI.8), and its derivative is h (VI.8)."}, {"heading": "C. Fast Iterative shrinkage thresholding algorithm (FISTA)", "text": "The fast iterative shrinkage threshold algorithm (FISTA) is an improvement of ISTA. FISTA [82] not only maintains the efficiency of the original ISTA, but also promotes the effectiveness of ISTA so that FISTA can achieve global convergence. Given that the Hessian matrix Hf (\u03b1) is approximated by using a scalar 1\u03c4 I for ISTA in Eq. VI.9, FISTA can use the minimum Lipschitz constant of the gradient f (\u03b1) to approximate the Hessian matrix f (\u03b1), i.e. L (f) = 2\u03bbmax (XTX) \u2212 value on the problem below: f (\u03b1) 1 2 (XT) points to the previous target value of f (\u03b1)."}, {"heading": "D. Sparse reconstruction by separable approximation (SpaRSA)", "text": "The main contributions of SpaRSA are to optimize the parameters of problem VI.8 by applying the worm startup method, i.e. the continuation and selection of amorescent reliable approximation of Hf (\u03b1) in problem VI.9 using the Barzilai-Borwein (BB) spectral method [85]. The worm startup method and BB spectral approach are introduced as follows: (1) Using the worm startup technique to optimize the worm startup method. The above discussed economical presentation methods are always set to a specific small constant."}, {"heading": "F. Augmented Lagrange Multiplier based optimization strategy", "text": "The Lagrange Multiplier is a widely used tool for eliminating the equality problem and solving the problem without restriction with an appropriate streamlining function. Specifically, the sparse representation problem III.9 can be considered as an equality problem and the equivalent problem II.12 as an unrestricted problem that reinforces the objective function of problem II.9 with a weighted constraint function. In this section, the augmented Lagrange Method (ALM) is introduced to address the sparse representation problem II.9.First, the augmented Lagrangian function of problem II.9 is conceived by introducing an additional equality problem function applied to the Lagrange function in problem III.12. That is, L (L) = the augmented Lagrangian function of problem II.9 is conceived by introducing a new equality problem function."}, {"heading": "G. Other proximity algorithm based optimization methods", "text": "The theoretical basis of the proximity algorithm is to first construct a proximity operator and then use the proximity operator to solve the convex optimization problem. Massive proximity algorithms have followed with improved techniques to improve the effectiveness and efficiency of proximity algorithm-based optimization methods. For example, Elad et al. proposed an iterative method called the Parallel Coordinate Descent Algorithm (PCDA) [90] by introducing the elemental optimization algorithm to solve the regulated linear smallest squares with non-square regulation problems. Inspired by the propagation of belief in graphical models, Donoho et al. developed a modified version of the iterative threshold method called Approximate Message Passing (AMP) method [91] to satisfy the requirement that the new compromise between the algorithms is worth-while."}, {"heading": "VII. HOMOTOPY ALGORITHM BASED SPARSE REPRESENTATION", "text": "The concept of homotopy is derived from topology, and the homotopy technique is mainly used to solve a non-linear equation problem. Originally, the homotopy method was proposed to solve the problem of the l1 penalty [94]. The main idea of homotopy is to solve the original optimization problems by pursuing a continuous parameterized solution path with different parameters. Since the homotopy algorithm has a very close relationship to the conventional methods of sparse representation such as the smallest angle regression (LAR) [42], OMP [64] and the tracking of polytopic surfaces (PFP) [95], it has been successfully used to solve the l1 standard minimization problems. Unlike LAR and OMP, the homotopy method is more advantageous for the sequential updating of the sparse solution by addition or removal of elements from the active theorem to solve the representative problem of the following homotopy strategy using the following paragraph 1."}, {"heading": "A. LASSO homotopy", "text": "It is shown that problem III.12 with an appropriate parameter value is equivalent to problem III.9 [29]. Furthermore, it is obvious that if we switch from a very high value to zero, the solution of problem III.12 approaches the solution of problem II.9 [29]. The amount of different values on the solution path is obvious that we switch from a very high value to zero, the solution of problem III.12 is more convergent than the solution of problem II.9 [29]."}, {"heading": "B. BPDN homotopy", "text": "The problem II.11, which is called basic tracking, denoising (BPDN) in signal processing, is the full Lagrangian function of the LASSO problem II.9, which is an unrestricted problem. BPDN homotopy algorithm is very similar to the LASSO homotopy algorithm. If we look at the KKT optimality condition for problem II.12, the following condition should be met for the solution. (y \u2212 X\u03b1) BPDN homotopy algorithm is very detailed. (VII.8) As for each given value of LASSO homotopy and support, the following two conditions must also be met. (y \u2212 X\u03b1) The BPDN homotopy algorithm is the solution. (VII.9) The BPDN homotopy algorithm is the homotopy algorithm of the homotopy (BI.9)."}, {"heading": "D. Other homotopy algorithms for sparse representation", "text": "The general principle of the homotopy method is to achieve the optimal solution together with the homotopy solution path by developing the homotopy parameter from a known initial value to the final expected value. There are extensive hotomopy algorithms related to the sparse representation associated with the l1 norm regularization. Malioutov et al. first used the homotopy method to select a suitable parameter for l1 norm regularization with a noisy term in a subspecified system and to apply the homotopy continuation-based method for solving BPDN for sparse signal processing [97]. Garrigues and Ghaoui have proposed a modified homotopy algorithm to solve the lasso problem with online observations by optimizing algorithm 11. Iterative reweighting homotopy algorithm for minimized normalization: To minimize the norm 1."}, {"heading": "VIII. THE APPLICATIONS OF THE SPARSE REPRESENTATION METHOD", "text": "Scanty representation techniques have been used successfully in numerous applications, especially in computer vision, image processing, pattern recognition and machine learning. More specifically, scanty representation has also been successfully applied to large-scale real-world applications such as image analysis, mitigation, inpainting, super-resolution, restoration, quality evaluation, classification, segmentation, signal processing, object tracking, texture classification, image recovery, bioinformatics, biometrics and other artificial intelligence systems. In addition, dictionary learning is one of the most typical representative examples of scanty representation to realize the scanty of a signal. In this paper, we focus only on the three application areas of scanty representation, i.e. scanty representation in dictionary learning, image processing, image classification and visual tracking."}, {"heading": "A. Sparse representation in dictionary learning", "text": "In fact, it is so that most of them are able to embark on a search for a solution that has its origin in the real world. (...) Most of them are at home in the real world in the real world. (...) Most of them are at home in the real world in the real world. (...) Most of them are at home in the real world in the real world. (...) Most of them are at home in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world. (...) Most of them are at home in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world, in the, in the world, in the world, in the, in the world, in the world in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the, in the world, in the world, in the world, in the world, in the world, in the, in the world, in the, in the, in the world, in the world, in the world, in the, in the, in the"}, {"heading": "B. Sparse representation in image processing", "text": "The techniques of sparse representation were gradually introduced into image processing, such as image processing in overlapping image fields or block images."}, {"heading": "C. Sparse representation in image classification and visual tracking", "text": "In addition to these effective applications in image processing, several other fields of sparse representation have been proposed and extensively studied. Since Wright et al. [20] proposed using sparse representation to perform robust face recognition, more and more researchers have applied the sparse representation theory to the fields of computer vision and pattern recognition, especially in the fields of image classification and object tracking. Algorithm 16: Partition of the degraded image in M overlapped patches. While t \u2264 T do: For each image patch, update the corresponding dictation parameters for each cluster via k-means and PCA. Step 3: Update the regulation parameters."}, {"heading": "IX. EXPERIMENTAL EVALUATION", "text": "In this section, we will take the object categorization problem as an example to evaluate the performance of the various sparse representations using classification methods. We will analyze and compare the performance of the sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], Homotopy [99], and TPTSR [9]. Plenary seats have been collected for object categorization, especially for image classification. Several image sets are used in our experimental evaluations. ORL: The ORL database contains 400 face images, each of which provides 10 face images."}, {"heading": "A. Parameter selection", "text": "Parameter selection, in particular the selection of regularization parameters \u03bb in different minimization problems, plays an important role in the sparse representation. In order to make fair comparisons with different sparse representation algorithms, it is advisable and indispensable to carry out the optimal parameter selection for different sparse representation algorithms on different datasets. In this subsection, we conduct extensive experiments to select the best value of the regularization parameter \u03bb with a wide range of options. Specifically, we implement the l1 ls, FISTA, DALM, homotopy and TPTSR algorithms on different databases in order to analyze the importance of the regularization parameter \u03bb on different datasets. Fig. 5 summarizes the classification accuracy of the different sparse representation methods with different values regularization parameters, DALM, homotopy and TPTSR alcensensensensensing parameters on two datasets each, COLW and COLL"}, {"heading": "B. Experimental results", "text": "In order to test the performance of the various types of frugal representation methods, in this subsection an empirical study of the experimental results is conducted and seven typical frugal representation methods are selected for performance evaluation, followed by extensive experimental results. For all data sets following most of the earlier published work, we will randomly select several samples from each class as training samples and use the rest as test samples, and the experiments will be repeated 10 times with the optimal parameter achieved with the cross-validation approach. The grayscale characteristics of all images in these data sets will be used to perform the classification. To improve computing power, the principle component analysis algorithm is used as a pre-processing step to obtain 98% of the energy of all data sets. Classification results and computation time are summarized in Table I. From the experimental results on different databases, we can conclude that there is still no exceptional algorithm that can achieve the best classification accuracy on all databases."}, {"heading": "C. Discussion", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "X. CONCLUSION", "text": "This year is the highest in the history of the country."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported in part by the National Natural Science Foundation of China under Grant 61370163, Grant 61233011 and Grant 61332011, and in part by the Shenzhen Municipal Science and Technology Innovation Council under Grant JCYJ20130329151843309, Grant JCYJ201329151843309, Grant JCYJ201417172417174, Grant CXZZ20140904154910774, Project # 2014M560264, funded by the China Postdoctoral Science Foundation, and the Shaanxi Key Innovation Team of Science and Technology (Grant # 2012KCT-04). We would like to thank Jian Wu for many inspiring discussions and he is ultimately responsible for many of the ideas in the algorithm and analysis. We would also like to thank Dr. Zhihui Lai, Dr. Jinxing Liu and Xiaozhao Fang for constructive suggestions."}], "references": [{"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM journal on computing, vol. 24, no. 2, pp. 227\u2013234, 1995.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Brain extraction based on locally linear representation-based classification", "author": ["M. Huang", "W. Yang", "J. Jiang", "Y. Wu", "Y. Zhang", "W. Chen", "Q. Feng"], "venue": "NeuroImage, vol. 92, pp. 322\u2013339, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Group sparse reconstruction for image segmentation", "author": ["X. Lu", "X. Li"], "venue": "Neurocomputing, vol. 136, pp. 41\u201348, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "On the role of sparse and redundant representations in image processing", "author": ["M. Elad", "M. Figueiredo", "Y. Ma"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 972\u2013 982, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "A wavelet tour of signal processing: the sparse way", "author": ["S. Mallat"], "venue": "Academic press,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Sparse image and signal processing: wavelets, curvelets, morphological diversity", "author": ["J. Starck", "F. Murtagh", "J.M. Fadili"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Sparse and redundant representations: from theory to applications in signal and image processing", "author": ["M. Elad"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "From sparse solutions of systems of equations to sparse modeling of signals and images", "author": ["A.M. Bruckstein", "D.L. Donoho", "M. Elad"], "venue": "SIAM review, vol. 51, no. 1, pp. 34\u201381, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "A two-phase test sample sparse representation method for use with face recognition", "author": ["Y. Xu", "D. Zhang", "J. Yang", "J. Yang"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 21, no. 9, pp. 1255\u2013 1262, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse representation for computer vision and pattern recognition", "author": ["J. Wright", "Y. Ma", "J. Mairal", "G. Sapiro", "T. Huang", "S. Yan"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 1031\u20131044, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 4, pp. 1289\u2013 1306, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressive sensing", "author": ["R.G. Baraniuk"], "venue": "IEEE signal processing magazine, vol. 24, no. 4, pp. 118\u2013121, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E.J. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 2, pp. 489\u2013 509, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to compressive sampling", "author": ["E.J. Candes", "M.B. Wakin"], "venue": "IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 21\u201330, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Extensions of compressed sensing", "author": ["Y. Tsaig", "D.L. Donoho"], "venue": "Signal processing, vol. 86, no. 3, pp. 549\u2013571, 2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressive sampling", "author": ["E.J. Candes"], "venue": "Proceedings of the International Congress of Mathematicians: Madrid, August 22-30, 2006: invited lectures, 2006, pp. 1433\u20131452.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Sparsity and incoherence in compressive sampling", "author": ["E. Candes", "J. Romberg"], "venue": "Inverse problems, vol. 23, no. 3, p. 969, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Manifold regularized sparse nmf for hyperspectral unmixing", "author": ["X. Lu", "H. Wu", "Y. Yuan", "P. Yan", "X. Li"], "venue": "IEEE Transactions on Geoscience and Remote Sensing, vol. 51, no. 5, pp. 2815\u20132826, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Binary sparse nonnegative matrix factorization", "author": ["Y. Yuan", "X. Li", "Y. Pang", "X. Lu", "D. Tao"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 19, no. 5, pp. 772\u2013779, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 2, pp. 210\u2013227, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Integrating globality and locality for robust representation based classification", "author": ["Z. Zhang", "Z. Li", "B. Xie", "L. Wang", "Y. Chen"], "venue": "Mathematical Problems in Engineering, vol. 2014, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse representation and learning in visual recognition: Theory and applications", "author": ["H. Cheng", "Z. Liu", "L. Yang", "X. Chen"], "venue": "Signal Processing, vol. 93, no. 6, pp.  1408\u20131425, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Algorithms for simultaneous sparse approximation. part i: Greedy pursuit", "author": ["J.A. Tropp", "A.C. Gilbert", "M.J. Strauss"], "venue": "Signal Processing, vol. 86, no. 3, pp. 572\u2013588, 2006.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Algorithms for simultaneous sparse approximation. part ii: Convex relaxation", "author": ["J.A. Tropp"], "venue": "Signal Processing, vol. 86, no. 3, pp. 589\u2013602, 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Optimization methods for l1-regularization", "author": ["M. Schmidt", "G. Fung", "R. Rosales"], "venue": "University of British Columbia, West Mall Vancouver, B.C. Canada V6T 1Z4, Tech. Rep., 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems", "author": ["E. Amaldi", "V. Kann"], "venue": "Theoretical Computer Science, vol. 209, no. 1, pp. 237\u2013260, 1998.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "Greed is good: Algorithmic results for sparse approximation", "author": ["J.A. Tropp"], "venue": "IEEE Transactions on Information Theory, vol. 50, no. 10, pp. 2231\u20132242, 2004.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Proximal algorithms", "author": ["N. Parikh", "S. Boyd"], "venue": "Foundations and Trends in Optimization, vol. 1, no. 3, pp. 123\u2013231, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Noise modeling and representation based classification methods for face recognition", "author": ["Z. Zhang", "L. Wang", "Q. Zhu", "Z. Liu", "Y. Chen"], "venue": "Neurocomputing, vol. 148, pp. 420\u2013429, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Theoretical results on sparse representations of multiple-measurement vectors", "author": ["J. Chen", "X. Huo"], "venue": "IEEE Transactions on Signal Processing, vol. 54, no. 12, pp. 4634\u20134643, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Unconstrained regularized lp-norm based algorithm for the reconstruction of sparse signals", "author": ["J.K. Pant", "W.S. Lu", "A. Antoniou"], "venue": "Proceedings of the IEEE International Symposium on Circuits and Systems, 2011, pp. 1740\u20131743.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Iteratively reweighted algorithms for compressive sensing", "author": ["R. Chartrand", "W. Yin"], "venue": "Proceedings of the IEEE international conference on Acoustics, speech and signal processing, 2008, pp. 3869\u20133872.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Beyond sparsity: The role of l1-optimizer in pattern classification", "author": ["J. Yang", "L. Zhang", "Y. Xu", "J. Yang"], "venue": "Pattern Recognition, vol. 45, no. 3, pp. 1104\u20131118, 2012.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit", "author": ["J.A. Tropp", "A.C. Gilbert"], "venue": "IEEE Transactions on Information Theory, vol. 53, no. 12, pp. 4655\u20134666, 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Uniform uncertainty principle and signal recovery via regularized orthogonal matching pursuit", "author": ["D. Needell", "R. Vershynin"], "venue": "Foundations of computational mathematics, vol. 9, no. 3, pp. 317\u2013334, 2009.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Stable sparse  JOURNAL  35 approximations via nonconvex optimization", "author": ["R. Saab", "R. Chartrand", "O. Yilmaz"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2008, pp. 3885\u20133888.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Exact reconstruction of sparse signals via nonconvex minimization", "author": ["R. Chartrand"], "venue": "IEEE Signal Processing Letters, vol. 14, no. 10, pp. 707\u2013710, 2007.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Data modeling: Visual psychology approach and l1/2 regularization theory", "author": ["Z.B. Xu"], "venue": "Proceeding of International Congress of Mathmaticians, 2010, pp. 3151\u2013 3184.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society Series B, pp. 267\u2013288, 1996.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1996}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "I. Johnstone", "R. Tibshirani"], "venue": "The Annals of statistics, vol. 32, no. 2, pp. 407\u2013499, 2004.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2004}, {"title": "Alternating direction algorithms for l1-problems in compressive sensing", "author": ["J. Yang", "Y. Zhang"], "venue": "SIAM journal on scientific computing, vol. 33, no. 1, pp. 250\u2013278, 2011.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast optimization methods for l1 regularization: A comparative study and two new approaches", "author": ["M. Schmidt", "G. Fung", "R. Rosales"], "venue": "Machine Learning: ECML 2007. Springer, 2007, pp. 286\u2013297.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient and robust feature selection via joint l2,1-norms minimization", "author": ["F. Nie", "H. Huang", "X. Cai", "C. Ding"], "venue": "Proceedings of the Advances in Neural Information Processing Systems, 2010, pp. 1813\u20131821.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "l2,1-norm regularized discriminative feature selection for unsupervised learning", "author": ["Y. Yang", "H.T. Shen", "Z. Ma", "Z. Huang", "X. Zhou"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence, vol. 22, no. 1, 2011, pp. 1589\u20131594.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Face recognition by sparse discriminant analysis via joint l2,1-norm minimization", "author": ["X.S. Shi", "Y.J. Yang", "Z.H. Guo", "Z.H. Lai"], "venue": "Pattern Recognition, vol. 47, no. 7, pp. 2447\u20132453, 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-task feature learning via efficient l2,1-norm minimization", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, 2009, pp. 339\u2013348.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Joint embedding learning and sparse regression: A framework for unsupervised feature selection", "author": ["C. Hou", "F. Nie", "X. Li", "D. Yi", "Y. Wu"], "venue": "IEEE Transactions on Cybernetics, vol. 44, no. 6, pp. 793\u2013804, 2014.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Linear regression for face recognition", "author": ["I. Naseem", "R. Togneri", "M. Bennamoun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 11, pp. 2106\u20132112, 2010.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse representation or collaborative representation: Which helps face recognition?", "author": ["D. Zhang", "M. Yang", "X. Feng"], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2011}, {"title": "Optimally sparse representation in general (nonorthogonal) dictionaries via l1 minimization", "author": ["D. Donoho", "M. Elad"], "venue": "Proceedings of the National Academy  of Sciences, vol. 100, no. 5, pp. 2197\u20132202, 2003.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2003}, {"title": "Realistic action recognition via sparsely-constructed gaussian processes", "author": ["L. Liu", "L. Shao", "F. Zheng", "X. Li"], "venue": "Pattern Recognition, vol. 47, no. 12, pp. 3819\u2013 3827, 2014.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse representations, compressive sensing and dictionaries for pattern recognition", "author": ["V.M. Patel", "R. Chellappa"], "venue": "Proceedings of the IEEE First Asian Conference on Pattern Recognition, 2011, pp. 325\u2013329.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning hash functions using sparse reconstruction", "author": ["Y. Yuan", "X. Lu", "X. Li"], "venue": "Proceedings of International Conference on Internet Multimedia Computing and Service, 2014, pp. 14\u201318.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "For most large underdetermined systems of linear equations the minimal l1-norm solution is also the sparsest solution", "author": ["D. Donoho"], "venue": "Communications on pure and applied mathematics, vol. 59, no. 6, pp. 797\u2013829, 2006.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2006}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E. Candes", "J. Romberg", "T. Tao"], "venue": "Communications on pure and applied mathematics, vol. 59, no. 8, pp. 1207\u20131223, 2006.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2006}, {"title": "A comparison of typical lp minimization algorithms", "author": ["L. Qin", "Z.C. Lin", "Y.Y. She", "C. Zhang"], "venue": "Neurocomputing, vol. 119, pp. 413\u2013424, 2013.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2013}, {"title": "l1/2 regularization: A thresholding representation theory and a fast solver", "author": ["Z. Xu", "X. Chang", "F. Xu", "H. Zhang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 7, pp. 1013\u20131027, 2012.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2012}, {"title": "Enhancing sparsity via lp (0<p<1) minimization for robust face recognition", "author": ["S. Guo", "Z. Wang", "Q. Ruan"], "venue": "Neurocomputing, vol. 99, pp. 592\u2013602, 2013.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2013}, {"title": "R1-pca: rotational invariant l1-norm principal component analysis for robust subspace factorization", "author": ["C. Ding", "D. Zhou", "X. He", "H. Zha"], "venue": "Proceedings of the 23rd international conference on Machine learning, 2006, pp. 281\u2013288.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2006}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["S.G. Mallat", "Z. Zhang"], "venue": "IEEE Transactions on Signal Processing, vol. 41, no. 12, pp. 3397\u20133415, 1993.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1993}, {"title": "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition", "author": ["Y.C. Pati", "R. Rezaiifar", "P.S. Krishnaprasad"], "venue": "Proceedings of the Twenty-Seventh Asilomar Conference on Signals, Systems and Computers, 1993, pp. 40\u201344.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1993}, {"title": "Efficient orthogonal matching pursuit using sparse random projections for scene and video classification", "author": ["S.N. Vitaladevuni", "P. Natarajan", "R. Prasad"], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2011, pp. 2312\u20132319.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2011}, {"title": "Cosamp: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J.A. Tropp"], "venue": "Applied and Computational Harmonic Analysis, vol. 26, no. 3, pp. 301\u2013321, 2009.  JOURNAL  36", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse solution of underdetermined systems of linear equations by stagewise orthogonal matching pursuit", "author": ["D.L. Donoho", "Y. Tsaig", "I. Drori", "J. Starck"], "venue": "IEEE Transactions on Information Theory, vol. 58, no. 2, pp. 1094\u2013 1121, 2012.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2012}, {"title": "Subspace pursuit for compressive sensing signal reconstruction", "author": ["W. Dai", "O. Milenkovic"], "venue": "IEEE Transactions on Information Theory, vol. 55, no. 5, pp. 2230\u2013 2249, 2009.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparsity adaptive matching pursuit algorithm for practical compressed sensing", "author": ["T.T. Do", "L. Gan", "N. Nguyen", "T. Tran"], "venue": "Proceedings of the 42nd Asilomar Conference on Signals, Systems and Computers, 2008, pp. 581\u2013587.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2008}, {"title": "Tree-based pursuit: Algorithm and properties", "author": ["P. Jost", "P. Vandergheynst", "P. Frossard"], "venue": "IEEE Transactions on Signal Processing, vol. 54, no. 12, pp. 4685\u20134697, 2006.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2006}, {"title": "Tree-based orthogonal matching pursuit algorithm for signal reconstruction", "author": ["C. La", "M.N. Do"], "venue": "IEEE International Conference on Image Processing, 2006, pp. 1277\u20131280.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressed sensing signal recovery via forward\u2013backward pursuit", "author": ["N.B. Karahanoglu", "H. Erdogan"], "venue": "Digital Signal Processing, vol. 23, no. 5, pp. 1539\u2013 1548, 2013.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems", "author": ["M. Figueiredo", "R.D. Nowak", "S.J. Wright"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 1, no. 4, pp. 586\u2013597, 2007.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2007}, {"title": "An interior-point method for largescale l1-regularized least squares", "author": ["S.J. Kim", "K. Koh", "M. Lustig", "S. Boyd", "D. Gorinevsky"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 1, no. 4, pp. 606\u2013617, 2007.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2007}, {"title": "A truncated primal-infeasible dual-feasible network interior point method", "author": ["L.F. Portugal", "M. Resende", "G. Veiga", "J. Judice"], "venue": "Networks, vol. 35, no. 2, pp. 91\u2013108, 2000.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2000}, {"title": "An interior-point method for large-scale l1-regularized logistic regression.", "author": ["K. Koh", "S.J. Kim", "S.P. Boyd"], "venue": "Journal of Machine learning research,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2007}, {"title": "On the implementation of a primal-dual interior point method", "author": ["S. Mehrotra"], "venue": "SIAM Journal on optimization, vol. 2, no. 4, pp. 575\u2013601, 1992.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 1992}, {"title": "Primal-dual interior-point methods", "author": ["S.J. Wright"], "venue": null, "citeRegEx": "78", "shortCiteRegEx": "78", "year": 1997}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2011}, {"title": "A bound optimization approach to wavelet-based image deconvolution", "author": ["M. Figueiredo", "R. Nowak"], "venue": "Proceedings of the IEEE International Conference on Image Processing, vol. 2, 2005, pp. II\u2013782\u20135.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2005}, {"title": "Proximal splitting methods in signal processing", "author": ["P.L. Combettes", "J.C. Pesquet"], "venue": "Fixed-point algorithms for inverse problems in science and engineering, 2011, pp. 185\u2013212.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2011}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast l1-minimization algorithms and an application in robust face recognition: A review", "author": ["A.Y. Yang", "S. Sastryand", "A. Ganesh", "Y. Ma"], "venue": "Proceedings of the IEEE International Conference on Image Processing (ICIP), 2010, pp. 1849\u20131852.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R.D. Nowak", "M. Figueiredo"], "venue": "IEEE Transactions on Signal Processing, vol. 57, no. 7, pp. 2479\u20132493, 2009.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2009}, {"title": "The cyclic barzilai\u2014borwein method for unconstrained optimization", "author": ["Y.H. Dai", "W. Hager", "K. Schittkowski", "H. Zhang"], "venue": "IMA Journal of Numerical Analysis, vol. 26, no. 3, pp. 604\u2013627, 2006.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2006}, {"title": "A fixed-point continuation method for l1-regularized minimization with applications to compressed sensing", "author": ["E.T. Hale", "W. Yin", "Y. Zhang"], "venue": "Rice University, St, Houston, USA, Tech. Rep., 2007.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2007}, {"title": "Accelerated l1/2 regularization based sar imaging via bcr and reduced newton skills", "author": ["J. Zeng", "Z. Xu", "B. Zhang", "W. Hong", "Y. Wu"], "venue": "Signal Processing, vol. 93, no. 7, pp. 1831\u20131844, 2013.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 1831}, {"title": "l1/2 regularization: convergence of iterative half thresholding algorithm", "author": ["H.J. Zeng", "S. Lin", "Y. Wang", "Z. Xu"], "venue": "IEEE Transactions on Signal Processing, vol. 62, no. 9, pp. 2317\u20132329, 2014.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast l1-minimization algorithms for robust face recognition", "author": ["A. Yang", "Z. Zhou", "A. Balasubramanian", "S. Sastry", "Y. Ma"], "venue": "IEEE Transactions on Image Processing, vol. 22, no. 8, pp. 3234\u20133246, 2013.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2013}, {"title": "Coordinate and subspace optimization methods for linear least squares with non-quadratic regularization", "author": ["M. Elad", "B. Matalon", "M. Zibulevsky"], "venue": "Applied and Computational Harmonic Analysis, vol. 23, no. 3, pp. 346\u2013367, 2007.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2007}, {"title": "Messagepassing algorithms for compressed sensing", "author": ["D.L. Donoho", "A. Maleki", "A. Montanari"], "venue": "Proceedings of the National Academy of Sciences, vol. 106, no. 45, pp. 18 914\u201318 919, 2009.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2009}, {"title": "Nesta: a fast and accurate first-order method for sparse recovery", "author": ["S. Becker", "J. Bobin", "E.J. Candes"], "venue": "SIAM Journal on Imaging Sciences, vol. 4, no. 1, pp. 1\u201339, 2011.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2011}, {"title": "Templates for convex cone problems with applications to sparse signal recovery", "author": ["S.R. Becker", "E.J. Candes", "M.C. Grant"], "venue": "Mathematical Programming Computation, vol. 3, no. 3, pp. 165\u2013218, 2011.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2011}, {"title": "A new approach to variable selection in least squares problems", "author": ["M.R. Osborne", "B. Presnell", "B.A. Turlach"], "venue": "IMA journal of numerical analysis, vol. 20, no. 3, pp. 389\u2013403, 2000.  JOURNAL  37", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2000}, {"title": "Recovery of sparse representations by polytope faces pursuit", "author": ["M.D. Plumbley"], "venue": "Independent Component Analysis and Blind Signal Separation, 2006, pp. 206\u2013 213.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast and accurate algorithms for re-weighted l1-norm minimization", "author": ["M.S. Asif", "J. Romberg"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 23, pp. 5905 \u2013 5916, 2012.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2012}, {"title": "Homotopy continuation for sparse signal representation", "author": ["D.M. Malioutov", "M. Cetin", "A.S. Willsky"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP\u201905), vol. 5, 2005, pp. v733\u2013v736.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2005}, {"title": "An homotopy algorithm for the lasso with online observations", "author": ["P. Garrigues", "L.E. Ghaoui"], "venue": "Advances in neural information processing systems, 2009, pp. 489\u2013 496.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2009}, {"title": "Primal dual pursuit: A homotopy based algorithm for the dantzig selector", "author": ["M.S. Asif"], "venue": "Master\u2019s thesis, Georgia Institute of Technology, Atlanta, Georgia, USA, 2008.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2008}, {"title": "Dynamic updating for l1 minimization", "author": ["M.S. Asif", "J. Romberg"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 4, no. 2, pp. 421\u2013434, 2010.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse recovery of streaming signals using l1homotopy", "author": ["\u2014\u2014"], "venue": "arXiv preprint arXiv:1306.3331, 2013.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic compressive sensing: Sparse recovery algorithms for streaming signals and video", "author": ["M.S. Asif"], "venue": "Ph.D. dissertation, Georgia Institute of Technology, Atlanta, Georgia, Unit State of America, 2013.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2013}, {"title": "An algorithm for the machine calculation of complex fourier series", "author": ["J. Cooley", "J. Tukey"], "venue": "Mathematics of computation, vol. 19, no. 90, pp. 297\u2013301, 1965.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 1965}, {"title": "Dictionaries for sparse representation modeling", "author": ["R. Rubinstein", "A. Bruckstein", "M. Elad"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 1045\u20131057, 2010.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2010}, {"title": "From heuristic optimization to dictionary learning: a review and comprehensive comparison of image denoising algorithms", "author": ["L. Shao", "R. Yan", "X. Li", "Y. Liu"], "venue": "IEEE Transactions on Cybernetics, vol. 44, no. 7, pp. 1001\u20131013, 2014.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2014}, {"title": "Noise removal via bayesian wavelet coring", "author": ["E. Simoncelli", "E. Adelson"], "venue": "International Conference on Image Processing, vol. 1, 1996, pp. 379\u2013382.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 1996}, {"title": "Automatic fault feature extraction of mechanical anomaly on induction motor bearing using ensemble super-wavelet transform", "author": ["W. He", "Y. Zi", "B. Chen", "F. Wu", "Z. He"], "venue": "Mechanical Systems and Signal Processing, vol. 54, pp. 457\u2013480, 2015.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse geometric image representations with bandelets", "author": ["E.L. Pennec", "S. Mallat"], "venue": "IEEE Transactions on Image Processing, vol. 14, no. 4, pp. 423\u2013438, 2005.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2005}, {"title": "The curvelet transform for image denoising", "author": ["J. Starck", "E. Candes", "D. Donoho"], "venue": "Image Processing, IEEE Transactions on, vol. 11, no. 6, pp. 670\u2013684, 2002.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2002}, {"title": "The contourlet transform: an efficient directional multiresolution image representa-  tion", "author": ["M. Do", "M. Vetterli"], "venue": "IEEE Transactions on Image Processing, vol. 14, no. 12, pp. 2091\u20132106, 2005.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2005}, {"title": "Shiftable multiscale transforms", "author": ["E. Simoncelli", "W. Freeman", "E. Adelson", "D. Heeger"], "venue": "IEEE Transactions on Information Theory, vol. 38, no. 2, pp. 587\u2013607, 1992.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 1992}, {"title": "A nonconvex relaxation approach to sparse dictionary learning", "author": ["J. Shi", "X. Ren", "G. Dai", "J. Wang", "Z. Zhang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011, pp. 1809\u20131816.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2011}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["J. Fan", "R. Li"], "venue": "Journal of the American Statistical Association, vol. 96, no. 456, pp. 1348\u20131360, 2001.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2001}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "author": ["C.H. Zhang"], "venue": "The Annals of Statistics, pp. 894\u2013942, 2010.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast sparse regression and classification", "author": ["J.H. Friedman"], "venue": "International Journal of Forecasting, vol. 28, no. 3, pp. 722\u2013738, 2012.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2012}, {"title": "Dictionaries for sparse representation modeling", "author": ["R. Rubinstein", "A.M. Bruckstein", "M. Elad"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 1045\u20131057, 2010.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2010}, {"title": "Dictionary learning", "author": ["I. Tosic", "P. Frossard"], "venue": "IEEE Signal Processing Magazine, vol. 28, no. 2, pp. 27\u201338, 2011.", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2011}, {"title": "l0 norm based dictionary learning by proximal methods with global convergence", "author": ["C. Bao", "H. Ji", "Y. Quan", "Z. Shen"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 3858\u20133865.", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-frame compression: Theory and design", "author": ["K. Engan", "S.O. Aase", "J.H.Husy"], "venue": "Signal Processing, vol. 80, no. 10, pp. 2121\u20132140, 2000.", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2000}, {"title": "Dictionary learning algorithms for sparse representation", "author": ["K. Kreutz-Delgado", "J.F. Murray", "B.D. Rao", "K. Engan", "T.W. Lee", "T.J. Sejnowski"], "venue": "Neural computation, vol. 15, no. 2, pp. 349\u2013396, 2003.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2003}, {"title": "k-svd: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M.E.M. Aharon", "A. Bruckstein"], "venue": "IEEE Transactions on Signal Processing, vol. 54, no. 11, pp. 4311\u20134322, 2006.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 2006}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "Proceedings of the 27th International Conference on Machine Learning, 2010, pp. 487\u2013494.", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2010}, {"title": "Group sparse coding", "author": ["S. Bengio", "F. Pereira", "Y. Singer", "D. Strelow"], "venue": "Proceedings of the Advances in Neural Information Processing Systems, 2009, pp. 82\u2013 89.", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2009}, {"title": "Online detection of unusual events in videos via dynamic sparse coding", "author": ["B. Zhao", "F. L", "E.P. Xing"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 3313\u20133320.  JOURNAL  38", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust visual tracking via multi-task sparse learning", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2042\u20132049.", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminative k-svd for dictionary learning in face recognition", "author": ["Q. Zhang", "B. Li"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 2691\u20132698.", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2010}, {"title": "Tag localization with spatial correlations and joint group sparsity", "author": ["Y. Yang", "Y. Yang", "Z. Huang", "H.T. Shen", "F. Nie"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 881\u2013888.", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2011}, {"title": "Compression of facial images using the k-svd algorithm", "author": ["O. Bryt", "M. Elad"], "venue": "Journal of Visual Communication and Image Representation, vol. 19, no. 4, pp. 270\u2013282, 2008.", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2008}, {"title": "Locality-constrained linear coding for image classification", "author": ["J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 3360\u20133367.", "citeRegEx": "130", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonparametric bayesian dictionary learning for analysis of noisy and incomplete images", "author": ["M. Zhou", "H. Chen", "J. Paisley", "L. Ren", "L. Li", "Z. Xing", "D. Dunson", "G. Sapiro", "L. Carin"], "venue": "IEEE Transactions on Image Processing, vol. 21, no. 1, pp. 130\u2013144, 2012.", "citeRegEx": "131", "shortCiteRegEx": null, "year": 2012}, {"title": "An mdl framework for sparse coding and dictionary learning", "author": ["I. Ramirez", "G. Sapiro"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 6, pp. 2913\u20132927, 2012.", "citeRegEx": "132", "shortCiteRegEx": null, "year": 2012}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 19\u2013 60, 2010.", "citeRegEx": "133", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse variation dictionary learning for face recognition with a single sample per person", "author": ["M. Yang", "L. Van", "L. Zhang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2013, pp. 689\u2013696.", "citeRegEx": "134", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning a discriminative dictionary for sparse coding via label consistent k-svd", "author": ["Z. Jiang", "Z. Lin", "L.S. Davis"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011, pp. 1697\u20131704.", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2011}, {"title": "Label consistent k-svd: learning a discriminative dictionary for recognition", "author": ["Z. Jiang", "Z. Lin", "L. Davis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 11, pp. 2651\u20132664, 2013.", "citeRegEx": "136", "shortCiteRegEx": null, "year": 2013}, {"title": "Fisher discrimination dictionary learning for sparse representation", "author": ["M. Yang", "D. Zhang", "X. Feng"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2011, pp. 543\u2013550.", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2011}, {"title": "Iterative projection methods for structured sparsity regularization", "author": ["L. Rosasco", "A. Verri", "M. Santoro", "S. Mosci", "S. Villa"], "venue": "Massachusetts institute of technology, cambridge, MA, USA, Tech. Rep., 2009.", "citeRegEx": "138", "shortCiteRegEx": null, "year": 2009}, {"title": "Metaface learning for sparse representation based face recogni-  tion", "author": ["M. Yang", "L. Zhang", "J. Yang", "D. Zhang"], "venue": "Proceedings of the IEEE International Conference on Image Processing (ICIP), 2010, pp. 1601\u20131604.", "citeRegEx": "139", "shortCiteRegEx": null, "year": 2010}, {"title": "Classification and clustering via dictionary learning with structured incoherence and shared features", "author": ["I. Ramirez", "P. Sprechmann", "G. Sapiro"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 3501\u20133508.", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2010}, {"title": "A dictionary learning approach for classification: separating the particularity and the commonality", "author": ["S. Kong", "D. Wang"], "venue": "European Conference on Computer Vision (ECCV), 2012, pp. 186\u2013199.", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning categoryspecific dictionary and shared dictionary for finegrained image categorization", "author": ["S. Gao", "I.W. Tsang", "Y. Ma"], "venue": "IEEE Transactions on Image Processing, vol. 23, no. 2, pp. 623\u2013634, 2013.", "citeRegEx": "142", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast dictionary learning for sparse representations of speech signals", "author": ["M.G. Jafari", "M.D. Plumbley"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 5, no. 5, pp. 1025\u20131031, 2011.", "citeRegEx": "143", "shortCiteRegEx": null, "year": 2011}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "Proceedings of the Advances in neural information processing systems, 2009, pp. 1033\u20131040.", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning inter-related visual dictionary for object recognition", "author": ["N. Zhou", "Y. Shen", "J. Peng", "J. Fan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3490\u20133497.", "citeRegEx": "145", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse representation for face recognition based on discriminative low-rank dictionary learning", "author": ["L. Ma", "C. Wang", "B. Xiao", "W. Zhou"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2586\u20132593.", "citeRegEx": "146", "shortCiteRegEx": null, "year": 2012}, {"title": "Simultaneous feature and dictionary learning for image set based face recognition", "author": ["J. Lu", "G. Wang", "W. Deng", "P. Moulin"], "venue": "European Conference on Computer Vision (ECCV), 2014, pp. 265\u2013280.", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2014}, {"title": "Latent dictionary learning for sparse representation based classification", "author": ["M. Yang", "D. Dai", "L. Shen", "L.V. Gool"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 4138\u20134145.", "citeRegEx": "148", "shortCiteRegEx": null, "year": 2014}, {"title": "Submodular dictionary learning for sparse coding", "author": ["Z. Jiang", "G. Zhang", "L.S. Davis"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3418\u20133425.", "citeRegEx": "149", "shortCiteRegEx": null, "year": 2012}, {"title": "Support vector guided dictionary learning", "author": ["S. Cai", "W. Zuo", "L. Zhang", "X. Feng", "P. Wang"], "venue": "European Conference on Computer Vision (ECCV), 2014, pp. 624\u2013639.", "citeRegEx": "150", "shortCiteRegEx": null, "year": 2014}, {"title": "A new sparse featurebased patch for dense correspondence", "author": ["X. Qin", "J. Shen", "X. Li", "Y. Jia"], "venue": "Proceedings of the IEEE International Conference on Multimedia and Expo (ICME), 2014, pp. 1\u20136.", "citeRegEx": "151", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse representation for blind image quality assessment", "author": ["L. He", "D. Tao", "X. Li", "X. Gao"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 1146\u20131153.", "citeRegEx": "152", "shortCiteRegEx": null, "year": 2012}, {"title": "Image  JOURNAL  39 super-resolution as sparse representation of raw image patches", "author": ["J. Yang", "J. Wright", "Y. Ma", "T. Huang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2008, pp. 1\u2013 8.", "citeRegEx": "153", "shortCiteRegEx": null, "year": 2008}, {"title": "Example-based super-resolution", "author": ["W.T. Freeman", "T.R. Jones", "E.C. Pasztor"], "venue": "IEEE Computer Graphics and Applications, vol. 22, no. 2, pp. 56\u201365, 2002.", "citeRegEx": "154", "shortCiteRegEx": null, "year": 2002}, {"title": "Motion analysis for image enhancement: Resolution, occlusion, and transparency", "author": ["M. Irani", "S. Peleg"], "venue": "Journal of Visual Communication and Image Representation, vol. 4, no. 4, pp. 324\u2013335, 1993.", "citeRegEx": "155", "shortCiteRegEx": null, "year": 1993}, {"title": "Image super-resolution via sparse representation", "author": ["J. Yang", "J. Wright", "T.S. Huang", "Y. Ma"], "venue": "IEEE Transactions on Image Processing, vol. 19, no. 11, pp. 2861\u2013 2873, 2010.", "citeRegEx": "156", "shortCiteRegEx": null, "year": 2010}, {"title": "Greedy regression in sparse coding space for single-image superresolution", "author": ["Y. Tang", "Y. Yuan", "P. Yan", "X. Li"], "venue": "Journal of visual communication and image representation, vol. 24, no. 2, pp. 148\u2013159, 2013.", "citeRegEx": "157", "shortCiteRegEx": null, "year": 2013}, {"title": "Image super-resolution via dual-dictionary learning and sparse representation", "author": ["J. Zhang", "C. Zhao", "R. Xiong", "S. Ma", "D. Zhao"], "venue": "Proceedings of the IEEE International Symposium on Circuits and Systems (IS- CAS), 2012, pp. 1688\u20131691.", "citeRegEx": "158", "shortCiteRegEx": null, "year": 2012}, {"title": "Image superresolution with sparse neighbor embedding", "author": ["X. Gao", "K. Zhang", "D. Tao", "X. Li"], "venue": "IEEE Transactions on Image Processing, vol. 21, no. 7, pp. 3194\u20133205, 2012.", "citeRegEx": "159", "shortCiteRegEx": null, "year": 2012}, {"title": "Superresolution via transform-invariant group-sparse regularization", "author": ["C. Fernandez-Granda", "E.J. Candes"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2013, pp. 3336\u20133343.", "citeRegEx": "160", "shortCiteRegEx": null, "year": 2013}, {"title": "Geometry constrained sparse coding for single image superresolution", "author": ["X. Lu", "H. Yuan", "P. Yan", "Y. Yuan", "X. Li"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 1648\u20131655.", "citeRegEx": "161", "shortCiteRegEx": null, "year": 2012}, {"title": "Superresolution with nonlocal regularized sparse representation", "author": ["W. Dong", "G. Shi", "L. Zhang", "X. Wu"], "venue": "Proceedings of the Visual Communications and Image Processing, 2010, pp. 77 440H\u201377 440H.", "citeRegEx": "162", "shortCiteRegEx": null, "year": 2010}, {"title": "Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization", "author": ["W. Dong", "D. Zhang", "G. Shi"], "venue": "IEEE Transactions on Image Processing, vol. 20, no. 7, pp. 1838\u20131857, 2011.", "citeRegEx": "163", "shortCiteRegEx": null, "year": 1838}, {"title": "Super-resolution with sparse mixing estimators", "author": ["S. Mallat", "G. Yu"], "venue": "Transactions on Image Processing, vol. 19, no. 11, pp. 2889\u20132900, 2010.", "citeRegEx": "164", "shortCiteRegEx": null, "year": 2010}, {"title": "De-noising by soft-thresholding", "author": ["D.L. Donoho"], "venue": "IEEE Transactions on Information Theory, vol. 41, no. 3, pp. 613\u2013627, 1995.", "citeRegEx": "165", "shortCiteRegEx": null, "year": 1995}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Transactions on Image Processing, vol. 15, no. 12, pp. 3736\u20133745, 2006.", "citeRegEx": "166", "shortCiteRegEx": null, "year": 2006}, {"title": "Image denoising by sparse 3-d transform-domain col-  laborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Transactions on Image Processing, vol. 16, no. 8, pp. 2080\u20132095, 2007.", "citeRegEx": "167", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse representation for color image restoration", "author": ["J. Mairal", "M. Elad", "G. Sapiro"], "venue": "IEEE Transactions on Image Processing, vol. 17, no. 1, pp. 53\u201369, 2008.", "citeRegEx": "168", "shortCiteRegEx": null, "year": 2008}, {"title": "Image sequence denoising via sparse and redundant representations", "author": ["M. Protter", "M. Elad"], "venue": "IEEE Transactions on Image Processing, vol. 18, no. 1, pp. 27\u201335, 2009.", "citeRegEx": "169", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparsity-based image denoising via dictionary learning and structural clustering", "author": ["W. Dong", "X. Li", "D. Zhang", "G. Shi"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 457\u2013464.", "citeRegEx": "170", "shortCiteRegEx": null, "year": 2011}, {"title": "Mixed noise removal by weighted encoding with sparse nonlocal regularization", "author": ["J. Jiang", "L. Zhang", "J. Yang"], "venue": "IEEE Transactions on Image Processing, vol. 23, no. 6, pp. 2651\u20132662, 2014.", "citeRegEx": "171", "shortCiteRegEx": null, "year": 2014}, {"title": "Weighted nuclear norm minimization with application to image denoising", "author": ["S. Gu", "L. Zhang", "W. Zuo", "X. Feng"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014.", "citeRegEx": "172", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust video denoising using low rank matrix completion", "author": ["H. Ji", "C. Liu", "Z. Shen", "Y. Xu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 1791\u20131798.", "citeRegEx": "173", "shortCiteRegEx": null, "year": 2010}, {"title": "Image denoising via group sparse representation over learned dictionary", "author": ["P. Cheng", "C. Deng", "S. Wang", "C. Zhang"], "venue": "Proceedings of the Eighth International Symposium on Multispectral Image Processing and Pattern Recognition, 2013, pp. 891 916\u2013891 916.", "citeRegEx": "174", "shortCiteRegEx": null, "year": 2013}, {"title": "A new twist: two-step iterative shrinkage/thresholding algorithms for image restoration", "author": ["J.M. Bioucas-Dias", "M. Figueiredo"], "venue": "IEEE Transactions on Image Processing, vol. 16, no. 12, pp. 2992\u20133004, 2007.", "citeRegEx": "175", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning multiscale sparse representations for image and video restoration", "author": ["J. Mairal", "G. Sapiro", "M. Elad"], "venue": "Multiscale Modeling and Simulation, vol. 7, no. 1, pp. 214\u2013241, 2008.", "citeRegEx": "176", "shortCiteRegEx": null, "year": 2008}, {"title": "Non-local sparse models for image restoration", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2009, pp. 2272\u20132279.", "citeRegEx": "177", "shortCiteRegEx": null, "year": 2009}, {"title": "From learning models of natural image patches to whole image restoration", "author": ["D. Zoran", "Y. Weiss"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2011, pp. 479\u2013486.", "citeRegEx": "178", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast sparsity-based orthogonal dictionary learning for image restoration", "author": ["C. Bao", "J.F. Cai", "H. Ji"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 3384\u20133391.", "citeRegEx": "179", "shortCiteRegEx": null, "year": 2013}, {"title": "Group-based sparse representation for image restoration", "author": ["J. Zhang", "D. Zhao", "W. Gao"], "venue": "IEEE Transactions on Image Processing, vol. 34, no. 9, pp. 1864\u2013 1870, 2014.", "citeRegEx": "180", "shortCiteRegEx": null, "year": 1864}, {"title": "Centralized sparse representation for image restoration", "author": ["W. Dong", "D. Zhang", "G. Shi"], "venue": "Proceedings of the IEEE International Conference on Computer Vision,  JOURNAL  40 2011, pp. 1259\u20131266.", "citeRegEx": "181", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonlocally centralized sparse representation for image restoration", "author": ["W. Dong", "L. Zhang", "G. Shi", "X. Li"], "venue": "IEEE Transactions on Image Processing, vol. 22, no. 4, pp. 1620\u20131630, 2013.", "citeRegEx": "182", "shortCiteRegEx": null, "year": 2013}, {"title": "A non-local algorithm for image denoising", "author": ["A. Buades", "B. Coll", "J.M. Morel"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, vol. 2, 2005, pp. 60\u201365.", "citeRegEx": "183", "shortCiteRegEx": null, "year": 2005}, {"title": "Convex analysis and optimization", "author": ["A. Nedic", "D. Bertsekas", "A. Ozdaglar"], "venue": "Athena Scientific, 2003.", "citeRegEx": "184", "shortCiteRegEx": null, "year": 2003}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C.D. Mol"], "venue": "Communications on pure and applied mathematics, vol. 57, no. 11, pp. 1413\u20131457, 2004.", "citeRegEx": "185", "shortCiteRegEx": null, "year": 2004}, {"title": "Extended src: Undersampled face recognition via intraclass variant dictionary", "author": ["W. Deng", "J. Hu", "J. Guo"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 9, pp. 1864\u20131870, 2012.", "citeRegEx": "186", "shortCiteRegEx": null, "year": 1864}, {"title": "In defense of sparsity based face recognition", "author": ["\u2014\u2014"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 399\u2013406.", "citeRegEx": "187", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum correntropy criterion for robust face recognition", "author": ["R. He", "W.S. Zheng", "B.G. Hu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 8, pp. 1561\u20131576, 2011.", "citeRegEx": "188", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust sparse coding for face recognition", "author": ["M. Yang", "D. Zhang", "J. Yang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 625\u2013632.", "citeRegEx": "189", "shortCiteRegEx": null, "year": 2011}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 1794\u20131801.", "citeRegEx": "190", "shortCiteRegEx": null, "year": 2009}, {"title": "Kernel sparse representation for image classification and face recognition", "author": ["S. Gao", "I.W.H. Tsang", "L.T. Chia"], "venue": "European Conference on Computer Vision (ECCV), 2010, pp. 1\u201314.", "citeRegEx": "191", "shortCiteRegEx": null, "year": 2010}, {"title": "Local features are not lonely-laplacian sparse coding for image classification", "author": ["S. Gao", "I.W. Tsang", "L.T. Chia", "P. Zhao"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 3555\u20133561.", "citeRegEx": "192", "shortCiteRegEx": null, "year": 2010}, {"title": "Discriminative affine sparse codes for image classification", "author": ["N. Kulkarni", "B. Li"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 1609\u20131616.", "citeRegEx": "193", "shortCiteRegEx": null, "year": 2011}, {"title": "Image classification by non-negative sparse coding, low-rank and sparse decomposition", "author": ["C. Zhang", "J. Liu", "Q. Tian", "C. Xu", "H. Lu", "S. Ma"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 1673\u20131680.", "citeRegEx": "194", "shortCiteRegEx": null, "year": 2011}, {"title": "Low-rank sparse coding for image classification", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "C. Xu", "N. Ahuja"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 281\u2013288.", "citeRegEx": "195", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning structured low-rank representations for image classification", "author": ["Y. Zhang", "Z. Jiang", "L.S. Davis"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 676\u2013683.", "citeRegEx": "196", "shortCiteRegEx": null, "year": 2013}, {"title": "Rank preserving sparse learning for kinect based scene classification", "author": ["D. Tao", "L. Jin", "Y. Zhao", "X. Li"], "venue": "IEEE Transactions on Cybernetics, vol. 43, no. 5, pp. 1406\u20131417, 2013.", "citeRegEx": "197", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative tensor sparse coding for image classification", "author": ["Y. Zhang", "Z. Jiang", "L.S. Davis"], "venue": "Proceedings of the British Machine Vision Conference, 2013.", "citeRegEx": "198", "shortCiteRegEx": null, "year": 2013}, {"title": "Constructing a non-negative low rank and sparse graph with data-adaptive features", "author": ["L. Zhuang", "S. Gao", "J. Tang", "J. Wang", "Z. Lin", "Y. Ma"], "venue": "arXiv preprint arXiv:1409.0964, 2014.", "citeRegEx": "199", "shortCiteRegEx": null, "year": 2014}, {"title": "On the relevance of sparsity for image classification", "author": ["R. Rigamonti", "V. Lepetit", "G. Gonzlez", "E. Turetken", "F. Benmansour", "M. Brown", "P. Fua"], "venue": "Computer Vision and Image Understanding, vol. 125, pp. 115\u2013 127, 2014.", "citeRegEx": "200", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust visual tracking using l1 minimization", "author": ["X. Mei", "H. Ling"], "venue": "Proceedings of the IEEE 12th International Conference on Computer Vision, 2009, pp. 1436\u20131443.", "citeRegEx": "201", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust visual tracking and vehicle classification via sparse representation", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 11, pp. 2259\u20132272, 2011.", "citeRegEx": "202", "shortCiteRegEx": null, "year": 2011}, {"title": "Real-time visual tracking using compressive sensing", "author": ["H. Li", "C. Shen", "Q. Shi"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 1305\u20131312.", "citeRegEx": "203", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust object tracking based on sparse representation", "author": ["S. Zhang", "H. Yao", "X. Sun", "S. Liu"], "venue": "Proceedings of the Visual Communications and Image Processing, 2010, pp. 77 441N\u201377 441N\u20138.", "citeRegEx": "204", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust visual tracking via multi-task sparse learning", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2042\u20132049.", "citeRegEx": "205", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual tracking via adaptive structural local sparse appearance model", "author": ["X. Jia", "H. Lu", "M.H. Yang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 1822\u20131829.", "citeRegEx": "206", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust and fast collaborative tracking with two stage sparse optimization", "author": ["B. Liu", "L. Yang", "J. Huang", "P. Meer", "L. Gong", "C. Kulikowski"], "venue": "European Conference on Computer Vision (ECCV), 2010, pp. 624\u2013637.", "citeRegEx": "207", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust tracking using local sparse appearance model and kselection", "author": ["B. Liu", "J. Huang", "C. Kulikowski", "L. Yang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 1313\u20131320.", "citeRegEx": "208", "shortCiteRegEx": null, "year": 2011}, {"title": "Real time robust l1 tracker using accelerated proximal gradient approach", "author": ["C. Bao", "Y. Wu", "H. Ling", "H. Ji"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 1830\u20131837.  JOURNAL  41", "citeRegEx": "209", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust object tracking via sparsity-based collaborative model", "author": ["W. Zhong", "H. Lu", "M.H. Yang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 1838\u20131845.", "citeRegEx": "210", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast compressive tracking", "author": ["K. Zhang", "L. Zhang", "M. Yang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 10, pp. 2002\u20132015, 2014.", "citeRegEx": "211", "shortCiteRegEx": null, "year": 2002}, {"title": "Robust visual tracking with discriminative sparse learning", "author": ["X. Lu", "Y. Yuan", "P. Yan"], "venue": "Pattern Recognition, vol. 46, no. 7, pp. 1762\u20131771, 2013.", "citeRegEx": "212", "shortCiteRegEx": null, "year": 2013}, {"title": "Online robust non-negative dictionary learning for visual tracking", "author": ["N. Wang", "J. Wang", "D.Y. Yeung"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 657\u2013664.", "citeRegEx": "213", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse coding based visual tracking: Review and experimental comparison", "author": ["S. Zhang", "H. Yao", "X. Sun", "X. Lu"], "venue": "Pattern Recognition, vol. 46, no. 7, pp. 1772\u2013 1788, 2013.", "citeRegEx": "214", "shortCiteRegEx": null, "year": 2013}, {"title": "Visual tracking: An experimental survey", "author": ["A. Smeulders", "D. Chu", "R. Cucchiara", "S. Calderara", "A. Dehghan", "M. Shah"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 7, pp. 1442\u2013 1468, 2013.", "citeRegEx": "215", "shortCiteRegEx": null, "year": 2013}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F. Samaria", "A. Harter"], "venue": "Proceedings of the Second IEEE Workshop on Applications of Computer Vision, 1994, pp. 138\u2013142.", "citeRegEx": "216", "shortCiteRegEx": null, "year": 1994}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["G. Huang", "M. Ramesh", "T. Berg", "E. Learned-Miller"], "venue": "Technical Report 07-49, University of Massachusetts, Amherst, Tech. Rep., 2007.", "citeRegEx": "217", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse tensor discriminant color space for face verification", "author": ["S. Wang", "J. Yang", "M. Sun", "X. Peng", "M. Sun", "C. Zhou"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 6, pp. 876\u2013 888, 2012.", "citeRegEx": "218", "shortCiteRegEx": null, "year": 2012}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A. Georghiades", "P. Belhumeur", "D. Kriegman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 6, pp. 643\u2013660, 2001.", "citeRegEx": "219", "shortCiteRegEx": null, "year": 2001}, {"title": "Columbia object image library (coil-20)", "author": ["S. Nene", "S. Nayar", "H. Murase"], "venue": "Technical Report CUCS-005- 96, Tech. Rep., 1996.", "citeRegEx": "220", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "W ITH advancements in mathematics, linear representation methods (LRBM) have been well studied and have recently received considerable attention [1, 2].", "startOffset": 145, "endOffset": 151}, {"referenceID": 1, "context": "W ITH advancements in mathematics, linear representation methods (LRBM) have been well studied and have recently received considerable attention [1, 2].", "startOffset": 145, "endOffset": 151}, {"referenceID": 2, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 3, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 4, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 5, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 6, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 7, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 8, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 9, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 10, "context": "Sparse representation, from the viewpoint of its origin, is directly related to compressed sensing (CS) [11\u201313], which is one of the most popular topics in recent years.", "startOffset": 104, "endOffset": 111}, {"referenceID": 11, "context": "Sparse representation, from the viewpoint of its origin, is directly related to compressed sensing (CS) [11\u201313], which is one of the most popular topics in recent years.", "startOffset": 104, "endOffset": 111}, {"referenceID": 12, "context": "Sparse representation, from the viewpoint of its origin, is directly related to compressed sensing (CS) [11\u201313], which is one of the most popular topics in recent years.", "startOffset": 104, "endOffset": 111}, {"referenceID": 10, "context": "Donoho [11] first proposed the original concept of compressed sensing.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "[13], from the mathematical perspective, demonstrated the rationale of CS theory, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Baraniuk [12] provided a concrete analysis of compressed sensing and presented a specific interpretation on some solutions of different signal reconstruction algorithms.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 11, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 12, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 13, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 14, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 15, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 16, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 3, "context": "As an indispensable prerequisite of CS theory, the sparse representation theory [4, 7\u201310, 17] is the most outstanding technique used to conquer difficulties that appear in many fields.", "startOffset": 80, "endOffset": 93}, {"referenceID": 6, "context": "As an indispensable prerequisite of CS theory, the sparse representation theory [4, 7\u201310, 17] is the most outstanding technique used to conquer difficulties that appear in many fields.", "startOffset": 80, "endOffset": 93}, {"referenceID": 7, "context": "As an indispensable prerequisite of CS theory, the sparse representation theory [4, 7\u201310, 17] is the most outstanding technique used to conquer difficulties that appear in many fields.", "startOffset": 80, "endOffset": 93}, {"referenceID": 8, "context": "As an indispensable prerequisite of CS theory, the sparse representation theory [4, 7\u201310, 17] is the most outstanding technique used to conquer difficulties that appear in many fields.", "startOffset": 80, "endOffset": 93}, {"referenceID": 9, "context": "As an indispensable prerequisite of CS theory, the sparse representation theory [4, 7\u201310, 17] is the most outstanding technique used to conquer difficulties that appear in many fields.", "startOffset": 80, "endOffset": 93}, {"referenceID": 16, "context": "As an indispensable prerequisite of CS theory, the sparse representation theory [4, 7\u201310, 17] is the most outstanding technique used to conquer difficulties that appear in many fields.", "startOffset": 80, "endOffset": 93}, {"referenceID": 3, "context": "For example, the methodology of sparse representation is a novel signal sampling method for the sparse or compressible signal and has been successfully applied to signal processing [4\u20136].", "startOffset": 181, "endOffset": 186}, {"referenceID": 4, "context": "For example, the methodology of sparse representation is a novel signal sampling method for the sparse or compressible signal and has been successfully applied to signal processing [4\u20136].", "startOffset": 181, "endOffset": 186}, {"referenceID": 5, "context": "For example, the methodology of sparse representation is a novel signal sampling method for the sparse or compressible signal and has been successfully applied to signal processing [4\u20136].", "startOffset": 181, "endOffset": 186}, {"referenceID": 17, "context": "Sparse representation has attracted much attention in recent years and many examples in different fields can be found where sparse representation is definitely beneficial and favorable [18, 19].", "startOffset": 185, "endOffset": 193}, {"referenceID": 18, "context": "Sparse representation has attracted much attention in recent years and many examples in different fields can be found where sparse representation is definitely beneficial and favorable [18, 19].", "startOffset": 185, "endOffset": 193}, {"referenceID": 19, "context": "The sparse representation based classification (SRC) method [20] first assumes that the test sample can be sufficiently represented by samples from the same subject.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "The literature [20] has also demonstrated that the SRC method has great superiorities when addressing the image classification issue on corrupted or disguised images.", "startOffset": 15, "endOffset": 19}, {"referenceID": 3, "context": "In the context of compressed sensing, it is first assumed that all the signals are sparse or approximately sparse enough [4, 6, 7].", "startOffset": 121, "endOffset": 130}, {"referenceID": 5, "context": "In the context of compressed sensing, it is first assumed that all the signals are sparse or approximately sparse enough [4, 6, 7].", "startOffset": 121, "endOffset": 130}, {"referenceID": 6, "context": "In the context of compressed sensing, it is first assumed that all the signals are sparse or approximately sparse enough [4, 6, 7].", "startOffset": 121, "endOffset": 130}, {"referenceID": 20, "context": "Moreover, in the field of image classification, the representation based classification methods consist of two main categories in terms of the way of exploiting the \u201catoms\u201d: the holistic representation based method and local representation based method [21].", "startOffset": 253, "endOffset": 257}, {"referenceID": 8, "context": "A typical and representative local sparse representation methods is the two-phase test sample sparse representation (TPTSR) method [9].", "startOffset": 131, "endOffset": 134}, {"referenceID": 21, "context": "The literature [22] suggests that sparse representation algorithms roughly fall into three classes: convex relaxation, greedy algorithms, and combinational methods.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "In the literature [23, 24], from the perspective of sparse problem modeling and problem solving, sparse decomposition algorithms are generally divided into two sections: greedy algorithms and convex relaxation algorithms.", "startOffset": 18, "endOffset": 26}, {"referenceID": 23, "context": "In the literature [23, 24], from the perspective of sparse problem modeling and problem solving, sparse decomposition algorithms are generally divided into two sections: greedy algorithms and convex relaxation algorithms.", "startOffset": 18, "endOffset": 26}, {"referenceID": 24, "context": "[25] reviewed some optimization techniques for solving l1norm regularization problems and roughly divided these approaches into three optimization strategies: sub-gradient methods, unconstrained approximation methods, and constrained optimization methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Because of the fact that this problem is an NP-hard problem [26], the greedy strategy provides an approximate solution to alleviate this difficulty.", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "The greedy strategy searches for the best local optimal solution in each iteration with the goal of achieving the optimal holistic solution [27].", "startOffset": 140, "endOffset": 144}, {"referenceID": 27, "context": "(3) Proximal algorithms can be treated as a powerful tool for solving nonsmooth, constrained, large-scale, or distributed versions of the optimization problem [28].", "startOffset": 159, "endOffset": 163}, {"referenceID": 6, "context": "2: Geometric interpretations of different norms in 2-D space [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "Actually, the l0-norm is the limit as p \u2192 0 of the lp-norms [8] and the definition of the l0-norm is formulated as", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "4: The geometry of the solutions of different norm regularization in 2-D space [7].", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "3: Geometric interpretations of different norms in 1-D space [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 28, "context": "However, it has been demonstrated that the representation solution of the l2-norm minimization is not strictly sparse enough but \u201climitedly-sparse\u201d, which means it possesses the capability of discriminability [30].", "startOffset": 209, "endOffset": 213}, {"referenceID": 29, "context": "The representation results in sparse representation, however, can be greatly dominated by the regularizer (or optimizer) imposed on the representation solution [32\u201335].", "startOffset": 160, "endOffset": 167}, {"referenceID": 30, "context": "The representation results in sparse representation, however, can be greatly dominated by the regularizer (or optimizer) imposed on the representation solution [32\u201335].", "startOffset": 160, "endOffset": 167}, {"referenceID": 31, "context": "The representation results in sparse representation, however, can be greatly dominated by the regularizer (or optimizer) imposed on the representation solution [32\u201335].", "startOffset": 160, "endOffset": 167}, {"referenceID": 32, "context": "The representation results in sparse representation, however, can be greatly dominated by the regularizer (or optimizer) imposed on the representation solution [32\u201335].", "startOffset": 160, "endOffset": 167}, {"referenceID": 33, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 197, "endOffset": 205}, {"referenceID": 34, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 197, "endOffset": 205}, {"referenceID": 37, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 267, "endOffset": 275}, {"referenceID": 41, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 329, "endOffset": 337}, {"referenceID": 42, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 393, "endOffset": 400}, {"referenceID": 43, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 393, "endOffset": 400}, {"referenceID": 44, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 393, "endOffset": 400}, {"referenceID": 45, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 393, "endOffset": 400}, {"referenceID": 46, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 393, "endOffset": 400}, {"referenceID": 8, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 454, "endOffset": 465}, {"referenceID": 47, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 454, "endOffset": 465}, {"referenceID": 48, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 454, "endOffset": 465}, {"referenceID": 49, "context": "2 with the l0norm minimization constraint [52].", "startOffset": 42, "endOffset": 46}, {"referenceID": 38, "context": "The l1-norm originates from the Lasso problem [41, 42] and it has been extensively used to address issues in machine learning, pattern recognition, and statistics [53\u201355].", "startOffset": 46, "endOffset": 54}, {"referenceID": 39, "context": "The l1-norm originates from the Lasso problem [41, 42] and it has been extensively used to address issues in machine learning, pattern recognition, and statistics [53\u201355].", "startOffset": 46, "endOffset": 54}, {"referenceID": 50, "context": "The l1-norm originates from the Lasso problem [41, 42] and it has been extensively used to address issues in machine learning, pattern recognition, and statistics [53\u201355].", "startOffset": 163, "endOffset": 170}, {"referenceID": 51, "context": "The l1-norm originates from the Lasso problem [41, 42] and it has been extensively used to address issues in machine learning, pattern recognition, and statistics [53\u201355].", "startOffset": 163, "endOffset": 170}, {"referenceID": 52, "context": "The l1-norm originates from the Lasso problem [41, 42] and it has been extensively used to address issues in machine learning, pattern recognition, and statistics [53\u201355].", "startOffset": 163, "endOffset": 170}, {"referenceID": 25, "context": "Although the sparse representation method with l0-norm minimization can obtain the fundamental sparse solution of \u03b1 over the matrix X , the problem is still a non-deterministic polynomial-time hard (NP-hard) problem and the solution is difficult to approximate [26].", "startOffset": 261, "endOffset": 265}, {"referenceID": 19, "context": "Recent literature [20, 56\u201358] has demonstrated that when the representation solution obtained by using the l1-norm minimization constraint is also content with the condition of sparsity and the solution using l1-norm minimization with sufficient sparsity can be equivalent to the solution obtained by l0-norm minimization with full probability.", "startOffset": 18, "endOffset": 29}, {"referenceID": 53, "context": "Recent literature [20, 56\u201358] has demonstrated that when the representation solution obtained by using the l1-norm minimization constraint is also content with the condition of sparsity and the solution using l1-norm minimization with sufficient sparsity can be equivalent to the solution obtained by l0-norm minimization with full probability.", "startOffset": 18, "endOffset": 29}, {"referenceID": 54, "context": "Recent literature [20, 56\u201358] has demonstrated that when the representation solution obtained by using the l1-norm minimization constraint is also content with the condition of sparsity and the solution using l1-norm minimization with sufficient sparsity can be equivalent to the solution obtained by l0-norm minimization with full probability.", "startOffset": 18, "endOffset": 29}, {"referenceID": 55, "context": "9 [59\u201361].", "startOffset": 2, "endOffset": 9}, {"referenceID": 56, "context": "9 [59\u201361].", "startOffset": 2, "endOffset": 9}, {"referenceID": 57, "context": "9 [59\u201361].", "startOffset": 2, "endOffset": 9}, {"referenceID": 28, "context": "the solution has the property that it is discriminative and distinguishable but is not really sparse enough [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 58, "context": "16) On the other hand, the l2,1-norm is also called the rotation invariant l1-norm, which is proposed to overcome the difficulty of robustness to outliers [62].", "startOffset": 155, "endOffset": 159}, {"referenceID": 42, "context": "Sparse representation with the l2,1-norm minimization can be implemented by exploiting the proposed algorithms in literature [45\u201347].", "startOffset": 125, "endOffset": 132}, {"referenceID": 43, "context": "Sparse representation with the l2,1-norm minimization can be implemented by exploiting the proposed algorithms in literature [45\u201347].", "startOffset": 125, "endOffset": 132}, {"referenceID": 44, "context": "Sparse representation with the l2,1-norm minimization can be implemented by exploiting the proposed algorithms in literature [45\u201347].", "startOffset": 125, "endOffset": 132}, {"referenceID": 6, "context": "The core idea of the greedy strategy [7, 23] is to determine the position based on the relationship between the atom and probe sample, and then to use the least square to evaluate the amplitude value.", "startOffset": 37, "endOffset": 44}, {"referenceID": 22, "context": "The core idea of the greedy strategy [7, 23] is to determine the position based on the relationship between the atom and probe sample, and then to use the least square to evaluate the amplitude value.", "startOffset": 37, "endOffset": 44}, {"referenceID": 6, "context": "However, the greedy algorithm can always produce the global optimal solution or an approximate overall solution [7, 23].", "startOffset": 112, "endOffset": 119}, {"referenceID": 22, "context": "However, the greedy algorithm can always produce the global optimal solution or an approximate overall solution [7, 23].", "startOffset": 112, "endOffset": 119}, {"referenceID": 19, "context": "3, is an NP hard problem [20, 56].", "startOffset": 25, "endOffset": 33}, {"referenceID": 53, "context": "3, is an NP hard problem [20, 56].", "startOffset": 25, "endOffset": 33}, {"referenceID": 59, "context": "The matching pursuit (MP) algorithm [63] is the earliest and representative method of using the greedy strategy to approximate problem III.", "startOffset": 36, "endOffset": 40}, {"referenceID": 59, "context": "More detailed analysis on matching pursuit algorithms can be found in the literature [63].", "startOffset": 85, "endOffset": 89}, {"referenceID": 33, "context": "The orthogonal matching pursuit (OMP) algorithm [36, 64] is an improvement of the MP algorithm.", "startOffset": 48, "endOffset": 56}, {"referenceID": 60, "context": "The orthogonal matching pursuit (OMP) algorithm [36, 64] is an improvement of the MP algorithm.", "startOffset": 48, "endOffset": 56}, {"referenceID": 33, "context": "It has been verified that the OMP algorithm can be converged in limited iterations [36].", "startOffset": 83, "endOffset": 87}, {"referenceID": 61, "context": "Many more greedy algorithms based on the MP and OMP algorithm such as the efficient orthogonal matching pursuit algorithm [65] subsequently have been proposed to improve the pursuit algorithm.", "startOffset": 122, "endOffset": 126}, {"referenceID": 34, "context": "proposed an regularized version of orthogonal matching pursuit (ROMP) algorithm [37], which recovered all k sparse signals based on the Restricted Isometry Property of random frequency measurements, and then proposed another variant of OMP algorithm called compressive sampling matching pursuit (CoSaMP) algorithm [66], which incorporated several existing ideas such as restricted isometry property (RIP) and pruning technique into a greedy iterative structure of OMP.", "startOffset": 80, "endOffset": 84}, {"referenceID": 62, "context": "proposed an regularized version of orthogonal matching pursuit (ROMP) algorithm [37], which recovered all k sparse signals based on the Restricted Isometry Property of random frequency measurements, and then proposed another variant of OMP algorithm called compressive sampling matching pursuit (CoSaMP) algorithm [66], which incorporated several existing ideas such as restricted isometry property (RIP) and pruning technique into a greedy iterative structure of OMP.", "startOffset": 314, "endOffset": 318}, {"referenceID": 63, "context": "proposed an extension of OMP, called stage-wise orthogonal matching pursuit (StOMP) algorithm [67], which depicted an iterative algorithm with three main steps, i.", "startOffset": 94, "endOffset": 98}, {"referenceID": 64, "context": "Dai and Milenkovic proposed a new method for sparse signal reconstruction named subspace pursuit (SP) algorithm [68], which sampled signals satisfying the constraints of the RIP with a constant parameter.", "startOffset": 112, "endOffset": 116}, {"referenceID": 65, "context": "presented a sparsity adaptive matching pursuit (SAMP) algorithm [69], which borrowed the idea of the EM algorithm to alternatively estimate the sparsity and support set.", "startOffset": 64, "endOffset": 68}, {"referenceID": 66, "context": "proposed a tree-based matching pursuit (TMP) algorithm [70], which constructed a tree structure and employed a structuring strategy to cluster similar signal atoms from a highly redundant dictionary as a new dictionary.", "startOffset": 55, "endOffset": 59}, {"referenceID": 67, "context": "Subsequently, La and Do proposed a new tree-based orthogonal matching pursuit (TBOMP) algorithm [71], which treated the sparse tree representation as an additional prior knowledge for linear inverse systems by using a small number of samples.", "startOffset": 96, "endOffset": 100}, {"referenceID": 68, "context": "Recently, Karahanoglu and Erdogan conceived a forward-backward pursuit (FBP) method [72] with two greedy stages, in which the forward stage enlarged the support estimation and the backward stage removed some unsatisfied atoms.", "startOffset": 84, "endOffset": 88}, {"referenceID": 22, "context": "More detailed treatments of the greedy pursuit for sparse representation can be found in the literature [23].", "startOffset": 104, "endOffset": 108}, {"referenceID": 69, "context": "The first key procedure of gradient projection sparse reconstruction (GPSR) [73] provides a constrained formulation where each value of \u03b1 can be split into its positive and negative parts.", "startOffset": 76, "endOffset": 80}, {"referenceID": 69, "context": "For more detailed information, one can refer to the literature [73].", "startOffset": 63, "endOffset": 67}, {"referenceID": 27, "context": "The Interior-point method [31] is not an iterative algorithm but a smooth mathematic model and it always incorporates the Newton method to efficiently solve unconstrained smooth problems of modest size [28].", "startOffset": 202, "endOffset": 206}, {"referenceID": 70, "context": "l1 ls) problem [74].", "startOffset": 15, "endOffset": 19}, {"referenceID": 70, "context": "For further description and analyses, please refer to the literature [74].", "startOffset": 69, "endOffset": 73}, {"referenceID": 71, "context": "[75] is a very effective method to solve the l1-norm regularization problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 72, "context": "[76] also utilized the TNIPM to solve large scale logistic regression problems, which employed a preconditioned conjugate gradient method to compute the search step size with warm-start techniques.", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "Mehrotra proposed to exploit the interior-point method to address the primal-dual problem [77] and introduced the second-order derivation of Taylor polynomial to approximate a primal-dual trajectory.", "startOffset": 90, "endOffset": 94}, {"referenceID": 74, "context": "More analyses of interior-point method for sparse representation can be found in the literature [78].", "startOffset": 96, "endOffset": 100}, {"referenceID": 40, "context": "This section shows how the ADM [43] is used to solve primal and dual problems in III.", "startOffset": 31, "endOffset": 35}, {"referenceID": 40, "context": "For more information, please refer to the literature [43, 79].", "startOffset": 53, "endOffset": 61}, {"referenceID": 75, "context": "For more information, please refer to the literature [43, 79].", "startOffset": 53, "endOffset": 61}, {"referenceID": 27, "context": "The proximity algorithm is frequently employed to solve nonsmooth, constrained convex optimization problems [28].", "startOffset": 108, "endOffset": 112}, {"referenceID": 76, "context": "The objective function of ISTA [80] has the form of", "startOffset": 31, "endOffset": 35}, {"referenceID": 76, "context": "The techniques used here are called linearization or preconditioning and more detailed information can be found in the literature [80, 81].", "startOffset": 130, "endOffset": 138}, {"referenceID": 77, "context": "The techniques used here are called linearization or preconditioning and more detailed information can be found in the literature [80, 81].", "startOffset": 130, "endOffset": 138}, {"referenceID": 78, "context": "FISTA [82] not only preserves the efficiency of the original ISTA but also promotes the effectiveness of ISTA so that FISTA can obtain global convergence.", "startOffset": 6, "endOffset": 10}, {"referenceID": 78, "context": "The backtracking linear research strategy can also be utilized to explore a more feasible value of L and more detailed analyses on FISTA can be found in the literature [82, 83].", "startOffset": 168, "endOffset": 176}, {"referenceID": 79, "context": "The backtracking linear research strategy can also be utilized to explore a more feasible value of L and more detailed analyses on FISTA can be found in the literature [82, 83].", "startOffset": 168, "endOffset": 176}, {"referenceID": 80, "context": "Sparse reconstruction by separable approximation (SpaRSA) [84] is another typical proximity algorithm based on sparse representation, which can be viewed as an accelerated version of ISTA.", "startOffset": 58, "endOffset": 62}, {"referenceID": 81, "context": "9 using the Barzilai-Borwein (BB) spectral method [85].", "startOffset": 50, "endOffset": 54}, {"referenceID": 82, "context": "[86] concluded that the technique that exploits a decreasing value of \u03bb from a warm-starting point can more efficiently solve the sub-problem VI.", "startOffset": 0, "endOffset": 4}, {"referenceID": 80, "context": "The sparse reconstruction by separable approximation (SpaRSA) is summarized in Algorithm 6 and more information can be found in the literature [84].", "startOffset": 143, "endOffset": 147}, {"referenceID": 83, "context": "However, the research group led by Zongben Xu summarizes the conclusion that the most impressive and representative algorithm of the lp-norm (0<p<1) regularization is sparse representation with the l1/2-norm regularization [87].", "startOffset": 223, "endOffset": 227}, {"referenceID": 56, "context": "Moreover, they have proposed some effective methods to solve the l1/2norm regularization problem [60, 88].", "startOffset": 97, "endOffset": 105}, {"referenceID": 84, "context": "Moreover, they have proposed some effective methods to solve the l1/2norm regularization problem [60, 88].", "startOffset": 97, "endOffset": 105}, {"referenceID": 56, "context": "In this section, a half proximal algorithm is introduced to solve the l1/2-norm regularization problem [60], which matches the iterative shrinkage thresholding algorithm for the l1-norm regularization discussed above and the iterative hard thresholding algorithm for the l0-norm regularization.", "startOffset": 103, "endOffset": 107}, {"referenceID": 56, "context": "To this end, the resolvent operator [60] is introduced to compute the resolvent solution of the right part of Eq.", "startOffset": 36, "endOffset": 40}, {"referenceID": 56, "context": "The resolvent operator is always satisfied no matter whether the resolvent solution of \u2207(\u2016 \u2022 \u2016 1/2) exists or not [60].", "startOffset": 114, "endOffset": 118}, {"referenceID": 56, "context": "which have been demonstrated in the literature [60].", "startOffset": 47, "endOffset": 51}, {"referenceID": 56, "context": "3 has been conceived and demonstrated in the literature [60].", "startOffset": 56, "endOffset": 60}, {"referenceID": 56, "context": "where the half proximal thresholding operator H [60] is deductively constituted by Eq.", "startOffset": 48, "endOffset": 52}, {"referenceID": 56, "context": "The half proximal thresholding algorithm for l1/2-norm regularization based sparse representation is summarized in Algorithm 7 and more detailed inferences and analyses can be found in the literature [60, 88].", "startOffset": 200, "endOffset": 208}, {"referenceID": 84, "context": "The half proximal thresholding algorithm for l1/2-norm regularization based sparse representation is summarized in Algorithm 7 and more detailed inferences and analyses can be found in the literature [60, 88].", "startOffset": 200, "endOffset": 208}, {"referenceID": 85, "context": "33 is denoted as the primal augmented Lagrangian method (PALM) [89], the dual function of problem III.", "startOffset": 63, "endOffset": 67}, {"referenceID": 85, "context": "9 can also be addressed by the ALM algorithm, which is denoted as the dual augmented Lagrangian method (DALM) [89].", "startOffset": 110, "endOffset": 114}, {"referenceID": 85, "context": "For more detailed description, please refer to the literature [89].", "startOffset": 62, "endOffset": 66}, {"referenceID": 86, "context": "proposed an iterative method named parallel coordinate descent algorithm (PCDA) [90] by introducing the elementwise optimization algorithm to solve the regularized linear least squares with non-quadratic regularization problem.", "startOffset": 80, "endOffset": 84}, {"referenceID": 87, "context": "developed a modified version of the iterative thresholding method, called approximate message passing (AMP) method [91], to satisfy the requirement that the sparsity undersampling tradeoff of the new algorithm is equivalent to the corresponding convex optimization approach.", "startOffset": 115, "endOffset": 119}, {"referenceID": 88, "context": "proposed a generalized Nesterov\u2019s algorithm (NESTA) [92] by employing the continuation-like scheme to accelerate the efficiency and flexibility.", "startOffset": 52, "endOffset": 56}, {"referenceID": 89, "context": "[93] further constructed a general framework, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Further detailed analyses and inference information related to proximity algorithms can be found in the literature [28, 83].", "startOffset": 115, "endOffset": 123}, {"referenceID": 79, "context": "Further detailed analyses and inference information related to proximity algorithms can be found in the literature [28, 83].", "startOffset": 115, "endOffset": 123}, {"referenceID": 90, "context": "The homotopy method was originally proposed to solve the least square problem with the l1-penalty [94].", "startOffset": 98, "endOffset": 102}, {"referenceID": 39, "context": "Having a highly intimate relationship with the conventional sparse representation method such as least angle regression (LAR) [42], OMP [64] and polytope faces pursuit (PFP) [95], the homotopy algorithm has been successfully employed to solve the l1-norm minimization problems.", "startOffset": 126, "endOffset": 130}, {"referenceID": 60, "context": "Having a highly intimate relationship with the conventional sparse representation method such as least angle regression (LAR) [42], OMP [64] and polytope faces pursuit (PFP) [95], the homotopy algorithm has been successfully employed to solve the l1-norm minimization problems.", "startOffset": 136, "endOffset": 140}, {"referenceID": 91, "context": "Having a highly intimate relationship with the conventional sparse representation method such as least angle regression (LAR) [42], OMP [64] and polytope faces pursuit (PFP) [95], the homotopy algorithm has been successfully employed to solve the l1-norm minimization problems.", "startOffset": 174, "endOffset": 178}, {"referenceID": 92, "context": "The fundamental of the homotopy algorithm is that the homotopy solution path is a piecewise linear path with a discrete number of operations while the value of the homotopy parameter changes, and the direction of each segment and the step size are absolutely determined by the sign sequence and the support of the solution on the corresponding segment, respectively [96].", "startOffset": 366, "endOffset": 370}, {"referenceID": 92, "context": "For further description and analyses, please refer to the literature [29, 96].", "startOffset": 69, "endOffset": 77}, {"referenceID": 39, "context": "For further detail description and analyses, please refer to the literature [42].", "startOffset": 76, "endOffset": 80}, {"referenceID": 92, "context": "Based on the homotopy algorithm, Asif and Romberg [96] presented a enhanced sparse representation objective function, a weighted l1-norm minimization, and then provided two fast and accurate solutions, i.", "startOffset": 50, "endOffset": 54}, {"referenceID": 39, "context": "A common method [42, 73] to update the weight W is achieved by exploiting the solution of problem VII.", "startOffset": 16, "endOffset": 24}, {"referenceID": 69, "context": "A common method [42, 73] to update the weight W is achieved by exploiting the solution of problem VII.", "startOffset": 16, "endOffset": 24}, {"referenceID": 92, "context": "The main steps of this algorithm are summarized in Algorithm 11 and more information can be found in literature [96].", "startOffset": 112, "endOffset": 116}, {"referenceID": 93, "context": "first exploited the homotopy method to choose a suitable parameter for l1-norm regularization with a noisy term in an underdetermined system and employed the homotopy continuation-based method to solve BPDN for sparse signal processing [97].", "startOffset": 236, "endOffset": 240}, {"referenceID": 94, "context": "Garrigues and Ghaoui [98] proposed a modified homotopy algorithm to solve the Lasso problem with online observations by optimizing the", "startOffset": 21, "endOffset": 25}, {"referenceID": 39, "context": "[42] proposed a basic pursuit denoising (BPDN) homotopy algorithm, which shrinked the parameter to a final value with series of efficient optimization steps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 95, "context": "Similar to BPDN homotopy, Asif [99] presented a homotopy algorithm for the Dantzing selector (DS) under the consideration of primal and dual solution.", "startOffset": 31, "endOffset": 35}, {"referenceID": 96, "context": "Asif and Romberg [100] proposed a framework of dynamic updating solutions for solving l1-norm minimization programs based on homotopy algorithm and demonstrated its effectiveness in addressing the decoding issue.", "startOffset": 17, "endOffset": 22}, {"referenceID": 97, "context": "More recent literature related to homotopy algorithms can be found in the streaming recovery framework [101] and a summary [102].", "startOffset": 103, "endOffset": 108}, {"referenceID": 98, "context": "More recent literature related to homotopy algorithms can be found in the streaming recovery framework [101] and a summary [102].", "startOffset": 123, "endOffset": 128}, {"referenceID": 99, "context": "The history of modeling dictionary could be traced back to 1960s, such as the fast Fourier transform (FFT) [103].", "startOffset": 107, "endOffset": 112}, {"referenceID": 4, "context": "transform domain method [5], or is devised based on learning, i.", "startOffset": 24, "endOffset": 27}, {"referenceID": 100, "context": "dictionary learning methods [104].", "startOffset": 28, "endOffset": 33}, {"referenceID": 101, "context": "Both of the transform domain and dictionary learning based methods transform image samples into other domains and the similarity of transformation coefficients are exploited [105].", "startOffset": 174, "endOffset": 179}, {"referenceID": 102, "context": "Specifically, the transform domain methods usually represent the image patches by using the orthonormal basis such as over-complete wavelets transform [106], super-wavelet transform [107], bandelets [108], curvelets transform [109], contourlets transform [110] and steerable wavelet filters [111].", "startOffset": 151, "endOffset": 156}, {"referenceID": 103, "context": "Specifically, the transform domain methods usually represent the image patches by using the orthonormal basis such as over-complete wavelets transform [106], super-wavelet transform [107], bandelets [108], curvelets transform [109], contourlets transform [110] and steerable wavelet filters [111].", "startOffset": 182, "endOffset": 187}, {"referenceID": 104, "context": "Specifically, the transform domain methods usually represent the image patches by using the orthonormal basis such as over-complete wavelets transform [106], super-wavelet transform [107], bandelets [108], curvelets transform [109], contourlets transform [110] and steerable wavelet filters [111].", "startOffset": 199, "endOffset": 204}, {"referenceID": 105, "context": "Specifically, the transform domain methods usually represent the image patches by using the orthonormal basis such as over-complete wavelets transform [106], super-wavelet transform [107], bandelets [108], curvelets transform [109], contourlets transform [110] and steerable wavelet filters [111].", "startOffset": 226, "endOffset": 231}, {"referenceID": 106, "context": "Specifically, the transform domain methods usually represent the image patches by using the orthonormal basis such as over-complete wavelets transform [106], super-wavelet transform [107], bandelets [108], curvelets transform [109], contourlets transform [110] and steerable wavelet filters [111].", "startOffset": 255, "endOffset": 260}, {"referenceID": 107, "context": "Specifically, the transform domain methods usually represent the image patches by using the orthonormal basis such as over-complete wavelets transform [106], super-wavelet transform [107], bandelets [108], curvelets transform [109], contourlets transform [110] and steerable wavelet filters [111].", "startOffset": 291, "endOffset": 296}, {"referenceID": 21, "context": "From the notations of the literature [22, 112], the framework of dictionary learning can be generally formulated as an optimization problem", "startOffset": 37, "endOffset": 46}, {"referenceID": 108, "context": "From the notations of the literature [22, 112], the framework of dictionary learning can be generally formulated as an optimization problem", "startOffset": 37, "endOffset": 46}, {"referenceID": 7, "context": "The most representative dictionary learning based on the l0-norm penalty is the K-SVD algorithm [8], which is widely used in image denoising.", "startOffset": 96, "endOffset": 99}, {"referenceID": 108, "context": "In the stage of convex relaxation methods, there are three optimal forms for updating a dictionary: the one by one atom updating method, group atoms updating method, and all atoms updating method [112].", "startOffset": 196, "endOffset": 201}, {"referenceID": 109, "context": "For example, Fan and Li proposed a smoothly clipped absolution deviation (SCAD) penalty [113], which employed an iterative approximate Newton-Raphson method for penalizing least sequences and exploited the penalized likelihood approaches for variable selection in linear regression models.", "startOffset": 88, "endOffset": 93}, {"referenceID": 110, "context": "Zhang introduced and studied the non-convex minimax concave (MC) family [114] of non-convex piecewise quadratic penalties to make unbiased variable selection for the estimation of regression coefficients, which was demonstrated its effectiveness by employing an oracle inequality.", "startOffset": 72, "endOffset": 77}, {"referenceID": 111, "context": "Friedman proposed to use the logarithmic penalty for a model selection [115] and used it to solve the minimization problems with non-convex regularization terms.", "startOffset": 71, "endOffset": 76}, {"referenceID": 112, "context": "From the viewpoint of updating strategy, most of the dictionary learning methods always iteratively update the sparse approximation or representation solution and the dictionary alternatively, and more dictionary learning theoretical results and analyses can be found in the literature [116, 117].", "startOffset": 286, "endOffset": 296}, {"referenceID": 113, "context": "From the viewpoint of updating strategy, most of the dictionary learning methods always iteratively update the sparse approximation or representation solution and the dictionary alternatively, and more dictionary learning theoretical results and analyses can be found in the literature [116, 117].", "startOffset": 286, "endOffset": 296}, {"referenceID": 114, "context": "For example, dictionary learning methods can be divided into three groups in the context of different norms utilized in the penalty term, that is, l0-norm regularization based methods, convex relaxation methods and non-convex relaxation methods [118].", "startOffset": 245, "endOffset": 250}, {"referenceID": 115, "context": "The first category is dictionary learning under the probabilistic framework such as maximum likelihood methods [119], the method of optimal directions (MOD) [120], and the maximum a posteriori probability method [121].", "startOffset": 157, "endOffset": 162}, {"referenceID": 116, "context": "The first category is dictionary learning under the probabilistic framework such as maximum likelihood methods [119], the method of optimal directions (MOD) [120], and the maximum a posteriori probability method [121].", "startOffset": 212, "endOffset": 217}, {"referenceID": 117, "context": "The second category is clustering based dictionary learning approaches such as KSVD [122], which can be viewed as a generalization of K-means.", "startOffset": 84, "endOffset": 89}, {"referenceID": 118, "context": "There are two typical models for these kinds of dictionary learning algorithms, sparse and shift-invariant representation of dictionary learning and structure sparse regularization based dictionary learning, such as hierarchical sparse dictionary learning [123] and group or block sparse dictionary learning [124].", "startOffset": 256, "endOffset": 261}, {"referenceID": 119, "context": "There are two typical models for these kinds of dictionary learning algorithms, sparse and shift-invariant representation of dictionary learning and structure sparse regularization based dictionary learning, such as hierarchical sparse dictionary learning [123] and group or block sparse dictionary learning [124].", "startOffset": 308, "endOffset": 313}, {"referenceID": 21, "context": "Recently, some researchers [22] categorized the latest methods of dictionary learning into four groups, online dictionary learning [125], joint dictionary learning [126], discriminative dictionary learning [127], and supervised dictionary learning [128].", "startOffset": 27, "endOffset": 31}, {"referenceID": 120, "context": "Recently, some researchers [22] categorized the latest methods of dictionary learning into four groups, online dictionary learning [125], joint dictionary learning [126], discriminative dictionary learning [127], and supervised dictionary learning [128].", "startOffset": 131, "endOffset": 136}, {"referenceID": 121, "context": "Recently, some researchers [22] categorized the latest methods of dictionary learning into four groups, online dictionary learning [125], joint dictionary learning [126], discriminative dictionary learning [127], and supervised dictionary learning [128].", "startOffset": 164, "endOffset": 169}, {"referenceID": 122, "context": "Recently, some researchers [22] categorized the latest methods of dictionary learning into four groups, online dictionary learning [125], joint dictionary learning [126], discriminative dictionary learning [127], and supervised dictionary learning [128].", "startOffset": 206, "endOffset": 211}, {"referenceID": 123, "context": "Recently, some researchers [22] categorized the latest methods of dictionary learning into four groups, online dictionary learning [125], joint dictionary learning [126], discriminative dictionary learning [127], and supervised dictionary learning [128].", "startOffset": 248, "endOffset": 253}, {"referenceID": 124, "context": "Unsupervised dictionary learning methods have been widely implemented to solve image processing problems, such as image compression, and feature coding of image representation [129, 130].", "startOffset": 176, "endOffset": 186}, {"referenceID": 125, "context": "Unsupervised dictionary learning methods have been widely implemented to solve image processing problems, such as image compression, and feature coding of image representation [129, 130].", "startOffset": 176, "endOffset": 186}, {"referenceID": 117, "context": "(1) KSVD for unsupervised dictionary learning One of the most representative unsupervised dictionary learning algorithms is the KSVD method [122], which is a modification or an extension of method of directions (MOD) algorithm.", "startOffset": 140, "endOffset": 145}, {"referenceID": 117, "context": "The specific KSVD algorithm for dictionary learning is summarized to Algorithm 12 and more information can be found in the literature [122].", "startOffset": 134, "endOffset": 139}, {"referenceID": 125, "context": "(2) Locality constrained linear coding for unsupervised dictionary learning The locality constrained linear coding (LLC) algorithm [130] is an efficient local coordinate linear coding method, which projects each descriptor into a local constraint system to obtain an effective codebook or dictionary.", "startOffset": 131, "endOffset": 136}, {"referenceID": 125, "context": "It has been demonstrated that the property of locality is more essential than sparsity, because the locality must lead to sparsity but not vice-versa, that is, a necessary condition of sparsity is locality, but not the reverse [130].", "startOffset": 227, "endOffset": 232}, {"referenceID": 125, "context": "The main steps of the incremental codebook optimization algorithm are summarized in Algorithm 13 and more information can be found in the literature [130].", "startOffset": 149, "endOffset": 154}, {"referenceID": 118, "context": "[123] proposed a tree-structured dictionary learning problem, which exploited tree-structured sparse regularization to model the relationship between each atom and defined a proximal operator to solve the primaldual problem.", "startOffset": 0, "endOffset": 5}, {"referenceID": 126, "context": "[131] developed a nonparametric Bayesian dictionary learning algorithm, which utilized hierarchical Bayesian to model parameters and employed the truncated beta-Bernoulli process to learn the dictionary.", "startOffset": 0, "endOffset": 5}, {"referenceID": 127, "context": "Ramirez and Shapiro [132] employed minimum description length to", "startOffset": 20, "endOffset": 25}, {"referenceID": 128, "context": "proposed an online dictionary learning [133] algorithm based on stochastic approximations, which treated the dictionary learning problem as the optimization of a smooth convex problem over a convex set and employed an iterative online algorithm at each step to solve the subproblems.", "startOffset": 39, "endOffset": 44}, {"referenceID": 129, "context": "Yang and Zhang proposed a sparse variation dictionary learning (SVDL) algorithm [134] for face recognition with a single training sample, in which a joint learning framework of adaptive projection and a sparse variation dictionary with sparse bases were simultaneously constructed from the gallery image set to the generic image set.", "startOffset": 80, "endOffset": 85}, {"referenceID": 108, "context": "proposed a minimax concave penalty based sparse dictionary learning (MCPSDL) [112] algorithm, which employed a non-convex relaxation online", "startOffset": 77, "endOffset": 82}, {"referenceID": 114, "context": "proposed a dictionary learning by proximal algorithm (DLPM) [118], which provided an efficient alternating proximal algorithm for solving the l0-norm minimization based dictionary learning problem and demonstrated its global convergence property.", "startOffset": 60, "endOffset": 65}, {"referenceID": 122, "context": "Discriminative KSVD (DKSVD) [127] was designed to solve image classification problems.", "startOffset": 28, "endOffset": 33}, {"referenceID": 117, "context": "13 is the same as the framework of KSVD [122] in Eq.", "startOffset": 40, "endOffset": 45}, {"referenceID": 19, "context": "The original sparse representation for face recognition [20] regards the raw data as the dictionary, and then reports its promising classification results.", "startOffset": 56, "endOffset": 60}, {"referenceID": 130, "context": "In this section, a label consistent KSVD (LC-KSVD) [135, 136] is introduced to learn an effective discriminative dictionary for image classification.", "startOffset": 51, "endOffset": 61}, {"referenceID": 131, "context": "In this section, a label consistent KSVD (LC-KSVD) [135, 136] is introduced to learn an effective discriminative dictionary for image classification.", "startOffset": 51, "endOffset": 61}, {"referenceID": 130, "context": "The LC-KSVD demonstrates that the obtained solution, compared to other methods, can prevent learning a suboptimal or local optimal solution in the process of learning a dictionary [135].", "startOffset": 180, "endOffset": 185}, {"referenceID": 132, "context": "(3) Fisher discrimination dictionary learning for sparse representation Fisher discrimination dictionary learning (FDDL) [137] incorporates the supervised information (class label information) and the Fisher discrimination message into the objective function for learning a structured discriminative dictionary, which is used for pattern classification.", "startOffset": 121, "endOffset": 126}, {"referenceID": 133, "context": "23 can be solved by the iterative projection method in the literature [138].", "startOffset": 70, "endOffset": 75}, {"referenceID": 134, "context": "25 computes the dictionary class by class and it can be solved by exploiting the algorithm in the literature [139].", "startOffset": 109, "endOffset": 114}, {"referenceID": 134, "context": "[139] presented a metaface dictionary learning method, which is motivated by \u2018metagenes\u2019 in gene expression data analysis.", "startOffset": 0, "endOffset": 5}, {"referenceID": 135, "context": "Rodriguez and Sapiro [140] produced a discriminative non-parametric dictionary learning (DNDL) framework based on the OMP algorithm for image classification.", "startOffset": 21, "endOffset": 26}, {"referenceID": 136, "context": "[141] introduced a learned dictionary with commonalty and particularity, called DL-COPAR, which integrated an incoherence penalty term into the objective function for obtaining the class-specific sub-dictionary.", "startOffset": 0, "endOffset": 5}, {"referenceID": 137, "context": "[142] learned a hybrid dictionary, i.", "startOffset": 0, "endOffset": 5}, {"referenceID": 138, "context": "Jafari and Plumbley [143] presented a greedy adaptive dictionary learning method, which updated the learned dictionary with a minimum sparsity index.", "startOffset": 20, "endOffset": 25}, {"referenceID": 139, "context": "Some other supervised dictionary learning methods are also competent in image classification, such as supervised dictionary learning in [144].", "startOffset": 136, "endOffset": 141}, {"referenceID": 140, "context": "[145] developed a joint dictionary learning algorithm for object categorization, which jointly learned a commonly shared dictionary and multiply category-specific dictionaries for correlated object classes and incorporated the Fisher discriminant fidelity term into the process of dictionary learning.", "startOffset": 0, "endOffset": 5}, {"referenceID": 135, "context": "proposed a method of dictionary learning with structured incoherence (DLSI) [140], which unified the dictionary learning and sparse decomposition into a sparse dictionary learning framework for image classification and data clustering.", "startOffset": 76, "endOffset": 81}, {"referenceID": 141, "context": "presented a discriminative lowrank dictionary learning for sparse representation (DLRD SR) [146], in which the sparsity and the low-rank properties were integrated into one dictionary learning scheme where subdictionary with discriminative power was required to be lowrank.", "startOffset": 91, "endOffset": 96}, {"referenceID": 142, "context": "developed a simultaneous feature and dictionary learning [147] method for face recognition, which jointly learned the feature projection matrix for subspace learning and the discriminative structured dictionary.", "startOffset": 57, "endOffset": 62}, {"referenceID": 143, "context": "introduced a latent dictionary learning (LDL) [148] method for sparse representation based image classification, which simultaneously learned a discriminative dictionary and a latent representation model based on the correlations between label information and dictionary atoms.", "startOffset": 46, "endOffset": 51}, {"referenceID": 144, "context": "presented a submodular dictionary learning (SDL) [149] method, which integrated the entropy rate of a random walk on a graph and a discriminative term into a unified objective function and devised a greedybased approach to optimize it.", "startOffset": 49, "endOffset": 54}, {"referenceID": 145, "context": "developed a support vector guided dictionary learning (SVGDL) [150] method, which constructed a discriminative term by using adaptively weighted summation of the squared distances for all pairwise of the sparse representation solutions.", "startOffset": 62, "endOffset": 67}, {"referenceID": 146, "context": "Recently, sparse representation methods have been extensively applied to numerous real-world applications [151, 152].", "startOffset": 106, "endOffset": 116}, {"referenceID": 147, "context": "Recently, sparse representation methods have been extensively applied to numerous real-world applications [151, 152].", "startOffset": 106, "endOffset": 116}, {"referenceID": 148, "context": "The most representative work was proposed to exploit the sparse representation theory to generate a superresolution (SRSR) image from a single low-resolution image in literature [153].", "startOffset": 178, "endOffset": 183}, {"referenceID": 148, "context": "To this end, SRSR [153] provides a prior knowledge assumption, which is formulated as", "startOffset": 18, "endOffset": 23}, {"referenceID": 149, "context": "One-pass algorithm similar to that of [154] is introduced to enhance the compatibility between adjacent patches.", "startOffset": 38, "endOffset": 43}, {"referenceID": 150, "context": "33 can be solved by the back-projection method in [155] and the obtained image X\u2217 is regarded as the final optimal high-resolution image.", "startOffset": 50, "endOffset": 55}, {"referenceID": 148, "context": "The entire super-resolution via sparse representation is summarized in algorithm 14 and more information can be found in the literature [153].", "startOffset": 136, "endOffset": 141}, {"referenceID": 151, "context": "presented a modified version called joint dictionary learning via sparse representation (JDLSR) [156], which jointly learned", "startOffset": 96, "endOffset": 101}, {"referenceID": 152, "context": "[157] first explicitly analyzed the rationales of the sparse representation theory in performing the super-resolution task, and proposed to exploit the L2-Boosting strategy to learn coupled dictionaries, which were employed to construct sparse coding space.", "startOffset": 0, "endOffset": 5}, {"referenceID": 153, "context": "[158] presented an image super-resolution reconstruction scheme by employing the dualdictionary learning and sparse representation method for image super-resolution reconstruction and Gao et al.", "startOffset": 0, "endOffset": 5}, {"referenceID": 154, "context": "[159] proposed a sparse neighbor embedding method, which incorporated the sparse neighbor search and HoG clustering method into the process of image super-resolution reconstruction.", "startOffset": 0, "endOffset": 5}, {"referenceID": 155, "context": "FernandezGranda and Candes [160] designed a transform-invariant group sparse regularizer by implementing a data-driven nonparametric regularizers with learned domain transform on group sparse representation for high image super-resolution.", "startOffset": 27, "endOffset": 32}, {"referenceID": 156, "context": "[161] proposed a geometry constrained sparse representation method for single image super-resolution by jointly obtaining an optimal sparse solution and learning a discriminative and reconstructive dictionary.", "startOffset": 0, "endOffset": 5}, {"referenceID": 157, "context": "[162] proposed to harness an adaptive sparse optimization with nonlocal regularization based on adaptive principal component analysis enhanced by nonlocal similar patch grouping and", "startOffset": 0, "endOffset": 5}, {"referenceID": 158, "context": "[163] proposed to integrate an adaptive sparse domain selection and an adaptive regularization based on piecewise autoregressive models into the sparse representations framework for single image superresolution reconstruction.", "startOffset": 0, "endOffset": 5}, {"referenceID": 159, "context": "Mallat and Yu [164] proposed a sparse mixing estimator for image super-resolution, which introduced an adaptive estimator models by combining a group of linear inverse estimators based on different prior knowledge for sparse representation.", "startOffset": 14, "endOffset": 19}, {"referenceID": 3, "context": "In the presence of image sparsity and redundancy representation [4, 7], sparse representation for image denoising first extracts the sparse image components, which are regarded as useful information, and then abandons the representation residual, which is treated as the image noise term, and finally reconstructs the image exploiting the pre-obtained sparse components, i.", "startOffset": 64, "endOffset": 70}, {"referenceID": 6, "context": "In the presence of image sparsity and redundancy representation [4, 7], sparse representation for image denoising first extracts the sparse image components, which are regarded as useful information, and then abandons the representation residual, which is treated as the image noise term, and finally reconstructs the image exploiting the pre-obtained sparse components, i.", "startOffset": 64, "endOffset": 70}, {"referenceID": 7, "context": "For example, Donoho [8, 29, 165] first discovered the connection between the compressed sensing and image denoising.", "startOffset": 20, "endOffset": 32}, {"referenceID": 160, "context": "For example, Donoho [8, 29, 165] first discovered the connection between the compressed sensing and image denoising.", "startOffset": 20, "endOffset": 32}, {"referenceID": 161, "context": "Subsequently, the most representative work of using sparse representation to make image denoising was proposed in literature [166], in which a global sparse representation model over learned dictionaries (SRMLD) was used for image denoising.", "startOffset": 125, "endOffset": 130}, {"referenceID": 161, "context": "The algorithm of image denoising exploiting sparse and redundant representation over learned dictionary is summarized in Algorithm 15, and more information can be found in literature [166].", "startOffset": 183, "endOffset": 188}, {"referenceID": 162, "context": "[167] proposed an enhanced sparse representation with a block-matching 3-D (BM3D) transform-domain filter based on self-similarities and an enhanced sparse representation by clustering similar 2-D image patches into 3-D data spaces and an iterative collaborative filtering procedure for image denoising.", "startOffset": 0, "endOffset": 5}, {"referenceID": 163, "context": "[168] proposed the use of extending the KSVD-based grayscale algorithm and a generalized weighted average algorithm for color image denoising.", "startOffset": 0, "endOffset": 5}, {"referenceID": 164, "context": "Protter and Elad [169] extended the techniques of sparse and redundant representations for image sequence denoising by exploiting spatio-temporal atoms, dictionary propagation over time and dictionary learning.", "startOffset": 17, "endOffset": 22}, {"referenceID": 165, "context": "[170] designed a clustering based sparse representation algorithm, which was formulated by a double-header sparse optimization problem built upon dictionary learning and structural clustering.", "startOffset": 0, "endOffset": 5}, {"referenceID": 166, "context": "[171] proposed a variational encoding framework with a weighted sparse nonlocal constraint, which was constructed by integrating image sparsity prior and nonlocal self-similarity prior into a unified regularization term to overcome the mixed noise removal problem.", "startOffset": 0, "endOffset": 5}, {"referenceID": 167, "context": "[172] studied a weighted nuclear norm minimization (WNNM) method with F -norm fidelity under different weighting rules optimized by non-local self-similarity for image denoising.", "startOffset": 0, "endOffset": 5}, {"referenceID": 168, "context": "[173] proposed a patch-based video denoising algorithm by stacking similar patches in both spatial and temporal domain to formulate a low-rank matrix problem with the nuclear norm.", "startOffset": 0, "endOffset": 5}, {"referenceID": 169, "context": "[174] proposed an impressive image denoising method based on an extension of the KSVD algorithm via group sparse representation.", "startOffset": 0, "endOffset": 5}, {"referenceID": 170, "context": "For example, Bioucas-Dias and Figueirdo [175] introduced a two-step iterative shrinkage/thresholding (TwIST) algorithm for image restoration, which is more efficient and can be viewed as an extension of the IST method.", "startOffset": 40, "endOffset": 45}, {"referenceID": 171, "context": "[176] presented a multiscale sparse image representation framework based on the KSVD dictionary learning algorithm and shift-invariant sparsity prior knowledge for restoration of color images and video image sequence.", "startOffset": 0, "endOffset": 5}, {"referenceID": 172, "context": "[177] proposed a learned simultaneous sparse coding (LSSC) model, which integrated sparse dictionary learning and nonlocal self-similarities of natural images into a unified framework for image restoration.", "startOffset": 0, "endOffset": 5}, {"referenceID": 173, "context": "Zoran and Weiss [178] proposed an expected patch log likelihood (EPLL) optimization model, which restored the image from patch to the whole image based on the learned prior knowledge of any patch acquired by Maximum A-Posteriori estimation instead of using simple patch averaging.", "startOffset": 16, "endOffset": 21}, {"referenceID": 174, "context": "[179] proposed a fast orthogonal dictionary learning algorithm, in which a sparse image representation based orthogonal dictionary was learned in image restoration.", "startOffset": 0, "endOffset": 5}, {"referenceID": 175, "context": "[180] proposed a groupbased sparse representation, which combined characteristics from local sparsity and nonlocal self-similarity of natural images to the domain of the group.", "startOffset": 0, "endOffset": 5}, {"referenceID": 176, "context": "[181, 182] proposed a centralized sparse representation (CSR) model, which combined the local and nonlocal sparsity and redundancy properties for variational problem optimization by introducing a concept of sparse coding noise term.", "startOffset": 0, "endOffset": 10}, {"referenceID": 177, "context": "[181, 182] proposed a centralized sparse representation (CSR) model, which combined the local and nonlocal sparsity and redundancy properties for variational problem optimization by introducing a concept of sparse coding noise term.", "startOffset": 0, "endOffset": 10}, {"referenceID": 176, "context": "Here we mainly introduce a recently proposed simple but effective image restoration algorithm CSR model [181].", "startOffset": 104, "endOffset": 109}, {"referenceID": 176, "context": "Moreover, the unbiased estimation of \u03b1x, denoted by E[\u03b1x], empirically can be approximate to \u03b1x under some prior knowledge [181], and then SCN algorithm employs the nonlocal means estimation method [183] to evaluate the unbiased estimation of \u03b1x, that is, using the weighted average of all \u03b1il to approach E[\u03b1x], i.", "startOffset": 123, "endOffset": 128}, {"referenceID": 178, "context": "Moreover, the unbiased estimation of \u03b1x, denoted by E[\u03b1x], empirically can be approximate to \u03b1x under some prior knowledge [181], and then SCN algorithm employs the nonlocal means estimation method [183] to evaluate the unbiased estimation of \u03b1x, that is, using the weighted average of all \u03b1il to approach E[\u03b1x], i.", "startOffset": 198, "endOffset": 203}, {"referenceID": 179, "context": "47 can be optimized by the augmented Lagrange multiplier method [184] or the iterative shrinkage algorithm in [185].", "startOffset": 64, "endOffset": 69}, {"referenceID": 180, "context": "47 can be optimized by the augmented Lagrange multiplier method [184] or the iterative shrinkage algorithm in [185].", "startOffset": 110, "endOffset": 115}, {"referenceID": 176, "context": "The main procedures of the CSR algorithm are summarized in Algorithm 16 and readers may refer to literature [181] for more details.", "startOffset": 108, "endOffset": 113}, {"referenceID": 19, "context": "[20] proposed to employ sparse representation to perform robust face recognition, more and more researchers have been applying the sparse representation theory to the fields of computer vision and pattern recognition, especially in image classification and object tracking.", "startOffset": 0, "endOffset": 4}, {"referenceID": 180, "context": "48) by using the extended iterative shrinkage algorithm in literature [185].", "startOffset": 70, "endOffset": 75}, {"referenceID": 17, "context": "The most representative sparse representation for face recognition has been presented in literature [18] and the general scheme of sparse representation based classification method is summarized in Algorithm 17.", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "[9] proposed a two-phase sparse representation based classification method, which exploited the l2-norm regularization rather than the l1-norm regularization to perform a coarse to fine sparse representation based classification, which was very efficient in comparison with the conventional l1norm regularization based sparse representation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 181, "context": "[186] proposed an extended sparse representation method (ESRM) for improving the robustness of SRC by eliminating the variations in face recognition, such as disguise, occlusion, expression and illumination.", "startOffset": 0, "endOffset": 5}, {"referenceID": 182, "context": "[187] also proposed a framework of superposed sparse representation based classification, which emphasized the prototype and vari-", "startOffset": 0, "endOffset": 5}, {"referenceID": 183, "context": "[188] proposed utilizing the maximum correntropy criterion named CESR embedding non-negative constraint and half-quadratic optimization to present a robust face recognition algorithm.", "startOffset": 0, "endOffset": 5}, {"referenceID": 184, "context": "[189] developed a new robust sparse coding (RSC) algorithm, which first obtained a sparsity-constrained regression model based on maximum likelihood estimation and exploited an iteratively reweighted regularized robust coding algorithm to solve the pre-proposed model.", "startOffset": 0, "endOffset": 5}, {"referenceID": 185, "context": "[190] introduced an extension of the spatial pyramid matching (SPM) algorithm called ScSPM, which incorporated SIFT sparse representation into the spatial pyramid matching algorithm.", "startOffset": 0, "endOffset": 5}, {"referenceID": 186, "context": "[191] developed a kernel sparse representation with the SPM algorithm called KSRSPM, and then proposed another version of an improvement of the SPM called LScSPM [192], which integrated the Laplacian matrix with local features into the objective function of the sparse representation method.", "startOffset": 0, "endOffset": 5}, {"referenceID": 187, "context": "[191] developed a kernel sparse representation with the SPM algorithm called KSRSPM, and then proposed another version of an improvement of the SPM called LScSPM [192], which integrated the Laplacian matrix with local features into the objective function of the sparse representation method.", "startOffset": 162, "endOffset": 167}, {"referenceID": 188, "context": "Kulkarni and Li [193] proposed a discriminative affine sparse codes method (DASC) on a learned affineinvariant feature dictionary from input images and exploited the AdaBoost-based classifier to perform image classification.", "startOffset": 16, "endOffset": 21}, {"referenceID": 189, "context": "[194] proposed integrating the non-negative sparse coding, low-rank and sparse matrix decomposition (LR-ScSPM) method, which exploited non-negative sparse coding and SPM for achieving local features representation and employed low-rank and sparse matrix decomposition for sparse representation, for image classification.", "startOffset": 0, "endOffset": 5}, {"referenceID": 190, "context": "[195] presented a low-rank sparse representation (LRSR) learning method, which preserved the sparsity and spatial consistency in each procedure of feature representation and jointly exploited local features from the same spatial proximal regions for image classification.", "startOffset": 0, "endOffset": 5}, {"referenceID": 191, "context": "[196] developed a structured low-rank sparse representation (SLRSR) method for image classification, which constructed a discriminative dictionary in training terms and exploited low-rank matrix reconstruction for obtaining discriminative representations.", "startOffset": 0, "endOffset": 5}, {"referenceID": 192, "context": "[197] proposed a novel dimension reduction method based on the framework of rank preserving sparse learning, and then exploited the projected samples to make effective Kinect-based scene classification.", "startOffset": 0, "endOffset": 5}, {"referenceID": 193, "context": "[198] proposed a discriminative tensor sparse coding (RTSC) method for robust image classification.", "startOffset": 0, "endOffset": 5}, {"referenceID": 194, "context": "Recently, low-rank based sparse representation became a popular topic such as non-negative low-rank and sparse graph [199].", "startOffset": 117, "endOffset": 122}, {"referenceID": 79, "context": "Some sparse representation methods in face recognition can be found in a review [83] and other more image classification methods can be found in a more recent review [200].", "startOffset": 80, "endOffset": 84}, {"referenceID": 195, "context": "Some sparse representation methods in face recognition can be found in a review [83] and other more image classification methods can be found in a more recent review [200].", "startOffset": 166, "endOffset": 171}, {"referenceID": 196, "context": "employed the idea of sparse representation to visual tracking [201] and vehicle classification [202], which introduced nonnegative sparse constraints and dynamic template updating strategy.", "startOffset": 62, "endOffset": 67}, {"referenceID": 197, "context": "employed the idea of sparse representation to visual tracking [201] and vehicle classification [202], which introduced nonnegative sparse constraints and dynamic template updating strategy.", "startOffset": 95, "endOffset": 100}, {"referenceID": 198, "context": "[203] proposed two realtime compressive sensing visual tracking algorithms based on sparse representation, which adopted dimension reduction and the OMP algorithm to improve the efficiency of recovery procedure in tracking, and also developed a modified version of fusing background templates into the tracking procedure for robust object tracking.", "startOffset": 0, "endOffset": 5}, {"referenceID": 199, "context": "[204] directly treated object tracking as a pattern recognition problem by regarding all the targets as training samples, and then employed the sparse representation classification method to do effective object tracking.", "startOffset": 0, "endOffset": 5}, {"referenceID": 200, "context": "[205] employed the concept of sparse representation based on a particle filter framework to construct a multi-task sparse learning method denoted as multi-task tracking for robust visual tracking.", "startOffset": 0, "endOffset": 5}, {"referenceID": 201, "context": "[206] conceived a structural local sparse appearance model for robust object tracking by integrating the partial and spatial information from the target based on an alignment-pooling algorithm.", "startOffset": 0, "endOffset": 5}, {"referenceID": 202, "context": "[207] proposed constructing a two-stage sparse optimization based online visual tracking method, which jointly minimized the objective reconstruction error and maximized the discriminative capability by choosing distinguishable features.", "startOffset": 0, "endOffset": 5}, {"referenceID": 203, "context": "[208] introduced a local sparse appearance model (SPT) with a static sparse dictionary learned from k-selection and dynamic updated basis distribution to eliminate potential drifting problems in the process of visual tracking.", "startOffset": 0, "endOffset": 5}, {"referenceID": 204, "context": "[209] developed a fast real time l1-tracker called the APG-l1tracker, which exploited the accelerated proximal gradient algorithm to improve the l1-tracker solver in [201].", "startOffset": 0, "endOffset": 5}, {"referenceID": 196, "context": "[209] developed a fast real time l1-tracker called the APG-l1tracker, which exploited the accelerated proximal gradient algorithm to improve the l1-tracker solver in [201].", "startOffset": 166, "endOffset": 171}, {"referenceID": 205, "context": "[210] addressed the object tracking problem by developing a sparsitybased collaborative model, which combined a sparsity-based classifier learned from holistic templates and a sparsity-based template model generated from local representations.", "startOffset": 0, "endOffset": 5}, {"referenceID": 206, "context": "[211] proposed to formulate a sparse feature measurement matrix based on an appearance model by exploiting nonadaptive random projections, and employed a coarse-to-fine strategy to accelerate the computational efficiency of tracking task.", "startOffset": 0, "endOffset": 5}, {"referenceID": 207, "context": "[212] proposed to employ both non-local selfsimilarity and sparse representation to develop a non-local selfsimilarity regularized sparse representation method based on geometrical structure information of the target template data set.", "startOffset": 0, "endOffset": 5}, {"referenceID": 208, "context": "[213] proposed a sparse representation based online two-stage tracking algorithm, which learned a linear classifier based on local sparse representation on favorable image patches.", "startOffset": 0, "endOffset": 5}, {"referenceID": 209, "context": "More detailed visual tracking algorithms can be found in the recent reviews [214, 215].", "startOffset": 76, "endOffset": 86}, {"referenceID": 210, "context": "More detailed visual tracking algorithms can be found in the recent reviews [214, 215].", "startOffset": 76, "endOffset": 86}, {"referenceID": 33, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 102, "endOffset": 106}, {"referenceID": 72, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 114, "endOffset": 118}, {"referenceID": 85, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 125, "endOffset": 129}, {"referenceID": 78, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 137, "endOffset": 141}, {"referenceID": 85, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 148, "endOffset": 152}, {"referenceID": 95, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 163, "endOffset": 167}, {"referenceID": 8, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 178, "endOffset": 181}, {"referenceID": 211, "context": "ORL: The ORL database includes 400 face images taken from 40 subjects each providing 10 face images [216].", "startOffset": 100, "endOffset": 105}, {"referenceID": 212, "context": "LFW face dataset: The Labeled Faces in the Wild (LFW) face database is designed for the study of unconstrained identity verification and face recognition [217].", "startOffset": 154, "endOffset": 159}, {"referenceID": 213, "context": "In our experiments, we chose 1251 images from 86 peoples and each subject has 10-20 images [218].", "startOffset": 91, "endOffset": 96}, {"referenceID": 214, "context": "Extended YaleB face dataset: The extended YaleB database contains 2432 front face images of 38 individuals and each subject having around 64 near frontal images under different illuminations [219].", "startOffset": 191, "endOffset": 196}, {"referenceID": 215, "context": "COIL20 dataset: Columbia Object Image Library (COIL20) database consists of 1,440 size normalized gray-scale images of 20 objects [220].", "startOffset": 130, "endOffset": 135}, {"referenceID": 212, "context": "For example, all these representative algorithms can achieve relatively inferior experimental results on the LFW dataset shown in Subsection IX-B, because the LFW dataset is designed for studying the problem of unconstrained face recognition [217] and most of the face images are captured under complex environments.", "startOffset": 242, "endOffset": 247}], "year": 2016, "abstractText": "Sparse representation has attracted much attention from researchers in fields of signal processing, image processing, computer vision and pattern recognition. Sparse representation also has a good reputation in both theoretical research and practical applications. Many different algorithms have been proposed for sparse representation. The main purpose of this article is to provide a comprehensive study and an updated review on sparse representation and to supply a guidance for researchers. The taxonomy of sparse representation methods can be studied from various viewpoints. For example, in terms of different norm minimizations used in sparsity constraints, the methods can be roughly categorized into five groups: sparse representation with l0-norm minimization, sparse representation with lp-norm (0<p<1) minimization, sparse representation with l1-norm minimization and sparse representation with l2,1-norm minimization. In this paper, a comprehensive overview of sparse representation is provided. The available sparse representation algorithms can also be empirically categorized into four groups: greedy strategy approximation, constrained optimization, proximity algorithmbased optimization, and homotopy algorithm-based sparse representation. The rationales of different algorithms in each category are analyzed and a wide range of sparse representation applications are summarized, which could sufficiently reveal the potential nature of the sparse representation theory. Specifically, an experimentally comparative study of these sparse representation algorithms was presented. The Matlab code used in this paper can be available at: http://www.yongxu.org/lunwen.html.", "creator": "LaTeX with hyperref package"}}}