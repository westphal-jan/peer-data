{"id": "1602.07320", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2016", "title": "Stuck in a What? Adventures in Weight Space", "abstract": "Deep learning researchers commonly suggest that converged models are stuck in local minima. More recently, some researchers observed that under reasonable assumptions, the vast majority of critical points are saddle points, not true minima. Both descriptions suggest that weights converge around a point in weight space, be it a local optima or merely a critical point. However, it's possible that neither interpretation is accurate. As neural networks are typically over-complete, it's easy to show the existence of vast continuous regions through weight space with equal loss. In this paper, we build on recent work empirically characterizing the error surfaces of neural networks. We analyze training paths through weight space, presenting evidence that apparent convergence of loss does not correspond to weights arriving at critical points, but instead to large movements through flat regions of weight space. While it's trivial to show that neural network error surfaces are globally non-convex, we show that error surfaces are also locally non-convex, even after breaking symmetry with a random initialization and also after partial training.", "histories": [["v1", "Tue, 23 Feb 2016 21:23:24 GMT  (255kb,D)", "http://arxiv.org/abs/1602.07320v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zachary c lipton"], "accepted": false, "id": "1602.07320"}, "pdf": {"name": "1602.07320.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zachary C. Lipton"], "emails": ["zlipton@cs.ucsd.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In the worst-case scenario, finding the optimal weights in a neural network is a NP-hard problem. Furthermore, the fault surfaces of neural networks are highly non-convex and represent seemingly formidable obstacles to learning through gradient descent. Yet, practitioners train deep neural networks every day through stochastic gradient descent, achieving state-of-the-art results across a wide range of tasks. In fact, this method easily results in zero loss of training set for many problems. So, while optimization is a huge problem in theory, it could be argued that regulation is more important in practice. This discrepancy between the apparent hopelessness of the optimization problem and the de facto simple training has led several researchers to attempt to characterize both theoretically and empirically the fault surfaces of deep neural networks. Goodfellow et al (2014) in particular, showing the loss along straight lines through the weight space, shows monotonic increases and then decreases in loss."}, {"heading": "1.1 CONTRIBUTIONS", "text": "In this paper, we conduct preliminary experiments in which we analyze a standard three-layer convolutionary neural network with 819557 parameters on the MNIST dataset (LeCun et al., 1998) using dropout methods (http: / / zacklipton.comar Xiv: 160 2.07 320v 1 [cs.L G] February 23, 2016 regularization and \"22 weight decay). We analyze the paths through the weight space taken in the course of the weight decline, and present the following results: \u2022 Weights do not converge to critical points, but travel large (euclidean) distances through flat basins in the weight space. \u2022 While a straight line in the weight space can correspond to monotonically decreasing losses from initialization to resolution, the path actually taken by the weight descent appears to be anything but straight. \u2022 A small number of main components in the weight space may correspond to the monotonically decreasing losses, but the path actually taken by the weight descent does not appear to be soft in either direction of the other neural areas."}, {"heading": "2 EXPERIMENTS", "text": "Instead of drawing straight lines through the weight space, as Goodfellow et al. (2014) did, we examine the paths through the weight space taken during the training of models. We analyze these trajectories qualitatively by visualizing them using 2D-PCA, and quantitatively by analyzing the variance explained by the largest main components. We train a single model for 200 epochs and capture its full parameters after each epoch. We plot this trajectory with a 2D-PCA that shows the high degree of nonlinearity in the learned orbit. We then train 5 models for 200 epochs each, starting from different random initializations and capturing their parameters every ten epochs. Next, we repeat the same experiment but with the same initialization. We train each of the 5 networks on a different basis of data from the same starting point in the weight space. Finally, we train a model for 10 epochs. Then we repeat five times."}, {"heading": "3 RESULTS", "text": "The first two main components explain 81.39% of the variance. The top 10 main components explain 95.63% of the variance. In speculative terms, it seems that the small dimension of the trajectories, together with the smoothness of the curves, could be useful properties for projecting where to look. However, if we build models from the same initialization as in Figure 2, they deviate from each other and find solutions that are far from each other, measured at Euclidean distances. Interestingly, all the solution pairs were equally far from each other and just as far from the origin, indicating strong symmetry in the weight space. These observations apply even if we first prepare the network for 10 epochs (achieving training errors of 1%) before cloning and mixing (Figure 3)."}, {"heading": "4 CONCLUSION", "text": "In these experiments, we present several novel observations on the fault surfaces of neural networks. We demonstrated that paths through the weight space are highly nonlinear and that local minima (albeit good ones) are abundant. Furthermore, we showed that the fault surfaces of neural networks appear to be highly nonconvex even after a symmetry interrupted by random initialization. Furthermore, we showed that the stochasticity induced by the rearrangement of data appears to be sufficient to deflect otherwise identical (and partially trained) networks toward different local minima. We also showed that weight spaces with almost zero loss are not critical points, but large flat basins through which weights continue to train. While these results are interesting, this work should be expanded in future iterations to ensure that these observations hold larger datasets and other common neural network topologies, including multi-layer perceptions and LM net recurrent networks, are actually impossible to achieve by tracing these data networks."}, {"heading": "ACKNOWLEDGMENTS", "text": "Zachary C. Lipton is supported by the Department of Biomedical Informatics, University of California, San Diego, through a NIH / NLM Education Scholarship (T15LM011271). Thanks to NVIDIA Corporation for their generous hardware donations. Thanks to John Berkowitz, Anima Anandkumar, Charles Elkan, and Julian McAuley."}], "references": [{"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["Ian J Goodfellow", "Oriol Vinyals", "Andrew M Saxe"], "venue": "arXiv preprint arXiv:1412.6544,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar"], "venue": "CoRR abs/1506.08473,", "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes", "Christopher JC Burges"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Notably Goodfellow et al. (2014), plotted loss along straight lines through weight space, between two converged models, showing monotonic increases and then decreases in loss.", "startOffset": 8, "endOffset": 33}, {"referenceID": 0, "context": "Dauphin et al. (2014) presented a case based on both empirical study and results from statistical physics suggesting that the ratio of saddle points to local minima on a neural network\u2019s loss surface grows exponentially in the number of parameters.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Dauphin et al. (2014) presented a case based on both empirical study and results from statistical physics suggesting that the ratio of saddle points to local minima on a neural network\u2019s loss surface grows exponentially in the number of parameters. Janzamin et al. (2015) presented a theoretical study, showing that under reasonable conditions on the data, the optimization problem can be made and solved via tensor decomposition.", "startOffset": 0, "endOffset": 272}, {"referenceID": 3, "context": "In this paper, we conduct preliminary experiments, training a standard three layer convolutional neural network with 819557 parameters on the MNIST dataset (LeCun et al., 1998), using dropout \u2217http://zacklipton.", "startOffset": 156, "endOffset": 176}, {"referenceID": 1, "context": "Rather than plotting straight lines through weight space like Goodfellow et al. (2014), we investigate the paths through weight space taken as models are trained.", "startOffset": 62, "endOffset": 87}], "year": 2016, "abstractText": "Deep learning researchers commonly suggest that converged models are stuck in local minima. More recently, some researchers observed that under reasonable assumptions, the vast majority of critical points are saddle points, not true minima. Both descriptions suggest that weights converge around a point in weight space, be it a local optima or merely a critical point. However, it\u2019s possible that neither interpretation is accurate. As neural networks are typically over-complete, it\u2019s easy to show the existence of vast continuous regions through weight space with equal loss. In this paper, we build on recent work empirically characterizing the error surfaces of neural networks. We analyze training paths through weight space, presenting evidence that apparent convergence of loss does not correspond to weights arriving at critical points, but instead to large movements through flat regions of weight space. While it\u2019s trivial to show that neural network error surfaces are globally non-convex, we show that error surfaces are also locally nonconvex, even after breaking symmetry with a random initialization and also after partial training.", "creator": "LaTeX with hyperref package"}}}