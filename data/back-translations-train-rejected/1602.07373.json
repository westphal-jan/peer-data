{"id": "1602.07373", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "On Study of the Binarized Deep Neural Network for Image Classification", "abstract": "Recently, the deep neural network (derived from the artificial neural network) has attracted many researchers' attention by its outstanding performance. However, since this network requires high-performance GPUs and large storage, it is very hard to use it on individual devices. In order to improve the deep neural network, many trials have been made by refining the network structure or training strategy. Unlike those trials, in this paper, we focused on the basic propagation function of the artificial neural network and proposed the binarized deep neural network. This network is a pure binary system, in which all the values and calculations are binarized. As a result, our network can save a lot of computational resource and storage. Therefore, it is possible to use it on various devices. Moreover, the experimental results proved the feasibility of the proposed network.", "histories": [["v1", "Wed, 24 Feb 2016 02:39:47 GMT  (272kb)", "http://arxiv.org/abs/1602.07373v1", "9 pages, 6 figures. Rejected conference (CVPR 2015) submission. Submission date: November, 2014. This work is patented in China (NO. 201410647710.3)"]], "COMMENTS": "9 pages, 6 figures. Rejected conference (CVPR 2015) submission. Submission date: November, 2014. This work is patented in China (NO. 201410647710.3)", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["song wang", "dongchun ren", "li chen", "wei fan", "jun sun", "satoshi naoi"], "accepted": false, "id": "1602.07373"}, "pdf": {"name": "1602.07373.pdf", "metadata": {"source": "CRF", "title": "On Study of the Binarized Deep Neural Network for Image Classification", "authors": ["Song Wang", "Dongchun Ren", "Li Chen", "Wei Fan", "Jun Sun", "Satoshi Naoi"], "emails": ["naoi}@cn.fujitsu.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.07 373v 1 [cs.N E] 24 Feb 20"}, {"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to abide by the rules that they have imposed on themselves, and that they are able to abide by the rules that they have imposed on themselves. (...) In fact, it is so that they are able to break the rules, to break the rules and to change the rules. (...) In fact, it is so that people are able to break the rules, to break the rules. (...) \"(...)\" It is so. (...) \"(\") \"(\") \"(\") (\"(\") \"(\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") () (\" () () () (\"() () () () () (\" () () () () () () (\"() () () () () () () () () () () () () (\" () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () (() () () () () (() (() () (() (() () () ((() () (((() () ((() (() () ((() (() (((() (() (() (() ((() (() ((() (() (() ((() (() () ((() () (((() () (((() () () (() () (((() () ((() (() (((() (() ((() () ("}, {"heading": "2. The binarized deep neural network", "text": "This section introduces the forward propagation of BDNN. As mentioned above, only binary variables and Boolean operations are used in this process. To apply BDNN to non-binary input data, we also introduced the hybrid BDNN, which included both conventional neural network parts and BDNN parts."}, {"heading": "2.1. The basic function for BDNN", "text": "As shown in Fig. 1, with this basic function, we can build a network of any complicated structure, although the BDNN = the selected basic functions is actually a new definition of the basic function. \u2212 Like conventional DNN, with the basic function of BDNN, any neural network structure can be built. Normally, a basic function of the neural network should contain two different types of calculation: linear and nonlinear. \u2212 As shown in Fig. 1, linear calculation is an internal product of the input neurons ~ x and the corresponding weights ~ w, that is, the nonlinear calculation of the conventional neural network is activation function (sigmoid, hyperbolic tangent, etc.) or pooling (commonly used in DNN). Consequently, in the definition of the basic function of BDNN, we define both linear and nonlinear calculations."}, {"heading": "2.2. The hybrid-BDNN", "text": "If the input data is non-binary, such as grayscale image, the BDNN cannot be used directly. Therefore, we should first convert the non-binary data to a binary part before using the BDNN. For such a situation, we have proposed the hybrid BDNN, which is a combination of conventional neural network and BDNN.As shown in Fig. 3, the hybrid BDNN contains three parts: the normal neural network part, the transition part, and the BDNN part. The lower layers of the hybrid BDNN are normal neural network parts connected to the input data (binary or non-binary); the higher layers are the BDNN layers that generate the result.The transition part is a single layer between the normal neural network part and the BDNN part that connects the two different neural networks."}, {"heading": "3. Training method for BDNN and hybridBDNN", "text": "Normally, the gradient descent algorithm is considered a training method for neural networks. We also used this algorithm for the training of BDNN and hybrid BDNN. However, compared to the conventional neural network, the BDNN obviously has different properties, so we cannot apply this algorithm directly to the BDNN training. To solve this problem, some approach and conversion methods are applied to BDNN to make it suitable for gradient descent training."}, {"heading": "3.1. Gradient descent training for BDNN", "text": "In conventional descent education, in each iteration, the weights are adjusted by a small value = = 1 (depends on the error spread and learning rate). However, since the weights in BDNN are \"binarized,\" it is difficult to adjust the weights in the same way. Therefore, to apply the descent algorithm in the training of BDNN, it is difficult to convert the real numbers into corresponding binary values, the isC (x) = {1, if x \u2265 0 \u2212 1, if x < 0. (3.1) With the function (3.1) we can convert the trained weights of the real number into binary values. However, the key point of using real numbers is that we must maintain the forward propagation; otherwise the training is pointless."}, {"heading": "3.2. The training of transition part of hybrid-BDNN", "text": "As shown in Fig. 3, the reverse propagation can now be applied to the normal part of the neural network and the BDNN part. However, in order to apply the reverse propagation to hybrid BDNN, we still need to think about the reverse propagation of the transition part. Firstly, the same with the BDNN training, the real numbers are used in the formation of the transition part instead of the binary values. Suppose that x \u2032 1 is the corresponding real number of x1 and function t \u2032 is the real number version of function t. Then, we define the same with f \u2032 to get the same result of forward propagation, t \u2032 should fulfill the following equation C (~ g, ~ w) = C (~ g, ~ w). Therefore, we define t \u2032 ast \u2032 (~ w) = A (n \u0445 i = 1giwi) \u2212 T. Obviously, the backward propagation of (3.6) can fulfill the derivative of (neural).If we set ~ 0 to T = the normal function, then the hybridx is only two."}, {"heading": "3.3. Special training technique for BDNN", "text": "The training method for BDNN proposed in this paper is clearly very close to the traditional training method. Therefore, it is possible to borrow the training techniques of conventional neural networks from BDNN, such as dropouts and dropconnect. In addition, there are several special training techniques required for BDNN. Firstly, in training we must maintain the n of (3,3) all the time an odd number, because the output 0 of f \u00b2 is not defined. The result of f \u00b2 should be either a positive number or a negative number. Consequently, in subsequent experiments in the BDNN the error is determined not only by the difference, but also by the sign of each value. Suppose that the error of a conventional neural network is determined by the difference between the initial neuron and its basic truth."}, {"heading": "4. Experiments", "text": "The experiments examined two different network structures - the classical three-layer network and the conventional neural network (CNN). In addition to the BDNN and the hybrid BDNN, the conventional neural networks of the same structures were also tested for comparison. As we now have only the basic training method for BDNN, most of the training optimization techniques of the conventional neural network were not used to make a fair comparison. For example, the learning rate was determined in the experiments and the simple hyperbolic tangent function was used as an activation function. In addition, two data sets, MNIST and CIFAR10, were used for the experiments. As we know, MNIST is a data set of handwritten digits and is therefore suitable for binary data classification tests. In contrast, the CIFAR10 was used as a non-binary data on which the hybrid BDNN was tested. In each data set, the training data was used to complete the networks with the Validation results."}, {"heading": "4.1. The classical three-layer network", "text": "In the early stages of ANN, before the introduction of DNN, the three-layer network was widely used for classification tasks. As shown in Figure 5, in such a network there are three distinct layers: the input layer, the hidden layer, and the output layer. The input layer contains the input data, while the output layer generates the classification results. In addition, all layers are fully interconnected, so we used a simple binarization method, so that the BDNN of that structure was tested first. MNIST's binary data was used for the three-layer BDNN experiments. Since the original images of MNIST were Grayscale, we used the binarization method to convert the grayscale images into binary structures. All training data from MNIST was used to train the network."}, {"heading": "4.2. The convolutional neural network", "text": "In the deep learning research, the structure of CNN is widely used for image classification. Therefore, in the experiments, we also tested the CNN structure. As shown in Fig. 6, we used a CNN structure of five layers. The first layer is the input data, followed by two layers of feature maps (calculated by the Convolutionary DNKernel). The last two layers are the fully connected neurons and one of them is used as the output layer. In most CNNs, after conventional operation, the feature map size is then decreased by pooling. Nevertheless, since it is difficult to realize the pooling operation on binary values, so the feature map size is obtained by skipping each other pixel in the Convolutionary Operation.With different sizes of the feature maps and cores, three different CNN x x x structures are achieved as follows: Structure C: layer C: layer 1 is the input image, layer 5 is the size of the BB5 x 5 x 5 x, the size of the BB5 x 5 x 7 x x x the size of the 29x x x x x x, which contains 29x x x x x x x."}, {"heading": "5. Conclusion and future work", "text": "In BDNN, we have first tested the completely new binary propagation function for neural networks. Furthermore, a special training method for gradient parentage is proposed for BDNN. Experimental results have shown that it is able to use BDNN in the same way as conventional DNN. Besides changing the network structure and training strategy, our attempt could generate new ideas for improving DNN and extending its use. Due to the computational limitations, we have not tested the BDNN on a large scale in this paper. Therefore, the performance of BDNN may not be comparable to current results. In the future, we will develop a GPU-based training program to train the BDNN on a larger scale. Furthermore, various network structures, such as the deeply networked neural network, the recursive neural network, etc., will be investigated for optimizing the forward propagation of the BDNN on the CPU."}, {"heading": "6. Acknowledgments", "text": "This work was started in early 2014 and on the basis of this work, a patent application (China) will be filed in November 2014. Application number of the patent is 201410647710.3. We also submitted this paper to the CVPR 2015, but it was rejected. Nevertheless, we believe that this work is valuable and that the BDNN is a promising solution for low-performance devices using deep learning models."}], "references": [{"title": "Neural network approaches versus statistical methods in classification of multisource remote sensing data", "author": ["J. Benediktsson", "P.H. Swain", "O.K. Ersoy"], "venue": "IEEE Transactions on geoscience and remote sensing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1990}, {"title": "Multispectral classification of landsat-images using neural networks", "author": ["H. Bischof", "W. Schneider", "A.J. Pinz"], "venue": "IEEE Transactions on Geoscience and Remote Sensing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1992}, {"title": "Multicolumn deep neural network for traffic sign classification", "author": ["D. Ciresan", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Design of effective neural network ensembles for image classification purposes", "author": ["G. Giacinto", "F. Roli"], "venue": "Image and Vision Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Multi-digit number recognition from street view imagery using deep convolutional neural networks", "author": ["I.J. Goodfellow", "Y. Bulatov", "J. Ibarz", "S. Arnoud", "V. Shet"], "venue": "arXiv preprint arXiv:1312.6082,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["A. Graves", "M. Liwicki", "S. Fern\u00e1ndez", "B. Roman", "B. Horst", "J. Schmidhuber"], "venue": "IEEE Transactions on 4328  Pattern Analysis and Machine Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "A training algorithm for binary feedforward neural networks", "author": ["D.L. Gray", "A.N. Michel"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "The organization of behavior", "author": ["D. Hebb"], "venue": "New York: Wiley,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1949}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1982}, {"title": "Neurons with graded response have collective computational properties like those of two-state neurons", "author": ["J.J. Hopfield"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1984}, {"title": "Computing with neural circuits- a model", "author": ["J.J. Hopfield", "D.W. Tank"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1986}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1989}, {"title": "The geometrical learning of binary neural networks", "author": ["J.H. Kim", "S.K. Park"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1995}, {"title": "Face recognition: A convolutional neural-network approach", "author": ["S. Lawrence", "C.L. Giles", "A.C. Tsoi", "A.D. Back"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["W.S. McCulloch", "W. Pitts"], "venue": "The bulletin of mathematical biophysics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1943}, {"title": "On sequential construction of binary neural networks", "author": ["M. Muselli"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1995}, {"title": "A neural network approach to character recognition", "author": ["A. Rajavelu", "M.T. Musavi", "M.V. Shirvaikar"], "venue": "Neural Networks,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1989}, {"title": "The perceptron: A probalistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1958}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition", "author": ["D.E. Rumelhart", "J. McClelland"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1986}, {"title": "An artificial hysteresis binary neuron: A model suppressing the oscillatory behaviors of neural dynamics", "author": ["Y. Takefuji", "K.C. Lee"], "venue": "Biological Cybernetics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1991}, {"title": "Modular construction of time-delay neural networks for speech recognition", "author": ["A. Waibel"], "venue": "Neural computation,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1989}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML-", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Beyond regression: New tools for prediction and analysis in the behavioral sciences", "author": ["P. Werbos"], "venue": "PhD thesis, Harvard University,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1975}], "referenceMentions": [{"referenceID": 21, "context": "The research of artificial neural networks (ANN) began more than 70 years ago, proposed by Warren McCulloch and Walter Pitts [22], Donald Hebb [12] and Frank Rosenblatt [25].", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "The research of artificial neural networks (ANN) began more than 70 years ago, proposed by Warren McCulloch and Walter Pitts [22], Donald Hebb [12] and Frank Rosenblatt [25].", "startOffset": 143, "endOffset": 147}, {"referenceID": 24, "context": "The research of artificial neural networks (ANN) began more than 70 years ago, proposed by Warren McCulloch and Walter Pitts [22], Donald Hebb [12] and Frank Rosenblatt [25].", "startOffset": 169, "endOffset": 173}, {"referenceID": 24, "context": "Especially in [25], a two-layer network is introduced for pattern recognition.", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": "However, there was no solution for the network training until backpropagation (gradient descent) algorithm was created by Paul Werbos [30].", "startOffset": 134, "endOffset": 138}, {"referenceID": 25, "context": "After James McClelland [26] introduced the ANN as simulation of natural neural process and its usage in artificial intelligence (AI), the research of ANN became popular.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "Since then, ANN was successfully applied to image classification [1, 2, 5], character recognition [24], face recognition [20], speech recognition [28] and so on.", "startOffset": 65, "endOffset": 74}, {"referenceID": 1, "context": "Since then, ANN was successfully applied to image classification [1, 2, 5], character recognition [24], face recognition [20], speech recognition [28] and so on.", "startOffset": 65, "endOffset": 74}, {"referenceID": 4, "context": "Since then, ANN was successfully applied to image classification [1, 2, 5], character recognition [24], face recognition [20], speech recognition [28] and so on.", "startOffset": 65, "endOffset": 74}, {"referenceID": 23, "context": "Since then, ANN was successfully applied to image classification [1, 2, 5], character recognition [24], face recognition [20], speech recognition [28] and so on.", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "Since then, ANN was successfully applied to image classification [1, 2, 5], character recognition [24], face recognition [20], speech recognition [28] and so on.", "startOffset": 121, "endOffset": 125}, {"referenceID": 27, "context": "Since then, ANN was successfully applied to image classification [1, 2, 5], character recognition [24], face recognition [20], speech recognition [28] and so on.", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "Moreover, a theoretical explanation of ANN\u2019s success was also given by Kurt Hornik [18].", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "For example, Geoff Hinton\u2019s deep belief nets [13], Yann LeCun and Dan Ciresan\u2019s study on deep convolutional neural networks [21, 4], and Alex Graves\u2019s deep recurrent neural networks [9].", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": "For example, Geoff Hinton\u2019s deep belief nets [13], Yann LeCun and Dan Ciresan\u2019s study on deep convolutional neural networks [21, 4], and Alex Graves\u2019s deep recurrent neural networks [9].", "startOffset": 124, "endOffset": 131}, {"referenceID": 3, "context": "For example, Geoff Hinton\u2019s deep belief nets [13], Yann LeCun and Dan Ciresan\u2019s study on deep convolutional neural networks [21, 4], and Alex Graves\u2019s deep recurrent neural networks [9].", "startOffset": 124, "endOffset": 131}, {"referenceID": 8, "context": "For example, Geoff Hinton\u2019s deep belief nets [13], Yann LeCun and Dan Ciresan\u2019s study on deep convolutional neural networks [21, 4], and Alex Graves\u2019s deep recurrent neural networks [9].", "startOffset": 182, "endOffset": 185}, {"referenceID": 6, "context": "\u2022 Better performance: DNN archived the state-of-theart results on various tasks and competitions, and brought breakthroughs to those fields, such as handwritten character recognition [7, 10], image classification [4, 3], speech recognition [8] and so on.", "startOffset": 183, "endOffset": 190}, {"referenceID": 9, "context": "\u2022 Better performance: DNN archived the state-of-theart results on various tasks and competitions, and brought breakthroughs to those fields, such as handwritten character recognition [7, 10], image classification [4, 3], speech recognition [8] and so on.", "startOffset": 183, "endOffset": 190}, {"referenceID": 3, "context": "\u2022 Better performance: DNN archived the state-of-theart results on various tasks and competitions, and brought breakthroughs to those fields, such as handwritten character recognition [7, 10], image classification [4, 3], speech recognition [8] and so on.", "startOffset": 213, "endOffset": 219}, {"referenceID": 2, "context": "\u2022 Better performance: DNN archived the state-of-theart results on various tasks and competitions, and brought breakthroughs to those fields, such as handwritten character recognition [7, 10], image classification [4, 3], speech recognition [8] and so on.", "startOffset": 213, "endOffset": 219}, {"referenceID": 7, "context": "\u2022 Better performance: DNN archived the state-of-theart results on various tasks and competitions, and brought breakthroughs to those fields, such as handwritten character recognition [7, 10], image classification [4, 3], speech recognition [8] and so on.", "startOffset": 240, "endOffset": 243}, {"referenceID": 13, "context": "one hand, in order to pursue higher recognition rate, several optimization methods for training were proposed, such as dropout [14] and dropconnect [29].", "startOffset": 127, "endOffset": 131}, {"referenceID": 28, "context": "one hand, in order to pursue higher recognition rate, several optimization methods for training were proposed, such as dropout [14] and dropconnect [29].", "startOffset": 148, "endOffset": 152}, {"referenceID": 5, "context": "For example, in Ian Goodfellow\u2019s recent work [6] for digit string recognition, the output layer is trained to show both the digit number and the recognition result of each digit.", "startOffset": 45, "endOffset": 48}, {"referenceID": 14, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 83, "endOffset": 103}, {"referenceID": 15, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 83, "endOffset": 103}, {"referenceID": 16, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 83, "endOffset": 103}, {"referenceID": 22, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 83, "endOffset": 103}, {"referenceID": 26, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 83, "endOffset": 103}, {"referenceID": 10, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 142, "endOffset": 150}, {"referenceID": 18, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 142, "endOffset": 150}], "year": 2016, "abstractText": "Recently, the deep neural network (derived from the artificial neural network) has attracted many researchers\u2019 attention by its outstanding performance. However, since this network requires high-performance GPUs and large storage, it is very hard to use it on individual devices. In order to improve the deep neural network, many trials have been made by refining the network structure or training strategy. Unlike those trials, in this paper, we focused on the basic propagation function of the artificial neural network and proposed the binarized deep neural network. This network is a pure binary system, in which all the values and calculations are binarized. As a result, our network can save a lot of computational resource and storage. Therefore, it is possible to use it on various devices. Moreover, the experimental results proved the feasibility of the proposed network.", "creator": "LaTeX with hyperref package"}}}