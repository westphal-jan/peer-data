{"id": "1602.07383", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Automatic Moth Detection from Trap Images for Pest Management", "abstract": "Monitoring the number of insect pests is a crucial component in pheromone-based pest management systems. In this paper, we propose an automatic detection pipeline based on deep learning for identifying and counting pests in images taken inside field traps. Applied to a commercial codling moth dataset, our method shows promising performance both qualitatively and quantitatively. Compared to previous attempts at pest detection, our approach uses no pest-specific engineering which enables it to adapt to other species and environments with minimal human effort. It is amenable to implementation on parallel hardware and therefore capable of deployment in settings where real-time performance is required.", "histories": [["v1", "Wed, 24 Feb 2016 03:35:42 GMT  (6875kb,D)", "http://arxiv.org/abs/1602.07383v1", "Preprints accepted by Computers and electronics in agriculture"]], "COMMENTS": "Preprints accepted by Computers and electronics in agriculture", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["weiguang ding", "graham taylor"], "accepted": false, "id": "1602.07383"}, "pdf": {"name": "1602.07383.pdf", "metadata": {"source": "META", "title": "Automatic moth detection from trap images for pest management", "authors": ["Weiguang Ding", "Graham Taylor"], "emails": ["wding@uoguelph.ca", "gwtaylor@uoguelph.ca"], "sections": [{"heading": null, "text": "Monitoring the number of insect pests is a critical component in pheromone-based pest management systems. In this paper, we propose an automatic detection pipeline based on in-depth learning to identify and count pests in field trap images. Applied to a commercial dataset of spider mites, our method shows promising results both qualitatively and quantitatively. Compared to previous pest detection attempts, our approach does not use a pest-specific technique that allows it to adapt to other species and environments with minimal human effort. It can be implemented on parallel hardware and therefore in environments where real-time performance is required. Keywords: precision farming, integrated pest management, pest control, snare images, object detection, conventional neural network."}, {"heading": "1. Introduction", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "2. Data collection", "text": "In this section we describe the collection, curation and pre-editing of images. Details on how to detect edited images are given in section 3."}, {"heading": "2.1. Data acquisition", "text": "RGB color images are captured by pheromone traps installed in multiple locations by a commercial provider of pheromone-based pest control solutions, whose name is withheld upon request. In the traps are a pheromone bait, adhesive film, digital camera, and radio transmitter, which lure the pheromone into the trap where it adheres to the surface of the adhesive. Digital images are stored in JPEG format with 640 x 480 resolution and sent daily to a remote server at a fixed time. Codling moths are identified by entomology-trained technicians and labeled with boxes. Only one image of each temporal sequence is labeled and used in this study, so labeled images have no temporal correlation to each other. As a result, all labeled moths are unique. Figure 1a shows a trap image with all codling moths labeled with blue boxes."}, {"heading": "2.2. Dataset construction", "text": "The set of collected images is randomly divided into 3 sets: the training set, the validation set, and the test set. After columning, the statistics for each set are roughly the same as the entire record, including the ratio between the number of images with or without moths and the number of moths per image. Table 1 provides specific statistics for the entire record and the three subsequent columns."}, {"heading": "2.3. Preprocessing", "text": "In order to eliminate the possible negative effects of illumination variability on detection performance, we perform a color correction using a variant [38] of the \"grey world\" method. This algorithm assumes that the average value of the red (R), green (G) and blue (B) channels should correspond to each other. Specifically, for each image, we define the amplification of the R and B channels as follows: Gred = \u00b5red / micro-green, Gblue = \u00b5blue / micro-green (1), with micro-red, micro-green and micro-blue being the original average intensities of the red, green and blue channels, respectively. Gred and Gblue are multiplicative increases applied to the pixel intensity values of the red and blue channels. Figure 2b shows images processed by the grey world algorithm. We see that the images are so balanced that they are all similarly balanced for detection of paper, but still usable."}, {"heading": "3. Detection pipeline", "text": "The automatic detection pipeline consists of several steps, as shown in Figure 3. We choose a sliding window approach, where a trained image classifier is applied to local windows at different points of the entire image. Output of the classifier is a single scalar p [0, 1], which represents the probability that a particular patch contains an apple wrapper. These patches are arranged regularly and densely over the image and therefore overlap to a large extent. Therefore, we perform a non-maximum suppression (NMS) to keep only the windows whose respective probability is maximum locally, and the remaining boxes are then turned off so that only patches above a certain probability are preserved. The location of these patches with their respective probabilities (confidence values) are the final results of the detection pipeline. We will now discuss each of these phases in more detail."}, {"heading": "3.1. Convolutional neural networks", "text": "With a sliding window approach, the detection problem is divided into the classification of each local patch performed by the image classifier C, an image from image I to a probability p: C: I 7 \u2192 S. We use the Convolutionary Neural Network [31] (ConvNet) as our image classifier, as it is the most popular and powerful classifier for image detection both on a large scale [32, 37] and on a small scale [39, 40]. It is also very fast to deploy and is suitable for parallel hardware. Specifically, we used a network structure similar to Lenet5 [31]. As shown in Figure 4, our network contains 2 revolutionary layers, 2 max-pooling layers and 2 fully connected layers (as described below). Prior to applying ConvNet, each dimension of the input patch1 is normalized to obtain a mean and unit variance of zero."}, {"heading": "3.1.1. Convolutional layers", "text": "A Convolutionary Layer applies a linear filter bank and elemental nonlinearity to its input \"Character Cards\" and transforms them into a different set of Character Cards. By repeatedly applying Convolutionary Layers, we can extract increasingly high-grade representations of the input characteristics while preserving their spatial relationship. At the first level, the Input Character Cards are simply the channels of the input. On the subsequent layers, these represent more abstract transformations of the image. A Convolutionary Layer is a special case of a completely connected layer introduced in Section 3.1.3, where only local connections have unequal values and the weights are bound in all places. Local connectivity is efficiently implemented by applying evolution: hlk = preservation mWlm, k h \u2212 1 m + b l k, (2) where l is the layer index; m is the index of the input characteristics; k is the index of the output characteristics; k = the index of the input characteristics; k = the index \u2212 \u2212 1; \u2212 \u2212 the input characteristic is \u2212 1; \u2212 the \u2212 1."}, {"heading": "3.1.2. Max-pooling layers", "text": "This layer applies local pooling operations to its input feature maps by maintaining only the maximum value within a local receiver field and discarding all other values. It resembles a revolutionary layer in the sense that both work locally. Applying max pooling layers has two major advantages: 1) reducing the number of free parameters and 2) introducing a low level of translational invariance into the network."}, {"heading": "3.1.3. Fully connected layers", "text": "The first fully connected layer flattens (vectorizes) all feature cards after the last max pooling layer and treats this one-dimensional vector as a feature representation of the entire image. The second fully connected layer is parameterized like a linear classifier. Mathematically, the fully connected layer can be described as follows: hl = BA (Wlhl \u2212 1 + bl), (3) where l is the layer index; input hl \u2212 1 is the vector representation on layer l \u2212 1; hl is the output of layer l \u2212 1; W is the weight matrix; b is the bias vector; and vice versa (\u00b7) is an elementary non-linear function: We select RELUs for the fully connected hidden layer and a softmax for the output layer. For a more detailed explanation of the evolutionary neural networks, refer the reader to [31, 32, 42]."}, {"heading": "3.2. Non-maximum suppression", "text": "If we applied ConvNet in a sliding fashion, we would get probabilities associated with each tightly sampled patch. If we simply applied thresholds at this point, we would get many overlapping detections. This problem is usually solved by a non-maximum suppression (NMS), which aims to keep only patches with the maximum local probability. We chose a strategy similar to [43]. Specifically, we first sort all detections according to their probability. Then, from high to low probability, we look at each detection and remove other boundary fields that overlap at least 10% with the current detection. After this greedy process, we produce final detection results, as shown in Figure 3 and later in Section 5.1."}, {"heading": "4. Experiments and evaluation", "text": "Next, we will present how we conducted the experiments and the evaluation protocol."}, {"heading": "4.1. Classifier training", "text": "The classifier is trained using generated fields of different sizes, described in detail in Section 4.2. To form the ConvNet, a stochastic minibatch gradient descent (SGD) with impulse [44] was used. The gradient is estimated using the known reverse propagation algorithm [45]. We used a fixed learning rate of 0.002, a fixed minibatch size of 256 and a fixed pulse coefficient of 0.9. The validation set is used to monitor the training process and to select hyperparameters. We report on performance using a classifier whose parameters are selected according to the best observed validation set accuracy. The filters and fully connected weight matrices of the ConvNets are initialized with values selected from a uniform random distribution at an interval that is a function of the number of pre-synaptic and post-synaptic units (see [46] for more details)."}, {"heading": "4.2. Training data extraction", "text": "In the sliding-window classification pipeline, the classifier uses a local window as input, so we need to extract small local patches from the original high-resolution resolution resolution to train the classifier, in a memory-efficient manner, using pointer arithmetic to generate \"views\" on the data, rather than storing all patches in memory."}, {"heading": "4.2.1. Positive patches", "text": "\"Positive Patch\" refers to patches derived from manually labeled Bounding Boxes, each of which is a codling moth. Because ConvNet processes square input, we ignored the original aspect ratio of the manually labeled Bounding Boxes and took the square area, which has the same center as the original rectangular Bounding Box. Figure 5a shows positive patches extracted from the training set."}, {"heading": "4.2.2. Negative patches", "text": "It would be difficult to cover all possible wrong positions simply by scanning the regions that are not covered by the marked boundary fields. This is because the area of the mothless regions is much larger than the area covered by the boundary fields. In images that are not very overloaded, most of the \"negative\" area is uninteresting (e.g. trap liners), so in order to obtain negative training examples, we intentionally use \"hard\" spots, i.e. those that contain texture. Specifically, we use the canny edge detector [47] to find spots in \"negative images,\" i.e. those that do not contain moths. We set the threshold so that the number of negative spots roughly corresponds to the number of marked moths. Figure 5b shows a random sample of negative spots."}, {"heading": "4.2.3. Bootstrapping", "text": "After the first negative patches have been extracted, we use a bootstrapping approach to find useful negative training patches that can make the classifier more discriminatory. In the first round of training, the originally generated patches are used to train the classifier. At test time, the false positive patches are collected from the training set, and we most likely isolate the 6000 negative patches that the classifier assigned, which are merged with the originally generated patches to form a new data set for a second stage of training. Potentially, more rounds of bootstrapping could be used to collect more informative negative patches, but we found that more than two training phases do not improve performance. Figure 5c shows randomly selected patches collected in the test phase after one stage of training."}, {"heading": "4.3. Data augmentation", "text": "In machine learning, the larger the data set, the better the generalization power. In our case, the amount of training data represented by the number of training packages is much smaller than the standard small-scale image classification datasets [40, 48] commonly used by the deep learning community, which have about 50,000 training examples. Therefore, we have enlarged the data to increase the number of images for training, and also included the inventory of basic geometric transformations in the classifier. Based on the \"top view\" of the trap images, a particular patch does not change its class name when it is easily translated, rotated or rotated. Therefore, we apply these simple geometric transformations to the original patches to increase the number of training examples. For each patch, we create 8 translated copies by shifting \u00b1 3 pixels horizontally, vertically and diagonally."}, {"heading": "4.4. Detection", "text": "For detection, we have to set the step, i.e. the distance between adjacent sliding windows. A smaller step means a denser patch cover, which leads to better moths localization, but also requires more calculation. To compensate, we set the step to the size of a patch 14."}, {"heading": "4.5. Evaluation protocol", "text": "Detection of pests is still a relatively \"niche\" area of computer visualization and therefore there is no standardized evaluation protocol. We opted for a protocol inspired by standardization within the pedestrian detection community. For a complete overview, see [49], which we summarize below."}, {"heading": "4.5.1. Matching detections with ground truth", "text": "We evaluate detection performance based on the statistics of false detections, correct detections and false positives. Here, a false detection refers to a manually described region that is overlooked by the algorithm, and a false positive refers to a delimitation box suggested by the algorithm that does not correspond to a manually described region. To determine whether a delimitation box proposed by the detector is a correct detection or a false detection, we determine its equivalent to a manually described delimitation box by heuristically calculating the delimitation over the minimum (IOMin): A (BBdt, BBgt) = area (BBdt, BBgt) min (area (BBgt)))), (4) where BBdt is a delimitation box proposed by the algorithm (a detection) and BBdt is a delimitation that represents a ground-level delimitation box."}, {"heading": "4.5.2. Object level evaluation", "text": "Based on the statistics of correct discoveries (also known as true positives), false diagnoses (also known as false negatives) and false positives, we could evaluate performance on two levels: (1) object level, which focuses on the performance of the detection of individual moths; and (2) image level, which focuses on determining whether an image contains any moths or not. At object level, we use five threshold-dependent measurements: false rate, false positives per image (FPPI), precision, recall, and foot score: false rate = number of false positive moths (5) FPPI = number of false positive total number of images (6) precision = number of correct detections Total number of detections (7) recall = number of correct detection numbers of moths (8) positive positive numbers (8) F\u03b2 = (1 + \u03b22) accuracy \u00b7 Recallness \u00b7 2 + precision of PI, with the exception of 9)."}, {"heading": "4.5.3. Image level evaluation", "text": "In this setting, the algorithm simply has to make a suggestion for \"moths\" or \"no moths\" per image, regardless of how many moths it thinks there are. At this point, we will call a \"true moths image\" an image that contains at least one moth, and a \"no moths image\" an image that does not contain moths. Similar to object evaluation, there are five threshold-dependent metrics: sensitivity (synonymous with recall), specificity, precision, and F\u03b2 score: sensitivity = recall = number of correctly identified true moths images (10) specificity = number of correctly identified true moths images (10) specificity = number of correctly identified folded images (11) and precision = number of correctly identified real moths images (8)."}, {"heading": "5. Results", "text": "In this section, we first present some visual (qualitative) results and then describe the results of the performance evaluation introduced in Section 4."}, {"heading": "5.1. Qualitative", "text": "Figure 7 shows an example of our detector in operation. In both panels, the image on the left shows manually annotated delimiter boxes in green and the suggestions of our detector in magenta. The image on the right shows the results of matching annotations with suggestions. False detections, false positions and correct detections are displayed in blue, red and yellow boxes, respectively. In Figure 7a and 7b, the thresholds are set to maximize the F2 score at object level and image level, respectively. Figure 9 and 10 show further examples of detection results on large format images."}, {"heading": "5.2. Quantitative", "text": "We chose logistic regression as the basis for comparison with ConvNets. We tested both logistic regression and ConvNets on five different input sizes: 21 \u00d7 21 \u00d7 28, 35 \u00d7 35, and 49 \u00d7 49. Results are shown in this domain, the cost of not responding to a potential problem outweighs the unnecessary use of treatment. 3We also tried to develop a popular vision of local characteristics (SIFT [50]), followed by visual words and a supporting vector. This approach did not provide reasonable recognition performance and was therefore not reported."}, {"heading": "5.3. Individual detection results", "text": "Figure 11 shows various image fields with different detection results, including Figure 11a, which shows correct detection, 11b, which shows error detection, and 11c, which shows false positives. These image fields are all at 100 \u00d7 100 resolution and are extracted from the detection results with an input size of 21 \u00d7 21. We see that moth images are highly variable due to several factors, including different wing positions, occlusion by other objects, different decay conditions, different lighting conditions, different background textures, and different blurring. Some of these moth images are successfully detected among these distorting factors, and some are ignored by the detector. Figure 11c also shows that within the 21 \u00d7 21 window viewed by the classifier, some of the false positives are visually similar to the 21 \u00d7 21 image fields that are actually moth. Although it seems easier for a human reader to distinguish moths from non-moth text when looking at 100 \u00d7 i.e."}, {"heading": "6. Discussion", "text": "Compared to the majority of previous work, the proposed method relies more on data and less on human knowledge. No knowledge of moths has been taken into account in designing our approach, and the network learned to identify moths based on positive and negative training examples, which makes it easier for the system to adapt to new pest species and new environments without much manual effort, as long as relevant data is available. Errors caused by various factors have been analyzed in Section 5.3. Many of them relate to time. The same moth could exhibit different wing positions, coverage levels, lighting and expiration conditions over time. Visual texture can also be associated with time. For example, decaying insects could dirty the original white trap and reduce the contrast between moths and background. Errors caused by time-related factors could be largely avoided in real production systems if temporary image sequences with appropriate frequency are provided."}, {"heading": "7. Conclusions", "text": "This paper describes an automatic method for monitoring pests from snare images. We propose a sliding, window-based detection pipeline, in which a revolutionary neural network is applied to image fields in different locations to determine the likelihood of containing a particular type of pest. Image fields are then filtered by non-maximum suppression and threshold, depending on location and associated confidentiality, to achieve final detections. Qualitative and quantitative experiments demonstrate the effectiveness of the proposed method on a dataset of coding motes. We also analyzed detection errors with corresponding impacts on real production systems and potential future improvement guidelines."}, {"heading": "Acknowledgements", "text": "This work was funded by the Natural Sciences and Engineering Research Council (NSERC) EGP 453816-13, EGP 453816-14 and an industry partner whose name was not provided on request. We also thank Dr. Rebecca Hallett, Jordan Hazell and the industry partner for their support in data collection."}], "references": [{"title": "Control of moth pests by mating disruption: successes and constraints", "author": ["R.T. Carde", "A.K. Minks"], "venue": "Annual review of entomology 40 (1) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Sex pheromones and their impact on pest management", "author": ["P. Witzgall", "P. Kirsch", "A. Cork"], "venue": "Journal of chemical ecology 36 (1) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Identification of butterfly species with a single neural network system", "author": ["S.-H. Kang", "S.-H. Song", "S.-H. Lee"], "venue": "Journal of Asia-Pacific Entomology 15 (3) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Identification of butterfly based on their shapes when viewed from different angles using an artificial neural network", "author": ["S.-H. Kang", "J.-H. Cho", "S.-H. Lee"], "venue": "Journal of Asia-Pacific Entomology 17 (2) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Biodiversity informatics in action: identification and monitoring of bee species using abis", "author": ["T. Arbuckle", "S. Schroder", "V. Steinhage", "D. Wittmann"], "venue": "in: Proc. 15th Int. Symp. Informatics for Environmental Protection, Vol. 1, Citeseer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "M", "author": ["P. Weeks"], "venue": "O\u00a2\u00a2Neill, K. Gaston, I. Gauld, Automating insect identification: exploring the limitations of a prototype system, Journal of Applied Entomology 123 (1) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Drawwing", "author": ["A. Tofilski"], "venue": "a program for numerical description of insect wings, Journal of Insect Science 4 (1) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "A new automatic identification system of insect images at the order level", "author": ["J. Wang", "C. Lin", "L. Ji", "A. Liang"], "venue": "Knowledge-Based Systems 33 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "E", "author": ["N. Larios", "H. Deng", "W. Zhang", "M. Sarpola", "J. Yuen", "R. Paasch", "A. Moldenke", "D.A. Lytle", "S.R. Correa"], "venue": "N. Mortensen, et al., Automated insect identification through concatenated histograms of local appearance features: feature vector generation and region detection for deformable objects, Machine Vision and Applications 19 (2) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "S", "author": ["G. Martinez-Munoz", "N. Larios", "E. Mortensen", "W. Zhang", "A. Yamamuro", "R. Paasch", "N. Payet", "D. Lytle", "L. Shapiro"], "venue": "Todorovic, et al., Dictionary-free categorization of very similar objects via stacked evidence trees, in: Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "G", "author": ["N. Larios", "B. Soran", "L.G. Shapiro"], "venue": "Mart\u0131\u0301nez-Mu\u00f1oz, J. Lin, T. G. Dietterich, Haar random forest features and svm spatial matching kernel for stonefly species identification., in: ICPR, Vol. 1", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "G", "author": ["D.A. Lytle"], "venue": "Mart\u0131\u0301nez-Mu\u00f1oz, W. Zhang, N. Larios, L. Shapiro, R. Paasch, A. Moldenke, E. N. Mortensen, S. Todorovic, T. G. Dietterich, Automated processing and identification of benthic invertebrate samples, Journal of the North American Benthological Society 29 (3) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Identification of pecan weevils through image processing", "author": ["S. Al-Saqer", "P. Weckler", "J. Solie", "M. Stone", "A. Wayadande"], "venue": "American Journal of Agricultural and Biological Sciences 6 (1) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "M", "author": ["J. Cho", "J. Choi"], "venue": "Qiao, C.-w. Ji, H.-y. Kim, K.-b. Uhm, T.-s. Chon, Automatic identification of whiteflies, aphids and thrips in greenhouse based on image analysis, Red 346 (246) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic species identification of live moths", "author": ["M. Mayo", "A.T. Watson"], "venue": "Knowledge-Based Systems 20 (2) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Auto-classification of insect images based on color histogram and glcm", "author": ["Z. Le-Qing", "Z. Zhen"], "venue": "in: Fuzzy Systems and Knowledge Discovery (FSKD), 2010 Seventh International Conference on, Vol. 6, IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Application of artificial neural network for automatic detection of butterfly species using color and texture features", "author": ["Y. Kaya", "L. Kayci"], "venue": "The Visual Computer 30 (1) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Thrips (thysanoptera) identification using artificial neural networks", "author": ["P. Fedor", "I. Malenovsk\u1ef3", "J. Va\u0148hara", "W. Sierka", "J. Havel"], "venue": "Bulletin of entomological research 98 (05) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "An insect classification analysis based on shape features using quality threshold artmap and moment invariant", "author": ["S.N. Yaakob", "L. Jain"], "venue": "Applied Intelligence 37 (1) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Local feature-based identification and classification for orchard insects", "author": ["C. Wen", "D.E. Guyer", "W. Li"], "venue": "Biosystems engineering 104 (3) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Image-based orchard insect automated identification and classification method", "author": ["C. Wen", "D. Guyer"], "venue": "Computers and Electronics in Agriculture 89 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Insect species recognition using discriminative local soft coding", "author": ["A. Lu", "X. Hou", "C.-L. Liu", "X. Chen"], "venue": "in: Pattern Recognition (ICPR), 2012 21st International Conference on, IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Stacked spatial-pyramid kernel: An object-class recognition method to combine scores from random trees", "author": ["N. Larios", "J. Lin", "M. Zhang", "D. Lytle", "A. Moldenke", "L. Shapiro", "T. Dietterich"], "venue": "in: Applications of Computer Vision (WACV), 2011 IEEE Workshop on, IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Knn-spectral regression lda for insect recognition", "author": ["L. Xiao-Lin", "H. Shi-Guo", "Z. Ming-Quan", "G. Guo-Hua"], "venue": "in: Information Science and Engineering (ICISE), 2009 1st International Conference on, IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Detection of insects in bulk wheat samples with machine vision", "author": ["I. Zayas", "P. Flinn"], "venue": "Transactions of the ASAE-American Society of Agricultural Engineers 41 (3) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Rapid machine vision method for the detection of insects and other particulate bio-contaminants of bulk grain in transit", "author": ["C. Ridgway", "E. Davies", "J. Chambers", "D. Mason", "M. Bateman"], "venue": "Biosystems engineering 83 (1) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "L", "author": ["Y. Qing"], "venue": "Jun, Q.-j. LIU, G.-q. DIAO, B.-j. YANG, H.-m. CHEN, T. Jian, An insect imaging system to automate rice light-trap pest identification, Journal of Integrative Agriculture 11 (6) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Segmentation of touching insects based on optical flow and ncuts", "author": ["Q. Yao", "Q. Liu", "T.G. Dietterich", "S. Todorovic", "J. Lin", "G. Diao", "B. Yang", "J. Tang"], "venue": "Biosystems Engineering 114 (2) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Object class detection: A survey", "author": ["X. Zhang", "Y.-H. Yang", "Z. Han", "H. Wang", "C. Gao"], "venue": "ACM Computing Surveys (CSUR) 46 (1) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "50 years of object recognition: Directions forward", "author": ["A. Andreopoulos", "J.K. Tsotsos"], "venue": "Computer Vision and Image Understanding 117 (8) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86 (11) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "in: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, IEEE", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-task bayesian optimization", "author": ["K. Swersky", "J. Snoek", "R.P. Adams"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Applicability of white-balancing algorithms to restoring faded colour slides: An empirical evaluation", "author": ["D. Nikitenko", "M. Wirth", "K. Trudel"], "venue": "Journal of Multimedia 3 (5) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Computer Science Department, University of Toronto, Tech. Rep 1 (4) ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep convolutional networks for scene parsing", "author": ["D. Grangier", "L. Bottou", "R. Collobert"], "venue": "in: ICML 2009 Deep Learning Workshop, Vol. 3, Citeseer", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "A", "author": ["L. Fei-Fei"], "venue": "Karpathy, Cs231n: Convolutional neural networks for visual recognition ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 32 (9) ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "in: Neural networks: Tricks of the trade, Springer", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Tech. rep., DTIC Document ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1985}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "in: International conference on artificial intelligence and statistics", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "A computational approach to edge detection", "author": ["J. Canny"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on (6) ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1986}, {"title": "C", "author": ["Y. LeCun"], "venue": "Cortes, The mnist database of handwritten digits ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1998}, {"title": "Pedestrian detection: An evaluation of the state of the art", "author": ["P. Dollar", "C. Wojek", "B. Schiele", "P. Perona"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 34 (4) ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision 60 (2) ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning fine-grained image similarity with deep ranking", "author": ["J. Wang", "Y. Song", "T. Leung", "C. Rosenberg", "J. Wang", "J. Philbin", "B. Chen", "Y. Wu"], "venue": "in: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, IEEE", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "in: Computer Vision and Pattern Recognition Workshops ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Monitoring is a crucial component in pheromone-based pest control [1, 2] systems.", "startOffset": 66, "endOffset": 72}, {"referenceID": 1, "context": "Monitoring is a crucial component in pheromone-based pest control [1, 2] systems.", "startOffset": 66, "endOffset": 72}, {"referenceID": 2, "context": "In terms of image sources, many previous methods have considered insect specimens [3, 4, 5, 6, 7, 8].", "startOffset": 82, "endOffset": 100}, {"referenceID": 3, "context": "In terms of image sources, many previous methods have considered insect specimens [3, 4, 5, 6, 7, 8].", "startOffset": 82, "endOffset": 100}, {"referenceID": 4, "context": "In terms of image sources, many previous methods have considered insect specimens [3, 4, 5, 6, 7, 8].", "startOffset": 82, "endOffset": 100}, {"referenceID": 5, "context": "In terms of image sources, many previous methods have considered insect specimens [3, 4, 5, 6, 7, 8].", "startOffset": 82, "endOffset": 100}, {"referenceID": 6, "context": "In terms of image sources, many previous methods have considered insect specimens [3, 4, 5, 6, 7, 8].", "startOffset": 82, "endOffset": 100}, {"referenceID": 7, "context": "In terms of image sources, many previous methods have considered insect specimens [3, 4, 5, 6, 7, 8].", "startOffset": 82, "endOffset": 100}, {"referenceID": 8, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 9, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 10, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 11, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 12, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 13, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 14, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 2, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 127, "endOffset": 142}, {"referenceID": 3, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 127, "endOffset": 142}, {"referenceID": 4, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 127, "endOffset": 142}, {"referenceID": 5, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 127, "endOffset": 142}, {"referenceID": 6, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 127, "endOffset": 142}, {"referenceID": 15, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 170, "endOffset": 178}, {"referenceID": 16, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 170, "endOffset": 178}, {"referenceID": 17, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 206, "endOffset": 220}, {"referenceID": 18, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 206, "endOffset": 220}, {"referenceID": 6, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 206, "endOffset": 220}, {"referenceID": 7, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 206, "endOffset": 220}, {"referenceID": 15, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 243, "endOffset": 267}, {"referenceID": 16, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 243, "endOffset": 267}, {"referenceID": 19, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 243, "endOffset": 267}, {"referenceID": 20, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 243, "endOffset": 267}, {"referenceID": 21, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 243, "endOffset": 267}, {"referenceID": 22, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 243, "endOffset": 267}, {"referenceID": 23, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 295, "endOffset": 299}, {"referenceID": 19, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 129, "endOffset": 140}, {"referenceID": 7, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 129, "endOffset": 140}, {"referenceID": 10, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 129, "endOffset": 140}, {"referenceID": 7, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 175, "endOffset": 186}, {"referenceID": 16, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 175, "endOffset": 186}, {"referenceID": 17, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 175, "endOffset": 186}, {"referenceID": 23, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 215, "endOffset": 223}, {"referenceID": 20, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 215, "endOffset": 223}, {"referenceID": 8, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 246, "endOffset": 257}, {"referenceID": 9, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 246, "endOffset": 257}, {"referenceID": 20, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 246, "endOffset": 257}, {"referenceID": 24, "context": "This technique was applied for inspection of bulk wheat samples [25], where local patches from the original image were represented by engineered features and classified by discriminant analysis.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "Another work on bulk grain inspection [26] employed different customized rule-based algorithms to detect", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "These candidates are then represented by engineered features and classified [27, 28].", "startOffset": 76, "endOffset": 84}, {"referenceID": 27, "context": "These candidates are then represented by engineered features and classified [27, 28].", "startOffset": 76, "endOffset": 84}, {"referenceID": 28, "context": "Various methods and datasets [29, 30] have been proposed in the last several decades to push this area forward.", "startOffset": 29, "endOffset": 37}, {"referenceID": 29, "context": "Various methods and datasets [29, 30] have been proposed in the last several decades to push this area forward.", "startOffset": 29, "endOffset": 37}, {"referenceID": 30, "context": "Recently, convolutional neural networks (ConvNets) [31, 32] and their variants have emerged as the most effective method for object recognition and detection, by achieving state-of-the-art performance on many well recognized datasets [33, 34, 35], and winning different object recognition challenges [36, 32, 37].", "startOffset": 51, "endOffset": 59}, {"referenceID": 31, "context": "Recently, convolutional neural networks (ConvNets) [31, 32] and their variants have emerged as the most effective method for object recognition and detection, by achieving state-of-the-art performance on many well recognized datasets [33, 34, 35], and winning different object recognition challenges [36, 32, 37].", "startOffset": 51, "endOffset": 59}, {"referenceID": 32, "context": "Recently, convolutional neural networks (ConvNets) [31, 32] and their variants have emerged as the most effective method for object recognition and detection, by achieving state-of-the-art performance on many well recognized datasets [33, 34, 35], and winning different object recognition challenges [36, 32, 37].", "startOffset": 234, "endOffset": 246}, {"referenceID": 33, "context": "Recently, convolutional neural networks (ConvNets) [31, 32] and their variants have emerged as the most effective method for object recognition and detection, by achieving state-of-the-art performance on many well recognized datasets [33, 34, 35], and winning different object recognition challenges [36, 32, 37].", "startOffset": 234, "endOffset": 246}, {"referenceID": 31, "context": "Recently, convolutional neural networks (ConvNets) [31, 32] and their variants have emerged as the most effective method for object recognition and detection, by achieving state-of-the-art performance on many well recognized datasets [33, 34, 35], and winning different object recognition challenges [36, 32, 37].", "startOffset": 300, "endOffset": 312}, {"referenceID": 34, "context": "To eliminate the potential negative effects of illumination variability on detection performance, we perform colour correction using one variant [38] of the \u201cgrey-world\u201d method.", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "The classifier\u2019s output is a single scalar p \u2208 [0, 1], which represents the probability that a particular patch contains a codling moth.", "startOffset": 47, "endOffset": 53}, {"referenceID": 34, "context": "(a) Original images (b) Images processed by the \u201cgrey world\u201d algorithm [38].", "startOffset": 71, "endOffset": 75}, {"referenceID": 30, "context": "We adopt convolutional neural network [31] (ConvNet) as our image classifier, as it is the most popular and best performing classifier for image recognition in both large scale [32, 37] and small scale [39, 40] problems.", "startOffset": 38, "endOffset": 42}, {"referenceID": 31, "context": "We adopt convolutional neural network [31] (ConvNet) as our image classifier, as it is the most popular and best performing classifier for image recognition in both large scale [32, 37] and small scale [39, 40] problems.", "startOffset": 177, "endOffset": 185}, {"referenceID": 35, "context": "We adopt convolutional neural network [31] (ConvNet) as our image classifier, as it is the most popular and best performing classifier for image recognition in both large scale [32, 37] and small scale [39, 40] problems.", "startOffset": 202, "endOffset": 210}, {"referenceID": 30, "context": "Specifically, we used a network structure similar to Lenet5 [31].", "startOffset": 60, "endOffset": 64}, {"referenceID": 36, "context": "where l is the layer index; m is the index of input feature maps; k is the index of output feature maps; input hl\u22121 m is the mth feature map at layer l \u2212 1; output hk the kth feature map at layer l; W is the convolutional weight tensor; b is the bias term; and we choose the element-wise nonlinearity \u03c6(\u00b7) to be the rectified linear unit (RELU) [41] function.", "startOffset": 345, "endOffset": 349}, {"referenceID": 30, "context": "For a more detailed explanation of convolutional neural networks, we refer the reader to [31, 32, 42].", "startOffset": 89, "endOffset": 101}, {"referenceID": 31, "context": "For a more detailed explanation of convolutional neural networks, we refer the reader to [31, 32, 42].", "startOffset": 89, "endOffset": 101}, {"referenceID": 37, "context": "For a more detailed explanation of convolutional neural networks, we refer the reader to [31, 32, 42].", "startOffset": 89, "endOffset": 101}, {"referenceID": 38, "context": "We adopted a strategy similar to [43].", "startOffset": 33, "endOffset": 37}, {"referenceID": 39, "context": "Minibatch stochastic gradient descent (SGD) with momentum [44] was used to train the ConvNet.", "startOffset": 58, "endOffset": 62}, {"referenceID": 40, "context": "The gradient is estimated with the well known back-propagation algorithm [45].", "startOffset": 73, "endOffset": 77}, {"referenceID": 41, "context": "The filters and fully-connected weight matrices of the ConvNets are initialized with values selected from a uniform random distribution on an interval that is a function of the number of pre-synaptic and postsynaptic units (see [46] for more detail).", "startOffset": 228, "endOffset": 232}, {"referenceID": 42, "context": "Specifically, we apply the Canny edge detector [47] to find patches in \u201cnegative images\u201d, i.", "startOffset": 47, "endOffset": 51}, {"referenceID": 35, "context": "In our case, the amount of training data, which is represented by the number of training patches, is much smaller than standard small-scale image classification datasets [40, 48] frequently used by the deep learning community, which have on the order of 50,000 training examples.", "startOffset": 170, "endOffset": 178}, {"referenceID": 43, "context": "In our case, the amount of training data, which is represented by the number of training patches, is much smaller than standard small-scale image classification datasets [40, 48] frequently used by the deep learning community, which have on the order of 50,000 training examples.", "startOffset": 170, "endOffset": 178}, {"referenceID": 44, "context": "A complete overview is provided in [49] which we summarize below.", "startOffset": 35, "endOffset": 39}, {"referenceID": 44, "context": "FPPI is a common performance measure in the pedestrian detection community [49].", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "recall plots by a single value, we employ two scalar performance measures: (1) log-average miss rate when FPPI is in the range [1, 10], and (2) area under the precisionrecall curve (AUC).", "startOffset": 127, "endOffset": 134}, {"referenceID": 9, "context": "recall plots by a single value, we employ two scalar performance measures: (1) log-average miss rate when FPPI is in the range [1, 10], and (2) area under the precisionrecall curve (AUC).", "startOffset": 127, "endOffset": 134}, {"referenceID": 45, "context": "3We also tried a popular vision pipeline of local feature descriptors (SIFT [50]), followed by bag of visual words and a support vector machine classifier.", "startOffset": 76, "endOffset": 80}, {"referenceID": 46, "context": "[51, 52].", "startOffset": 0, "endOffset": 8}, {"referenceID": 47, "context": "[51, 52].", "startOffset": 0, "endOffset": 8}], "year": 2016, "abstractText": "Monitoring the number of insect pests is a crucial component in pheromone-based pest management systems. In this paper, we propose an automatic detection pipeline based on deep learning for identifying and counting pests in images taken inside field traps. Applied to a commercial codling moth dataset, our method shows promising performance both qualitatively and quantitatively. Compared to previous attempts at pest detection, our approach uses no pest-specific engineering which enables it to adapt to other species and environments with minimal human effort. It is amenable to implementation on parallel hardware and therefore capable of deployment in settings where real-time performance is required.", "creator": "LaTeX with hyperref package"}}}