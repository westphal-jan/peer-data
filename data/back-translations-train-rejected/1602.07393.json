{"id": "1602.07393", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Domain Specific Author Attribution Based on Feedforward Neural Network Language Models", "abstract": "Authorship attribution refers to the task of automatically determining the author based on a given sample of text. It is a problem with a long history and has a wide range of application. Building author profiles using language models is one of the most successful methods to automate this task. New language modeling methods based on neural networks alleviate the curse of dimensionality and usually outperform conventional N-gram methods. However, there have not been much research applying them to authorship attribution. In this paper, we present a novel setup of a Neural Network Language Model (NNLM) and apply it to a database of text samples from different authors. We investigate how the NNLM performs on a task with moderate author set size and relatively limited training and test data, and how the topics of the text samples affect the accuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement of fitness of a trained language model to the test data. Given 5 random test sentences, it also increases the author classification accuracy by 3.43% on average, compared with the N-gram methods using SRILM tools. An open source implementation of our methodology is freely available at", "histories": [["v1", "Wed, 24 Feb 2016 04:32:34 GMT  (597kb,D)", "http://arxiv.org/abs/1602.07393v1", "International Conference on Pattern Recognition Application and Methods (ICPRAM) 2016"]], "COMMENTS": "International Conference on Pattern Recognition Application and Methods (ICPRAM) 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["zhenhao ge", "yufang sun"], "accepted": false, "id": "1602.07393"}, "pdf": {"name": "1602.07393.pdf", "metadata": {"source": "CRF", "title": "Domain Specific Author Attribution Based on Feedforward Neural Network Language Models", "authors": ["Zhenhao Ge", "Yufang Sun"], "emails": ["zhenhao.ge@gmail.com,", "sun361@purdue.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "It is possible to do this because the author profile or style of the author embodies many characteristics, including personality, cultural and educational background, language and experience of the author, who has his own style and whose identity is easily recognizable."}, {"heading": "2 DATA PREPARATION", "text": "The database is a selection of course transcripts from Coursera, one of the largest platforms for Massive Open Online Course (MOOC). To ensure that the author responds less to the domain information, 16 courses have been selected from a specific technical and engineering text domain covering 8 areas: algorithm, data mining, information technologies (IT), machine learning, mathematics, natural language processing (NLP), programming and digital signal processing (DSP). Table 1 lists more details for each course in the database, such as the number of sentences and words, the number of words per sentence and vocabulary sizes in several levels. For privacy reasons, the exact course titles and the names of the teacher (author) are hidden. However, in order to identify the authors, it is necessary to point out that all courses are taught by different teachers, with the exception of the courses with IDs 7 and 16. This was done deliberately to examine how the subject affects."}, {"heading": "3 DSP 8,126 129,665 15.96 3,815 / 2,699 / 1,869", "text": "4 Data Mining 7,392 129,552 17,53 4,531 / 3,140 / 2,141 5 Data Mining 6,906 129,068 18,69 3,008 / 2,041 / 1,475"}, {"heading": "6 DSP 20,271 360,508 17.78 8,878 / 5,820 / 2,687", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 IT 9,103 164,812 18.11 4,369 / 2,749 / 1,979", "text": "8 Mathematics 5,736 101,012 17,61 3,095 / 2,148 / 1,500 9 Machine learning 11,090 224,504 20,24 6,293 / 4,071 / 2,259 10 Programming 8,185 160,390 19,60 4,045 / 2,771 / 1,898"}, {"heading": "11 NLP 7,095 111,154 15.67 3,691 / 2,572 / 1,789", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "12 NLP 4,395 100,408 22.85 3,973 / 2,605 / 1,789", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "13 NLP 4,382 96,948 22.12 4,730 / 3,467 / 2,071", "text": "14 Machine Learning 6,174 116,344 18.84 5,844 / 4,127 / 2,686 15 Mathematics 5,895 152,100 25.80 3,933 / 2,697 / 1,918 16 Programming 6,400 136,549 21.34 4,997 / 3,322 / 2,243ure 1 shows how the vocabulary of each course shrinks by descent and cut. There are only 0.5 x 1.5% words among all records associated with < unk >, but vocabulary sizes are significantly reduced to an average of 2000."}, {"heading": "3 NEURAL NETWORK LANGUAGE MODEL", "text": "The language model is trained using a neural network illustrated in Figure 2, in which weights are trained using a sequence of N-words W1, W2,.., WN from the training text to predict successively the word Wt, t [1, N] in a particular target word position, using the information from the remaining words as it is used in Equation (1).W * = argmax t P (Wt | W1W2 \u00b7 \u00b7 \u00b7 Wi \u00b7 \u00b7 \u00b7 WN), i 6 = t (1). It is similar to the classic N-gram language model, where the primary task is to predict the next word, the N \u2212 1 previous word. However, the network can be trained here to predict the target word in any position, as the adjacent words are predefined. The network contains 4 different types of layers: the word layer, the embedding layer, the hidden layer \u2212 and the output (softmax) layer. The weights are transferred between the adjacent words to the weights placed in the 3."}, {"heading": "3.1 Cost Function", "text": "Given the vocabulary size V, predicting a single word from V operations is a multinomial classification problem, so the cost function to be minimized can be formulated as: asC = \u2212 \u2211 V t j logy j. (2) C is the cross entropy, and y j, where j-V and \u2211 j-V y-j = 1 is the output of node j in the last output layer of the network, i.e. the probability of selecting the jth word as the predicted word. The parameter t j is the target designation and t-j classification {0.1}. As a 1-of-V multiclass classification problem, there is only one target 1, and the rest are 0s."}, {"heading": "3.2 Forward Propagation", "text": "Forward propagation is a process for calculating the output y j of each layer L j with a) its neural function (i.e. max.: sigmoid, linear, rectified, binary, etc.) and b) the input z \u2212 j, calculated using the output of the previous layer yi, weights Wi j from layer Li to layer L j and preload b j of the current layer L j. After weight and preset, neural network training begins from the forward propagation of the word input to the output in the last layer yi, weights Wi j from layer Li to layer L j, and preset b j of the current layer L j. According to weight and preset, each of the N \u2212 1 input words wi is represented by a binary index vector xi with length equal to the word size V. It contains all 0s, but only one 1 in a certain position to distinguish it from all other words. The word xi is converted into its distributed representation in the so-called embed layer xi (b)."}, {"heading": "3.3 Backward Propagation", "text": "After the propagation of the input words xi to the final output yout of the network, through Eq. (3) to Eq. (8), the next task is to move the error derivatives backwards from the output level to the input level, so that we know the directions and orders of magnitude to update the weights between the levels. However, it starts with the derivative digit C. \u2212 zout (i) of the node i in the output level, i.e. \u2212 zout (i) zout (i) = zout (j) zout (j) zout (j) zout (i) = yout (i) = yout (i) \u2212 ti. (9) The further derivative of Eq. (9) requires a division of the digit yout (i) into cases of i = j and i = 6 = j."}, {"heading": "3.4 Weight and Bias Update", "text": "Describe W as the general form of the weight matrices Wword \u2212 emb, Wemb \u2212 hid and Whid \u2212 out and \u2206 as the averaged version of the weight gradient containing information from previous iterations and initialized with zeros, the weight matrices will be updated with: {\u2206 i + 1 = \u03b1 \u0445 i + \u2202 C \u2202 Wi Wi Wi + 1 = Wi \u2212 \u03b5 \u0445 i + 1 (19), where \u03b1 is the impulse determining the percentage of weight gradients taken from the previous iteration and \u03b5 the learning rate determining the step size to update the weight distribution in the direction of descent. Similarly, the weight increases will be updated by replacing W with b in equation. (19)"}, {"heading": "3.5 Summary of NNLM", "text": "In NNLM training, the entire training dataset is segmented into mini-lots with lot size M. The neural network in terms of weights and biases is updated with each iteration of the mini-lot training. \u2202 C \u2202 Wi in Eq. (19) should be normalized by M. A cycle of feeding all data is called an epoch and has the flexibility to change training parameters such as learning rate \u03b5 and impulse \u03b1 through different epochs, and usually includes 10 to 20 epochs to obtain a well-trained network. Next, we present a method for training the NLM. It includes all the key components described above, has the flexibility to change training parameters through different epochs, and includes an early termination criterion."}, {"heading": "4 IMPLEMENTATION AND RESULTS", "text": "This section discusses the implementation details of the Authorisation System as a N-way classification problem using NNLM. Results are compared with N-gram language models trained with the SRILM toolkit (Stolcke et al., 2002)."}, {"heading": "4.1 NNLM Implementation and Optimization", "text": "The database for each of the 16 courses is randomly divided into training, validation and test sets at a ratio of 8: 1: 1. To compensate for the variation of the model due to the limited data size, the segmentation is performed 10 times with different randomization seeds, so that the mean and the variation of performance can be measured. For each course in this project, we trained a different 4-gram NLM, i.e. the context size N = 4, to predict the fourth word using the 3 preceding words. However, the other model parameters are searched for and optimized within certain ranges, using a) the number of epochs (15), b) the epoch in which the decay of the learning rate begins (10), c) the decay factor of the learning rate (0.9). However, the other model parameters are searched for and optimized within certain ranges, using a multi-resolution optimization scheme, whereby a) the dimension of embedding of the Nemb 200 profile (25: 800) impulse (0.0): 0.3 (0.3), the dimension of the insertion of the profile profile profile node (0): 0.8: 0.0 (0)."}, {"heading": "4.2 Classification with Perplexity Measurement", "text": "Statistical language models provide a tool for calculating the probability of the target word Wt given N \u2212 1 context words W1, W2,.., Wi,.., WN, i-N, i \u2212 6 = t. Normally, the target word is the ninth word and the context words are the preceding N \u2212 1 words. Denote W n1 as word sequence (W1, W2,.., Wn). Using the chain rule of probability, the probability of the sequence W n1 can be formulated asP (W n1) = P (W1) P (W2 | W1) P (W2,.,.) M \u2212 performations M n \u2212 k = 1P (Wk | W k \u2212 11). (20) Use a Markov chain that approximates the probability of a word sequence with arbitrary length n to the probability of a sequence with the narrowest N words."}, {"heading": "4.3 Classification Accuracy and Confusion Matrix", "text": "To test the classification accuracy for a particular instructor, the set-by-set confusion with the trained NNLMs from different classes is calculated, and the sets are randomly selected from the test set. Figure 3 (a) graphically shows the accuracy vs. number of sets for a particular course with ID 3. Accuracy is obtained from 3 different methods, SRI uniqram, 4-gram and NNLM 4-gram. The number of randomly selected sets ranges from 1 to 20, and for each specific number of sets 100 attempts have been made, and the mean accuracy with standard deviations is shown in Figure 2. As mentioned in Figure 2, courses with ID 7 and 16 were taught by the same instructor, these two courses are excluded and 14 courses / instructors are used to calculate their 16-way classification accuracies. Figure 3 (b) shows the mean accuracy over these 14 courses."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "The results in the terms LM-Fitness in perplexity, classification and precision are promising compared to the basic N-Gram methods. The performance is very competitive in terms of the state of classification and testing of sensitivity, i.e. the length of the test text used to achieve certain results 1, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 7, 7, 7."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors thank Coursera Incorporation for providing the transcript data for research in this paper."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": "Oxford university press.", "citeRegEx": "Bishop,? 1995", "shortCiteRegEx": "Bishop", "year": 1995}, {"title": "Authorship attribution using word sequences", "author": ["R.M. Coyotl-Morales", "L. Villase\u00f1or-Pineda", "M. Montes-y G\u00f3mez", "P. Rosso"], "venue": "Progress in Pattern Recognition, Image Analysis and Applications, pages 844\u2013853. Springer.", "citeRegEx": "Coyotl.Morales et al\\.,? 2006", "shortCiteRegEx": "Coyotl.Morales et al\\.", "year": 2006}, {"title": "Automated authorship attribution using advanced signal classification techniques", "author": ["M. Ebrahimpour", "T.J. Putni\u0146\u0161", "M.J. Berryman", "A. Allison", "Ng", "B.W.-H.", "D. Abbott"], "venue": "PloS one, 8(2):e54998.", "citeRegEx": "Ebrahimpour et al\\.,? 2013", "shortCiteRegEx": "Ebrahimpour et al\\.", "year": 2013}, {"title": "Sleep stages classification using neural networks with multi-channel neural data", "author": ["Z. Ge", "Y. Sun"], "venue": "Brain Informatics and Health, pages 306\u2013316. Springer.", "citeRegEx": "Ge and Sun,? 2015", "shortCiteRegEx": "Ge and Sun", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "Mohamed", "A.r", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "Sainath", "T. N"], "venue": "Signal Processing", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Authorship attribution", "author": ["P. Juola"], "venue": "Foundations and Trends in information Retrieval, 1(3):233\u2013334.", "citeRegEx": "Juola,? 2006", "shortCiteRegEx": "Juola", "year": 2006}, {"title": "N-gram-based author profiles for authorship attribution", "author": ["V. Ke\u0161elj", "F. Peng", "N. Cercone", "C. Thomas"], "venue": "Proceedings of the conference pacific association for computational linguistics, PACLING, volume 3, pages 255\u2013264.", "citeRegEx": "Ke\u0161elj et al\\.,? 2003", "shortCiteRegEx": "Ke\u0161elj et al\\.", "year": 2003}, {"title": "Computational methods in authorship attribution", "author": ["M. Koppel", "J. Schler", "S. Argamon"], "venue": "Journal of the American Society for information Science and Technology, 60(1):9\u201326.", "citeRegEx": "Koppel et al\\.,? 2009", "shortCiteRegEx": "Koppel et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Scalability issues in authorship attribution", "author": ["K. Luyckx"], "venue": "ASP/VUBPRESS/UPA.", "citeRegEx": "Luyckx,? 2011", "shortCiteRegEx": "Luyckx", "year": 2011}, {"title": "Authorship attribution and verification with many authors and limited data", "author": ["K. Luyckx", "W. Daelemans"], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 513\u2013520. Association for Computational Linguistics.", "citeRegEx": "Luyckx and Daelemans,? 2008", "shortCiteRegEx": "Luyckx and Daelemans", "year": 2008}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH 2010, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Learning Distributed Representations for Statistical Language Modelling and Collaborative Filtering", "author": ["A. Mnih"], "venue": "PhD thesis, University of Toronto.", "citeRegEx": "Mnih,? 2010", "shortCiteRegEx": "Mnih", "year": 2010}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th international conference on Machine learning, pages 641\u2013648. ACM.", "citeRegEx": "Mnih and Hinton,? 2007", "shortCiteRegEx": "Mnih and Hinton", "year": 2007}, {"title": "An algorithm for suffix stripping", "author": ["M.F. Porter"], "venue": "Program, 14(3):130\u2013137.", "citeRegEx": "Porter,? 1980", "shortCiteRegEx": "Porter", "year": 1980}, {"title": "Authorship attribution with latent dirichlet allocation", "author": ["Y. Seroussi", "I. Zukerman", "F. Bohnert"], "venue": "Proceedings of the fifteenth conference on computational natural language learning, pages 181\u2013189. Association for Computational Linguistics.", "citeRegEx": "Seroussi et al\\.,? 2011", "shortCiteRegEx": "Seroussi et al\\.", "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "C. Manning", "A.Y. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML11), pages 129\u2013136.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "A survey of modern authorship attribution methods", "author": ["E. Stamatatos"], "venue": "Journal of the American Society for information Science and Technology, 60(3):538\u2013556.", "citeRegEx": "Stamatatos,? 2009", "shortCiteRegEx": "Stamatatos", "year": 2009}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["A Stolcke"], "venue": "INTERSPEECH.", "citeRegEx": "Stolcke,? 2002", "shortCiteRegEx": "Stolcke", "year": 2002}], "referenceMentions": [{"referenceID": 6, "context": "Juola (Juola, 2006) and Stamatatos (Stamatatos, 2009) for example, have surveyed the state of the art and proposed a set of recommendations to move forward.", "startOffset": 6, "endOffset": 19}, {"referenceID": 18, "context": "Juola (Juola, 2006) and Stamatatos (Stamatatos, 2009) for example, have surveyed the state of the art and proposed a set of recommendations to move forward.", "startOffset": 35, "endOffset": 53}, {"referenceID": 8, "context": "As more text data become available from the Web and computational linguistic models using statistical methods mature, more opportunities and challenges arise in this area (Koppel et al., 2009).", "startOffset": 171, "endOffset": 192}, {"referenceID": 16, "context": "Many statistical models have been successfully applied in this area, such as Latent Dirichlet Allocation (LDA) for topic modeling and dimension reduction (Seroussi et al., 2011), Naive Bayes for text classification (CoyotlMorales et al.", "startOffset": 154, "endOffset": 177}, {"referenceID": 3, "context": ", 2006), Multiple Discriminant Analysis (MDA) and Support Vector Machines (SVM) for feature selection and classification (Ebrahimpour et al., 2013).", "startOffset": 121, "endOffset": 147}, {"referenceID": 7, "context": "Methods based on language modeling are also among the most popular methods for authorship attribution (Ke\u0161elj et al., 2003).", "startOffset": 102, "endOffset": 123}, {"referenceID": 5, "context": "Neural networks with deep learning have been successfully applied in many applications, such as speech recognition (Hinton et al., 2012), object detection (Krizhevsky et al.", "startOffset": 115, "endOffset": 136}, {"referenceID": 9, "context": ", 2012), object detection (Krizhevsky et al., 2012), natural language processing (Socher et al.", "startOffset": 26, "endOffset": 51}, {"referenceID": 17, "context": ", 2012), natural language processing (Socher et al., 2011), and other pattern recognition and classification tasks (Bishop, 1995), (Ge and Sun, 2015).", "startOffset": 37, "endOffset": 58}, {"referenceID": 1, "context": ", 2011), and other pattern recognition and classification tasks (Bishop, 1995), (Ge and Sun, 2015).", "startOffset": 64, "endOffset": 78}, {"referenceID": 4, "context": ", 2011), and other pattern recognition and classification tasks (Bishop, 1995), (Ge and Sun, 2015).", "startOffset": 80, "endOffset": 98}, {"referenceID": 0, "context": "Neural Network based Language Models (NNLM) have surpassed the performance of traditional N-gram LMs (Bengio et al., 2003), (Mnih and Hinton, 2007) and are purported to generalize better in smaller datasets (Mnih, 2010).", "startOffset": 101, "endOffset": 122}, {"referenceID": 14, "context": ", 2003), (Mnih and Hinton, 2007) and are purported to generalize better in smaller datasets (Mnih, 2010).", "startOffset": 9, "endOffset": 32}, {"referenceID": 13, "context": ", 2003), (Mnih and Hinton, 2007) and are purported to generalize better in smaller datasets (Mnih, 2010).", "startOffset": 92, "endOffset": 104}, {"referenceID": 10, "context": "The performance of the proposed method depends highly on the settings of the experiment, in particular the experimental design, author set size and data size (Luyckx, 2011).", "startOffset": 158, "endOffset": 172}, {"referenceID": 11, "context": "This often leads to contextbiased models, where the accuracy of author detection is highly dependent on the degree to which the topics in training and test sets match each other (Luyckx and Daelemans, 2008).", "startOffset": 178, "endOffset": 206}, {"referenceID": 15, "context": "The sentence-wise datasets are then stemmed using the Porter Stemming algorithm (Porter, 1980).", "startOffset": 80, "endOffset": 94}, {"referenceID": 10, "context": "Since purely topic-neutral text data may not even exist (Luyckx, 2011), developing general author LMs with mixed-topic data, and then adapting them to particular topics may also be desirable.", "startOffset": 56, "endOffset": 70}, {"referenceID": 13, "context": "Because the NNLM assigns a unique representation for a single word, it is difficult to model words with multiple meanings (Mnih, 2010).", "startOffset": 122, "endOffset": 134}, {"referenceID": 12, "context": "The recurrent NNLM, which captures more context size than the current feed-forward model (Mikolov et al., 2010), may also be worth exploring.", "startOffset": 89, "endOffset": 111}], "year": 2016, "abstractText": "Authorship attribution refers to the task of automatically determining the author based on a given sample of text. It is a problem with a long history and has a wide range of application. Building author profiles using language models is one of the most successful methods to automate this task. New language modeling methods based on neural networks alleviate the curse of dimensionality and usually outperform conventional N-gram methods. However, there have not been much research applying them to authorship attribution. In this paper, we present a novel setup of a Neural Network Language Model (NNLM) and apply it to a database of text samples from different authors. We investigate how the NNLM performs on a task with moderate author set size and relatively limited training and test data, and how the topics of the text samples affect the accuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement of fitness of a trained language model to the test data. Given 5 random test sentences, it also increases the author classification accuracy by 3.43% on average, compared with the N-gram methods using SRILM tools. An open source implementation of our methodology is freely available at https://github.com/zge/authorship-attribution/.", "creator": "LaTeX with hyperref package"}}}