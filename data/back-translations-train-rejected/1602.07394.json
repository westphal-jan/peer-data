{"id": "1602.07394", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Improved Accent Classification Combining Phonetic Vowels with Acoustic Features", "abstract": "Researches have shown accent classification can be improved by integrating semantic information into pure acoustic approach. In this work, we combine phonetic knowledge, such as vowels, with enhanced acoustic features to build an improved accent classification system. The classifier is based on Gaussian Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual Linear Predictive (PLP) features. The features are further optimized by Principle Component Analysis (PCA) and Hetroscedastic Linear Discriminant Analysis (HLDA). Using 7 major types of accented speech from the Foreign Accented English (FAE) corpus, the system achieves classification accuracy 54% with input test data as short as 20 seconds, which is competitive to the state of the art in this field.", "histories": [["v1", "Wed, 24 Feb 2016 04:33:49 GMT  (520kb,D)", "http://arxiv.org/abs/1602.07394v1", "International Congress on Image and Signal Processing (CISP) 2015"]], "COMMENTS": "International Congress on Image and Signal Processing (CISP) 2015", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["zhenhao ge"], "accepted": false, "id": "1602.07394"}, "pdf": {"name": "1602.07394.pdf", "metadata": {"source": "CRF", "title": "Improved Accent Classification Combining Phonetic Vowels with Acoustic Features", "authors": ["Zhenhao Ge"], "emails": [], "sections": [{"heading": null, "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "II. DATA AND FEATURE PREPARATION", "text": "In this section we present the database used in this thesis, discuss feature extraction including normalization and gaussianization, as well as feature optimization and dimension reduction with PCA and HLDA. The entire process is illustrated in Fig.1."}, {"heading": "A. Database", "text": "The Foreign Accented English (FAE) corpus of the Linguistic Data Consortium (LDC), catalog number LDC2007S08, is used in this thesis. It is one of the most comprehensive accented English language databases currently available and contains 4925 sentences with 23 accent types with an average duration of 20 seconds.Accents are divided into 7 main categories based on their relationships shown in Table I, and one accent is selected from each group to develop a 7-way accent classifier. Table IIar Xiv: 160 2.07 394v 1 [cs.S D] February 24, 2016 summarizes some statistics in each of these accent groups, e.g. 1) the number of utterances; 2) their share in the total FAE corpus; 3) the total duration before and after the silence removal and 4) their corresponding compression ratio (Dir.2 / Dir.1). As there is no transcription of the speech in the FAE's important data, we also transcribe the audio sequence in this FAE 15th."}, {"heading": "B. Silence Removal", "text": "In practice, only the high signal-to-noise ratio (SNR) of the waveform is retained for classification. Therefore, the removal of silence or so-called voice activity detection (VAD) is often performed prior to feature extraction, using the method described in [7], which recognizes silence by determining the short-term energy rate and spectral centroids of speech. It is also possible to use the Auditory Toolbox [8] or the Voicebox [9] for the same purpose.Given the distance of si (n), n [1, N] as the audio samples in the ith frame, their short-term energy rate can be formulated as total energy ei: asei = 1N N \u2211 n = 1 | si (n) | 2, (1) where N is the number of samples in a frame. Spectral centroids can be formulated as asci = 1 (k + 1) Si (k), Si (k), (2)."}, {"heading": "C. Feature Extraction and Optimization", "text": "This is particularly useful because the distribution characteristics here can be modelled by Gaussians [12]. The standardised and packaged characteristics have been further improved by PCA and HLDA to mitigate the effects of locally linear channel mixing. PCA is often used for dimension reduction, which preserves the data dimensions with larger deviations in the eigenspace. It has been applied to many applications, such as face recognition [13] and language evaluation, etc. It also helps to regulate the data and avoid overadjustments in HLDA, which are then performed in the eigenspace."}, {"heading": "III. PHONETIC VOWEL REPRESENTATION", "text": "Minematsu et al. [20] and Suzuki et al. [4] showed that for a particular speaker, the position of 5 basic vowels in the characteristic space of a target language (such as English in this paper) is relatively uniform. Therefore, they can be extracted as accent-adapted characteristics and used to identify the accent of that speaker. Fig.3 is a simple demonstration of 5 vowels from both accentuated and unaccentuated (standard) languages in the reduced two-dimensional characteristic space [20]. The center in each pentagon is the weighted average of five vowels based on their positions in the characteristic space and the frequency of occurrence in the corpus. By classifying the center of the pentagon of the standard and the accented language into the overlapped pentagon in the lower pentagon of Figure 3, the Bhattacharyya spacing of five vowels [21] between each pair of corresponding vowels and their angles in a vowel of the standard and the accented pentagon of the accented pentagon is stored into the overlapped pentagon in the lower pentagon of the pentagon of the category 3, the bhattacharyya spacing of five vowels [21] between each pair of the vowels in the characteristic space and frequency of the corpentagon of the corpus. By classifying the center of the pentagon of the standard and the accented language into the overlapped pentagon of the overlapped pentagon in the overlapped pentagon in the lower pentagon of the pentagon of the pentagon of the pentagon of Figure 3, the bhattacharyya is the weighted average of five vowels of five vowels [21] between each pair of the corresponding vowels in the pentagonal space and the corpentagon and the corpentagon of the corpentagon of the corpentagon of the standard and the corpentagon of the corpentagon of the standard and the corpentagon of the corpentagon [20]."}, {"heading": "A. Phoneme Alignment in System Development", "text": "In order to extract vowels from speech data, phoneme alignment was performed during system development, with internal transcriptions of the FAE subcorpus covering the 7 accents, each of which comes from a main type of accent groups. We prepared the dictionary required for phoneme alignment using HVite in HTK [22], by a process called transcription purification, word collection, word-to-pronunciation conversion, etc. Figure 4 demonstrates the process of dictionary creation and phoneme alignment for FAE corpus. The dictionary file is a list of word pairs and pronunciations in HTK format that can be generated through the process of word collection, word-to-pronunciation conversion using ININ Lexicon tester and HTK dictionary file."}, {"heading": "B. Phoneme Recognition in System Test", "text": "No transcription is available during the system test. To find vowel-related characteristics, phoneme recognition was performed on the language accented in the test using HTK. Because recognition cannot be perfect, only a subset of recognized vowels with a confidence level above a threshold based on the n-gram protocol probability was used, which was predefined with training and development data."}, {"heading": "IV. GMM-UBM FRAMEWORK FOR ACCENT CLASSIFICATION", "text": "The accent classification algorithm developed in this paper treats accents like loudspeakers and models accent attributes using GMMUBM, which is similar to modeling the attributes of loudspeakers in loudspeaker classification systems. Modern loudspeaker classification systems build a general Gaussian mixing model (GMM) with data from all loudspeakers, the so-called Universal Background Model (UBM), and then generate individual GMMs for each speaker by adapting UBM with characteristics of individual speakers. Subsec. IV-A and IV-B give an overview of the application of the similar framework to an accent classification problem."}, {"heading": "A. Universal Background Model (UBM)", "text": "The Universal Background Model (UBM) is a general GMM formed with characteristics of all kinds of accents. In view of a GMM \u03bb = {wi, \u00b5i, \u0439i}, i [1, N] and N is the number of mixing components, the probability function for a characteristic frame x can be formulated as follows: asp (x | \u03bb) = N \u2211 i = 1 wipi (x), (5) where (x) = 1 (2\u03c0) M / 2 | 1 / 2 exp {\u2212 1 2 (x \u2212 \u00b5i) T \u03a3 \u2212 1i (x \u2212 \u00b5i)}. (6) The parameters of GMM \u03bb, including weight wi, \u00b5i and covariance matrix \u0432i, can be optimized by the ExpectationMaximization (EM) algorithm [24]."}, {"heading": "B. Adaptation of Accent Model", "text": "In the GMM-UBM system, we derive the individual accent of GMMs by adjusting the parameters of the UBM \u03bbUBM using the training language X = {x1, x2,.., xK} of each accent and a form of Bayesian adaptation. Adaptation is a two-stage process and the first step is identical to the E-step of the EM algorithm, in which we determine the probable orientation of X in the UBM mixtures. (8) Then, weight, mean and variance of ni = K \u2211 k = 1 Pr (i | xk), (9) Ei (x) = 1 wnpn (xk) can be calculated."}, {"heading": "C. Baseline Classifier", "text": "The first equation is conditioned by the Bayes rule. Pr (1 / S) and P (X) are the first proportions."}, {"heading": "V. RESULTS AND CONCLUSION", "text": "The basic classification was based on the accent GMM classifier with 256 mixtures, adapted to UBM, using normalized and distorted 39-dimensional PLPs. Characteristics were then optimized with PCA and HLDA with context size C = 1, and the dimension was reduced from 39 to 20. With the improved feature, the basic accuracy was increased from 42.3% to 47.9%. The main contribution to improving accuracy came from the classifier of the combination of weighted vowels, which further increased the 7-fold classification rate to 53.7%. Table IV shows the performance of all these 3 experiments with different models and characteristics. This work shows that loudspeaker recognition methods can be used for accent classification. With multiple feature optimization techniques and phonetic vowel information, the accuracy gained from accented language is as short as 20 seconds, competitive with the state of the art [29, 1, 3] and [29, 3] [in terms of classification]."}], "references": [{"title": "An empirical study of automatic accent classification", "author": ["G. Choueiter", "G. Zweig", "P. Nguyen"], "venue": "ICASSP 2008. IEEE International Conference on. IEEE.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "A novel approach to detecting non-native speakers and their native language", "author": ["M.K. Omar", "J. Pelecanos"], "venue": "ICASSP, 2010 IEEE International Conference on. IEEE.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Acoustic adaptation and accent identification in the ICSI MR and FAE corpora", "author": ["J. Mac\u0131\u0301as-Guarasa"], "venue": "ICSI Meeting slides, 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Improved structurebased automatic estimation of pronunciation proficiency", "author": ["M. Suzuki", "L. Dean", "N. Minematsu", "K. Hirose"], "venue": "Proc. SLaTE, vol. 5, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Mispronunciation Detection with Multiple Applications: for Language Learning and Speech Recognition Adaptation and Improvement", "author": ["Z. Ge"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Accent classification with phonetic vowel representation", "author": ["Z. Ge", "Y. Tan", "A. Ganapathiraju"], "venue": "Pattern Recognition (ACPR), 2015 3rd IAPR Asian Conference on. IEEE, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A method for silence removal and segmentation of speech signals, implemented in matlab", "author": ["T. Giannakopoulos"], "venue": "University of Athens, Athens, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Auditory toolbox", "author": ["M. Slaney"], "venue": "Interval Research Corporation, Tech. Rep, vol. 10, p. 1998, 1998.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Voicebox: Speech processing toolbox for matlab", "author": ["M. Brookes"], "venue": "Software, available [Mar. 2011] from www. ee. ic. ac. uk/hp/staff/dmb/voicebox/voicebox. html, 1997.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "PLP and RASTA (and MFCC, and inversion) in MATLAB", "author": ["D. Ellis"], "venue": "http://labrosa.ee.columbia.edu/matlab/rastamat/, accessed 2015-07-01.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "MSR identity toolbox v1. 0: A matlab toolbox for speaker recognition research", "author": ["S.O. Sadjadi", "M. Slaney", "L. Heck"], "venue": "Speech and Language Processing Technical Committee Newsletter, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature warping for robust speaker verification", "author": ["J. Pelecanos", "S. Sridharan"], "venue": "2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Face recognition using eigenfaces", "author": ["M. Turk", "A.P. Pentland"], "venue": "Computer Vision and Pattern Recognition, 1991. Proceedings CVPR\u201991., IEEE Computer Society Conference on. IEEE, 1991, pp. 586\u2013591.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1991}, {"title": "PCA method for automated detection of mispronounced words", "author": ["Z. Ge", "S.R. Sharma", "M.J. Smith"], "venue": "SPIE Defense, Security, and Sensing. International Society for Optics and Photonics, 2011, pp. 80 581D\u201380 581D.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Why can LDA be performed in PCA transformed space?", "author": ["J. Yang", "J.-y. Yang"], "venue": "Pattern recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Face recognition using LDA-based algorithms", "author": ["J. Lu", "K.N. Plataniotis", "A.N. Venetsanopoulos"], "venue": "Neural Networks, IEEE Transactions on, vol. 14, no. 1, pp. 195\u2013200, 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "PCA/LDA approach for text-independent speaker recognition", "author": ["Z. Ge", "S.R. Sharma", "M.J. Smith"], "venue": "SPIE Defense, Security, and Sensing. International Society for Optics and Photonics, 2012, pp. 840 108\u2013840 108.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Investigation of silicon auditory models and generalization of linear discriminant analysis for improved speech recognition", "author": ["N. Kumar", "A.G. Andreou"], "venue": "Ph.D. dissertation, Johns Hopkins University, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Mispronunciation detection for language learning and speech recognition adaptation", "author": ["Z. Ge"], "venue": "2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Yet another acoustic representation of speech sounds", "author": ["N. Minematsu"], "venue": "ICASSP\u201904. IEEE International Conference on. IEEE.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 0}, {"title": "On a measure of divergence between two multinomial populations", "author": ["A. Bhattacharyya"], "venue": "Sankhy\u0101: The Indian Journal of Statistics (1933- 1960), vol. 7, no. 4, pp. 401\u2013406, 1946.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1933}, {"title": "The HTK book (for HTK version 3.4)", "author": ["S. Young", "G. Evermann", "M. Gales", "T. Hain", "D. Kershaw", "X.A. Liu", "G. Moore", "J. Odell", "D. Ollason", "D. Povey"], "venue": "2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Speaker verification using adapted gaussian mixture models", "author": ["D.A. Reynolds", "T.F. Quatieri", "R.B. Dunn"], "venue": "Digital signal processing, vol. 10, no. 1, pp. 19\u201341, 2000.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 1\u201338, 1977.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1977}, {"title": "Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains", "author": ["J.-L. Gauvain", "C.-H. Lee"], "venue": "Speech and audio processing, ieee transactions on, vol. 2, no. 2, pp. 291\u2013298, 1994.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1994}, {"title": "Arpabet", "author": ["Wikipedia"], "venue": "http://en.wikipedia.org/wiki/Arpabet, August 2011, accessed 2015-07-01.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Multivariate online kernel density estimation with gaussian kernels", "author": ["M. Kristan", "A. Leonardis", "D. Sko\u010daj"], "venue": "Pattern Recognition, vol. 44, no. 10, pp. 2630\u20132642, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Full-covariance UBM and heavy-tailed PLDA in i-vector speaker verification", "author": ["P. Mat\u011bjka", "O. Glembek", "F. Castaldo", "M.J. Alam", "O. Plchot", "P. Kenny", "L. Burget", "J.H. \u010cernocky"], "venue": "Acoustics, Speech and Signal Processing, 2011 IEEE International Conference on. IEEE, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Sleep stages classification using neural networks with multi-channel neural data", "author": ["Z. Ge", "Y. Sun"], "venue": "Brain Informatics and Health. Springer, 2015, pp. 306\u2013316.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "achieved accuracy of 32% classifying 23 types of accented English [1], using methods in language identification (LID), such as Maximum Mutual Information (MMI) training and Gaussian tokenization.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "recently integrated Universal Background Model (UBM) into Support Vector Machine (SVM) classifier and claimed that it outperformed the results in [1] by 75.", "startOffset": 146, "endOffset": 149}, {"referenceID": 1, "context": "3% relatively [2].", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "Spanish classification in [3] reported classification rates of 73% and 58.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "2% were reported in [3], for 4-, 13- and 23-way classification using naive Bayes.", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "Since most identifiable accents are presented from the pronunciation of vowels rather than consonants [4], multiple vowel-specific GMMs were computed with features of the vowel components, extracted either from phoneme alignment (in system development) or phoneme recognition (in system test).", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "This work was initiated during the author\u2019s internship at Interactive Intelligence (ININ) [5], and the algorithm and experiments were later refined for better accuracy and efficiency [6].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "This work was initiated during the author\u2019s internship at Interactive Intelligence (ININ) [5], and the algorithm and experiments were later refined for better accuracy and efficiency [6].", "startOffset": 183, "endOffset": 186}, {"referenceID": 6, "context": "Here we use the method described in [7], which detects the silence by thresholding on the short-time energy rate and spectral centroids of the speech.", "startOffset": 36, "endOffset": 39}, {"referenceID": 7, "context": "One can also use either Auditory Toolbox [8] or Voicebox [9] for the same purpose.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "One can also use either Auditory Toolbox [8] or Voicebox [9] for the same purpose.", "startOffset": 57, "endOffset": 60}, {"referenceID": 9, "context": "After silence removal, the data of the selected accents were then transformed to 39-dimensional PLP windowed feature frames with 10 millisecond each, using the method in [10].", "startOffset": 170, "endOffset": 174}, {"referenceID": 10, "context": "a feature warping) were applied afterwards using the method in [11].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "This is specially useful because the features distribution here can be modeled by Gaussians [12].", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "It has been applied to many applications, such as face recognition [13] and speech evaluation [14], etc.", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "It has been applied to many applications, such as face recognition [13] and speech evaluation [14], etc.", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "It also helps to regularize the data and avoid over-fitting in HLDA which is performed afterwards [15].", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "It has been also applied to many problems, such as face recognition [16] and speaker recognition [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "It has been also applied to many problems, such as face recognition [16] and speaker recognition [17].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "The first definitions are consistent with the LDA definition used in Kumar\u2019s HLDA work [18] and are used in this work.", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "An improved version of Kumar\u2019s HLDA algorithm with more flexibility and higher efficiency was developed in MATLAB and used in this work [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "[20] and Suzuki et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] demonstrated that, for a particular speaker, the location of 5 fundamental vowels in the feature space of a target language (such as English in this work), is relatively consistent.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "3 is a simple demonstration of 5 vowels from both accented and non-accented (standard) languages in the reduced 2-dimensional feature space [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 20, "context": "By matching the center of the pentagon of the standard and the accented language into the overlapped pentagon in the bottom of Figure 3, the Bhattacharyya distances [21] between each pair of corresponding vowels and their angles can be computed and stored in a vector.", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "We prepared the dictionary needed for phoneme alignment using HVite in HTK [22], through a procedure including transcription cleaning, word collection, word-to-pronunciation conversion, etc.", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "The Gaussian Mixture Model-Universal Background Model (GMM-UBM) framework has been sussessfully applied to speech verification and classification systems [23].", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "(6) The parameters of GMM \u03bb, including weight wi, \u03bci and covariance matrix \u03a3i can be optimized by ExpectationMaximization (EM) algorithm [24].", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "They can be derived from the Maximum a posteriori (MAP) estimation equations for a GMM using constraints on the prior distribution described in [25].", "startOffset": 144, "endOffset": 148}, {"referenceID": 25, "context": "III, the same concept was generalized and all 15 vowels in Arpabet [26] were used, which are listed in Table III.", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "8 shows the Hellinger distances [27] computed between any 2 GMM distributions of 7 different accent types for the vowel aa.", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "With several feature optimization techniques and phonetic vowel information, the accuracy obtained from accented speech as short as 20 seconds, is competitive compared with the state of the art in [1], [2] and [3].", "startOffset": 197, "endOffset": 200}, {"referenceID": 1, "context": "With several feature optimization techniques and phonetic vowel information, the accuracy obtained from accented speech as short as 20 seconds, is competitive compared with the state of the art in [1], [2] and [3].", "startOffset": 202, "endOffset": 205}, {"referenceID": 2, "context": "With several feature optimization techniques and phonetic vowel information, the accuracy obtained from accented speech as short as 20 seconds, is competitive compared with the state of the art in [1], [2] and [3].", "startOffset": 210, "endOffset": 213}, {"referenceID": 27, "context": "In the future, more recent classification methods such as i-vector (eigenvoice component) [28], or neural network classifier [29] can be explored, used or combined with the current methods.", "startOffset": 90, "endOffset": 94}, {"referenceID": 28, "context": "In the future, more recent classification methods such as i-vector (eigenvoice component) [28], or neural network classifier [29] can be explored, used or combined with the current methods.", "startOffset": 125, "endOffset": 129}], "year": 2016, "abstractText": "Researches have shown accent classification can be improved by integrating semantic information into pure acoustic approach. In this work, we combine phonetic knowledge, such as vowels, with enhanced acoustic features to build an improved accent classification system. The classifier is based on Gaussian Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual Linear Predictive (PLP) features. The features are further optimized by Principle Component Analysis (PCA) and Hetroscedastic Linear Discriminant Analysis (HLDA). Using 7 major types of accented speech from the Foreign Accented English (FAE) corpus, the system achieves classification accuracy 54% with input test data as short as 20 seconds, which is competitive to the state of the art in this field.", "creator": "LaTeX with hyperref package"}}}