{"id": "1602.07495", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Active Learning from Positive and Unlabeled Data", "abstract": "During recent years, active learning has evolved into a popular paradigm for utilizing user's feedback to improve accuracy of learning algorithms. Active learning works by selecting the most informative sample among unlabeled data and querying the label of that point from user. Many different methods such as uncertainty sampling and minimum risk sampling have been utilized to select the most informative sample in active learning. Although many active learning algorithms have been proposed so far, most of them work with binary or multi-class classification problems and therefore can not be applied to problems in which only samples from one class as well as a set of unlabeled data are available.", "histories": [["v1", "Wed, 24 Feb 2016 13:32:36 GMT  (755kb,D)", "http://arxiv.org/abs/1602.07495v1", "6 pages, presented at IEEE ICDM 2011 Workshops"]], "COMMENTS": "6 pages, presented at IEEE ICDM 2011 Workshops", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alireza ghasemi", "hamid r rabiee", "mohsen fadaee", "mohammad t manzuri", "mohammad h rohban"], "accepted": false, "id": "1602.07495"}, "pdf": {"name": "1602.07495.pdf", "metadata": {"source": "CRF", "title": "Active Learning from Positive and Unlabeled Data", "authors": ["Alireza Ghasemi", "Hamid R. Rabiee", "Mohsen Fadaee", "Mohammad T. Manzuri", "Mohammad H. Rohban"], "emails": [], "sections": [{"heading": null, "text": "This year, it has come to the point where only once will such a process take place, in which the question is to what extent it is a country in which it is not a country but a country in which it is a country, a country in which it is not a country but a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "II. RELATED WORKS", "text": "This is one of the first and most popular methods of active learning in the USA, which is based on the selection of the least self-confident method for querying the least self-confident method for querying data. However, the definition of trust depends on the fact that the sample of data with maximum entropy [11] contains other approaches applied to active learning. [14] For classifiers who are unable to define a similarity measurement of their predictions, the committee-based active learning methods have been proposed, which form a group of different classifiers and measurements of discrepancy between committee members for a data sample [14].In problems where only one class is available, methods of traditional inconsistency cannot work because they require at least two specific pieces of information about their hyperactivity."}, {"heading": "III. THE PROPOSED APPROACH", "text": "In this section, we present the two suggested approaches to learn from positive and unlabeled data."}, {"heading": "A. Expected Margin Sampling", "text": "Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Sampler-Samp"}, {"heading": "B. Entropy Based Active Learning from Positive and Unlabelled Data", "text": "Another method for assessing the uncertainty of data samples in active learning is the well-known entropy method = x (Log). Entropie is a measure calculated for continuous and discrete probability distributions and which measures the uncertainty of the distribution. For a discrete distribution C, entropy is calculated as: H (C) = \u2212 more information bearing a probability distribution, the more uncertainty there is in its values. For active learning tasks, the entropy for the posterior class distribution of each sample of data is calculated (p) -p p p p \u2212 p p p \u2212 p p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p"}, {"heading": "IV. EXPERIMENTS AND RESULTS", "text": "To evaluate the proposed methods, we used different data sets from the real world. In our experiments, a number of data sets from the UCI repository [2] and the Caltech image set were used to evaluate the proposed active learning strategies [8]. CEDD features were extracted from each image for the Caltech images [5]. Table I shows the properties of the data sets used in our evaluations."}, {"heading": "A. Evaluation Criteria and Algorithms", "text": "Because for many problems of learning from positive and blank data, the goal is actually a query task, performance measures from the information gathering field are popular for evaluating single-class learning methods [6]. Therefore, we have used a measurement to query information to evaluate our methods.There are many performance measures used in the information gathering field, the most popular of which are precision and callback [13]. Here, we have used the formula F1 measurement as a performance measure. It is the harmonious means of precision and retrieval that is more meaningful than any of them alone. With this measurement, we punish situations where only one of the precision and callback methods has a high value and force the requirement that both lie within an acceptable range. Our proposed approaches do not use unique characteristics of certain single-class learning algorithms and are independent of the one-class learning methods used. Therefore, any single-class learning algorithm can be used as a basis."}, {"heading": "B. Experiment Setup and Result", "text": "For the experiment setup, initially 200 samples were selected as a pool of unused data, most of which were selected by the proposed active learning rule for the query. Then, from the remaining data samples, half is replaced by the margin ALPUD with the entropy USPS 3. The last two columns show the results of our two proposed methods."}, {"heading": "V. CONCLUSION", "text": "We proposed an active learning algorithm that uses only positive and blank data to select the most insecure samples for active learning. Our approach can use many models of single-class learning as a baseline classifier and density estimation method, and the ideas presented in this paper can also be used in other active learning paradigms. A well-known and principled framework for active learning, for example, is the risk minimization approach, which attempts to find the sample of data that a model with minimal risk provides on the training set. Risk calculation, which includes both negative and positive class information, can be easily adapted to work with positive and blank data by directly calculating the probability of the sample and the expectation approach outlined in this paper."}, {"heading": "ACKNOWLEDGEMENT", "text": "The authors thank the AICTC Research Center for supporting this work."}], "references": [{"title": "Outlier detection by active learning", "author": ["N. Abe", "B. Zadrozny", "J. Langford"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 504\u2013509. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Margin based active learning", "author": ["M.F. Balcan", "A. Broder", "T. Zhang"], "venue": "Learning Theory, pages 35\u201350", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Cedd: Color and edge directivity descriptor: A compact descriptor for image indexing and retrieval", "author": ["S.A. Chatzichristofis", "Y.S. Boutalis"], "venue": "Proceedings of the 6th international conference on Computer vision systems, pages 312\u2013322. Springer-Verlag", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "One-class svm for learning in image retrieval", "author": ["Y. Chen", "X.S. Zhou", "T.S. Huang"], "venue": "Image Processing, 2001. Proceedings. 2001 International Conference on, volume 1, pages 34\u201337. IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Text classification and co-training from positive and unlabeled examples", "author": ["F. Denis", "R. Gilleron", "A. Laurent", "M. Tommasi"], "venue": "Proceedings of the ICML 2003 workshop: the continuum from labeled to unlabeled data, pages 80\u201387. Citeseer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Object class recognition by unsupervised scale-invariant learning", "author": ["R. Fergus", "P. Perona", "A. Zisserman"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Active and semisupervised data domain description", "author": ["N. Grnitz", "M. Kloft", "U. Brefeld"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 407\u2013422", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Generalized manifold-ranking-based image retrieval", "author": ["J. He", "M. Li", "H.J. Zhang", "H. Tong", "C. Zhang"], "venue": "Image Processing, IEEE Transactions on, 15(10):3170\u20133177", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Entropy-based active learning for object recognition", "author": ["A. Holub", "P. Perona", "M.C. Burl"], "venue": "Computer Vision and Pattern Recognition Workshops, 2008. CVPRW\u201908. IEEE Computer Society Conference on, pages 1\u20138. IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "One-class classification with gaussian processes", "author": ["M. Kemmler", "E. Rodner", "J. Denzler"], "venue": "Computer Vision\u2013ACCV 2010, pages 489\u2013500", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "and Ebooks Corporation", "author": ["C.D. Manning", "P. Raghavan", "H. Schutze"], "venue": "Introduction to information retrieval, volume 1. Cambridge University Press Cambridge, UK", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "Technical Report 1648, University of Wisconsin-Madison", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Support vector data description", "author": ["D.M.J. Tax", "R.P.W. Duin"], "venue": "Machine learning, 54(1):45\u201366", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Support vector machine active learning for image retrieval", "author": ["Simon Tong", "Edward Y. Chang"], "venue": "In ACM Multimedia,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Learning from positive and unlabeled examples: A survey", "author": ["B. Zhang", "W. Zuo"], "venue": "Information Processing (ISIP), 2008 International Symposiums on, pages 650\u2013654. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 11, "context": "Some other approaches are selecting the sample which yields a model with minimum risk or the data sample which yields fastest convergence in gradient based methods [14].", "startOffset": 164, "endOffset": 168}, {"referenceID": 14, "context": "Although a wide range methods have been proposed for learning from positive and unlabeled data [17], few efforts have been made to propose active learning methods consistent with this settings.", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "[14] is a comprehensive survey of recent works in this field.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "For example, [16] proposes an active learning approach for SVM which selects for querying the sample which is closest to the separating hyperplane.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "selecting the sample with minimum margin [3] and the data sample with maximum entropy [11] are other approaches which have been applied to active learning problems.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "selecting the sample with minimum margin [3] and the data sample with maximum entropy [11] are other approaches which have been applied to active learning problems.", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "these methods form an ensemble or committee of diverse classifiers and measure uncertainty by the amount of disagreement between committee members\u2019 votes for a data sample [14].", "startOffset": 172, "endOffset": 176}, {"referenceID": 0, "context": "One of the earlier works is [1] which uses active learning for outlier detection.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "However, [1] approaches the problem of one-class learning by a traditional binary classification method.", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "Another method for active learning from positive and unlabeled data has been proposed by [10].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "A more recent approach has been proposed in [9], which tries to apply active learning to the well-known SVDD method.", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "[9] considers likelihood as well as local density of data point to assess their uncertainty.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "The main advantage of [9] is that it considers both selection based on uncertainty of data, and exploring unknown regions of the feature space.", "startOffset": 22, "endOffset": 25}, {"referenceID": 11, "context": "Margin sampling is among the well-known approaches used for uncertainty-based active learning in different application domains [14].", "startOffset": 127, "endOffset": 131}, {"referenceID": 4, "context": "Noting the fact that p(x|+) can be estimated from positive samples in the same way that p(x) is estimated from unlabelled data, and the p(x|\u2212) is un-known in one-class settings, the equation can be reorganized to compute an estimate for p(x|\u2212) [7]:", "startOffset": 244, "endOffset": 247}, {"referenceID": 5, "context": "the proposed active learning strategies[8].", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "For the Caltech images, CEDD features were extracted from each image [5].", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "Since in many problems of learning from positive and unlabeled data the goal is indeed a retrieval task, performance measure from the field of information retrieval are popular for evaluation of one-class learning methods [6].", "startOffset": 222, "endOffset": 225}, {"referenceID": 10, "context": "There are many performance measures used in the context of information retrieval, the most popular among them are precision and recall[13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 12, "context": "In the experiments, we used SVDD [15] as base classifier and kernel density estimation (KDE) as the density estimation method to find likelihood of positive and unlabelled data.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "Although we used SVDD and KDE for the reasons mentioned above, other one-class learning methods (like one-class Gaussian processes [12]) or density estimation methods (like GMM) can be utilized in the algorithm as well.", "startOffset": 131, "endOffset": 135}, {"referenceID": 7, "context": "Random [10] [9] ALPUD with Margin ALPUD with Entropy USPS 3 2.", "startOffset": 7, "endOffset": 11}, {"referenceID": 6, "context": "Random [10] [9] ALPUD with Margin ALPUD with Entropy USPS 3 2.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "As the baseline method, we used random sample selection, as well as the method proposed in [10] and the approach of [9].", "startOffset": 91, "endOffset": 95}, {"referenceID": 6, "context": "As the baseline method, we used random sample selection, as well as the method proposed in [10] and the approach of [9].", "startOffset": 116, "endOffset": 119}], "year": 2016, "abstractText": "During recent years, active learning has evolved into a popular paradigm for utilizing user\u2019s feedback to improve accuracy of learning algorithms. Active learning works by selecting the most informative sample among unlabeled data and querying the label of that point from user. Many different methods such as uncertainty sampling and minimum risk sampling have been utilized to select the most informative sample in active learning. Although many active learning algorithms have been proposed so far, most of them work with binary or multi-class classification problems and therefore can not be applied to problems in which only samples from one class as well as a set of unlabeled data are available. Such problems arise in many real-world situations and are known as the problem of learning from positive and unlabeled data. In this paper we propose an active learning algorithm that can work when only samples of one class as well as a set of unlabelled data are available. Our method works by separately estimating probability desnity of positive and unlabeled points and then computing expected value of informativeness to get rid of a hyper-parameter and have a better measure of informativeness./ Experiments and empirical analysis show promising results compared to other similar methods. Keywords-active learning; one-class learning; learning from positive and unlabelled data; semi-supervised learning;uncertainty sampling", "creator": "LaTeX with hyperref package"}}}