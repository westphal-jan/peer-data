{"id": "1602.07563", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Multilingual Twitter Sentiment Classification: The Role of Human Annotators", "abstract": "What are the limits of automated Twitter sentiment classification? We analyze a large set of manually labeled tweets in different languages, use them as training data, and construct automated classification models. It turns out that the quality of classification models depends much more on the quality and size of training data than on the type of the model trained. Experimental results indicate that there is no statistically significant difference between the performance of the top classification models. We quantify the quality of training data by applying various annotator agreement measures, and identify the weakest points of different datasets. We show that the model performance approaches the inter-annotator agreement when the size of the training set is sufficiently large. However, it is crucial to regularly monitor the self- and inter-annotator agreements since this improves the training datasets and consequently the model performance. Finally, we show that there is strong evidence that humans perceive the sentiment classes (negative, neutral, and positive) as ordered.", "histories": [["v1", "Wed, 24 Feb 2016 15:34:22 GMT  (517kb)", "https://arxiv.org/abs/1602.07563v1", null], ["v2", "Thu, 5 May 2016 07:05:52 GMT  (516kb)", "http://arxiv.org/abs/1602.07563v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["igor mozetic", "miha grcar", "jasmina smailovic"], "accepted": false, "id": "1602.07563"}, "pdf": {"name": "1602.07563.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["Igor.Mozetic@ijs.si", "Jasmina.Smailovic@ijs.si"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.07 563v 2 [cs.C L] 5M ay2 01What are the limits of automated Twitter sentiment classification? We analyze a large number of manually labeled tweets in different languages, use them as training data, and construct automated classification models. It turns out that the quality of classification models depends much more on the quality and size of training data than on the type of training data. Experimental results indicate that there is no statistically significant difference between the performance of the top classification models. We quantify the quality of training data by applying various annotator agreement measures and identify the weaknesses of various data sets. We show that the model performance of the Inter-Annotator Agreement approaches when the size of the training set is sufficiently large. However, it is crucial to regularly monitor the self-annotator and inter-annotator agreements, as this improves training data sets and thus model performance. Finally, we show that there are positive, negative, and that there is strong evidence that there are."}, {"heading": "1 Introduction", "text": "It's about the future of the Internet, and it's about the future of the Internet, and it's about the future of the Internet, and it's about the future of the Internet. (...) It's about the future of the Internet. (...) It's about the future of the Internet. (...) It's about the future of the Internet. (...) It's about the future of the Internet. (...) It's about the future of the Internet. (...) It's about the future. (...) It's about the future. (...) It's about the future. (...) It's about the future. (...) It's about the future. (...) It's about the future. (...) It's about the future. (...) It's about the future. (...) It's about the future. (...) It's about the future. (...) It's about the future. (...) It's about the future. (...)"}, {"heading": "2 Results and Discussion", "text": "In this paper, we analyze a large set of sentiment-designated tweets. We assume that a sentiment label takes one of three possible values: negative, neutral or positive. The analysis sheds light on two aspects of the data: the quality of the human caption of the tweets and the performance of the sentiment classification models constructed from the same data. We argue that the performance of a classification model is primarily limited by the quality of the labeled data. This, in turn, can be estimated by the agreement between the human annotators. Evaluation yardsticks generally measure the quality of both human annotations and the quality of the classification models. We report all the results in relation to four selected measures that we consider appropriate for the three rated sensation task (see the details in the Subscale Methods section)."}, {"heading": "2.1 The limits of performance", "text": "In fact, it is the case that most of them are in a position to move into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live."}, {"heading": "2.2 Language datasets analyses", "text": "To mark the 1.6 million tweets in the 13 languages, 83 native speakers were hired, and an estimated 38-month effort was spent on the following classification. Can one reduce the effort and instead focus on more problematic records? For example, it seems that the note of over 200,000 tweets in succession was an exaggeration. Worse, the note of over 250,000 Spanish tweets was largely a waste of effort due to the poor quality of the annotations. We conduct a posthoc analysis of the 13 language files by measuring the performance of the classifiers over time. We emulate the evolution of performance by feeding increasingly large volumes into the classification process. The labeled sentences are ordered according to the time of the tweets, so you can detect potential shifts in themes during the Twitter discussions. At each stage, the labeled amount is increased by 10,000 tweets, and the accumulated consent is used for training and classification purposes."}, {"heading": "2.3 Application datasets analyses", "text": "The purpose of building sentiment classification models is to apply them in specific areas, for example, to monitor elections or predict stock prices; the models are built from labeled data (where the sentiment is given) and applied to unlabeled data (where the sentiment must be predicted in each case); the models are also evaluated on the labeled data (typically by 10-fold cross-validation); and the estimated performance can be extended to the application if the labeled data is representative, i.e., drawn from the same distribution as the application data. In the context of Twitter classification, this means that the labeled tweets must be not only language specifics, but also domain specifics. In the previous subsection, we analyzed the classification data and related to the labeled datasets and related to the application data. The potential improvements can be achieved by providing additional training data by excluding the interotator agreements and by improving quality adjustments."}, {"heading": "3 Conclusions", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "4 Methods", "text": "Ethics DeclarationThe tweets were collected via the public Twitter API and are subject to the Twitter Terms of Use. Human commentators were tasked with the purpose of mood labeling and were aware that their comments would be used to create the mood classification models and to evaluate the commentator's self-agreement and the agreement between the commentators."}, {"heading": "4.1 Datasets", "text": "In this study, we analyze two corpora of data (see Table 1). The first corpus is a collection of tweets in 13 European languages posted between April 2013 and February 2015. Tweets, other than English, were collected as part of a joint project with Gama System (http: / / www.gama-system.si), using its PerceptionAnalytics platform (http: / / www.perceptionanalytics.net). Tweets were collected through the Twitter Search API by specifying the geolocations of the largest cities. For English tweets, we used the Twitter Streaming API (a random sample of 1% of all public tweets) and filtered out the English postings.83 native speakers (excluding English) were busy manually tagging the collected tweets. The annotation process was supported by the Stieglitz platform (provided by Sowa Labs, http: / www.sowalabs.com)."}, {"heading": "4.2 Evaluation measures", "text": "One of the main ideas of this paper is to use the same metrics to estimate the concordance between human commentators as well as the concordance between the results of automated classification and the \"random principle.\" There are different metrics of concordance, and to obtain robust estimates, we apply four well-known metrics from the fields of interfactional concordance and machine learning (e.g., nominal, ordered, interval, etc.) Alpha is defined as follows: Alpha = 1 \u2212 DoDe, where Do is the observed discordance between the annotators, and De is a discordance expected by chance. If annotators fully agree, Alpha = 1, and if the level of concordance corresponds to coincidence, Alpha = 0. Do is the observed discordance between the annotators, and De is a discordance expected by chance."}, {"heading": "4.3 The annotator agreements", "text": "Table 4 shows the results of the annotator agreements with respect to the four benchmarks. Self-agreement is calculated from tweets commented twice by the same annotator and tweets commented twice by two different annotators, as far as possible. 95% confidence intervals for Alpha are calculated from 1,000 bootstrap samples. Note that the Albanian and Spanish datasets have very low alpha match values. All results for Alpha reported here and throughout the paper refer to the alpha instance for the reasons outlined in the next subsection."}, {"heading": "4.4 Ordering of sentiment values", "text": "Should the sentiment classes be treated negative (\u2212), neutral (\u2212 0) and positive (+) as nominal (categorical, disordered) or ordered? One can use the sentiment measures to estimate how the three classes are perceived by human commentators. Firstly, we can compare the matches with respect to two alpha variants: alpha-int (interval) and alphanome (nominal). The difference between the two measures is that alpha-int assigns extreme discrepancies to four times higher costs (between the negative and positive classes) than alphanome. A measurement that yields higher matches indicates the nature of the sentiment class, which is perceived by humans as neutral. The results in Table 5, column two, show that alpha-int always produces higher matches than alphanome, with the exception of Spanish. We calculate the average relative matches by showing the Albanian quality and the Spanish datasets (13) as bad ones."}, {"heading": "4.5 Related sentiment classification approaches", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "4.6 Classification models performance", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think they are able to survive themselves. \""}, {"heading": "4.7 The Friedman-Nemenyi test", "text": "Are there significant differences between the six classifiers in terms of their performance? Results depend on the rating metric used, but in general the top classifiers are indistinguishable. A standardized statistical method for testing the significant differences between multiple classifiers [44] is the well-known ANOVA and its non-parametric counterpart, the Friedman Test. The Friedman Test then compares the classifiers for each data set separately. The most powerful classifier gets rank 1, the second best rank 2, etc. If there are rankings, average rankings are assigned. The Friedman Test then compares the average rankings of the classifiers. The zero hypothesis is that all classifiers are equivalent and therefore their rankings should be the same. If the zero hypothesis is rejected, a post-hoc test is used. If you want to compare a control classifier with other classifiers, the planner differs from the other classifiers in each case, however, in each case the classifier differs from the other classifiers."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the EU projects SIMPOL (No. 610704), MULTIPLEX (No. 317532) and DOLFINS (No. 640772), as well as the Slovenian ARRS program Knowledge Technologies (No. P2-103). We would like to thank Gama System (http: / / www.gama-system.si), which collected the most tweets (except English), and Sowa Labs (http: / / www.sowalabs.com) for providing the Stieglitz platform for comments on sentiment. Special thanks go to Sas o Rutar, who implemented several classification algorithms and evaluation procedures in the LATINO Library for Text Mining (https: / / github.com / latinolib). We would like to thank Mojca Mikac for creating Krippendorff and Dragi Kocev's alpha confidence intervals for helping with the Friedman test Neminolib."}], "references": [{"title": "Sentiment analysis and opinion mining", "author": ["B. Liu"], "venue": "Synthesis Lectures on Human Language Technologies", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Sentiment Analysis: Mining Opinions, Sentiments, and Emotions", "author": ["B. Liu"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Human language reveals a universal positivity bias", "author": ["PS Dodds", "EM Clark", "S Desu", "MR Frank", "AJ Reagan", "JR Williams"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining", "author": ["S Baccianella", "A Esuli", "F. Sebastiani"], "venue": "In: LREC. vol", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Sentiment analysis in Twitter", "author": ["E Mart\u0301\u0131nez-C\u00e1mara", "MT Mart\u0301\u0131n-Valdivia", "LA Urena-L\u00f3pez", "AR. Montejo-R\u00e1ez"], "venue": "Natural Language Engineering", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and Their Combination", "author": ["O Kolchyna", "TTP Souza", "PC Treleaven", "T. Aste"], "venue": "arXiv preprint", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Content Analysis, An Introduction to Its Methodology", "author": ["K. Krippendorff"], "venue": "3rd ed. Thousand Oaks, CA: Sage Publications;", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Emotional dynamics in the age of misinformation", "author": ["F Zollo", "P Kralj Novak", "M Del Vicario", "A Bessi", "I Mozeti\u010d", "A Scala"], "venue": "PLoS ONE", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "The effects of Twitter sentiment on stock price returns", "author": ["G Ranco", "D Aleksovski", "G Caldarelli", "M Gr\u010dar", "I. Mozeti\u010d"], "venue": "PLoS ONE", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Sentiment leaning of influential communities in social networks", "author": ["B Sluban", "J Smailovi\u0107", "S Battiston", "I. Mozeti\u010d"], "venue": "Computational Social Networks", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Sentiment of emojis", "author": ["P Kralj Novak", "J Smailovi\u0107", "B Sluban", "I. Mozeti\u010d"], "venue": "PLoS ONE", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Bootstrapping: A Nonparametric Approach to Statistical Inference", "author": ["Mooney CZ", "Duval RD"], "venue": "Thousand Oaks, CA: Sage Publications;", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1993}, {"title": "The Nature of Statistical Learning Theory", "author": ["Vapnik VN"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1995}, {"title": "Artificial Intelligence: A Modern Approach. 2nd ed", "author": ["S Russell", "P. Norvig"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "A comparison of alternative tests of significance for the problem of m rankings", "author": ["M. Friedman"], "venue": "Annals of Mathematical Statistics", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1940}, {"title": "Distribution-free multiple comparisons", "author": ["Nemenyi PB"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1963}, {"title": "Evaluation datasets for Twitter sentiment analysis: A survey and a new dataset, the STS-Gold. In: 1st Intl. Workshop on Emotion and Sentiment in Social and Expressive Media: Approaches and Perspectives from AI (ESSEM)", "author": ["H Saif", "M Fern\u00e1ndez", "Y He", "H. Alani"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["CN dos Santos", "M. Gatti"], "venue": "Proc. 25th Intl. Conf. on Computational Linguistics (COLING),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Think positive: Towards Twitter sentiment analysis from scratch", "author": ["dos Santos CN"], "venue": "Proc. 8th Intl. Workshop on Semantic Evaluation (SemEval);", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Coooolll: A deep learning system for Twitter sentiment classification", "author": ["D Tang", "F Wei", "B Qin", "T Liu", "M. Zhou"], "venue": "Proc. 8th Intl. Workshop on Semantic Evaluation (SemEval);", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "SemEval-2015 task 11: Sentiment analysis of figurative language in Twitter", "author": ["A Ghosh", "G Li", "T Veale", "P Rosso", "E Shutova", "J Barnden"], "venue": "Proc. 9th Intl. Workshop on Semantic Evaluation (SemEval);", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Using hashtags to capture fine emotion categories from tweets", "author": ["SM Mohammad", "S. Kiritchenko"], "venue": "Computational Intelligence", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Information Retrieval", "author": ["Van Rijsbergen CJ"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1979}, {"title": "Sentiment analysis of short informal texts", "author": ["S Kiritchenko", "X Zhu", "SM. Mohammad"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Evaluation methods for ordinal classification", "author": ["L Gaudette", "N. Japkowicz"], "venue": "Advances in Artificial Intelligence", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Twitter sentiment classification using distant supervision", "author": ["A Go", "R Bhayani", "L. Huang"], "venue": "CS224N Project Report,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Sentiment Uncertainty and Spam in Twitter Streams and Its Implications for General Purpose Realtime Sentiment Analysis", "author": ["N Haldenwang", "O. Vornberger"], "venue": "Proc. Intl. Conf. of the German Society for Computational Linguistics and Language Technology (GSCL);", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Semantic patterns for sentiment analysis of Twitter. In: The Semantic Web (ISWC)", "author": ["H Saif", "Y He", "M Fernandez", "H. Alani"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Sentiment analysis of user-generated Twitter updates using various classification techniques", "author": ["R Parikh", "M. Movassate"], "venue": "CS224N Final Report", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Twitter as a Corpus for Sentiment Analysis and Opinion Mining", "author": ["A Pak", "P. Paroubek"], "venue": "In: LREC. vol", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "If you are happy and you know it.", "author": ["A Asiaee T", "M Tepper", "A Banerjee", "G. Sapiro"], "venue": "tweet. In: Proc. 21st ACM Intl. Conf. on Information and Knowledge Management", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Semantic sentiment analysis of Twitter. In: The Semantic Web (ISWC)", "author": ["H Saif", "Y He", "H. Alani"], "venue": "vol. 7649 of Lecture Notes in Computer Science", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Robust sentiment detection on Twitter from biased and noisy data", "author": ["L Barbosa", "J. Feng"], "venue": "Proc. 23rd Intl. Conf. on Computational Linguistics: Posters", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Target-dependent Twitter sentiment classification", "author": ["L Jiang", "M Yu", "M Zhou", "X Liu", "T. Zhao"], "venue": "Proc. 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Enhanced sentiment learning using Twitter hashtags and smileys", "author": ["D Davidov", "O Tsur", "A. Rappoport"], "venue": "Proc. 23rd Intl. Conf. Computational Linguistics: Posters", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Twitter sentiment analysis: The good the bad and the OMG", "author": ["E Kouloumpis", "T Wilson", "J. Moore"], "venue": "Proc. Intl. Conf. on Weblogs and Social Media (ICWSM). vol", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "A comparison of methods for multiclass Support Vector Machines", "author": ["C Hsu", "C. Lin"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2002}, {"title": "Mining text-enriched heterogeneous information networks", "author": ["M. Gr\u010dar"], "venue": "PhD thesis, Joz\u030cef Stefan International Postgraduate School,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Sentiment analysis in streams of microblogging posts", "author": ["J. Smailovi\u0107"], "venue": "PhD thesis, Joz\u030cef Stefan International Postgraduate School,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Monitoring the Twitter sentiment during the Bulgarian elections", "author": ["J Smailovi\u0107", "J Kranjc", "M Gr\u010dar", "M \u017dnidar\u0161i\u010d", "I. Mozeti\u010d"], "venue": "Proc. IEEE Intl. Conf. on Data Science and Advanced Analytics", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "On the Use of Cross-validation for Time Series Predictor Evaluation", "author": ["Bergmeir C", "Be\u0144\u0131tez JM"], "venue": "Information Sciences", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Delta TFIDF: An improved feature space for sentiment analysis", "author": ["J Martineau", "T. Finin"], "venue": "Proc. 3rd AAAI Intl. Conf. on Weblogs and Social Media (ICWSM);", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "Statistical comparison of classifiers over multiple data sets", "author": ["J. Dem\u0161ar"], "venue": "Journal of Machine Learning Research", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Its goal is to extract opinions, emotions or attitudes towards different objects of interest [1, 2].", "startOffset": 93, "endOffset": 99}, {"referenceID": 1, "context": "Its goal is to extract opinions, emotions or attitudes towards different objects of interest [1, 2].", "startOffset": 93, "endOffset": 99}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Another well-known sentiment lexicon is SentiWordNet [4], constructed semi-automatically for over 100,000 words, but limited to English only.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "given in [5].", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "A more recent overview of the lexicon-based and machine learning methods, and their combination, is in [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "In this section, however, the results are summarized only in terms of Krippendorff\u2019s Alpha-reliability (Alpha) [7], to highlight the main conclusions.", "startOffset": 111, "endOffset": 114}, {"referenceID": 7, "context": "The application datasets are: Facebook(it) - the Facebook comments on conspiracy theories in Italian, to study the emotional dynamics [8], DJIA30 - tweets about the Dow Jones stocks, to analyze the effects of Twitter sentiment on their price movements [9], Environment - tweets about environmental issues, to compare the sentiment leaning of different communities [10], and Emojis - a subset of the tweets from the above 13 language datasets which contain emojis, used to derive the emoji sentiment lexicon [11].", "startOffset": 134, "endOffset": 137}, {"referenceID": 8, "context": "The application datasets are: Facebook(it) - the Facebook comments on conspiracy theories in Italian, to study the emotional dynamics [8], DJIA30 - tweets about the Dow Jones stocks, to analyze the effects of Twitter sentiment on their price movements [9], Environment - tweets about environmental issues, to compare the sentiment leaning of different communities [10], and Emojis - a subset of the tweets from the above 13 language datasets which contain emojis, used to derive the emoji sentiment lexicon [11].", "startOffset": 252, "endOffset": 255}, {"referenceID": 9, "context": "The application datasets are: Facebook(it) - the Facebook comments on conspiracy theories in Italian, to study the emotional dynamics [8], DJIA30 - tweets about the Dow Jones stocks, to analyze the effects of Twitter sentiment on their price movements [9], Environment - tweets about environmental issues, to compare the sentiment leaning of different communities [10], and Emojis - a subset of the tweets from the above 13 language datasets which contain emojis, used to derive the emoji sentiment lexicon [11].", "startOffset": 364, "endOffset": 368}, {"referenceID": 10, "context": "The application datasets are: Facebook(it) - the Facebook comments on conspiracy theories in Italian, to study the emotional dynamics [8], DJIA30 - tweets about the Dow Jones stocks, to analyze the effects of Twitter sentiment on their price movements [9], Environment - tweets about environmental issues, to compare the sentiment leaning of different communities [10], and Emojis - a subset of the tweets from the above 13 language datasets which contain emojis, used to derive the emoji sentiment lexicon [11].", "startOffset": 507, "endOffset": 511}, {"referenceID": 11, "context": "The confidence intervals for the agreements are estimated by bootstrapping [12].", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "Our classifiers are all based on Support Vector Machines (SVM) [13], and for reference we also constructed a Naive Bayes classifier [14].", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "Our classifiers are all based on Support Vector Machines (SVM) [13], and for reference we also constructed a Naive Bayes classifier [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "When comparing the classifiers\u2019 performance with the Friedman-Nemenyi test [15,16], it turns out that there is no statistically significant difference between most of them (see the Friedman-Nemenyi test subsection in Methods).", "startOffset": 75, "endOffset": 82}, {"referenceID": 15, "context": "When comparing the classifiers\u2019 performance with the Friedman-Nemenyi test [15,16], it turns out that there is no statistically significant difference between most of them (see the Friedman-Nemenyi test subsection in Methods).", "startOffset": 75, "endOffset": 82}, {"referenceID": 10, "context": "Sentiment distribution is captured by the sentiment score which is computed as the mean of a discrete probability distribution\u2014details are in [11].", "startOffset": 142, "endOffset": 146}, {"referenceID": 7, "context": "Facebook(it) [8].", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "DJIA30 [9].", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "Environment [10].", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "Emojis [11].", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "The posts to be labeled multiple times could be based on their \u201cimportance\u201d as measured by their retweet count [10], for example.", "startOffset": 111, "endOffset": 115}, {"referenceID": 5, "context": "In [6], authors already", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "For the languages covered in this study, one can construct a basic sentiment lexicon from the annotated tweets, in the analogy to derivation of the emoji sentiment lexicon [11].", "startOffset": 172, "endOffset": 176}, {"referenceID": 16, "context": "Finally, since the analysis of opinions expressed in social media is an active and evolving research area, we plan to keep up with the newest trends, such as performing entity-based sentiment analysis [17], applying deep learning techniques [18\u201320], analyzing figurative language (e.", "startOffset": 201, "endOffset": 205}, {"referenceID": 17, "context": "Finally, since the analysis of opinions expressed in social media is an active and evolving research area, we plan to keep up with the newest trends, such as performing entity-based sentiment analysis [17], applying deep learning techniques [18\u201320], analyzing figurative language (e.", "startOffset": 241, "endOffset": 248}, {"referenceID": 18, "context": "Finally, since the analysis of opinions expressed in social media is an active and evolving research area, we plan to keep up with the newest trends, such as performing entity-based sentiment analysis [17], applying deep learning techniques [18\u201320], analyzing figurative language (e.", "startOffset": 241, "endOffset": 248}, {"referenceID": 19, "context": "Finally, since the analysis of opinions expressed in social media is an active and evolving research area, we plan to keep up with the newest trends, such as performing entity-based sentiment analysis [17], applying deep learning techniques [18\u201320], analyzing figurative language (e.", "startOffset": 241, "endOffset": 248}, {"referenceID": 20, "context": ", irony or sarcasm) [21], and detecting different types of emotions (e.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": ", joy, sadness or anger) [22].", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "The second corpus of data are four application datasets, used in different application scenarios and already published [8\u201311].", "startOffset": 119, "endOffset": 125}, {"referenceID": 8, "context": "The second corpus of data are four application datasets, used in different application scenarios and already published [8\u201311].", "startOffset": 119, "endOffset": 125}, {"referenceID": 9, "context": "The second corpus of data are four application datasets, used in different application scenarios and already published [8\u201311].", "startOffset": 119, "endOffset": 125}, {"referenceID": 10, "context": "The second corpus of data are four application datasets, used in different application scenarios and already published [8\u201311].", "startOffset": 119, "endOffset": 125}, {"referenceID": 10, "context": "training phase in terms of the sentiment score (the mean of a discrete probability distribution, see [11] for details).", "startOffset": 101, "endOffset": 105}, {"referenceID": 6, "context": "Krippendorff\u2019s Alpha-reliability (Alpha) [7] is a generalization of several specialized agreement measures.", "startOffset": 41, "endOffset": 44}, {"referenceID": 11, "context": "The sampling distribution can be approximated by bootstrapping [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 22, "context": "F score (F1) is an instance of a well-known effectiveness measure in information retrieval [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 23, "context": "We use an instance specifically designed to evaluate the 3-class sentiment classifiers [24].", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "Accuracy within 1 (Acc\u00b11) is a special case of accuracy within n [25].", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": ", emoticons used in the tweets to determine the intended sentiment [26], however, high quality labeling requires engagement of human annotators.", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "[17] describe eight Twitter sentiment datasets and also introduce a new one which contains separate sentiment labels for tweets and entities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Haldenwang and Vornberger [28] present a publicly available collection of Twitter posts, which were labeled not only with the positive or negative sentiment, but also as uncertain or spam.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "[26] employ the keyword-based approach, Naive Bayes, Maximum Entropy, and SVM, and show that the best performing algorithm is Maximum Entropy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The authors in [29] show that Maximum Entropy outperforms Naive Bayes.", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "In contrast, the authors in [30] report that Naive Bayes performs considerably better than Maximum Entropy.", "startOffset": 28, "endOffset": 32}, {"referenceID": 29, "context": "Pak and Paroubek [31] show that Naive Bayes outperforms the SVM and Conditional Random Fields algorithms.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "[32] employ a dictionary learning approach, weighted SVM, k-Nearest-Neighbor, and Naive Bayes\u2014Naive Bayes and its weighted variant are among the best performing algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] employ Naive Bayes for predicting sentiment in tweets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "For example, [34] test several algorithms implemented in Weka, and SVM performed best.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "The authors in [6] test the Naive Bayes, Decision Trees, and SVM algorithms, and find that the best performing algorithm is SVM.", "startOffset": 15, "endOffset": 18}, {"referenceID": 23, "context": "Preliminary results reported in [24] show that linear SVM yields better performance than the Maximum Entropy classifier.", "startOffset": 32, "endOffset": 36}, {"referenceID": 33, "context": "[35] employ SVM models for subjectivity and polarity classification of Twitter posts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[36] employ k-Nearest-Neighbor.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[37] employ AdaBoost.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Recently, researchers also applied deep learning for Twitter sentiment classification [18\u201320].", "startOffset": 86, "endOffset": 93}, {"referenceID": 18, "context": "Recently, researchers also applied deep learning for Twitter sentiment classification [18\u201320].", "startOffset": 86, "endOffset": 93}, {"referenceID": 19, "context": "Recently, researchers also applied deep learning for Twitter sentiment classification [18\u201320].", "startOffset": 86, "endOffset": 93}, {"referenceID": 12, "context": "Variants of Support Vector Machine (SVM) [13] are often used, because they are well suited for large-scale text categorization tasks, are robust on large feature spaces, and perform well.", "startOffset": 41, "endOffset": 45}, {"referenceID": 36, "context": "A binary SVM can be extended into multi-class and regression classifiers [38].", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "All the SVM algorithms, and several others, including Naive Bayes [14], are implemented in the open-source LATINO library [39] (a light-weight set of software components for building text mining applications, available at https://github.", "startOffset": 66, "endOffset": 70}, {"referenceID": 37, "context": "All the SVM algorithms, and several others, including Naive Bayes [14], are implemented in the open-source LATINO library [39] (a light-weight set of software components for building text mining applications, available at https://github.", "startOffset": 122, "endOffset": 126}, {"referenceID": 38, "context": "Various realizations of \u201ctoo close\u201d are described in [40, 41].", "startOffset": 53, "endOffset": 61}, {"referenceID": 39, "context": "Various realizations of \u201ctoo close\u201d are described in [40, 41].", "startOffset": 53, "endOffset": 61}, {"referenceID": 24, "context": "TwoPlaneSVM assumes the ordering of sentiment classes and implements ordinal classification [25].", "startOffset": 92, "endOffset": 96}, {"referenceID": 40, "context": "With time-ordered data, as is the Twitter stream, one should also consider blocked form of cross-validation [42], where there is no randomization, and each fold is a block of consecutive tweets.", "startOffset": 108, "endOffset": 112}, {"referenceID": 41, "context": "The feature vectors are constructed by the Delta TF-IDF weighting scheme [43].", "startOffset": 73, "endOffset": 77}, {"referenceID": 42, "context": "A standard statistical method for testing the significant differences between multiple classifiers [44] is the well-known ANOVA and its non-parametric counterpart, the Friedman test [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "A standard statistical method for testing the significant differences between multiple classifiers [44] is the well-known ANOVA and its non-parametric counterpart, the Friedman test [15].", "startOffset": 182, "endOffset": 186}, {"referenceID": 15, "context": "In our case, however, all the classifiers are compared to each other, and the weaker Nemenyi test [16] is used.", "startOffset": 98, "endOffset": 102}], "year": 2016, "abstractText": "What are the limits of automated Twitter sentiment classification? We analyze a large set of manually labeled tweets in different languages, use them as training data, and construct automated classification models. It turns out that the quality of classification models depends much more on the quality and size of training data than on the type of the model trained. Experimental results indicate that there is no statistically significant difference between the performance of the top classification models. We quantify the quality of training data by applying various annotator agreement measures, and identify the weakest points of different datasets. We show that the model performance approaches the inter-annotator agreement when the size of the training set is sufficiently large. However, it is crucial to regularly monitor the selfand inter-annotator agreements since this improves the training datasets and consequently the model performance. Finally, we show that there is strong evidence that humans perceive the sentiment classes (negative, neutral, and positive) as ordered.", "creator": "LaTeX with hyperref package"}}}