{"id": "1602.07565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Stochastic Shortest Path with Energy Constraints in POMDPs", "abstract": "We consider partially observable Markov decision processes (POMDPs) with a set of target states and positive integer costs associated with every transition. The traditional optimization objective (stochastic shortest path) asks to minimize the expected total cost until the target set is reached. We extend the traditional framework of POMDPs to model energy consumption, which represents a hard constraint. The energy levels may increase and decrease with transitions, and the hard constraint requires that the energy level must remain positive in all steps till the target is reached. First, we present a novel algorithm for solving POMDPs with energy levels, developing on existing POMDP solvers and using RTDP as its main method. Our second contribution is related to policy representation. For larger POMDP instances the policies computed by existing solvers are too large to be understandable. We present an automated procedure based on machine learning techniques that automatically extracts important decisions of the policy allowing us to compute succinct human readable policies. Finally, we show experimentally that our algorithm performs well and computes succinct policies on a number of POMDP instances from the literature that were naturally enhanced with energy levels.", "histories": [["v1", "Wed, 24 Feb 2016 15:41:22 GMT  (99kb,D)", "https://arxiv.org/abs/1602.07565v1", "Technical report accompanying a paper published in proceedings of AAMAS 2016"], ["v2", "Wed, 11 May 2016 16:26:20 GMT  (86kb,D)", "http://arxiv.org/abs/1602.07565v2", "Technical report accompanying a paper published in proceedings of AAMAS 2016"]], "COMMENTS": "Technical report accompanying a paper published in proceedings of AAMAS 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tom\\'a\\v{s} br\\'azdil", "krishnendu chatterjee", "martin chmel\\'ik", "anchit gupta", "petr novotn\\'y"], "accepted": false, "id": "1602.07565"}, "pdf": {"name": "1602.07565.pdf", "metadata": {"source": "CRF", "title": "Stochastic Shortest Path with Energy Constraints in POMDPs", "authors": ["Tom\u00e1\u0161 Br\u00e1zdil"], "emails": ["xbrazdil@fi.muni.cz", "kchatterjee@ist.ac.at", "mchmelik@ist.ac.at", "anchit@iitb.ac.in", "pnovotny@ist.ac.at"], "sections": [{"heading": null, "text": "We look at partially observable Markov decision-making processes (POMDPs) with a range of target states and positive integer costs associated with each transition. The traditional optimization goal (stochastic shortest path) calls for minimizing the expected total cost to reach the goal. We extend the traditional framework of POMDPs to energy consumption, which is a hard constraint. There are energy levels that can rise and fall with transitions, and the hard constraint requires that the energy level must remain positive in all steps until the goal is reached. Our contribution is twofold. First, we present a novel algorithm for solving POMDPs with energy levels that is developed on existing POMDPs and is used as the main method of dynamic real-time programming. Our second contribution relates to political representation. For larger POMDP instances, the strategies calculated by existing solvers are too large to be comprehensible. We automatically present an automated process based on POMDP learning mechanisms, based on the number of POMDPs, and the automation of POMDPs."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Work", "text": "This year, the time has come for an agreement to be reached in just a few days."}, {"heading": "3 Preliminaries", "text": "We use N0, N, Z to identify the number of non-negative, positive and all whole numbers depicting persons. (For n, N, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S,"}, {"heading": "4 Solving Energy-Reachability", "text": "We will analyze the complexity of the presented algorithms with respect to the accessibility of both types of energy accessibility problems by directly interpreting a product POMDP M into the states by encoding theresource levels in M. Formally, M has a series of states S = S \u00b7 S \u00b7 S \u00b7 S [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap] [cap]]] [cap]] [cap]]]] s, with both types of energy accessibility problems being newly added, and the same set of actions asM \u00b7 A transitional function is defined as follows: for all (s, n)."}, {"heading": "5 Succinct Representation of", "text": "Formally, RTDP-Bel considers only discredited beliefs, i.e. beliefs whose probabilities are rounded to a finite net. Therefore, for technical reasons, RTDP-Bel represents such discredited beliefs as vectors of non-negative integers from an interval [0, B] in which B is a limit determining the precision of approximation. Therefore, any faith in M x can be represented as a vector b-ZS + 1 whose first | S components are integers from [0, B] and whose last component is in [cap]. Considering such a belief b, the true probability of being in a state s with the energy n is (approximately) equal to b / B, where bs is the component corresponding to the state we call A."}, {"heading": "5.1 Decision Trees", "text": "There are numerous ways of concise representation of vectors of numbers (and functions on such sets) in a human readable form. (One of the most popular formalisms suitable for this purpose are decision trees (DT see [44, 38]). We use DTs to represent functions of beliefs in POMDPs. (For convenience, we closely follow the definition of DT used in [10]. (Leave V = {v1,..) A series of variable names is used. (Definition 1.) A decision tree on the set of variables V is a tuple T = (Tr,.), where Tr is a finite rooting of binary (ordered) trees with a series of inner nodes N and a series of leaves L, which assign a predicate of form to each inner node [vi,.V] where we apply it, const. (Z,)."}, {"heading": "5.2 Learning a DT Policy", "text": "The goal is to use a DT as a tool to recognize the most important decisions of decision-making, which is always a decision. We use RTDP-Bel to generate training with the following procedure: \u2022 Calculate a policy with RTDP-Bel, which solves the quantitative energy accessibility problem. \u2022 Let's run a certain number of simulations of DP, each of a fixed length. \"In each step of each simulation, a new training instance is produced (b), where - b is the current belief, - b) the action chosen by the current DP at the current stage is chosen. The above procedure generates a training sequence of the pairs (b1, - b1)., (bk, -) where this sequence is fed into a learning algorithm for DT and the behavior is approximated by the beliefs."}, {"heading": "6 Experimental results", "text": "We have expanded the POMDP file format introduced in [14] with constructs necessary to model resource consumption. We have implemented algorithm 1 which implements the product design of Section 4. We use a modified version of the RTDP-Bel POMDP solver [8] to solve these energy-related POMDPs and generate training data for machine learning tools (see previous section). The algorithm 1Input: POMDP M with energy accessibility objectives output: A sucessive policy approach when there are oneM construction products (M). Section 4 \u03c3r \u2190 RTDP Bel (M-Bel) brings out the traceability objectives: A sucessive policy approach when there are oneM construction products (M). It brings out the traceability objectives (M)."}, {"heading": "6.1 Succinct Policies: Example", "text": "In this part, we will discuss an example of a concise human-readable policy for the field examples example 1. Figure 3 shows a decision tree calculated using the tree package for an instance of a field example that models a robot navigation in a labyrinth. We use variable names x1,..., x8, y1,..., y8, Energy, where the values xn, yn represent the probability that the robot's X or Y coordinate is equal to n (we have B = 20, i.e. the value 20 represents probability 1). For better readability, predicates are not contained within nodes, but highlight edges. To execute a policy represented by the tree, the robot looks at each step according to its current belief. If the current resource level is at least 3 (remember that resource level is integer), it executes action 0 (\"move forward\"). Otherwise, the probability that the action tree is preceded by a unanimous one (if the current coordinate is 7)."}, {"heading": "6.2 Discussion on Experimental results.", "text": "We present the results of our approach in Table 1. Each entry contains the following information: (i) the name of the benchmark; (ii) the size of the state space; (iii) the maximum resource level limit; (iv) the size of the product state space S \u00d7 after a pre-process step that removes unreachable states; (v) the value of the policy guidelines that randomly execute all permitted measures; (vi) RTDP bel entries that represent the size of the calculated policy and its corresponding value; (vii) for Weka, RPart, Tree, we present the size of the calculated decision tree and the value of the corresponding policy. The entries labeled with \"-\" had no run that reached the number of target states T, entries in italics do not reach the number of target states T with all runs within the run length of 1000, i.e. the expected cost of the policy could be higher."}, {"heading": "7 Conclusion", "text": "In this paper, we have looked at POMDPs with a range of target states, positive integer costs associated with each transition, and resource levels. We present a novel algorithm for solving POMDPs that has been enhanced with resource levels based on existing POMDP solvers and the RTDP method. We look at three different approaches to obtain concise and human-readable strategies. In two scalable areas from the existing literature, we present concise strategies that perform only slightly worse than the optimal policy, while they are significantly smaller."}], "references": [{"title": "Constrained Markov Decision Processes", "author": ["Eitan Altman"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "A Closer Look at MOMDPs", "author": ["Mauricio Araya-L\u00f3pez", "Vincent Thomas", "Olivier Buffet", "Francois Charpillet"], "venue": "In Tools with Artificial Intelligence (ICTAI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Probabilistic \u03c9-automata", "author": ["Christel Baier", "Marcus Gr\u00f6sser", "Nathalie Bertrand"], "venue": "J. ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Learning to act using real-time dynamic programming", "author": ["Andrew G Barto", "Steven J Bradtke", "Satinder P Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Dynamic Programming and Optimal Control", "author": ["Dimitri P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Probability and Measure", "author": ["Patrick Billingsley"], "venue": "Wiley, 3rd edition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Learning Sorting and Decision Trees with POMDPs", "author": ["Blai Bonet", "H\u00e9ctor Geffner"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Solving POMDPs: RTDP-Bel vs. Point-based Algorithms", "author": ["Blai Bonet", "H\u00e9ctor Geffner"], "venue": "In IJCAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Computing optimal policies for partially observable decision processes using compact representations", "author": ["Craig Boutilier", "David Poole"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Counterexample Explanation by Learning Small Strategies in Markov Decision Processes", "author": ["Tom\u00e1\u0161 Br\u00e1zdil", "Krishnendu Chatterjee", "Martin Chme\u013a\u0131k", "Andreas Fellner", "Jan Kret\u0301\u0131nsk\u00fd"], "venue": "In Proceedings of Computer Aided Verification - 27th International Conference (CAV 2015),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Efficient Controller Synthesis for Consumption Games with Multiple Resource Types", "author": ["Tom\u00e1\u0161 Br\u00e1zdil", "Krishnendu Chatterjee", "Anto\u0144\u0131n Ku\u010dera", "Petr Novotn\u00fd"], "venue": "Computer Aided Verification 2012,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Classification and Regression Trees", "author": ["Leo Breiman", "Jerome H. Friedman", "Charles J. Stone", "Richard A. Olshen"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1984}, {"title": "Multi-Robot Coordinated Decision Making under Mixed Observability through Decentralized Data Fusion", "author": ["Jesus Capit\u00e1n", "Luis Merino", "Anibal Ollero"], "venue": "In Proceedings of the 11th International Conference on Mobile Robots and Competitions (Robotica", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "MOMDPs: a solution for modelling adaptive management problems", "author": ["Iadine Chades", "Josie Carwardine", "Tara Martin", "Samuel Nicol", "R\u00e9gis Sabbadin", "Olivier Buffet"], "venue": "In Twenty-Sixth AAAI Conference on Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Optimal Cost Almost- Sure Reachability in POMDPs", "author": ["Krishnendu Chatterjee", "Martin Chme\u013a\u0131k", "Raghav Gupta", "Ayush Kanodia"], "venue": "In AAAI 2015,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Energy and Mean-Payoff Parity Markov Decision Processes", "author": ["Krishnendu Chatterjee", "Laurent Doyen"], "venue": "In Proceedings of MFCS 2011,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Qualitative Analysis of Partially-Observable Markov Decision Processes", "author": ["Krishnendu Chatterjee", "Laurent Doyen", "Thomas A. Henzinger"], "venue": "In MFCS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "A survey of partial observation stochastic parity games", "author": ["Krishnendu Chatterjee", "Laurent Doyen", "Thomas A Henzinger"], "venue": "Formal Methods in System Design,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Incremental Policy Iteration with Guaranteed Escape from Local Optima in POMDP Planning", "author": ["Marek Grzes", "Pascal Poupart"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Controller Compilation and Compression for Resource Constrained Applications", "author": ["Marek Grzes", "Pascal Poupart", "Jesse Hoey"], "venue": "editors, ADT,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Isomorph-Free Branch and Bound Search for Finite State Controllers", "author": ["Marek Grzes", "Pascal Poupart", "Jesse Hoey"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Policy tree: Adaptive representation for policy gradient", "author": ["Ujjwal Das Gupta", "Erik Talvitie", "Michael Bowling"], "venue": "In Proceedings of the Twenty- Ninth AAAI Conference on Artificial Intelligence, January 25-30,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Simplicity considered fundamental to design for predictability", "author": ["Wolfgang A Halang"], "venue": "In Design of Systems with Predictable Behaviour,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "The WEKA data mining software: an update", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": "ACM SIGKDD,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "An Improved Policy Iteration Algorithm for Partially Observable MDPs", "author": ["Eric A. Hansen"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1997}, {"title": "The power of 10: rules for developing safety-critical code", "author": ["Gerard J. Holzmann"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Piecewise Linear Dynamic Programming for Constrained POMDPs", "author": ["Joshua D. Isom", "Sean P. Meyn", "Richard D. Braatz"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Finding Optimal Observation-based Policies for Constrained POMDPs under the Expected Average Reward Criterion", "author": ["Xiaofeng Jiang", "Hongsheng Xi", "Xiaodong Wang", "Falin Liu"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial intelligence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["Leslie P. Kaelbling", "Michael L. Littman", "Anthony R. Cassandra"], "venue": "Artificial intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1998}, {"title": "Point-Based Value Iteration for Constrained POMDPs", "author": ["Dongho Kim", "Jaesong Lee", "Kee-Eung Kim", "Pascal Poupart"], "venue": "IJCAI/AAAI,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Real-time systems: design principles for distributed embedded applications", "author": ["Hermann Kopetz"], "venue": "Springer Science & Business Media,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Learning policies for partially observable environments: Scaling up", "author": ["Michael L. Littman", "Anthony R. Cassandra", "Leslie P. Kaelbling"], "venue": "In ICML,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1995}, {"title": "Solving POMDPs by searching the space of finite policies", "author": ["Nicolas Meuleau", "Kee-Eung Kim", "Leslie Pack Kaelbling", "Anthony R. Cassandra"], "venue": "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1999}, {"title": "Machine Learning", "author": ["Thomas M. Mitchell"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1997}, {"title": "Power laws, Pareto distributions and Zipf\u2019s law", "author": ["Mark E.J. Newman"], "venue": "Contemporary physics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Planning under Uncertainty for Robotic Tasks with Mixed Observability", "author": ["Sylvie C.W. Ong", "Shao Wei Png", "David Hsu", "Wee Sun Lee"], "venue": "I. J. Robotic Res.,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "The complexity of Markov decision processes", "author": ["Christos H. Papadimitriou", "John N. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1987}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["Joelle Pineau", "Geoff Gordon", "Sebastian Thrun", "Others"], "venue": "In IJCAI,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2003}, {"title": "Induction of decision trees", "author": ["J. Ross Quinlan"], "venue": "Machine Learning,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1986}, {"title": "Programs for Machine Learning", "author": ["J. Ross Quinlan. C"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1993}, {"title": "A Language and Environment for Statistical Computing", "author": ["R R Core Team"], "venue": "R Foundation for Statistical Computing, Vienna,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "tree: Classification and Regression Trees, 2015. R package version 1.0-36", "author": ["Brian Ripley"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Pattern recognition and neural networks", "author": ["Brian D. Ripley"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1996}, {"title": "Optimal scheduling of interactive and noninteractive traffic in telecommunication systems", "author": ["Keith W. Ross", "Bintong Chen"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1988}, {"title": "Markov decision processes with sample path constraints: the communicating case", "author": ["Keith W. Ross", "Ravi Varadarajan"], "venue": "Operations Research,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1989}, {"title": "Multichain Markov Decision Processes with a Sample Path Constraint: A Decomposition", "author": ["Keith W. Ross", "Ravi Varadarajan"], "venue": "Approach. Math. Oper. Res.,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1991}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["Richard D Smallwood", "Edward J. Sondik"], "venue": "Operations Research,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1973}, {"title": "Heuristic search value iteration for POMDPs", "author": ["Trey Smith", "Reid Simmons"], "venue": "In UAI,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2004}, {"title": "The Optimal Control of Partially Observable Markov Processes over the Infinite Horizon: Discounted Costs", "author": ["Edward J. Sondik"], "venue": "Operations Research,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1978}, {"title": "A pointbased POMDP algorithm for robot planning", "author": ["Matthijs T.J. Spaan", "Nikos Vlassis"], "venue": "In ICRA. IEEE,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2004}, {"title": "The Ten Commandments for C Programmers (Annotated Edition)", "author": ["Henry Spencer"], "venue": "https://www. lysator.liu.se/c/ten-commandments.html. Accessed: November", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2016}, {"title": "rpart: Recursive Partitioning and Regression Trees, 2015. R package version 4.1-10", "author": ["Terry Therneau", "Beth Atkinson", "Brian Ripley"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2015}, {"title": "An online algorithm for constrained POMDPs", "author": ["Aditya Undurti", "Jonathan P. How"], "venue": "In ICRA,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2010}, {"title": "A decentralized approach to multi-agent planning in the presence of constraints and uncertainty", "author": ["Aditya Undurti", "Jonathan P. How"], "venue": "In ICRA,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2011}, {"title": "Sample-Based Policy Iteration for Constrained DEC-POMDPs", "author": ["Feng Wu", "Nicholas R. Jennings", "Xiaoping Chen"], "venue": "ECAI, volume 242 of Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2012}], "referenceMentions": [{"referenceID": 29, "context": "One of the main challenges that make the problem difficult is the presence of uncertainty about the state of the agent and its environment [33], caused for instance by the agent\u2019s unreliable sensors.", "startOffset": 139, "endOffset": 143}, {"referenceID": 37, "context": "Each POMDP describes a discrete, typically finitestate system that exhibits both probabilistic and nondeterministic behaviour [54, 41].", "startOffset": 126, "endOffset": 134}, {"referenceID": 47, "context": "cost) over a finite-horizon [52], or over an infinite horizon [55], where the sequence of rewards (resp.", "startOffset": 28, "endOffset": 32}, {"referenceID": 49, "context": "cost) over a finite-horizon [52], or over an infinite horizon [55], where the sequence of rewards (resp.", "startOffset": 62, "endOffset": 66}, {"referenceID": 38, "context": "costs) can be aggregated by considering the discounted reward [42, 53] or the average reward [43, 21], etc.", "startOffset": 62, "endOffset": 70}, {"referenceID": 48, "context": "costs) can be aggregated by considering the discounted reward [42, 53] or the average reward [43, 21], etc.", "startOffset": 62, "endOffset": 70}, {"referenceID": 7, "context": "Particularly relevant from the planning point of view is the indefinite-horizon (or stochastic shortest path) objective [8, 5, 17], which asks to compute a policy that reaches a state from a given set of target states T and minimizes the expected total cost till the target set T is reached, i.", "startOffset": 120, "endOffset": 130}, {"referenceID": 4, "context": "Particularly relevant from the planning point of view is the indefinite-horizon (or stochastic shortest path) objective [8, 5, 17], which asks to compute a policy that reaches a state from a given set of target states T and minimizes the expected total cost till the target set T is reached, i.", "startOffset": 120, "endOffset": 130}, {"referenceID": 14, "context": "Particularly relevant from the planning point of view is the indefinite-horizon (or stochastic shortest path) objective [8, 5, 17], which asks to compute a policy that reaches a state from a given set of target states T and minimizes the expected total cost till the target set T is reached, i.", "startOffset": 120, "endOffset": 130}, {"referenceID": 0, "context": "executionbased constraints was already examined in the setting of perfectly observable MDPs, see [1, 50, 51, 49].", "startOffset": 97, "endOffset": 112}, {"referenceID": 45, "context": "executionbased constraints was already examined in the setting of perfectly observable MDPs, see [1, 50, 51, 49].", "startOffset": 97, "endOffset": 112}, {"referenceID": 46, "context": "executionbased constraints was already examined in the setting of perfectly observable MDPs, see [1, 50, 51, 49].", "startOffset": 97, "endOffset": 112}, {"referenceID": 44, "context": "executionbased constraints was already examined in the setting of perfectly observable MDPs, see [1, 50, 51, 49].", "startOffset": 97, "endOffset": 112}, {"referenceID": 7, "context": "a table [8] or plan graphs [33], which are equivalent to so called finite-memory policies used in verification [20].", "startOffset": 8, "endOffset": 11}, {"referenceID": 29, "context": "a table [8] or plan graphs [33], which are equivalent to so called finite-memory policies used in verification [20].", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "a table [8] or plan graphs [33], which are equivalent to so called finite-memory policies used in verification [20].", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "the size of the POMDP [19].", "startOffset": 22, "endOffset": 26}, {"referenceID": 25, "context": "merous informal rules for safety-critical system design that enforce \u201dsimplicity\u201d and \u201dreadability\u201d [29, 57] as well as in academic treatments of the subject [35, Chapter 2 on \u201dSimplicity\u201d].", "startOffset": 100, "endOffset": 108}, {"referenceID": 51, "context": "merous informal rules for safety-critical system design that enforce \u201dsimplicity\u201d and \u201dreadability\u201d [29, 57] as well as in academic treatments of the subject [35, Chapter 2 on \u201dSimplicity\u201d].", "startOffset": 100, "endOffset": 108}, {"referenceID": 55, "context": "59, 34] a generic framework for enforcing constraints in POMDPs which has received considerable attention in various application domains [61, 31, 60] (see also [1] for related concepts in the setting of perfectly observable MPDs).", "startOffset": 137, "endOffset": 149}, {"referenceID": 27, "context": "59, 34] a generic framework for enforcing constraints in POMDPs which has received considerable attention in various application domains [61, 31, 60] (see also [1] for related concepts in the setting of perfectly observable MPDs).", "startOffset": 137, "endOffset": 149}, {"referenceID": 54, "context": "59, 34] a generic framework for enforcing constraints in POMDPs which has received considerable attention in various application domains [61, 31, 60] (see also [1] for related concepts in the setting of perfectly observable MPDs).", "startOffset": 137, "endOffset": 149}, {"referenceID": 0, "context": "59, 34] a generic framework for enforcing constraints in POMDPs which has received considerable attention in various application domains [61, 31, 60] (see also [1] for related concepts in the setting of perfectly observable MPDs).", "startOffset": 160, "endOffset": 163}, {"referenceID": 7, "context": "As mentioned earlier, we extend the previous work on indefinite-horizon objective [8, 5, 17] by adding energy constraints.", "startOffset": 82, "endOffset": 92}, {"referenceID": 4, "context": "As mentioned earlier, we extend the previous work on indefinite-horizon objective [8, 5, 17] by adding energy constraints.", "startOffset": 82, "endOffset": 92}, {"referenceID": 14, "context": "As mentioned earlier, we extend the previous work on indefinite-horizon objective [8, 5, 17] by adding energy constraints.", "startOffset": 82, "endOffset": 92}, {"referenceID": 15, "context": "Our notion of energy constraints is similar to the one used in verification, in particular to so called energy games and MDPs [16, 18] and consumption games [11], although none of these concepts was considered in a partially observable setting so far.", "startOffset": 126, "endOffset": 134}, {"referenceID": 10, "context": "Our notion of energy constraints is similar to the one used in verification, in particular to so called energy games and MDPs [16, 18] and consumption games [11], although none of these concepts was considered in a partially observable setting so far.", "startOffset": 157, "endOffset": 161}, {"referenceID": 9, "context": "DTs have already been successfully used to represent policies in verification of perfectly observable MDPs modelled in the well-known PRISM tool [10].", "startOffset": 145, "endOffset": 149}, {"referenceID": 8, "context": "For POMDPs, in [9] they consider a situation where the POMDP itself is encoded succinctly using DTs and similar structures, and they use this assumption to design a specific algorithm", "startOffset": 15, "endOffset": 18}, {"referenceID": 29, "context": "1These should not be confused with policy trees that represent a complete behaviour of a POMDP under a fixed policy [33].", "startOffset": 116, "endOffset": 120}, {"referenceID": 6, "context": "In [7] they", "startOffset": 3, "endOffset": 6}, {"referenceID": 21, "context": "DTs were also used to represent policies in a reinforcement-learning setting [25], where the agent has no a priori model of the environment.", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "The need for succinct and efficient representation of policies motivated the study of finite-state controllers (FSCs) in POMDPs [22, 28, 37, 24].", "startOffset": 128, "endOffset": 144}, {"referenceID": 24, "context": "The need for succinct and efficient representation of policies motivated the study of finite-state controllers (FSCs) in POMDPs [22, 28, 37, 24].", "startOffset": 128, "endOffset": 144}, {"referenceID": 33, "context": "The need for succinct and efficient representation of policies motivated the study of finite-state controllers (FSCs) in POMDPs [22, 28, 37, 24].", "startOffset": 128, "endOffset": 144}, {"referenceID": 20, "context": "The need for succinct and efficient representation of policies motivated the study of finite-state controllers (FSCs) in POMDPs [22, 28, 37, 24].", "startOffset": 128, "endOffset": 144}, {"referenceID": 18, "context": "One crucial difference between previous approaches to policy succinctness in both POMDP [22] and other settings [25] is that in previous work they concurrently optimize both the performance of a policy and its size,", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "One crucial difference between previous approaches to policy succinctness in both POMDP [22] and other settings [25] is that in previous work they concurrently optimize both the performance of a policy and its size,", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "which requires dedicated algorithms, while we separate these tasks: first we search for a well-performing, though possibly \u201dugly\u201d policy, and then learn its succinct representation (similar approach was used in [23],", "startOffset": 211, "endOffset": 215}, {"referenceID": 0, "context": "of all functions f : X \u2192 [0, 1] s.", "startOffset": 25, "endOffset": 31}, {"referenceID": 14, "context": "Informally, the probabilistic aspect of the observation function is captured in the transition function, and by enlarging the state space with the product with the observations, we obtain an observation function only on states [17].", "startOffset": 227, "endOffset": 231}, {"referenceID": 5, "context": "The construction of P is standard [6].", "startOffset": 34, "endOffset": 37}, {"referenceID": 36, "context": "In this our model resembles mixedobservability POMDPs [40, 2], and indeed in the next section we will present a transformation of POMDPs with energy constraints into standard POMDPs in which resource levels are a fully observable component of each state.", "startOffset": 54, "endOffset": 61}, {"referenceID": 1, "context": "In this our model resembles mixedobservability POMDPs [40, 2], and indeed in the next section we will present a transformation of POMDPs with energy constraints into standard POMDPs in which resource levels are a fully observable component of each state.", "startOffset": 54, "endOffset": 61}, {"referenceID": 1, "context": "Since, in the words of [2], online techniques cannot be probably adapted to benefit from mixed observability, we stick to standard POMDP formulations.", "startOffset": 23, "endOffset": 26}, {"referenceID": 13, "context": "[15, 13].", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[15, 13].", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "The problem of approximating optimal cost optCost in POMDPs without energy constraints for positive costs was shown to be decidable in [17].", "startOffset": 135, "endOffset": 139}, {"referenceID": 14, "context": "The problem of approximating optimal cost optCost for general costs (positive and negative) was proved to be undecidable in [17] already for POMDPs with-", "startOffset": 124, "endOffset": 128}, {"referenceID": 14, "context": "Algorithm solving the qualitative reachability problem based on belief supports was presented in [17, 3].", "startOffset": 97, "endOffset": 104}, {"referenceID": 2, "context": "Algorithm solving the qualitative reachability problem based on belief supports was presented in [17, 3].", "startOffset": 97, "endOffset": 104}, {"referenceID": 16, "context": "POMDPs [19].", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "To solve the quantitative energy-reachability problem, we again use an algorithm for POMDPs without energy, namely the one from [17], and apply it to M\u00d7.", "startOffset": 128, "endOffset": 132}, {"referenceID": 7, "context": "method, finds a policy of small cost that almost surely reaches T\u00d7, using a modified version of RTDP-Bel [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 3, "context": "RTDP-Bel is an adaptation of the real-time dynamic programming value iteration [4] to POMDPs.", "startOffset": 79, "endOffset": 82}, {"referenceID": 14, "context": "optimal policies on many instances where optimal costs can be computed using exact methods [17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 29, "context": "the probability distribution over the set of states representing the likelihood of being in particular states given the current history of states and observations [33].", "startOffset": 163, "endOffset": 167}, {"referenceID": 39, "context": "One of the most popular formalisms suitable for this purpose are decision trees (DT see [44, 38]).", "startOffset": 88, "endOffset": 96}, {"referenceID": 34, "context": "One of the most popular formalisms suitable for this purpose are decision trees (DT see [44, 38]).", "startOffset": 88, "endOffset": 96}, {"referenceID": 9, "context": "For convenience, we follow closely the definition of DT used in [10].", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "A standard process of learning according to the algorithm ID3 [44, 38] proceeds as follows:", "startOffset": 62, "endOffset": 70}, {"referenceID": 34, "context": "A standard process of learning according to the algorithm ID3 [44, 38] proceeds as follows:", "startOffset": 62, "endOffset": 70}, {"referenceID": 40, "context": "5 algorithm and refer to [45, 38].", "startOffset": 25, "endOffset": 33}, {"referenceID": 34, "context": "5 algorithm and refer to [45, 38].", "startOffset": 25, "endOffset": 33}, {"referenceID": 11, "context": "We also use the CART algorithm [12] with so called Gini index instead of the information gain to select the best splits (there are also differences in pruning).", "startOffset": 31, "endOffset": 35}, {"referenceID": 28, "context": "Figure 1 depicts the Energy-constrained Tiger POMDP which is an extension of the famous Tiger problem introduced in [32].", "startOffset": 116, "endOffset": 120}, {"referenceID": 7, "context": "solver [8] to solve these energy-constrained POMDPs and generate training data for machine learning tools (see the previous section).", "startOffset": 7, "endOffset": 10}, {"referenceID": 23, "context": "In the first scenario decision trees are constructed using the Weka machine learning package [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 40, "context": "5 algorithm [45]; 2.", "startOffset": 12, "endOffset": 16}, {"referenceID": 52, "context": "We use the package rpart [58] from R [46] which implements the CART algorithms of [12].", "startOffset": 25, "endOffset": 29}, {"referenceID": 41, "context": "We use the package rpart [58] from R [46] which implements the CART algorithms of [12].", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "We use the package rpart [58] from R [46] which implements the CART algorithms of [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 52, "context": "We have experimented with tree pruning using complexity parameters (see [58]).", "startOffset": 72, "endOffset": 76}, {"referenceID": 42, "context": "Finally, we constructed trees using the package tree [47] of R which implements algorithms of [48].", "startOffset": 53, "endOffset": 57}, {"referenceID": 43, "context": "Finally, we constructed trees using the package tree [47] of R which implements algorithms of [48].", "startOffset": 94, "endOffset": 98}, {"referenceID": 43, "context": "In this case, the default measure for selecting splits is the deviance (see [48]).", "startOffset": 76, "endOffset": 80}, {"referenceID": 32, "context": "The POMDP examples we considered are the following: (A) We consider the Hallway example from [36, 56, 53, 8].", "startOffset": 93, "endOffset": 108}, {"referenceID": 50, "context": "The POMDP examples we considered are the following: (A) We consider the Hallway example from [36, 56, 53, 8].", "startOffset": 93, "endOffset": 108}, {"referenceID": 48, "context": "The POMDP examples we considered are the following: (A) We consider the Hallway example from [36, 56, 53, 8].", "startOffset": 93, "endOffset": 108}, {"referenceID": 7, "context": "The POMDP examples we considered are the following: (A) We consider the Hallway example from [36, 56, 53, 8].", "startOffset": 93, "endOffset": 108}, {"referenceID": 7, "context": "RockSample example from [8, 53].", "startOffset": 24, "endOffset": 31}, {"referenceID": 48, "context": "RockSample example from [8, 53].", "startOffset": 24, "endOffset": 31}, {"referenceID": 2, "context": "RockSample[3,4] 435 7 2403 12.", "startOffset": 10, "endOffset": 15}, {"referenceID": 3, "context": "RockSample[3,4] 435 7 2403 12.", "startOffset": 10, "endOffset": 15}, {"referenceID": 2, "context": "638 RockSample[3,5] 1011 7 5425 12.", "startOffset": 14, "endOffset": 19}, {"referenceID": 4, "context": "638 RockSample[3,5] 1011 7 5425 12.", "startOffset": 14, "endOffset": 19}, {"referenceID": 3, "context": "275 RockSample[4,4] 771 7 4297 14.", "startOffset": 14, "endOffset": 19}, {"referenceID": 3, "context": "275 RockSample[4,4] 771 7 4297 14.", "startOffset": 14, "endOffset": 19}, {"referenceID": 4, "context": "644 RockSample[5,4] 1803 6 5679 16.", "startOffset": 14, "endOffset": 19}, {"referenceID": 3, "context": "644 RockSample[5,4] 1803 6 5679 16.", "startOffset": 14, "endOffset": 19}, {"referenceID": 35, "context": "Interestingly, this suggests that POMDPs with SSP objectives exhibit a phenomenon known as Pareto principle [39], where a small fraction of decisions accounts for majority of optimization effort.", "startOffset": 108, "endOffset": 112}], "year": 2016, "abstractText": "We consider partially observable Markov decision processes (POMDPs) with a set of target states and positive integer costs associated with every transition. The traditional optimization objective (stochastic shortest path) asks to minimize the expected total cost until the target set is reached. We extend the traditional framework of POMDPs to model energy consumption, which represents a hard constraint. There are energy levels that may increase and decrease with transitions, and the hard constraint requires that the energy level must remain positive in all steps till the target is reached. Our contribution is twofold. First, we present a novel algorithm for solving POMDPs with energy levels, developing on existing POMDP solvers and using real-time dynamic programming as its main method. Our second contribution is related to policy representation. For larger POMDP instances the policies computed by existing solvers are too large to be understandable. We present an automated procedure based on machine learning techniques that automatically extracts important decisions of a policy and computes its succinct, human readable representation. Finally, we show experimentally that our algorithm performs well and computes succinct policies on a number of POMDP instances from the literature that were naturally enhanced with energy levels.", "creator": "LaTeX with hyperref package"}}}