{"id": "1602.07570", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Bayesian Exploration: Incentivizing Exploration in Bayesian Games", "abstract": "We consider a ubiquitous scenario in the Internet economy when individual decision-makers (henceforth, agents) both produce and consume information as they make strategic choices in an uncertain environment. This creates a three-way tradeoff between exploration (trying out insufficiently explored alternatives to help others in the future), exploitation (making optimal decisions given the information discovered by other agents), and incentives of the agents (who are myopically interested in exploitation, while preferring the others to explore). We posit a principal who controls the flow of information from agents that came before, and strives to coordinate the agents towards a socially optimal balance between exploration and exploitation, not using any monetary transfers. The goal is to design a recommendation policy for the principal which respects agents' incentives and minimizes a suitable notion of regret.", "histories": [["v1", "Wed, 24 Feb 2016 15:57:28 GMT  (169kb)", "http://arxiv.org/abs/1602.07570v1", null], ["v2", "Wed, 16 Nov 2016 00:53:19 GMT  (199kb)", "http://arxiv.org/abs/1602.07570v2", null]], "reviews": [], "SUBJECTS": "cs.GT cs.DS cs.LG", "authors": ["yishay mansour", "aleksandrs slivkins", "vasilis syrgkanis", "zhiwei steven wu"], "accepted": false, "id": "1602.07570"}, "pdf": {"name": "1602.07570.pdf", "metadata": {"source": "CRF", "title": "Bayesian Exploration: Incentivizing Exploration in Bayesian Games", "authors": ["Yishay Mansour", "Aleksandrs Slivkins", "Vasilis Syrgkanis", "Zhiwei Steven Wu"], "emails": ["mansour@microsoft.com", "slivkins@microsoft.com", "vasy@microsoft.com", "wuzhiwei@cis.upenn.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.07 570v 1 [cs.G T] 24 FeWe consider a pervasive scenario in the Internet economy, where individual decision-makers (henceforth agents) both produce and consume information while making strategic decisions in an uncertain environment, creating a threefold trade-off between exploration (testing insufficiently explored alternatives to help others in the future), exploitation (making optimal decisions in the face of information discovered by other agents) and incentives for agents (who are myopically interested in exploitation while they prefer to explore the others).We establish a principle that controls the flow of information from agents who previously came to agents who arrive later, and strive to coordinate agents toward a socially optimal balance between exploration and exploitation, without transferring money.The goal is to design a policy of rejection of the principles that protect and enforce the incentives."}, {"heading": "1 Introduction", "text": "This year, there is going to be such a process, and the question is to what extent there is going to be such a process until there is such a process."}, {"heading": "2 Bayesian Exploration: our model and results", "text": "Our model, called Bayesian Exploration, is a game between a principal and several agents. It consists of T rounds in which the time horizon T is commonly known. There is a global parameter \u03b80, called the (realized) state. It is assumed by a Bayesian prior distribution that the principal can learn it via a finite state space; it is chosen before the first round and remains the same throughout the game. The state space and the previous action are common knowledge, but the state itself is not revealed to the principal or the agent. (Instead, the principal can learn it over time.) Elements of the action are designated as (feasible) to distinguish them from the realized state. In each round there is a new set of agents that are denounced: = {1,.,. n} who play a simultaneous game. Each agent i selects one action from some possible actions."}, {"heading": "2.1 Statement of the main results", "text": "Given that our benchmark does not have a built-in time horizon, our benchmark will be the optimal reward achieved in each round. (2) It follows that the OPT reward is achieved in each round. (3) It follows that the OPT reward is at least as large as the optimal temporal reward. (4) It follows that the OPT reward is at least as large as the optimal temporal reward we expect. (5) It follows that the OPT reward is at least as large as the optimal temporal reward we expect. (5) It follows that the OPT reward is at least as large as the optimal temporal reward we expect. (5) It follows that the OPT reward is at least as large as the optimal temporal reward we expect."}, {"heading": "3 Warm-up and tools", "text": "As a warm-up, we look at the deterministic tool version and focus on a relatively simple scenario, where an iterative BIC recommendation policy examines all common actions and then exploits them (in the sense that it becomes more precise later). We show that this policy can achieve optimum perround performance once all common actions are examined. Let's remember that our benchmark is OPT (BIC), where BIC is the class of all BIC iterative recommendation actions, and OPT (\u00b7) is defined in (2). Lemma 3.1. Consider Bayesian exploration with deterministic tools. Let's be an iterative BIC recommendation policy that examines all joint actions with a fixed time T\u03c0 T. Then there is a BIC policy that agrees with these pre-round T\u03c0 measures and achieves the expected reward at least with deterministic tools."}, {"heading": "3.1 The recommendation game: a single round of Bayesian Exploration", "text": "We consider a single round of Bayesian exploration as a stand-alone game between the principal and the agents, called a recommendation game. Here, the principal observes an additional \"signal\" that represents the information received in previous rounds (and possibly the internal random seeds), then recommends a joint action, and the agents choose their actions. Formally, the principal views the recommendation game as a version of the Bayesian Persuasion game (Kamenica and Gentzkow, 2011) with multiple agents. Unlike the original Bayesian Persuasion game, in our version, the signal observed by the principal is from (but correlated with) the unknown \"state.\" Game specification of the Bayesian Exploration with a given supply structure, the corresponding recommendation game proceeds as follows: \u2022 the state is drawn by a Bayesian before distribution via a signal; \u2022 the principal observes a signal via a signal; the principal observes a signal by a signal; the principal observes a signal by a signal \u2022 the principal is followed by a signal for each signal; the agent is followed by a signal for each action."}, {"heading": "3.2 Properties of the recommendation game", "text": "In their notation, we use the special case in which the information structure S is empty, and S * is induced by the messaging policy. Then, the \"decision rule\" \u03c0 follows in the theorem of the recommendation rule. In their notation, we use the special case in which the characteristics of a game depend on a certain signal S, we consider multiple signals like the signals and the realized state variables in the same probability space. Such signals are called coupled. While each of these signals corresponds to a separate game instance (with a common signal 0), we refer to all these game instances as a recommendation game with coupled signals.Optimal ones are called coupled."}, {"heading": "3.3 Subroutines and the proof of Lemma 3.1", "text": "We design iterative recommendation policies in a modular way, through \"subroutines\" that span multiple rounds and perform a specific task. Specifically, we need a formal framework to argue that our \"subroutines\" are BIC when viewed separately and together form a BIC policy. We model a \"subroutine\" as an iterative recommendation policy that incorporates the history of previous rounds and chooses its own duration. Formally, we consider a common generalization of Bayesian exploration and the recommendation game: the state is plotted and the signal S is observed exactly as in the recommendation game, and then the game runs over several rounds, just as in Bayesian Exploration. Note that the recommendation game is simply a special case with time. We focus on a time when we observe the horizons (and irrelevant)."}, {"heading": "4 Bayesian Exploration with deterministic utilities", "text": "In this section, we will focus on deterministic tools and prove Theorem 2.2. Specifically, we will construct a BIC iterative recommendation policy \u03c0 whose expected reward fulfills REW (\u03c0) (3). Instead of directly comparing with OPT, we will use an IC intermediate benchmark of the form REW \u0445 [AllInfo (Adet\u03b80)], for some sub-areas BIC maximum measures that depend on the state approach implemented. While we will use an IC covered approach 0 = A to prove Lemma 3.1, this is not a \"fair\" intermediate benchmark, as there may be some joint actions that cannot be explored by a BIC policy. Instead, we will consider only joint actions that can be explored.Definition 4.1 (Explorability). A joint action is referred to as \"ultimately explorable\" because a state approach is given."}, {"heading": "4.1 Explorability in the recommendation game", "text": "We adopt a very permissive definition of \"explorability,\" examine some of its properties, and design a common action that examines all these common actions. We consider a recommendation game with signal S, the support of which is X. Definition 4.4. Let's consider a recommendation game with signal S. A common action a is called signal explorable, for a given realizable signal s, if there is a BIC recommendation policy, so that Pr. [S] s [S] s [S] s [S] s [S] s [S] s [S] s [S] s [S] s [S] s [S] s [S] s [S] s [S] s [S] s [S] s [S] s [S] s] s is a fixed subset of common actions (determined by the feasible signal s), while EX [S] is a random variable signal (actually a 2A-rated signal) whose realization is determined by the realization."}, {"heading": "4.2 A maximally-exploring subroutine", "text": "We start with the \"maximum exploration\" of this signal by calling out the next \"signal.\" \"We start with the\" maximum exploration \"of the signal.\" \"We start with the\" maximum exploration \"of the signal.\" \"We start with the\" maximum exploration \"of the signal.\" \"We start with the\" maximum exploration \"of the signal.\" \"We start with the\" maximum exploration \"of the signal.\" \"We stop after the\" maximum exploration \"of the signal, which (as we prove) cannot guarantee further progress.\" \"The subroutine goes ahead in the phases.\" \"We start with the\" current \"signal exploration.\" \"We start with the\" maximum exploration of the signal. \"\" We start with the \"maximum exploration\" by emitting this signal. \""}, {"heading": "4.3 Putting this all together: proof of Theorem 2.2", "text": "We use the maximum exploratory subroutine RepeatMaxExplore from Section 4.2 (let T0 specify its duration) and an optimal recommendation policy for the signal S = AllInfo (Adet\u03b80), denote it \u03c0. Let us be \u03c0 the composition of the subroutine RepeatMaxExplore followed by T \u2212 T0 copies of \u03c0. By Lemma 4.3 this policy has guaranteed the claimed rewards. Let us argue about the arithmetic implementation of \u03c0. To put it briefly: Polytimecomputable means \"computable in time polynomial in | | | | |\" We must prove that each round of polytime computable is polytime computable. Each round of RepeatMaxExplore is polytime computable in terms of claim 4.8, and each round of \u0432\u043e\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441"}, {"heading": "5 Bayesian Exploration with stochastic utilities", "text": "As we have pointed out in Section 2.1, we are competing against a policy that is against the iterative recommendations of \u03b4 > 0.6. We are constructing a policy whose time-averaged expected reward is close to OPT, where this value is the class of all III-BIC policies. In fact, we are achieving a stronger result. We will try to compete with a benchmark in the deterministic instance: a version of the original problem with deterministic supply systems. Our benchmark is OPTdet, where the IC is the class of all policies that constitute such a policy. \u2212 BIC for the deterministic instance is the value of OPT (\u00b7) to the deterministic instance. This is indeed a stronger benchmark: 6Recall this policy of the chip BIC fulfills a stronger version of the definition 2.1, in which the right side of (1) is."}, {"heading": "5.1 Preliminaries: from BIC to \u03b4-BIC", "text": "We extend some of the mechanisms developed in the previous sections to include the \u03b4-BIC policy. In the interest of space, we sometimes refer to the said machinery instead of spelling out the full details. We start with the terms of \"finally explorable joint action\" and \"maximum explorable sub-task.\" (Note: the former only considers the deterministic instance 5.4 (Explorability). Consider the deterministic instance and the specified sum of all such joint actions as \"ultimately explorable.\" Definition 5.5. A sub-routine is referred to as \"finally explorable\" when a state is explorable. \"(Note: Pr [a = a] for a specific BIC- IC signal)."}, {"heading": "5.2 Approximate deterministic utilities using stochastic utilities", "text": "Let us discuss how we can evaluate the expected utilities on the basis of several samples of stochastic utilities, and how we can use the resulting approximate signal. (In particular, we present an important tool (Lemma 5.10): a fact about approximate signals that we use in this section. (Then the formulation of this discussion in precise terms requires a certain notation. (One, two subset B): a state in which the expected utilities of the common actions are expressed in the form of a tableau B: (f (a, \u03b8),. (One, two subset B): a state in which the expected utilities of the common actions are expressed in the form of a tableau B. (B) (B, U (B, 0)), as defined in (5).Now let the utility appear as a realized state."}, {"heading": "5.3 Approximate signals in a recommendation game", "text": "Let us abstract (15) as a property of two coupled signals and give the corresponding generalization of Lemma 5.10.Definition 5.11. Let us consider a recommendation game with coupled signals S, S, where the two signals have the same support. Signal S, is called \u03b2-approximation for S, \u03b2 > 0, ifPr [S], ifPr [S], ifPr [S], ifPr [S], ifPr [S], ifPr [S], ifPr [S], ifPr [S], ifPr [S], [S], [S], [S] where the randomness about the realization of (S, S), [S), i].Lemma 5.12. Let us consider the definition of Definition 5.11. Let us leave the following technical lemma.Lemma 5.13. Let us consider the definition of 5.11."}, {"heading": "5.4 A \u03b4-maximally-exploring subroutine under stochastic utitlities", "text": "In this subsection, we will provide our maximum exploration structure (with access to stochastic supply systems) for exploration of all common actions (with access to deterministic supply systems) Our BIC sub-function is largely similar to that in Section 4.2, except that our recommendation policy will only have access to approximate signals based on stochastic supply systems. We will first introduce a BIC sub-function that allows us to send an approach signal S to signal S, explores all common actions within the framework of set A. \"In the process, the sub-routine will collect multiple utility samples for each joint action explored, which will allow us to construct a new signal approximation for the AllInfo signal. Algorithm 4 Subroutine MaxExploration (U, S, S, \u03b2, \u03b2): exploration of a new signal structure for signal approximation AllInfo (A)."}, {"heading": "5.5 Putting this all together: proof of Theorem 2.3", "text": "As an intermediate step, we show that if an exploration sub-routine emits an approximate signal for AllInfo, we can then receive approximately optimal rewards. Lemma 5.18. Let us have an optimal BIC recommendation policy for signal S (0.1 / 2) and an approximate maximum research of the sub-routine with duration T\u03c3, which emits an approximation of signal S (to signal S). Let us arrive at an optimal BIC recommendation policy for signal S (0.1 / 2), and it will be the composition of the sub-routine followed by T \u2212 T signal approximation S (S). Then it is BIC as long as we have BIC, as long as we have the BIC signal."}, {"heading": "6 Monotonicity in information: proof of Lemma 3.6 and Lemma 4.5", "text": "In this section, we will consider a coupled signal recommendation game and prove the following problem, which combines the 3.6 and 4.5.Lemma 6.1 (monotonicity in information) =. Let us consider a coupled signal game S, S, so that S is at least as informative as S, X, X. All expectations are about the random choice of (S, S,) and also the internal randomness of recommendation strategies (if applicable). We will use the notion of \"at least as informative\" for the following conclusion: Claim 6.2."}, {"heading": "7 Conclusions and open questions", "text": "We present a model that provides incentives for exploration in Bayesian games and solve the first-order problems in this model: explorability and constant / logarithmic regret. Our strategies are computationally efficient: the runtime per turn is polynomial in input size under a generic game representation. Our results pave the way for future work in several directions. The most immediate direction is computationally: Can we achieve polynomial runtimes per turn if the game has a concise representation? A simultaneous paper (Dughmi and Xu, 2016) examines a similar question for Bayesian belief. In terms of statistical guarantees, one might want to improve the limits of regret, such as reducing the dependence of asymptotic constants on the number of common actions and parameters of the previous year. In an economic sense, it appeals to the heterogeneity of agents by incorporating idiosyncratic signals that can be observed and / provoked by the principles."}, {"heading": "A Approximating the expected utilities: proof of Lemma 5.9", "text": "We will use the Chernoff-Hoeffding barrier, a standard result on the concentration of measured variables. Lemma A.1 (Chernoff-Hoeffding barrier). Let us leave X1,..., Xn i.i.d. random variables with EX [Xi] = \u00b5 and a \u2264 Xi \u2264 b for all i. To show that S \u00b2 is a \u03b2 approximation for S, we will first show that for each specified state, with a probability of at least 1 \u2212 \u03b2 for the realization of the stochastic supply variables, the realizations of S \u00b2 coincide with S. In particular, we will show that the average supply variables U = (ui (a)) i [n], with a probability of 1 \u2212 \u03b2 \u00b2 for the realization of the stochastic supply variables, coincide with S."}], "references": [{"title": "Incentivizing exploration", "author": ["Peter Frazier", "David Kempe", "Jon M. Kleinberg", "Robert Kleinberg"], "venue": null, "citeRegEx": "Frazier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Frazier et al\\.", "year": 2016}, {"title": "Strategic Experimentation with Exponential Ban", "author": ["Keller", "Sven Rady", "Martin Cripps"], "venue": null, "citeRegEx": "Keller et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Keller et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Frazier et al. (2014) and Che and H\u00f6rner (2013) study related, but technically different problems: in the former, the principal can pay the agents, and the latter consider continuous information flow and a continuum of agents (and restrict to two actions and binary rewards).", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Frazier et al. (2014) and Che and H\u00f6rner (2013) study related, but technically different problems: in the former, the principal can pay the agents, and the latter consider continuous information flow and a continuum of agents (and restrict to two actions and binary rewards).", "startOffset": 0, "endOffset": 48}, {"referenceID": 0, "context": "Frazier et al. (2014) and Che and H\u00f6rner (2013) study related, but technically different problems: in the former, the principal can pay the agents, and the latter consider continuous information flow and a continuum of agents (and restrict to two actions and binary rewards). Bayesian Exploration is closely related to three prominent subareas of theoretical economics andmachine learning: multi-armed bandits, design of information structures, and strategic learning in games. We briefly survey these connections below. Multi-armed bandits (Bubeck and Cesa-Bianchi, 2012) is a well-studiedmodel for explorationexploitation tradeoff. Absent the BIC constraint (and assuming the agents always follow the recommendations) Bayesian Exploration reduces to the multi-armed bandit problem with action set A and rewards equal to the principal\u2019s utility. The recommendation game (a single round of Bayesian Exploration) is a version of the Bayesian Persuasion game (Kamenica and Gentzkow, 2011) with multiple agents, where the signal observed by the principal is distinct from, but correlated with, the unknown \u201cstate\u201d. Our analysis of this game contributes to the line of work on Bayesian Persuasion and, more generally, on the design of information structures, see (Bergemann and Morris, 2016; Taneva, 2016) for background and references, and Dughmi and Xu (2016) for a more algorithmic perspective.", "startOffset": 0, "endOffset": 1358}], "year": 2017, "abstractText": "We consider a ubiquitous scenario in the Internet economywhen individual decision-makers (henceforth, agents) both produce and consume information as they make strategic choices in an uncertain environment. This creates a three-way tradeoff between exploration (trying out insufficiently explored alternatives to help others in the future), exploitation (making optimal decisions given the information discovered by other agents), and incentives of the agents (who are myopically interested in exploitation, while preferring the others to explore). We posit a principal who controls the flow of information from agents that came before to the ones that arrive later, and strives to coordinate the agents towards a socially optimal balance between exploration and exploitation, not using any monetary transfers. The goal is to design a recommendation policy for the principal which respects agents\u2019 incentives and minimizes a suitable notion of regret. We extend prior work in this direction to allow the agents to interact with one another in a shared environment: at each time step, multiple agents arrive to play a Bayesian game, receive recommendations, choose their actions, receive their payoffs, and then leave the game forever. The agents now face two sources of uncertainty: the actions of the other agents and the parameters of the uncertain game environment. Our main contribution is to show that the principal can achieve constant regret when the utilities are deterministic (where the constant depends on the prior distribution, but not on the time horizon), and logarithmic regret when the utilities are stochastic. As a key technical tool, we introduce the concept of explorable actions, the actions which some incentive-compatible policy can recommend with non-zero probability. We show how the principal can identify (and explore) all explorable actions, and use the revealed information to perform optimally. In particular, our results significantly improve over the prior work on the special case of a single agent per round, which relies on assumptions to guarantee that all actions are explorable. Interestingly, we do not require the principal\u2019s utility to be aligned with the cumulative utility of the agents; instead, the principal can optimize an arbitrary notion of per-round reward. \u2217Microsoft Research and Tel Aviv University. mansour@microsoft.com \u2020Microsoft Research, New York, NY, USA. slivkins@microsoft.com \u2021Microsoft Research, New York, NY, USA. vasy@microsoft.com \u00a7University of Pennsylvania, Philadelphia, PA, USA. wuzhiwei@cis.upenn.edu", "creator": "LaTeX with hyperref package"}}}