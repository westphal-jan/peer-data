{"id": "1602.07749", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Toward Mention Detection Robustness with Recurrent Neural Networks", "abstract": "One of the key challenges in natural language processing (NLP) is to yield good performance across application domains and languages. In this work, we investigate the robustness of the mention detection systems, one of the fundamental tasks in information extraction, via recurrent neural networks (RNNs). The advantage of RNNs over the traditional approaches is their capacity to capture long ranges of context and implicitly adapt the word embeddings, trained on a large corpus, into a task-specific word representation, but still preserve the original semantic generalization to be helpful across domains. Our systematic evaluation for RNN architectures demonstrates that RNNs not only outperform the best reported systems (up to 9\\% relative error reduction) in the general setting but also achieve the state-of-the-art performance in the cross-domain setting for English. Regarding other languages, RNNs are significantly better than the traditional methods on the similar task of named entity recognition for Dutch (up to 22\\% relative error reduction).", "histories": [["v1", "Wed, 24 Feb 2016 23:14:01 GMT  (44kb)", "http://arxiv.org/abs/1602.07749v1", "13 pages, 11 tables, 3 figures"]], "COMMENTS": "13 pages, 11 tables, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thien huu nguyen", "avirup sil", "georgiana dinu", "radu florian"], "accepted": false, "id": "1602.07749"}, "pdf": {"name": "1602.07749.pdf", "metadata": {"source": "CRF", "title": "Toward Mention Detection Robustness with Recurrent Neural Networks", "authors": ["Thien Huu Nguyen", "Avirup Sil", "Georgiana Dinu", "Radu Florian"], "emails": ["thien@cs.nyu.edu,{avi,gdinu,raduf}.us.ibm.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.07 749v 1 [cs.C L] 24 Feb 2016"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to go to another world, to get lost in another world, to get lost in another world, to get into another world."}, {"heading": "2 Related Work", "text": "Both mentioned entity recognition (Bikel et al., 1997; Borthwick et al., 1997; Tjong Kim Sang and De Meulder, 2003; Florian et al., 2003; Miller et al., 2004; Ando and Zhang, 2005; Suzuki and Isozaki, 2008; Ratinov and Roth, 2009; Lin and Wu, 2009; Turian et al., 2010; Ritter et al., 2011; Passos et al., 2014; Cherry and Guo, 2015) and mentioning tasks (Florian et al., 2004) have been extensively studied with various assessments in recent decades: MUC6, MUC7, CoNLL '02, CoNLL' 03 and ACE. Previous work on MD has examined the cascade models (Florian et al., 2006), transferring knowledge from resource-poor languages to machine translation."}, {"heading": "3 Models", "text": "In view of a sentence X = w1w2... wn, where wi is the i word and n is the length of the sentence, we want to predict the caption sequence Y = y1y2... yn for X, where yi is the label for wi. Labels yi follow the BIO2 encoding to capture the entities mentioned in X (Ratinov and Roth, 2009). To prepare the sentence for RNNNs, we first transform each word wi into a real vector using the concatenation of two vectors ei and fi: wi = [ei, fi] 2, where: ei is the word embedding the vector of wi, obtained by forming a language model on a large corpus (which will be discussed later). \u2022 fi is a binary vector that includes different characteristics for wi. In this work, we use four types of word markup: this word label, which is signed by the sayman."}, {"heading": "3.1 The Basic Models", "text": "In standard recursive neural networks, there are three main vectors at each step (word position in the sentence): the input vector xi-RI, the hidden vector hi-RH, and the output vector oi-RO (I, H, and O are the dimensions of the input vectors, the dimension of the hidden vectors, and the number of possible labels for each word).The output vector oi is the probable distribution over the possible labels for the word xi and is obtained from hi via the Softmax function."}, {"heading": "3.2 Gated Recurrent Units", "text": "The hidden units in the two basic models above are essentially the standard feed nets, which take the vectors hi \u2212 1, oi \u2212 1 and xi as inputs and perform a linear transformation followed by nonlinearity to generate the hidden vector hi. ELMAN and JORDAN models are then essentially a stack of these hidden units. Unfortunately, this staking mechanism causes the so-called \"vanishing gradients\" and \"exploding gradients\" problems (Bengio et al., 1994), which makes it difficult to train the networks properly in practice (Pascanu et al., 2012). These problems are addressed by the long-term storage units (LSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2009), which propose the idea of four-door storage cells to allow information storage and access over a long period of time."}, {"heading": "3.3 The Extended Networks", "text": "One of the limitations of the four basic models presented above is their inability to include the future context information that could be critical to predicting in the current step. Consider, for example, the first word \"Liverpool\" in the following sentence: Liverpool suffered an angry first home defeat of the season, beaten 1-0 by a goal by Guy Whittingham for Sheffield Wednesday. In this case, the correct designation ORGANIZATION can only be recognized if we first go through the entire sentence and then use the context words \"Liverpool\" to make his label decision. The limitation of the four models stems from their mechanism of making a single pass from left to right and making the prediction for one word the first time they encounter it. Below, we examine two different networks to overcome this limitation."}, {"heading": "3.3.1 The Contextual Networks", "text": "The contextual networks are motivated by the RNN encoder decoder models, which have recently become very popular in neural machine translation (Cho et al., 2014a; Bahdanau et al., 2015). In these networks, we first run an RNN Re over the entire sentence X = x1x2.... xn to collect the hidden vector sequence c1, c2,..., cn, where ci is the hidden vector for the i-th step in the sentence. For convenience, this process is referred to as Re (x1x2.. xn) = c1, c2,.., cn, the last hidden vector cn being then considered as a distributed representation of X encoding the global context or topic information for X (the coding phrase), and thus possibly helpful for predicting the label of each word in X. Consequently, we run the second RNN Rd over X to encode the sentence (which can be used as an additional code in the coding unit)."}, {"heading": "3.3.2 The Bidirectional Networks", "text": "The bidirectional networks include three passes over the set, in which the first two passes are intended to encode the set, while the third pass is responsible for the decoding.The procedure for the set X = x1x2.. xn is below: (i) Run the first RNN Ref from left to right via x1x2... xn to get the first hidden vector or output vector sequence (ii) Run the second RNN Reb from right to left via x1x2.. xn) = ln (forward encoding). (ii) Run the second RNN Reb from right to left via x1x2. xn to get the second hidden vector or output vector: Reb (xnxn \u2212 1. x1) = rn, rn \u2212 1,., r1."}, {"heading": "3.4 Training and Inference", "text": "In particular, each training example consists of a word xi and its corresponding designation yi in a sentence X = x1x2.. xn (denoted by E = (xi, yi, X). In the encryption phase, we first compute the necessary inputs according to the specific model of interest. This can be the original input vector x1, x2,.., xn in the four basic models or the concatenated vectors \u03b11, \u03b12,.., \u03b1n in the bidirectional models. In the contextual models, we also have the context vector cn. \u2212 Finally, in the decryption phase, a sequence of VD input vectors preceding the current position i is fed into the decoding network in order to obtain the output vector sequence. \u2212 The last vector in this output sequence \u2212 v\u03b1- corresponds to the probability marker distribution for the current position i."}, {"heading": "4 Word Representation", "text": "Following Collobert et al. (2011), we train word embedding from a large corpus and use it to initialize word representation in the models. One of the most advanced mod-5We are trying the AdaDelta algorithm (Lines, 2012) and dropout regularization, but do not see much difference. In Mikolov et al. (2013a; 2013b), it was recently proposed to train word embedding, which introduces two loglinear models, namely the continuous bag-of-words model (CBOW) and the continuous skip-gram model (Skip-gram). The CBOW model attempts to predict the current word based on the average of the contextual word vectors, while the Skip-gram model aims to predict the surrounding words in a sentence that gives the current word. In this work, in addition to the CBOW model and the skip-gram models, we examine word concatenation based on the BOW variant, whereby BOW is a variant of the BOAT."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Dataset", "text": "Our main focus in this work is on assessing the robustness of MD systems across domains and languages. To investigate robustness between domains, we use the 2005 ACE dataset, which contains 6 domains: Broadcast News (bn), Newswire (nw), Broadcast Conversation (bc), Phone Talks (cts), Weblogs (wl), Usenet (un) and 7 entity types: Person, Organization, GPE, Establishment, Weapon, Vehicle. The union of bn and nw is considered to be a single domain, called News. We take half of bc as the only development data and use the remaining data and domains for evaluation. Some statistics on domains are listed in Table 1."}, {"heading": "5.2 Resources and Parameters", "text": "In all experiments for RNNs below, we use the context window vc = 5, the decoding window vd = 9. We find that the optimal number of hidden units (or the dimension of hidden vectors) and the learning rate vary according to the dataset. For the ACE 2005 dataset, we use 200 hidden units with learning rate = 0.01, while these numbers for the CoNLL datasets are 100 and 0.06, respectively. Note that the number of hidden units remains the same in both the encoding phase and the decoding phase.6 http: / / www.cnts.ua.ac.be / conll2003 / ner / 7 http: / / www.cnts.ua.ac.be / conll2002 / ner / For word representation."}, {"heading": "5.3 Model Architecture Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1 Model Architecture Evaluation", "text": "In this section, we evaluate different RNN models by training the models in the message area and reporting on the performance on the development set. As shown in the previous sections, we have 4 basic models M = {ELMAN, JORDAN, ELMAN _ GRU, JORDAN _ GRU}, 8 contextual models (two options for the coding model Re in {ELMAN, ELMAN _ GRU} and 4 options for the decoding model Rd-M), and 16 bidirectional models (4 options for coding and decoding models Re, Rd in M). Performance for the basic models, the contextual models and the bidirectional models are shown in Table 2, Table 3 and Table 4 respectively. There are several important observations from the three tables: -Elman vs Jordan: In the decoding phase, the Elman models are consistently better than the jordinal models, than the contextual models, the table 2 and the contextual models, the models become contextual models and the models in the 3."}, {"heading": "5.3.2 Comparison to other Bidirectional RNN Work", "text": "Mesnil et al. (2013) also present an RNN system with bidirectional modeling for the task of slot filling. As described in Section 3.3.2, the main difference between the bidirectional models in this work and Mesnil el al. (2013) is the repetition of our decoding phase. Table 5 compares the performance of the bidirectional model of Mesnil et al. (2013), called MESNIL, and the BIDIRECT model. In order to verify the effectiveness of reciprocity in decoding, the performance of MESNIL in the decoding phase is compared with the JORDAN _ GRU model (MESNIL + JORDAN _ GRU). Generally, we see that the bidirectional model in this work is much better than the model in Mesnil et al. (2013) for MD."}, {"heading": "5.4 Word Embedding Evaluation", "text": "The section examines the effectiveness of various techniques to learn word embeddings to initialize the RNNs for MD. Table 6 presents the performance of the BASIC, CONTEXT and BIDIRECT models on the development set (trained on messages) when the CBOW, SKIP-GRAM and C-CONCAT techniques are used to obtain word embeddings from the same English corpus. We also report on the performance of the models when they are initialized with the word2vec word embeddings from Mikolov et al. (2013a; 2013b) (trained with the Skip-gram model on 100 billion words from Google News) (WORD2VEC). All of these word embeddings are updated during the training of the RNNNNs to incorporate the task-specific word embeddings from Mikolov et al. (2013a; 2013b) (trained with the Skip-gram model on 100 billion words from Google News (VORDEC)."}, {"heading": "5.5 Comparison to the State-of-the-art", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.5.1 The ACE 2005 Dataset for Mention Detection", "text": "The most advanced systems for mentioning detection on the ACE 2005 dataset were the common extraction system for mentions and relationships of entities of Li and Ji (2014a) and the information networks to unify the results of three information extraction tasks: entity mentions, relationships and events with structured perceptrons by Li et al. (2014b). They comprehensively design a large set of features (parsers, gazettes, word clusters, coreference, etc.) to capture the interdependencies between different tasks. In this section, the RNN systems above are compared with these state-of-the-art systems, we also implement a Maximum Entropy-Markov Model (MEMM) System10, following the description and features in Florian et al. (2004; 2006), and include it in the comparison for the completeness of the system."}, {"heading": "5.5.2 The CoNLL 2003 Dataset for English Named Entity Recognition", "text": "In this section, the RNN systems are evaluated on the basis of the similar task of the Named Entity Recognition for English, in order to compare them with other neural network approaches for completeness. On the CoNLL 2003 dataset for the English NER, the best neural network system to date is Collobert et al. (2011). This system, called CNN Sentence, uses Convolutionary Neural Networks to encode the sentences and then decode them at the sentence level. Table 8 shows the performance of CNN Sentence and our RNN systems on this dataset. As we can see from the table, the RNN systems are equivalent to the CNN Sentence System by Collobert et al. (2011), with the exception of the CONTEXT system, which is worse in this case."}, {"heading": "5.6 Cross-Domain Experiments", "text": "One of the main problems we want to address in this work is robustness between the domains of the MD systems. This section tests the MEMM (baseline) and the RNN systems on the cross-domain settings to get an insight into how they work when the domain changes. In particular, in the first experiment after the previous work of domain adaptation on the ACE dataset 2005 (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2015a) we treat messages as source domains and the other domains: bc, cts, wl and un as target domains. We then examine the systems in two scenarios: (i) the systems are trained and tested on the source domain via 5-fold cross-validation (in-domain performance), and (ii) the systems are distributed on the source domain."}, {"heading": "5.7 Named Entity Recognition for Dutch", "text": "In this section, we will examine the ability of systems to adapt quickly and effectively to a new language. Recent performance of this dataset is due to Carreras et al. (2002) in the CoNLL 2002 evaluation and Nothman et al. (2013). Recently, while preparing this paper, Gillick el al. (2015) introduced a multilingual language processing system and also reported on the performance of this dataset. Table 11 compares the systems. Note that the systems in Gillick el al. (2015) are also based on RNNs and the series with * for Gillick el al. (2015) introduce a multilingual language processing system and record performance on this dataset."}, {"heading": "6 Conclusion", "text": "The comparison between the RNN models and the state-of-the-art systems in the literature reveals the strong potential of the RNN models. In particular, the bidirectional model achieves the best performance in the general environment (up to 9% reduction in relative errors) and exceeds the very strong baseline of feature-based exponential models in the cross-domain environment, demonstrating its robustness across domains. We also show that the RNN models are more portable for new languages as they are significantly better than the best reported systems for the NR in the Netherlands (up to 22% reduction in relative errors). In the future, we plan to apply the bidirectional modeling technique to other tasks and to study the combination of different network architectures and resources to further improve the performance of the systems."}, {"heading": "Acknowledgment", "text": "We would like to thank Ralph Grishman for his valuable suggestions."}], "references": [{"title": "A high-performance semi-supervised learning method for text chunking", "author": ["Ando", "Zhang2005] Rie Ando", "Tong Zhang"], "venue": null, "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "In Journal of Machine Learning Research", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Nymble: a high-performance learning name-finder", "author": ["Scott Miller", "Richard Schwartz", "Ralph Weischedel"], "venue": "ANLP", "citeRegEx": "Bikel et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bikel et al\\.", "year": 1997}, {"title": "Domain adaptation with structural correspondence learning", "author": ["Blitzer et al.2006] John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": null, "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Exploiting diverse knowledge sources via maximum entropy in named entity recognition", "author": ["John Sterling", "Eugene Agichtein", "Ralph Grishman"], "venue": "In Sixth Workshop on Very Large Corpora", "citeRegEx": "Borthwick et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Borthwick et al\\.", "year": 1997}, {"title": "Named entity extraction using adaboost", "author": ["Llu\u00eds M\u00e0rques", "Llu\u00eds Padr\u00f3"], "venue": "In CoNLL", "citeRegEx": "Carreras et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2002}, {"title": "The unreasonable effectiveness of word representations for twitter named entity recognition", "author": ["Cherry", "Guo2015] Colin Cherry", "Hongyu Guo"], "venue": null, "citeRegEx": "Cherry et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cherry et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014a] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Quick introduction to natural language processing with neural networks", "author": ["Kyunghyun Cho"], "venue": "In Lecture at the Ecole Polytechnique de Montreal", "citeRegEx": "Cho.,? \\Q2014\\E", "shortCiteRegEx": "Cho.", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00c3l\u2019on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa"], "venue": "In CoRR", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Boosting named entity recognition with neural character embeddings. In the Fifth Named Entity Workshop, ACL-IJCNLP", "author": ["dos Santos", "Victor Guimar\u00e3es"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Classifying relations by ranking with convolutional neural networks. In ACL-IJCNLP", "author": ["Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "In Cognitive Science", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": null, "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "A statistical model for multilingual entity detection and tracking", "author": ["Florian et al.2004] R Florian", "H Hassan", "A Ittycheriah", "H Jing", "N Kambhatla", "X Luo", "N Nicolov", "S Roukos"], "venue": "In HLT-NAACL", "citeRegEx": "Florian et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2004}, {"title": "Factorizing complex models: A case study in mention detection", "author": ["Florian et al.2006] Radu Florian", "Hongyan Jing", "Nanda Kambhatla", "Imed Zitouni"], "venue": null, "citeRegEx": "Florian et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2006}, {"title": "Improving mention detection robustness to noisy input", "author": ["Florian et al.2010] Radu Florian", "John Pitrelli", "Salim Roukos", "Imed Zitouni"], "venue": null, "citeRegEx": "Florian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2010}, {"title": "Multilingual language processing from bytes", "author": ["Gillick et al.2015] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": "In arXiv preprint arXiv:1512.00103", "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Improved relation extraction with feature-rich compositional embedding models", "author": ["Mo Yu", "Mark Dredze"], "venue": null, "citeRegEx": "Gormley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gormley et al\\.", "year": 2015}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves et al.2009] A. Graves", "Marcus EichenbergerLiwicki", "S. Fernandez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "In IEEE Transactions on Pattern Analysis and Machine", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "In Neural Computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Exploring representation-learning approaches to domain adaptation. In The ACL Workshop on Domain Adaptation for Natural Language Processing (DANLP)", "author": ["Huang", "Yates2010] Fei Huang", "Alexander Yates"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Serial order: A parallel distributed processing approach", "author": ["Michael I. Jordan"], "venue": "In Tech. Rep. No. 8604", "citeRegEx": "Jordan.,? \\Q1986\\E", "shortCiteRegEx": "Jordan.", "year": 1986}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Joint entity and relation extraction using card-pyramid parsing", "author": ["Kate", "Mooney2010] J. Rohit Kate", "Raymond Mooney"], "venue": "In CoNLL", "citeRegEx": "Kate et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kate et al\\.", "year": 2010}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In EMNLP", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Incremental joint extraction of entity mentions and relations", "author": ["Li", "Ji2014a] Qi Li", "Heng Ji"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Constructing information networks using one single model", "author": ["Li et al.2014b] Qi Li", "Heng Ji", "Yu Hong", "Sujian Li"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Phrase clustering for discriminative learning", "author": ["Lin", "Wu2009] Dekang Lin", "Xiaoyun Wu"], "venue": "In ACL-IJCNLP", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Investigation of recurrent neural network architectures and learning methods for spoken language understanding", "author": ["Xiaodong He", "Li Deng", "Yoshua Bengio"], "venue": "In Interspeech", "citeRegEx": "Mesnil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space. In ICLR", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Name tagging with word clusters and discriminative training", "author": ["Miller et al.2004] Scott Miller", "Jethran Guinness", "Alex Zamanian"], "venue": "HLTNAACL", "citeRegEx": "Miller et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2004}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2008] Andriy Mnih", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2008}, {"title": "A focused backpropagation algorithm for temporal pattern recognition", "author": ["Michael C. Mozer"], "venue": "In Complex Systems", "citeRegEx": "Mozer.,? \\Q1989\\E", "shortCiteRegEx": "Mozer.", "year": 1989}, {"title": "Employing word representations and regularization for domain adaptation of relation extraction", "author": ["Nguyen", "Grishman2014] Thien Huu Nguyen", "Ralph Grishman"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "2015b. Event detection and domain adaptation with convolutional neural networks. In ACL-IJCNLP", "author": ["Nguyen", "Grishman2015b] Thien Huu Nguyen", "Ralph Grishman"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Semantic representations for domain adaptation: A case study on the tree kernel-based method for relation extraction", "author": ["Barbara Plank", "Ralph Grishman"], "venue": "ACL-IJCNLP", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Learning multilingual named entity recognition from wikipedia", "author": ["Nothman et al.2013] Joel Nothman", "Nicky Ringland", "Will Radford", "Tara Murphy", "James R Curran"], "venue": null, "citeRegEx": "Nothman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nothman et al\\.", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "In arXiv preprint arXiv:1211.5063", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning", "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Embedding semantic similarity in tree kernels for domain adaptation of relation extraction", "author": ["Plank", "Moschitti2013] Barbara Plank", "Alessandro Moschitti"], "venue": null, "citeRegEx": "Plank et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2013}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In CoNLL", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Named entity recognition in tweets: An experimental study", "author": ["Ritter et al.2011] Alan Ritter", "Sam Clark", "Mausam", "Oren Etzioni"], "venue": null, "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Global inference for entity and relation identification via a linear programming formulation", "author": ["Roth", "Yih2007] D. Roth", "W. Yih"], "venue": null, "citeRegEx": "Roth et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2007}, {"title": "Flors: Fast and simple domain adaptation for part-of-speech tagging", "author": ["Schnabel", "Sch\u00fctze2014] Tobias Schnabel", "Hinrich Sch\u00fctze"], "venue": "In Transactions of the Association of Computational Linguistics", "citeRegEx": "Schnabel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2014}, {"title": "Learning semantic representations using convolutional neural networks for web", "author": ["Shen et al.2014] Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gregoire Mesnil"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data", "author": ["Suzuki", "Isozaki2008] Jun Suzuki", "Hideki Isozaki"], "venue": "In ACL-HLT", "citeRegEx": "Suzuki et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Suzuki et al\\.", "year": 2008}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. In ACL-IJCNLP", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In CoNLL", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Introduction to the conll-2002 shared task: Language-independent named entity recognition", "author": [], "venue": "In CoNLL", "citeRegEx": "Sang.,? \\Q2002\\E", "shortCiteRegEx": "Sang.", "year": 2002}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Domain adaptation for sequence labeling tasks with a probabilistic language adaptation model", "author": ["Xiao", "Guo2013] Min Xiao", "Yuhong Guo"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2013}, {"title": "Recurrent neural networks for language understanding", "author": ["Yao et al.2013] Kaisheng Yao", "Geoffrey Zweig", "MeiYuh Hwang", "Yangyang Shi", "Dong Yu"], "venue": "In Interspeech. Interspeech", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Spoken language understanding using long shortterm memory", "author": ["Yao et al.2014] Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Semantic parsing for single-relation question answering", "author": ["Yih et al.2014] Wen-tau Yih", "Xiaodong He", "Christopher Meek"], "venue": null, "citeRegEx": "Yih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "In CoRR,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Relation classification via convolutional deep neural network. In COLING", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Endto-end learning of semantic role labeling using recurrent neural networks. In ACL-IJCNLP", "author": ["Zhou", "Xu2015] Jie Zhou", "Wei Xu"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Mention detection crossing the language barrier", "author": ["Zitouni", "Florian2008] Imed Zitouni", "Radu Florian"], "venue": null, "citeRegEx": "Zitouni et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zitouni et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 29, "context": "Random Fields (CRFs) (Lafferty et al., 2001).", "startOffset": 21, "endOffset": 44}, {"referenceID": 5, "context": "The problem might originate from various mismatches between the source and the target domains (domain shifts) such as the vocabulary difference, the distribution mismatches etc (Blitzer et al., 2006; Daume, 2007; Plank and Moschitti, 2013).", "startOffset": 177, "endOffset": 239}, {"referenceID": 1, "context": "The recent emerging interest in deep learning has produced many successful applications of RNNs for NLP problems such as machine translation (Cho et al., 2014a; Bahdanau et al., 2015), semantic role labeling (Zhou and Xu, 2015) etc.", "startOffset": 141, "endOffset": 183}, {"referenceID": 17, "context": ", 2014; Cherry and Guo, 2015) and mention detection (Florian et al., 2004) have been extensively studied with various evaluation in the last decades: MUC6, MUC7, CoNLL\u201902, CoNLL\u201903 and ACE.", "startOffset": 52, "endOffset": 74}, {"referenceID": 18, "context": "The previous work on MD has examined the cascade models (Florian et al., 2006), transferred knowledge from rich-resource languages to", "startOffset": 56, "endOffset": 78}, {"referenceID": 19, "context": "low-resource ones via machine translation (Zitouni and Florian, 2008) or improved the systems on noisy input (Florian et al., 2010).", "startOffset": 109, "endOffset": 131}, {"referenceID": 51, "context": "dos Santos and Guimar\u00e3es, 2015b), recurrent/recursive neural networks (Socher et al., 2012; Cho et al., 2014a; Bahdanau et al., 2015; Zhou and Xu, 2015; Tai et al., 2015), to name a few.", "startOffset": 70, "endOffset": 170}, {"referenceID": 1, "context": "dos Santos and Guimar\u00e3es, 2015b), recurrent/recursive neural networks (Socher et al., 2012; Cho et al., 2014a; Bahdanau et al., 2015; Zhou and Xu, 2015; Tai et al., 2015), to name a few.", "startOffset": 70, "endOffset": 170}, {"referenceID": 53, "context": "dos Santos and Guimar\u00e3es, 2015b), recurrent/recursive neural networks (Socher et al., 2012; Cho et al., 2014a; Bahdanau et al., 2015; Zhou and Xu, 2015; Tai et al., 2015), to name a few.", "startOffset": 70, "endOffset": 170}, {"referenceID": 1, "context": ", 2014a; Bahdanau et al., 2015; Zhou and Xu, 2015; Tai et al., 2015), to name a few. For NER, Collobert et al. (2011) propose a CNN-based framework while Mesnil et al.", "startOffset": 9, "endOffset": 118}, {"referenceID": 1, "context": ", 2014a; Bahdanau et al., 2015; Zhou and Xu, 2015; Tai et al., 2015), to name a few. For NER, Collobert et al. (2011) propose a CNN-based framework while Mesnil et al. (2013) and Yao et al.", "startOffset": 9, "endOffset": 175}, {"referenceID": 21, "context": "Recent work has drawn attention to relation extraction (Plank and Moschitti, 2013; Nguyen et al., 2015a; Gormley et al., 2015).", "startOffset": 55, "endOffset": 126}, {"referenceID": 21, "context": ", 2015a; Gormley et al., 2015). In the field of neural networks, to the best of our knowledge, there is only one work from Nguyen and Grishman (2015b) that evaluates CNNs for event detection in the cross-domain setting.", "startOffset": 9, "endOffset": 151}, {"referenceID": 33, "context": "dow of vc for each word in the sentence to capture the short-term dependencies for prediction (Mesnil et al., 2013).", "startOffset": 94, "endOffset": 115}, {"referenceID": 15, "context": "\u2022 In the Elman model (Elman, 1990), called ELMAN, the hidden vector from the previous step hi\u22121, along with the input in the current step xi, constitute the inputs to compute the current hidden state hi:", "startOffset": 21, "endOffset": 34}, {"referenceID": 25, "context": "\u2022 In the Jordan model (Jordan, 1986), called JORDAN, the output vector from the previous step oi\u22121 is fed into the current hidden layer rather than the hidden vector from the previous steps hi\u22121.", "startOffset": 22, "endOffset": 36}, {"referenceID": 3, "context": "\u201cvanishing gradient\u201d and \u201cexploding gradient\u201d problems (Bengio et al., 1994), making it challenging to train the networks properly in practice (Pascanu et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 43, "context": ", 1994), making it challenging to train the networks properly in practice (Pascanu et al., 2012).", "startOffset": 74, "endOffset": 96}, {"referenceID": 22, "context": "These problems are addressed by the long-short term memory units (LSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2009) that propose the idea of memory cells with four gates to allow the information storage and access over a long period of time.", "startOffset": 72, "endOffset": 127}, {"referenceID": 1, "context": "comparable performance in machine translation (Bahdanau et al., 2015).", "startOffset": 46, "endOffset": 69}, {"referenceID": 1, "context": "comparable performance in machine translation (Bahdanau et al., 2015). The introduction of GRUs into the models ELMAN and JORDAN amounts to two new models, named ELMAN_GRU and JORDAN_GRU respectively, with two new methods to compute the hidden vectors hi. The formula for ELMAN_GRU is adopted directly from Cho et al. (2014b) and given below:", "startOffset": 47, "endOffset": 326}, {"referenceID": 1, "context": "The contextual networks are motivated by the RNN Encoder-Decoder models that have become very popular in neural machine translation recently (Cho et al., 2014a; Bahdanau et al., 2015).", "startOffset": 141, "endOffset": 183}, {"referenceID": 33, "context": "The model on the right is from Mesnil et al. (2013) with the forward and backward context size of 1.", "startOffset": 31, "endOffset": 52}, {"referenceID": 33, "context": "We notice that Mesnil et al. (2013) also investigate the bidirectional models for the task of slot filling in spoken language understanding.", "startOffset": 15, "endOffset": 36}, {"referenceID": 33, "context": "We notice that Mesnil et al. (2013) also investigate the bidirectional models for the task of slot filling in spoken language understanding. However, compared to the work presented here, Mesnil et al. (2013) does not use any special transition memory cells (like the GRUs we are employing", "startOffset": 15, "endOffset": 208}, {"referenceID": 43, "context": "in this paper) to avoid numerical stability issues (Pascanu et al., 2012).", "startOffset": 51, "endOffset": 73}, {"referenceID": 38, "context": "The gradients are computed using the back-propagation through time algorithm (Mozer, 1989) and inference is performed by running the networks over the whole sentences and taking argmax over the output sequence: yi = argmax(oi).", "startOffset": 77, "endOffset": 90}, {"referenceID": 11, "context": "Following Collobert et al. (2011), we pre-train word embeddings from a large corpus and em-", "startOffset": 10, "endOffset": 34}, {"referenceID": 61, "context": "We try the AdaDelta algorithm (Zeiler, 2012) and the dropout regularization but do not see much difference.", "startOffset": 30, "endOffset": 44}, {"referenceID": 12, "context": "Tjong Kim Sang and De Meulder, 2003) and compare the performance with the state-ofthe-art neural network system on this dataset (Collobert et al., 2011).", "startOffset": 128, "endOffset": 152}, {"referenceID": 7, "context": "Regarding the robustness across languages, we further evaluate the RNN models on the CoNLL 2002 dataset for Dutch Named Entity Recognition7 (Carreras et al., 2002; Tjong Kim Sang, 2002).", "startOffset": 140, "endOffset": 185}, {"referenceID": 11, "context": "is applied for the English CoNLL dataset to ensure the compatibility with Collobert et al. (2011).", "startOffset": 74, "endOffset": 98}, {"referenceID": 2, "context": "Following Baroni et al. (2014), we use the context window of 5, subsampling set to 1e-05 and negative sampling with the number of instances set to 10.", "startOffset": 10, "endOffset": 31}, {"referenceID": 33, "context": "Table 5 compares the performance of the bidirectional model from Mesnil et al. (2013), called MESNIL, and the BIDIRECT model.", "startOffset": 65, "endOffset": 86}, {"referenceID": 33, "context": "Table 5 compares the performance of the bidirectional model from Mesnil et al. (2013), called MESNIL, and the BIDIRECT model. In order to verify the effectiveness of recurrence in decoding, the performance of MESNIL incorporated with the JORDAN_GRU model in the decoding phase (MESNIL+JORDAN_GRU) is also reported. In general, we see that the bidirectional model in this work is much better than the model in Mesnil et al. (2013) for MD.", "startOffset": 65, "endOffset": 430}, {"referenceID": 33, "context": "Table 5 compares the performance of the bidirectional model from Mesnil et al. (2013), called MESNIL, and the BIDIRECT model. In order to verify the effectiveness of recurrence in decoding, the performance of MESNIL incorporated with the JORDAN_GRU model in the decoding phase (MESNIL+JORDAN_GRU) is also reported. In general, we see that the bidirectional model in this work is much better than the model in Mesnil et al. (2013) for MD. This is significant with Model P R F1 MESNIL (2013) 81.", "startOffset": 65, "endOffset": 490}, {"referenceID": 33, "context": "Table 5: Comparison to Mesnil et al. (2013).", "startOffset": 23, "endOffset": 44}, {"referenceID": 16, "context": "In this section, besides comparing the RNN systems above with these state-of-the-art systems, we also implement a Maximum Entropy Markov Model (MEMM) system10, following the description and features in Florian et al. (2004; 2006), and include it in the comparison for completeness11 . For this comparison, following Li and Ji (2014a), we remove the documents from the two informal domains cts", "startOffset": 202, "endOffset": 334}, {"referenceID": 11, "context": "On the CoNLL 2003 dataset for English NER, the best neural network system so far is Collobert et al. (2011). This system, called CNN-Sentence, em-", "startOffset": 84, "endOffset": 108}, {"referenceID": 11, "context": "As we can see from the table, the RNN systems are on par with the CNN-Sentence system from Collobert et al. (2011) except the CONTEXT system that is worse in this case.", "startOffset": 91, "endOffset": 115}, {"referenceID": 7, "context": "The state-of-the-art performance for this dataset is due to Carreras et al. (2002) in the CoNLL 2002 evaluation and Nothman et al.", "startOffset": 60, "endOffset": 83}, {"referenceID": 7, "context": "The state-of-the-art performance for this dataset is due to Carreras et al. (2002) in the CoNLL 2002 evaluation and Nothman et al. (2013). Very recently, while we were preparing this paper, Gillick el al.", "startOffset": 60, "endOffset": 138}, {"referenceID": 7, "context": "The state-of-the-art performance for this dataset is due to Carreras et al. (2002) in the CoNLL 2002 evaluation and Nothman et al. (2013). Very recently, while we were preparing this paper, Gillick el al. (2015) introduce a multilingual language processing system and also report the performance on this dataset.", "startOffset": 60, "endOffset": 212}, {"referenceID": 42, "context": "05 Nothman et al. (2013) - - 78.", "startOffset": 3, "endOffset": 25}, {"referenceID": 42, "context": "05 Nothman et al. (2013) - - 78.60 Gillick el al. (2015) - - 78.", "startOffset": 3, "endOffset": 57}, {"referenceID": 42, "context": "05 Nothman et al. (2013) - - 78.60 Gillick el al. (2015) - - 78.08 Gillick el al. (2015)* - - 82.", "startOffset": 3, "endOffset": 89}], "year": 2016, "abstractText": "One of the key challenges in natural language processing (NLP) is to yield good performance across application domains and languages. In this work, we investigate the robustness of the mention detection systems, one of the fundamental tasks in information extraction, via recurrent neural networks (RNNs). The advantage of RNNs over the traditional approaches is their capacity to capture long ranges of context and implicitly adapt the word embeddings, trained on a large corpus, into a task-specific word representation, but still preserve the original semantic generalization to be helpful across domains. Our systematic evaluation for RNN architectures demonstrates that RNNs not only outperform the best reported systems (up to 9% relative error reduction) in the general setting but also achieve the state-of-the-art performance in the cross-domain setting for English. Regarding other languages, RNNs are significantly better than the traditional methods on the similar task of named entity recognition for Dutch (up to 22% relative error reduction).", "creator": "LaTeX with hyperref package"}}}