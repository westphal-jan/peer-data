{"id": "1602.07803", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2016", "title": "Automated Word Prediction in Bangla Language Using Stochastic Language Models", "abstract": "Word completion and word prediction are two important phenomena in typing that benefit users who type using keyboard or other similar devices. They can have profound impact on the typing of disable people. Our work is based on word prediction on Bangla sentence by using stochastic, i.e. N-gram language model such as unigram, bigram, trigram, deleted Interpolation and backoff models for auto completing a sentence by predicting a correct word in a sentence which saves time and keystrokes of typing and also reduces misspelling. We use large data corpus of Bangla language of different word types to predict correct word with the accuracy as much as possible. We have found promising results. We hope that our work will impact on the baseline for automated Bangla typing.", "histories": [["v1", "Thu, 25 Feb 2016 05:35:16 GMT  (373kb)", "http://arxiv.org/abs/1602.07803v1", "in International Journal in Foundations of Computer Science &amp; Technology (IJFCST) Vol.5, No.6, November 2015"]], "COMMENTS": "in International Journal in Foundations of Computer Science &amp; Technology (IJFCST) Vol.5, No.6, November 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["md masudul haque", "md tarek habib", "md mokhlesur rahman"], "accepted": false, "id": "1602.07803"}, "pdf": {"name": "1602.07803.pdf", "metadata": {"source": "CRF", "title": "AUTOMATED WORD PREDICTION IN BANGLA LANGUAGE USING STOCHASTIC LANGUAGE MODELS", "authors": ["Md. Masudul Haque", "Tarek Habib", "Mokhlesur Rahman"], "emails": [], "sections": [{"heading": null, "text": "DOI: 10.5121 / ijfcst.2015.5607 67Word completion and word prediction are two important input phenomena that benefit users who type using keyboards or similar devices, and can have profound effects on the typing of disabled people. Our work is based on word predictions of Bangla sentences by using stochastic, i.e. N gram language models such as Unigram, Bigram, Trigram, Deleted Interpolation and Backoff models to automatically complete a sentence by predicting a correct word in a sentence, which saves time and keystrokes while typing and also reduces spelling errors. We use large amounts of Bangla language data of different word types to predict correct words as accurately as possible. We have found promising results. We hope that our work will have an impact on the baseline of automated Bangla typing. KEYWORD prediction, natural language prediction, backpole prediction, gramme, backpole prediction."}, {"heading": "1. INTRODUCTION", "text": "Automatic completion or word completion works in such a way that the user types the first letter or letters of a word and the program provides one or more more more likely words. If the word he wants to type is included in the list, he can select it, for example by using the number of keys. If the word the user wants is not predicted, the user must enter the next letter of the predicted word. At this point, the word selection is modified so that the words that are provided start with the same letters as the ones that were selected or the word that the user wants, it appears selected. The word prediction technique predicts the word by analyzing the previous word flow to complete a sentence with greater accuracy by saving the maximum keystroke of a user or student, and also reducing spelling errors. The N-gram language model is an important technique for predicting the word flow, so as to complete a sentence with greater accuracy by saving the maximum keystroke of a user or student, and also reducing spelling errors."}, {"heading": "2. RELATED WORK", "text": "It is indeed the case that we are able to go in search of a solution that enables us to put ourselves in a situation where we are able, in which we are able to find a solution, in which we are able to find a solution, in which we are able to find a solution, in which we are able to find a solution, in which we are able to find a solution, in which we are able to find a solution, in which we are able to find a solution, in which we are able to find a solution. \""}, {"heading": "4. METHODOLOGY", "text": "Our approach begins with a sentence fragment and yields a word using a stochastic language model as shown in Fig. 2. We use five stochastic language models, namely unicram, bigram, trigram, backoff and deleted interpolation.Conditional frequency distributions come into play when learning language models. A conditional frequency distribution refers to capturing the frequency distribution for the same experiment conducted under different conditions. We intend to predict the text (result) of a word based on the text of the word it follows (context). To predict the results of an experiment, a training corpus is first examined, with the context and result known for each experiment run. When we present a new course of the experiment, the result that () () () () () () () () () () () () () () () () () 1 1 1 1 2 2 2 2 2 2 3 ^ | | P n n) is shown."}, {"heading": "5. IMPLEMENTATION", "text": "Our work begins with a training corpus the size of 0.25 million words. The corpus was predicted by the popular Bangla newspaper for two thirds of the training = 1.0ram = = 1.0ram = = \"Prothom Alo.\" The corpus contains \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "P( | ) = 0.5 \u00d7 P( | ) + 0.33 \u00d7 P( | ) + 0.17 \u00d7", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "P( ) = 0.5 \u00d7 1.0 + 0.33 \u00d7 0.07 + 0.17 \u00d7 0.1E-3", "text": "Test Example No. 2 Test Sentence Fragment: Word to be predicted: For the Unigram model we get the following results.Word prognosticates:"}, {"heading": "P( ) = P( ) \u00d7 P( ) \u00d7 P( ) = 0.0 \u00d7 1.02 E-4 \u00d7 0.01", "text": "For the Bigram model we get the following results.Word predicted: P () = P (|) \u00b7 P (|) \u00b7 P (|) = 0,0 \u00b7 0,0 \u00b7 0,33For the trigram model we get the following results.Word predicted: Empty string because no word in the training corpusFor the backoff model we get the following results.Word predicted: P () = P (|) \u00b7 P (|) \u00b7 P (|) = 0,0 \u00d7 0,0 \u00b7 0,33For the deleted interpolation model we get the following results.Word predicted:"}, {"heading": "P( | ) = 0.5 \u00d7 P( | ) + 0.33 \u00d7 P( | ) + 0.17 \u00d7 P( )", "text": "= 0,5 \u00b7 0,0 + 0,33 \u00b7 0,33 + 0,17 \u00b7 5,1E-5"}, {"heading": "6. RESULT ANALYSIS", "text": "In order to understand the merits of our work in predicting words in Bangla, we need to deepen all the results found. Table I and Fig. 4 show that trigram, backoff and deleted interpolational models have almost the same trend line. Bigram model performs modestly, while the performance of uniform models is obviously very poor. Average accuracies of all models used are in Table II and Fig. 5. We see from Fig. 5 and Table II that the average accuracies of trigram, backoff and deleted interpolation models are close to each other, with the accuracy (63.5%) of the backoff model being the maximum. Some results of predictions of all models used are given hereinafter. The performance of word predictors depends on some important components, such as the language model used, the average length of the sentence in the language, the amount of data sets trained, etc. In our work, we use 5 to 100 words in one trained sentence of 25 sentences."}, {"heading": "4. CONCLUSIONS", "text": "N-gram-based word prediction works well for English, but we use it for the Bangla language, which is more difficult to achieve 100% performance because it depends on the training corpus of large data. We use smoothed corpus data and increase the corpus size in the future to achieve higher performance. In our work we use single word prediction, but we can develop our process to predict a set of words to complete a sentence in a very meaningful way."}], "references": [{"title": "Predicting Sentences using N-Gram Language Models", "author": ["Steffen Bickel", "Peter Haider", "Tobais Scheffer", "(2005"], "venue": "Proceedings of Conference on Empirical Methods in Natural language Processing.  Table II. Results of all models used Model  Average Accuracy Unigram 21.24 Bigram 45.84 Trigram 63.04 Backoff 63.50 Deleted Interpolation 62.86 Fig. 5. Graphs of the results of all models used International Journal in Foundations of Computer Science & Technology (IJFCST) Vol.5, No.6, November 2015 75", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Application of Artificial Intelligence Methods in a Word-Prediction Aid", "author": ["Nestor Garay-Vitoria", "Julio Gonzalez-Abascal", "(2005"], "venue": "Laboratory of Human-Computer Interaction for Special Needs.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 0}, {"title": "Word Prediction via a Clustered Optimal Binary Search Tree", "author": ["Eyas El-Qawasmeh", "(2004"], "venue": "International Arab Journal of Information Technology, Vol. 1, No. 1.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 0}, {"title": "A Learning-Classification Based Approach for Word Prediction", "author": ["Hisham Al-Mubaid"], "venue": "The International Arab Journal of Information Technology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "A Stochastic Prediction Interface for Urdu", "author": ["Qaiser Abbas"], "venue": "Intelligent Systems and Applications ,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Disambiguating Hindi Words Using N- Gram Smoothing Models", "author": ["Umrinder Pal Singh", "Vishal Goyal", "Anisha Rani"], "venue": "International Journal of Engineering Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "N-gram based Statistical Grammar Checker for Bangla and English", "author": ["Jahangir Alam", "Naushad Uzzaman", "Mumit khan"], "venue": "In Proceedings of International Conference on Computer and Information Technology", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Checking the Correctness of Bangla Words using N-Gram", "author": ["Nur Hossain Khan", "Gonesh Chandra Saha", "Bappa Sarker", "Md"], "venue": "Habibur Rahman,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Verification of Bangla Sentence Structure using N-Gram", "author": ["Nur Hossain Khan", "Md. Farukuzzaman Khan", "Md. Mojahidul Islam", "Md. Habibur Rahman", "Bappa Sarker", "(2014"], "venue": "Global Journal of Computer Science and Technology , vol.14 ,issue-1 .", "citeRegEx": "10", "shortCiteRegEx": null, "year": 0}], "referenceMentions": [{"referenceID": 0, "context": "In an analysis of predicting sentences [2] researcher developed a sentence completion method based on N-gram language models and they derived a k best Viterbi beam search decoder for strongly completing a sentence.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "We also observed use of Artificial Intelligence [3] for word prediction.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "Another researcher suggests an approach [4] of word prediction via a Clustered Optimal Binary Search Tree.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "In a paper of a learning classification based approach word prediction [5] they present an effective method of word prediction using machine learning and new feature extraction and selection techniques adapted from Mutual Information (MI) and Chi square (\u03a7 2 ).", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "Some researchers use N-gram language model for word completion in Urdu language [6] and in Hindi language [7] for detecting disambiguation in Hindi word.", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "Some researchers use N-gram language model for word completion in Urdu language [6] and in Hindi language [7] for detecting disambiguation in Hindi word.", "startOffset": 106, "endOffset": 109}, {"referenceID": 6, "context": "There are some related works also on Bangla language using N-gram language model such as grammar checker of Bangla language [8], checking the correctness of Bangla word [9] and verification of Bangla sentence structure [10].", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "There are some related works also on Bangla language using N-gram language model such as grammar checker of Bangla language [8], checking the correctness of Bangla word [9] and verification of Bangla sentence structure [10].", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "There are some related works also on Bangla language using N-gram language model such as grammar checker of Bangla language [8], checking the correctness of Bangla word [9] and verification of Bangla sentence structure [10].", "startOffset": 219, "endOffset": 223}], "year": 2015, "abstractText": "Word completion and word prediction are two important phenomena in typing that benefit users who type using keyboard or other similar devices. They can have profound impact on the typing of disable people. Our work is based on word prediction on Bangla sentence by using stochastic, i.e. N-gram language model such as unigram, bigram, trigram, deleted Interpolation and backoff models for auto completing a sentence by predicting a correct word in a sentence which saves time and keystrokes of typing and also reduces misspelling. We use large data corpus of Bangla language of different word types to predict correct word with the accuracy as much as possible. We have found promising results. We hope that our work will impact on the baseline for automated Bangla typing.", "creator": "PScript5.dll Version 5.2.2"}}}