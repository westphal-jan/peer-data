{"id": "1602.08045", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2016", "title": "PCA/LDA Approach for Text-Independent Speaker Recognition", "abstract": "Various algorithms for text-independent speaker recognition have been developed through the decades, aiming to improve both accuracy and efficiency. This paper presents a novel PCA/LDA-based approach that is faster than traditional statistical model-based methods and achieves competitive results. First, the performance based on only PCA and only LDA is measured; then a mixed model, taking advantages of both methods, is introduced. A subset of the TIMIT corpus composed of 200 male speakers, is used for enrollment, validation and testing. The best results achieve 100%; 96% and 95% classification rate at population level 50; 100 and 200, using 39-dimensional MFCC features with delta and double delta. These results are based on 12-second text-independent speech for training and 4-second data for test. These are comparable to the conventional MFCC-GMM methods, but require significantly less time to train and operate.", "histories": [["v1", "Thu, 25 Feb 2016 19:18:06 GMT  (207kb,D)", "http://arxiv.org/abs/1602.08045v1", "Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series"]], "COMMENTS": "Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["zhenhao ge", "sudhendu r sharma", "mark j t smith"], "accepted": false, "id": "1602.08045"}, "pdf": {"name": "1602.08045.pdf", "metadata": {"source": "CRF", "title": "PCA/LDA Approach for Text-Independent Speaker Recognition", "authors": ["Zhenhao Ge", "Sudhendu R. Sharma", "Mark J.T. Smith"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Speaker Recognition, PCA, LDA, GMM, MFCCs"}, {"heading": "1. INTRODUCTION", "text": "The mentioned for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref) for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the re"}, {"heading": "2. PCA METHOD FOR TEXT-INDEPENDENT SPEAKER RECOGNITION", "text": "In this section, we present a PCA classification method and show how PCA eigenspaces are calculated and data transformed into eigenspaces with reduced dimensions. Subsequently, a PCA classifier based on both Principal Component Space (PCS) and Truncation Error Space (TES) is examined and evaluated."}, {"heading": "2.1 Introduction of PCA", "text": "Principal Component Analysis (PCA) is a standard data analysis technique that transforms the original dataset GM1 (= 1, 2,...) into a K-dimensional, uncorrelated and orthogonal data pattern. The dimension of the transformed dataset D \"{\u03c91,..., \u03c9N} is normally reduced and sorted by the corresponding variance, i.e. the eigenvalue of the data covariance matrix C can be implemented on this dimension. A new eigenspace Uk with a reduced eigendimension k is formed by the k-normalized eigenvector of C, which is associated with the k largest eigendimensions.This process of data transformation or projection from the original space into the eigenspace can be implemented as follows: 1. Summarizing each original sample i, i [1, N] is M-dimensional, find the sample mean values of M\" i. \""}, {"heading": "2.2 PCA-based Classifier", "text": "As mentioned in Section 2.1, the PCA-based classification requires the calculation of own space UK (s) for each class s (s), s (s), s (s), s (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s), S (s (s), S (s), S (s), S (s (s), S (s), S (s), S (s (s), S (s), S (s (s), S (s (s), S (s), S (s (s), S (s), S (s (s (s), S (s (s), S (s), S (s (s), S (s (s), S (s), S (s (s), S (s (s), S (s), S (S (s), S (S (s), S (S (s), S (S (s), S (s), S (S (s), S (S (s), S (s), S (S (S (s), S (S (s), S (S (s), S (s), S (s, S (s, S (s), S (s), S (s, S (s), S ("}, {"heading": "2.3 Performance Evaluation of the PCA-based classifier", "text": "In Table 1, the classification services using PCS and TES are compared and listed individually, and the mixed parameter set w.r.t = {kp, kt, p} is generally better than the other two, while the TES classifier is better than the PCS and thus contributes more to the mixed classifier. kt in TES is relatively consistent compared to the dimension in PCS as population size increases. For the mixed classifier at each population level, there are several points (kp, kt, p) that achieve peak performance, w.r.t. Both the accuracy in terms of classification rate and efficiency in terms of the number of total dimension kp + kt. A more detailed list of all points (kp, kt, p) is included in Annex A."}, {"heading": "3. LDA METHOD FOR TEXT-INDEPENDENT SPEAKER RECOGNITION", "text": "While the PCA strives for a dimensionally reduced orthogonal eigenspace with the largest data variance in each direction, the goal of Linear Discriminant Analysis (LDA) is to find another K-dimensional eigenspace (K \u2264 M, the data dimension), with the first K-directions differentiating between different classes at most. In this section, we first introduce LDA, then project MFCCs onto a new eigenspace based on LDA with fewer dimensions. Afterwards, we use a Gaussian mixing model (GMM) -based classifier with dimensionally reduced features to perform the speaker classification."}, {"heading": "3.1 Introduction of LDA", "text": "For this discussion of the Linear Discrimination Analysis (LDA) we assume that there are M \u00b7 Ts data X for the class s [1, S], where M is the sample dimension and Ts is the number of samples in this class s. \u03a6 and \u03a6s are the global mean for all classes and the local mean for each class s. Then we define between-class dispersion SB and within-class dispersion SW bySB = 1S S \u2211 s = 1 (\u03a6s \u2212 \u03a6) (\u03a6s \u2212 \u03a6) T, (4) SW = 1S \u2211 s = 1 Ts TA = 1 (Xt \u2212 \u03a6s) (Xt \u2212 \u03a6s) T. (5) If we choose w from the underlying space W, then wTSBw and w TSWw are the projections of SB and SW on the direction w. The search for the directions w for the best class discrimination is equivalent to the maximization of the ratio of (wTSBw / TW space) SWW = SWW = SWW = SWW = SWW = SWW = SWW = SWW = SWW = SWW = SWW = SWW = SWW = = SWW = SWW = SWW = = SWW = SWW = SWW = SWW = SWW = SWW = = SWW = = SWW = SWW = = SWW = = SWW = SWW = = SWW = = SWW = = SWW = SWW = SWW = SWW = SWW = = SWW = = SWW = SWW = SWW = SWW = SWW = = SWW = SWW = = SWW = = SWW = SWW = = SWW = = SWW = SWW = = SWW = = = SWW = = SWW = = = SWW = = = SWW = = SWW = = = SWW = = SWW = = = SWW = = = = SWW = = = SWW = = SWW = = = SWW = = = = = = SWW = = = = SWW = = = SWW = SWW = SWW = = SWW = = = = = SWW = = = = = = = SWW = = = = = = = SWW = = = = = = ="}, {"heading": "3.2 GMM for speaker recognition", "text": "Gaussian mixture density models the feature distribution of each speaker as a weighted sum of multiple Gaussian distributions = large. For each feature vector x in the M \u00b7 T feature group X, the probability of x can be determined by the equation (x-i) = N \u2211 i = 1 pibi (x), bi (x) = 1 (2\u03c0) M / 2 | 1 / 2 exp (x-i) T \u2212 1i (x-i)}, (7) where M is the dimension of the feature vector x, N is the number of mixing components, bi (x), i = 1, N, are the component densities, pi, N, are the mixing weights and the efficiency, i = {pi, \u00b5i}, i = 1, 2,..., N is the collective representation of the parameters. GMMs are attractive for modeling the speakers because they can reveal the underlying configuration."}, {"heading": "3.3 Performance Evaluation of the LDA-GMM classifier", "text": "Jin et al. have shown that the application of LDA can not only reduce the feature dimension and calculation costs, but also improve the classification rate.9 In this work, we aim to quantitatively find the \"optimal\" reduced feature dimension k (k < M), which balances both classification accuracy and efficiency.9 Fig. 6 shows the classification rate r of the LDA-GMM classifier, which increases along with the LDA property dimension k. If the dimension is optimized by maximizing accuracy, it can be close to the original full dimension M and the calculation costs are similar to the GMM classifier without LDA. Therefore, a common performance / complexity optimized dimension k is developed by minimizing the product of the error rate (1 \u2212 r) and dimensioning k and is formulated below: k \u00b2 = arg min \u00b2 n \u00b2 n \u00b2."}, {"heading": "4. IMPLEMENTATION OF THE PCA/LDA COMBINED METHOD", "text": "Although PCA and LDA are commonly used to reduce feature dimensions, both have their own advantages and disadvantages. This is not the case with LDA. In addition, PCA requires less computation, especially if we calculate PCA eigenspaces for each class and form a PCA-based classifier without other pattern classification techniques such as the PCA classifier designed in this paper. This is the weakness of PCA and where LDA excels. LDA suffers from the problem of singularity and \"peaking\" problem, 16 when the training database is small. Recent studies show that LDA may outperform LDA when the training set is small and PCA is less sensitive to different training sets.Since PCA and LDA suffer from the problem of \"peaking,\" 16 when the training database is small, we run PCA-2 and implement PCA-2 methods to supplement individual text processing techniques."}, {"heading": "4.1 Database and Feature Extraction", "text": "A subset of the TIMIT corpus, consisting of 200 male speakers, each with 10 pronouncements from regions 1 to 4, is used for the implementation, and the 10 pronouncements of each speaker are then divided into 3 parts, one for registration, one for validation, and one for testing. 1. The first 12 seconds after concatenation of pronouncements from nos. 3 to no.8 are used for registration, such as calculating the classification rate of the PCA and LDA GMM classifiers and determining the settings of the parameters \u2022 PCA = {kp, kt, p} and \u2022 LDA \u2212 GMM = {N, k} and the weight p in the PCA / LDA classification. 3. The first 4 seconds after concatenation of nos 9 and 10 are used for testing the energy."}, {"heading": "4.2 GMM Initialization and Training", "text": "Given an M \u00b7 T feature set X for each loudspeaker, where M is the dimension of each feature vector and T is the number of vectors (samples), the N-dimensional feature vectors xi, i [1, N] of X are selected as the initial means \u00b5i to initialize a N-dimensional GMM, the initial variance matrix \u0439i is the M \u00b7 M identity matrix and the weight pi is 1 / N. In GMM training, a variance-limiting constraint Var (x) / T 2 is used for each dimension of X to avoid singularity in the probability function of the model. The maximum number of iterations in the EM algorithm to find the MLE of the GMM is 100, with an early termination condition that the increase in the probability of logic between two consecutive iterations is less than 0.001."}, {"heading": "4.3 Combined Classifier based on PCA/LDA", "text": "In order to combine the PCA-based (PCS & TES) classifier and the LDA-GMM-based (DA) classifier, we first normalized them on a uniform scale using the following equation: g (s) 1 = g (s) PCS & TES (X) \u2211 S = 1 (g (s) PCS & TES (X) 2, g (s) 2 = g (s) LDA \u2212 GMM (X) \u2211 S s = 1 (g (s) LDA \u2212 GMM (X)) 2, (13), where g1 and g2 are the normalized classifiers of both approaches. Then, we optimized the weight p on the combined classifier byp \u0445 = arg max p (p) = 0.1 (p) = arg max p (X) p (0.1) = 0.1 = 1 Ist = 1 = argmaxs [1], S] pg (s) 1 (s) 1 (s) pg (s) 2 (s) (S), S (2), S (S) \u2212 14) where."}, {"heading": "5. CONCLUSION AND FUTURE WORK", "text": "This paper presents classifiers based on PCA and LDA with optimized parameter settings. However, in the PCA approach, a classifier based on PCS and TES is discussed and evaluated with globally optimized settings in kp, kt the dimension of PCS and TES and the weight p that balances the two individual PCS and TES classifiers. In the LDA approach, the feature dimension is reduced with the help of LDA before being passed as input to the GMM classifier. GMMs are initialized with an appropriate model sequence taking into account accuracy and efficiency. Then, an LDA-GMM classifier with optimized settings is shown to achieve comparable accuracy in loudspeaker classification with significantly reduced time. After the development of PCA and LDA-GMM classifier, a classifier that combines these two techniques can achieve higher performance."}, {"heading": "APPENDIX A. PARAMETER SETTING OF PCA-BASED CLASSIFIER WITH OPTIMIZED PERFORMANCE", "text": "When searching exhaustively for the optimized parameter set \u03bb = {kp, kt, p} that provides the best classification performance, several points were found in the parameter space, some of which were grouped and show a stable region that achieves high performance while the others were scattered. At this point, we offer a more detailed list of these points of parameter setting than shown in Table 1 in paragraph 2.3. These points were sorted in ascending order of computing costs w.r.t. kp + kt. After taking into account both classification accuracy and efficiency, we have selected some points to form a combined classification with LDA, and the results are provided in paragraph 4."}], "references": [{"title": "Automatic speaker recognition: Current approaches and future trends", "author": ["D. Reynolds"], "venue": "Speaker Verification: From Research to Reality (2001).", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Speaker recognition: A tutorial", "author": ["J. Campbell Jr"], "venue": "Proceedings of the IEEE 85(9), 1437\u20131462 (1997).", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Robust text-independent speaker identification using gaussian mixture speaker models", "author": ["D. Reynolds", "R. Rose"], "venue": "Speech and Audio Processing, IEEE Transactions on 3(1), 72\u201383 (1995).", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "Variable parameter speaker verification system based on hidden markov modeling", "author": ["M. Savic", "S. Gupta"], "venue": "[Acoustics, Speech, and Signal Processing, 1990. ICASSP-90., 1990 International Conference on ], 281\u2013284, IEEE (1990).", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "User-customized password speaker verification using multiple reference and background models", "author": ["M.F. BenZeghiba", "H. Bourlard"], "venue": "Speech Communication 8 (0 2006). IDIAP-RR 04-41.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Support vector machines for speaker verification and identification", "author": ["V. Wan", "W. Campbell"], "venue": "[Neural Networks for Signal Processing X, 2000. Proceedings of the 2000 IEEE Signal Processing Society Workshop ], 2, 775\u2013784, IEEE (2000).", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Speaker recognition using neural networks and conventional classifiers", "author": ["K. Farrell", "R. Mammone", "K. Assaleh"], "venue": "Speech and Audio Processing, IEEE Transactions on 2(1), 194\u2013205 (1994).", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1994}, {"title": "Exploiting pca classifiers to speaker recognition", "author": ["W. Zhang", "Y. Yang", "Z. Wu"], "venue": "[Neural Networks, 2003. Proceedings of the International Joint Conference on ], 1, 820\u2013823, IEEE (2003).", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Application of lda to speaker recognition", "author": ["Q. Jin", "A. Waibel"], "venue": "[Sixth International Conference on Spoken Language Processing ], (2000).", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Gmm based on local pca for speaker identification", "author": ["C. Seo", "K. Lee", "J. Lee"], "venue": "Electronics Letters 37(24), 1486\u20131488 (2001).", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Noise robust speaker identification using pca based genetic algorithm", "author": ["R. Islam", "F. Rahman"], "venue": "International Journal of Computer Applications IJCA 4(12), 27\u201331 (2010).", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Speaker identification by k-nearest neighbors: Application of pca and lda prior to knn", "author": ["J. Kacur", "R. Vargic", "P. Mulinka"], "venue": "[Systems, Signals and Image Processing (IWSSIP), 2011 18th International Conference on ], 1\u20134, IEEE (2011).", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) , 1\u201338 (1977).", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1977}, {"title": "Theoretical, statistical, and practical perspectives on pattern-based classification approaches to the analysis of functional neuroimaging data", "author": ["A. O\u2019Toole", "F. Jiang", "H. Abdi", "N. P\u00e9nard", "J. Dunlop", "M. Parent"], "venue": "Journal of cognitive neuroscience 19(11), 1735\u20131752 (2007).", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Why can lda be performed in pca transformed space", "author": ["J. Yang", "J. Yang"], "venue": "Pattern recognition 36(2), 563\u2013566 (2003).", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "The peaking phenomenon in the presence of feature-selection", "author": ["C. Sima", "E. Dougherty"], "venue": "Pattern Recognition Letters 29(11), 1667\u20131674 (2008).", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Pca versus lda", "author": ["A. Martinez", "A. Kak"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 23(2), 228\u2013233 (2001).", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Reproducing the feature outputs of common programs using matlab and melfcc.m", "author": ["D. Ellis"], "venue": "(May 2005). http://http://labrosa.ee.columbia.edu/matlab/rastamat/mfccs.html.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Speech and language processing: An introduction to speech recognition", "author": ["D. Jurafsky", "J. Martin"], "venue": "Computational Linguistics and Natural Language Processing. 2nd Edn., Prentice Hall, ISBN 10.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 0}], "referenceMentions": [{"referenceID": 0, "context": "Thus, it is sufficient to implement exhausted search in kp, kt \u2208 (1, bM/2c) and p \u2208 [0, 1] with interval 0.", "startOffset": 84, "endOffset": 90}, {"referenceID": 14, "context": "5), we found the range of N \u2208 [15, 40] is appropriate for our database.", "startOffset": 30, "endOffset": 38}, {"referenceID": 7, "context": ")[k] 100 [24] 100 [26] 95 [8] 96 [36] 96 [32] 93.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": ")[k] 100 [24] 100 [26] 95 [8] 94 [8] 93.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": ")[k] 100 [24] 100 [26] 95 [8] 94 [8] 93.", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "5 [10] 87 [8]", "startOffset": 2, "endOffset": 6}, {"referenceID": 7, "context": "5 [10] 87 [8]", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "p\u2217 = arg max p\u2208[0,1] r(p) = arg max p\u2208[0,1] \u2211S s=1 s=argmaxs\u2208[1,S] pg (s) 1 (X)+(1\u2212p)g (s) 2 (X) S , (14)", "startOffset": 15, "endOffset": 20}, {"referenceID": 0, "context": "p\u2217 = arg max p\u2208[0,1] r(p) = arg max p\u2208[0,1] \u2211S s=1 s=argmaxs\u2208[1,S] pg (s) 1 (X)+(1\u2212p)g (s) 2 (X) S , (14)", "startOffset": 38, "endOffset": 43}], "year": 2016, "abstractText": "Various algorithms for text-independent speaker recognition have been developed through the decades, aiming to improve both accuracy and efficiency. This paper presents a novel PCA/LDA-based approach that is faster than traditional statistical model-based methods and achieves competitive results. First, the performance based on only PCA and only LDA is measured; then a mixed model, taking advantages of both methods, is introduced. A subset of the TIMIT corpus composed of 200 male speakers, is used for enrollment, validation and testing. The best results achieve 100%, 96% and 95% classification rate at population level 50, 100 and 200, using 39dimensional MFCC features with delta and double delta. These results are based on 12-second text-independent speech for training and 4-second data for test. These are comparable to the conventional MFCC-GMM methods, but require significantly less time to train and operate.", "creator": "LaTeX with hyperref package"}}}