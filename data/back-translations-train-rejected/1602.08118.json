{"id": "1602.08118", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2016", "title": "Hierarchical Conflict Propagation: Sequence Learning in a Recurrent Deep Neural Network", "abstract": "Recurrent neural networks (RNN) are capable of learning to encode and exploit activation history over an arbitrary timescale. However, in practice, state of the art gradient descent based training methods are known to suffer from difficulties in learning long term dependencies. Here, we describe a novel training method that involves concurrent parallel cloned networks, each sharing the same weights, each trained at different stimulus phase and each maintaining independent activation histories. Training proceeds by recursively performing batch-updates over the parallel clones as activation history is progressively increased. This allows conflicts to propagate hierarchically from short-term contexts towards longer-term contexts until they are resolved. We illustrate the parallel clones method and hierarchical conflict propagation with a character-level deep RNN tasked with memorizing a paragraph of Moby Dick (by Herman Melville).", "histories": [["v1", "Thu, 25 Feb 2016 21:12:25 GMT  (841kb)", "http://arxiv.org/abs/1602.08118v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew j r simpson"], "accepted": false, "id": "1602.08118"}, "pdf": {"name": "1602.08118.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Conflict Propagation: Sequence Learning in a Recurrent Deep Neural Network", "authors": ["Andrew J.R. Simpson"], "emails": ["Andrew.Simpson@Surrey.ac.uk"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}], "references": [{"title": "On the computational power of neural nets", "author": ["HT Siegelmann", "ED Sontag"], "venue": "Journal of computer and system sciences", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Learning complex, extended sequences using the principle of history compression,", "author": ["J Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1992}, {"title": "Learning long-term dependencies with gradient descent is difficult,", "author": ["Y Bengio", "P Simard", "P Frasconi"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies,", "author": ["S Hochreiter", "Y Bengio", "P Frasconi", "J Schmidhuber"], "venue": "A Field Guide", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Beyond regression: New tools for prediction and analysis in the behavioral sciences,", "author": ["PJ Werbos"], "venue": "PhD thesis, Harvard", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1974}, {"title": "Learning representations by back-propagating errors", "author": ["D Rumelhart", "GE Hinton", "RJ Williams"], "venue": "Nature 323:", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1986}, {"title": "The Y (2006) \u201cA fast learning algorithm for deep belief nets", "author": ["GE Hinton", "S Osindero"], "venue": "Neural Computation", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Abstract Learning via Demodulation in a Deep Neural Network\u201d, arxiv.org abs/1502.04042", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Measuring errors in text entry tasks: An application of the Levenshtein string distance statistic", "author": ["Soukoreff RW", "MacKenzie"], "venue": "Extended Abstracts of the ACM Conference on Human Factors in Computing Systems - CHI", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "In principle, recurrent neural networks (RNN) are powerful general computing machines capable of learning long-term dependencies in sequences [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "However, in practice, traction in supervised learning problems has been limited by difficulties in the optimization problem; Gradient descent based training methods appear insufficiently powerful to learn long-term dependencies [2] and this is thought to be due to the so-called vanishing gradient problem [3,4].", "startOffset": 228, "endOffset": 231}, {"referenceID": 2, "context": "However, in practice, traction in supervised learning problems has been limited by difficulties in the optimization problem; Gradient descent based training methods appear insufficiently powerful to learn long-term dependencies [2] and this is thought to be due to the so-called vanishing gradient problem [3,4].", "startOffset": 306, "endOffset": 311}, {"referenceID": 3, "context": "However, in practice, traction in supervised learning problems has been limited by difficulties in the optimization problem; Gradient descent based training methods appear insufficiently powerful to learn long-term dependencies [2] and this is thought to be due to the so-called vanishing gradient problem [3,4].", "startOffset": 306, "endOffset": 311}, {"referenceID": 0, "context": ", \u201ca\u201d = [1 0 0 0 0]).", "startOffset": 8, "endOffset": 19}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": ", online or back propagation through time [5-7]) do not provide a good solution.", "startOffset": 42, "endOffset": 47}, {"referenceID": 5, "context": ", online or back propagation through time [5-7]) do not provide a good solution.", "startOffset": 42, "endOffset": 47}, {"referenceID": 6, "context": ", online or back propagation through time [5-7]) do not provide a good solution.", "startOffset": 42, "endOffset": 47}, {"referenceID": 7, "context": "Biased sigmoid activation functions [8] were used with a softmax output layer.", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "All the N clones swept the training sequence in parallel, calculating weight updates using backpropagation gradient descent [6] for each parallel clone at each step of the sweep, averaging the weight updates (gradients) and applying the averaged update to the shared weights after each step.", "startOffset": 124, "endOffset": 127}, {"referenceID": 8, "context": "Then, the output stream was decoded using the dictionary and recall accuracy was evaluated using the Levenshtein edit distance metric [9], which captures the degree of editing necessary to correct the predicted text to match the training sequence.", "startOffset": 134, "endOffset": 137}], "year": 2016, "abstractText": "Recurrent neural networks (RNN) are capable of learning to encode and exploit activation history over an arbitrary timescale. However, in practice, state of the art gradient descent based training methods are known to suffer from difficulties in learning long term dependencies. Here, we describe a novel training method that involves concurrent parallel cloned networks, each sharing the same weights, each trained at different stimulus phase and each maintaining independent activation histories. Training proceeds by recursively performing batch-updates over the parallel clones as activation history is progressively increased. This allows conflicts to propagate hierarchically from short-term contexts towards longer-term contexts until they are resolved. We illustrate the parallel clones method and hierarchical conflict propagation with a character-level deep RNN tasked with memorizing a paragraph of Moby Dick (by Herman Melville).", "creator": "PDFCreator Version 1.7.1"}}}