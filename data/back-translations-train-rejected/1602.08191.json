{"id": "1602.08191", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2016", "title": "DeepSpark: A Spark-Based Distributed Deep Learning Framework for Commodity Clusters", "abstract": "The increasing complexity of deep neural networks (DNNs) has made it challenging to exploit existing large-scale data process pipelines for handling massive data and parameters involved in DNN training. Distributed computing platforms and GPGPU-based acceleration provide a mainstream solution to this computational challenge. In this paper, we propose DeepSpark, a distributed and parallel deep learning framework that simultaneously exploits Apache Spark for large-scale distributed data management and Caffe for GPU-based acceleration. DeepSpark directly accepts Caffe input specifications, providing seamless compatibility with existing designs and network structures. To support parallel operations, DeepSpark automatically distributes workloads and parameters to Caffe-running nodes using Spark and iteratively aggregates training results by a novel lock-free asynchronous variant of the popular elastic averaging stochastic gradient descent (SGD) update scheme, effectively complementing the synchronized processing capabilities of Spark. DeepSpark is an on-going project, and the current release is available at", "histories": [["v1", "Fri, 26 Feb 2016 04:18:21 GMT  (3754kb,D)", "http://arxiv.org/abs/1602.08191v1", null], ["v2", "Tue, 8 Mar 2016 08:32:16 GMT  (3755kb,D)", "http://arxiv.org/abs/1602.08191v2", null], ["v3", "Sat, 1 Oct 2016 02:44:07 GMT  (439kb,D)", "http://arxiv.org/abs/1602.08191v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hanjoo kim", "jaehong park", "jaehee jang", "sungroh yoon"], "accepted": false, "id": "1602.08191"}, "pdf": {"name": "1602.08191.pdf", "metadata": {"source": "CRF", "title": "DeepSpark: Spark-Based Deep Learning Supporting Asynchronous Updates and Caffe Compatibility", "authors": ["Hanjoo Kim", "Jaehong Park", "Jaehee Jang", "Sungroh Yoon"], "emails": ["sryoon@snu.ac.kr", "uwanggood@snu.ac.kr", "hukla@snu.ac.kr", "sryoon@snu.ac.kr", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS concepts \u2022 Networks \u2192 Cloud Computing; \u2022 Computing Theory \u2192 Distributed Computing Models; \u2022 Computing Methods \u2192 Neural Networks; keywords Distributed Computing; Deep Learning; asynchronous SGD"}, {"heading": "1. INTRODUCTION", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Deep learning training", "text": "The training of a deep neural network consists of two steps, forward and backward propagation [20]. Forward propagation generates the output using previous activations and hidden parameters. Afterwards, the error is calculated against the target at the output classifier and propagated throughout the network back to the previous level. Optimization is performed during the backward propagation step to achieve a better approximate target distribution.This process is iterated until the end of the training. Although complex neural network models successfully approximate the distribution of input data, it naturally leads to a large amount of parameters to be learned. This requires an enormous amount of time and computing resources, which is one of the main concerns in the deeper learning research.Workers Driver nodeloadRaw data RDDDPreprocessfilter (), map (),... # 1 # 2 # 3 # 3 # 4Processed RDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD"}, {"heading": "2.2 Stochastic Gradient Descent", "text": "It is a question of how it could come to this, how it has come so far in the USA, how it has come so far in the USA, and how it has come so far in the USA, how it has come so far in the USA, how it has come so far in the USA, how it has come so far in the USA, how it has come so far in the USA, and how it has come so far in the USA, how it has come so far in the USA, and how it has come so far in the USA, and how it has come so far in the USA, and how it has come so far in the USA, and how it has come in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA in the USA, in the USA in the USA in the USA, in the USA in the USA in the USA, in the USA in the USA in the USA in the USA in the USA, in the USA in the USA in the USA in the USA in the USA in the USA, in the USA in the USA in the USA in the USA in the USA in the USA in the USA, in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA, in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA, in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA, in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA, in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA in the USA"}, {"heading": "2.4 Apache Hadoop and Spark", "text": "In Hadoop, the data sets in the Hadoop Distributed File System (HDFS) [30] are divided into several blocks. HDFS provides overall control of these blocks and maintains fault tolerance. Hadoop YARN, the resource management and order monitoring framework, is responsible for containers working in parallel. Spark is the cluster computing engine running on YARN, Apache Mesos, or EC2. At the heart of Spark is distributed processing in memory using robust distributed data sets (RDD). RDD is the read-only collection of data distributed across a range of machines. RDD performs parallel actions to produce actual results or apply transformations to convert RDD into other types of RDD."}, {"heading": "3.1 Motivations", "text": "Apache Spark is an attractive platform for data processing pipelines such as processing database queries. However, Spark RDD is primarily suitable for batch-synchronous actions and provides limited asynchronous operations between the master and the workers. To compensate for Spark's drawbacks, we implemented a new asynchronous SGD solver with a custom parameter exchanger in the Spark environment. To improve asynchronous performance, we significantly improved the elastic average stochastic gradient descent algorithm (EASGD) [27] by taking adaptive parameter updates into account and achieved faster convergence."}, {"heading": "3.2 Structure overview", "text": "DeepSpark consists of three main parts, namely Apache Spark, an asynchronous SGD parameter exchanger, and the Caffe software, as shown in Figure 1. Apache Spark manages the manpower and available resources that are allocated by the resource manager. Figure 2 shows how the Spark workflow for asynchronous SGD develops, and we provide more detailed descriptions in Section 3.3. Then we explain the interface for parameter exchangers and asynchronous SGD process, the Spark in Sections 3.4-6 and Figure 3. We explain how Caffe, as our SGD calculator, can be integrated with Spark using Java Native Access (JNA) 1. The entire process of DeepSpark is summarized in Algorithm 1. We assumed that DeepSpark runs on Hadoop YARN [29] and the distributed file system Hadoop (HDFS) [30]."}, {"heading": "3.3 Distributed setup for Spark", "text": "In this section, we explain that DeepSpark's distributed workflow, from data preparation to asynchronous SGD, corresponding to lines 3-9 in algorithm 1, and from load to spill in Figure 2. Since DeepSpark runs on the Spark framework, it must load and transform data in the form of Spark RDD [16]. The first step of the DeepSpark training is to create RDD for training and inferences. We define a container class that stores labeling information and related data for each sample. The data-specific loader then creates the RDD of that data container class, followed by the preprocessing phase. In the preprocessing phase, data containers connected to the preprocessing pipeline, such as filtering, mapping, transforming, or shuffling, are then repartitioned to allow the processed RDD to access the number of data layers relative to the actual work force, rather than the number of partitions on the data base."}, {"heading": "3.4 Asynchronous EASGD operation", "text": "It is possible to imitate the asynchrony by the dummy RDD shown in Figure 2. Once the local repository of the LMDB has been prepared for each worker, the dummy RDD is created and distributed to all workers. These dummy data play a special role in the introduction of distributed actions where model training is carried out in parallel, taking advantage of Spark's property that the Spark scheduler reuses the already existing worker node. The size of the dummy RDD is explicitly set to the number of workers for full parallel operation, and for each action is executed on that dummy RDD. Within the predictive partition process, each worker is able to use the local data repository created in the previous job, and starts training step by step."}, {"heading": "3.5 Adaptive update rules", "text": "In this section, we explain two variants of the EASGD update rules for fast convergence. For model parameters in the master node, the moving rate \u03b1 behaves like the moving rate for the sequential SGD process. As an adaptive learning rate policy [31] in the single-node SGD process, we expect to improve the convergence rate of the training result during the training process by adapting the moving rate. In the DeepSpark framework, we also investigated the effects of modifying \u03b1 during the training. Equation 5 describes our implementation on decay rate \u03b1 with the power rate when the update rate does not arrive at every step size."}, {"heading": "3.6 Parameter exchanger", "text": "The parameter exchanger is the DeepSpark implementation of parameter server concepts, which is indispensable for asynchronous updates. In DeepSpark, the application driver node serves as a parameter exchanger to enable workers to update their weight requirements asynchronously. Figure 3 shows a sketch of a learning cycle of parameter exchangers; the thedriver node starts the parameter exchanger as a separate thread before workers start training the model.Because if there are multiple weight exchange requests from workers nodes, we implement a thread pool to process the requests concurrently. For each connection request, the thread pool assigns the predefined threads that process the weight exchange requests. The size of the thread pool is specified in the program, and we set this to eight threads, depending on the size of the queue requests - if the number of threads is exceeded in the network queue requests."}, {"heading": "3.7 SGD engine", "text": "In DeepSpark, each worker node uses the Caffe library as a GPU-accelerated SGD engine. However, the Spark application is written in Java, Scala, and Python, which are unable to use the native Caffe library directly at source level. We implemented our code with JNA, so Spark executors are able to point to the native Caffe library. To parallelise the Caffe models, the Caffe model parameters should be readable and writable. Caffe's original SGDSolver class does not provide an interface for this, so we have derived a custom solver class from the SGDSolver. We have defined some derivative solver class operations to perform an atomic isolation action, acquire current trained parameter weights, and modify them in parameter weights."}, {"heading": "4. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental setup", "text": "The distributed cluster shown in Figure 9 consisted of 25 identical machines, one of which was used for the experiments on a single computer. Each machine had an Intel Core i7-4790 processor with 16 GB of RAM and an NVIDIA GTX970 GPU with 4 GB of memory and communicated with each other via Gigabit Ethernet connections. We examined not only DeepSpark on the cluster, but also Caffe [17] (stand-alone machine) and SparkNet (cluster) as sequential and parallel adversaries. To manage each node, we also set up a separate Hadoop server machine. This server machine did not participate in the calculation, but operated only YARN, which functions as a manager of cluster nodes and HDFS nameplate daemon."}, {"heading": "4.2 Dataset and neural network models", "text": "To observe the advantages of parallelization in terms of data scalability, we used two sets of data: CIFAR10 and ImageNet.4.2.1 CIFAR10 The CIFAR10 dataset contains 60,000 images with a size of 32x32, 3 channels and 10 classes [18]. It consists of 50,000 images for training and 10,000 images for testing. On this CIFAR10 dataset we trained a Convolutionary Neural Network Model, which is a variant of Cuda-Convnet [5]. However, in this model there are three sets, including a folding layer (5x5, step 1), a pool layer and a Reflected Linear Unit (ReLU) activation layer, followed by a fully connected layer and a Softmax layer for calculating the results (128 functional sketches of maps).This model achieves a test accuracy of 75%."}, {"heading": "4.3 Experimental results", "text": "4.3.1 Training on CIFAR10 We investigated the time it took a single Caffe machine and the distributed net stars DeepSpark and SparkNet to achieve target accuracy (.72,.73,.74,.75). For all experiments in this section, we first compared the learning rate \u03b7 = 0.001, momentum \u03b4 = 0.9, weight decay as 0.004, and maximum iterations up to 20,000. From the results shown in Table 1 and Figure 6, we compared DeepSpark clusters with Caffe and the result is shown in Table 1. From the results shown in Table 1, we confirmed about 66% of the experiments that converged faster than Caffe, with those that were unable to achieve target accuracy within maximum iterations, the best speedup rate for.75 test accuracy we took 28% less time than Caffe when 8 and grouping = 40."}, {"heading": "5. DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Speedup analysis", "text": "DeepSpark achieved a high acceleration rate compared to the sequential caffe and the distributed net time of DNK = 75. We found that the communication overhead played a crucial role in the slowdown. According to SparkNet [14], the acceleration of parallelization compared to a single sequential machine is also given as the following equation 7, in which Na (b) and Ma (B, K) are the required number of iterations to achieve accuracy, reducing the accuracy at level A for sequential and parallel acceleration. B is the size of the minibatch, C (b) is the time for a batch compilation, and K represents the number of nodes.SpeedUp = Na (b) C (b) + S) Ma (especially C (b) + S).We tried to loosen the communication overhead S through asynchronous updating and successful training."}, {"heading": "5.3 Data spilling", "text": "In order to implement asynchronous EASGD, the planning provided by Spark had to be sacrificed. Originally, Spark operates data in memory, which uses performance up to 100 times faster for certain applications [34], and if the data is larger than the memory, Spark stores remaining data in disk and scheduling operations. However, in order to perform asynchronous operations that are not suitable for Spark, we imitated the spilling process of registry allocation and buried data into local memory. However, time delays caused by disk operations did not affect the overall performance of DeepSpark. At 8 workers nodes, we spent about a few seconds or 8 minutes spilling CIFAR10 or ImageNet records, respectively. These delays accounted for less than 2% of the total runtime, which were negligible."}, {"heading": "5.4 Further improvements", "text": "Our analysis assumes that serialization of the weight parameters during network communication may have caused additional network effort. In our experiments, we used the basic Java serialization method for convenience and to optimize the speed of object creation. The basic process of object creation in Java is known to be inferior to many other advanced libraries [35]. By replacing the serialization process with more complex libraries, we expect to reduce the time for object / serialization and the size of serialized data. In addition, changes to the network protocols may also prove helpful for additional performance enhancements. To ensure convenience and reliability, our asynchronous update communication structure was based on TCP sockets. By replacing the current communication structure with UDP datagram communication [36], additional spectrums could be possible. Although the reliability of the UDP datagram is not guaranteed to make GASN more reliable per DNC, the use of the robustness of the network can normally be relied on."}, {"heading": "6. CONCLUSION", "text": "We have described our new deep learning framework called DeepSpark, which provides seamless integration with existing large data processing pipelines and a highly accelerated DNN training process. DeepSpark is an example of the successful combination of various components such as Apache Spark, asynchronous parameter updates, and the GPGPU-based Caffe Framework. According to our experiments with common benchmarks, DeepSpark has demonstrated its effectiveness by showing faster convergence than the alternative parallelization programs that were compared."}], "references": [{"title": "et al", "author": ["K. He"], "venue": "Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["J.K. Chorowski"], "venue": "Attention-based models for speech recognition. In NIPS, pages 577\u2013585", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "et al", "author": ["C. Szegedy"], "venue": "Going deeper with convolutions. In CVPR, pages 1\u20139", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["A. Krizhevsky"], "venue": "Imagenet classification with deep convolutional neural networks. In NIPS, pages 1097\u2013 1105", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["S. Chetlur"], "venue": "cudnn: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["A. Krizhevsky"], "venue": "arXiv preprint arXiv:1404.5997", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["M. Li"], "venue": "Scaling distributed machine learning with the parameter server. In OSDI, pages 583\u2013598", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["J. Dean"], "venue": "Large scale distributed deep networks. In NIPS, pages 1223\u20131231", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["Q. Ho"], "venue": "More effective distributed ml via a stale synchronous parallel parameter server. In NIPS, pages 1223\u20131231", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["E.P. Xing"], "venue": "Petuum: A new platform for distributed machine learning on big data. In SIGKDD, KDD \u201915, pages 1335\u20131344, New York, NY, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["B.C. Ooi"], "venue": "Singa: A distributed deep learning platform. In Proceedings of the ACM International Conference on Multimedia, pages 685\u2013688. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["J.T. Geiger"], "venue": "Investigating nmf speech enhancement for neural network based acoustic models. In IN- TERSPEECH, pages 2405\u20132409", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["P. Moritz"], "venue": "Sparknet: Training deep networks in spark. arXiv preprint arXiv:1511.06051", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["M. Zaharia"], "venue": "Spark: cluster computing with working sets. In Proceedings of the 2nd USENIX Conference on Hot Topics in Cloud Computing, volume 10, page 10", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["M. Zaharia"], "venue": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. In Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation, pages 2\u20132. USENIX Association", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["Y. Jia"], "venue": "Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International Conference on Multimedia, pages 675\u2013678. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "et al", "author": ["O. Russakovsky"], "venue": "ImageNet Large Scale Visual Recognition Challenge. IJCV, 115(3):211\u2013252", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Theory of the backpropagation neural network", "author": ["R. Hecht-Nielsen"], "venue": "IJCNN, pages 593\u2013605. IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1989}, {"title": "et al", "author": ["Y. LeCun"], "venue": "Deep learning. Nature, 521(7553):436\u2013 444", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Pattern Recognition & Machine Learning", "author": ["Y. Anzai"], "venue": "Elsevier", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["Y.N. Dauphin"], "venue": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS, pages 2933\u20132941", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["C.H. Teo"], "venue": "A scalable modular convex solver for regularized risk minimization. In SIGKDD, pages 727\u2013 736. ACM", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "et al", "author": ["M. Zinkevich"], "venue": "Parallelized stochastic gradient descent. In NIPS, pages 2595\u20132603", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["X. Lian"], "venue": "Asynchronous parallel stochastic gradient for nonconvex optimization. In NIPS, pages 2719\u2013 2727", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["S. Zhang"], "venue": "Deep learning with elastic averaging sgd. In NIPS, pages 685\u2013693", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["D.M. Blei"], "venue": "Latent dirichlet allocation. the Journal of machine Learning research, 3:993\u20131022", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "et al", "author": ["V.K. Vavilapalli"], "venue": "Apache hadoop yarn: Yet another resource negotiator. In Proceedings of the 4th Annual Symposium on Cloud Computing, page 5. ACM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["K. Shvachko"], "venue": "The hadoop distributed file system. In MSST, pages 1\u201310. IEEE", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Increased rates of convergence through learning rate adaptation", "author": ["R.A. Jacobs"], "venue": "Neural networks, 1(4):295\u2013 307", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1988}, {"title": "et al", "author": ["B. Recht"], "venue": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In NIPS, pages 693\u2013701", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot"], "venue": "In AISTATS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "et al", "author": ["R.S. Xin"], "venue": "Shark: Sql and rich analytics at scale. In Proceedings of the 2013 ACM SIGMOD International Conference on Management of data, pages 13\u201324. ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "A comparison of data serialization formats for optimal efficiency on a mobile platform", "author": ["A. Sumaray"], "venue": "In IMCOM,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Profiling and reducing processing overheads in tcp/ip", "author": ["J. Kay"], "venue": "IEEE/ACM TON,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "For instance, convolutional neural networks (CNNs) have become the de facto standard method for image/object recognition in computer vision [1].", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "xxx/xxx x performance in various machine learning problems including speech recognition [2] and image classification [1, 3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 0, "context": "xxx/xxx x performance in various machine learning problems including speech recognition [2] and image classification [1, 3].", "startOffset": 117, "endOffset": 123}, {"referenceID": 2, "context": "xxx/xxx x performance in various machine learning problems including speech recognition [2] and image classification [1, 3].", "startOffset": 117, "endOffset": 123}, {"referenceID": 0, "context": ", a recent CNN model called ResNet has over 150 layers [1]), which effectively provide intermediate representations of the original input data.", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "This leads to the training time ranging from several hours to days even with GPGPU-based acceleration [1, 3, 4, 5].", "startOffset": 102, "endOffset": 114}, {"referenceID": 2, "context": "This leads to the training time ranging from several hours to days even with GPGPU-based acceleration [1, 3, 4, 5].", "startOffset": 102, "endOffset": 114}, {"referenceID": 3, "context": "This leads to the training time ranging from several hours to days even with GPGPU-based acceleration [1, 3, 4, 5].", "startOffset": 102, "endOffset": 114}, {"referenceID": 4, "context": "This leads to the training time ranging from several hours to days even with GPGPU-based acceleration [1, 3, 4, 5].", "startOffset": 102, "endOffset": 114}, {"referenceID": 5, "context": "Highly optimized GPGPU implementations have significantly shortened the time spent in training DNNs, often showing dozens of times speedup [6, 7].", "startOffset": 139, "endOffset": 145}, {"referenceID": 6, "context": "Highly optimized GPGPU implementations have significantly shortened the time spent in training DNNs, often showing dozens of times speedup [6, 7].", "startOffset": 139, "endOffset": 145}, {"referenceID": 4, "context": "However, acceleration on a single machine has limitations due to the limited resources such as the host machine\u2019s GPU memory or main memory [5].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "To overcome the issues of training DNNs on a single machine, scaling out methods in distributed environments have been suggested [8, 9, 10, 11, 12].", "startOffset": 129, "endOffset": 147}, {"referenceID": 8, "context": "To overcome the issues of training DNNs on a single machine, scaling out methods in distributed environments have been suggested [8, 9, 10, 11, 12].", "startOffset": 129, "endOffset": 147}, {"referenceID": 9, "context": "To overcome the issues of training DNNs on a single machine, scaling out methods in distributed environments have been suggested [8, 9, 10, 11, 12].", "startOffset": 129, "endOffset": 147}, {"referenceID": 10, "context": "To overcome the issues of training DNNs on a single machine, scaling out methods in distributed environments have been suggested [8, 9, 10, 11, 12].", "startOffset": 129, "endOffset": 147}, {"referenceID": 11, "context": "To overcome the issues of training DNNs on a single machine, scaling out methods in distributed environments have been suggested [8, 9, 10, 11, 12].", "startOffset": 129, "endOffset": 147}, {"referenceID": 4, "context": "Many real-world datasets used for DNN training (such as raw images or speech signals) need to be converted into a trainable format on deep learning platforms and often require preprocessing to improve robustness [5, 13].", "startOffset": 212, "endOffset": 219}, {"referenceID": 12, "context": "Many real-world datasets used for DNN training (such as raw images or speech signals) need to be converted into a trainable format on deep learning platforms and often require preprocessing to improve robustness [5, 13].", "startOffset": 212, "endOffset": 219}, {"referenceID": 13, "context": "SparkNet [14] was motivated by this and combined deep learning algorithms with existing data analytic pipelines on Apache Spark [15].", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "SparkNet [14] was motivated by this and combined deep learning algorithms with existing data analytic pipelines on Apache Spark [15].", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "For instance, Spark underperforms on the jobs that require updating shared parameters in asynchronous manner, which is the general scheme of distributed deep learning systems [16].", "startOffset": 175, "endOffset": 179}, {"referenceID": 16, "context": "(b) Adopting the handy and qualified Caffe engine: To integrate the widely used Caffe framework [17] into Spark, we designed a novel Java Caffe wrapper.", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "Experimental evaluations that demonstrate the effectiveness of our system scaling and asynchrony for expediting DNN training: We tested DeepSpark with popular benchmarks including CIFAR-10 [18] and ILSVRC 2012 ImageNet [19] datasets and observed consistent speedups over existing scale-up and scale-out approaches.", "startOffset": 189, "endOffset": 193}, {"referenceID": 18, "context": "Experimental evaluations that demonstrate the effectiveness of our system scaling and asynchrony for expediting DNN training: We tested DeepSpark with popular benchmarks including CIFAR-10 [18] and ILSVRC 2012 ImageNet [19] datasets and observed consistent speedups over existing scale-up and scale-out approaches.", "startOffset": 219, "endOffset": 223}, {"referenceID": 19, "context": "Training deep neural network is composed of two steps, feed-forward and back propagation [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "Gradient descent is the most commonly used optimization method for training deep neural network practically [21].", "startOffset": 108, "endOffset": 112}, {"referenceID": 21, "context": "Minibatch stochastic gradient descent (SGD) [22] is the sequential variant of the batch gradient descent in that it only takes the gradient of the randomly sampled data at each update rather than the gradient of the whole data.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "where w is the parameters to learn, N is the minibatch size of SGD, i is the index of sampled data, Jw(x ) is the loss on sample x, Reg(w) is the regularization term [22] and \u03bb is the associated regularization weight.", "startOffset": 166, "endOffset": 170}, {"referenceID": 22, "context": "In addition, since the high dimensionality of DNN parameter space reduces the risk of being trapped in local minima [23], thus SGD generally runs appropriately in optimizing DNN parameters.", "startOffset": 116, "endOffset": 120}, {"referenceID": 23, "context": "3 Distributed Deep Learning A n\u00e4\u0131ve parallelization SGD can be implemented by splitting batch calculations over multiple worker nodes [24, 25].", "startOffset": 134, "endOffset": 142}, {"referenceID": 24, "context": "3 Distributed Deep Learning A n\u00e4\u0131ve parallelization SGD can be implemented by splitting batch calculations over multiple worker nodes [24, 25].", "startOffset": 134, "endOffset": 142}, {"referenceID": 8, "context": "[9, 26, 27]", "startOffset": 0, "endOffset": 11}, {"referenceID": 25, "context": "[9, 26, 27]", "startOffset": 0, "endOffset": 11}, {"referenceID": 26, "context": "[9, 26, 27]", "startOffset": 0, "endOffset": 11}, {"referenceID": 7, "context": "[8, 10] It has master slave architecture, while data and tasks are distributed over workers and server nodes manage the global parameters.", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[8, 10] It has master slave architecture, while data and tasks are distributed over workers and server nodes manage the global parameters.", "startOffset": 0, "endOffset": 7}, {"referenceID": 27, "context": "In previous works, distributed parameter server has been successfully used in training various machine learning algorithms such as logistic regression and latent Dirichlet allocation [28] on petabytes of real data.", "startOffset": 183, "endOffset": 187}, {"referenceID": 26, "context": "[27] The former is to explore the parameter space to find the optimal weight parameters and the latter is to update center parameter weights using local worker\u2019s training results and proceed to the next step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "SparkNet presented the iteration hyperparameter \u03c4 , which is the number of processed minibatches before the next communication [14].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "[14, 27] Zhang et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "[14, 27] Zhang et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "[27] In EASGD, master and local workers exchange their weight parameters like the original SGD methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "This method is different from downpour SGD [9], where gradients of local workers are shipped to the master and updated center parameters are sent back to workers at every update.", "startOffset": 43, "endOffset": 46}, {"referenceID": 26, "context": "[27]", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Apache Hadoop YARN [29] and Spark [15] are cluster frameworks that allows large scale data processing on commodity hardware.", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "Apache Hadoop YARN [29] and Spark [15] are cluster frameworks that allows large scale data processing on commodity hardware.", "startOffset": 34, "endOffset": 38}, {"referenceID": 29, "context": "In Hadoop, data sets are split into multiple blocks in Hadoop Distributed File System (HDFS) [30].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "They are, however, less suited for jobs that requires asynchronous actions on parallel workers [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "To improve asynchronous performance, we noticeably improved the elastic averaging stochastic gradient descent algorithm (EASGD) [27] by considering adaptive parameter updates, delivering faster convergence.", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "We assumed that DeepSpark runs on Hadoop YARN [29] and the Hadoop distributed file system (HDFS) [30] in the following explanation of our workflow.", "startOffset": 46, "endOffset": 50}, {"referenceID": 29, "context": "We assumed that DeepSpark runs on Hadoop YARN [29] and the Hadoop distributed file system (HDFS) [30] in the following explanation of our workflow.", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "Since DeepSpark is running on top of Spark framework, it needs to load and transform data in form of Spark RDD [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 30, "context": "As the adaptive learning rate policy [31] in the single node SGD process, we expect to improved the converge rate of training result during training process by adjusting the moving rate adaptively.", "startOffset": 37, "endOffset": 41}, {"referenceID": 31, "context": "Nevertheless, training results accumulate successfully as proved in [32].", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "but also Caffe [17] (single machine) and SparkNet (cluster), as a sequential and parallel opponents, respectively.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "The CIFAR10 dataset contains 60,000 images with 32x32 size, 3 channels, and 10 classes [18].", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "On this CIFAR10 dataset, we trained a convolutional neural network model which is a variant of Cuda-convnet [5].", "startOffset": 108, "endOffset": 111}, {"referenceID": 18, "context": "ILSVRC 2012 ImageNet dataset consists of 1,281,167 color images with 1,000 classes of different image concepts [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 3, "context": "To train the ImageNet dataset, we used a Caffe model which is a replication of Google\u2019s GoogLeNet [4].", "startOffset": 98, "endOffset": 101}, {"referenceID": 32, "context": "The replica GoogLeNet Caffe model, however, has several differences: it does not train with the relighting and scale/aspect-ratio data-augmentation and its weight initialization method is Xavier [33] instead of Gaussian.", "startOffset": 195, "endOffset": 199}, {"referenceID": 13, "context": "According to SparkNet [14], speedup of parallelization versus a single sequential machine is given as the following Equation 7, where Na(b) and Ma(\u03c4, b,K) are the required number of iterations to achieve accuracy level a for sequential and parallel, b is the size of minibatch, C(b) is time for a batch computation, S is the communication overhead, and K represents the number of nodes.", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "In addition, the adaptive EASGD update rules relieved the share of S even more, while suppressing the increase in Ma(\u03c4, b,K) [27], and we confirmed this from the results in Figure 8.", "startOffset": 125, "endOffset": 129}, {"referenceID": 33, "context": "Originally, Spark operates data in memory, which leverages performance up to 100 times faster for certain applications [34] and if data is bigger than memory, Spark stores lingering data in disk and schedules operations.", "startOffset": 119, "endOffset": 123}, {"referenceID": 34, "context": "The basic object serialization process in Java is known to be inferior to many other advanced libraries [35].", "startOffset": 104, "endOffset": 108}, {"referenceID": 35, "context": "By replacing the current communication structure by the UDP datagram communication [36], additional speedups may become possible.", "startOffset": 83, "endOffset": 87}], "year": 2017, "abstractText": "The increasing complexity of deep neural networks (DNNs) has made it challenging to exploit existing large-scale data process pipelines for handling massive data and parameters involved in DNN training. Distributed computing platforms and GPGPU-based acceleration provide a mainstream solution to this computational challenge. In this paper, we propose DeepSpark, a distributed and parallel deep learning framework that simultaneously exploits Apache Spark for large-scale distributed data management and Caffe for GPU-based acceleration. DeepSpark directly accepts Caffe input specifications, providing seamless compatibility with existing designs and network structures. To support parallel operations, DeepSpark automatically distributes workloads and parameters to Caffe-running nodes using Spark and iteratively aggregates training results by a novel lock-free asynchronous variant of the popular elastic averaging stochastic gradient descent (SGD) update scheme, effectively complementing the synchronized processing capabilities of Spark. DeepSpark is an on-going project, and the current release is available at http://deepspark.snu.ac.kr. CCS Concepts \u2022Networks \u2192 Cloud computing; \u2022Theory of computation\u2192Distributed computing models; \u2022Computing methodologies \u2192 Neural networks;", "creator": "LaTeX with hyperref package"}}}