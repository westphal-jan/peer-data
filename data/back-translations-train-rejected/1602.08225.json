{"id": "1602.08225", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2016", "title": "Multimodal Emotion Recognition Using Multimodal Deep Learning", "abstract": "To enhance the performance of affective models and reduce the cost of acquiring physiological signals for real-world applications, we adopt multimodal deep learning approach to construct affective models from multiple physiological signals. For unimodal enhancement task, we indicate that the best recognition accuracy of 82.11% on SEED dataset is achieved with shared representations generated by Deep AutoEncoder (DAE) model. For multimodal facilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE) achieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets, respectively, which are much superior to the state-of-the-art approaches. For cross-modal learning task, our experimental results demonstrate that the mean accuracy of 66.34% is achieved on SEED dataset through shared representations generated by EEG-based DAE as training samples and shared representations generated by eye-based DAE as testing sample, and vice versa.", "histories": [["v1", "Fri, 26 Feb 2016 07:43:14 GMT  (162kb)", "http://arxiv.org/abs/1602.08225v1", null]], "reviews": [], "SUBJECTS": "cs.HC cs.CV cs.LG", "authors": ["wei liu", "wei-long zheng", "bao-liang lu"], "accepted": false, "id": "1602.08225"}, "pdf": {"name": "1602.08225.pdf", "metadata": {"source": "CRF", "title": "Multimodal Emotion Recognition Using Multimodal Deep Learning", "authors": ["Wei Liu", "Wei-Long Zheng", "Bao-Liang Lu"], "emails": ["bllu}@sjtu.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.08 225v 1 [cs.H C] 26 Feb 20"}, {"heading": "1 Introduction", "text": "Emotionalization is becoming more and more important for human-machine interfaces (HMI). Emotion recognition could be done through text, images, and physiological signals. Bravo-Marquez et al. learned an expanded opinion (positive, neutral, and negative) on emotionality [Bravo-Marquez et al., 2015]. Wang and Pal used a limited optimization framework to discover the emotions of social media users [Wang and Pal, 2015]. Recently, many researchers investigated emotion recognition from EEG. Uses fractal dimension-based algorithms to detect and visualize emotions in real time [Liu et al., 2010]. Murugappan et al. used discrete waveforms to extract frequency features from EEG signals and two classifiers that are used to classify the features."}, {"heading": "2 Multimodal Deep Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Restricted Boltzmann Machine", "text": "A restricted Boltzmann machine (RBM) is an undirected graph model having a visible layer and a hidden layer. Connections exist only between the visible layer and the hidden layer and there is no connection either in the visible layer or in the hidden layer. Assuming visible variables v = 0, 1} M and hidden variables h = 0, 1} N, we have the following energy function E: E (v, h; \u03b8) = \u2212 M \u00b2 i = 1Wijvihj \u2212 M \u00b2 i = 1bivi \u2212 N \u00b2 j = 1ajhj (1), where \u03b8 = {a, b, W} are parameters, Wij is the symmetrical weight between visible unit i and hidden unit j, bi, aj = bias terms of visible unit or hidden unit. With the energy function we can obtain the joint distribution over the visible and hidden units: p (v, h) p \u00b2 l \u00b2 i."}, {"heading": "2.2 Model construction", "text": "In order to improve emotion recognition by combining EEG and eye movement data, we adopt a Bimodal Deep Autoencoder (BPCE) to extract common representations of EEG and eye movement data. If only one modality is available, the unimodal Deep Autoencoder (PCS) is used to extract common representations. These two types of deep autoencoder models are shown in Figure 1.BDAE Training. To train BADE, we first trained two RBMs for EEG signals and eye movement data, i.e. the first two layers in Figure 1 (b). After training the respective RBMs, two hidden layers are directly linked by hEEG and hEye. We treated the hidden layer as the visible layer of an upper RBM layer and eye movement data, i.e., the first two layers in Figure 1 (b)."}, {"heading": "3 Experiment settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Dataset", "text": "Two public datasets, SEED dataset 1 and DEAP dataset 2, were used in this thesis; the SEED dataset was first presented in [Zheng and Lu, 2015], which contains EEG singals and eye movement signals from 15 subjects while watching emotional film clips; the dataset contains 15 film clips and each clip lasts about 4 minutes; the EEG signals are from 62 channels at a sampling rate of 1000 Hz; and the eye movement signals contain information about blinking, saccade fixation, etc. To compare our proposed method with [Lu et al., 2015], we use the same datas1 http: / / bcmi.sjtu.edu.cn / \u02dc seed / index.html 2 http: / www.eecs.qmul.ac.com / mmv / datasets / readme.htmlas in [Lu et al, 2015], which are 27 datasets from the 9 subjects."}, {"heading": "3.2 Feature Extraction", "text": "Both types of characteristics include five frequency bands: Delta (1-4Hz), Theta (4-8Hz), Alpha (8-14Hz), Beta (14-31), and Gamma (31-50Hz). In terms of eye movement data, we used the same characteristics as in [Lu et al., 2015] as in Table 1. Extracted EEG characteristics and eye movement characteristics were then scaled between 0 and 1, and the scaled characteristics were used as visible units of the BDAE or DAE network.DEAP datasetInstead of using the downloaded preprocessed data directly as inputs of the BDAE network and the DAE network, the data was directly segmented as inputs of the DAE network."}, {"heading": "3.3 Classification", "text": "Inspired by [Guo and Guo, 2005], we conducted experiments on the following three types of emotion recognition tasks to test the efficiency of our proposed method. (1) For this task, an unimodal PCS network for EEG characteristics or eye movement characteristics was built to reconstruct both modalities. To train a classifier, the middle layers of split representations were extracted, using only the SEED dataset. (2) For multimodal facilitation tasks, both modalities were required; the common representations generated by the BDAE network were fed into linear SVM to train a classifier. Both the SEED dataset and the DEAP dataset were used. (3) For intermodal learning tasks, two non-imodal DAEs for EEG characteristics and eye movement characteristics were used."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Unimodal enhancement", "text": "In unimodal enhancement task, we used EEG signals to reconstruct information from two modalities. Once the PCS network was trained, we could use it as a feature selector to generate common representations, even if only one modality information is available. For eye movement data, the process was the same when only EEG signals were available. Figure 2 is the summary of all unimodal enhancement results. We can see from Figure 2 that the PCS model performs best in both EEG characteristics and eye movement traces.For EEG-based unimodal enhancement experiments, we constructed an affective model using EEG characteristics from different frequency bands. The experimental results are shown in Table 2 that an EEG-based PCS network was built to reconstruct both EEG characteristics and eye movement characteristics, and the common representations were used as new characteristics to classify emotions."}, {"heading": "4.2 Multimodal Facilitation", "text": "We have conducted two different experiments to compare our BPCS network with other models.The experimental results of the BPCS model show that only a single modality is available. (2) When both modalities are available, the common representations are obtained by linking the characteristics directly. SEED results Figure 3 shows the summary of the multimodal facilitation experimental results. We can see from Figure 3 that our BPCS model performs best (91.01%). In addition, the standard deviation of our BPCS model is also the smallest, indicating that the BPCS model exhibits good robustness. Table 5 shows the results when linking the characteristics of EEG signals and eye movement data directly to each other. The last column of Table 5 means that we directly link both five frequency bands of EEG signals and eye movement data. Compared with Table 2, we can see that we are linking different modeling frequencies to each other, and increasing accuracy in almost all."}, {"heading": "4.3 Cross-modal learning", "text": "The key point of both the PCS model and the BPCS model is the common representation. In this section, intermodal experiments will be conducted to investigate whether the common representations can learn common information between two different modalities. In the traditional machine learning framework, a classifier trained by EEG characteristics is usually considered a bad result when it is generated by eye characteristics. However, things are different when we use the PCS model. The PCS network is considered to be able to learn something common between different modalities. We can test this by using common representations generated by EEG characteristics, and common representations generated by eye characteristics, and vice versa. Both settings are examined, and the results are shown in Tables 8 and 9."}, {"heading": "5 Discussion", "text": "All three types of emotion recognition mentioned above are important for HMI systems in practice, and the multimodal moderation task allows us to use different modalities so that HMI systems have better detection accuracy. Furthermore, the experimental results have shown that when using both modalities, the standard deviation has become smaller than before, a phenomenon that indicates that our system is becoming more reliable. Unimodal enhancement results have shown that when we train the DAE network with two modalities, it is possible to use only one modal in practice. Inspired by these results, EEG signals may not be needed in practice and only some easily detectable signals are used. Finally, intermodal learning tries to find the common characteristics between EEG signals and eye movement data. Experimental results have shown that our common representations show some common characteristics between EEG characteristics and eye movement characteristics."}, {"heading": "6 Conclusions and Future Work", "text": "This work has shown that by merging EEG characteristics and other characteristics with bimodal deep autoencoders (BPCS), the common representations are good features for distinguishing different emotions. In the SEED dataset, the BPCS model is better than others with the best accuracy of 89.94% compared to other feature merging strategies. To avoid complexity in acquiring EEG signals, we used the unimodal deep autoencoder model (DAE) to extract common representations, even though only one modality was available. Experimental results on the unimodal improvement task showed that the DAE model (82.11%) performs better than the direct use of individual models (78.51% in [Lu et al., 2015]). Furthermore, the experimental results of the intermodal learning task showed that the common representations are higher common characteristics between EEG signals and DAE characteristics than the common eye movement characteristics (3.33%)."}, {"heading": "Acknowledgment", "text": "This work was partially supported by grants from the National Natural Science Foundation of China (scholarship # 61272248) and the National Basic Research Program of China (scholarship # 2013CB329401)."}], "references": [{"title": "emotion", "author": ["Margaret M Bradley", "Peter J Lang. Memory"], "venue": "and pupil diameter: Repetition of natural scenes. Psychophysiology, 52(9):1186\u201393,", "citeRegEx": "Bradley and Lang. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "negative", "author": ["Felipe Bravo-Marquez", "Eibe Frank", "Bernhard Pfahringer. Positive"], "venue": "or neutral: Learning an expanded opinion lexicon from emoticon-annotated tweets. In IJCAI\u201915, pages 1229\u2013 1235. AAAI Press,", "citeRegEx": "Bravo.Marquez et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In 2013 6th International IEEE/EMBS Conference on Neural Engineering", "author": ["Ruo-Nan Duan", "Jia-Yi Zhu", "BaoLiang Lu. Differential entropy feature for eeg-based emotion classification"], "venue": "pages 81\u201384. IEEE,", "citeRegEx": "Duan et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Science", "author": ["Jianzeng Guo", "Aike Guo. Crossmodal interactions between olfactory", "visual learning in drosophila"], "venue": "309(5732):307\u2013310,", "citeRegEx": "Guo and Guo. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural Computation", "author": ["Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence"], "venue": "14(8):1771\u20131800,", "citeRegEx": "Hinton. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Deap: A database for emotion analysis", "author": ["Koelstra et al", "2012] Sander Koelstra", "Christian M\u00fchl", "Mohammad Soleymani", "Jong-Seok Lee", "Ashkan Yazdani", "Touradj Ebrahimi", "Thierry Pun", "Anton Nijholt", "Ioannis Patras"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "In SIGIR2015 Workshop on Neuro-Physiological Methods in IR Research", "author": ["Xiang Li", "Peng Zhang", "Dawei Song", "Guangliang Yu", "Yuexian Hou", "Bin Hu. EEG based emotion identification using unsupervised deep feature learning"], "venue": "August", "citeRegEx": "Li et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In 2010 International Conference on Cyberworlds", "author": ["Yisi Liu", "Olga Sourina", "Minh Khoa Nguyen. Real-time EEG-based human emotion recognition", "visualization"], "venue": "pages 262\u2013269. IEEE,", "citeRegEx": "Liu et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In IJCAI\u201915", "author": ["Yifei Lu", "Wei-Long Zheng", "Binbin Li", "Bao-Liang Lu. Combining eye movements", "EEG to enhance emotion recognition"], "venue": "pages 1170\u2013 1176,", "citeRegEx": "Lu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["Murugappan Murugappan", "Nagarajan Ramachandran", "Yaacob Sazali"], "venue": "Classification of human emotion from EEG using discrete wavelet transform. Journal of Biomedical Science and Engineering, 3(04):390\u2013396,", "citeRegEx": "Murugappan et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Distinguishing the roles of trait and state anxiety on the nature of anxiety-related attentional biases to threat using a free viewing eye movement", "author": ["Nelson et al", "2015] Andrea L Nelson", "Christine Purdon", "Leanne Quigley", "Jonathan Carriere", "Daniel Smilek"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "In ICML\u201911", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng. Multimodal deep learning"], "venue": "pages 689\u2013696,", "citeRegEx": "Ngiam et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In 2013 IEEE International Conference on Acoustics", "author": ["Viktor Rozgic", "Shiv N Vitaladevuni", "Ranga Prasad. Robust EEG emotion classification using segment level decision fusion"], "venue": "Speech and Signal Processing, pages 1286\u20131290. IEEE,", "citeRegEx": "Rozgic et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "The Journal of Machine Learning Research", "author": ["Nitish Srivastava", "Ruslan Salakhutdinov. Multimodal learning with deep boltzmann machines"], "venue": "15(1):2949\u20132980,", "citeRegEx": "Srivastava and Salakhutdinov. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In ICML\u201908", "author": ["Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood gradient"], "venue": "pages 1064\u20131071. ACM,", "citeRegEx": "Tieleman. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Multimodal fusion framework: A multiresolution approach for emotion classification and recognition from physiological signals", "author": ["Gyanendra K Verma", "Uma Shanker Tiwary"], "venue": "NeuroImage, 102:162\u2013172,", "citeRegEx": "Verma and Tiwary. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey on human emotion recognition approaches", "author": ["C Vinola", "K Vimaladevi"], "venue": "databases and applications. ELCVIA: electronic letters on computer vision and image analysis, pages 24\u201344", "citeRegEx": "Vinola and Vimaladevi. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Detecting emotions in social media: A constrained optimization approach", "author": ["Yichen Wang", "Aditya Pal"], "venue": "IJCAI\u201915, pages 996\u20131002. AAAI Press,", "citeRegEx": "Wang and Pal. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In IJCAI\u201915", "author": ["Yang Yang", "Han-Jia Ye", "De-Chuan Zhan", "Yuan Jiang. Auxiliary information regularized machine for multiple modality feature learning"], "venue": "pages 1033\u20131039. AAAI Press,", "citeRegEx": "Yang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-modality tracker aggregation: from generative to discriminative", "author": ["Xiaoqin Zhang", "Wei Li", "Mingyu Fan", "Di Wang", "Xiuzi Ye"], "venue": "IJCAI\u201915, pages 1937\u20131943. AAAI Press,", "citeRegEx": "Zhang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on Autonomous Mental Development", "author": ["Wei-Long Zheng", "Bao-Liang Lu. Investigating critical frequency bands", "channels for eeg-based emotion recognition with deep neural networks"], "venue": "7(3):162\u2013175,", "citeRegEx": "Zheng and Lu. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "learned an expanded opinion (positive, neutral and negative) lexicon from emoticon annotated tweets [Bravo-Marquez et al., 2015].", "startOffset": 100, "endOffset": 128}, {"referenceID": 17, "context": "Wang and Pal used constraint optimization framework to discover user\u2019 emotions from social media content [Wang and Pal, 2015].", "startOffset": 105, "endOffset": 125}, {"referenceID": 7, "context": "used fractal dimension based algorithm to recognize and visualize emotions in real time [Liu et al., 2010].", "startOffset": 88, "endOffset": 106}, {"referenceID": 9, "context": "employed discrete wavelet transform to extract frequency features from EEG signals and two classifiers are used to classify the features [Murugappan et al., 2010].", "startOffset": 137, "endOffset": 162}, {"referenceID": 2, "context": "found that differential entropy features are more suited for emotion recognition tasks [Duan et al., 2013].", "startOffset": 87, "endOffset": 106}, {"referenceID": 20, "context": "\u2217 Corresponding author deep neural network to classify EEG signals and examined critical bands and channels of EEG for emotion recognition [Zheng and Lu, 2015].", "startOffset": 139, "endOffset": 159}, {"referenceID": 0, "context": "Bradley and Lang recorded eye movement signals to study the relationship between memory, emotion and pupil diameters [Bradley and Lang, 2015].", "startOffset": 117, "endOffset": 141}, {"referenceID": 18, "context": "proposed an auxiliary information regularized machine which treats different modalities with different strategies [Yang et al., 2015].", "startOffset": 114, "endOffset": 133}, {"referenceID": 19, "context": "proposed a multimodal ranking aggregation framework for fusion of multiple visual tracking algorithms [Zhang et al., 2015].", "startOffset": 102, "endOffset": 122}, {"referenceID": 11, "context": "In [Ngiam et al., 2011], the authors built a single modal deep autoencoder and a bimodal deep autoencoder to generate shared representations of images and audios.", "startOffset": 3, "endOffset": 23}, {"referenceID": 11, "context": "Srivastava and Salakhutdinov extended the methods developed by [Ngiam et al., 2011] to bimodal deep Boltzmann machines to handle multimodal deep learning problems [Srivastava and Salakhutdinov, 2014].", "startOffset": 63, "endOffset": 83}, {"referenceID": 13, "context": ", 2011] to bimodal deep Boltzmann machines to handle multimodal deep learning problems [Srivastava and Salakhutdinov, 2014].", "startOffset": 87, "endOffset": 123}, {"referenceID": 15, "context": "As for multimodal emotion recognition, Verma and Tiwary carried out emotion classification experiments with EEG singals and peripheral physiological signals [Verma and Tiwary, 2014].", "startOffset": 157, "endOffset": 181}, {"referenceID": 8, "context": "used two different fusion strategies for combining EEG and eye movement data: feature level fusion and decision level fusion [Lu et al., 2015].", "startOffset": 125, "endOffset": 142}, {"referenceID": 16, "context": "Vinola and Vimaladevi gave a detailed survey on human emotion recognition and listed many other multimodal datasets and methods [Vinola and Vimaladevi, 2015].", "startOffset": 128, "endOffset": 157}, {"referenceID": 4, "context": "In this paper, we use Contrastive Divergence (CD) algorithm [Hinton, 2002] or Persistent CD algorithm [Tieleman, 2008] to train a RBM.", "startOffset": 60, "endOffset": 74}, {"referenceID": 14, "context": "In this paper, we use Contrastive Divergence (CD) algorithm [Hinton, 2002] or Persistent CD algorithm [Tieleman, 2008] to train a RBM.", "startOffset": 102, "endOffset": 118}, {"referenceID": 20, "context": "The SEED dataset was first introduced in [Zheng and Lu, 2015].", "startOffset": 41, "endOffset": 61}, {"referenceID": 8, "context": "In order to compare our proposed method with [Lu et al., 2015], we use the same data", "startOffset": 45, "endOffset": 62}, {"referenceID": 8, "context": "as in [Lu et al., 2015], that is, 27 data files from 9 subjects.", "startOffset": 6, "endOffset": 23}, {"referenceID": 12, "context": "In order to compare the performance of our proposed method with previous results in [Rozgic et al., 2013] and [Li et al.", "startOffset": 84, "endOffset": 105}, {"referenceID": 6, "context": ", 2013] and [Li et al., 2015], we did not take familiarity into consideration.", "startOffset": 12, "endOffset": 29}, {"referenceID": 8, "context": "As for eye movement data, we used the same features as in [Lu et al., 2015], which were listed in Table 1.", "startOffset": 58, "endOffset": 75}, {"referenceID": 3, "context": "Inspired by [Guo and Guo, 2005], we performed experiments on the following three kinds of emotion recognition tasks to examine the efficiency of our proposed method.", "startOffset": 12, "endOffset": 31}, {"referenceID": 8, "context": "We also compared our results with [Lu et al., 2015].", "startOffset": 34, "endOffset": 51}, {"referenceID": 8, "context": "The middle bars show the results in [Lu et al., 2015], and the right two bars are the results of our DAE model.", "startOffset": 36, "endOffset": 53}, {"referenceID": 8, "context": "In [Lu et al., 2015], the best result achieved when only EEG signal used was 78.", "startOffset": 3, "endOffset": 20}, {"referenceID": 8, "context": "The fourth Fuzzy bar denotes the best result in [Lu et al., 2015].", "startOffset": 48, "endOffset": 65}, {"referenceID": 8, "context": "11% (in Table 4) in comparison with the state-of-the-art approach [Lu et al., 2015] (77.", "startOffset": 66, "endOffset": 83}, {"referenceID": 8, "context": "In [Lu et al., 2015], the authors employed fuzzy integral method to fuse different modalities.", "startOffset": 3, "endOffset": 20}, {"referenceID": 8, "context": "Compared with [Lu et al., 2015], the BDAE model enhanced the performance of affective model significantly.", "startOffset": 14, "endOffset": 31}, {"referenceID": 12, "context": "treated the EEG signals as a sequence of overlapping segments and a novel non-parametric nearest neighbor model was employed to extract response-level feature from these segments [Rozgic et al., 2013].", "startOffset": 179, "endOffset": 200}, {"referenceID": 6, "context": "used Deep Belief Network (DBN) to automatically extract high-level features from raw EEG signals [Li et al., 2015].", "startOffset": 97, "endOffset": 114}, {"referenceID": 6, "context": "We compared the BDAE results with results in [Li et al., 2015] and [Rozgic et al.", "startOffset": 45, "endOffset": 62}, {"referenceID": 12, "context": ", 2015] and [Rozgic et al., 2013].", "startOffset": 12, "endOffset": 33}, {"referenceID": 12, "context": "Valence Arousal Dominance liking [Rozgic et al., 2013] 76.", "startOffset": 33, "endOffset": 54}, {"referenceID": 6, "context": "3 [Li et al., 2015] 58.", "startOffset": 2, "endOffset": 19}, {"referenceID": 8, "context": "51% in [Lu et al., 2015]).", "startOffset": 7, "endOffset": 24}], "year": 2016, "abstractText": "To enhance the performance of affective models and reduce the cost of acquiring physiological signals for real-world applications, we adopt multimodal deep learning approach to construct affective models from multiple physiological signals. For unimodal enhancement task, we indicate that the best recognition accuracy of 82.11% on SEED dataset is achieved with shared representations generated by Deep AutoEncoder (DAE) model. For multimodal facilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE) achieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets, respectively, which are much superior to the state-of-theart approaches. For cross-modal learning task, our experimental results demonstrate that the mean accuracy of 66.34% is achieved on SEED dataset through shared representations generated by EEGbased DAE as training samples and shared representations generated by eye-based DAE as testing sample, and vice versa.", "creator": "LaTeX with hyperref package"}}}