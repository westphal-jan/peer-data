{"id": "1602.08254", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2016", "title": "Theoretical Analysis of the $k$-Means Algorithm - A Survey", "abstract": "The $k$-means algorithm is one of the most widely used clustering heuristics. Despite its simplicity, analyzing its running time and quality of approximation is surprisingly difficult and can lead to deep insights that can be used to improve the algorithm. In this paper we survey the recent results in this direction as well as several extension of the basic $k$-means method.", "histories": [["v1", "Fri, 26 Feb 2016 09:39:50 GMT  (38kb)", "http://arxiv.org/abs/1602.08254v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["johannes bl\\\"omer", "christiane lammersen", "melanie schmidt", "christian sohler"], "accepted": false, "id": "1602.08254"}, "pdf": {"name": "1602.08254.pdf", "metadata": {"source": "CRF", "title": "Theoretical Analysis of the k-Means Algorithm \u2013 A Survey", "authors": ["Johannes Bl\u00f6mer", "Christiane Lammersen", "Melanie Schmidt", "Christian Sohler"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 2.08 254v 1 [cs.D S] 2 6Fe b20 16Theoretical analysis of the k-mean algorithm - A SurveyJohannes Blo \ufffd mer \u043a Christiane Lammersen \u2020 Melanie Schmidt \u2021 Christian Sohler \u00a7 29 February 2016 The k-mean algorithm is one of the most widely used cluster heuristics. Despite its simplicity, the analysis of its runtime and the quality of the approximation is surprisingly difficult and can lead to deep insights that can be used to improve the algorithm. In this paper we examine the latest results in this direction as well as several extensions of the basic k-mean method."}, {"heading": "1 Introduction", "text": "It aims to divide a set of objects into groups called clusters, so that ideally the objects in the same group are similar and objects in different groups are not similar to each other. There are many scenarios in which such a partition is useful. It can be used, for example, to structure the data to enable efficient information retrieval, to reduce the data by replacing a cluster with one or more representatives, or to extract the most important \"topics\" in the data. There are many surveys of clustering algorithms, including well-known classics [45, 48] and more recent [24, 47]. Notice that the title of [47] is data clustering: 50 years beyond K-means in terms of probably the most widely used algorithms of all time. It was proposed by Lloyd in 1957 [58] (and independently in 1956 by Steinhaus] and is the subject of this study."}, {"heading": "2. repeat", "text": "3. P1,.., Pk., Pk., 4. for each p., P., 5. Let i = argmini = 1,..., k., p., Pi., Pi., Pi., Pi., Pi., Pi., Pi., Pi., p., Pi., Pi., Pi., Pi., Pi., Pi., Pi., Pi., Pi., Pi., Pi., Pi., Pi., until the centers do not change. The k mean algorithm is a local heuristic improvement, because replacing the center of a specified Pi number can only improve the solution (see fact 1 below), and then the allocation of points to its nearest center in C. The algorithm converges, but the first important question is how many iterations are necessary until an optimal or good solution is found. The second natural question is how good the solution will be when the algorithm stops. We examine the upper and lower limit of runtime and the quality of the generic algorithm in Section 2. Since the quality of the algorithm depends substantially on the initial part of the generic solution, we can choose the most generic."}, {"heading": "2.1 Analysis of Running Time", "text": "The runtime of the k mean algorithm depends on the number of iterations and the runtime for an iteration. While the runtime for an iteration in n, d, and k is clearly polynomial, this is not obvious (and generally not true) for the number of iterations. In practice, however, it is often observed that the k mean algorithm does not significantly improve after a relatively small number of steps. Therefore, one often performs only a constant number of steps. It is also common to simply stop the algorithm after a given maximum number of iterations, even if it has not converged. Runtime analysis therefore focuses on two things: firstly, what is the asymptotic runtime of an iteration and how it can be accelerated for favorable inputs; secondly, whether there is a theoretical explanation for why the algorithm tends to converge quickly in practice."}, {"heading": "2.1.1 Running Time of One Iteration", "text": "A simple implementation results in the distances in each iteration being set in time BA (ndk) and over the entire input point. We call this the \"naive\" implementation. Asymptotically, the runtime for this is dominated by the number of iterations, which is generally not polyunmically limited. The question is whether and how to avoid calculating the distances between all points and centers, even if this does not lead to an asymptomatic improvement. Consider the following rule, which should be a center in the current iteration: whether and how to avoid calculating the distances between all points and centers, even if these do not lead to an asymptomatic improvement."}, {"heading": "2.1.2 Worst-Case Analysis", "text": "It is known that the number of iterations of algorithms can be limited by the number of partitioning points of Voronoi diagram centers. It is known that the number of iterations of algorithms is limited by the number of partitioning points of Voronoi diagrams."}, {"heading": "2.1.3 Smoothed Analysis", "text": "In order to close this gap between theory and practice, the algorithm was also examined in the smoothed analysis model [12, 15, 63]. This model is particularly useful when both the worst-case analysis and the average case analysis are not realistic and reflect the fact that real-world datasets are likely to contain measurement errors or inaccurate data. In the case of an algorithm, a low time complexity in the smoothed environment has a low runtime on real datasets as well as the fact that we will explain the model in more detail. For the given parameters n and \u03c3, an opponent selects an input instance of the size n. Then, each input point is analyzed by adding a small amount of random noise using a Gaussian distribution with mean 0 and standard deviation, which we execute."}, {"heading": "2.2 Analysis of Quality", "text": "As mentioned above, the k mean algorithm is a local heuristic improvement. It is known that the k mean algorithm converts to a local optimum [70] and that no approximation ratio can be guaranteed [55]. The distances between the first and second, third and fourth points are given in Figure 2. We assume that x < y < z, so that x is the smallest distance and the placement of two centers in Figure 2 mean that the distances between the first and third and fourth points are called x, y and z."}, {"heading": "3.1 Adaptive Sampling", "text": "We assume that it will be an unforeseen way in which there will be an unforeseen, unforeseen, unforeseen situation in which there will be an unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseeable, unforeseeable, probable, probable, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseeable, unforeseeable, unforeseeable, unforeseen, unforeseen, unforeseeable, unforeseeable, unforeseen, unforeseen, unforeseen, unforeseeable, unforeseen,, unforeseeable,,, unforeseeable,, unforeseeable,,, unforeseeable,,, unforeseeable,,, unforeseen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, unforeforeforeforeforeforeforeforeforecoming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, foreforeforeforeforeforeforeforeforecoming,,,,,,,,,,,,,,,,,,,,,,,,, unforefore"}, {"heading": "3.2 Singular Value Decomposition and Best-Fit Subspaces", "text": "There is an interesting result for the SSE problem if we make certain assumptions. (In this context, a series of observations X is made (in our case, points) together with a statistical model that aims to restore these parameters.) A family of density functions over a set of parameters, namely as a problem of learning parameters of mixture models. (In this context, it is assumed that X was generated by the parameter function for a specific parameter and the goal is to restore these parameters. The desired results are parameters that best explain X because they lead to the highest probability that X was drawn. For us, the special case is that the density function is a mixture of Gaussian distributions on R, which is of particular interest."}, {"heading": "6 Complexity of SSE", "text": "Before looking at the variants of the k-mean algorithm, which deals with objective functions that differ from SSE, we conclude our SSE-related study by looking at the complexity of SSE in general. We start by providing proof of the following fact, which we have already applied above. We also reflect the insights it gives us about the structure of optimal solutions to the SSE problem (Fact 3: Let us present the results of a certain P-point and let it come to any point. Then, we have a problem that goes beyond the structure of optimal solutions to the SSE problem."}], "references": [{"title": "On spectral learning of mixtures of distributions. In: COLT", "author": ["D. Achlioptas", "F. McSherry"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Coresets and approximate clustering for bregman divergences", "author": ["M.R. Ackermann", "J. Bl\u00f6mer"], "venue": "Proceedings of the 20th Annual ACM- SIAM Symposium on Discrete Algorithms (SODA", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Bregman clustering for separable instances", "author": ["M.R. Ackermann", "J. Bl\u00f6mer"], "venue": "Proceedings of the 12th Scandinavian Symposium and Workshop on Algorithm Theory (SWAT", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Hardness and non-approximability of bregman clustering problems", "author": ["M.R. Ackermann", "J. Bl\u00f6mer", "C. Scholz"], "venue": "Electronic Colloquium on Computational Complexity (ECCC) 18(15),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Clustering for metric and non-metric distance measures", "author": ["M.R. Ackermann", "J. Bl\u00f6mer", "C. Sohler"], "venue": "ACM Transactions on Algorithms 6(4),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Streamkm++: A clustering algorithm for data streams", "author": ["M.R. Ackermann", "M. M\u00e4rtens", "C. Raupach", "K. Swierkot", "C. Lammersen", "C. Sohler"], "venue": "ACM Journal of Experimental Algorithmics 17,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Adaptive sampling for k-means clustering", "author": ["A. Aggarwal", "A. Deshpande", "R. Kannan"], "venue": "In: APPROX-RANDOM. pp", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Streaming k-means approximation", "author": ["N. Ailon", "R. Jaiswal", "C. Monteleoni"], "venue": "Proceedings of the 22nd Annual Conference on Neural Information Processing Systems. pp", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "NP-hardness of Euclidean sum-ofsquares clustering", "author": ["D. Aloise", "A. Deshpande", "P. Hansen", "P. Popat"], "venue": "Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "An efficient k-means clustering algorithm", "author": ["K. Alsabti", "S. Ranka", "V. Singh"], "venue": "Proceeding of the First Workshop on High-Performance Data Mining", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Learning mixtures of separated nonspherical gaussians", "author": ["S. Arora", "R. Kannan"], "venue": "Annals of Applied Probability", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "k-means has polynomial smoothed complexity", "author": ["D. Arthur", "B. Manthey", "H. R\u00f6glin"], "venue": "Proceedings of the 50th Annual IEEE Symposium on Foundations of Computer Science (FOCS", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "How slow is the k-means method", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the 22nd ACM Symposium on Computational Geometry (SoCG", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Worst-case and smoothed analysis of the ICP algorithm, with an application to the k-means method", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SIAM Journal on Computing", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Stability yields a ptas for k-median and k-means clustering", "author": ["P. Awasthi", "A. Blum", "O. Sheffet"], "venue": "In: FOCS. pp", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "The hardness of approximation of euclidean k-means", "author": ["P. Awasthi", "M. Charikar", "R. Krishnaswamy", "A.K. Sinop"], "venue": "SoCG", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Approximate clustering without the approximation", "author": ["M.F. Balcan", "A. Blum", "A. Gupta"], "venue": "In: SODA. pp", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "On the optimality of conditional expectation as a bregman predictor", "author": ["A. Banerjee", "X. Guo", "H. Wang"], "venue": "Information Theory, IEEE Transactions on 51(7),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Clustering with bregman divergences", "author": ["A. Banerjee", "S. Merugu", "I.S. Dhillon", "J. Ghosh"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Toward learning gaussian mixtures with arbitrary separation", "author": ["M. Belkin", "K. Sinha"], "venue": "COLT. pp", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Learning gaussian mixtures with arbitrary separation", "author": ["M. Belkin", "K. Sinha"], "venue": "CoRR abs/0907.1054", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Polynomial learning of distribution families", "author": ["M. Belkin", "K. Sinha"], "venue": "FOCS. pp. 103\u2013", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A survey of clustering data mining techniques", "author": ["P. Berkhin"], "venue": "Grouping Multidimensional Data,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Streaming k-means on well-clusterable data", "author": ["V. Braverman", "A. Meyerson", "R. Ostrovsky", "A. Roytman", "M. Shindler", "B. Tagiku"], "venue": "SODA. pp", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Isotropic pca and affine-invariant clustering", "author": ["S.C. Brubaker", "S. Vempala"], "venue": "In: FOCS. pp", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Finding metric structure in information theoretic clustering. In: COLT", "author": ["K. Chaudhuri", "A. McGregor"], "venue": "Citeseer", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Learning mixtures of product distributions using correlations and independence", "author": ["K. Chaudhuri", "S. Rao"], "venue": "COLT. pp", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "On coresets for k-median and k-means clustering in metric and euclidean spaces and their applications", "author": ["K. Chen"], "venue": "SIAM Journal on Computing", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Learning mixtures of gaussians", "author": ["S. Dasgupta"], "venue": "In: FOCS. pp", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1999}, {"title": "How fast is k-means", "author": ["S. Dasgupta"], "venue": "In: COLT. p", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2003}, {"title": "The hardness of k-means clustering", "author": ["S. Dasgupta"], "venue": "Tech. Rep. CS2008-0916,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "A probabilistic analysis of em for mixtures of separated, spherical gaussians", "author": ["S. Dasgupta", "L.J. Schulman"], "venue": "Journal of Machine Learning Research", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "A unified framework for approximating and clustering data", "author": ["D. Feldman", "M. Langberg"], "venue": "Proceedings of the 43th Annual ACM Symposium on Theory of Computing (STOC). pp", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "A ptas for k-means clustering based on weak coresets", "author": ["D. Feldman", "M. Monemizadeh", "C. Sohler"], "venue": "Proceedings of the 23rd ACM Symposium on Computational Geometry (SoCG). pp", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Learning mixtures of product distributions over discrete domains", "author": ["J. Feldman", "R. O\u2019Donnell", "R.A. Servedio"], "venue": "SIAM J. Comput", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "BICO: BIRCH Meets Coresets for k-Means Clustering", "author": ["H. Fichtenberger", "M. Gill\u00e9", "M. Schmidt", "C. Schwiegelshohn", "C. Sohler"], "venue": "Proceedings of the 21st European Symposium on Algorithms (ESA). pp", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Coresets in dynamic geometric data streams", "author": ["G. Frahling", "C. Sohler"], "venue": "Proceedings of the 37th STOC. pp", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "Null models in cluster validation. In: From data to knowledge : theoretical and practical aspects of classification, data analysis, and knowledge organization, pp", "author": ["A. Gordon"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1996}, {"title": "Clustering data streams: Theory and practice", "author": ["S. Guha", "A. Meyerson", "N. Mishra", "R. Motwani", "L. O\u2019Callaghan"], "venue": "IEEE Transactions on Knowledge and Data Engineering 15(3),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Accelerating lloyd\u2019s algorithm for k-means clustering", "author": ["G. Hamerly", "J. Drake"], "venue": "Partitional Clustering Algorithms, pp", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Smaller coresets for k-median and k-means clustering", "author": ["S. Har-Peled", "A. Kushal"], "venue": "Discrete & Computational Geometry 37(1),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "On coresets for k-means and k-median clustering", "author": ["S. Har-Peled", "S. Mazumdar"], "venue": "Proceedings of the 36th Annual ACM Symposium on Theory of Computing (STOC", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2004}, {"title": "How fast is the k-means method", "author": ["S. Har-Peled", "B. Sadri"], "venue": "In: SODA. pp", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2005}, {"title": "Clustering Algorithms", "author": ["J.A. Hartigan"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1975}, {"title": "Applications of weighted voronoi diagrams and randomization to variance-based k-clustering (extended abstract)", "author": ["M. Inaba", "N. Katoh", "H. Imai"], "venue": "In: Symposium on Computational Geometry (SoCG", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1994}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters 31(8),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2010}, {"title": "Data clustering: A review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Computing Surveys 31(3),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1999}, {"title": "Approximation algorithms for metric facility location and kmedian problems using the primal-dual schema and lagrangian relaxation", "author": ["K. Jain", "V.V. Vazirani"], "venue": "J. ACM 48(2),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2001}, {"title": "Large-scale parallel data clustering", "author": ["D. Judd", "P.K. McKinley", "A.K. Jain"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 20(8),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1998}, {"title": "Efficiently learning mixtures of two gaussians", "author": ["A.T. Kalai", "A. Moitra", "G. Valiant"], "venue": "In: STOC. pp", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "Spectral algorithms. Foundations and Trends in Theoretical Computer Science", "author": ["R. Kannan", "S. Vempala"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2009}, {"title": "The spectral method for general mixture models", "author": ["R. Kannan", "H. Salmasian", "S. Vempala"], "venue": "SIAM Journal Comput", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2008}, {"title": "An efficient k-means clustering algorithm: Analysis and implementation", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "IEEE Transactions on pattern analysis and machine intelligence 24(7),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2002}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Computational Geometry", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2004}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["A. Kumar", "R. Kannan"], "venue": "Proceedings of the 51st Annual Symposium on Foundations of Computer Science (FOCS", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2010}, {"title": "Linear-time approximation schemes for clustering problems in any dimensions", "author": ["A. Kumar", "Y. Sabharwal", "S. Sen"], "venue": "Journal of the ACM", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2010}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "Bell Laboratories Technical Memorandum", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1957}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Transactions on Information Theory 28(2),", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1982}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J.B. MacQueen"], "venue": "Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1967}, {"title": "The planar k-means problem is nphard", "author": ["M. Mahajan", "P. Nimbhorkar", "K.R. Varadarajan"], "venue": "In: WALCOM. pp", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2009}, {"title": "Worst-case and smoothed analysis of k-means clustering with Bregman divergences", "author": ["B. Manthey", "H. R\u00f6glin"], "venue": "JoCG 4(1),", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}, {"title": "Improved smoothed analysis of the k-means method", "author": ["B. Manthey", "H. R\u00f6lin"], "venue": "Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms. pp. 461\u2013470. Society for Industrial and Applied Mathematics", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2009}, {"title": "On approximate geometric k-clustering", "author": ["J. Matou\u0161ek"], "venue": "Discrete & Computational Geometry 24(1),", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2000}, {"title": "Sparsest cuts and bottlenecks in graphs", "author": ["D.W. Matula", "F. Shahrokhi"], "venue": "Discrete Applied Mathematics 27,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 1990}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["A. Moitra", "G. Valiant"], "venue": null, "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2010}, {"title": "Mixed bregman clustering with approximation guarantees. In: Machine Learning and Knowledge Discovery in Databases", "author": ["R. Nock", "P. Luosto", "J. Kivinen"], "venue": null, "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2008}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy"], "venue": "In: FOCS. pp", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2006}, {"title": "Accelerating exact k -means algorithms with geometric reasoning", "author": ["D. Pelleg", "A.W. Moore"], "venue": "Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 1999}, {"title": "k-means-type algorithms: A generalized convergence theorem and characterization of local optimality", "author": ["S.Z. Selim", "M.A. Ismail"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1984}, {"title": "Sur la division des corps mat\u00e9riels en parties", "author": ["H. Steinhaus"], "venue": "Bulletin de l\u2019Acade\u0301mie Polonaise des Sciences IV(12),", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 1956}, {"title": "Estimating the number of clusters in a dataset via the gap statistic", "author": ["R. Tibshirani", "G. Walther", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 63,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2001}, {"title": "k-means requires exponentially many iterations even in the plane", "author": ["A. Vattani"], "venue": "Proceedings of the 25th ACM Symposium on Computational Geometry (SoCG", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2009}, {"title": "Approximation schemes for clustering problems", "author": ["W.F. de la Vega", "M. Karpinski", "C. Kenyon", "Y. Rabani"], "venue": "Proceedings of the 35th Annual ACM Symposium on Theory of Computing (STOC", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2003}, {"title": "A spectral algorithm for learning mixture models", "author": ["S. Vempala", "G. Wang"], "venue": "J. Comput. Syst. Sci. 68(4),", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2004}, {"title": "Choosing the number of clusters I-III", "author": ["S. Venkatasubramanian"], "venue": "http://blog.geomblog.org/p/conceptual-view-of-clustering.html", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2010}, {"title": "BIRCH: A New Data Clustering Algorithm and Its Applications", "author": ["T. Zhang", "R. Ramakrishnan", "M. Livny"], "venue": "Data Mining and Knowledge Discovery 1(2),", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 1997}], "referenceMentions": [{"referenceID": 44, "context": "There are many surveys on clustering algorithms, including well-known classics [45, 48] and more recent ones [24, 47].", "startOffset": 79, "endOffset": 87}, {"referenceID": 47, "context": "There are many surveys on clustering algorithms, including well-known classics [45, 48] and more recent ones [24, 47].", "startOffset": 79, "endOffset": 87}, {"referenceID": 23, "context": "There are many surveys on clustering algorithms, including well-known classics [45, 48] and more recent ones [24, 47].", "startOffset": 109, "endOffset": 117}, {"referenceID": 46, "context": "There are many surveys on clustering algorithms, including well-known classics [45, 48] and more recent ones [24, 47].", "startOffset": 109, "endOffset": 117}, {"referenceID": 46, "context": "Notice that the title of [47] is Data clustering: 50 years beyond K-means in reference to the k-means algorithm, the probably most widely used clustering algorithm of all time.", "startOffset": 25, "endOffset": 29}, {"referenceID": 57, "context": "It was proposed in 1957 by Lloyd [58] (and independently in 1956 by Steinhaus [71]) and is the topic of this survey.", "startOffset": 33, "endOffset": 37}, {"referenceID": 70, "context": "It was proposed in 1957 by Lloyd [58] (and independently in 1956 by Steinhaus [71]) and is the topic of this survey.", "startOffset": 78, "endOffset": 82}, {"referenceID": 75, "context": "A good introduction to the topic is the overview by Venkatasubramanian [76] as well as Section 5 in the paper by Tibshirani, Walther, and Hastie [72] and the summary by Gordon [39].", "startOffset": 71, "endOffset": 75}, {"referenceID": 71, "context": "A good introduction to the topic is the overview by Venkatasubramanian [76] as well as Section 5 in the paper by Tibshirani, Walther, and Hastie [72] and the summary by Gordon [39].", "startOffset": 145, "endOffset": 149}, {"referenceID": 38, "context": "A good introduction to the topic is the overview by Venkatasubramanian [76] as well as Section 5 in the paper by Tibshirani, Walther, and Hastie [72] and the summary by Gordon [39].", "startOffset": 176, "endOffset": 180}, {"referenceID": 46, "context": "As Jain [47] also notices, the k-means algorithm is still widely used for clustering and in particular for solving the SSE problem.", "startOffset": 8, "endOffset": 12}, {"referenceID": 49, "context": "The example pruning rules are from [50].", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "Different algorithms based on this idea are given in [10, 54, 69].", "startOffset": 53, "endOffset": 65}, {"referenceID": 53, "context": "Different algorithms based on this idea are given in [10, 54, 69].", "startOffset": 53, "endOffset": 65}, {"referenceID": 68, "context": "Different algorithms based on this idea are given in [10, 54, 69].", "startOffset": 53, "endOffset": 65}, {"referenceID": 76, "context": "as a building block of the well-known data stream clustering algorithm BIRCH [77].", "startOffset": 77, "endOffset": 81}, {"referenceID": 40, "context": "For an extensive overview and more pointers to the literature, see [41].", "startOffset": 67, "endOffset": 71}, {"referenceID": 30, "context": "For the special case of d = 1 and k < 5, Dasgupta [31] proved an upper bound of O(n) iterations.", "startOffset": 50, "endOffset": 54}, {"referenceID": 43, "context": "Later, for d = 1 and any k, Har-Peled and Sadri [44] showed an upper bound of O(n\u22062) iterations, where \u2206 is the ratio between the diameter and the smallest pairwise distance of the input points.", "startOffset": 48, "endOffset": 52}, {"referenceID": 43, "context": "In the following, we will explain the idea to obtain the upper bound given in [44].", "startOffset": 78, "endOffset": 82}, {"referenceID": 43, "context": "Figure 1: Illustration of the upper bound for the k-means algorithm [44].", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "Lower Bounds Lower bounds on the worst-case running time of the k-means algorithm have been studied in [13, 31, 73].", "startOffset": 103, "endOffset": 115}, {"referenceID": 30, "context": "Lower Bounds Lower bounds on the worst-case running time of the k-means algorithm have been studied in [13, 31, 73].", "startOffset": 103, "endOffset": 115}, {"referenceID": 72, "context": "Lower Bounds Lower bounds on the worst-case running time of the k-means algorithm have been studied in [13, 31, 73].", "startOffset": 103, "endOffset": 115}, {"referenceID": 30, "context": "Dasgupta [31] proved that the k-means algorithm has a worstcase running time of \u03a9(n) iterations.", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "Using a construction in some \u03a9( \u221a n)-dimensional space, Arthur and Vassilvitskii [13] were able to improve this result to obtain a super-polynomial worst-case running time of 2\u03a9( \u221a n) iterations.", "startOffset": 81, "endOffset": 85}, {"referenceID": 72, "context": "This has been simplified and further improved by Vattani [73] who proved an exponential lower bound on the worst-case running time of the k-means algorithm showing that k-means requires 2\u03a9(n) iterations even in the plane.", "startOffset": 57, "endOffset": 61}, {"referenceID": 72, "context": "In the following, we will give a high-level view on the construction presented in [73].", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "To close this gap between theory and practice, the algorithm has also been studied in the model of smoothed analysis [12, 15, 63].", "startOffset": 117, "endOffset": 129}, {"referenceID": 14, "context": "To close this gap between theory and practice, the algorithm has also been studied in the model of smoothed analysis [12, 15, 63].", "startOffset": 117, "endOffset": 129}, {"referenceID": 62, "context": "To close this gap between theory and practice, the algorithm has also been studied in the model of smoothed analysis [12, 15, 63].", "startOffset": 117, "endOffset": 129}, {"referenceID": 14, "context": "Arthur and Vassilvitskii [15] showed that, in the smoothed setting, the number of iterations", "startOffset": 25, "endOffset": 29}, {"referenceID": 62, "context": "This was improved by Manthey and R\u00f6glin [63] who proved the upper bounds poly(n \u221a k, 1/\u03c3) and kkd \u00b7poly(n, 1/\u03c3) on the number of iterations.", "startOffset": 40, "endOffset": 44}, {"referenceID": 11, "context": "[12] showed that k-means has a polynomial-time smoothed complexity of poly(n, 1/\u03c3).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In the following, we will give a high-level view on the intricate analysis presented in [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 69, "context": "It is known that the k-means algorithm converges to a local optimum [70] and that no approximation ratio can be guaranteed [55].", "startOffset": 68, "endOffset": 72}, {"referenceID": 54, "context": "It is known that the k-means algorithm converges to a local optimum [70] and that no approximation ratio can be guaranteed [55].", "startOffset": 123, "endOffset": 127}, {"referenceID": 54, "context": "[55] illustrate the latter fact by the simple example given in Figure 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "Figure 2: Example illustrating the fact that no approximation guarantee can be given for the k-means algorithm [55].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "Arthur and Vassilvitskii [14] proposed a seeding method for the k-means algorithm which applies adaptive sampling.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "In the end, this process leads to a set of k centers that is an expected O(log k)-approximation [14].", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "[7] show that when sampling O(k) centers instead of k centers, one obtains a constant-factor approximation algorithm for SSE.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18], who proposed the idea to recover a \u2018true\u2019 (but not necessarily optimal) clustering and introduced assumptions under which this is possible.", "startOffset": 0, "endOffset": 4}, {"referenceID": 67, "context": "[68] that we will describe next and triggered a lot of follow-up work on other clustering variants.", "startOffset": 0, "endOffset": 4}, {"referenceID": 67, "context": "[68] analyze adaptive sampling under the following \u03b5-separability: The input is \u03b5-separated if clustering it (optimally) with k \u2212 1 instead of the desired k clusters increases the cost by a factor of at least 1/\u03b52.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The Notice that though we present these results after [14] and [7] for reasons of presentation, the work of Ostrovsky et al.", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "The Notice that though we present these results after [14] and [7] for reasons of presentation, the work of Ostrovsky et al.", "startOffset": 63, "endOffset": 66}, {"referenceID": 67, "context": "[68] appeared first.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] improved this result by giving an algorithm where the approximation guarantee and the separation condition are decoupled, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] developed a streaming algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "On this topic, there has been a lot of research lately, which started by Dasgupta [30] who analyzed the problem under separation conditions.", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 10, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 25, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 27, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 32, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 52, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 74, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 58, "endOffset": 85}, {"referenceID": 21, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 120, "endOffset": 144}, {"referenceID": 20, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 120, "endOffset": 144}, {"referenceID": 22, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 120, "endOffset": 144}, {"referenceID": 35, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 120, "endOffset": 144}, {"referenceID": 50, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 120, "endOffset": 144}, {"referenceID": 65, "context": "Several improvements were made with separation conditions [1, 11, 26, 28, 33, 53, 75] and without separation conditions [22, 21, 23, 36, 51, 66].", "startOffset": 120, "endOffset": 144}, {"referenceID": 55, "context": "However, in [56], the authors prove a result which can be decoupled from this assumption, and the paper proposes an initialization method for the k-means algorithm.", "startOffset": 12, "endOffset": 16}, {"referenceID": 55, "context": "Kumar and Kannan [56] assume a given target clustering which is to be recovered and then show the following.", "startOffset": 17, "endOffset": 21}, {"referenceID": 51, "context": "For an in-depth introduction to spectral algorithms and singular value decompositions, see [52].", "startOffset": 91, "endOffset": 95}, {"referenceID": 43, "context": "Har-Peled and Sadri [44] study a variant of the k-means algorithm in which the assignment step assigns only one misclassified point to the closest cluster center instead of all misclassified points at once as done in the original algorithm.", "startOffset": 20, "endOffset": 24}, {"referenceID": 43, "context": "In the following, we will describe the proof given in [44].", "startOffset": 54, "endOffset": 58}, {"referenceID": 43, "context": "Generalization of Misclassification Har-Peled and Sadri [44] study another variant of the k-means algorithm, which they call Lazy-k-Means.", "startOffset": 56, "endOffset": 60}, {"referenceID": 43, "context": "In the following, we will sketch the proof given in [44].", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "We have already discussed one prime example for this type of algorithm, the k-means++ algorithm by Arthur and Vassilvitskii [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 53, "context": "[54] that we describe in more detail in Section 6.", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "An old variant of the k-means algorithm, proposed independently of Lloyd\u2019s work by MacQueen [60], gives a very fast alternative to the k-means algorithm.", "startOffset": 92, "endOffset": 96}, {"referenceID": 76, "context": "The famous streaming algorithm BIRCH [77] is also very fast and is perceived as producing better clusterings, yet, it still shares the property that there is no approximation guarantee [37].", "startOffset": 37, "endOffset": 41}, {"referenceID": 36, "context": "The famous streaming algorithm BIRCH [77] is also very fast and is perceived as producing better clusterings, yet, it still shares the property that there is no approximation guarantee [37].", "startOffset": 185, "endOffset": 189}, {"referenceID": 28, "context": "Various data stream algorithms for the SSE problem have been proposed, see for example [29, 34, 35, 38, 42, 43], achieving (1 + \u03b5)-approximations in one pass over the data for constant k (and constant d, for some of the algorithms).", "startOffset": 87, "endOffset": 111}, {"referenceID": 33, "context": "Various data stream algorithms for the SSE problem have been proposed, see for example [29, 34, 35, 38, 42, 43], achieving (1 + \u03b5)-approximations in one pass over the data for constant k (and constant d, for some of the algorithms).", "startOffset": 87, "endOffset": 111}, {"referenceID": 34, "context": "Various data stream algorithms for the SSE problem have been proposed, see for example [29, 34, 35, 38, 42, 43], achieving (1 + \u03b5)-approximations in one pass over the data for constant k (and constant d, for some of the algorithms).", "startOffset": 87, "endOffset": 111}, {"referenceID": 37, "context": "Various data stream algorithms for the SSE problem have been proposed, see for example [29, 34, 35, 38, 42, 43], achieving (1 + \u03b5)-approximations in one pass over the data for constant k (and constant d, for some of the algorithms).", "startOffset": 87, "endOffset": 111}, {"referenceID": 41, "context": "Various data stream algorithms for the SSE problem have been proposed, see for example [29, 34, 35, 38, 42, 43], achieving (1 + \u03b5)-approximations in one pass over the data for constant k (and constant d, for some of the algorithms).", "startOffset": 87, "endOffset": 111}, {"referenceID": 42, "context": "Various data stream algorithms for the SSE problem have been proposed, see for example [29, 34, 35, 38, 42, 43], achieving (1 + \u03b5)-approximations in one pass over the data for constant k (and constant d, for some of the algorithms).", "startOffset": 87, "endOffset": 111}, {"referenceID": 39, "context": "[40] develop a framework for clustering algorithms in the data stream setting that they call Stream.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "The experiments included in [40] actually use the SSE criterion to evaluate their results, since the intention is to compare with the k-means algorithm, which is optimized for SSE.", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "Ailon, Jaiswal and Monteleoni [8] use the Stream framework and combine it with different approximation algorithms.", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "[6] develop a streaming algorithm based on k-means++ motivated from a different line of work5.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "1, it is sufficient to sample O(k) centers to obtain a constant factor approximation as later discovered by Aggarwal et al [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 76, "context": "Sufficient statistics The renown algorithm BIRCH7 [77] computes a clustering in one pass over the data by maintaining a preclustering.", "startOffset": 50, "endOffset": 54}, {"referenceID": 36, "context": "[37] develop the algorithm BICO8.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "compare BICO to StreamKM++, BIRCH and MacQueen\u2019s k-means algorithm on the same data sets as in [6] and one additional 128-dimensional data set.", "startOffset": 95, "endOffset": 98}, {"referenceID": 54, "context": "We in particular follow [55].", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "Notice that there exist different proofs for the fact that SSE is NP-hard [9, 32, 61] and the proof presented here is the one due to Aloise et al.", "startOffset": 74, "endOffset": 85}, {"referenceID": 31, "context": "Notice that there exist different proofs for the fact that SSE is NP-hard [9, 32, 61] and the proof presented here is the one due to Aloise et al.", "startOffset": 74, "endOffset": 85}, {"referenceID": 60, "context": "Notice that there exist different proofs for the fact that SSE is NP-hard [9, 32, 61] and the proof presented here is the one due to Aloise et al.", "startOffset": 74, "endOffset": 85}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 64, "context": "This problem is NP-hard because it is equivalent to finding the cut with minimal density in the complement graph, which is known to be NP-hard due to [65].", "startOffset": 150, "endOffset": 154}, {"referenceID": 60, "context": "It is also hard for constant dimension d and arbitrary k [61].", "startOffset": 57, "endOffset": 61}, {"referenceID": 45, "context": "[46].", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[55] show that a simple swapping algorithm suffices.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "also refine their algorithm in two ways: First, they use a result by Matou\u0161ek [64] that says that one can find a set S of size O(n\u03b5\u2212d log(1/\u03b5)) in time O(n log n + n\u03b5\u2212d log(1/\u03b5)) such that the best choice of centers from S is a (1 + \u03b5)approximation of the best choice of centers from Rd.", "startOffset": 78, "endOffset": 82}, {"referenceID": 48, "context": "The first constant approximation algorithm was given by Jain and Vazirani [49] who developed a primal dual approximation algorithm for a related problem and extended it to the SSE setting.", "startOffset": 74, "endOffset": 78}, {"referenceID": 45, "context": "[46] developed the first polynomial-time (1 + \u03b5)approximation algorithm for the case of k = 2 clusters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "Matu\u0161ek [64] improved this and obtained a polynomial-time (1 + \u03b5)-approximation algorithm for constant k and d with running time O(n log n) if \u03b5 is also fixed.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "Further (1 + \u03b5)-approximations were for example given by [29, 74, 34, 38, 43, 57].", "startOffset": 57, "endOffset": 81}, {"referenceID": 73, "context": "Further (1 + \u03b5)-approximations were for example given by [29, 74, 34, 38, 43, 57].", "startOffset": 57, "endOffset": 81}, {"referenceID": 33, "context": "Further (1 + \u03b5)-approximations were for example given by [29, 74, 34, 38, 43, 57].", "startOffset": 57, "endOffset": 81}, {"referenceID": 37, "context": "Further (1 + \u03b5)-approximations were for example given by [29, 74, 34, 38, 43, 57].", "startOffset": 57, "endOffset": 81}, {"referenceID": 42, "context": "Further (1 + \u03b5)-approximations were for example given by [29, 74, 34, 38, 43, 57].", "startOffset": 57, "endOffset": 81}, {"referenceID": 56, "context": "Further (1 + \u03b5)-approximations were for example given by [29, 74, 34, 38, 43, 57].", "startOffset": 57, "endOffset": 81}, {"referenceID": 16, "context": "Recently, Awasthi, Charikar, Krishnaswamy and Sinop [17] showed that there exists an \u03b5 such that it is NP-hard to approximate SSE within a factor of (1 + \u03b5) for arbitrary k and d.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "Shannon entropy Kullback-Leibler divergence [0, 1] \u2211", "startOffset": 44, "endOffset": 50}, {"referenceID": 19, "context": "More precisely, Fact 1 completely carries over to Bregman divergences (see [20]).", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "This was first explicitly stated in [20].", "startOffset": 36, "endOffset": 40}, {"referenceID": 3, "context": "This was first observed in [4] and can be shown in two steps.", "startOffset": 27, "endOffset": 30}, {"referenceID": 26, "context": "Chaudhuri and McGregor [27] give an O(log(n)) approximation algorithm for the Kullback-Leibler divergence (n is the size of the input set P ).", "startOffset": 23, "endOffset": 27}, {"referenceID": 54, "context": "[55] can be generalized to \u03bc-similar Bregman divergences to obtain approximation algorithms with approximation factor 18/\u03bc2 + \u01eb for arbitrary \u01eb > 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "with Matou\u0161ek\u2019s technique [64] to obtain better constant factor approximation algorithms is not known.", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "[5], \u03bc-similarity has been used to obtain a probabilistic (1 + \u01eb)-approximation algorithm for SBE, whose running time is exponential in k, d, 1/\u01eb, and 1/\u03bc, but linear in |P |.", "startOffset": 0, "endOffset": 3}, {"referenceID": 56, "context": "Building upon results in [57], Ackermann at al.", "startOffset": 25, "endOffset": 29}, {"referenceID": 56, "context": "Using the same algorithm as in [57] but a combinatorial rather than geometric analysis, Ackermann et al.", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "In [19] it is shown, that under some mild smoothness conditions, any divergence that satisfies Fact 4 is a Bregman divergence.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Recall that the smoothed complexity of the k-means algorithm is polynomial in n and 1/\u03c3, when each input point is perturbed by random noise generated using a Gaussian distribution with mean 0 and standard deviation \u03c3, a result due to Arthur, Manthey, and R\u00f6glin [12].", "startOffset": 262, "endOffset": 266}, {"referenceID": 61, "context": "For almost any Bregman divergence d\u03a6 Manthey and R\u00f6glin [62] prove two upper bounds on the smoothed complexity of the k-means algorithm.", "startOffset": 56, "endOffset": 60}, {"referenceID": 62, "context": "These bounds match bounds that Manthey and R\u00f6gin achieved for the squared Euclidean distance in [63].", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "[20] already showed a close connection between Bregman divergences and exponential families, indicating that noise chosen according to an exponential distribution may be appropriate for some Bregman divergences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] on the smoothed complexity of the k-means algorithm can be generalized to Bregman divergences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "In [2] the k-means++ randomized seeding algorithm by Arthur and Vassilvitskii [14] is generalized to \u03bc-similar Bregman divergences.", "startOffset": 3, "endOffset": 6}, {"referenceID": 13, "context": "In [2] the k-means++ randomized seeding algorithm by Arthur and Vassilvitskii [14] is generalized to \u03bc-similar Bregman divergences.", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "In [3] Ackermann and Bl\u00f6mer generalize the result by Ostrovsky et a.", "startOffset": 3, "endOffset": 6}, {"referenceID": 67, "context": "[68] on adaptive sampling for \u01eb-separable instances to Bregman divergences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 66, "context": "[67] generalize k-means++ to certain symmetrized versions of Bregman divergences d\u03a6, called mixed Bregman divergences.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Clustering is a basic process in data analysis. It aims to partition a set of objects into groups called clusters such that, ideally, objects in the same group are similar and objects in different groups are dissimilar to each other. There are many scenarios where such a partition is useful. It may, for example, be used to structure the data to allow efficient information retrieval, to reduce the data by replacing a cluster by one or more representatives or to extract the main \u2018themes\u2019 in the data. There are many surveys on clustering algorithms, including well-known classics [45, 48] and more recent ones [24, 47]. Notice that the title of [47] is Data clustering: 50 years beyond K-means in reference to the k-means algorithm, the probably most widely used clustering algorithm of all time. It was proposed in 1957 by Lloyd [58] (and independently in 1956 by Steinhaus [71]) and is the topic of this survey. The k-means algorithm solves the problem of clustering to minimize the sum of squared errors (SSE). In this problem, we are given a set of points P \u2282 Rd in a Euclidean space, and the goal is to find a set C \u2282 Rd of k points (not necessarily included in P ) such that the sum of the squared distances of the points in P to their nearest center in C is minimized. Thus, the objective function to be minimized is", "creator": "LaTeX with hyperref package"}}}