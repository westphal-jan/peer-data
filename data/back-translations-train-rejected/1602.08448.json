{"id": "1602.08448", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2016", "title": "Simple Bayesian Algorithms for Best Arm Identification", "abstract": "This paper considers the optimal adaptive allocation of measurement effort for identifying the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design after a small number of measurements. I propose three simple Bayesian algorithms for adaptively allocating measurement effort. One is Top-Two Probability sampling, which computes the two designs with the highest posterior probability of being optimal, and then randomizes to select among these two. One is a variant a top-two sampling which considers not only the probability a design is optimal, but the expected amount by which it exceeds other designs. The final algorithm is a modified version of Thompson sampling that is tailored for identifying the best design. I prove that these simple algorithms satisfy a strong optimality property. In a frequestist setting where the true quality of the designs is fixed, the posterior is said to be consistent if it correctly identifies the optimal design, in the sense that that the posterior probability assigned to the event that some other design is optimal converges to zero as measurements are collected. I show that under the proposed algorithms this convergence occurs at an exponential rate, and the corresponding exponent is the best possible among all allocation", "histories": [["v1", "Fri, 26 Feb 2016 19:39:01 GMT  (46kb,D)", "https://arxiv.org/abs/1602.08448v1", null], ["v2", "Mon, 29 Feb 2016 07:28:19 GMT  (46kb,D)", "http://arxiv.org/abs/1602.08448v2", null], ["v3", "Wed, 7 Dec 2016 21:30:55 GMT  (70kb,D)", "http://arxiv.org/abs/1602.08448v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel russo"], "accepted": false, "id": "1602.08448"}, "pdf": {"name": "1602.08448.pdf", "metadata": {"source": "CRF", "title": "Simple Bayesian Algorithms for Best-Arm Identification", "authors": ["Daniel Russo"], "emails": [], "sections": [{"heading": null, "text": "An experimenter selects designs one after the other to measure their quality and observes noisy signals of their quality with the aim of identifying the best design after a small number of measurements. This paper proposes three simple and intuitive Bayesian algorithms for adaptive allocation of measurement effort and formalizes a sense in which these seemingly na\u00efve rules are the best possible. A proposal is the double probability sample, in which the two designs are calculated with the highest probability of being optimal, and then randomizes them to choose between the two. One is a variant of the double sample, which takes into account not only the probability that a design is optimal, but also the expected amount by which its quality exceeds that of other designs. The final algorithm is a modified version of Thompson Sampling, which is tailored to identify the best design. We prove that these simple algorithms satisfy a sharp optimality characteristic, in which the true quality of the design is assigned to the optimum event."}, {"heading": "1 Introduction", "text": "This paper considers the optimal adaptive allocation of measurements to determine the share of traffic in the allocation of individual websites? \u2022 It considers the optimal allocation of measurements to individual websites to be uncertain in order to identify the best among a finite number of options or designs. An experimenter selects designs sequentially to measure and observe how independent the signals are of their quality. \u2022 Efficient A / B / C Testing: An e-commerce platform is considering changing its website and would identify the best candidate among many potential new designs. To do this, the platform conducts an experiment, showing different designs that visit different designs. \u2022 It considers the optimal adaptive allocation of measurements to identify the best among a finite number of options or designs. \u2022 An experimenter selects designs sequentially to measure and observe how independent the signals of their quality are. \u2022 Efficient A / B / C Testing: An e-commerce platform is considering changing its website and would identify the best candidate among many potential new designs."}, {"heading": "1.1 Main Contributions", "text": "This paper provides both algorithmic and theoretical contributions. On the algorithmic side, we are developing three new adaptive measurement rules. Specifically, the top Thompson sampling rule could have a direct impact in areas where Thompson's sampling is already in use. For example, there are various reports of Thompson's sampling being used in A / B tests [Scott, 2016] and in clinical trials [Berry, 2004]. However, practitioners in these areas typically hope to commit to a decision after a definitive experimental phase, and the top Thompson sampling can significantly reduce the number of measurements required for this purpose. In addition, due to its simplicity, the proposed assignment rules can be easily adapted to address problems beyond the scope of problem formulation in this paper. See Section 7 for examples. The paper also provides several theoretical contributions. Most importantly, it is of broad scientific interest to understand when very simple measurement strategies are best for convergence."}, {"heading": "1.2 Related Literature", "text": "There is a sophisticated literature on algorithms for Bayesian multi-armed bandit problems. In neglected Bandit problems with independent arms, Gittins indices characterized the Bayes optimal policy [Gittins and Jones, 1974, Gittins, 1979]. In addition, a variety of simpler Bayesian allocation rules have been developed, including Bayesian upper-confidence bound algorithms [Kaufmann et al., 2015], information-directed sampling [Russo et al.], Thompson sampling [Agrawal and Goyal, 2012, Korda et al., 2013, Gopalan et al., 2014, Johnson et al.], the knowledge gradient [Ryzhov et al., 2012], and optimistic indices [Gutin and Farias, 2016]."}, {"heading": "2 Problem Formulation", "text": "Consider the problem of efficiently identifying the best among a finite set of designs, which are based on a finite set of sequential measurements of their quality. (...) At any time, a decision maker is ready to measure the design. (...) The measurement Yn, i, R, associated with design i and time n is determined by a fixed, unknown, probability distribution and the vector Yn, (..., Yn, k) is drawn independently over time. The decision maker chooses a policy or adaptive allocation rule that establishes a (possibly randomized) rule for choosing a design as a function of past observations I1, I1, I1, Yn \u2212 1. The goal is to efficiently identify the design with the utmost importance."}, {"heading": "3 Algorithms", "text": "This section proposes three algorithms for allocating the measurement effort. Each of these algorithms depends on a tuning parameter \u03b2 > 0, which is sometimes set to a default value of 1 / 2. Each algorithm is based on the same principle at the highest level. At each step, each algorithm calculates an estimate of the optimal design, and measures this with probability \u03b2. Otherwise, we are looking at a counterfactual calculation: in the (possibly unlikely) case that I am not the best design, which alternative J 6 = I will most likely be the best design? With probability 1 \u2212 \u03b2, the algorithm measures the alternative J. The algorithms differ in the calculation of I and J. The most mathematically efficient is the modified version of Thompson's sampling, in which I and J = I myself will be the best design. We will see that asymptotically all three algorithms divide the share of the measurement effort between the best measurement effort and the remaining measurement effort less clearly distributed over the collected measurement effort."}, {"heading": "3.1 Top-Two Probability Sampling (TTPS)", "text": "With the probability \u03b2, the politics of the top two probability samples (TTPS) plays the plot I-n = arg maxi \u03b1n, i, which is probably optimal under the rear one. If the algorithm I-n does not play, it plays the most likely alternative J-n = arg maxj 6 = I-n-n-n, j, which is the plot that should be the second most optimal among the rear ones. In other words, the algorithm sets n-n, I-n = \u03b2 and n-n, J-n = 1-\u03b2."}, {"heading": "3.2 Top-Two Value Sampling (TTVS)", "text": "We now propose a variant of the top-two sample that takes into account not only the probability that a design is optimal, but also the expected amount by which its quality exceeds that of other designs. In particular, we define below a measurement variable Vn, i of the value of the design i below the rear distribution at the time n. The top-two value sample then calculates the top-two designs under this measurement variable: I-N = arg maxi Vn, i and J-N = arg maxj 6 = I-n Vn, j. The definition of the top design I-N is then evaluated with the probability \u03b2 and the best alternative J-N otherwise. As the observations are collected, the beliefs are updated and so the two top designs change over time. The measurement of the value Vn, i is defined below. The definition of TTVS depends on a choice of (utility) function and:"}, {"heading": "3.3 Thompson Sampling", "text": "Thompson's sample is an old and popular heuristic for multi-arm problems. The algorithm selects random actions according to the posterior probability with which they are optimal. Specifically, it selects action i with the probability that n, i = \u03b1n, i, where \u03b1n, i denotes the probability action that i is optimal according to a parameter from the posterior distribution. Thompson's sample can exhibit very poor asymptotic performance for the best arm identification problem. Intuitively, this is because once the algorithm estimates that a particular arm is the best with a relatively high probability, it selects that arm in almost all periods of time at the expense of refining its knowledge of other arms."}, {"heading": "3.4 Top-Two Thompson Sampling (TTTS)", "text": "As with TTPS and TTVS, this algorithm depends on a tuning parameter \u03b2 > 0, which is sometimes set to a default value of 1 / 2. As with Thompson sampling, the algorithm will sample a draft I \u0445 \u03b1n at a time.Design I is measured with probability \u03b2, but to prevent the algorithm from focusing solely on one action, an alternative design is measured with probability 1 \u2212 \u03b2. To generate this alternative, the algorithm samples designs J \u0445 \u03b1n until the first time J 6 = I. This can be considered a top-two sampling algorithm, where the top two are selected by executing Thompson sampling until two distinct designs.Under top-two sampling, the probability of measuring design i \u2212 n at a time n is simple."}, {"heading": "3.5 Computing and Sampling According to Optimal Action Probabilities", "text": "Here we offer some insights on how to implement the proposed top two rules efficiently in important problem classes. We start with the top two Thompson sampling values, which are often the easiest to implement. Note that given the ability to sample from these two values, it is easy to sample from the back distribution over the optimal design class. Especially if it is possible to sample from the back row for many interesting models.Algorithm 1 shows how to sample a TTTS action directly. Prior to the back distribution.It is worth noting that this algorithm does not require any calculation or approximation of distribution."}, {"heading": "4 A Numerical Experiment", "text": "Some of the paper's key findings are reflected in a simple numerical experiment. Consider a problem where the observations are binary Yn, i, 1, and the unknown vector. < p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "5 Main Theoretical Results", "text": "Our main theoretical results concern the frequency and rate of convergence of posterior distribution."}, {"heading": "5.1 An upper bound on the error exponent", "text": "Before proceeding, we will give an upper limit for the error exponent if \u03b2 = 1 / 2, which is closely related to complexity concepts found in the best-arm identification literature (e.g. Audibert and Bubeck [2010]), depends on the gaps between the means of the different observation distributions.We say that a real estimated random variable X is equal if E [exp {\u03bb (X \u2212 E [X])}] \u2264 exp {\u03bb2\u03c322}, so that the momentary generating function of X \u2212 E [X] is dominated by that of a ceromeanic random variable with variance \u03c32. Gaussian random variables are sub \u2212 Gaussian variables, since they are uniformly limited random variables. The next result refers to both Bernoulli and Gaussian distributions, as each of them can be parameterized with sufficient statistical T (y)."}, {"heading": "6 Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Asymptotic Notation.", "text": "To simplify the presentation, it is helpful to introduce additional asymptotic notation. We say that two sequences on and bn, which take values in R, are logarithmically equivalent, denoted by a = bn, if 1 n log (an bn) \u2192 0 is n \u00b2. This notation means that an and bn are equal up to the first order in the exponent. Theorem 1 implies with this notation the top two sampling rules with the parameter \u03b2 the convergence rate \u043d. = e \u2212 n\u0432. This is an equivalence relationship, in the sense that if a. = bn and bn. = cn then an. = cn. Note that a + bn. = max. {an, bn}, so that the sequence with the largest exponent dominates. In addition to any positive constant c, can. = an, so that constant multiplications of the sequences are equal up to the first order in the exponent."}, {"heading": "6.2 Posterior Consistency", "text": "The next sentence provides a result of consistency and inconsistency for the rear distribution. The first part states that if design i receives infinite measurement effort, the rear boundary distribution of its quality focuses on the true value of the design, which is not measured infinitely often. The second part states that the rear part does not concentrate on any value, since infinite proofs are collected, but nothing can be excluded with certainty on the basis of finite proofs. Sentence 3. Probability 1: For each i [1,..., k], if it is any value, the rear part focuses on the truth, then for all [1,...,..., k], if it is a value, then for all [1,...,..., k], if it is a value, then it is not empty, but for all [2]."}, {"heading": "6.3 Posterior Large Deviations", "text": "This section provides an asymptotic characterization of the following probabilities: for two parameters, the log probability ratio (log probability ratio), log probability, log probability, log probability, log probability, log probability, log probability, log probability, log probability, log probability, log probability, log probability, log probability, log probability, log probability, the log probability of the observations made (y probability), and then, when the design of the measurement is selected, the probability distribution is determined by sampling."}, {"heading": "6.4 Characterizing the Optimal Allocation", "text": "In this essay, I would like to collect enough evidence to confirm that I am optimal, but since she does not know which measurements provide the most information, it is useful to look at the simplest problem of collecting the most effective evidence when the most effective evidence is known. We can present this as a game between two players: \u2022 An experimenter who knows the true parameters selects a (possibly adaptive) measurement rule. \u2022 An arbitrator observes the resulting sequence of observations (I1, Y1, I1, I1, Yn \u2212 1, In \u2212 1, which determines the optimal sequence of observations. \u2022 How can the experimenter collect the most convincing evidence that is optimal? A rule that is optimal."}, {"heading": "In addition, each Ci is a strictly increasing concave function satisfying", "text": "II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II II"}, {"heading": "6.5 Optimal Adaptive Allocation", "text": "While the last subsection describes an asymptotically optimal exploration strategy, the implementation of this strategy requires knowledge of the parameter vector. A simple approach to achieving the rate (11) is to divide the experiment into two phases. For the first o (n) periods, the algorithm randomly selects measures based on which it constructs a point estimate of the rate. In the second phase, it solves the optimization problem (11) with the allocation to the remaining time periods and follows this allocation. In the conception of sequential experiments, this idea dates back to pine and bags based on the work of Chernoff [1959]. For the problem of best arm identification, it dates at least to Jennison et al. [1982] These two-level rules can be demonstrated to achieve the optimal large deviation rates described in the previous section. But they also have significant practical limitations, which were explicitly discussed in early papers."}, {"heading": "6.6 Asymptotics of the Value Measure", "text": "The proof for the sampling of the top two values is based on the following problem, which shows that the posterior value of a suboptimal design logarithmically corresponds to its probability of being optimal.Lemma 3. For all i 6 = I \u0445, Vn, i. = \u03b1n, iNote, all asymptotic results in relation to the value assigned to suboptimal alternatives below the posterior value could be reformulated by this lemma. The problem is not so surprising, since Vn, i =... \"i vi (\u03b8) and thus all asymptotic results in this context differ from each other only on the basis of the function vi (\u03b8)."}, {"heading": "7 Extensions and Open Problems", "text": "This is an example of how these seemingly na\u00efve algorithms are demonstrated to satisfy a large number of extensions and open questions. The top two sampling measures seem to be a general design principle that can be extended to a variety of problems beyond the scope of this paper. To accelerate research in this area, we briefly discuss a number of extensions and open questions. The top two sampling measures seem to be a general design principle that can be expanded beyond the scope of this paper. Here, we present a version of Toptwo Sampling that uses MAP estimates, which can simplify calculations as MAP estimates are worked out without a solution to the normalizing constant of posterization or density."}, {"heading": "A Outline", "text": "This technical appendix is structured as follows: 1. Section B describes a numerical algorithm that can be used to implement TTP. 2. Section C provides a more precise discussion of related work by Ryzhov [2016].3. The theoretical analysis begins in Section D. There, we begin by noting some basic facts of exponential family distributions as well as some results relating to martyrdom with respect to its quadratic variation process. 4. Section E establishes results relating to the concentration of posterior distribution, including the evidence of Prop. 3, Prop. 4 and Lemma 3.5. Section F examines and simplifies the optimal exponents and the evidence for Prop. 1, Prop. 6, Lemma 2, Prop. 1 and Prop. 2.6."}, {"heading": "B An Implementation of TTPS", "text": "This section describes an implementation of the top two probability components i according to a problem with previous and binary observations. In this problem, measurements are binary with probability of success according to P (Yn, i = 1) = clues. The algorithm begins with an independent prior distribution under which the ith component follows a beta distribution with parameters (in line 19 of algorithm 3.This algorithm uses quadrature to approximate the integral definition. To understand this implementation, we consider a random vector (X1,.., XK) whose components are independently distributed according to scale 19 of algorithm 3.This algorithm uses quadrature to approximate the integral system defining a random vector (X1,.., XK), whose components are independently distributed according to scale Beta (in line 19 of algorithm 3.This algorithm is used). Then the unit of probability i can be calculated according to a measurement component."}, {"heading": "C Discussion of the Expected Improvement Algorithm", "text": "Here we briefly discuss the interesting recent results of Ryzhov [2016]. He examines a setting with an uncorrelated Gaussian prediction and Gaussian observation noise Yn, i \u00b2 \u2212 N (\u03b8i, \u03c32i). To simplify our discussion, we limit attention to the case of the frequent variance \u03c31 =... = \u03c3k = \u03c3. Ryzhov [2016] shows that under the expected improvement algorithm, in the limit as n \u00b2 n \u00b2 n, i = O (logn) (13) and vice versa, i (\u03b8 I \u2212 \u03b8i) 2 \u00b2 n, j \u00b2 n, I \u2212 \u03b8j 2 \u00b2 i, j = I \u0432 (14) Recall that: n, i = n'n, i = n, i denotes the total measurement effort for design i. The sampling ratios (14) are those proposed in the optimal calculation of Chen et al."}, {"heading": "D Preliminaries", "text": "This section presents some basic results used in the subsequent analysis. Firstly, unless clearly stated, all statements concerning random variables are likely to be considered. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}, {"heading": "E Posterior Concentration and anti-Concentration", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "E.1 Uniform Convergence of the Log-Likelihood", "text": "We examine the Logarithmetic and the Mn-Liquidity (n-Liquidity). \u2212 Liquidity (n-Liquidity). \u2212 Liquidity (n-Liquidity). \u2212 Liquidity (n-Liquidity). \u2212 Liquidity (n-Liquidity). \u2212 Liquidity (n-Liquidity). \u2212 Liquidity (n-Liquidity). \u2022 Liquidity (n-Liquidity). \u2022 Liquidity (n-Liquidity). \u2022 Liquidity (n-Liquidity). \u2022 Liquidity (n-Liquidity). \u2022 Liquidity (n-Liquidity). \u2022 Liquidity (n-Liquidity)."}, {"heading": "E.2 Posterior Consistency: Proof of Prop. 3", "text": "Proposition 3: For any i-density, a product is i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density i-density."}, {"heading": "E.3 Large Deviations: Proof of Proposition 4", "text": "All statements in this section apply if observations are drawn below the parameter \"Set Value.\" Since \"Set Value\" (Set Value) Set Value (Set Value) Set Value (Set Value) Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n Set Value (Set Value) n"}, {"heading": "E.4 Large Deviations of the Value Measure: Proof of Lemma 3", "text": "Lemon 3: For all i 6 = me, Vn, i.Proof. First: laterVn, i = vain (vain), i = vain (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain), II (vain)."}, {"heading": "F Simplifying and Bounding the Error Exponent", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "F.1 Proof of Lemma 1", "text": "x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}, {"heading": "F.2 Proof of Proposition 6", "text": "The solution to the optimization problem (12) is the unique mapping. (27) If this is not the case, then it is the case. (27) If this is not the case, then it will be the case. (27) Furthermore, under any other adaptive mapping rule, every function is Ci continuous, and therefore every case must be continuous. (27) If this is not the case, then every protocol will be continuous. (27) If this is the case, then every protocol will be continuous. (27) If this is not the case, then every protocol will be continuous. (27) According to Lemma 1, every function is Ci continuous, and therefore every detail will be continuous."}, {"heading": "F.3 Proof of Lemma 2", "text": "Remember that notation * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "F.4 Sub-Gaussian Bound: Proof of Proposition 1", "text": "The proof of Proposition 1 is based on the following variation form of the Kullback-Leibler divergence shown in Theorem 5.2.1 of the textbook Entropy and Information Theory Gray (2011). Fact 2: Fix two probability measures P and Q defined in a common measurable space. Suppose that the expectation of X under P is well defined and eX under Q.When comparing two normal distributions N (EP [X] \u2212 log EQ [eX]}}, where the premise is taken over all random variables X so that the expectation of X under P is well defined, and eX integral under Q.When comparing two normal distributions N (EP) and N (EQ [eX]}}, the premise is taken over all random variables X."}, {"heading": "F.5 Convergence of Uniform Allocation: Proof of Proposition 2", "text": "Without loss of universality, it is assumed that the problem is parameterized in such a way that the mean value of the design i = \u03b2 \u03b2 \u03b2 = \u03b2 \u03b2 \u03b2 = \u03b2 \u03b2 \u03b2 (2). = exp {\u2212 nmin i = Kong (k \u2212 1, k \u2212 1). From Lemma 1, Ci (k \u2212 1, k \u2212 1) = k \u2212 1d (n), where (n), where (n), where (n), where (n), I (n), I (n), I (n), I (n), I (n), I (n), and (n), where (n), the formula for the KL divergence of the standard Gaussian random variables Ci (k \u2212 1) = (k \u2212 n), I (n)."}, {"heading": "G.2.1 Top-Two Thompson Sampling", "text": "Remember that among the first two Thompson samples for each i-number {1,..., k}, n-number, i = \u03b1n, i \u03b2 + (1 \u2212 \u03b2) \u2211 j 6 = i \u03b1n, j 1 \u2212 \u03b1n, j."}, {"heading": "Proof for TTTS.", "text": "Step 1: Show that (30) n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2, i > 0, which means that n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2, i > 2: Show (30) n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2, i \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n"}, {"heading": "G.2.2 Top-Two Probability Sampling", "text": "Remember that the top two probability sample sets, i.e., I, N = \u03b2 and N, J, N = 1 \u2212 \u03b2, where I, N = arg maxi \u03b1n, I and N = arg maxj 6 = I, n \u03b1n, i are the two designs with the highest probability of being optimal."}, {"heading": "Proof for TTPS.", "text": "Step 1: Show me that I am not able to design myself. Step 1: Show me that I am not able. Step 1: Show me that I am not able to design myself. Step 2: Show me that I am not able to design myself. Step 3: Show me that I am not able to design myself. Step 3: Show me that I am not able to design myself. Step 3: Show me that I am not able to design myself. Step 3: Show me that I am not able to design myself. Step 3: Show me that I am not able to design myself. Step 3: Show me that I am not able to design myself. Step 3: Show me that I am not able to design myself. Step 3: Show me that I am not able to design myself. Step 3: Show me that I am not able to design myself. Step 3: Show me that I am not able to design myself."}, {"heading": "G.2.3 Top-Two Value Sampling", "text": "Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling Sampling"}], "references": [{"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["S. Agrawal", "N. Goyal"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),", "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "The sequential design of experiments for infinitely many states of nature", "author": ["A.E. Albert"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Albert.,? \\Q1961\\E", "shortCiteRegEx": "Albert.", "year": 1961}, {"title": "Best arm identification in multi-armed bandits", "author": ["J.-Y. Audibert", "S. Bubeck"], "venue": "In COLT-23th Conference on Learning Theory-2010, pages 13\u2013p,", "citeRegEx": "Audibert and Bubeck.,? \\Q2010\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2010}, {"title": "A single-sample multiple decision procedure for ranking means of normal populations with known variances", "author": ["R.E. Bechhofer"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Bechhofer.,? \\Q1954\\E", "shortCiteRegEx": "Bechhofer.", "year": 1954}, {"title": "Bayesian statistics and the efficiency and ethics of clinical trials", "author": ["D.A. Berry"], "venue": "Statistical Science,", "citeRegEx": "Berry.,? \\Q2004\\E", "shortCiteRegEx": "Berry.", "year": 2004}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "An empirical evaluation of Thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Chapelle and Li.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Li.", "year": 2011}, {"title": "Simulation budget allocation for further enhancing the efficiency of ordinal optimization", "author": ["C.-H. Chen", "J. Lin", "E. Y\u00fccesan", "S.E. Chick"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "Chen et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2000}, {"title": "Sequential design of experiments", "author": ["H. Chernoff"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Chernoff.,? \\Q1959\\E", "shortCiteRegEx": "Chernoff.", "year": 1959}, {"title": "Approaches in sequential design of experiments. A Survey of Statistical Design and Linear Models (Edited by J. N", "author": ["H. Chernoff"], "venue": null, "citeRegEx": "Chernoff.,? \\Q1975\\E", "shortCiteRegEx": "Chernoff.", "year": 1975}, {"title": "Sequential sampling with economics of selection procedures", "author": ["S.E. Chick", "P. Frazier"], "venue": "Management Science,", "citeRegEx": "Chick and Frazier.,? \\Q2012\\E", "shortCiteRegEx": "Chick and Frazier.", "year": 2012}, {"title": "Economic analysis of simulation selection problems", "author": ["S.E. Chick", "N. Gans"], "venue": "Management Science,", "citeRegEx": "Chick and Gans.,? \\Q2009\\E", "shortCiteRegEx": "Chick and Gans.", "year": 2009}, {"title": "Sequential sampling to myopically maximize the expected value of information", "author": ["S.E. Chick", "J. Branke", "C. Schmidt"], "venue": "INFORMS Journal on Computing,", "citeRegEx": "Chick et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chick et al\\.", "year": 2010}, {"title": "Pac bounds for multi-armed bandit and markov decision processes", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "In Computational Learning Theory,", "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}, {"title": "A fully sequential elimination procedure for indifference-zone ranking and selection with tight bounds on probability of correct selection", "author": ["P.I. Frazier"], "venue": "Operations Research,", "citeRegEx": "Frazier.,? \\Q2014\\E", "shortCiteRegEx": "Frazier.", "year": 2014}, {"title": "A knowledge-gradient policy for sequential information collection", "author": ["P.I. Frazier", "W.B. Powell", "S. Dayanik"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Frazier et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Frazier et al\\.", "year": 2008}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2012}, {"title": "Optimal best arm identification with fixed confidence", "author": ["Aur\u00e9lien Garivier", "Emilie Kaufmann"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Garivier and Kaufmann.,? \\Q2016\\E", "shortCiteRegEx": "Garivier and Kaufmann.", "year": 2016}, {"title": "Bandit processes and dynamic allocation indices", "author": ["J.C. Gittins"], "venue": "J. R. Statist. Soc. B,", "citeRegEx": "Gittins.,? \\Q1979\\E", "shortCiteRegEx": "Gittins.", "year": 1979}, {"title": "A dynamic allocation index for the sequential design of experiments", "author": ["J.C. Gittins", "D.M. Jones"], "venue": "In J. Gani, editor, Progress in Statistics,", "citeRegEx": "Gittins and Jones.,? \\Q1974\\E", "shortCiteRegEx": "Gittins and Jones.", "year": 1974}, {"title": "A large deviations perspective on ordinal optimization", "author": ["P. Glynn", "S. Juneja"], "venue": "In Simulation Conference,", "citeRegEx": "Glynn and Juneja.,? \\Q2004\\E", "shortCiteRegEx": "Glynn and Juneja.", "year": 2004}, {"title": "Ordinal optimization-empirical large deviations rate estimators, and stochastic multi-armed bandits", "author": ["P. Glynn", "S. Juneja"], "venue": "arXiv preprint arXiv:1507.04564,", "citeRegEx": "Glynn and Juneja.,? \\Q2015\\E", "shortCiteRegEx": "Glynn and Juneja.", "year": 2015}, {"title": "Thompson sampling for complex online problems", "author": ["A. Gopalan", "S. Mannor", "Y. Mansour"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Gopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2014}, {"title": "Web-scale Bayesian click-through rate prediction for sponsored search advertising in Microsoft\u2019s Bing search engine", "author": ["T. Graepel", "J.Q. Candela", "T. Borchert", "R. Herbrich"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Graepel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Graepel et al\\.", "year": 2010}, {"title": "Entropy and information theory", "author": ["R.M. Gray"], "venue": null, "citeRegEx": "Gray.,? \\Q2011\\E", "shortCiteRegEx": "Gray.", "year": 2011}, {"title": "Bayesian look ahead one-stage sampling allocations for selection of the best population", "author": ["S.S. Gupta", "K.J. Miescke"], "venue": "Journal of statistical planning and inference,", "citeRegEx": "Gupta and Miescke.,? \\Q1996\\E", "shortCiteRegEx": "Gupta and Miescke.", "year": 1996}, {"title": "Optimistic gittins indices", "author": ["E. Gutin", "V. Farias"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Gutin and Farias.,? \\Q2016\\E", "shortCiteRegEx": "Gutin and Farias.", "year": 2016}, {"title": "Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting", "author": ["K. Jamieson", "R. Nowak"], "venue": "In Information Sciences and Systems (CISS),", "citeRegEx": "Jamieson and Nowak.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson and Nowak.", "year": 2014}, {"title": "Asymptotically optimal procedures for sequential adaptive selection of the best of several normal means", "author": ["C. Jennison", "I.M. Johnstone", "B.W. Turnbull"], "venue": "Statistical decision theory and related topics III,", "citeRegEx": "Jennison et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Jennison et al\\.", "year": 1982}, {"title": "Almost optimal exploration in multi-armed bandits", "author": ["Z. Karnin", "T. Koren", "O. Somekh"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Karnin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karnin et al\\.", "year": 2013}, {"title": "Thompson sampling: an asymptotically optimal finite time analysis", "author": ["E. Kauffmann", "N. Korda", "R. Munos"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "Kauffmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kauffmann et al\\.", "year": 2012}, {"title": "On bayesian index policies for sequential resource allocation", "author": ["E. Kaufmann"], "venue": "arXiv preprint arXiv:1601.01190,", "citeRegEx": "Kaufmann.,? \\Q2016\\E", "shortCiteRegEx": "Kaufmann.", "year": 2016}, {"title": "Information complexity in bandit subset selection", "author": ["E. Kaufmann", "S. Kalyanakrishnan"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Kaufmann and Kalyanakrishnan.,? \\Q2013\\E", "shortCiteRegEx": "Kaufmann and Kalyanakrishnan.", "year": 2013}, {"title": "On Bayesian upper confidence bounds for bandit problems", "author": ["E. Kaufmann", "O. Capp\u00e9", "A. Garivier"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "On the complexity of best arm identification in multiarmed bandit models", "author": ["E. Kaufmann", "O. Capp\u00e9", "A. Garivier"], "venue": "arXiv preprint arXiv:1407.4443,", "citeRegEx": "Kaufmann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2014}, {"title": "Second order efficiency in the sequential design of experiments", "author": ["R. Keener"], "venue": "The Annals of Statistics,", "citeRegEx": "Keener.,? \\Q1984\\E", "shortCiteRegEx": "Keener.", "year": 1984}, {"title": "Asymptotically optimum sequential inference and design", "author": ["J. Kiefer", "J. Sacks"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Kiefer and Sacks.,? \\Q1963\\E", "shortCiteRegEx": "Kiefer and Sacks.", "year": 1963}, {"title": "Selecting the best system", "author": ["S.-H. Kim", "B.L. Nelson"], "venue": "Handbooks in operations research and management science,", "citeRegEx": "Kim and Nelson.,? \\Q2006\\E", "shortCiteRegEx": "Kim and Nelson.", "year": 2006}, {"title": "Recent advances in ranking and selection. In Proceedings of the 39th conference on Winter simulation: 40 years! The best is yet to come, pages 162\u2013172", "author": ["S.-H. Kim", "B.L. Nelson"], "venue": null, "citeRegEx": "Kim and Nelson.,? \\Q2007\\E", "shortCiteRegEx": "Kim and Nelson.", "year": 2007}, {"title": "Thompson sampling for one-dimensional exponential family bandits", "author": ["N. Korda", "E. Kaufmann", "R. Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Korda et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Korda et al\\.", "year": 2013}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "The sample complexity of exploration in the multi-armed bandit problem", "author": ["S. Mannor", "J.N. Tsitsiklis"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Mannor and Tsitsiklis.,? \\Q2004\\E", "shortCiteRegEx": "Mannor and Tsitsiklis.", "year": 2004}, {"title": "Active sequential hypothesis testing", "author": ["M. Naghshvar", "T. Javidi"], "venue": "The Annals of Statistics,", "citeRegEx": "Naghshvar and Javidi,? \\Q2013\\E", "shortCiteRegEx": "Naghshvar and Javidi", "year": 2013}, {"title": "Controlled sensing for multihypothesis testing", "author": ["S. Nitinawarat", "G. K Atia", "V.V. Veeravalli"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Nitinawarat et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nitinawarat et al\\.", "year": 2013}, {"title": "A sequential procedure for selecting the population with the largest mean from k normal populations", "author": ["E. Paulson"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Paulson.,? \\Q1964\\E", "shortCiteRegEx": "Paulson.", "year": 1964}, {"title": "On two-stage selection procedures and related probability-inequalities", "author": ["Y. Rinott"], "venue": "Communications in Statistics-Theory and methods,", "citeRegEx": "Rinott.,? \\Q1978\\E", "shortCiteRegEx": "Rinott.", "year": 1978}, {"title": "Learning to optimize via information-directed sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Russo and Roy.,? \\Q2014\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2014}, {"title": "How much does your data exploration overfit? Controlling bias via information usage", "author": ["D. Russo", "J. Zou"], "venue": "arXiv preprint arXiv:1511.05219,", "citeRegEx": "Russo and Zou.,? \\Q2015\\E", "shortCiteRegEx": "Russo and Zou.", "year": 2015}, {"title": "On the convergence rates of expected improvement methods", "author": ["I.O. Ryzhov"], "venue": "Operations Research,", "citeRegEx": "Ryzhov.,? \\Q2016\\E", "shortCiteRegEx": "Ryzhov.", "year": 2016}, {"title": "The knowledge gradient algorithm for a general class of online learning problems", "author": ["I.O. Ryzhov", "W.B. Powell", "P.I. Frazier"], "venue": "Operations Research,", "citeRegEx": "Ryzhov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ryzhov et al\\.", "year": 2012}, {"title": "Overview of content experiments: Multi-armed bandit experiments, 2016", "author": ["S.L. Scott"], "venue": "URL https: //support.google.com/analytics/answer/2844870?hl=en. [Online; accessed 9-November2016]", "citeRegEx": "Scott.,? \\Q2016\\E", "shortCiteRegEx": "Scott.", "year": 2016}, {"title": "Information-theoretic regret bounds for Gaussian process optimization in the bandit setting", "author": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Srinivas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2012}, {"title": "Automatic ad format selection via contextual bandits", "author": ["L. Tang", "R. Rosales", "A. Singh", "D. Agarwal"], "venue": "In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}, {"title": "Multi-armed bandit models for the optimal design of clinical trials: Benefits and challenges", "author": ["S.S. Villar", "J. Bowden", "J. Wason"], "venue": "Statistical Science,", "citeRegEx": "Villar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Villar et al\\.", "year": 2015}, {"title": "Probability with martingales", "author": ["D. Williams"], "venue": "Cambridge university press,", "citeRegEx": "Williams.,? \\Q1991\\E", "shortCiteRegEx": "Williams.", "year": 1991}, {"title": "As a result, there is no problem with finite k for which the sampling ratios in (14) are optimal. One can show, in fact, that any optimal multi-armed bandit algorithm that attains the lower bound of Lai and Robbins [1985] also satisfies equations (13) and (14). The main innovation in this paper is to show how to build on such bandit algorithms to attain near-optimal rates for the best-arm identification", "author": ["Glynn", "Juneja", "Jennison"], "venue": null, "citeRegEx": "Glynn et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Glynn et al\\.", "year": 1982}, {"title": "2016] also studies the knowledge gradient policy, which could offer improved performance", "author": ["problem. Ryzhov"], "venue": null, "citeRegEx": "Ryzhov,? \\Q2016\\E", "shortCiteRegEx": "Ryzhov", "year": 2016}], "referenceMentions": [{"referenceID": 54, "context": "1 Multi-armed bandit models of clinical trails date back to Thompson [1933], but bandit algorithms lack statistical power in detecting the best treatment at the end of the trial [Villar et al., 2015].", "startOffset": 178, "endOffset": 199}, {"referenceID": 53, "context": "1 Multi-armed bandit models of clinical trails date back to Thompson [1933], but bandit algorithms lack statistical power in detecting the best treatment at the end of the trial [Villar et al.", "startOffset": 60, "endOffset": 76}, {"referenceID": 4, "context": "However, we will see that optimal rules from this perspective also allocate fewer patients to very poor treatments, potentially leading to more ethical trials [Berry, 2004].", "startOffset": 159, "endOffset": 172}, {"referenceID": 50, "context": "For example, there are various reports of Thompson sampling being used in A/B testing [Scott, 2016] and in clinical trials [Berry, 2004].", "startOffset": 86, "endOffset": 99}, {"referenceID": 4, "context": "For example, there are various reports of Thompson sampling being used in A/B testing [Scott, 2016] and in clinical trials [Berry, 2004].", "startOffset": 123, "endOffset": 136}, {"referenceID": 49, "context": ", 2015], information-directed sampling [Russo and Van Roy, 2014], the knowledge gradient [Ryzhov et al., 2012], and optimistic Gittins indices [Gutin and Farias, 2016].", "startOffset": 89, "endOffset": 110}, {"referenceID": 26, "context": ", 2012], and optimistic Gittins indices [Gutin and Farias, 2016].", "startOffset": 40, "endOffset": 64}, {"referenceID": 11, "context": "Recent work has cast this problem in a decision-theoretic framework [Chick and Gans, 2009].", "startOffset": 68, "endOffset": 90}, {"referenceID": 10, "context": ", 2010] or a continuous-time problem with only two alternatives [Chick and Frazier, 2012] \u2013 and then extend those solutions heuristically to build measurement and stopping rules in more general settings.", "startOffset": 64, "endOffset": 89}, {"referenceID": 0, "context": ", 2012, Kaufmann, 2016], Thompson sampling [Agrawal and Goyal, 2012, Korda et al., 2013, Gopalan et al., 2014, Johnson et al., 2015], information-directed sampling [Russo and Van Roy, 2014], the knowledge gradient [Ryzhov et al., 2012], and optimistic Gittins indices [Gutin and Farias, 2016]. These heuristic algorithms can be applied effectively to complicated learning problems beyond the specialized settings in which the Gittins index theorem holds, have been shown to have strong performance in simulation, and have theoretical performance guarantees. In several cases, they are known to attain sharp asymptotic limits on the performance of any adaptive algorithm due to Lai and Robbins [1985]. The pure-exploration problem studied in this paper is not nearly as well understood.", "startOffset": 44, "endOffset": 700}, {"referenceID": 47, "context": "work by Ryzhov [2016] studies the long run distribution of measurement effort allocated by the expected-improvement and shows this is related to the optimal computing budget allocation of Chen et al.", "startOffset": 8, "endOffset": 22}, {"referenceID": 7, "context": "work by Ryzhov [2016] studies the long run distribution of measurement effort allocated by the expected-improvement and shows this is related to the optimal computing budget allocation of Chen et al. [2000]. This contribution is very similar in spirit to this paper, as it relates the longrun behavior of a simple Bayesian measurement strategy to a notion of an approximately optimal allocation.", "startOffset": 188, "endOffset": 207}, {"referenceID": 33, "context": "See Kim and Nelson [2006] and Kim and Nelson [2007] for reviews.", "startOffset": 4, "endOffset": 26}, {"referenceID": 33, "context": "See Kim and Nelson [2006] and Kim and Nelson [2007] for reviews.", "startOffset": 4, "endOffset": 52}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal.", "startOffset": 15, "endOffset": 32}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation.", "startOffset": 15, "endOffset": 552}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/\u03b4) ) samples on average.", "startOffset": 15, "endOffset": 838}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/\u03b4) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound.", "startOffset": 15, "endOffset": 1061}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/\u03b4) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance.", "startOffset": 15, "endOffset": 1173}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/\u03b4) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance. Since Paulson [1964], many authors have sought to reduce the number of samples required on easier problem instances by designing algorithms that sequentially eliminate arms once they are determined to be suboptimal with high confidence.", "startOffset": 15, "endOffset": 1272}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/\u03b4) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance. Since Paulson [1964], many authors have sought to reduce the number of samples required on easier problem instances by designing algorithms that sequentially eliminate arms once they are determined to be suboptimal with high confidence. See the recent work of Frazier [2014] and the references therein.", "startOffset": 15, "endOffset": 1526}, {"referenceID": 3, "context": "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , \u03b4 > 0, the goal is to guarantee with probability at least 1 \u2212 \u03b4 the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance \u03c32, one can guarantee this indifference-zone criterion by gathering O ( (\u03c3k/ 2) log(k/\u03b4) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/\u03b4) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance. Since Paulson [1964], many authors have sought to reduce the number of samples required on easier problem instances by designing algorithms that sequentially eliminate arms once they are determined to be suboptimal with high confidence. See the recent work of Frazier [2014] and the references therein. However, in a sense described below, Jennison et al. [1982] show formally that there are problems with Gaussian observations where any sequential-elimination algorithm will require substantially more samples than optimal adaptive allocation rules.", "startOffset": 15, "endOffset": 1614}, {"referenceID": 20, "context": ", 1982], simulation optimization [Glynn and Juneja, 2004], and, concurrently with this paper, in the machine learning literature [Garivier and Kaufmann, 2016].", "startOffset": 33, "endOffset": 57}, {"referenceID": 17, "context": ", 1982], simulation optimization [Glynn and Juneja, 2004], and, concurrently with this paper, in the machine learning literature [Garivier and Kaufmann, 2016].", "startOffset": 129, "endOffset": 158}, {"referenceID": 11, "context": "We described attainable rates of performance on a worst-case problem instance characterized by Even-Dar et al. [2002] and Mannor and Tsitsiklis [2004].", "startOffset": 95, "endOffset": 118}, {"referenceID": 11, "context": "We described attainable rates of performance on a worst-case problem instance characterized by Even-Dar et al. [2002] and Mannor and Tsitsiklis [2004]. A great deal of work has sought \u201cproblem dependent\u201d bounds, which reveal that the best-arm can be identified more rapidly when the true problem instance is easier.", "startOffset": 95, "endOffset": 151}, {"referenceID": 5, "context": "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation.", "startOffset": 83, "endOffset": 102}, {"referenceID": 5, "context": "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the \u201cfixed-budget\u201d setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge.", "startOffset": 83, "endOffset": 640}, {"referenceID": 5, "context": "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the \u201cfixed-budget\u201d setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge. Later work by Glynn and Juneja [2015] provides a substantial discussion of this issue.", "startOffset": 83, "endOffset": 714}, {"referenceID": 5, "context": "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the \u201cfixed-budget\u201d setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge. Later work by Glynn and Juneja [2015] provides a substantial discussion of this issue. This paper was highly influenced by a classic paper by Chernoff [1959] on the sequential design of experiments for binary hypothesis testing.", "startOffset": 83, "endOffset": 834}, {"referenceID": 5, "context": "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the \u201cfixed-budget\u201d setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge. Later work by Glynn and Juneja [2015] provides a substantial discussion of this issue. This paper was highly influenced by a classic paper by Chernoff [1959] on the sequential design of experiments for binary hypothesis testing. Chernoff\u2019s asymptotic derivations give great insight best-arm identification, which can be formulated as a multiple-hypothesis testing problem with sequentially chosen experiments, but surprisingly this connection does not seem to be discussed in the literature. Chernoff looks at a different scaling than Glynn and Juneja [2004]. Rather than take the budget of available measurements to infinity, he allows the algorithm to stop and declare the hypothesis true or false at any time, but takes the cost of gathering measurements to zero while the cost of an incorrect terminal decision stays fixed.", "startOffset": 83, "endOffset": 1235}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design.", "startOffset": 102, "endOffset": 228}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance.", "startOffset": 102, "endOffset": 335}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit.", "startOffset": 102, "endOffset": 769}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al.", "startOffset": 102, "endOffset": 1467}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings.", "startOffset": 102, "endOffset": 1494}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the \u201cfixed-confidence\u201d setting.", "startOffset": 102, "endOffset": 1865}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the \u201cfixed-confidence\u201d setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone.", "startOffset": 102, "endOffset": 2045}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the \u201cfixed-confidence\u201d setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone. The current paper looks at a different measure. We study a frequentist setting in which the true quality of each design is fixed, and characterize the rate of posterior convergence attainable for each problem instance. We also describe, as a function of the problem instance, the long-run fraction of measurement effort allocated to each design by any algorithm attaining this rate of convergence. These asymptotic limits turn out to be closely related to some of the aforementioned results. In particular, the optimal exponent given in Subsection 6.4 mirrors the complexity measure of Chernoff [1959]. This exponent is then simplified into a form that mirrors one derived by Glynn and Juneja [2004], and, for Gaussian distributions, one derived by Jennison et al.", "startOffset": 102, "endOffset": 2774}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the \u201cfixed-confidence\u201d setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone. The current paper looks at a different measure. We study a frequentist setting in which the true quality of each design is fixed, and characterize the rate of posterior convergence attainable for each problem instance. We also describe, as a function of the problem instance, the long-run fraction of measurement effort allocated to each design by any algorithm attaining this rate of convergence. These asymptotic limits turn out to be closely related to some of the aforementioned results. In particular, the optimal exponent given in Subsection 6.4 mirrors the complexity measure of Chernoff [1959]. This exponent is then simplified into a form that mirrors one derived by Glynn and Juneja [2004], and, for Gaussian distributions, one derived by Jennison et al.", "startOffset": 102, "endOffset": 2872}, {"referenceID": 1, "context": "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 \u2212 \u03b4 > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as \u03b4 \u2192 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the \u201cfixed-confidence\u201d setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the \u201cfixed-confidence\u201d setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone. The current paper looks at a different measure. We study a frequentist setting in which the true quality of each design is fixed, and characterize the rate of posterior convergence attainable for each problem instance. We also describe, as a function of the problem instance, the long-run fraction of measurement effort allocated to each design by any algorithm attaining this rate of convergence. These asymptotic limits turn out to be closely related to some of the aforementioned results. In particular, the optimal exponent given in Subsection 6.4 mirrors the complexity measure of Chernoff [1959]. This exponent is then simplified into a form that mirrors one derived by Glynn and Juneja [2004], and, for Gaussian distributions, one derived by Jennison et al. [1982]. While the", "startOffset": 102, "endOffset": 2944}, {"referenceID": 8, "context": "The allocation rules proposed by Chernoff [1959], Jennison et al.", "startOffset": 33, "endOffset": 49}, {"referenceID": 8, "context": "The allocation rules proposed by Chernoff [1959], Jennison et al. [1982] and Glynn and Juneja [2004] are essentially developed as a means of proving certain rates are attainable asymptotically, and as Chernoff [1975] writes, \u201csidestep the issue of how to experiment in the early stages.", "startOffset": 33, "endOffset": 73}, {"referenceID": 8, "context": "The allocation rules proposed by Chernoff [1959], Jennison et al. [1982] and Glynn and Juneja [2004] are essentially developed as a means of proving certain rates are attainable asymptotically, and as Chernoff [1975] writes, \u201csidestep the issue of how to experiment in the early stages.", "startOffset": 33, "endOffset": 101}, {"referenceID": 8, "context": "The allocation rules proposed by Chernoff [1959], Jennison et al. [1982] and Glynn and Juneja [2004] are essentially developed as a means of proving certain rates are attainable asymptotically, and as Chernoff [1975] writes, \u201csidestep the issue of how to experiment in the early stages.", "startOffset": 33, "endOffset": 217}, {"referenceID": 5, "context": "The work of Bubeck et al. [2009] shows formally that algorithms satisfying regret bounds of order log(n) are necessarily far from optimal for the problem of identifying the best arm.", "startOffset": 12, "endOffset": 33}, {"referenceID": 2, "context": "Audibert and Bubeck [2010]).", "startOffset": 0, "endOffset": 27}, {"referenceID": 8, "context": "As highlighted in the literature review, the max-min problem (7) closely mirrors the main sample complexity term in Chernoff\u2019s classic paper on the sequential design of experiments (Chernoff [1959]).", "startOffset": 116, "endOffset": 198}, {"referenceID": 20, "context": "The results in this proposition are closely related to those in Glynn and Juneja [2004], in which large deviations rate functions take the place of the functions Ci.", "startOffset": 64, "endOffset": 88}, {"referenceID": 33, "context": "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959].", "startOffset": 65, "endOffset": 89}, {"referenceID": 8, "context": "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al.", "startOffset": 116, "endOffset": 132}, {"referenceID": 8, "context": "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982]. These two-stage rules can be shown to attain the optimal large deviations rates described in the previous section.", "startOffset": 116, "endOffset": 226}, {"referenceID": 8, "context": "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982]. These two-stage rules can be shown to attain the optimal large deviations rates described in the previous section. But they also have substantial practical limitations, which were discussed explicitly in early papers. Jennison et al. [1982] writes their proposed procedures \u201ctypically.", "startOffset": 116, "endOffset": 468}, {"referenceID": 8, "context": "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982]. These two-stage rules can be shown to attain the optimal large deviations rates described in the previous section. But they also have substantial practical limitations, which were discussed explicitly in early papers. Jennison et al. [1982] writes their proposed procedures \u201ctypically...do not have good small sample size properties. A better procedure would have several stages and a more sophisticated sampling rule.\u201d In a 1975 review of the sequential design of experiments, Chernoff [1975] notes that asymptotic approaches to the optimal sequential design of experiments had been fairly successful in circumventing the need to compute Bayesian optimal designs via dynamic programming, but \u201cthe approach is very coarse for moderate sample size problems.", "startOffset": 116, "endOffset": 721}, {"referenceID": 8, "context": "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982]. These two-stage rules can be shown to attain the optimal large deviations rates described in the previous section. But they also have substantial practical limitations, which were discussed explicitly in early papers. Jennison et al. [1982] writes their proposed procedures \u201ctypically...do not have good small sample size properties. A better procedure would have several stages and a more sophisticated sampling rule.\u201d In a 1975 review of the sequential design of experiments, Chernoff [1975] notes that asymptotic approaches to the optimal sequential design of experiments had been fairly successful in circumventing the need to compute Bayesian optimal designs via dynamic programming, but \u201cthe approach is very coarse for moderate sample size problems.\u201d He writes that two-stage procedures of Kiefer and Sacks [1963], \u201csidestep the issue of how to experiment in the early stages,\u201d while constructing the optimal allocations based on point estimates \u201ctreats estimates of \u03b8 based on a few observations with as much respect as that based on many observations.", "startOffset": 116, "endOffset": 1048}, {"referenceID": 8, "context": "This closely mirrors optimal results in Chernoff [1959], Jennison et al.", "startOffset": 40, "endOffset": 56}, {"referenceID": 8, "context": "This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016].", "startOffset": 40, "endOffset": 80}, {"referenceID": 8, "context": "This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016]. Does this rule also yield a frequentist probability of incorrect selection that is O(\u03b4) as \u03b4 \u2192 0? More generally, an open problem is to show that when combined with an appropriate stopping rule, top-two sampling schemes nearly minimize the expected number of samples E[\u03c4\u03b4] as in Jennison et al.", "startOffset": 40, "endOffset": 100}, {"referenceID": 8, "context": "This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016]. Does this rule also yield a frequentist probability of incorrect selection that is O(\u03b4) as \u03b4 \u2192 0? More generally, an open problem is to show that when combined with an appropriate stopping rule, top-two sampling schemes nearly minimize the expected number of samples E[\u03c4\u03b4] as in Jennison et al. [1982] or Kaufmann [2016].", "startOffset": 40, "endOffset": 403}, {"referenceID": 8, "context": "This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016]. Does this rule also yield a frequentist probability of incorrect selection that is O(\u03b4) as \u03b4 \u2192 0? More generally, an open problem is to show that when combined with an appropriate stopping rule, top-two sampling schemes nearly minimize the expected number of samples E[\u03c4\u03b4] as in Jennison et al. [1982] or Kaufmann [2016].", "startOffset": 40, "endOffset": 422}], "year": 2016, "abstractText": "This paper considers the optimal adaptive allocation of measurement effort for identifying the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design after a small number of measurements. This paper proposes three simple and intuitive Bayesian algorithms for adaptively allocating measurement effort, and formalizes a sense in which these seemingly naive rules are the best possible. One proposal is top-two probability sampling, which computes the two designs with the highest posterior probability of being optimal, and then randomizes to select among these two. One is a variant of top-two sampling which considers not only the probability a design is optimal, but the expected amount by which its quality exceeds that of other designs. The final algorithm is a modified version of Thompson sampling that is tailored for identifying the best design. We prove that these simple algorithms satisfy a sharp optimality property. In a frequentist setting where the true quality of the designs is fixed, one hopes the posterior definitively identifies the optimal design, in the sense that that the posterior probability assigned to the event that some other design is optimal converges to zero as measurements are collected. We show that under the proposed algorithms this convergence occurs at an exponential rate, and the corresponding exponent is the best possible among all allocation rules.", "creator": "LaTeX with hyperref package"}}}