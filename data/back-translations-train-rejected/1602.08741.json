{"id": "1602.08741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2016", "title": "Gibberish Semantics: How Good is Russian Twitter in Word Semantic Similarity Task?", "abstract": "The most studied and most successful language models were developed and evaluated mainly for English and other close European languages, such as French, German, etc. It is important to study applicability of these models to other languages. The use of vector space models for Russian was recently studied for multiple corpora, such as Wikipedia, RuWac, lib.ru. These models were evaluated against word semantic similarity task. For our knowledge Twitter was not considered as a corpus for this task, with this work we fill the gap. Results for vectors trained on Twitter corpus are comparable in accuracy with other single-corpus trained models, although the best performance is currently achieved by combination of multiple corpora.", "histories": [["v1", "Sun, 28 Feb 2016 16:58:01 GMT  (11kb)", "http://arxiv.org/abs/1602.08741v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nikolay n vasiliev"], "accepted": false, "id": "1602.08741"}, "pdf": {"name": "1602.08741.pdf", "metadata": {"source": "CRF", "title": "Gibberish Semantics: How Good is Russian Twitter in Word Semantic Similarity Task?", "authors": ["Nikolay N. Vasiliev"], "emails": ["vasiliev@spaziodati.eu"], "sections": [{"heading": null, "text": "ar Xiv: 160 2.08 741v 1 [cs.C L] 28 FeKeywords: vector space language model, Word2Vec, semantic similarity"}, {"heading": "1 Introduction", "text": "Word semantic similarity task is an important part of contemporary NLP. It can be applied in many areas, such as literal disambiguation, information retrieval, information extraction and others. It has a long history of improvements, starting with simple models, such as bag-of-words (often weighted by TF-IDF score), continuing with more complex ones, such as LSA [6], which tries to find \"latent\" meanings of words and phrases, and even more abstract models, such as NNLM [3]. Recent results are based on neural networking experiences, but are much simpler: different versions of Word2Vec, Skip-gram and CBOWmodels [8], which currently show the state-of-the-art results and have proven successful with morphologically complex languages such as Russian [1], [10].These are corpus-based approaches where the model is calculated or trained from a large corpus."}, {"heading": "1.1 Goals of this paper", "text": "The primary goal of this paper is to prove the usefulness of the Russian Twitter stream as a resource of semantic similarity (although private accounts are also available). Twitter is a popular social network1, also known as a \"microblogging service,\" which allows users to share and interact with text messages instantly and publicly. Users around the world generate hundreds of millions of tweets in many languages every day, generating vast amounts of verbal data. Traditional corporas for abandoning semantic similarity are news, Wikipedia, electronic libraries, and others (e.g. RUSSE workshop [10]). It has been shown that the type of corpus used for training affects the resulting accuracy. Twitter is not normally considered, and the intuition behind it is that everyday language is probably too simple and too casual to achieve good results."}, {"heading": "1.2 Previous work", "text": "Several \"gold standard\" datasets have been created to facilitate the evaluation of algorithms and models, including WordSim353 [4], RG-65 [11] for the English language, and others. These datasets consist of several pairs of words, with each pair receiving a score from human commentators, representing the similarity between two words, from 0% (not similar) to 100% (identical meaning, words are synonyms). Typically, these datasets are filled in by a number of human commentators, such as 13 in the case of WordSim353 4. Match between commentators is measured and the mean is embedded in datasets. Until recently, there was no such dataset for the Russian language. To mitigate this, \"RUSSE: The First Workshop on Russian Semantic Similarity\" [10] was conducted, with the evaluation of RUSSE Human Judgements."}, {"heading": "2 Data processing", "text": "In this section, we describe how we get data from Twitter, how we filter it, and how we feed it into the model."}, {"heading": "2.1 Acquiring data", "text": "There is a special type of API called Streaming API7 that provides a real-time stream of tweets. The main difference to the regular API is that the connection is maintained for as long as possible and tweets are sent to the customer in real-time. There are three endpoints of the streaming API of our interest: \"Example,\" \"Filter,\" and \"Fire Hose.\" The first provides a sample (random subset) of the full tweet stream, and tweets are sent to the customer in real-time. The second allows you to receive tweets that meet some search criteria: one or 4 http: / / www.cs.technion.il / gabr / resources / wordsim353 / wordsim353.html 5 https: / / / github.com / nlpub / russe-evaluation / hjtest.cv 6 https."}, {"heading": "2.2 Corpus preprocessing", "text": "It is common practice to filter out StopWords (e.g. [5]), but we do not use it in this work. Morphological richness of the Russian language forces us to use stamping, although models like Word2Vec do not require it. In our experiments, the stamp version is much better than the stamp system, so we report only on the results of the stamp system. To do this, we use Yandex Tomita Parser 9, which is an extractor of simple facts from text in Russian. It is based on Yandex stamp symbols [12]. It requires a set of grammar rules and facts (i.e. simple data structures) to be extracted. In this paper, we use it with a simple rule: S \u2212 > Word in t e r p (SimpleFact.Word); 8 https: / / molva / moltova / moltova / master file / httaset / parser file system. \""}, {"heading": "2.3 Training the model", "text": "In this model, word vectors are randomly initialized for each word and fed into a kind of neural network. Word2Vec authors suggest two different models: Skipgram and CBOW. The first is trained to predict the context of the word by giving only the word vector itself. The second model is somewhat opposite: it is trained to predict the word vector by its context. In our study CBOW performs worse and worse than Skip-gram, so we only describe results with Skip-gram model. These models have several training parameters, namely: vector size, vocabulary size (or minimum frequency of a word), context size, threshold of downsampling, amount of training periods. We select vector size based on the size of the corpus. We use \"context size\" as \"number of tokens before or after the current token.\" In all experiments presented in this paper, we use multiple vector sizes based on the size of the corpus."}, {"heading": "3 Experimental results", "text": "In this section we describe the characteristics of Twitter data, describe experiment protocols and results.10 https: / / code.google.com / p / word2vec / 11 https: / / radimrehurek.com / gensim / models / word2vec.html"}, {"heading": "3.1 Properties of the data", "text": "In order to train the Word2Vec model for a semantic similarity task, we collected Twitter messages for 15 full days, from 2015 / 07 / 21 to 2015 / 08 / 04. Each day contains an average of 3M tweets and 40M tokens. All measured properties are in Table 1. Our first observation was that we cannot estimate all words from the HY data set in one day, because they are too rare. We set the frequency threshold at 40 occurrences per day and counted how many words from the HY data set are below this threshold. Our second observation was that words from the HY data set vary from day to day. This is not very surprising given the dynamic nature of Twitter data. Therefore, the estimation of word vectors differs from day to day. To estimate the fluctuation of this semantic measure, we conduct a training of Word2Vec every day."}, {"heading": "3.2 Determining optimal corpus size", "text": "Word2Vec model was designed to be trained on large corpora. There are results of training in reasonable time with corpus size of 1 billion tokens [8]. It was mentioned that the accuracy of estimated word vectors improves with the size of the corpus. Twitter provides an enormous amount of data, therefore, it is a perfect job for Word2Vec. We fix parameters for the model with the following values: vector size of 300, min-freq of 40, context size of 5 and downsampling of 1e-3. We then train our model with 1, 7 and 15 days of Twitter data (each starting with 07 / 21 and followed by following days). The largest corpus of 15 days contains 580M tokens. Results of training are shown in Table 3. In this experiment, the best result includes 7-day corpus with 0.56 day corpus with HJ dataset, and 15-day corpus has slightly less, 0.55."}, {"heading": "3.3 Determining optimal context size", "text": "The authors of Word2Vec [8] and Paragraph Vector [7] advise to determine the optimal context size for each individual training session. In our Twitter corpus, the average length of the sentence appears to be 9.8 with a default size of 4.9, which means that most sentences have less than 20 tokens. This is one of the peculiarities of Twitter data: tweets are limited in size, so sentences are short. Context size greater than 10 is redundant. We opt for word vectors with three different context sizes: 2, 5, 10. We do two training rounds: firstly with Twitter data from the days of July 21 to 25, and secondly from July 26 to 30. Results of measuring the correlation with HJ data sets are shown in Table 4. According to these results, the context size of 5 is slightly better than others, but the difference is negligible compared to the fluctuation between several training attempts."}, {"heading": "3.4 Some further observations", "text": "The vector space model is capable of providing more information than just measuring the semantic distance of two predefined words. It has been shown that word vectors can exhibit several degrees of similarity. In particular, it is possible to model simple relationships, such as \"country\" - \"capital,\" gender, syntactic relationships with algebraic operations on these vectors. [8] The authors suggest to judge the quality of these vectors by the task of accurately predicting these word relationships. However, word vectors learned from Twitter seem to perform this task poorly. We are not doing systematic research on this subject here, because they go beyond the scope of the current paper, although they represent an important direction for future studies. Twitter posts often contain three specific types of words: usernames, hashtags, and hyperlinks. It may be advantageous to filter them (consider them stop words). In results presented in this paper, especially in Tables 3 and 4, we do not show such words."}, {"heading": "4 Conclusion", "text": "In this paper, we examined the use of the Twitter corpus to build the Word2Vec model to abandon semantic similarity of words. We described a method for obtaining Twitter messages and preparing them for training. We use the HJ dataset created for the RUSSE competition [10] to measure the correlation between similarity of word vectors and human judgments on similarity of word pairs. We obtain results comparable to results obtained in training Word2Vec on traditional corporations such as Wikipedia and websites [1]. [5] This is particularly important because Twitter data is highly dynamic and traditional sources are mostly static (rarely changed over time)."}], "references": [{"title": "Evaluating three corpus-based semantic similarity systems for russian", "author": ["I. Panchenko A", "V. Romanov P"], "venue": "Dialog 2015,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Ukp: Computing semantic textual similarity by combining multiple content similarity measures", "author": ["Daniel B\u00e4r", "Chris Biemann", "Iryna Gurevych", "Torsten Zesch"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational SemanticsVolume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Texts in, meaning out: Neural language models in semantic similarity tasks for russian. Dialog 2015, Russia", "author": ["A. Andreev I. Kutuzov"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "An introduction to latent semantic analysis", "author": ["Thomas K Landauer", "Peter W Foltz", "Darrell Laham"], "venue": "Discourse processes,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["George A Miller", "Walter G Charles"], "venue": "Language and cognitive processes,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "RUSSE: The First Workshop on Russian Semantic Similarity", "author": ["A. Panchenko", "N.V. Loukachevitch", "D. Ustalov", "D. Paperno", "C.M. Meyer", "N. Konstantinova"], "venue": "In Computational Linguistics and Intellectual Technologies: papers from the Annual conference \u201cDialogue\u201d,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1965}, {"title": "A fast morphological algorithm with unknown word guessing induced by a dictionary for a web search engine", "author": ["Ilya Segalovich"], "venue": "In MLMTA,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Analysis of the wikipedia category graph for nlp applications", "author": ["Torsten Zesch", "Iryna Gurevych"], "venue": "In Proceedings of the TextGraphs-2 Workshop (NAACL-HLT", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Using wiktionary for computing semantic relatedness", "author": ["Torsten Zesch", "Christof M\u00fcller", "Iryna Gurevych"], "venue": "In AAAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}], "referenceMentions": [{"referenceID": 5, "context": "It has long history of improvements, starting with simple models, like bag-of-words (often weighted by TF-IDF score), continuing with more complex ones, like LSA [6], which attempts to find \u201clatent\u201d meanings of words and phrases, and even more abstract models, like NNLM [3].", "startOffset": 162, "endOffset": 165}, {"referenceID": 2, "context": "It has long history of improvements, starting with simple models, like bag-of-words (often weighted by TF-IDF score), continuing with more complex ones, like LSA [6], which attempts to find \u201clatent\u201d meanings of words and phrases, and even more abstract models, like NNLM [3].", "startOffset": 271, "endOffset": 274}, {"referenceID": 7, "context": "Latest results are based on neural network experience, but are far more simple: various versions of Word2Vec, Skip-gram and CBOWmodels [8], which currently show the State-of-the-Art results and have proven success with morphologically complex languages like Russian [1], [10].", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "Latest results are based on neural network experience, but are far more simple: various versions of Word2Vec, Skip-gram and CBOWmodels [8], which currently show the State-of-the-Art results and have proven success with morphologically complex languages like Russian [1], [10].", "startOffset": 266, "endOffset": 269}, {"referenceID": 9, "context": "Latest results are based on neural network experience, but are far more simple: various versions of Word2Vec, Skip-gram and CBOWmodels [8], which currently show the State-of-the-Art results and have proven success with morphologically complex languages like Russian [1], [10].", "startOffset": 271, "endOffset": 275}, {"referenceID": 13, "context": "They usually make use of knowledge databases, like WordNet, Wikipedia, Wiktionary and others [14], [2].", "startOffset": 93, "endOffset": 97}, {"referenceID": 1, "context": "They usually make use of knowledge databases, like WordNet, Wikipedia, Wiktionary and others [14], [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 12, "context": "It was shown that Wikipedia data can be used in graph-based methods [13], and also in corpus-based ones.", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "RUSSE workshop [10]).", "startOffset": 15, "endOffset": 19}, {"referenceID": 3, "context": "Several \"Gold standard\" datasets were produced to facilitate the evaluation of algorithms and models, including WordSim353 [4], RG-65 [11] for English language and others.", "startOffset": 123, "endOffset": 126}, {"referenceID": 10, "context": "Several \"Gold standard\" datasets were produced to facilitate the evaluation of algorithms and models, including WordSim353 [4], RG-65 [11] for English language and others.", "startOffset": 134, "endOffset": 138}, {"referenceID": 9, "context": "To mitigate this the \u201cRUSSE: The First Workshop on Russian Semantic Similarity\u201d[10] was conducted, producing RUSSE Human-Judgements evaluation dataset (we will refer to it as HJ-dataset).", "startOffset": 79, "endOffset": 83}, {"referenceID": 8, "context": "Firstly, datasets WordSim353, MC [9] and RG-65 were combined and translated.", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "The RUSSE contest was followed by paper from its organizers [10] and several participators [1], [5], thus filling the gap in word semantic similarity task for Russian language.", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "The RUSSE contest was followed by paper from its organizers [10] and several participators [1], [5], thus filling the gap in word semantic similarity task for Russian language.", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "The RUSSE contest was followed by paper from its organizers [10] and several participators [1], [5], thus filling the gap in word semantic similarity task for Russian language.", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "[5]), but in this work we don\u2019t use it.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "It is based on Yandex stemmer mystem[12].", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "To compute correlation we use Spearman coefficient, since it was used as accuracy measure in RUSSE [10].", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "There are results of training it in reasonable time with corpus size of 1 billion of tokens [8].", "startOffset": 92, "endOffset": 95}, {"referenceID": 9, "context": "For convenience we replicate results for these corpora, originally presented in [10], alongside with our result in Table 5.", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "Authors of Word2Vec [8] and Paragraph Vector [7] advise to determine the optimal context size for each distinct training session.", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "Authors of Word2Vec [8] and Paragraph Vector [7] advise to determine the optimal context size for each distinct training session.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "Authors of [8] propose to assess quality of these", "startOffset": 11, "endOffset": 14}, {"referenceID": 9, "context": "We use HJ-dataset, which was created for RUSSE contest [10] to measure correlation between similarity of word vectors and human judgements on word pairs similarity.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "We achieve results comparable with results obtained while training Word2Vec on traditional corpora, like Wikipedia and Web pages [1], [5].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "We achieve results comparable with results obtained while training Word2Vec on traditional corpora, like Wikipedia and Web pages [1], [5].", "startOffset": 134, "endOffset": 137}], "year": 2016, "abstractText": "The most studied and most successful language models were developed and evaluated mainly for English and other close European languages, such as French, German, etc. It is important to study applicability of these models to other languages. The use of vector space models for Russian was recently studied for multiple corpora, such as Wikipedia, RuWac, lib.ru. These models were evaluated against word semantic similarity task. For our knowledge Twitter was not considered as a corpus for this task, with this work we fill the gap. Results for vectors trained on Twitter corpus are comparable in accuracy with other single-corpus trained models, although the best performance is currently achieved by combination of multiple corpora.", "creator": "LaTeX with hyperref package"}}}