{"id": "1602.08952", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Feb-2016", "title": "Representation of linguistic form and function in recurrent neural networks", "abstract": "We present novel methods for analysing the activation patterns of RNNs and identifying the types of linguistic structure they learn. As a case study, we use a multi-task gated recurrent network model consisting of two parallel pathways with shared word embeddings trained on predicting the representations of the visual scene corresponding to an input sentence, and predicting the next word in the same sentence. We show that the image prediction pathway is sensitive to the information structure of the sentence, and pays selective attention to lexical categories and grammatical functions that carry semantic information. It also learns to treat the same input token differently depending on its grammatical functions in the sentence. The language model is comparatively more sensitive to words with a syntactic function. Our analysis of the function of individual hidden units shows that each pathway contains specialized units tuned to patterns informative for the task, some of which can carry activations to later time steps to encode long-term dependencies.", "histories": [["v1", "Mon, 29 Feb 2016 13:31:17 GMT  (5317kb,D)", "http://arxiv.org/abs/1602.08952v1", null], ["v2", "Wed, 8 Jun 2016 12:30:12 GMT  (1798kb,D)", "http://arxiv.org/abs/1602.08952v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["\\'akos k\\'ad\\'ar", "grzegorz chrupa{\\l}a", "afra alishahi"], "accepted": false, "id": "1602.08952"}, "pdf": {"name": "1602.08952.pdf", "metadata": {"source": "META", "title": "Representation of linguistic form and function in recurrent neural networks", "authors": ["\u00c1kos K\u00e1d\u00e1r", "Grzegorz Chrupa\u0142a", "Afra Alishahi"], "emails": ["a.kadar@uvt.nl", "g.chrupala@uvt.nl", "a.alishahi@uvt.nl"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to move to another world in which they are able to live."}, {"heading": "2 Related work", "text": "In recent years, it has been shown that most of them are people who are able to survive themselves, and that they are able to survive themselves by going in search of themselves. In recent years, the number of people who are able to survive has multiplied. In recent years, the number of people who are able to survive has multiplied, and in recent years, the number of people who are able to survive and survive has multiplied."}, {"heading": "3 Models", "text": "One of the main difficulties in the formation of traditional Elman networks arises from the fact that they overwrite their hidden states at each step with a new value, which is ht from the current input text state and the previous hidden state \u2212 1. Similar to LSTMs, gated recurrent unit networks introduce a mechanism that facilitates the retention of information in several time steps. Specifically, the GRU calculates the hidden state in the current time step, because the linear combination of previous activation ht \u2212 1 and a new activation of candidate h-t: GRU (ht \u2212 1, xt) = (1 \u2212 zt) ht \u2212 1 + zt h-t (1), with elementary multiplication occurring, and the activation of the update gate zt determines the amount of new information mixed in the current state: zt = \u03c3s (Wzxt + Uzht \u2212 1) (2) Activation of the candidate is calculated as: h-t = \u03c3 (Wate + U rt \u2212 1) as the previous state is inserted \u2212 1 (the Gxt \u2212 1)."}, {"heading": "3.1 Imaginet", "text": "IMAGINET, introduced in Chrupa\u0142a et al. (2015), is a multimodal GRU network architecture that learns visually based representations of meaning from textual and visual inputs. It consists of two GRU paths, TEXTUAL and VISUAL, with a common word-embedding matrix. The inputs into the model are picture description pairs and associated images. Each sentence is assigned to two sequences of hidden states, one according to VISUAL and the other according to TEXTUAL: hVt = GRU V (hVt \u2212 1, xt) (5) hTt = GRU T (hTt \u2212 1, xt) (6) Each time TEXTUAL predicts the next word in sentence S from its current hidden state hTt, while VISUAL predicts the image vector i (hVt \u2212 1, xt) (6) from its last hidden display hVt \u2212 VT (VT)."}, {"heading": "4 Experiments", "text": "In the following sections, we report on a series of experiments in which we develop methods to investigate the types of linguistic laws that TEXTUAL and 1Our version of the model has two minor modifications compared to the original IMAGINET: We use cosinal distance as visual loss, and we use image vectors in which each dimension is transformed by subtracting the mean and dividing it by standard deviation. VISUAL IMAGINET paths learn from inputs at the word level. Section 5 presents the macro-plane analyses of the last hidden activation vectors of the recurring paths. Section 6 reports on exploratory experiments on the linguistic characteristics encoded by individual hidden units. For all experiments, we trained IMAGINET on the training part of the image signature dataset MSCOCO (Lin et al., 2014) and analyzed the representations of the sentences in the validation group encoded by individual hidden units."}, {"heading": "5 Analysis of hidden activation vectors", "text": "In this section, we propose a novel technique for interpreting the activation patterns of RNNs, which are used for measuring VAL values from a linguistic point of view, and focus on the high level of understanding on which types of words networks pay the most attention to. Complete sentences are represented by the activation vector at the end of the sentence symbol (hend). We measure the meaning of each word Si in an input sentence S1: n based on how much the representation of the subsentence S\\ i = S1: i \u2212 1Si + 1: n, with the omitted word Si, differs from the original sentence representation. For example, the distance between hend (the black dog runs) and hend (the dog runs) determines the meaning of the black sentence in the first sentence. We perform the measurement omission (i, S) for estimating the meaning of a word ISAL: omission (i, S) = cosine."}, {"heading": "5.1 Omission score distributions", "text": "Figure 2 shows the distribution of deletion values per POS and deprel labels for the two paths. The general trend is that for the VISUAL path, the deletion values are very high for a small subset of labels (which correspond to content words) and low for the rest. For TEXTUAL, the differences are smaller, and the path appears to be sensitive to the deletion of most types of words. Figure 3 compares the two paths directly based on the protocol of the ratio of VISUAL to TEXTUAL deletion values and records the distribution of this ratio for various POS and Deprel labels. Protocol ratios above zero point to a stronger association with the VISUAL path and below zero with the TEXTUAL path. We see that VISUAL is relatively more sensitive to nouns (NNP, NNS, NN, NUUM), numbers (CD) and JUJ and JUJ adjectives (JAUJ and JUJ), and JTUAL (some ISP) relationships."}, {"heading": "5.2 Sensitivity to grammatical function", "text": "In this section, we will examine the extent to which VISUAL distinguishes between the events of a particular content word in different grammatical functions. To quantify this, we will adjust two linear models that predict the omissions per token: MODELL 1 uses only the word type as a predictor, while MODELL 2 uses the word type, dependence, and interaction between the two. MODELL 2 adapts the data better than MODELL 1, then this indicates that for a particular word type, the network is sensitive to its grammatical function, and that it learns to pay attention to aspects of the sentence structure over and over lexical clues. We will divide the whole MSCOCO validation into two parts, fit the two models (regulated by L2 penalty)."}, {"heading": "5.3 Sensitivity to information structure", "text": "In English, the information structure (or pragmatic structure) of a sentence is expressed in linear order: the TOPIC of a sentence appears in sentences, and the COMMENT follows. In this section, we examine the extent to which the recurring paths of IMAGINET learn this information order. Figure 6 shows the distribution of the omission values depending on the linear position in the sentence and the POS labeling for both VISUAL and TEXTUAL. Since the omission values are measured at the end of the sentence, it is expected that for TEXTUAL as a language model, the words appearing closer to the end of the sentence would have a stronger effect on the omission values. This seems to be the case in Figure 6 (above) and is confirmed by the Pearson correlation coefficient of 0.24. For the VISUAL model, it is less clear what to expect: on the one hand, due to the chain structure of RNNs, they are better at tracking the short distance than the long distance between the subject (001 and the task)."}, {"heading": "6 Analysis of hidden units", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Top K contexts", "text": "The aim of this section is to gain an understanding of what the individual hidden dimensions in general encode. We are developing a simple method that we duplicate according to the uppermost K images by Zhou et al. (2015). We have passed each set of a corpus token by token through the RNN and the hidden state is for each unit at the time step. This results in an activation matrix M-Rd \u00b7 n, where d is the number of hidden dimensions and n is the total number of time steps (or tokens) in the entire corpus. Each cell With in the resulting matrix represents the activation value of the ith unit for some tokens at the time step. Starting from the assumption that high activation values indicate meaning, we sort the rows of the activation matrix by the size of the activations, resulting in the uppermost K contexts for each unit."}, {"heading": "6.2 Specialized hidden units", "text": "Table 1 shows the top 5 trigram contexts with the highest activations for five hidden units for VISUAL and TEXTUAL. It shows the general pattern that the individual dimensions react highly sensitively to contexts with syntactically and / or topically related patterns. For example, the top 20 trigram contexts for the first hidden unit of VISUAL in Table 1 include all brands that are thematically related to home electronics, such as phones, remote controls and camera parts. The top 20 5 gram contexts for this unit include: cell phone computers and rubber, all attached to wires such as pearls and cords. More interestingly, the first hidden unit for TEXTUAL in Table 1 appears to be highly active for a combined syntactic and semantic template: contexts, including a token corresponding to a vehicle, followed by a transport verb."}, {"heading": "6.3 Units predictive of a grammatical function", "text": "To explore the syntactic functions encoded by special dimensions, we train two logistic regression models (one for VISUAL and one for TEXTUAL) to predict the dependency designation of a token in the time step t. The models use two sets of predictors: \u2022 hidden activation vectors hVt or hTt \u2022 n-gram features up to a window size of 4; for example, to predict the label of a dog in the sentence the nice dog we use the 2, nice1, dog0, the 2 nice1, nice1 dog0, the 2 nice1 dog0, the 2 nice1 dog0, etc. For both VISUAL and TEXTUAL, we determine the hidden units that are predictive of the grammatical function by using the top 5 logistic regression coefficients \u03b2V and \u03b2T per dependency designation corresponding to the dimensions of hVt and h T."}, {"heading": "6.4 Units carrying over information", "text": "Further examination of some of the highly activated units reveals some that are predictions of demarches that require information about the identities or grammatical functions of previous symbols. For example, predictive units for POBJ generalize in the TEXTUAL path in Table 2 about the prepositions on and in, and contexts in the Top 20 include additional prepositions, such as around a dirt and on a grass. Interestingly, however, all the top 20 contexts belong to the PREP DET POBJ construction, where the object has to do with the outside world. For VISUAL, one of the top units for the CONJ category has top contexts with green and the table and, colored circles and wooden furniture and one. Given that the value of this dimension predicts the presence of a conjunction in the current time unit, this particular dimension seems to carry its high activation value into the next step."}, {"heading": "6.5 Comparison of models based on top contexts", "text": "The results in Section 5 highlight some of the differences between TEXTUAL and VISUAL. We see that VISUAL learns to pay more attention to satisfied words and TEXTUAL learns to pay more attention to words with purely grammatical functions. In this section, we explore a further comparison between the models based on their different objectives, the activity of the hidden dimensions of VISUAL learns to pay more attention to the semantic relationships between the contexts, while the dimensions of the sentence are based on the hypothesis that the activity of the hidden dimensions is characterized by VISUAL."}, {"heading": "7 Conclusion", "text": "Our analysis of the hidden activation patterns shows that the VISUAL model learns an abstract representation of the information structure of the language and pays selective attention to lexical categories and grammatical functions that contain semantic information. In contrast, the TEXTUAL language model is sensitive to syntactic categories. We have also shown that each network contains specialized units that are adapted to both lexical and structural patterns that are useful for the respective task, some of which can transfer activations to later time steps to encode long-term dependencies. In the future, we would like to apply our techniques to analyze the encoding of linguistic forms and functions in order to recurrent neural models trained for different purposes, such as neural machine translation systems (e.g. Sutskever et al al al al al al al al al al al al, 2014) or the explicit understanding of sentences that enhance the attention of the system Kiural Models-Function-2015 et al."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representation (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Sta-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning language through pictures", "author": ["Grzegorz Chrupa\u0142a", "\u00c1kos K\u00e1d\u00e1r", "Afra Alishahi."], "venue": "ACL.", "citeRegEx": "Chrupa\u0142a et al\\.,? 2015", "shortCiteRegEx": "Chrupa\u0142a et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "NIPS 2014 Deep Learning and Representation Learning Workshop.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Understanding deep image representations by inverting them", "author": ["Alexey Dosovitskiy", "Thomas Brox."], "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Dosovitskiy and Brox.,? 2015", "shortCiteRegEx": "Dosovitskiy and Brox.", "year": 2015}, {"title": "Understanding deep architectures using a recursive convolutional network", "author": ["David Eigen", "Jason Rolfe", "Rob Fergus", "Yann LeCun."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Eigen et al\\.,? 2014", "shortCiteRegEx": "Eigen et al\\.", "year": 2014}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman."], "venue": "Cognitive science, 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["Jeffrey L Elman."], "venue": "Machine learning, 7(2-3):195\u2013225.", "citeRegEx": "Elman.,? 1991", "shortCiteRegEx": "Elman.", "year": 1991}, {"title": "Visualizing higherlayer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent."], "venue": "International", "citeRegEx": "Erhan et al\\.,? 2009", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Extracting and learning an unknown grammar with recurrent neural networks", "author": ["C. Lee Giles", "Clifford B. Miller", "Dong Chen", "GuoZheng Sun", "Hsing-Hen Chen", "Yee-Chun Lee."], "venue": "Advances in Neural Information Processing Systems, pages", "citeRegEx": "Giles et al\\.,? 1991", "shortCiteRegEx": "Giles et al\\.", "year": 1991}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra."], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Gregor et al\\.,? 2015", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Attractor dynamics and parallelism in a connectionist sequential network", "author": ["Michael I Jordan."], "venue": "Proceedings of the Eighth Annual Conference of the Cognitive Science Society.", "citeRegEx": "Jordan.,? 1986", "shortCiteRegEx": "Jordan.", "year": 1986}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li."], "venue": "arXiv preprint arXiv:1506.02078.", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in Neural Information Processing Systems, pages 3276\u20133284.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Visualizing and understanding neural models in NLP", "author": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1506.01066.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "Computer Vision\u2013ECCV 2014, pages 740\u2013755. Springer.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Understanding deep image representations by inverting them", "author": ["Aravindh Mahendran", "Andrea Vedaldi."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 5188\u20135196.", "citeRegEx": "Mahendran and Vedaldi.,? 2015a", "shortCiteRegEx": "Mahendran and Vedaldi.", "year": 2015}, {"title": "Visualizing deep convolutional neural networks using natural pre-images", "author": ["Aravindh Mahendran", "Andrea Vedaldi."], "venue": "arXiv preprint arXiv:1512.02017.", "citeRegEx": "Mahendran and Vedaldi.,? 2015b", "shortCiteRegEx": "Mahendran and Vedaldi.", "year": 2015}, {"title": "Turning on the turbo: Fast thirdorder non-projective turbo parsers", "author": ["Andr\u00e9 FT Martins", "Miguel Almeida", "Noah A Smith."], "venue": "ACL (2), pages 617\u2013622.", "citeRegEx": "Martins et al\\.,? 2013", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune."], "venue": "arXiv preprint arXiv:1602.03616.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman."], "venue": "ICLR.", "citeRegEx": "Simonyan and Zisserman.,? 2015", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman."], "venue": "International Conference on Learning Representation (ICLR) Workshop.", "citeRegEx": "Simonyan et al\\.,? 2014", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems, pages 2755\u20132763.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Reseg: A recurrent neural network for object segmentation", "author": ["Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1511.07053.", "citeRegEx": "Cho.,? 2015", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "Understanding neural networks through deep visualization", "author": ["Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson."], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Yosinski et al\\.,? 2015", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu."], "venue": "Describing and Understanding Video & The Large Scale Movie Description Challenge", "citeRegEx": "Yu et al\\.,? 2015", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Object detectors emerge in deep scene cnns", "author": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "Variations of the RNN architectures have been applied in several NLP domains such as parsing (Vinyals et al., 2015) and machine translation (Bahdanau et al.", "startOffset": 93, "endOffset": 115}, {"referenceID": 0, "context": ", 2015) and machine translation (Bahdanau et al., 2015), as well as in computer vision applications such as image generation (Gregor et al.", "startOffset": 32, "endOffset": 55}, {"referenceID": 10, "context": ", 2015), as well as in computer vision applications such as image generation (Gregor et al., 2015) and object segmentation (Visin et al.", "startOffset": 77, "endOffset": 98}, {"referenceID": 13, "context": "image (Karpathy and Fei-Fei, 2015) and video captioning (Yu et al.", "startOffset": 6, "endOffset": 34}, {"referenceID": 29, "context": "image (Karpathy and Fei-Fei, 2015) and video captioning (Yu et al., 2015).", "startOffset": 56, "endOffset": 73}, {"referenceID": 5, "context": "Recurrent neural networks (RNNs) were introduced by Elman (1990) as a connectionist architecture with the ability to model the temporal dimension.", "startOffset": 52, "endOffset": 65}, {"referenceID": 1, "context": "It is a multi-task, multi-modal architecture consisting of two Gated-Recurrent Unit (GRU) (Cho et al., 2014; Chung et al., 2014) pathways and a shared word embedding matrix.", "startOffset": 90, "endOffset": 128}, {"referenceID": 3, "context": "It is a multi-task, multi-modal architecture consisting of two Gated-Recurrent Unit (GRU) (Cho et al., 2014; Chung et al., 2014) pathways and a shared word embedding matrix.", "startOffset": 90, "endOffset": 128}, {"referenceID": 1, "context": "As our case study we picked the IMAGINET model introduced by Chrupa\u0142a et al. (2015). It is a multi-task, multi-modal architecture consisting of two Gated-Recurrent Unit (GRU) (Cho et al.", "startOffset": 61, "endOffset": 84}, {"referenceID": 14, "context": "Recurrent neural language models akin to TEXTUAL which are trained to predict the next symbol in a sequence are relatively well understood, and there have been some attempts to analyze their internal states (e.g. Elman, 1991; Karpathy et al., 2015).", "startOffset": 207, "endOffset": 248}, {"referenceID": 6, "context": "The direct predecessors of modern architectures were first proposed in the seminal paper of Elman (1990). He modifies the recurrent neural network architecture of Jordan (1986) by changing the output-to-memory feedback connections to hidden-to-memory recurrence, enabling Elman networks to represent arbitrary dynamic systems.", "startOffset": 92, "endOffset": 105}, {"referenceID": 6, "context": "The direct predecessors of modern architectures were first proposed in the seminal paper of Elman (1990). He modifies the recurrent neural network architecture of Jordan (1986) by changing the output-to-memory feedback connections to hidden-to-memory recurrence, enabling Elman networks to represent arbitrary dynamic systems.", "startOffset": 92, "endOffset": 177}, {"referenceID": 6, "context": "The direct predecessors of modern architectures were first proposed in the seminal paper of Elman (1990). He modifies the recurrent neural network architecture of Jordan (1986) by changing the output-to-memory feedback connections to hidden-to-memory recurrence, enabling Elman networks to represent arbitrary dynamic systems. In Elman (1991) he trains an RNN on a small synthetic sentence dataset and analyzes the activation patterns of the hidden layer.", "startOffset": 92, "endOffset": 343}, {"referenceID": 6, "context": "The direct predecessors of modern architectures were first proposed in the seminal paper of Elman (1990). He modifies the recurrent neural network architecture of Jordan (1986) by changing the output-to-memory feedback connections to hidden-to-memory recurrence, enabling Elman networks to represent arbitrary dynamic systems. In Elman (1991) he trains an RNN on a small synthetic sentence dataset and analyzes the activation patterns of the hidden layer. His analysis shows that these distributed representations encode lexical categories, grammatical relationships and hierarchical constituent structures. Giles et al. (1991) trains RNNs similar to Elman networks on strings generated by small deterministic regular grammars with the objective to recognize positive and reject negative strings, and develops the dynamic state partitioning technique to extract the learned grammar from the networks in the form of deterministic finite state automatons.", "startOffset": 92, "endOffset": 628}, {"referenceID": 11, "context": "More specifically, they train Long Short-Term Memory networks (LSTM) (Hochreiter and Schmidhuber, 1997) for phrase level sentiment analysis and present novel methods to explore the inner workings of RNNs.", "startOffset": 69, "endOffset": 103}, {"referenceID": 16, "context": "More closely related is the recent work of Li et al. (2015), who develops techniques for a deeper understanding of the activation patterns of RNNs, but focuses on models with modern architectures trained on large scale data sets.", "startOffset": 43, "endOffset": 60}, {"referenceID": 16, "context": "(2016) train a Convolutional Neural Networks (CNN) with different random initializations on the ImageNet dataset (Krizhevsky et al., 2012).", "startOffset": 113, "endOffset": 138}, {"referenceID": 8, "context": "Other works on visualizing the role of individual hidden units in deep models for vision synthesize images by optimizing random images through backpropagation to maximize the activity of units (Erhan et al., 2009; Simonyan et al., 2014; Yosinski et al., 2015; Nguyen et al., 2016) or to approximate the activation vectors of particular layers (Mahendran and Vedaldi, 2015b; Dosovitskiy and Brox, 2015).", "startOffset": 193, "endOffset": 280}, {"referenceID": 24, "context": "Other works on visualizing the role of individual hidden units in deep models for vision synthesize images by optimizing random images through backpropagation to maximize the activity of units (Erhan et al., 2009; Simonyan et al., 2014; Yosinski et al., 2015; Nguyen et al., 2016) or to approximate the activation vectors of particular layers (Mahendran and Vedaldi, 2015b; Dosovitskiy and Brox, 2015).", "startOffset": 193, "endOffset": 280}, {"referenceID": 28, "context": "Other works on visualizing the role of individual hidden units in deep models for vision synthesize images by optimizing random images through backpropagation to maximize the activity of units (Erhan et al., 2009; Simonyan et al., 2014; Yosinski et al., 2015; Nguyen et al., 2016) or to approximate the activation vectors of particular layers (Mahendran and Vedaldi, 2015b; Dosovitskiy and Brox, 2015).", "startOffset": 193, "endOffset": 280}, {"referenceID": 22, "context": "Other works on visualizing the role of individual hidden units in deep models for vision synthesize images by optimizing random images through backpropagation to maximize the activity of units (Erhan et al., 2009; Simonyan et al., 2014; Yosinski et al., 2015; Nguyen et al., 2016) or to approximate the activation vectors of particular layers (Mahendran and Vedaldi, 2015b; Dosovitskiy and Brox, 2015).", "startOffset": 193, "endOffset": 280}, {"referenceID": 20, "context": ", 2016) or to approximate the activation vectors of particular layers (Mahendran and Vedaldi, 2015b; Dosovitskiy and Brox, 2015).", "startOffset": 70, "endOffset": 128}, {"referenceID": 4, "context": ", 2016) or to approximate the activation vectors of particular layers (Mahendran and Vedaldi, 2015b; Dosovitskiy and Brox, 2015).", "startOffset": 70, "endOffset": 128}, {"referenceID": 24, "context": "In general, there has been a growing interest within computer vision in understanding deep models, with a number of papers dedicated to visualizing learned CNN filters and pixel saliencies (Simonyan et al., 2014; Yosinski et al., 2015; Mahendran and Vedaldi, 2015a).", "startOffset": 189, "endOffset": 265}, {"referenceID": 28, "context": "In general, there has been a growing interest within computer vision in understanding deep models, with a number of papers dedicated to visualizing learned CNN filters and pixel saliencies (Simonyan et al., 2014; Yosinski et al., 2015; Mahendran and Vedaldi, 2015a).", "startOffset": 189, "endOffset": 265}, {"referenceID": 19, "context": "In general, there has been a growing interest within computer vision in understanding deep models, with a number of papers dedicated to visualizing learned CNN filters and pixel saliencies (Simonyan et al., 2014; Yosinski et al., 2015; Mahendran and Vedaldi, 2015a).", "startOffset": 189, "endOffset": 265}, {"referenceID": 5, "context": "These techniques have also led to improvements in model performance (Eigen et al., 2014) and transferability of features (Zhou et al.", "startOffset": 68, "endOffset": 88}, {"referenceID": 30, "context": ", 2014) and transferability of features (Zhou et al., 2015).", "startOffset": 40, "endOffset": 59}, {"referenceID": 2, "context": "IMAGINET introduced in Chrupa\u0142a et al. (2015) is a multi-modal GRU network architecture that learns visually grounded meaning representations from textual and visual input.", "startOffset": 23, "endOffset": 46}, {"referenceID": 2, "context": "For more details about the model and its performance see Chrupa\u0142a et al. (2015).", "startOffset": 57, "endOffset": 80}, {"referenceID": 18, "context": "For all the experiments, we trained IMAGINET on the training portion of the MSCOCO image-caption dataset (Lin et al., 2014), and analyzed the representations of the sentences in the validation set.", "startOffset": 105, "endOffset": 123}, {"referenceID": 23, "context": "The target image representations were extracted from the pre-softmax layer of the 16layer CNN (Simonyan and Zisserman, 2015).", "startOffset": 94, "endOffset": 124}, {"referenceID": 21, "context": "Both POS tagging and dependency parsing are performed jointly using the TurboParser dependency parser (Martins et al., 2013).", "startOffset": 102, "endOffset": 124}, {"referenceID": 30, "context": "We develop a simple method we dub top K contexts after the top K images of Zhou et al. (2015). We forwarded each sentence from a corpus token-by-token through the RNN, and registered the hidden state ht for each unit at time step t.", "startOffset": 75, "endOffset": 94}, {"referenceID": 17, "context": "Similarly to Li et al. (2016), the activation value distributions are discretized into percentile bins per dimension, such that each bin contains 5% of the marginal density.", "startOffset": 13, "endOffset": 30}, {"referenceID": 14, "context": ", 2014) or the purely distributional sentence embedding system of Kiros et al. (2015). A number of recurrent neural models rely on a so called attention mechanism, first introduced by Bahdanau et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 0, "context": "A number of recurrent neural models rely on a so called attention mechanism, first introduced by Bahdanau et al. (2015) under the name of soft alignment.", "startOffset": 97, "endOffset": 120}], "year": 2017, "abstractText": "We present novel methods for analysing the activation patterns of RNNs and identifying the types of linguistic structure they learn. As a case study, we use a multi-task gated recurrent network model consisting of two parallel pathways with shared word embeddings trained on predicting the representations of the visual scene corresponding to an input sentence, and predicting the next word in the same sentence. We show that the image prediction pathway is sensitive to the information structure of the sentence, and pays selective attention to lexical categories and grammatical functions that carry semantic information. It also learns to treat the same input token differently depending on its grammatical functions in the sentence. The language model is comparatively more sensitive to words with a syntactic function. Our analysis of the function of individual hidden units shows that each pathway contains specialized units tuned to patterns informative for the task, some of which can carry activations to later time steps to encode long-term dependencies.", "creator": "LaTeX with hyperref package"}}}