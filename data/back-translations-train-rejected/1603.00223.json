{"id": "1603.00223", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2016", "title": "Segmental Recurrent Neural Networks for End-to-end Speech Recognition", "abstract": "We study the segmental recurrent neural network for end-to-end acoustic modelling. This model connects the segmental conditional random field (CRF) with a recurrent neural network (RNN) used for feature extraction. Compared to most previous CRF-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. Instead, this model marginalises out all the possible segmentations, and features are extracted from the RNN trained together with the segmental CRF. In essence, this model is self-contained and can be trained end-to-end. In this paper, we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. We performed experiments on the TIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass decoding --- the best reported result using CRFs, despite the fact that we only used a zeroth-order CRF and without using any language model.", "histories": [["v1", "Tue, 1 Mar 2016 10:43:43 GMT  (178kb,D)", "http://arxiv.org/abs/1603.00223v1", "5 pages, 2 figures"], ["v2", "Mon, 20 Jun 2016 10:29:23 GMT  (178kb,D)", "http://arxiv.org/abs/1603.00223v2", "5 pages, 2 figures, accepted by Interspeech 2016"]], "COMMENTS": "5 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["liang lu", "lingpeng kong", "chris dyer", "noah a smith", "steve renals"], "accepted": false, "id": "1603.00223"}, "pdf": {"name": "1603.00223.pdf", "metadata": {"source": "CRF", "title": "Segmental Recurrent Neural Networks for End-to-end Speech Recognition", "authors": ["Liang Lu", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith", "Steve Renals"], "emails": ["s.renals}@ed.ac.uk,", "cdyer}@cs.cmu.edu,", "nasmith@cs.washington.edu"], "sections": [{"heading": "1. Introduction", "text": "It is a typical sequence on sequence transduction problem, i.e., given a sequence of acoustic observations, it is a strange approach, the speech recognition engine decodes the corresponding sequence of words (or phonemes). A key component in a speech recognition system is the acoustic model, which converts the conditional probability of the output sequence into a frame sequence. However, the direct calculation of this conditional probability requires a multitude of factors, which include the different lengths of the input and output sequences. The hidden Markov model (HMM) converts this sequencing task into a frame level, where each acoustic frame is classified into one of the hidden states and each output sequence corresponds to a hidden state. To make it compatible, we usually rely on the conditional independence of adoption and the first order Markov rule - the known weaknesses of HMMs [1]."}, {"heading": "2. Segmental Recurrent Neural Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Segmental Conditional Random Fields", "text": "Given a sequence of acoustic frames X = {x1, \u00b7 \u00b7, xT} and their corresponding sequence of output markers y = {y1, \u00b7 \u00b7 \u00b7, yJ}, where T \u2265 J, segmental (or semi-Markov) conditional random field defines the conditional probability at sequence level with the auxiliary segment designations E = {e1, \u00b7 \u00b7 \u00b7, eJ} asP (y, E | X) = 1 Z (X) J = 1 exp f (yj, ej, X), (1), where ej = < sj > is a tuple of the beginning (sj) and end (nj) timestamp for the segment of yj and nj > sj, while nj, sj [1, T]; yj, Y and Y denote the vocabulary set; Z (X) is the normalizer that covers all possible (y, E) pairs, CR.ej, jj function (jj), jj-forward (jj), jj (jj), jj-forward (jj), jj (jp)."}, {"heading": "2.2. Feature Representations", "text": "We use neural networks to define the function \u03a6 (\u00b7), which maps the acoustic segment and the associated label in a common attribute space. Specifically, yj is first represented as a uniform vector vj, and it is then mapped by a linear embedding matrix M asuj = Mvj (4) into a continuous space. In view of the segment designation ej, we use an RNN to map the acoustic segment to a fixed dimensional vector representation, i.e. hj1 = r (h0, xsj) (5) hj2 = r (h j 1, xsj + 1) (6)... hjdj = r (h j \u2212 1, xnj) (7), where h0 denotes the initial hidden state, dj = nj \u2212 sj = nj \u2212 sj) (nj \u2212 sj stands for the duration of the segment and (\u00b7) is a non-linear function."}, {"heading": "2.3. Conditional Maximum Likelihood Training", "text": "The problem can be solved by defining the loss function as a negative marginal log probability. < p (y) = \u2212 logP (y | X) = \u2212 log \u2211 E (y, E | X) = \u2212 log \u2211 E-jexp f (yj, ej, X) (X, y) + logZ (X), (9) where the set of model parameters is specified, and Z (X, y) the sum of all possible segmentations if only y is observed. To simplify the notations, the objective function L (\u03b8) is defined with only one training expression. However, the number of possible segmentations is exponentially impregnated with the length of X, which implies the naive calculation of Z (X, y) and Z (X)."}, {"heading": "2.4. Viterbi Decoding", "text": "When decoding, we must search through the target tag sequence y, which yields the highest probability that X is given by marginalizing all possible segmentations: y \u0445 = arg max y log \u2211 E P (y, E | X) (18) This is a minor modification of the recursive algorithm in equation. (11) Instead of summarizing all possible labels, the marginalization of all possible segmentations up to the timeframe t is\u03b1 t = 0 < k < t\u0430 k \u00d7 max y Y f (y, < k < k >, X) (19) However, marginalization of all possible segmentations is still costly. Calculation costs can be further reduced by greedily looking for the most likely segmentation, i.e., we can find the z = max 0 < k < t \u0445 < k < t k < k < y, Y, max, Y, < (which can only yield a high coefficient)."}, {"heading": "2.5. Further Speedup", "text": "It is mathematically expensive for RNNs to model long sequences, and the number of possible segmentations is exponential with the length of the aforementioned input sequence. Calculation costs can be significantly reduced by using the hierarchical subsampling RNN [21] to shorten the input sequences, with the subsampling layer taking a window with hidden states from the lower level as input, as in Figure 2. In this thesis, we will consider three variants: a) concatenation - the hidden states in the subsampling window are concatenated before they are fed into the next level; b) adding - the hidden states are inserted into a vector for the next level; c) skipping - only the last hidden state in the window is retained and all the others are skipped."}, {"heading": "3. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. System Setup", "text": "We used the TIMIT dataset to evaluate the segmental RNN acoustic models; this dataset was preferred for quick evaluation of various system settings and for comparison with other CRF and end-to-end systems; we followed the standard protocol of the TIMIT dataset, and our experiments were based on the Kaldi recipe [22]; we used the core test set of 192 expressions; we used 24-dimensional log fiterbanks (FBANKs) with delta and double delta coefficients, which yielded 72 dimensional characteristic vectors; our models were trained with 48 phonemes and their predictions were converted into 39 phonemes before evaluation; the dimension of uj was set to 32; for all our experiments we used the Long Term Memory (LSTM) networks as implementation of RNNs, and the networks were always bidirectional."}, {"heading": "3.2. Results of Hierarchical Subsampling", "text": "First, we show the results of the hierarchical subsampling network, which is the key to accelerating our experiments. We set the size of the subsampling window to 2, so each subsampling layer reduced the time resolution by a factor of 2. We set the maximum segment length L in equivalent (14) to 300 milliseconds, corresponding to 30 frames of FBANKs (sampled at a rate of 10 milliseconds). For two layers of recursive subsampling networks, the time resolution decreased by a factor of 4, and the value of L was reduced to 8, which was about 10 times faster than shown in Table 1.Table 2, compares the three implementations of the recursive subsampling network in detail in Section 2.5. We observed that concatenating all the hidden states in the subsampling window did not result in a lower error rate (PER) than the use of the simple oversampling network, which may be due to the fact that IMIT is smaller."}, {"heading": "3.3. Hyperparameters and Different Features", "text": "We then evaluated the model by adjusting the hyperparameters, and the results are in Table 3. The number of LSTM layers and dimension of LSTM cells, as well as the dimensions of w and segment vector hj, were adjusted. In general, larger models with failure control yielded higher detection accuracy. We achieved our best result with 6 layers of 250-dimensional LSTMs. Without failure control, however, the model can be easily overhauled due to the small size of the training set. In the future, we will evaluate this model with a large dataset. We then evaluated another two types of features with the same system configuration, the best result in Table 3. We increased the number of FBANKs from 24 to 40, which yielded slightly lower PER. We also evaluated the standard Kaldi characteristics - 39-dimensional MFCCs divided by a context window of 7, followed by LDA and MLT, and transformed loudspeakers with the same function of the M-M1 were improved by 5%."}, {"heading": "3.4. Comparison to Related Works", "text": "In Table 5, we compare our results with other reported results using segmental CRFs and newer end-to-end systems. Earlier state-of-the-art using segmental CRFs on the TIMIT dataset is reported in [28], where first-pass decoding was used to trim the search space and second-pass was used to reevaluate the hypothesis using various features, including neural network functionality. In addition, ground truth segmentation was used in [28]. We achieved a significantly lower PER in first-pass decoding, even though our CRF was a zero order and we did not use a language model. In addition, our results are also comparable to those of the CTC and attention-based RNN end-to-end systems. The accuracy of segmental RNNNNs can be further improved by using higher-order CRFs or by including a language model in the decoding step and reducing the search error occurring over the search area."}, {"heading": "4. Conclusions", "text": "In this paper, we present the segmental RNN - a novel acoustic model that combines the segmental CRF with an encoder RNN for end-to-end speech recognition. We discuss the practical training and decoding of algorithms of this model for speech recognition and the subsampling network to reduce computing costs. Our experiments were conducted with the TIMIT dataset and we achieved high recognition accuracy using CRF in zero order and without using any language model. In the future, we will investigate discriminatory training criteria and include a language model in the decoding step. Future work includes the implementation of a weighted finite-rate transducer (WFST) and scaling this model to large vocabulary datasets."}, {"heading": "5. References", "text": "[1] D. Gillick, L. Gillick, and S. Wegmann, \"Don't multiplylightly: Quantifying problems with the acoustic model assumptions in speech recognition,\" in Proc. ASRU. IEEE, 2011, pp. 71-76. [2] M. Ostendorf, V. Digalakis, and O. Kimball, \"From HMM's to segment models: A unified view of stochastic modeling for speech recognition,\" IEEE Transactions on Speech and Audio Processing, pp. 360-378, 1996. [3] N. Smith and M. Gales \"Speech recognition using SVMs,\" in neural information processing systems, 2001, pp. 1197-1204. A. Gunawardana, M. Mahajan, A. Acero, and J. C. Platt, \"Hidden random fields for phone classification.\""}], "references": [{"title": "Don\u2019t multiply lightly: Quantifying problems with the acoustic model assumptions in speech recognition", "author": ["D. Gillick", "L. Gillick", "S. Wegmann"], "venue": "Proc. ASRU. IEEE, 2011, pp. 71\u201376.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "From HMM\u2019s to segment models: A unified view of stochastic modeling for speech recognition", "author": ["M. Ostendorf", "V. Digalakis", "O. Kimball"], "venue": "IEEE Transactions on Speech and Audio Processing, pp. 360\u2013378, 1996.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Speech recognition using SVMs", "author": ["N. Smith", "M. Gales"], "venue": "Advances in neural information processing systems, 2001, pp. 1197\u20131204.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Hidden conditional random fields for phone classification.", "author": ["A. Gunawardana", "M. Mahajan", "A. Acero", "J.C. Platt"], "venue": "INTERSPEECH,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Speech recognition using augmented conditional random fields", "author": ["Y. Hifny", "S. Renals"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 17, no. 2, pp. 354\u2013365, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "Proc. ICML, 2014, pp. 1764\u20131772.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Speech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger"], "venue": "arXiv preprint arXiv:1412.5567, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["H. Sak", "A. Senior", "K. Rao", "F. Beaufays"], "venue": "Proc. INTERSPEECH, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "EESEN: Endto-end speech recognition using deep RNN models and WFST-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "Proc. ASRU, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "Proc. ICLR, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 577\u2013585.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A study of the recurrent neural network encoder-decoder for large vocabulary speech recognition", "author": ["L. Lu", "X. Zhang", "K. Cho", "S. Renals"], "venue": "Proc. INTERSPEECH, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Listen, attend and spell", "author": ["W. Chan", "N. Jaitly", "Q.V. Le", "O. Vinyals"], "venue": "arXiv preprint arXiv:1508.01211, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Segmental recurrent neural networks", "author": ["L. Kong", "C. Dyer", "N.A. Smith"], "venue": "arXiv preprint arXiv:1511.06018, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-markov conditional random fields for information extraction", "author": ["S. Sarawagi", "W.W. Cohen"], "venue": "Advances in neural information processing systems, 2004, pp. 1185\u2013 1192.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proc. ICML, 2001, pp. 282\u2013 289.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Speech recognition with segmental conditional random fields: A summary of the JHU CLSP 2010 summer workshop", "author": ["G. Zweig", "P. Nguyen", "D. Van Compernolle", "K. Demuynck", "L. Atlas", "P. Clark"], "venue": "Proc. ICASSP. IEEE, 2011, pp. 5044\u20135047.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Conditional random fields in speech, audio, and language processing", "author": ["E. Fosler-Lussier", "Y. He", "P. Jyothi", "R. Prabhavalkar"], "venue": "Proceedings of the IEEE, vol. 101, no. 5, pp. 1054\u20131075, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep segmental neural networks for speech recognition.", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu", "H. Jiang"], "venue": "in Proc. INTERSPEECH,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Segmental conditional random fields with deep neural networks as acoustic models for first-pass word recognition", "author": ["Y. He", "E. Fosler-Lussier"], "venue": "Proc. INTERSPEECH, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical subsampling networks", "author": ["A. Graves"], "venue": "Supervised Sequence Labelling with Recurrent Neural Networks. Springer, 2012, pp. 109\u2013131.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motl\u0131cek", "Y. Qian", "P. Schwarz", "J. Silovsk\u00fd", "G. Semmer", "K. Vesel\u00fd"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1929}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Classification and recognition with direct segment models", "author": ["G. Zweig"], "venue": "Proc. ICASSP. IEEE, 2012, pp. 4161\u2013 4164.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient segmental conditional random fields for phone recognition", "author": ["Y. He", "E. Fosler-Lussier"], "venue": "Proc. IN- TERSPEECH, 2012, pp. 1898\u20131901.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminative segmental cascades for feature-rich phone recognition", "author": ["H. Tang", "W. Wang", "K. Gimpel", "K. Livescu"], "venue": "Proc. ASRU, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-R. Mohamed", "G. Hinton"], "venue": "Proc. ICASSP. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "To make it computationally tractable, HMMs usually rely on the conditional independence assumption and the first-order Markov rule \u2014 the well-known weaknesses of HMMs [1].", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": ", [2, 3, 4, 5]; however these approaches have not yet improved speech recognition accuracy over HMMs.", "startOffset": 2, "endOffset": 14}, {"referenceID": 2, "context": ", [2, 3, 4, 5]; however these approaches have not yet improved speech recognition accuracy over HMMs.", "startOffset": 2, "endOffset": 14}, {"referenceID": 3, "context": ", [2, 3, 4, 5]; however these approaches have not yet improved speech recognition accuracy over HMMs.", "startOffset": 2, "endOffset": 14}, {"referenceID": 4, "context": ", [2, 3, 4, 5]; however these approaches have not yet improved speech recognition accuracy over HMMs.", "startOffset": 2, "endOffset": 14}, {"referenceID": 5, "context": "In particular, the connectionist temporal classification (CTC) [6, 7, 8, 9] approach defines the loss function directly to maximise the conditional probability of the output sequence given the input sequence, and it usually uses a recurrent neural network to extract features.", "startOffset": 63, "endOffset": 75}, {"referenceID": 6, "context": "In particular, the connectionist temporal classification (CTC) [6, 7, 8, 9] approach defines the loss function directly to maximise the conditional probability of the output sequence given the input sequence, and it usually uses a recurrent neural network to extract features.", "startOffset": 63, "endOffset": 75}, {"referenceID": 7, "context": "In particular, the connectionist temporal classification (CTC) [6, 7, 8, 9] approach defines the loss function directly to maximise the conditional probability of the output sequence given the input sequence, and it usually uses a recurrent neural network to extract features.", "startOffset": 63, "endOffset": 75}, {"referenceID": 8, "context": "In particular, the connectionist temporal classification (CTC) [6, 7, 8, 9] approach defines the loss function directly to maximise the conditional probability of the output sequence given the input sequence, and it usually uses a recurrent neural network to extract features.", "startOffset": 63, "endOffset": 75}, {"referenceID": 9, "context": ", in machine translation [10], and speech recognition [11, 12, 13].", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": ", in machine translation [10], and speech recognition [11, 12, 13].", "startOffset": 54, "endOffset": 66}, {"referenceID": 11, "context": ", in machine translation [10], and speech recognition [11, 12, 13].", "startOffset": 54, "endOffset": 66}, {"referenceID": 12, "context": ", in machine translation [10], and speech recognition [11, 12, 13].", "startOffset": 54, "endOffset": 66}, {"referenceID": 9, "context": "Instead, it maps the variable-length input sequence into a fixed-size vector representation at each decoding step by an attention-based scheme (see [10] for further explanation).", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "In this paper, we study segmental RNNs [14] for acoustic modelling.", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "This model is similar to CTC and attention-based RNN in the sense that an RNN encoder is also used for feature extraction, but it differs in the sense that the sequence-level conditional probability is defined using an segmental (semiMarkov) CRF [15], which is an extension on the standard CRF [16].", "startOffset": 246, "endOffset": 250}, {"referenceID": 15, "context": "This model is similar to CTC and attention-based RNN in the sense that an RNN encoder is also used for feature extraction, but it differs in the sense that the sequence-level conditional probability is defined using an segmental (semiMarkov) CRF [15], which is an extension on the standard CRF [16].", "startOffset": 294, "endOffset": 298}, {"referenceID": 3, "context": "g, [4, 5, 17] (see [18] for an overview).", "startOffset": 3, "endOffset": 13}, {"referenceID": 4, "context": "g, [4, 5, 17] (see [18] for an overview).", "startOffset": 3, "endOffset": 13}, {"referenceID": 16, "context": "g, [4, 5, 17] (see [18] for an overview).", "startOffset": 3, "endOffset": 13}, {"referenceID": 17, "context": "g, [4, 5, 17] (see [18] for an overview).", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "In particular, feed-forward neural networks have been used with segmental CRFs for speech recognition [19, 20].", "startOffset": 102, "endOffset": 110}, {"referenceID": 19, "context": "In particular, feed-forward neural networks have been used with segmental CRFs for speech recognition [19, 20].", "startOffset": 102, "endOffset": 110}, {"referenceID": 14, "context": "Fortunately, this can be addressed by using the following dynamic programming algorithm as proposed in [15]:", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "Figure 2: Hierarchical subsampling recurrent network [21] .", "startOffset": 53, "endOffset": 57}, {"referenceID": 20, "context": "The computational cost can be significantly reduced by using the hierarchical subsampling RNN [21] to shorten the input sequences, where the subsampling layer takes a window of hidden states from the lower layer as input as shown in Figure 2.", "startOffset": 94, "endOffset": 98}, {"referenceID": 21, "context": "We followed the standard protocol of the TIMIT dataset, and our experiments were based on the Kaldi recipe [22].", "startOffset": 107, "endOffset": 111}, {"referenceID": 22, "context": "For all our experiments, we used the long short-term memory (LSTM) networks [23] as the implementation of RNNs, and the networks were always bi-directional.", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "regularisation [24], using an specific implementation for recurrent networks [25].", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "regularisation [24], using an specific implementation for recurrent networks [25].", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "5 first-pass SCRF [26] \u221a \u00d7 33.", "startOffset": 18, "endOffset": 22}, {"referenceID": 26, "context": "1 Boundary-factored SCRF [27] \u00d7 \u00d7 26.", "startOffset": 25, "endOffset": 29}, {"referenceID": 18, "context": "5 Deep Segmental NN [19] \u221a \u00d7 21.", "startOffset": 20, "endOffset": 24}, {"referenceID": 27, "context": "9 Discriminative segmental cascade [28] \u221a \u00d7 21.", "startOffset": 35, "endOffset": 39}, {"referenceID": 28, "context": "9 CTC [29] \u00d7 \u00d7 18.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "4 RNN transducer [29] - \u00d7 17.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "7 Attention-based RNN [11] - \u00d7 17.", "startOffset": 22, "endOffset": 26}, {"referenceID": 27, "context": "Previous state-of-the-art result using segmental CRFs on the TIMIT dataset is reported in [28], where the first-pass decoding was used to prune the search space, and the second-pass was used to re-score the hypothesis using various features including neural network features.", "startOffset": 90, "endOffset": 94}, {"referenceID": 27, "context": "Besides, the ground-truth segmentation was used in [28].", "startOffset": 51, "endOffset": 55}], "year": 2017, "abstractText": "We study the segmental recurrent neural network for end-to-end acoustic modelling. This model connects the segmental conditional random field (CRF) with a recurrent neural network (RNN) used for feature extraction. Compared to most previous CRF-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. Instead, this model marginalises out all the possible segmentations, and features are extracted from the RNN trained together with the segmental CRF. In essence, this model is self-contained and can be trained end-to-end. In this paper, we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. We performed experiments on the TIMIT dataset. We achieved 17.3% phone error rate (PER) from the first-pass decoding \u2014 the best reported result using CRFs, despite the fact that we only used a zeroth-order CRF and without using any language model.", "creator": "LaTeX with hyperref package"}}}