{"id": "1603.00423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2016", "title": "Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs", "abstract": "Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN's on these problems. We present an artificial learning task that allows us to quantify the severity of these problems for both models. We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the benefits of including hierarchical structure and of including LSTM-style gating are complementary.", "histories": [["v1", "Tue, 1 Mar 2016 19:45:25 GMT  (201kb,D)", "http://arxiv.org/abs/1603.00423v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.NE", "authors": ["phong le", "willem zuidema"], "accepted": false, "id": "1603.00423"}, "pdf": {"name": "1603.00423.pdf", "metadata": {"source": "CRF", "title": "Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs", "authors": ["Phong Le"], "emails": ["p.le@uva.nl", "zuidema@uva.nl"], "sections": [{"heading": "1 Introduction", "text": "The recursive neural network (RNN) has become popular since the work of Socher et al. (2010), but it has been used to tackle several NLP tasks, such as syntactic parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and embedding words in learning processes (Luong et al., 2013). However, like traditional neural networks, the RNN appears to suffer from the problem of vanishing gradients, in which error signals that spread from the root in a plot tree to the children's nodes shrink very quickly. Furthermore, it encounters difficulties in detecting long-distance dependencies: information that spreads from children's nodes deep in a plot tree in which the root nodes are obscured. In the recursive neural network world, the nodes shrink very quickly to children's nodes."}, {"heading": "2 Background", "text": "Both the RNN and the RLSTM model are examples of a general framework that uses a sentence, a syntactic tree, and vector representations for the words in the sentence as input, and uses a composition function to recursively calculate vector representations for all phrases in the tree and the complete sentence. Technically, F is a single-layer, thrust-like neural network. In the RLSTM, a node u is represented by the vector u = [ur; uc] resulting from the concatenation of a vector representing the phrase covering the nodes and a memory vector. F could be any LSTM that can combine two such concatenation vectors, such as Structure-LSTM (Zhu et al., 2015), Tree-Tai (2015) and Le Tai (2015 and Le Tai)."}, {"heading": "3 Experiments", "text": "We will now examine how the two problems, the disappearing gradient problem and the long-distance dependency problem affect the RLSTM model and the RNN model. To do this, we propose the following artificial task, which requires a model to distinguish useful signals from noise. We define: \u2022 a set is a sequence of tokens that are integer numbers in the range [0, 10000]; \u2022 a set contains one and only one keyword token that is an integer number less than 1000; 1https: / / github.com / lephong / lstm-rnn \u2022 a set is labeled with the integer resulting from the division of the keyword by 100. For example, if the keyword is 607, the label is 6. Thus, there are 10 classes that range from 0 to 9. The task is to predict the class of a set, given its binary particle tree (Figure 1)."}, {"heading": "3.1 Experiment 1", "text": "To create a record with a length of l, we mix a list of randomly selected l \u2212 1 records without keywords and a keyword. The i-th record contains 12k records with lengths from 10i \u2212 9 characters to 10i characters and is divided into tensile, development, test records with sizes of 10k, 1k, 1k records. We analyzed each record by randomly generating a binary tree whose number of leaf nodes corresponds to the record length. The test accuracy of the two models on the 10 records is shown in Figure 2; for each record, we run each model five times and reported the highest accuracy for the RNN model and the distribution of accuracy (via box plot) for the RLSTM model. We can see that the RNN model performs relatively well on very short records (less than 11 characters)."}, {"heading": "3.2 Experiment 2", "text": "In Experiment 1, it is not clear whether the size of the tree or the depth of the keyword is the main factor in the rapid decline in the performance of the RNN. In this example, we determined the size of the tree and varied the depth of the keyword. We created a pool of sentences with lengths from 21 to 30 characters and analyzed them by randomly generating binary trees. Subsequently, we created 10 datasets with 12k trees each (10k for training, 1k for development, and 1k for testing).The i-th dataset consists of only trees where the distances from the keywords to the roots are i or i + 1 (to prevent the networks from directly exploiting the depth of the keyword).Figure 3 shows the test accuracy of the two models on these 10 datasets. Similarly in Experiment 1, we run each model five times and report the highest accuracy for the RNN model, and the distribution of the accuracy of the STLSTM model."}, {"heading": "3.3 Experiment 3", "text": "It is as if the two models were able to solve the problem they are trying to solve in order to solve it. (It is as if they were able to solve the problem.) (It is as if they were able to solve the problem. (It is as if they were able to solve it.) (It is as if they were able to solve it.) (It is as if they were able to do it, as if they were able to do it.)"}, {"heading": "4 Conclusions", "text": "The experimental results show that the RLSTM is superior to the RNN in overcoming the disappearing gradient problem and measuring long-term dependencies, in parallel to general conclusions about the performance of the LSTM architecture compared to traditional relapsing neural networks. In future work, we will focus on more complex cases involving negation, composition, etc."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "Neural Networks, IEEE Transactions on, 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, pages 2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Lstm recurrent networks learn simple context-free and context-sensitive languages", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, IEEE Transactions on, 12(6):1333\u20131340.", "citeRegEx": "Gers and Schmidhuber.,? 2001", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2001}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Compositional distributional semantics with long short term memory", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of the Joint Conference on Lexical and Computational Semantics (*SEM). Association for Computational Linguistics.", "citeRegEx": "Le and Zuidema.,? 2015", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "A recursive recurrent neural network for statistical machine translation", "author": ["Shujie Liu", "Nan Yang", "Mu Li", "Ming Zhou."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1491\u2013", "citeRegEx": "Liu et al\\.,? 2014", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL-2013, 104.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning", "citeRegEx": "Socher et al\\.,? 2010", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings EMNLP.", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Long short-term memory over recursive structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo."], "venue": "Proceedings of International Conference on Machine Learning, July.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "It has been employed to tackle several NLP tasks, such as syntactic parsing (Socher et al., 2013a), machine translation (Liu et al.", "startOffset": 76, "endOffset": 98}, {"referenceID": 5, "context": ", 2013a), machine translation (Liu et al., 2014), and word embedding learning (Luong et al.", "startOffset": 30, "endOffset": 48}, {"referenceID": 6, "context": ", 2014), and word embedding learning (Luong et al., 2013).", "startOffset": 37, "endOffset": 57}, {"referenceID": 3, "context": "In the recurrent neural network world, the long short term memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) is often used as a solution to these two problems.", "startOffset": 86, "endOffset": 120}, {"referenceID": 2, "context": "1 Introduction The recursive neural network (RNN) model became popular since the work of Socher et al. (2010). It has been employed to tackle several NLP tasks, such as syntactic parsing (Socher et al.", "startOffset": 89, "endOffset": 110}, {"referenceID": 2, "context": "In the recurrent neural network world, the long short term memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) is often used as a solution to these two problems. A natural extension of the LSTM can be defined for tree structures, which we call Recursive LSTM (RLSTM), as proposed independently by Tai et al. (2015), Zhu et al.", "startOffset": 87, "endOffset": 325}, {"referenceID": 2, "context": "In the recurrent neural network world, the long short term memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) is often used as a solution to these two problems. A natural extension of the LSTM can be defined for tree structures, which we call Recursive LSTM (RLSTM), as proposed independently by Tai et al. (2015), Zhu et al. (2015), and Le and Zuidema (2015).", "startOffset": 87, "endOffset": 344}, {"referenceID": 2, "context": "In the recurrent neural network world, the long short term memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) is often used as a solution to these two problems. A natural extension of the LSTM can be defined for tree structures, which we call Recursive LSTM (RLSTM), as proposed independently by Tai et al. (2015), Zhu et al. (2015), and Le and Zuidema (2015). However, while there is intensive research showing how the LSTM architecture can overcome those two problems compared to traditional recurrent models (e.", "startOffset": 87, "endOffset": 371}, {"referenceID": 2, "context": ", Gers and Schmidhuber (2001)), such research is, to our knowledge, still absent for the comparison between RNNs and RLSTMs.", "startOffset": 2, "endOffset": 30}, {"referenceID": 9, "context": "Using available annotated corpora such as the Stanford Sentiment Treebank (Socher et al., 2013b) and the Penn Treebank is thus inappropriate, as they are too small for this purpose (10k, 40k trees, respectively, compared to 240k trees in our experiments), and key nodes are not marked.", "startOffset": 74, "endOffset": 96}, {"referenceID": 11, "context": "F could be any LSTM that can combine two such concatenation vectors, such as Structure-LSTM (Zhu et al., 2015), Tree-LSTM (Tai et al.", "startOffset": 92, "endOffset": 110}, {"referenceID": 10, "context": ", 2015), Tree-LSTM (Tai et al., 2015), and LSTM-RNN (Le and Zuidema, 2015).", "startOffset": 19, "endOffset": 37}, {"referenceID": 4, "context": ", 2015), and LSTM-RNN (Le and Zuidema, 2015).", "startOffset": 22, "endOffset": 44}, {"referenceID": 4, "context": ", 2015), and LSTM-RNN (Le and Zuidema, 2015). In the current paper, we use the implementation1 of Le and Zuidema (2015).", "startOffset": 23, "endOffset": 120}, {"referenceID": 1, "context": "We trained the two models using the AdaGrad method (Duchi et al., 2011) with a learning rate of 0.", "startOffset": 51, "endOffset": 71}, {"referenceID": 6, "context": "Following Socher et al. (2013b), we used tanh as the activation function, and initialized word vectors by randomly sampling each value from a uniform distribution U(\u22120.", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": ") The fact the the RNN model still doesnot perform better than random guessing can be explained using the arguments given by Bengio et al. (1994), who show that there is a trade-off", "startOffset": 125, "endOffset": 146}], "year": 2016, "abstractText": "Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN\u2019s on these problems. We present an artificial learning task that allows us to quantify the severity of these problems for both models. We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the benefits of including hierarchical structure and of including LSTM-style gating are complementary.", "creator": "TeX"}}}