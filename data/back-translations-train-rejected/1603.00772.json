{"id": "1603.00772", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2016", "title": "Filter based Taxonomy Modification for Improving Hierarchical Classification", "abstract": "Large scale classification of data organized as a hierarchy of classes has received significant attention in the literature. Top-Down (TD) Hierarchical Classification (HC), which exploits the hierarchical structure during the learning process is an effective method for dealing with problems at scale due to its computational benefits. However, its accuracy suffers due to error propagation i.e., prediction errors made at higher levels in the hierarchy cannot be corrected at lower levels. One of the main reasons behind errors at the higher levels is the presence of inconsistent nodes and links that are introduced due to the arbitrary process of creating these hierarchies by domain experts. In this paper, we propose two efficient data driven filter based approaches for hierarchical structure modification: (i) Flattening (local and global) approach that identifies and removes inconsistent nodes present within the hierarchy and (ii) Rewiring approach modifies parent-child relationships to improve the classification performance of learned models. Our extensive empirical evaluation of the proposed approaches on several image and text datasets shows improved performance over competing approaches.", "histories": [["v1", "Wed, 2 Mar 2016 16:14:49 GMT  (583kb,D)", "https://arxiv.org/abs/1603.00772v1", null], ["v2", "Thu, 9 Jun 2016 06:41:42 GMT  (581kb,D)", "http://arxiv.org/abs/1603.00772v2", null], ["v3", "Sat, 15 Oct 2016 06:21:54 GMT  (650kb,D)", "http://arxiv.org/abs/1603.00772v3", "The conference version of the paper is submitted for publication"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["azad naik", "huzefa rangwala"], "accepted": false, "id": "1603.00772"}, "pdf": {"name": "1603.00772.pdf", "metadata": {"source": "CRF", "title": "Filter based Taxonomy Modification for Improving Hierarchical Classification", "authors": ["Azad Naik", "Huzefa Rangwala"], "emails": [], "sections": [{"heading": null, "text": "Keywords - Top-Down Hierarchical Classification, Rewiring, Clustering, Flattening"}, {"heading": "1 Introduction", "text": "These application domains (particularly highlighted by the interest in online prediction problems such as LSHTC4 and BioASQ5) pose unique mathematical and statistical challenges. As these datasets have several thousand classes, the methods developed need to be scaled during the learning and prediction phases. In addition, the majority of classes have very few training examples, leading to a class imbalance in which the learned models (for rare categories) tend to overload and misjudge the larger classes. Hierarchies provide useful structural relationships (such as parent-child and sibling) between different classes that can be used to learn generalized classification models. In the past, researchers have demonstrated the usefulness of hierarchies for classification and achieved promising results."}, {"heading": "2 Methods", "text": "In fact, the question is that most of us are able to survive on our own if we do not feel able to survive on our own, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I do not believe that we will be able to change the world. \"The question is only:\" What is it actually? \"The question is only:\" What is it actually? \"The question is:\" What is it actually? \"The question is:\" What is it actually? \"The question is:\" What is it actually? \"The answer?\" The answer? \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question\" The question \"The question The question\" The question \"The question\" The question \"The question\" The question"}, {"heading": "3 Experimental Protocol", "text": "3.1 Data Sets We have used an extensive set of data sets to evaluate the performance of our proposed rewiring approach. Various statistics of the data sets used are in Table 3. CLEF [22] and DIATOMS [23] are image data sets and the rest are text data sets. IPC7 is a collection of patent documents and the DMOZ data sets are an archive of web pages available from the LSHTC8 Challenge website. To evaluate the DMOZ 2010 and DMOZ 2012 data sets we use the provided test split. The results reported for these two benchmarks are blind predictions (i.e. we do not know the ground truth labels for the test set) obtained from the web portal interface 9.10. For all text data sets we apply the tf-idf transformation with l2 normalization to the word-frequency feature vector."}, {"heading": "3.2 Evaluation Metrics", "text": "To calculate \u00b5F1 we add the category-specific true positives (TPc), false positives (FPc) and false negatives (FNc) for different classes and calculate the score as follows: P = Reports / Classifications / iPC / en / 8http: / / lshtc.iit.demokritos.gr / 9http: / / lshtc.iit.demokritos.gr / lshtc.iit.demokritos.gr / node / 8110 http: / / lshtc.gr / P1 http: / / lshtc.iit.demokritos.gr / node / 8110 http: / / lshtc.gr / asc http: / lshtc.PF1 / Tc.iit.iit.it / lshtc.gr There is no hierarchy."}, {"heading": "3.3 Methods for Comparison", "text": "3.3.1 Hierarchical Methods Based on the hierarchy used during the training process, we use the following methods for comparison. Top-Down Logistic Regression (TD-LR): Expert-defined hierarchy provided by domain experts is used for training classifiers. Clustering Approach: Hierarchy generated with agglomerative clusters is used. To evaluate, we have limited the height of the cluster hierarchy to the original height by using cluster cohesion [8].Global Inconsistent nodes Flattening (Global-INF) [7]: Hierarchy is modified by flattening (removing) the inconsistent nodes based on the optimal optimization target value achieved at each node (eq. (2,1))) and empirically defined global cut-off thresholds. We seek [11]: Optimal hierarchy is identified in hierarchical space by promoting hierarchical hierarchical - by progressively modifying the hierarchy."}, {"heading": "4 Discussion and Results", "text": "4.1 Case Study To understand the quality of different hierarchical structures (defined by experts, bundled, flattened and rewired) for the newsgroup dataset shown in Figure 1, we use each of the hierarchies individually from top to bottom. The dataset includes 11269 training instances, 7505 test instances and 20 classes. We evaluate each of the hierarchies by randomly selecting five different training and test divisions in the same ratio as the original datasets. The results of the classification performance are shown in Table 4. We can see that using these modified hierarchies significantly improves classification performance compared to the expert-defined base hierarchy. When comparing the cluster, flat and proposed rewired hierarchies, the classification performance achieved by using the rewired hierarchy is significantly better than the flatter and bundled hierarchy."}, {"heading": "4.2 Evaluating Rewiring Approaches", "text": "This year is the highest in the history of the country."}, {"heading": "5 Related Work", "text": "Our work is closely related to the rewiring approach developed in Tang et al. [11], where the expert-defined hierarchy is gradually modified; iteratively, a subset of the hierarchy is modified and evaluated to improve classification using the HC learning algorithm. Changed changes are maintained as performance outcomes improve; otherwise, the changes are discarded and the process repeated; this repeated process of hierarchy modification continues until the optimal hierarchy is reached. A costly evaluation at each step makes this approach insoluble for large datasets. Another disadvantage of this approach is deciding which branch of the hierarchy should be examined first (for modification) and which elementary process (promotion, demotation, merging) will be applied at each step. Other work along similar lines can be found in [12,19]. Previous studies focused on flattening-based approaches that are kept away from (some level nodes)."}, {"heading": "6 Conclusion and Future Work", "text": "Our method is robust and can be adapted to be used in conjunction with all common HC approaches in the literature that use hierarchical relationships. Regardless of the classifiers that are trained, our modified hierarchy consistently yields better results than clustering or flattening to modify the original hierarchy. Compared to previous rewiring approaches, our method yields competitive results with significantly better runtime performance that allow HC approaches to scale to significantly large datasets (e.g. DMOZ). In addition, experiments on data sets with distorted distribution demonstrate the effectiveness of our proposed method compared to flat and state-of-the-art methods, especially for classes with rare categories. In the future, we plan to investigate the effect of our method in conjunction with feed selection and other non-linear classification methods."}, {"heading": "Acknowledgement", "text": "NSF Scholarships # 203337 and # 202882 to Huzefa Rangwala."}], "references": [{"title": "Hierarchical document categorization with support vector machines", "author": ["L. Cai", "T. Hofmann"], "venue": "In CIKM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Hierarchically classifying documents using very few words", "author": ["D. Koller", "M. Sahami"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Improving text classification by shrinkage in a hierarchy of classes", "author": ["A. McCallum", "R. Rosenfeld", "T. Mitchell", "A. Ng"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Hierarchical classification of web content", "author": ["S. Dumais", "H. Chen"], "venue": "In ACM SIGIR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Hierarchical text classification and evaluation", "author": ["A. Sun", "E. Lim"], "venue": "In ICDM, pages 521\u2013528,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Site abstraction for rare category classification in large-scale web directory", "author": ["T. Liu", "H. Wan", "T. Qin", "Z. Chen", "Y. Ren", "W. Ma"], "venue": "In WWW: Special interest tracks & posters,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Inconsistent node flattening for improving top-down hierarchical classification", "author": ["A. Naik", "H. Rangwala"], "venue": "In IEEE DSAA,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Hierarchical document classification using automatically generated hierarchy. JIIS", "author": ["T. Li", "S. Zhu", "M. Ogihara"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Hierarchical classification via orthogonal transfer", "author": ["L. Xiao", "D. Zhou", "M. Wu"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "A study of hierarchical and flat classification of proteins", "author": ["A. Zimek", "F. Buchwald", "E. Frank", "S. Kramer"], "venue": "IEEE/ACM TCBB,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Acclimatizing taxonomic semantics for hierarchical content classification", "author": ["L. Tang", "J. Zhang", "H. Liu"], "venue": "In ACM SIGKDD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Hierarchy evolution for improved classification", "author": ["X. Qi", "B. Davison"], "venue": "In CIKM, pages 2193\u20132196,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Hiercost: Improving large scale hierarchical classification with cost sensitive learning", "author": ["A. Charuvaka", "H. Rangwala"], "venue": "In ECML PKDD,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Maximum-margin framework for training data synchronization in large-scale hierarchical classification", "author": ["R. Babbar", "I. Partalas", "E. Gaussier", "MR. Amini"], "venue": "In Neural Information Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Flatten hierarchies for large-scale hierarchical text categorization", "author": ["X. Wang", "B. Lu"], "venue": "In ICDIM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "On flat versus hierarchical classification in large-scale taxonomies", "author": ["R. Babbar", "I. Partalas", "E. Gaussier", "M. Amini"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Automatically learning document taxonomies for hierarchical classification", "author": ["K. Punera", "S. Rajan", "J. Ghosh"], "venue": "In WWW: Special interest tracks & posters,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Improving hierarchical svms by hierarchy flattening and lazy classification", "author": ["H Malik"], "venue": "In Large-Scale HC Workshop of ECIR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Improving taxonomies for large-scale hierarchical classifiers of web docs", "author": ["K. Nitta"], "venue": "In CIKM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "The challenges of clustering high dimensional data", "author": ["M. Steinbach", "L. Ert\u00f6z", "V. Kumar"], "venue": "In New Directions in Statistical Physics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Recursive regularization for large-scale classification with hierarchical & graphical dependencies", "author": ["S. Gopal", "Y. Yang"], "venue": "In ACM SIGKDD,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Hierarchical annotation of medical images", "author": ["I. Dimitrovski", "D. Kocev", "S. Loskovska", "S. D\u017eeroski"], "venue": "Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Hierarchical classification of diatom images using predictive clustering trees", "author": ["I. Dimitrovski", "D. Kocev", "S. Loskovska", "S. D\u017eeroski"], "venue": "Ecological Informatics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "A re-examination of text categorization methods", "author": ["Y. Yang", "X. Liu"], "venue": "In ACM SIGIR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "Discriminative learning of relaxed hierarchy for large-scale visual recognition", "author": ["T. Gao", "D. Koller"], "venue": "In ICCV, pages 2072\u20132079,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "On the merits of building categorization systems by supervised clustering", "author": ["C. Aggarwal", "S. Gates", "P. Yu"], "venue": "In SIGKDD,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "A practical web-based approach to generating topic hierarchy for text segments", "author": ["S. Chuang", "L. Chien"], "venue": "In CIKM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "In the past, researchers have demonstrated the usefulness of hierarchies for classification and have obtained promising results [1\u20135].", "startOffset": 128, "endOffset": 133}, {"referenceID": 1, "context": "In the past, researchers have demonstrated the usefulness of hierarchies for classification and have obtained promising results [1\u20135].", "startOffset": 128, "endOffset": 133}, {"referenceID": 2, "context": "In the past, researchers have demonstrated the usefulness of hierarchies for classification and have obtained promising results [1\u20135].", "startOffset": 128, "endOffset": 133}, {"referenceID": 3, "context": "In the past, researchers have demonstrated the usefulness of hierarchies for classification and have obtained promising results [1\u20135].", "startOffset": 128, "endOffset": 133}, {"referenceID": 4, "context": "In the past, researchers have demonstrated the usefulness of hierarchies for classification and have obtained promising results [1\u20135].", "startOffset": 128, "endOffset": 133}, {"referenceID": 5, "context": "Utilizing the hierarchical structure has been shown to improve the classification performance for rare categories as well [6].", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "Top-down HC methods that leverage the hierarchy during the learning and prediction process are effective approaches to deal with large-scale problems [2].", "startOffset": 150, "endOffset": 153}, {"referenceID": 6, "context": "Though computationally efficient, these methods have higher number of misclassifications due to error propagation [7].", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "For several benchmarks, the HC approaches are outperformed by flat classifiers that ignore the hierarchy [9,10].", "startOffset": 105, "endOffset": 111}, {"referenceID": 9, "context": "For several benchmarks, the HC approaches are outperformed by flat classifiers that ignore the hierarchy [9,10].", "startOffset": 105, "endOffset": 111}, {"referenceID": 7, "context": "using various methods: (b) Agglomerative clustering with cluster cohesion to restrict the height to original height [8] (c) Global-INF flattening method [7] (d) Proposed rewiring method.", "startOffset": 116, "endOffset": 119}, {"referenceID": 6, "context": "using various methods: (b) Agglomerative clustering with cluster cohesion to restrict the height to original height [8] (c) Global-INF flattening method [7] (d) Proposed rewiring method.", "startOffset": 153, "endOffset": 156}, {"referenceID": 10, "context": "\u2022 We propose an efficient data-driven filter based rewiring approach for hierarchy modification which unlike previous wrapper based approaches [11,12] does not require multiple, expensive computations.", "startOffset": 143, "endOffset": 150}, {"referenceID": 11, "context": "\u2022 We propose an efficient data-driven filter based rewiring approach for hierarchy modification which unlike previous wrapper based approaches [11,12] does not require multiple, expensive computations.", "startOffset": 143, "endOffset": 150}, {"referenceID": 13, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 111, "endOffset": 115}, {"referenceID": 6, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 165, "endOffset": 168}, {"referenceID": 15, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 214, "endOffset": 218}, {"referenceID": 7, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 265, "endOffset": 268}, {"referenceID": 16, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 310, "endOffset": 314}, {"referenceID": 10, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 361, "endOffset": 365}, {"referenceID": 11, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 409, "endOffset": 413}, {"referenceID": 12, "context": "\u2022 The modified hierarchy can be used with any hierarchical classification approaches like top-down HC or stateof-the-art approaches incorporating hierarchical relationships [13].", "startOffset": 173, "endOffset": 177}, {"referenceID": 6, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 91, "endOffset": 105}, {"referenceID": 13, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 91, "endOffset": 105}, {"referenceID": 14, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 91, "endOffset": 105}, {"referenceID": 15, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 91, "endOffset": 105}, {"referenceID": 17, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 91, "endOffset": 105}, {"referenceID": 10, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 253, "endOffset": 263}, {"referenceID": 11, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 253, "endOffset": 263}, {"referenceID": 18, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 253, "endOffset": 263}, {"referenceID": 7, "context": "Clustering based methods have also been adapted in some of these studies [8,17] where consistent hierarchy is generated from scratch using agglomerative or divisive clustering algorithms.", "startOffset": 73, "endOffset": 79}, {"referenceID": 16, "context": "Clustering based methods have also been adapted in some of these studies [8,17] where consistent hierarchy is generated from scratch using agglomerative or divisive clustering algorithms.", "startOffset": 73, "endOffset": 79}, {"referenceID": 10, "context": "Hierarchy generated using clustering completely ignores the expert-defined hierarchy information, which contains valuable prior knowledge for classification [11].", "startOffset": 157, "endOffset": 161}, {"referenceID": 10, "context": "2 Proposed Rewiring Approach Wrapper based approaches [11, 12, 19] modify the hierarchy by making one or few changes, which are then evaluated for classification performance improvement using the HC learning", "startOffset": 54, "endOffset": 66}, {"referenceID": 11, "context": "2 Proposed Rewiring Approach Wrapper based approaches [11, 12, 19] modify the hierarchy by making one or few changes, which are then evaluated for classification performance improvement using the HC learning", "startOffset": 54, "endOffset": 66}, {"referenceID": 18, "context": "2 Proposed Rewiring Approach Wrapper based approaches [11, 12, 19] modify the hierarchy by making one or few changes, which are then evaluated for classification performance improvement using the HC learning", "startOffset": 54, "endOffset": 66}, {"referenceID": 19, "context": "Pairwise cosine similarity is used as the similarity measure in our experiments because it is less prone to the curse of dimensionality [20].", "startOffset": 136, "endOffset": 140}, {"referenceID": 0, "context": "rewire[1] = 1; /* check if rewiring is needed for s (1) i */ rewire[2] = 1; /* check if rewiring is needed for s (2) i */ /* Inconsistent pair check */", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "rewire[1] = 1; /* check if rewiring is needed for s (1) i */ rewire[2] = 1; /* check if rewiring is needed for s (2) i */ /* Inconsistent pair check */", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "if \u03c0(s (1) i ) 6= \u03c0(s (2) i ) then /* check similarity to all siblings */ foreach j \u2208 \u03b6(s i ) do if ( (j, s (2) i ) or (s (2) i , j) ) / \u2208 S then rewire[2] = 0; break; end", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "end foreach j \u2208 \u03b6(s i ) do if ( (j, s (1) i ) or (s (1) i , j) ) / \u2208 S then rewire[1] = 0; break; end", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "end if (rewire[1] == 0) and (rewire[2] == 0) then /* perform node creation */ Nnew = \u03c6 /* create new node */ [HM ] = NC(Nnew\u2192lca(si), si\u2192Nnew,HM ); /* lca denotes lowest common ancestor */", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "end if (rewire[1] == 0) and (rewire[2] == 0) then /* perform node creation */ Nnew = \u03c6 /* create new node */ [HM ] = NC(Nnew\u2192lca(si), si\u2192Nnew,HM ); /* lca denotes lowest common ancestor */", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "else if (rewire[1] == 1) then [HM ] = PCRewire(s i \u2192\u03c0(s (2) i ),HM ); else [HM ] = PCRewire(s i \u2192\u03c0(s (1) i ),HM ); end", "startOffset": 15, "endOffset": 18}, {"referenceID": 20, "context": "State-of-the-art HC classification approaches embed the parent-child relationships from the hierarchy either, within the regularization term [21], referred by HR-LR (or HR-SVM) or the loss term, referred by HierCost [13].", "startOffset": 141, "endOffset": 145}, {"referenceID": 12, "context": "State-of-the-art HC classification approaches embed the parent-child relationships from the hierarchy either, within the regularization term [21], referred by HR-LR (or HR-SVM) or the loss term, referred by HierCost [13].", "startOffset": 216, "endOffset": 220}, {"referenceID": 20, "context": "The intuition behind Hierarchy Regularized Logistic Regression (HR-LR) [21] approach is that data-sparse child nodes benefit during training from data-rich parent nodes, and this has been shown to achieve the best performance on standard HC benchmarks.", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "In case of HierCost [13], a cost-sensitive learning approach was adapted.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "In this paper, we use logistic regression (LR) as the underlying base model for training [21].", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "CLEF [22] and DIATOMS [23] are image datasets and the rest are text datasets.", "startOffset": 5, "endOffset": 9}, {"referenceID": 22, "context": "CLEF [22] and DIATOMS [23] are image datasets and the rest are text datasets.", "startOffset": 22, "endOffset": 26}, {"referenceID": 7, "context": "For evaluation, we have restricted the height of clustered hierarchy to the original height by flattening using cluster cohesion [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "Global Inconsistent Node Flattening (Global-INF) [7]: Hierarchy is modified by flattening (removing) the inconsistent nodes based on optimal optimization objective value obtained at each node (eq.", "startOffset": 49, "endOffset": 52}, {"referenceID": 10, "context": "Optimal Hierarchy Search [11]: Optimal hierarchy is identified in the hierarchical space by gradually modifying the expert-defined hierarchy using elementary operations \u2013 promote, demote and merge.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "In the original paper [11], the largest evaluated dataset has 244 classes and 15795 instances.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "3 State-of-the-art Cost-sensitive Learning [13] Similar to flat method but with cost value associated with each instance in the loss function as shown in eq.", "startOffset": 43, "endOffset": 47}, {"referenceID": 7, "context": "TD-LR Clustering [8] Flattening [7] Proposed Metric Agglomerative Global-INF rewHier [Figure 1(a)] [Figure 1(b)] [Figure 1(c)] [Figure 1(d)] \u03bcF1(\u2191) 77.", "startOffset": 17, "endOffset": 20}, {"referenceID": 6, "context": "TD-LR Clustering [8] Flattening [7] Proposed Metric Agglomerative Global-INF rewHier [Figure 1(a)] [Figure 1(b)] [Figure 1(c)] [Figure 1(d)] \u03bcF1(\u2191) 77.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "Dataset Evaluation TD-LR Agglomerative Flattening Rewiring Methods Metrics Clustering [8] Global-INF [7] T-Easy [11] rewHier", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "Dataset Evaluation TD-LR Agglomerative Flattening Rewiring Methods Metrics Clustering [8] Global-INF [7] T-Easy [11] rewHier", "startOffset": 101, "endOffset": 104}, {"referenceID": 10, "context": "Dataset Evaluation TD-LR Agglomerative Flattening Rewiring Methods Metrics Clustering [8] Global-INF [7] T-Easy [11] rewHier", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "Dataset Hierarchy Flattening Rewiring Methods used Global-INF T-Easy [11] rewHier", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "Dataset Baseline Flattening Rewiring Methods TD-LR Global-INF T-Easy [11] rewHier", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "T-Easy [11] 52 156 412 (promote, demote, merge)", "startOffset": 7, "endOffset": 11}, {"referenceID": 23, "context": "We compute the sign-test for \u03bcF1 [24] and non-parametric wilcoxon rank test for MF1 comparing the F1 scores obtained per class for the rewiring methods against the best baseline i.", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "[11], where the expert-defined hierarchy is gradually modified.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Other work in similar direction can be found in [12,19].", "startOffset": 48, "endOffset": 55}, {"referenceID": 18, "context": "Other work in similar direction can be found in [12,19].", "startOffset": 48, "endOffset": 55}, {"referenceID": 13, "context": "Earlier studies focused on flattening based approaches where some level or nodes are selectively flattened (removed) based on certain criterion [14,15,18].", "startOffset": 144, "endOffset": 154}, {"referenceID": 14, "context": "Earlier studies focused on flattening based approaches where some level or nodes are selectively flattened (removed) based on certain criterion [14,15,18].", "startOffset": 144, "endOffset": 154}, {"referenceID": 17, "context": "Earlier studies focused on flattening based approaches where some level or nodes are selectively flattened (removed) based on certain criterion [14,15,18].", "startOffset": 144, "endOffset": 154}, {"referenceID": 15, "context": "In other work, learning based approach have been proposed [16],", "startOffset": 58, "endOffset": 62}, {"referenceID": 6, "context": "[7] proposed a taxonomy adaptation where some nodes are intelligently flattened based on empirically defined cut-off threshold and objective function values computed at each node.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "Hierarchy modification using this approach is scalable and beneficial for classification and has been theoretically justified [25].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "These approaches exploit hierarchical clustering algorithms for generating the hierarchy [8,17,26,27].", "startOffset": 89, "endOffset": 101}, {"referenceID": 16, "context": "These approaches exploit hierarchical clustering algorithms for generating the hierarchy [8,17,26,27].", "startOffset": 89, "endOffset": 101}, {"referenceID": 25, "context": "These approaches exploit hierarchical clustering algorithms for generating the hierarchy [8,17,26,27].", "startOffset": 89, "endOffset": 101}, {"referenceID": 26, "context": "These approaches exploit hierarchical clustering algorithms for generating the hierarchy [8,17,26,27].", "startOffset": 89, "endOffset": 101}], "year": 2016, "abstractText": "Hierarchical Classification (HC) is a supervised learning problem where unlabeled instances are classified into a taxonomy of classes. Several methods that utilize the hierarchical structure have been developed to improve the HC performance. However, in most cases apriori defined hierarchical structure by domain experts is inconsistent; as a consequence performance improvement is not noticeable in comparison to flat classification methods. We propose a scalable data-driven filter based rewiring approach to modify an expertdefined hierarchy. Experimental comparisons of top-down HC with our modified hierarchy, on a wide range of datasets shows classification performance improvement over the baseline hierarchy (i.e., defined by expert), clustered hierarchy and flattening based hierarchy modification approaches. In comparison to existing rewiring approaches, our developed method (rewHier) is computationally efficient, enabling it to scale to datasets with large numbers of classes, instances and features. We also show that our modified hierarchy leads to improved classification performance for classes with few training samples in comparison to flat and state-of-the-art HC approaches. Source code available for reproducibility at: www.cs.gmu.edu/\u223cmlbio/TaxMod Keywords\u2014 Top-Down Hierarchical Classification, Rewiring, Clustering, Flattening", "creator": "LaTeX with hyperref package"}}}