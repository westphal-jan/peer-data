{"id": "1603.00806", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2016", "title": "Hybrid Collaborative Filtering with Autoencoders", "abstract": "Collaborative Filtering aims at exploiting the feedback of users to provide personalised recommendations. Such algorithms look for latent variables in a large sparse matrix of ratings. They can be enhanced by adding side information to tackle the well-known cold start problem. While Neu-ral Networks have tremendous success in image and speech recognition, they have received less attention in Collaborative Filtering. This is all the more surprising that Neural Networks are able to discover latent variables in large and heterogeneous datasets. In this paper, we introduce a Collaborative Filtering Neural network architecture aka CFN which computes a non-linear Matrix Factorization from sparse rating inputs and side information. We show experimentally on the MovieLens and Douban dataset that CFN outper-forms the state of the art and benefits from side information. We provide an implementation of the algorithm as a reusable plugin for Torch, a popular Neural Network framework.", "histories": [["v1", "Wed, 2 Mar 2016 17:48:25 GMT  (239kb,D)", "http://arxiv.org/abs/1603.00806v1", null], ["v2", "Wed, 9 Mar 2016 19:18:09 GMT  (303kb,D)", "http://arxiv.org/abs/1603.00806v2", null], ["v3", "Tue, 19 Jul 2016 08:10:08 GMT  (379kb,D)", "http://arxiv.org/abs/1603.00806v3", null]], "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.NE", "authors": ["florian strub", "jeremie mary", "romaric gaudel"], "accepted": false, "id": "1603.00806"}, "pdf": {"name": "1603.00806.pdf", "metadata": {"source": "META", "title": "Hybrid Collaborative Filtering with Neural Networks", "authors": ["Florian Strub", "Romaric Gaudel"], "emails": ["FLORIAN.STRUB@INRIA.FR", "JEREMIE.MARY@INRIA.FR", "ROMARIC.GAUDEL@INRIA.FR"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Denoising Autoencoders", "text": "The proposed approach is based on autoencoders popularized by Kramer (\u03b2 1991). They are unsupervised networks in which the output of the network aims to reconstruct the original input. The network is forced to use narrow hidden layers, which forces a dimensionality reduction of the data. The network is trained by repropagation of the square error loss during reconstruction. Such networks are divided into two parts: \u2022 the encoder: f (x) = cycle (W1x + b1), \u2022 the decoder: g (y) = cycle (W2y + b2), where x (RN) the input, y (RK) the size of the encoder bottleneck (K < < N), the decoder: g (y) = cycle (W2y + b2), whereby the circuit-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle-cycle), the size of the encoder of the encoder is: K < < N)"}, {"heading": "2.2. Matrix Factorization", "text": "One of the most successful approaches of collaborative filtering is matrix factorization (Koren et al., 2009). This method determines latent factors from the ratings of articles created by users. The underlying idea is that key features are hidden in the ratings themselves. Given N users and M articles, the rating is the rating given by the ith user for the jth element. It contains a sparse matrix of ratings R-RN-M. In collaborative filtering, the sparseness is originally generated by missing values and not by zero values. The goal of the matrix factorization is to find a K matrix in which R-RN-M with U-RN-K and V-RM-K as two matrices of rank K is a dense representation of the user / items with minimal U, V-K-K (i-u-Ti-v-J) and V-K-cR-EcK-line (Vro-Vro)."}, {"heading": "2.3. Related Work", "text": "In a preliminary paper (Salakhutdinov et al., 2007), the Netflix challenge was approached with limited Boltzmann machines, but little published work followed (Truyen et al., 2009). While deep learning has enormous success in image and speech recognition (LeCun et al., 2015), scant data has received less attention and remains a challenging problem for neural networks, neural networks are able to detect nonlinear latent variables with heterogeneous data (LeCun et al., 2015), making them a promising tool for CF. (Sedhain et al., 2015; Strub & Mary, 2015; Dziugaite & Roy, 2015), directly train autoencoders to provide the best predicted assessments (LeCun et al., 2015)."}, {"heading": "2.4. Notation", "text": "In the rest of the article we will use the following notations: \u2022 ui, vj are the sparse lines / columns of R \u2022 u \u044bi, v \u0435j are corrupt versions of ui, vj \u2022 u \u044bi, v \u0435j are dense estimates of R, u, i, v, j are dense representations of ui, vj."}, {"heading": "3. Autoencoders for Collaborative Filtering", "text": "The user preferences are encoded as a sparse matrix of ratings R. A user is represented by a sparse line ui-RN and an object is represented by a sparse column vj-RM. The goal of collaborative filtering can be formulated as follows: Turn the sparse vectors ui / vj into dense vectors u-i / v-j. We propose to perform this conversion using autoencoders. To do this, we need to define two types of autoencoders: \u2022 U-CFN is defined as nn (ui) = u-i, \u2022 V-CFN is defined as nn (vj) = v-j. The encoding part of these autoencoders aims to build a low density representation of the sparse input of ratings. The decoding part aims to predict a dense vector of ratings from the low representation of the encoder."}, {"heading": "3.1. Sparse Inputs", "text": "There is no standard approach to the use of sparse vectors as inputs of neural networks. Most of the work dealing with sparse inputs is circumvented by predicting an estimate of the missing values (Tresp et al., 1994; Bishop, 1995). In our case, we want the autoencoder to handle this prediction problem itself. Such problems have already been investigated in the industry (Miranda et al., 2012), where 5% of the values are missing. However, in collaborative filtering, we often face datasets with more than 95% missing values. Moreover, missing values are not known during training in collaborative filtration, which makes the task even more difficult. Our approach includes three ingredients to handle the formation of sparse autoencoders: \u2022 Inhibit to inhibit the edges of the input layers by eroding the values in the input layer. \u2022 Inhibit to inhibit the edges of the output layers by eroding the output values."}, {"heading": "3.2. Autoencoder and Low Rank Matrix Factorization", "text": "In fact, the response of the network is asnn (x) = W2 \u03c3 (W1x + b1) + b2, where W1, W2 are the weight matrices and b1, b2 are the weight matrices, up to the weight terms b2, if we replace x by the representation of the user i, we recover a predicted vector u: u The weight matrices and the weight matrices (ui) = W2 (ui).V The weight matrices and the weight matrices of the user i: u The weight matrices and the weight matrices (ui) = nn (ui) = W2 (ui).V The weight matrices and the weight matrices are able to process the weight matrices."}, {"heading": "4. Integrating side information", "text": "The simplest approach to rating prediction (Koren et al., 2009): r & # 8222; There is only one way to ensure a better initialization of the system. & # 8220; This is known in the recommendation community as hybridization.The simplest approach to integrating lateral information is to integrate more information. & # 8222; The simplest approach to rating prediction (Koren et al., 2009): & # 8220; The simplest approach to rating prediction (Koren et al.) & # 8220; & # 8222; & # 8220; & # 8220; & # 8220; & # 8220; & # 8222; & # 8222; & # 8222; & # 8220; & # 8222; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8220; & # 8222;."}, {"heading": "P << Ku << N and Q << Kv << M.", "text": "Finally, we get an autoencoder that records page information and can be trained through back propagation. If page information is sparse, the dimension of page information can be adjusted to the number of unequal parameters."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Benchmark Models", "text": "We designate CFN using two SVD techniques that are widely used in the industry. Alternately, smallest squares with weighted \u03bb regularization (ALS-WR) (Zhou et al., 2008) are used to solve the problem of low matrix factorization by alternatively specifying U and V and solving the resulting linear problem. Tikhonov regularization is applied because it provides empirically excellent results: L2 = \u2211 (i, j). The experiments are performed with the Apache-uTi vj) 2 + \u03bb (ni-ui-2Fro + nj-vj-2Fro), where ni is the number of evaluations for the ith user and nj the number of evaluations for the jth point. The experiments are performed with the Apache Mahout software. 2SVDFeature (Chen et al., 2012) is a machine learning toolkit for collaborative filtration."}, {"heading": "5.2. Data", "text": "The MovieLens-1M, MovieLens-10M and MovieLens20M datasets each provide 1 / 10 / 20 million discrete reviews of 6 / 72 / 138 thousand users of 4 / 10 / 27 thousand movies. The ancillary information for MovieLens-1M is the age, gender and gender of the user and the movie category (action, thriller, etc.). The ancillary information for MovieLens-10 / 20M is a matrix of tags T in which Tij is the occurrence of the jth tag for the ith movie and the movie category. No ancillary information is provided to users. The Douban datasets (Ma et al., 2011) provide 17 million discrete reviews of 129 thousand users on 58 thousand movies. The ancillary information is the bi-directional user / friend relationships with users."}, {"heading": "5.3. Error Function", "text": "Two mean square errors (MSEs) exist side by side in auto encoders and one must be careful to use the correct estimator for benchmarking. The classic auto encoder loss estimates the quality of the input reconstruction: MSErec (Xtr) = 1 | Rtr | \u2211 x-Xtr-K (x) [nn (x) k \u2212 xk] 2, where | Rtr | is the number of ratings in the training data sets. In the context of collaborative filtering, this loss provides useful information during the training phase, but the quality of the learned auto encoder is given by its predictive accuracy for unknown inputs. This predictive accuracy is summarized in pairs (xte, xtr) of training test examples: MSEpred (Xte, Xtr) = 1 | Rte (xtr, xtr)."}, {"heading": "5.4. Training Settings", "text": "We train 4-layer autoencoders for MovieLens-1M and 2-layer autoencoders for MovieLens-10 / 20M and the Douban datasets. The first and second layers have between 500 and 700 hidden neurons. The decoding layers have the same dimension in reverse order. The weights are initialized with the fan-in-rule Wij \u0445 U [\u2212 1 \u221a n, \u2212 1 \u221a n] (LeCun et al., 1998). Transmission functions are hyperbolic tangents. The neural network is optimized by stochastic backpropagation with a size 30 minibatch and a weight decay is added to regularize. Hyperparameters 4 are tuned by a simple genetic algorithm already used by (Teytaud et al., 2007) in another context."}, {"heading": "5.5. Results", "text": "To the best of our knowledge, the best results with respect to MovieLens-1M and MovieLens-10M are published by both (Lee et al., 2013; Sedhain et al., 2015) with a final RMSE of 0.831 \u00b1 0.003 and 0.782 \u00b1 0.003, respectively, with a training ratio of 90% / 10% and no incidental information. (Kim & Choi, 2014) use a scalable version of the BPMF with incidental information, and they each give 0.844 and 0.6856 RMSE values for MovieLens-10M and Douban, respectively, with a training rate of 80% / 20%, while V-CFN provides + 0.782 and 0.694, respectively, with the same experimental conditions."}, {"heading": "5.6. Discussion", "text": "It is competitive compared to the state-of-the-art collaborative filtering algorithms and clearly surpasses them in terms of MovieLens-10M. (Kim & Choi, 2014; Kumar et al., 2014) The confidence interval of CFN is also low in terms of the large data sets that make it a robust algorithm for collaborative filtering. In the experiments, it exceeds the source code of U-CFN. It indicates that the structure of the products is stronger than that of the users. It is easier to guess the taste of movies than to determine the number of users."}, {"heading": "6. Remarks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Source code", "text": "Torch is a powerful framework written in Lua to quickly prototype neural networks. It is a widely used (Facebook, Deep Mind) industry standard. However, Torch lacks some important basic tools to deal with sparse input. Therefore, we are developing several new modules to deal with DAE losses, sparse DAE losses and sparse input on both the CPU and the GPU. They can easily be plugged into existing code. An out-of-the-box tutorial is available to perform the experiments directly. The code is available free of charge on Github and Luarocks. 5"}, {"heading": "6.2. Scalability", "text": "A major problem that most collaborative filtering needs to solve is scalability, as datasets often have hundreds of thousands of users and elements. An efficient algorithm needs to be trained in a reasonable amount of time and provide rapid feedback during the evaluation period. Recent advances in GPU calculation have succeeded in reducing the training time of neural networks by several orders of magnitude. However, collaborative filtering deals with sparse data and GPUs are designed to work well with dense data. (Salakhutdinov et al., 2007; Sedhain et al., 2015) We meet this constraint by building small dense networks with common weights. Nevertheless, this approach can lead to important synchronization latencies. In our case, we address the problem by selectively condensing the inputs before sending them to the GPUs cores without altering the result of the calculation."}, {"heading": "6.3. Future Works", "text": "Implicit feedback could vastly improve the quality of collaborative filtering algorithms (Koren et al., 2009; Rendle, 2010). For example, implicit feedback would be fed into the CFN by providing the network with additional binary input. (Salakhutdinov et al., 2007) This would improve the quality of prediction for the restricted Boltzmann machine on the Netflix dataset. Furthermore, the Content Based Technique would be connected to the CFN with in-depth learning such as (den Oord et al., 2013; Wang et al., 2014b). The idea would be to train a common network that directly links the raw article attributes to ratings such as music, images or word representations."}, {"heading": "7. Conclusion", "text": "Unlike other experiments with neural networks, this common network integrates page information and learns a nonlinear representation of users or objects in a unique neural network. This approach surpasses both the state of the art in CF and the cold start problem in MovieLens and Douban datasets. CFN is also scalable and robust for handling large datasets. We have made several assertions that autoencoders are closely linked to the low-level matrix factorization in Collaborative Filtering. Finally, a reusable source code is provided in Torch and hyperparameters are provided to reproduce the results."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors would like to acknowledge the stimulating environment provided by the SequeL, Inria and CRIStAL research group, supported by the French Ministry of Higher Education and Research, the CPER Nord-Pas de Calais / FEDER DATA Advanced data science and technologies 2015-2020, the CHIST-ERA IGLU project and FUI Herme. Experiments were tested with Grid '5000, supported by Inria, CNRS, RENATER and several universities and other organisations."}], "references": [{"title": "Incorporating side information in probabilistic matrix factorization with gaussian processes", "author": ["R.P. Adams", "Murray", "G.E. Dahland I"], "venue": "arXiv preprint arXiv:1003.4944,", "citeRegEx": "Adams et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2010}, {"title": "Conditional random field autoencoders for unsupervised structured prediction", "author": ["W. Ammar", "C. Dyer", "N.A. Smith"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": "Oxford university press,", "citeRegEx": "Bishop,? \\Q1995\\E", "shortCiteRegEx": "Bishop", "year": 1995}, {"title": "Svdfeature: a toolkit for feature-based collaborative filtering", "author": ["T. Chen", "W. Zhang", "Q. Lu", "K. Chen", "Z. Zheng", "Y. Yong"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Deep content-based music recommendation", "author": ["den Oord", "A. Van", "S. Dieleman", "B. Schrauwen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2013}, {"title": "Neural network matrix factorization", "author": ["G.K. Dziugaite", "D.M. Roy"], "venue": "arXiv preprint arXiv:1511.06443,", "citeRegEx": "Dziugaite and Roy,? \\Q2015\\E", "shortCiteRegEx": "Dziugaite and Roy", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot and Bengio,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "The netflix recommender system: Algorithms, business value, and innovation", "author": ["C. Gomez-Uribe", "N. Hunt"], "venue": "ACM Trans. Manage. Inf. Syst.,", "citeRegEx": "Gomez.Uribe and Hunt,? \\Q2015\\E", "shortCiteRegEx": "Gomez.Uribe and Hunt", "year": 2015}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["K. J\u00e4rvelin", "K. Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen", "year": 2002}, {"title": "Scalable variational bayesian matrix factorization with side information", "author": ["Kim", "Yong-Deok", "Choi", "Seungjin"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), Reykjavik,", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": null, "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Nonlinear principal component analysis using autoassociative neural networks", "author": ["M.A. Kramer"], "venue": "AIChE journal,", "citeRegEx": "Kramer,? \\Q1991\\E", "shortCiteRegEx": "Kramer", "year": 1991}, {"title": "Social popularity based svd++ recommender system", "author": ["R. Kumar", "B.K. Verma", "S.S. Rastogi"], "venue": "International Journal of Computer Applications,", "citeRegEx": "Kumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2014}, {"title": "Non-linear matrix factorization with gaussian processes", "author": ["N.D. Lawrence", "R. Urtasun"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Lawrence and Urtasun,? \\Q2009\\E", "shortCiteRegEx": "Lawrence and Urtasun", "year": 2009}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.R. Muller"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A.Y. Ng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Lee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "A comparative study of collaborative filtering algorithms", "author": ["J. Lee", "M. Sun", "G. Lebanon"], "venue": "arXiv preprint arXiv:1205.3193,", "citeRegEx": "Lee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2012}, {"title": "Local lowrank matrix approximation", "author": ["J. Lee", "S. Kim", "G. Lebanon", "Y. Singerm"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Deep collaborative filtering via marginalized denoising auto-encoder", "author": ["S. Li", "J. Kawale", "Y. Fu"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Content-based recommender systems: State of the art and trends", "author": ["P. Lops", "Gemmis", "M. De", "G. Semeraro"], "venue": "In Recommender systems handbook,", "citeRegEx": "Lops et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lops et al\\.", "year": 2011}, {"title": "Recommender systems with social regularization", "author": ["H. Ma", "D. Zhou", "C. Liu", "M.R. Lyu", "I. King"], "venue": "In Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "Ma et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2011}, {"title": "Reconstructing Missing Data in State Estimation With Autoencoders", "author": ["V. Miranda", "J. Krstulovic", "H. Keko", "C. Moreira", "J. Pereira"], "venue": "IEEE Transactions on Power Systems,", "citeRegEx": "Miranda et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Miranda et al\\.", "year": 2012}, {"title": "Probabilistic matrix factorization", "author": ["A. Mnih", "R. Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih and Salakhutdinov,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Salakhutdinov", "year": 2007}, {"title": "Bayesian matrix factorization with side information and dirichlet process mixtures", "author": ["I. Porteous", "A.U. Asuncion", "M. Welling"], "venue": "In AAAI,", "citeRegEx": "Porteous et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Porteous et al\\.", "year": 2010}, {"title": "Factorization machines", "author": ["S. Rendle"], "venue": "In Data Mining (ICDM),", "citeRegEx": "Rendle,? \\Q2010\\E", "shortCiteRegEx": "Rendle", "year": 2010}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Salakhutdinov and Mnih,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Mnih", "year": 2008}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Autorec: Autoencoders meet collaborative filtering", "author": ["S. Sedhain", "A.K. Menon", "S. Sanner", "L. Xie"], "venue": "In Proceedings of the 24th International Conference on World Wide Web Companion,", "citeRegEx": "Sedhain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sedhain et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N Srivastava", "G. Hinton", "A. Krizhevsk", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Collaborative Filtering with Stacked Denoising AutoEncoders and Sparse Inputs", "author": ["F. Strub", "J. Mary"], "venue": "In NIPS Workshop on Machine Learning for eCommerce,", "citeRegEx": "Strub and Mary,? \\Q2015\\E", "shortCiteRegEx": "Strub and Mary", "year": 2015}, {"title": "Active learning in regression, with application to stochastic dynamic programming", "author": ["O. Teytaud", "J. Mary"], "venue": "In International Conference On Informatics in Control, Automation and Robotics (eds.), ICINCO and CAP,", "citeRegEx": "Teytaud et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teytaud et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 20, "context": "studied contest: CF only relies on the ratings of the users while Content-Based Filtering requires advanced engineering on items to perform well (Lops et al., 2011).", "startOffset": 145, "endOffset": 164}, {"referenceID": 11, "context": "Common latent factor techniques compute a low-rank rating matrix by applying Singular Value Decomposition through gradient descent (Koren et al., 2009) or Regularized Alternative Least Square algorithm (Zhou et al.", "startOffset": 131, "endOffset": 151}, {"referenceID": 25, "context": "Newer algorithms were explored to face those constraints such as Non Linear Probabilistic Matrix Factorization (Lawrence & Urtasun, 2009), Factorization Machines (Rendle, 2010) or Local Low Rank Matrix Approximation (Lee et al.", "startOffset": 162, "endOffset": 176}, {"referenceID": 18, "context": "Newer algorithms were explored to face those constraints such as Non Linear Probabilistic Matrix Factorization (Lawrence & Urtasun, 2009), Factorization Machines (Rendle, 2010) or Local Low Rank Matrix Approximation (Lee et al., 2013).", "startOffset": 216, "endOffset": 234}, {"referenceID": 17, "context": "However, recent algorithms outperform them in the general case (Lee et al., 2012).", "startOffset": 63, "endOffset": 81}, {"referenceID": 27, "context": "Compared to previous attempts in that direction (Salakhutdinov et al., 2007; Sedhain et al., 2015; Strub & Mary, 2015; Dziugaite & Roy, 2015), our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 141}, {"referenceID": 28, "context": "Compared to previous attempts in that direction (Salakhutdinov et al., 2007; Sedhain et al., 2015; Strub & Mary, 2015; Dziugaite & Roy, 2015), our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 141}, {"referenceID": 12, "context": "The proposed approach builds upon Autoencoders which are feed-forward Neural Networks popularized by Kramer (1991). They are unsupervised Networks where the output of the Network aims at reconstructing the initial input.", "startOffset": 101, "endOffset": 115}, {"referenceID": 11, "context": "One of the most successful approach of Collaborative Filtering is Matrix Factorization (Koren et al., 2009).", "startOffset": 87, "endOffset": 107}, {"referenceID": 27, "context": "In a preliminary work, (Salakhutdinov et al., 2007) tackled the Netflix challenge using Restricted Boltzmann Machines but little published work had follow (Truyen et al.", "startOffset": 23, "endOffset": 51}, {"referenceID": 28, "context": "(Sedhain et al., 2015; Strub & Mary, 2015; Dziugaite & Roy, 2015) directly train Autoencoders to provide the best predicted ratings.", "startOffset": 0, "endOffset": 65}, {"referenceID": 28, "context": "For instance, AutoRec (Sedhain et al., 2015) replaces unpredictable ratings by an arbitrary selected score.", "startOffset": 22, "endOffset": 44}, {"referenceID": 7, "context": "For instance, (Glorot et al., 2011; Wang et al., 2014a) respectively auto-encode bag-of-words from restaurant reviews and movie plots, (Li et al.", "startOffset": 14, "endOffset": 55}, {"referenceID": 19, "context": ", 2014a) respectively auto-encode bag-of-words from restaurant reviews and movie plots, (Li et al., 2015) auto-encode heterogeneous side information from users and items.", "startOffset": 88, "endOffset": 105}, {"referenceID": 2, "context": "Most of the papers dealing with sparse inputs get around by pre-computing an estimate of the missing values (Tresp et al., 1994; Bishop, 1995).", "startOffset": 108, "endOffset": 142}, {"referenceID": 22, "context": "Such problems have already been studied in industry (Miranda et al., 2012) where 5% of the values are missing.", "startOffset": 52, "endOffset": 74}, {"referenceID": 27, "context": "This operation is equivalent to removing the neurons with missing values described in (Salakhutdinov et al., 2007; Sedhain et al., 2015).", "startOffset": 86, "endOffset": 136}, {"referenceID": 28, "context": "This operation is equivalent to removing the neurons with missing values described in (Salakhutdinov et al., 2007; Sedhain et al., 2015).", "startOffset": 86, "endOffset": 136}, {"referenceID": 16, "context": "Importantly, Autoencoders with sparse inputs differs from sparse-Autoencoders (Lee et al., 2006) or Dropout regularization (Srivastava et al.", "startOffset": 78, "endOffset": 96}, {"referenceID": 11, "context": "The simplest approach to integrate side information is to append additional user/item bias to the rating prediction (Koren et al., 2009):", "startOffset": 116, "endOffset": 136}, {"referenceID": 1, "context": "One notable example for bitext word alignment was recently made by (Ammar et al., 2014).", "startOffset": 67, "endOffset": 87}, {"referenceID": 3, "context": "SVDFeature (Chen et al., 2012) is a Machine Learning Toolkit for feature-based Collaborative Filtering.", "startOffset": 11, "endOffset": 30}, {"referenceID": 21, "context": "The Douban dataset (Ma et al., 2011) provides 17 million discrete ratings from 129 thousands users on 58 thousands movies.", "startOffset": 19, "endOffset": 36}, {"referenceID": 15, "context": "Weights are initialized using the fan-in rule Wij \u223c U [ \u2212 1 \u221a n ,\u2212 1 \u221a n ] (LeCun et al., 1998).", "startOffset": 75, "endOffset": 95}, {"referenceID": 31, "context": "Hyperparameters 4 are tuned by a simple genetic algorithm already used by (Teytaud et al., 2007) in a different context.", "startOffset": 74, "endOffset": 96}, {"referenceID": 18, "context": "To the best of our knowledge, the best results published regarding MovieLens-1M and MovieLens-10M are reported by both (Lee et al., 2013; Sedhain et al., 2015) with a final RMSE of 0.", "startOffset": 119, "endOffset": 159}, {"referenceID": 28, "context": "To the best of our knowledge, the best results published regarding MovieLens-1M and MovieLens-10M are reported by both (Lee et al., 2013; Sedhain et al., 2015) with a final RMSE of 0.", "startOffset": 119, "endOffset": 159}, {"referenceID": 13, "context": "(Kim & Choi, 2014; Kumar et al., 2014).", "startOffset": 0, "endOffset": 38}, {"referenceID": 27, "context": "(Salakhutdinov et al., 2007; Sedhain et al., 2015) face this sparsity constraint by building small dense Networks with shared weights.", "startOffset": 0, "endOffset": 50}, {"referenceID": 28, "context": "(Salakhutdinov et al., 2007; Sedhain et al., 2015) face this sparsity constraint by building small dense Networks with shared weights.", "startOffset": 0, "endOffset": 50}, {"referenceID": 11, "context": "Implicit feedback may greatly enhance the quality of Collaborative Filtering algorithms (Koren et al., 2009; Rendle, 2010).", "startOffset": 88, "endOffset": 122}, {"referenceID": 25, "context": "Implicit feedback may greatly enhance the quality of Collaborative Filtering algorithms (Koren et al., 2009; Rendle, 2010).", "startOffset": 88, "endOffset": 122}, {"referenceID": 27, "context": "By doing so, (Salakhutdinov et al., 2007) enhance the quality of prediction for Restricted Boltzmann Machine on the Netflix Dataset.", "startOffset": 13, "endOffset": 41}], "year": 2017, "abstractText": "Collaborative Filtering aims at exploiting the feedback of users to provide personalised recommendations. Such algorithms look for latent variables in a large sparse matrix of ratings. They can be enhanced by adding side information to tackle the well-known cold start problem. While Neural Networks have tremendous success in image and speech recognition, they have received less attention in Collaborative Filtering. This is all the more surprising that Neural Networks are able to discover latent variables in large and heterogeneous datasets. In this paper, we introduce a Collaborative Filtering Neural network architecture aka CFN which computes a non-linear Matrix Factorization from sparse rating inputs and side information. We show experimentally on the MovieLens and Douban dataset that CFN outperforms the state of the art and benefits from side information. We provide an implementation of the algorithm as a reusable plugin for Torch, a popular Neural Network framework.", "creator": "LaTeX with hyperref package"}}}