{"id": "1603.00930", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2016", "title": "Super Mario as a String: Platformer Level Generation Via LSTMs", "abstract": "The procedural generation of video game levels has existed for at least 30 years, but only recently have machine learning approaches been used to generate levels without specifying the rules for generation. A number of these have looked at platformer levels as a sequence of characters and performed generation using Markov chains. In this paper we examine the use of Long Short-Term Memory recurrent neural networks (LSTMs) for the purpose of generating levels trained from a corpus of Super Mario Brothers levels. We analyze a number of different data representations and how the generated levels fit into the space of human authored Super Mario Brothers levels.", "histories": [["v1", "Wed, 2 Mar 2016 23:44:03 GMT  (775kb,D)", "http://arxiv.org/abs/1603.00930v1", null], ["v2", "Tue, 8 Mar 2016 21:26:58 GMT  (778kb,D)", "http://arxiv.org/abs/1603.00930v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["adam summerville", "michael mateas"], "accepted": false, "id": "1603.00930"}, "pdf": {"name": "1603.00930.pdf", "metadata": {"source": "CRF", "title": "Super Mario as a String: Platformer Level Generation Via LSTMs", "authors": ["Adam Summerville", "Michael Mateas"], "emails": ["asummerv@ucsc.edu", "michaelm@soe.ucsc.edu"], "sections": [{"heading": "INTRODUCTION", "text": "Artificial Content Generation (PCG) of video game layers has been around for many decades, with the earliest known use coming from Beneath Apple Manor in 1978. As an area of academic study, numerous approaches have been pursued, ranging from man-made rules of tile placement (Shaker et al. 2011) to restrictions on solution and planning (Smith, Whitehead and Mateas 2010), which require the algorithm designer to define either the rules of the generation or a method for evaluating the levels generated. More recently, machine learning approaches have been used so that a designer can build a generator from examples, ranging from matrix factorization (Shaker and Abou-Zleikha 2014) (Summerville and Mateas 2015) to a number of different Markov chain approaches (Dahlskog, Togelius and Nelson 2014) (Snodgrass and not 2013)."}, {"heading": "RELATED WORK", "text": "The transitions in the Markov chains have seen two main approaches that lead to either different transitions from tiles to tiles or to vertical parts of the plane (Dahlskog, Togelius and Nelson 2014) and was later learned by Summerville et al. (Summerville, Philip, and Mateas 2015). In this approach, the states of the Markov chain are taken as the vertical disk of the tiles and the transitions are learned from slice-to-slice. This has the advantage that the vertical structure is directly encoded, which is important given the fact that certain tiles are much more likely to stand at the top (such as empty tiles) and others at the bottom (such as the floor). However, this comes with the severe reduction of the ability to create novel vertical slots."}, {"heading": "LSTM SEQUENCE GENERATION", "text": "In this section, we will first discuss how LSTM Recurrent Neural Networks work, and then we will discuss the basics of our data specification and the variants we use."}, {"heading": "Recurrent Neural Network Architecture", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "Data Specification", "text": "In fact, it is not the case that we present ourselves in a way in which we present them to us, in a way in which we present them to us, in a way in which we present them to us, namely in a way in which we present them to us: in a way in which we present them to us, in a way in which we present them to us, in a way in which we understand them, in a way in which we present them to us, in a way in which we present them to us, in a way in which we present them to us, in a way in which we understand them, in a way in which we understand them, in a way in which we understand them, in which we understand them, in which we understand them, in which we present them, in which they, like them, like them, like them, like them, like them, like them, like them, like them, like them, like them, like them, like them, like them, like them, they, they, like them, they, they, they, like them, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they"}, {"heading": "RESULTS", "text": "We trained eight networks, one for each of the induced sequences that you can see in the table. We had a total of 15 levels of Super Mario Brothers and 24 levels of Japanese Super Mario Brothers 2 for a total of 39 levels. We used a 70% -30% training evaluation split during the course of the training. We had a total of 15 levels of Super Mario Brothers and 24 levels of Japanese Super Mario Brothers 2 for a total of 39 levels. We used a 70% -30% training evaluation split during the training. After each 200 tiles in the training sequence we performed an evaluation at a held evaluation level, noting how well the LSTM was able to predict invisible sequences. We save this version of the network when it has the best score on the evaluation level, believing that this will be the network that has the best ability to generate from the training. If the training is a plateau for more than 100% of the likelihood that the training will run through the error epoch (which is most likely)."}, {"heading": "CONCLUSION AND FUTURE WORK", "text": "In this paper, we have shown a novel interpretation of the Super Mario Brothers level as a linear sequence. Most importantly, the introduction of player paths can dramatically improve the quality of the levels generated, especially in terms of the playability of the levels generated. We have also used a wider range of metrics than previously used to perform a more nuanced comparison between the levels generated. We believe that the introduction of path metrics is closer to the human levels. We have used a wider range of metrics for the characterization of our levels so that we can make a nuanced comparison between the levels generated and the levels generated."}], "references": [{"title": "and Smith", "author": ["A. Canossa"], "venue": "G.", "citeRegEx": "Canossa and Smith 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "M", "author": ["S. Dahlskog", "J. Togelius", "Nelson"], "venue": "J.", "citeRegEx": "Dahlskog. Togelius. and Nelson 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "E", "author": ["Denton"], "venue": "L.; Chintala, S.; Szlam, A.; and Fergus, R.", "citeRegEx": "Denton et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks. CoRR abs/1303.5778", "author": ["rahman Mohamed Graves", "A. Hinton 2013] Graves", "A. rahman Mohamed", "G.E. Hinton"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor"], "venue": "In CoRR", "citeRegEx": "Gregor,? \\Q2015\\E", "shortCiteRegEx": "Gregor", "year": 2015}, {"title": "M", "author": ["M. Guzdial", "Riedl"], "venue": "O.", "citeRegEx": "Guzdial and Riedl 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Graph grammars for super mario bros levels", "author": ["no", "S.L. Missura 2015] no", "O. Missura"], "venue": "In Proceedings of the FDG workshop on Procedural Content Generation in Games", "citeRegEx": "no et al\\.,? \\Q2015\\E", "shortCiteRegEx": "no et al\\.", "year": 2015}, {"title": "and AbouZleikha", "author": ["N. Shaker"], "venue": "M.", "citeRegEx": "Shaker and Abou.Zleikha 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "P", "author": ["N. Shaker", "J. Togelius", "G.N. Yannakakis", "B.G. Weber", "T. Shimizu", "T. Hashiyama", "N. Sorenson", "P. Pasquier", "Mawhorter"], "venue": "A.; Takahashi, G.; Smith, G.; and Baumgarten, R.", "citeRegEx": "Shaker et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Whitehead", "author": ["G. Smith"], "venue": "J.", "citeRegEx": "Smith and Whitehead 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Tanagra: a mixed-initiative level design tool", "author": ["Whitehead Smith", "G. Mateas 2010] Smith", "J. Whitehead", "M. Mateas"], "venue": "In FDG,", "citeRegEx": "Smith et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2010}, {"title": "and n\u00f3n", "author": ["S. Snodgrass"], "venue": "S. O.", "citeRegEx": "Snodgrass and n\u00f3n 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and n\u00f3n", "author": ["S. Snodgrass"], "venue": "S. O.", "citeRegEx": "Snodgrass and n\u00f3n 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and n\u00f3n", "author": ["S. Snodgrass"], "venue": "S. O.", "citeRegEx": "Snodgrass and n\u00f3n 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Srivastava,? \\Q2014\\E", "shortCiteRegEx": "Srivastava", "year": 2014}, {"title": "and Mateas", "author": ["A. Summerville"], "venue": "M.", "citeRegEx": "Summerville and Mateas 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "The learning of zelda: Datadriven learning of level topology", "author": ["Summerville"], "venue": "In Proceedings of the FDG workshop on Procedural Content Generation in Games", "citeRegEx": "Summerville,? \\Q2015\\E", "shortCiteRegEx": "Summerville", "year": 2015}, {"title": "Mcmcts pcg 4 smb: Monte carlo tree search to guide platformer level generation", "author": ["Philip Summerville", "A. Mateas 2015] Summerville", "S. Philip", "M. Mateas"], "venue": "In AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment", "citeRegEx": "Summerville et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Summerville et al\\.", "year": 2015}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le"], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy"], "venue": "In CoRR", "citeRegEx": "Szegedy,? \\Q2014\\E", "shortCiteRegEx": "Szegedy", "year": 2014}, {"title": "R", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "Mooney"], "venue": "J.; and Saenko, K.", "citeRegEx": "Venugopalan et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Sutskever", "author": ["W. Zaremba"], "venue": "I.", "citeRegEx": "Zaremba and Sutskever 2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2017, "abstractText": "The procedural generation of video game levels has existed for at least 30 years, but only recently have machine learning approaches been used to generate levels without specifying the rules for generation. A number of these have looked at platformer levels as a sequence of characters and performed generation using Markov chains. In this paper we examine the use of Long ShortTerm Memory recurrent neural networks (LSTMs) for the purpose of generating levels trained from a corpus of Super Mario Brothers levels. We analyze a number of different data representations and how the generated levels fit into the space of human authored Super Mario", "creator": "LaTeX with hyperref package"}}}