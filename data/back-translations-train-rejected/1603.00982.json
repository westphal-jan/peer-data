{"id": "1603.00982", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Audio Word2Vec: Unsupervised Learning of Audio Segment Representations using Sequence-to-sequence Autoencoder", "abstract": "Representing audio segments expressed with variable-length acoustic feature sequences as fixed-length feature vectors is usually needed in many speech applications, including speaker identification, audio emotion classification and spoken term detection (STD). In this paper, we apply and extend sequence-to-sequence learning framework to learn representations for audio segments without any supervision. The model we used is called Sequence-to-sequence Autoencoder (SA), which consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN acts as an encoder that maps the input sequence into a vector representation of fixed dimensionality, and the second RNN acts as a decoder that maps the representation back to the input sequence. The two RNNs are then jointly trained by minimizing the reconstruction error. We further propose Denoising Sequence-to-sequence Autoencoder (DSA) that improves the learned representations. The vector representations learned by SA and DSA are shown to be very helpful for query-by-example STD. The experimental results have shown that the proposed models achieved better retrieval performance than using audio segment representation designed heuristically and the classical Dynamic Time Warping (DTW) approach.", "histories": [["v1", "Thu, 3 Mar 2016 05:44:51 GMT  (205kb,D)", "http://arxiv.org/abs/1603.00982v1", null], ["v2", "Tue, 15 Mar 2016 14:16:28 GMT  (196kb,D)", "http://arxiv.org/abs/1603.00982v2", null], ["v3", "Thu, 17 Mar 2016 06:11:47 GMT  (196kb,D)", "http://arxiv.org/abs/1603.00982v3", null], ["v4", "Sat, 11 Jun 2016 03:40:23 GMT  (201kb,D)", "http://arxiv.org/abs/1603.00982v4", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["yu-an chung", "chao-chung wu", "chia-hao shen", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1603.00982"}, "pdf": {"name": "1603.00982.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning of Audio Segment Representations using Sequence-to-sequence Recurrent Neural Networks", "authors": ["Yu-An Chung", "Chao-Chung Wu", "Chia-Hao Shen", "Hung-Yi Lee"], "emails": ["hungyilee}@ntu.edu.tw"], "sections": [{"heading": null, "text": "In this paper, we apply the Sequence-to-Sequence Autoencoder (SA) model, which consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN acts as an encoder that maps the input sequence into a fixed-dimensionality vector representation, and the second RNN acts as a decoder that maps the representation back to the input sequence, and the two RNNNs are then trained together to minimize the reconstruction error. Furthermore, we demonstrate that Denoising Sequence-to-Sequence Autoencoder (DSA) improves the learned representations, and the two RNNNNs are then better represented by minimizing the reconstruction error than the results obtained by using D models."}, {"heading": "1. Introduction", "text": "In fact, most of them will be able to orient themselves in a different direction from that in which they have gone."}, {"heading": "2. Proposed Approach", "text": "In this section, we describe the approach to learning unsupervised audio segment representations. An audio segment is usually expressed with an input of different length x = (x1, x2,..., xT), with each xt symbol being an acoustic characteristic like MFCC. The goal is to automatically encode any audio segment sequence x with different T into a fixed-length vector representation z-Rd, where d is the dimension of the encoded space. The learned representation z can be used in a variety of applications, for example as a query byexample STD as in Section 3. Below, we first give a brief review of the RNN encoder decoder framework [17, 18], which is widely used to implement sequence-to-sequence learning in Section 2.1."}, {"heading": "2.1. RNN Encoder-Decoder framework", "text": "In the neeisn eeisrrcnlhSe nvo the eeisn-eaJnlrmhsrteeeSrmnr-eaHnnlrrrrrrrrrrrrteeaeSrlcnlrrrrrrrrrrrrrrrrrrrrrrrrrrrrlgn-eaeeeeeeaeeaeeaeoiuiuiuiuiuiuiuiuiuioioiaeSrmnr-eaeaeeaeaeeeoioiKnlrsn-nlrsn-nlso \"rrrrrrrrrsn-nso\" teeos \"rrrrrrrrreos\" (rf\u00fc-eaeaeaeeeeeeeeeeeeerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrllll.com), rD (rfu-rrrfrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2.3. Denoising Sequence-to-sequence Autoencoder (DSA)", "text": "To learn a more robust and meaningful representation, we apply the denoising criterion to SA learning. The acoustic input sequence x is randomly added to produce a corrupt version x. here is the input of SA x, and SA must output (reconstruct) the original x from the corrupt input x. The SA that has learned with the denoising criterion is referred to in the following paper as Denoising SA (DSA)."}, {"heading": "3. Application: Query-by-example STD", "text": "The z audio segment representation learned in the last section is applied to unattended query-by-example STD. The objective of the unattended query-by-example STD is to locate the occurrence regions of the term in the input query without speech recognition. Figure 2 shows how we used the z representation in unattended query-by-example. This framework is inspired by the previous work [6], but here a completely different approach is used to represent the audio segments. In the upper half of Figure 2, the audio archives in the database are segmented into a series of variable length segments based on word boundaries, and then the system uses the RNN encoder in Figure 1 to encode the word segments in fixed length vectors. In view of the Audio query (the lower left corner of Figure 2), it is also converted by the RNN encoder into a vector."}, {"heading": "4. Experiments", "text": "In this section, we analyze the learned representations and apply them in Query-by-example STD. In Section 4.1, we first describe the experimental setup. Then, in Section 4.2, we quantitatively analyze the vector representations learned by SA and DSA. Finally, in Section 4.3, we show that SA and DSA are useful in Query-by-example STD."}, {"heading": "4.1. Experimental Setup", "text": "We used LibriSpeech1 Corpus [27], which is derived from audiobooks read from the LibriVox2 project, as data for experiments. For the lack of computing power, only the 5.4-hour dev-clean subset was used for training SA and DSA. The 5.4-hour testclean subset was the test set. Both the training and the test sets contain 40 different loudspeakers. MFCCs of 13-Dim were used as acoustic characteristics. Both the training and the test sets were segmented according to the word boundaries achieved by forced alignment with reference transcriptions. Although the oracle word boundaries were used, since the baselines used the same segmentation, the comparison in this paper was implemented fair.SA and DSA were implemented with Theano [28, 29] the network structure and hyperparameters were set as below, without further coordination if nothing was specified: Both NRN and NRN were not encoded with one or both."}, {"heading": "4.2. Analysis of the Learned Representations", "text": "In this section, we want to confirm that the words with similar pronunciation in the vicinity of SA and DSA would have vector representations. We used the SA and DSA learned from the training to encode the segments in the test group that were never seen in the training phase, and then calculated the cosmic similarity between the individual segment pairs in the test group. The results 1http: / / www.openslr.org / 12 / 2https: / librivox.orgare in Table 1, in which the average cosmic similarity of the segment pairs with different phoneme distances is displayed in each line. It is clear that if two segments are words with larger phoneme distances (i.e., more pronunciations), on average, the cosmic similarity of the word pairs with the word pairs with different phoneme editions is displayed in each line. We also found that SA and DSA can distinguish the words with only one other phoneme, because the similarity is greater than the distance that of the expressions."}, {"heading": "4.3. Query-by-example STD", "text": "Here we compared the performance of using the representations from SA and DSA in query-by-example with two baselines = 39. The first baseline uses DTW [34], which is a common algorithm in the current query-by-example application. Here, we used DTW to calculate the similarities between the input query and the audio segments directly, and ranked the NENENEX segments by the similarities. Only the vanilla version of DTW was used here, and the Euclidean distance was used to evaluate the difference between acoustic properties. The second baseline uses the same frame in Figure 2, except that the NEX segments are replaced by a manually designed encoder. We use the example in Figure 3 to illustrate this approach. The approach contains three procedures: di-vide, average, and linked. We assume that there is an input sequence x (x1, x18)."}, {"heading": "5. Conclusions and Future Work", "text": "We apply and extend Sequence-to-Sequence Autoencoder (SA) for unattended learning of audio segment representations and show that SA can learn meaningful representations; the learned representations can have many applications; in this preliminary study we used them in Query-by-Query STD. Experimental results showed that SA achieved higher MAP values than the DTW-based approach and heuristically designed audio segment representation, and the denocialization criterion further improved the learned representations; for future work, we train SA on larger corporations with different dimensions, and test the proposed approaches in terms of audio segmentation achieved through unattended segmentation; we will use the indexing approaches to make the framework in Figure 2 more efficient and compare the proposed approaches with more modern Query-for-100 examples STD methods.We have not adjusted the dimensionality of the representations to match the 3."}, {"heading": "6. References", "text": "[1] N. Dehak, R. Dehak, P. Kenny, N. Brummer, P. Ouellet, and P. Du-mouchel, \"Support vector machines versus fast scoring in the lowdimensional total variability space for speaker verification.\" [2] B. Schuller, S. Steidl, and A. Batliner, \"The INTERSPEECH 2009 emotion challenge,\" in INTERSPEECH, 2009. [3] H.-Y. Lee and L.-S. Lee, Enhanced spoken term detection using support vector machines and weighted pseudo examples, \"Audio, Speech, and Language Processing, IEEE Transactions on, vol."}], "references": [{"title": "Support vector machines versus fast scoring in the lowdimensional total variability space for speaker verification", "author": ["N. Dehak", "R. Dehak", "P. Kenny", "N. Brummer", "P. Ouellet", "P. Dumouchel"], "venue": "IN- TERSPEECH, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "The INTERSPEECH 2009 emotion challenge", "author": ["B. Schuller", "S. Steidl", "A. Batliner"], "venue": "INTERSPEECH, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Enhanced spoken term detection using support vector machines and weighted pseudo examples", "author": ["H.-Y. Lee", "L.-S. Lee"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 6, pp. 1272\u20131284, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "A hybrid HMM/DNN approach to keyword spotting of short words", "author": ["I.-F. Chen", "C.-H. Lee"], "venue": "INTERSPEECH, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting discriminative point process models for spoken term detection", "author": ["A. Norouzian", "A. Jansen", "R. Rose", "S. Thomas"], "venue": "INTERSPEECH, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B.V. Durme"], "venue": "ICASSP, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Word embeddings for speech recognition", "author": ["S. Bengio", "G. Heigold"], "venue": "INTERSPEECH, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Query-by-example keyword spotting using long short-term memory networks", "author": ["G. Chen", "C. Parada", "T.N. Sainath"], "venue": "ICASSP, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Autoencoders, unsupervised learning, and deep architectures", "author": ["P. Baldi"], "venue": "Unsupervised and Transfer Learning Challenges in Machine Learning, Volume 7, p. 43, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Semi-supervised sequence learning", "author": ["A.M. Dai", "Q.V. Le"], "venue": "arXiv preprint arXiv: 1511.01432, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["J. Li", "M.-T. Luong", "D. Jurafsky"], "venue": "arXiv preprint arXiv: 1506.01057, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "arXiv preprint arXiv: 1506.06726, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1502.04681, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "ICML, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "JMLR, vol. 11, pp. 3371\u20133408, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Training recurrent networks by evolino", "author": ["J. Schmidhuber", "D. Wierstra", "M. Gagliolo", "F. Gomez"], "venue": "Neural Computation, vol. 19, no. 3, pp. 757\u2013779, 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Evolving memory cell structures for sequence learning", "author": ["J. Bayer", "D. Wierstra", "J. Togelius", "J. Schmidhuber"], "venue": "ICANN, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "INTERSPEECH, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and robust training of recurrent neural networks for offline handwriting recognition", "author": ["P. Doetsch", "M. Kozielski", "H. Ney"], "venue": "ICFHR, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Lstm: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u0131\u0301k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Librispeech: an ASR corpus based on public domain audio books", "author": ["V. Panayotov", "G. Chen", "D. Povey", "S. Khudanpur"], "venue": "ICASSP, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "SciPy, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "CoRR, vol. abs/1211.5590, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "CoRR, vol. abs/1308.0850, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent nets that time and count", "author": ["F. Gers", "J. Schmidhuber"], "venue": "IJCNN, 2000.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "Sch?tze, Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Unsupervised spoken term detection with spoken queries by multi-level acoustic patterns with varying model granularity", "author": ["C.-T. Chung", "C.-A. Chan", "L.-s. Lee"], "venue": "ICASSP, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic programming algorithm optimization for spoken word recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 26, no. 1, pp. 43\u2013 49, 1978.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1978}], "referenceMentions": [{"referenceID": 0, "context": "In speaker identification [1], audio emotion classification [2], and spoken term detection (STD) [3, 4, 5], the audio segments are usually represented as feature vectors to apply typical classifiers to determine their speaker labels, emotion labels or whether containing input queries.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "In speaker identification [1], audio emotion classification [2], and spoken term detection (STD) [3, 4, 5], the audio segments are usually represented as feature vectors to apply typical classifiers to determine their speaker labels, emotion labels or whether containing input queries.", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "In speaker identification [1], audio emotion classification [2], and spoken term detection (STD) [3, 4, 5], the audio segments are usually represented as feature vectors to apply typical classifiers to determine their speaker labels, emotion labels or whether containing input queries.", "startOffset": 97, "endOffset": 106}, {"referenceID": 3, "context": "In speaker identification [1], audio emotion classification [2], and spoken term detection (STD) [3, 4, 5], the audio segments are usually represented as feature vectors to apply typical classifiers to determine their speaker labels, emotion labels or whether containing input queries.", "startOffset": 97, "endOffset": 106}, {"referenceID": 4, "context": "In speaker identification [1], audio emotion classification [2], and spoken term detection (STD) [3, 4, 5], the audio segments are usually represented as feature vectors to apply typical classifiers to determine their speaker labels, emotion labels or whether containing input queries.", "startOffset": 97, "endOffset": 106}, {"referenceID": 5, "context": "In query-by-example STD, by representing each word segment as a feature vector, indexing the segments can be easier, which makes retrieval much more efficient [6].", "startOffset": 159, "endOffset": 162}, {"referenceID": 0, "context": "It is common to use i-vectors to represent utterances in speaker identification [1], and there are several approaches successfully used in STD [6, 3, 4, 5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "It is common to use i-vectors to represent utterances in speaker identification [1], and there are several approaches successfully used in STD [6, 3, 4, 5].", "startOffset": 143, "endOffset": 155}, {"referenceID": 2, "context": "It is common to use i-vectors to represent utterances in speaker identification [1], and there are several approaches successfully used in STD [6, 3, 4, 5].", "startOffset": 143, "endOffset": 155}, {"referenceID": 3, "context": "It is common to use i-vectors to represent utterances in speaker identification [1], and there are several approaches successfully used in STD [6, 3, 4, 5].", "startOffset": 143, "endOffset": 155}, {"referenceID": 4, "context": "It is common to use i-vectors to represent utterances in speaker identification [1], and there are several approaches successfully used in STD [6, 3, 4, 5].", "startOffset": 143, "endOffset": 155}, {"referenceID": 6, "context": "Deep learning is also used for extracting representations [7, 8].", "startOffset": 58, "endOffset": 64}, {"referenceID": 7, "context": "Deep learning is also used for extracting representations [7, 8].", "startOffset": 58, "endOffset": 64}, {"referenceID": 7, "context": "By learning Recurrent Neural Network (RNN) with an audio segment as input and the word corresponding to the segment as target, the outputs of the hidden layer at the last few time steps can be used as the representation of the input segment [8].", "startOffset": 241, "endOffset": 244}, {"referenceID": 8, "context": "On the other hand, Autoencoder has been one of the most prominent machine learning techniques for extracting representations in an unsupervised way [9, 10], but its input should be vectors of fixed dimensionality.", "startOffset": 148, "endOffset": 155}, {"referenceID": 9, "context": "On the other hand, Autoencoder has been one of the most prominent machine learning techniques for extracting representations in an unsupervised way [9, 10], but its input should be vectors of fixed dimensionality.", "startOffset": 148, "endOffset": 155}, {"referenceID": 10, "context": "Therefore, a general framework to encode a sequence using sequence to sequence Autoencoder is proposed [11, 12, 13, 14], in which a RNN is used to encode a sequence into a fixed-length representation, and then another RNN is used to decode a sequence out of that representation.", "startOffset": 103, "endOffset": 119}, {"referenceID": 11, "context": "Therefore, a general framework to encode a sequence using sequence to sequence Autoencoder is proposed [11, 12, 13, 14], in which a RNN is used to encode a sequence into a fixed-length representation, and then another RNN is used to decode a sequence out of that representation.", "startOffset": 103, "endOffset": 119}, {"referenceID": 12, "context": "Therefore, a general framework to encode a sequence using sequence to sequence Autoencoder is proposed [11, 12, 13, 14], in which a RNN is used to encode a sequence into a fixed-length representation, and then another RNN is used to decode a sequence out of that representation.", "startOffset": 103, "endOffset": 119}, {"referenceID": 13, "context": "Therefore, a general framework to encode a sequence using sequence to sequence Autoencoder is proposed [11, 12, 13, 14], in which a RNN is used to encode a sequence into a fixed-length representation, and then another RNN is used to decode a sequence out of that representation.", "startOffset": 103, "endOffset": 119}, {"referenceID": 11, "context": "This general framework has been applied on natural language processing [12, 13] and video processing [14], but not yet on speech processing.", "startOffset": 71, "endOffset": 79}, {"referenceID": 12, "context": "This general framework has been applied on natural language processing [12, 13] and video processing [14], but not yet on speech processing.", "startOffset": 71, "endOffset": 79}, {"referenceID": 13, "context": "This general framework has been applied on natural language processing [12, 13] and video processing [14], but not yet on speech processing.", "startOffset": 101, "endOffset": 105}, {"referenceID": 6, "context": "Different from the previous work [7, 8], learning SA does not need any supervision.", "startOffset": 33, "endOffset": 39}, {"referenceID": 7, "context": "Different from the previous work [7, 8], learning SA does not need any supervision.", "startOffset": 33, "endOffset": 39}, {"referenceID": 14, "context": "Inspired from denoising Autoencoder [15, 16], we further propose Denoising Sequenceto-sequence Autoencoder (DSA) that improves the learned representations.", "startOffset": 36, "endOffset": 44}, {"referenceID": 15, "context": "Inspired from denoising Autoencoder [15, 16], we further propose Denoising Sequenceto-sequence Autoencoder (DSA) that improves the learned representations.", "startOffset": 36, "endOffset": 44}, {"referenceID": 16, "context": "Below we first give a brief recap on the RNN Encoder-Decoder framework [17, 18], which is widely used to realize the sequence-to-sequence learning in Section 2.", "startOffset": 71, "endOffset": 79}, {"referenceID": 17, "context": "Below we first give a brief recap on the RNN Encoder-Decoder framework [17, 18], which is widely used to realize the sequence-to-sequence learning in Section 2.", "startOffset": 71, "endOffset": 79}, {"referenceID": 18, "context": "In practice, RNN does not seem to learn long-term dependencies [19], so LSTM [20] is designed to conquer such difficulty.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "In practice, RNN does not seem to learn long-term dependencies [19], so LSTM [20] is designed to conquer such difficulty.", "startOffset": 77, "endOffset": 81}, {"referenceID": 20, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 21, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 22, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 23, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 17, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 24, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 25, "context": "Because many amazing results were achieved by LSTM, RNN is usually equipped with LSTM units [21, 22, 23, 24, 18, 25, 26].", "startOffset": 92, "endOffset": 120}, {"referenceID": 17, "context": "RNN Encoder-Decoder [18, 17] consists of an Encoder RNN and a Decoder RNN.", "startOffset": 20, "endOffset": 28}, {"referenceID": 16, "context": "RNN Encoder-Decoder [18, 17] consists of an Encoder RNN and a Decoder RNN.", "startOffset": 20, "endOffset": 28}, {"referenceID": 8, "context": "Following the philosophy of Autoencoder [9, 10], the target of the output sequence y = (y1, y2, .", "startOffset": 40, "endOffset": 47}, {"referenceID": 9, "context": "Following the philosophy of Autoencoder [9, 10], the target of the output sequence y = (y1, y2, .", "startOffset": 40, "endOffset": 47}, {"referenceID": 5, "context": "This framework is inspired from the previous work [6], but completely different approach is used to represent the audio segments here.", "startOffset": 50, "endOffset": 53}, {"referenceID": 26, "context": "We used LibriSpeech corpus [27], which is derived from read audiobooks from the LibriVox project, as data for experiments.", "startOffset": 27, "endOffset": 31}, {"referenceID": 27, "context": "SA and DSA were implemented with Theano [28, 29].", "startOffset": 40, "endOffset": 48}, {"referenceID": 28, "context": "SA and DSA were implemented with Theano [28, 29].", "startOffset": 40, "endOffset": 48}, {"referenceID": 29, "context": "We adopted the LSTM version described in the previous work [30], and the peephole connections [31] were added.", "startOffset": 59, "endOffset": 63}, {"referenceID": 30, "context": "We adopted the LSTM version described in the previous work [30], and the peephole connections [31] were added.", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "\u2022 For DSA, zero-masking [16] was used to generate the noisy input x\u0303, which randomly wiped out some elements in each input acoustic feature and set them to zero.", "startOffset": 24, "endOffset": 28}, {"referenceID": 31, "context": "Mean Average Precision (MAP) [32] was used as the evaluation measure for query-by-example STD as the previous work [33].", "startOffset": 29, "endOffset": 33}, {"referenceID": 32, "context": "Mean Average Precision (MAP) [32] was used as the evaluation measure for query-by-example STD as the previous work [33].", "startOffset": 115, "endOffset": 119}, {"referenceID": 33, "context": "The first baseline used DTW [34] which is a common algorithm used in current query-by-example application.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "Although NE is simple, similar approaches have been used in STD and achieved some successful results [3, 4, 5].", "startOffset": 101, "endOffset": 110}, {"referenceID": 3, "context": "Although NE is simple, similar approaches have been used in STD and achieved some successful results [3, 4, 5].", "startOffset": 101, "endOffset": 110}, {"referenceID": 4, "context": "Although NE is simple, similar approaches have been used in STD and achieved some successful results [3, 4, 5].", "startOffset": 101, "endOffset": 110}], "year": 2017, "abstractText": "Representing audio segments expressed with variablelength acoustic feature sequences as fixed-length feature vectors is usually needed in many speech applications, including speaker identification, audio emotion classification and spoken term detection (STD). In this paper, we apply and extend sequence-to-sequence learning framework to learn representations for audio segments without any supervision. The model we used is called Sequence-to-sequence Autoencoder (SA), which consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN acts as an encoder that maps the input sequence into a vector representation of fixed dimensionality, and the second RNN acts as a decoder that maps the representation back to the input sequence. The two RNNs are then jointly trained by minimizing the reconstruction error. We further propose Denoising Sequence-to-sequence Autoencoder (DSA) that improves the learned representations. The vector representations learned by SA and DSA are shown to be very helpful for query-by-example STD. The experimental results have shown that the proposed models achieved better retrieval performance than using audio segment representation designed heuristically and the classical Dynamic Time Warping (DTW) approach.", "creator": "LaTeX with hyperref package"}}}