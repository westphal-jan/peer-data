{"id": "1603.00988", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Learning Functions: When Is Deep Better Than Shallow", "abstract": "We describe computational tasks - especially in vision - that correspond to compositional/hierarchical functions. While the universal approximation property holds both for hierarchical and shallow networks, we prove that deep (hierarchical) networks can approximate the class of compositional functions with the same accuracy as shallow networks but with exponentially lower VC-dimension as well as the number of training parameters. This leads to the question of approximation by sparse polynomials (in the number of independent parameters) and, as a consequence, by deep networks. We also discuss connections between our results and learnability of sparse Boolean functions, settling an old conjecture by Bengio.", "histories": [["v1", "Thu, 3 Mar 2016 06:26:31 GMT  (445kb,D)", "http://arxiv.org/abs/1603.00988v1", null], ["v2", "Sat, 5 Mar 2016 22:41:36 GMT  (445kb,D)", "http://arxiv.org/abs/1603.00988v2", "Corrected typos"], ["v3", "Thu, 24 Mar 2016 04:22:29 GMT  (408kb,D)", "http://arxiv.org/abs/1603.00988v3", null], ["v4", "Sun, 29 May 2016 16:43:23 GMT  (686kb,D)", "http://arxiv.org/abs/1603.00988v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hrushikesh mhaskar", "qianli liao", "tomaso poggio"], "accepted": false, "id": "1603.00988"}, "pdf": {"name": "1603.00988.pdf", "metadata": {"source": "CRF", "title": "Learning Real and Boolean Functions: When Is Deep Better Than Shallow", "authors": ["Hrushikesh Mhaskar", "Qianli Liao", "Tomaso Poggio"], "emails": [], "sections": [{"heading": null, "text": "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by the NSF STC Award CCF 1231216. HNM was partially supported by ARO Grant W911NF-15-1-0385.ar Xiv: 1"}, {"heading": "1 INTRODUCTION", "text": "The main aim of this essay is to answer the question: why should deep networks be better than flat networks? Our assertion here is that hierarchical networks represent a more efficient approximation of the calculations that must be carried out on images - and possibly other sensory signals. The argument of the essay compares flat (a hidden layer) networks with deep networks (see Figure 1). Both types of networks use the same small set of operations - dot products, linear combinations, a fixed non-linear function of a variable, possibly folding and pooling. The logic of the paper is as follows: \u2022 both flat (a) and deep (b) networks are universal, i.e. they can arbitrarily approximate any continuous function of d variables in a compact domain, possibly folding and pooling. \u2022 We show that the approximation of functions with a compositional structure - such as f (x1), \u00b7 xd) = h1 (h2) \u00b7 \u00b7 \u00b7 \u00b7 1 (archaic) (x1) (x1) - (hierarchical)."}, {"heading": "2 PREVIOUS WORK", "text": "The success of deep learning in the current landscape of machine learning once again raises an old theoretical question: why are multi-layer networks better than single-layer networks? Under what conditions? The question is relevant in several related areas, from machine learning to functional approximations, and has been asked many times before. A personal (TP) version of this question begins with an old essay on nonlinear associative memories that describes the conditions under which higher and higher operators should be learned from data to improve the performance of linear regression. The idea that composition is important for learning and requires multiple layers in a network was the subject of a chapter in an early paper on (mostly RBF) regulation networks (Poggio and Girosi refer to the performance of linear regression)."}, {"heading": "3 MAIN RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 COMPOSITIONAL FUNCTIONS", "text": "It is natural to make assumptions that hierarchical compositions of functions such as ASF (x1, \u00b7 \u00b7 \u00b7, x8) = h3 (h21 (x1, x2), h12 (x3, x4))), h22 (h13 (x5, x6), h14 (x7, x8))) (1) are more efficiently approximated by deep networks than by flat networks. We assume that flat networks do not have structural information about the function to be learned (here their compositional structure) because they cannot directly represent it. Deep networks with standard architectures, on the other hand, represent compositionality and can recall the details of such prior information.In addition, both flat and deep representations may reflect inventory of the group transformations of the inputs of the function or not."}, {"heading": "3.2 VC BOUNDS", "text": "To calculate the relevant VC boundaries, we use a well-known result (Anthony and Bartlett, 2002): Theorem 1. Each binary function class in a Euclidean space parameterized by most n parameters and such that any activation function can be specified with p floating-point operations at most, has at most O (np). This result immediately provides VC boundaries for flat and deep networks with d input variables. Examples of d = 8 are shown in Figure 1. We remember that for ramp functions p = 1. We assume that the node in the flat network consists of N units calculating N dimensions (< wi, x > + bi), with wi, x-Rd and ci, bi-R. Likewise, each of the d \u2212 1 nodes of the deep network is made up of n units calculating n dimensions (< vi, x > + ti |, with vi, x-R2, i dimensions)."}, {"heading": "3.3 DEGREE OF APPROXIMATION", "text": "We now describe the complexity of shallow and deep nets, measured by the number of workable parameters that are required to achieve a uniform approximation to a target function. (...) We now describe the complexity of shallow and deep nets, measured by the number of workable parameters that are required for a uniform approximation to a target function. (...) We now describe the complexity of shallow and deep nets, measured by the number of workable parameters in Euclidean space. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (.... (...). (...). (...). (...). (.... (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...).). (...). (...). (...).).................................)..............................................................."}, {"heading": "4 WHY DOES VISION REQUIRE COMPOSITIONAL FUNCTIONS?", "text": "The final step in the argument is that deep hierarchical functions are critical calculations for visions; the general point is that they reflect symmetries in the physical world that manifest themselves in practice."}, {"heading": "4.1 SANITY CHECK: RECOGNITION IN CLUTTER", "text": "The advantage derives from the localization property of objects and concerns an important calculation: detection in disorder. Figure 2 summarizes the skeleton of the argument: the detection of object A suffers from disorder (B) in a flat network. The point can be formalized by starting from an (obvious) result (Anselmi et al., 2015), which shows that pooling via aconvolution is a group cross-section of point products of the image with transformations of templates by group elements. This group cross-section is invariant and can be unique. Adapting the evidence there (Anselmi et al., 2015), it is easy to show (as we do in a longer version of this paper) that inventory and potential uniqueness of pooling - defined as image hierarchies that can change from one presentation of the object to the next within the two layers - are more resistant to the respective deeper objects of the superimposed layers."}, {"heading": "5 SPARSE FUNCTIONS", "text": "In fact, the fact is that most of us are able to go in search of a solution that is capable, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, that they are able to find a solution, to find a solution."}, {"heading": "6 BOOLEAN FUNCTIONS", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "7 DISCUSSION", "text": "In this paper, we focus on the approximation properties of a polynomial in the input vector x, which is in the range of network parameters; the second question, which we do not address here, concerns learning the unknown coefficients from the data: Are there several solutions? Why is SGD so inappropriately efficient, at least in appearance? Are good minima with deep than flat networks easier to find? This last point is very important in practice. Note that empirical tests such as Figure 3 typically mix answers to the two questions. In this paper, we have introduced compositional functions and show that they are sparse in the sense that they can be enriched by economical polynomials."}, {"heading": "Acknowledgment", "text": "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by the NSF STC Award CCF 1231216. HNM was partially supported by ARO Grant W911NF-15-1-0385."}], "references": [{"title": "Unsupervised learning of invariant representations", "author": ["F. Anselmi", "J.Z. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio"], "venue": "Theoretical Computer Science.", "citeRegEx": "Anselmi et al\\.,? 2015", "shortCiteRegEx": "Anselmi et al\\.", "year": 2015}, {"title": "Neural Network Learning - Theoretical Foundations", "author": ["M. Anthony", "P. Bartlett"], "venue": "Cambridge University Press.", "citeRegEx": "Anthony and Bartlett,? 2002", "shortCiteRegEx": "Anthony and Bartlett", "year": 2002}, {"title": "Representations properties of multilayer feedforward networks", "author": ["B.B. Moore", "T. Poggio"], "venue": "Abstracts of the First annual INNS meeting, 320:502.", "citeRegEx": "Moore and Poggio,? 1998", "shortCiteRegEx": "Moore and Poggio", "year": 1998}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["A.R. Barron"], "venue": "Information Theory, IEEE Transactions on, 39(3):930\u2013945.", "citeRegEx": "Barron,? 1993", "shortCiteRegEx": "Barron", "year": 1993}, {"title": "Scaling learning algorithms towards ai", "author": ["Y. Bengio", "Y. LeCun"], "venue": "Bottou, L., Chapelle, O., and DeCoste, D.and Weston, J., editors, Large-Scale Kernel Machines. MIT Press.", "citeRegEx": "Bengio and LeCun,? 2007", "shortCiteRegEx": "Bengio and LeCun", "year": 2007}, {"title": "Neural networks for localized approximation", "author": ["C.K. Chui", "X. Li", "H.N. Mhaskar"], "venue": "Mathematics of Computation, 63(208):607\u2013623.", "citeRegEx": "Chui et al\\.,? 1994", "shortCiteRegEx": "Chui et al\\.", "year": 1994}, {"title": "Limitations of the approximation capabilities of neural networks with one hidden layer", "author": ["C.K. Chui", "X. Li", "H.N. Mhaskar"], "venue": "Advances in Computational Mathematics, 5(1):233\u2013243.", "citeRegEx": "Chui et al\\.,? 1996", "shortCiteRegEx": "Chui et al\\.", "year": 1996}, {"title": "Shallow vs", "author": ["O. Delalleau", "Y. Bengio"], "venue": "deep sum-product networks. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011,", "citeRegEx": "Delalleau and Bengio,? 2011", "shortCiteRegEx": "Delalleau and Bengio", "year": 2011}, {"title": "Optimal nonlinear approximation", "author": ["R.A. DeVore", "R. Howard", "C.A. Micchelli"], "venue": "Manuscripta mathematica, 63(4):469\u2013478.", "citeRegEx": "DeVore et al\\.,? 1989", "shortCiteRegEx": "DeVore et al\\.", "year": 1989}, {"title": "Neocognitron: A self-organizing neural network for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics, 36(4):193\u2013202.", "citeRegEx": "Fukushima,? 1980", "shortCiteRegEx": "Fukushima", "year": 1980}, {"title": "Regularization theory and neural networks architectures", "author": ["F. Girosi", "M. Jones", "T. Poggio"], "venue": "Neural Computation, 7:219\u2013269.", "citeRegEx": "Girosi et al\\.,? 1995", "shortCiteRegEx": "Girosi et al\\.", "year": 1995}, {"title": "Computational Limitations for Small Depth Circuits", "author": ["J.T. Hastad"], "venue": "MIT Press.", "citeRegEx": "Hastad,? 1987", "shortCiteRegEx": "Hastad", "year": 1987}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Bounds on rates of variable basis and neural network approximation", "author": ["V. Kurkov\u00e1", "M. Sanguineti"], "venue": "IEEE Transactions on Information Theory, 47(6):2659\u20132665.", "citeRegEx": "Kurkov\u00e1 and Sanguineti,? 2001", "shortCiteRegEx": "Kurkov\u00e1 and Sanguineti", "year": 2001}, {"title": "Comparison of worst case errors in linear and neural network approximation", "author": ["V. Kurkov\u00e1", "M. Sanguineti"], "venue": "IEEE Transactions on Information Theory, 48(1):264\u2013275.", "citeRegEx": "Kurkov\u00e1 and Sanguineti,? 2002", "shortCiteRegEx": "Kurkov\u00e1 and Sanguineti", "year": 2002}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G.", "H."], "venue": "Nature, pages 436\u2013444.", "citeRegEx": "LeCun et al\\.,? 2015", "shortCiteRegEx": "LeCun et al\\.", "year": 2015}, {"title": "Constant depth circuits, fourier transform, and learnability", "author": ["Linial N.", "Y.M.", "N.", "N."], "venue": "Journal of the ACM, 40(3):607620.", "citeRegEx": "N. et al\\.,? 1993", "shortCiteRegEx": "N. et al\\.", "year": 1993}, {"title": "A provably efficient algorithm for training deep networks", "author": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": "CoRR, abs/1304.7045.", "citeRegEx": "Livni et al\\.,? 2013", "shortCiteRegEx": "Livni et al\\.", "year": 2013}, {"title": "Learning boolean functions via the fourier transform", "author": ["Y. Mansour"], "venue": "Roychowdhury, V., Siu, K., and Orlitsky, A., editors, Theoretical Advances in Neural Computation and Learning, pages 391\u2013424. Springer US.", "citeRegEx": "Mansour,? 1994", "shortCiteRegEx": "Mansour", "year": 1994}, {"title": "Approximation properties of a multilayered feedforward artificial neural network", "author": ["H.N. Mhaskar"], "venue": "Advances in Computational Mathematics, 1(1):61\u2013", "citeRegEx": "Mhaskar,? 1993a", "shortCiteRegEx": "Mhaskar", "year": 1993}, {"title": "Neural networks for localized approximation of real functions", "author": ["H.N. Mhaskar"], "venue": "Neural Networks for Processing [1993] III. Proceedings of the 1993 IEEESP Workshop, pages 190\u2013196. IEEE.", "citeRegEx": "Mhaskar,? 1993b", "shortCiteRegEx": "Mhaskar", "year": 1993}, {"title": "Neural networks for optimal approximation of smotth and analytic functions", "author": ["H.N. Mhaskar"], "venue": "Neural Computation, 8:164\u2013177.", "citeRegEx": "Mhaskar,? 1996", "shortCiteRegEx": "Mhaskar", "year": 1996}, {"title": "On the tractability of multivariate integration and approximation by neural networks", "author": ["H.N. Mhaskar"], "venue": "Journal of Complexity, 20(4):561\u2013590.", "citeRegEx": "Mhaskar,? 2004", "shortCiteRegEx": "Mhaskar", "year": 2004}, {"title": "Degree of approximation by neural and translation networks with a single hidden layer", "author": ["H.N. Mhaskar", "C.A. Micchelli"], "venue": "Advances in Applied Mathematics, 16(2):151\u2013183.", "citeRegEx": "Mhaskar and Micchelli,? 1995", "shortCiteRegEx": "Mhaskar and Micchelli", "year": 1995}, {"title": "On the number of linear regions of deep neural networks", "author": ["G.F. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, 27:2924\u20132932.", "citeRegEx": "Montufar et al\\.,? 2014", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Approximation theory of the mlp model in neural networks", "author": ["A. Pinkus"], "venue": "Acta Numerica, pages 143\u2013195.", "citeRegEx": "Pinkus,? 1999", "shortCiteRegEx": "Pinkus", "year": 1999}, {"title": "I-theory on depth vs width: hierarchical function composition", "author": ["T. Poggio", "F. Anselmi", "L. Rosasco"], "venue": "CBMM memo 041.", "citeRegEx": "Poggio et al\\.,? 2015a", "shortCiteRegEx": "Poggio et al\\.", "year": 2015}, {"title": "A theory of networks for approximation and learning", "author": ["T. Poggio", "F. Girosi"], "venue": "Laboratory, Massachusetts Institute of Technology, A.I. memo n1140.", "citeRegEx": "Poggio and Girosi,? 1989", "shortCiteRegEx": "Poggio and Girosi", "year": 1989}, {"title": "Notes on hierarchical splines, dclns and i-theory", "author": ["T. Poggio", "L. Rosaco", "A. Shashua", "N. Cohen", "F. Anselmi"], "venue": "CBMM memo 037.", "citeRegEx": "Poggio et al\\.,? 2015b", "shortCiteRegEx": "Poggio et al\\.", "year": 2015}, {"title": "The mathematics of learning: Dealing with data", "author": ["T. Poggio", "S. Smale"], "venue": "Notices of the American Mathematical Society (AMS), 50(5):537\u2013544.", "citeRegEx": "Poggio and Smale,? 2003", "shortCiteRegEx": "Poggio and Smale", "year": 2003}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M. Riesenhuber", "T. Poggio"], "venue": "Nat. Neurosci., 2(11):1019\u20131025.", "citeRegEx": "Riesenhuber and Poggio,? 1999a", "shortCiteRegEx": "Riesenhuber and Poggio", "year": 1999}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M. Riesenhuber", "T. Poggio"], "venue": "Nature Neuroscience, 2(11):1019\u20131025.", "citeRegEx": "Riesenhuber and Poggio,? 1999b", "shortCiteRegEx": "Riesenhuber and Poggio", "year": 1999}, {"title": "Origins of scaling in natural images", "author": ["D. Ruderman"], "venue": "Vision Res., pages 3385 \u2013 3398.", "citeRegEx": "Ruderman,? 1997", "shortCiteRegEx": "Ruderman", "year": 1997}, {"title": "Steps Towards a Theory of Visual Information: Active Perception, Signal-to-Symbol Conversion and the Interplay Between Sensing and Control", "author": ["S. Soatto"], "venue": "arXiv:1110.2053, pages 0\u2013151.", "citeRegEx": "Soatto,? 2011", "shortCiteRegEx": "Soatto", "year": 2011}, {"title": "Representation benefits of deep feedforward networks", "author": ["M. Telgarsky"], "venue": "arXiv preprint arXiv:1509.08101v2 [cs.LG] 29 Sep 2015.", "citeRegEx": "Telgarsky,? 2015", "shortCiteRegEx": "Telgarsky", "year": 2015}, {"title": "Matconvnet: Convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference, pages 689\u2013692. ACM.", "citeRegEx": "Vedaldi and Lenc,? 2015", "shortCiteRegEx": "Vedaldi and Lenc", "year": 2015}], "referenceMentions": [{"referenceID": 27, "context": "The idea that compositionality is important in networks for learning and requires several layers in a network was the subject of a chapter in an early paper on (mostly RBF) networks for regularization(Poggio and Girosi, 1989).", "startOffset": 200, "endOffset": 225}, {"referenceID": 15, "context": "Most Deep Learning references these days start with Hinton\u2019s backpropagation and with Lecun\u2019s convolutional networks (see for a nice review (LeCun et al., 2015)).", "startOffset": 140, "endOffset": 160}, {"referenceID": 9, "context": "Fukushima\u2019s Neocognitron(Fukushima, 1980) was a convolutional neural network that was trained to recognize characters.", "startOffset": 24, "endOffset": 41}, {"referenceID": 30, "context": "The HMAX model of visual cortex(Riesenhuber and Poggio, 1999a) was described as a series of AND and OR layers to represent hierarchies of disjunctions of conjunctions.", "startOffset": 31, "endOffset": 62}, {"referenceID": 29, "context": "A version of the questions about why hierarchies was asked in (Poggio and Smale, 2003) as follow: A comparison with", "startOffset": 62, "endOffset": 86}, {"referenceID": 24, "context": "More specific, intriguing work (Montufar et al., 2014) provided an estimation of the number of linear regions that a network with ReLU nonlinearities can in principle synthesize but leaves open the question of whether they can be used for learning.", "startOffset": 31, "endOffset": 54}, {"referenceID": 17, "context": "Sum-Product networks, which are equivalent to polynomial networks (see (B. Moore and Poggio, 1998; Livni et al., 2013), are a simple case of a hierarchy that can be analyzed (Delalleau and Bengio, 2011).", "startOffset": 71, "endOffset": 118}, {"referenceID": 7, "context": ", 2013), are a simple case of a hierarchy that can be analyzed (Delalleau and Bengio, 2011).", "startOffset": 63, "endOffset": 91}, {"referenceID": 17, "context": "Work on hierarchical quadratic networks (Livni et al., 2013), together with function approximation results (Pinkus, 1999; Mhaskar, 1993b), is most relevant to the approach here.", "startOffset": 40, "endOffset": 60}, {"referenceID": 25, "context": ", 2013), together with function approximation results (Pinkus, 1999; Mhaskar, 1993b), is most relevant to the approach here.", "startOffset": 54, "endOffset": 84}, {"referenceID": 20, "context": ", 2013), together with function approximation results (Pinkus, 1999; Mhaskar, 1993b), is most relevant to the approach here.", "startOffset": 54, "endOffset": 84}, {"referenceID": 28, "context": "This paper is a short, updated version of material that appeared in (Poggio et al., 2015b) and especially in (Poggio et al.", "startOffset": 68, "endOffset": 90}, {"referenceID": 26, "context": ", 2015b) and especially in (Poggio et al., 2015a).", "startOffset": 27, "endOffset": 49}, {"referenceID": 33, "context": "In addition, both shallow and deep representations may or may not reflect invariance to group transformations of the inputs of the function (Soatto, 2011; Anselmi et al., 2015).", "startOffset": 140, "endOffset": 176}, {"referenceID": 0, "context": "In addition, both shallow and deep representations may or may not reflect invariance to group transformations of the inputs of the function (Soatto, 2011; Anselmi et al., 2015).", "startOffset": 140, "endOffset": 176}, {"referenceID": 10, "context": "The architecture of the deep networks reflects Equation 1 with each node hi being a ridge function (in particular it may be an additive piecewise linear spline in the generalized sense of (Girosi et al., 1995), see Figure 1).", "startOffset": 188, "endOffset": 209}, {"referenceID": 17, "context": "A direct connection between regression and binary classification is provided by the following observation due to (Livni et al., 2013).", "startOffset": 113, "endOffset": 133}, {"referenceID": 1, "context": "1 from (Anthony and Bartlett, 2002), it is possible to show that the fat-shattering dimension is upper-bounded by the VC-dimension of a slightly larger class of networks, which have an additional real input and an additional output node computing a linear threshold function in R.", "startOffset": 7, "endOffset": 35}, {"referenceID": 1, "context": "To compute the relevant VC bounds we use a well-known result (Anthony and Bartlett, 2002):", "startOffset": 61, "endOffset": 89}, {"referenceID": 31, "context": "An example is the old HMAX model (Riesenhuber and Poggio, 1999b) which took into account information about physiology of the ventral stream: it has an architecture of the binary tree type.", "startOffset": 33, "endOffset": 64}, {"referenceID": 25, "context": "Each of the nodes in b) consists of n ReLU units and computes the ridge function (Pinkus, 1999) \u2211n i=1 ai| \u3008vi,x\u3009 + ti|+, with vi,x \u2208 R , ai, ti \u2208 R.", "startOffset": 81, "endOffset": 95}, {"referenceID": 21, "context": "In fact, the following proposition is proved in (Mhaskar, 1996), (see also (Pinkus, 1999)):", "startOffset": 48, "endOffset": 63}, {"referenceID": 25, "context": "In fact, the following proposition is proved in (Mhaskar, 1996), (see also (Pinkus, 1999)):", "startOffset": 75, "endOffset": 89}, {"referenceID": 8, "context": "(DeVore et al., 1989))", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "It is shown in (DeVore et al., 1989) that curse(W, ) \u2265 c \u2212d.", "startOffset": 15, "endOffset": 36}, {"referenceID": 3, "context": ", (Barron, 1993; Kurkov\u00e1 and Sanguineti, 2001; Kurkov\u00e1 and Sanguineti, 2002; Mhaskar, 2004)).", "startOffset": 2, "endOffset": 91}, {"referenceID": 13, "context": ", (Barron, 1993; Kurkov\u00e1 and Sanguineti, 2001; Kurkov\u00e1 and Sanguineti, 2002; Mhaskar, 2004)).", "startOffset": 2, "endOffset": 91}, {"referenceID": 14, "context": ", (Barron, 1993; Kurkov\u00e1 and Sanguineti, 2001; Kurkov\u00e1 and Sanguineti, 2002; Mhaskar, 2004)).", "startOffset": 2, "endOffset": 91}, {"referenceID": 22, "context": ", (Barron, 1993; Kurkov\u00e1 and Sanguineti, 2001; Kurkov\u00e1 and Sanguineti, 2002; Mhaskar, 2004)).", "startOffset": 2, "endOffset": 91}, {"referenceID": 25, "context": "7 (Pinkus, 1999)).", "startOffset": 2, "endOffset": 16}, {"referenceID": 19, "context": "From the point of view of approximation theory as applied to arbitrary smooth functions, an interesting advantage of deep networks (with more than one hidden layer) is that they provide optimal local approximation analogous to spline approximation, while shallow networks are inherently unable to do so (Mhaskar, 1993a; Mhaskar, 1993b; Chui et al., 1994; Chui et al., 1996).", "startOffset": 303, "endOffset": 373}, {"referenceID": 20, "context": "From the point of view of approximation theory as applied to arbitrary smooth functions, an interesting advantage of deep networks (with more than one hidden layer) is that they provide optimal local approximation analogous to spline approximation, while shallow networks are inherently unable to do so (Mhaskar, 1993a; Mhaskar, 1993b; Chui et al., 1994; Chui et al., 1996).", "startOffset": 303, "endOffset": 373}, {"referenceID": 5, "context": "From the point of view of approximation theory as applied to arbitrary smooth functions, an interesting advantage of deep networks (with more than one hidden layer) is that they provide optimal local approximation analogous to spline approximation, while shallow networks are inherently unable to do so (Mhaskar, 1993a; Mhaskar, 1993b; Chui et al., 1994; Chui et al., 1996).", "startOffset": 303, "endOffset": 373}, {"referenceID": 6, "context": "From the point of view of approximation theory as applied to arbitrary smooth functions, an interesting advantage of deep networks (with more than one hidden layer) is that they provide optimal local approximation analogous to spline approximation, while shallow networks are inherently unable to do so (Mhaskar, 1993a; Mhaskar, 1993b; Chui et al., 1994; Chui et al., 1996).", "startOffset": 303, "endOffset": 373}, {"referenceID": 17, "context": "For the hierarchical quadratic networks described in (Livni et al., 2013) (see section 4) there a VC-dimension bound is much lower than for the corresponding shallow network.", "startOffset": 53, "endOffset": 73}, {"referenceID": 32, "context": "Ruderman\u2019s pioneering work (Ruderman, 1997) concludes that this set of properties is equivalent to the statement that natural images con-", "startOffset": 27, "endOffset": 43}, {"referenceID": 31, "context": "The first property \u2013 compositionality \u2013 was a main motivation for hierarchical architectures such as Fukushima\u2019s and later imitations of it such as HMAX which can be seen to be similar to a pyramid of AND and OR layers (Riesenhuber and Poggio, 1999b), that is a sequence of conjunctions and disjunctions.", "startOffset": 219, "endOffset": 250}, {"referenceID": 0, "context": "The point can be formalized by starting from an (obvious) result (Anselmi et al., 2015) showing that pooling over a", "startOffset": 65, "endOffset": 87}, {"referenceID": 0, "context": "By adapting the proof there (Anselmi et al., 2015), it is easy to show (as we are doing in a longer version of this paper), that invariance and potential uniquenss of pooling do not hold when clutter \u2013 defined as image patches that may change from one presentation of the object to the next presentation \u2013 is present within the pooling regions.", "startOffset": 28, "endOffset": 50}, {"referenceID": 23, "context": ", see (Mhaskar and Micchelli, 1995), and the layers after that can continue to be regarded as effectively approximating algebraic polynomials of high degree as above.", "startOffset": 6, "endOffset": 35}, {"referenceID": 29, "context": "It is equally intriguing to speculate whether in practice (Poggio and Smale, 2003) only sparse functions may be learnable.", "startOffset": 58, "endOffset": 82}, {"referenceID": 11, "context": "Classical results (Hastad, 1987) about the depth-breadth tradeoff in circuits design show that deep circuits are more efficient in representing certain Boolean functions than shallow circuits.", "startOffset": 18, "endOffset": 32}, {"referenceID": 4, "context": "For instance Bengio and LeCun (Bengio and LeCun, 2007) write \u201cWe claim that most", "startOffset": 30, "endOffset": 54}, {"referenceID": 34, "context": "We remark that a nice theorem was recently published (Telgarsky, 2015), showing that a certain family of classification problems with real-valued inputs cannot be approximated well by shallow networks with fewer than exponentially many nodes whereas a deep network achieves zero error.", "startOffset": 53, "endOffset": 70}, {"referenceID": 12, "context": "For the experiments with 2 and 3 hidden layers, batch normalization (Ioffe and Szegedy, 2015) was used between every two hidden layers.", "startOffset": 68, "endOffset": 93}, {"referenceID": 35, "context": "Implementations were based on MatConvNet (Vedaldi and Lenc, 2015).", "startOffset": 41, "endOffset": 65}, {"referenceID": 18, "context": "In general, two algorithms (Mansour, 1994) seems to allow learning of certain Boolean function classes:", "startOffset": 27, "endOffset": 42}], "year": 2017, "abstractText": "We describe computational tasks \u2013 especially in vision \u2013 that correspond to compositional/hierarchical functions. While the universal approximation property holds both for hierarchical and shallow networks, we prove that deep (hierarchical) networks can approximate the class of compositional functions with the same accuracy as shallow networks but with exponentially lower VC-dimension as well as the number of training parameters. This leads to the question of approximation by sparse polynomials (in the number of independent parameters) and, as a consequence, by deep networks. We also discuss connections between our results and learnability of sparse Boolean functions, settling an old conjecture by Bengio. This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216. HNM was supported in part by ARO Grant W911NF-15-1-0385. ar X iv :1 60 3. 00 98 8v 1 [ cs .L G ] 3 M ar 2 01 6", "creator": "LaTeX with hyperref package"}}}