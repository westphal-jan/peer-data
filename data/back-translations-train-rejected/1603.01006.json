{"id": "1603.01006", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Automatic learning of gait signatures for people identification", "abstract": "This work targets people identification in video based on the way they walk (i.e. gait). While classical methods typically derive gait signatures from sequences of binary silhouettes, in this work we explore the use of convolutional neural networks (CNN) for learning high-level descriptors from low-level motion features (i.e. optical flow components). We carry out a thorough experimental evaluation of the proposed CNN architecture on the challenging TUM-GAID dataset. The experimental results indicate that using spatio-temporal cuboids of optical flow as input data for CNN allows to obtain state-of-the-art results on the gait task with an image resolution eight times lower than the previously reported results (i.e. 80x60 pixels).", "histories": [["v1", "Thu, 3 Mar 2016 08:07:14 GMT  (7633kb,D)", "http://arxiv.org/abs/1603.01006v1", "Proof of concept paper. Technical report on the use of ConvNets (CNN) for gait recognition"], ["v2", "Tue, 14 Jun 2016 16:07:07 GMT  (7634kb,D)", "http://arxiv.org/abs/1603.01006v2", "Proof of concept paper. Technical report on the use of ConvNets (CNN) for gait recognition. Data and code:this http URL"]], "COMMENTS": "Proof of concept paper. Technical report on the use of ConvNets (CNN) for gait recognition", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["f m castro", "m j marin-jimenez", "n guil", "n perez de la blanca"], "accepted": false, "id": "1603.01006"}, "pdf": {"name": "1603.01006.pdf", "metadata": {"source": "CRF", "title": "Automatic learning of gait signatures for people identification", "authors": ["F.M. Castro", "M.J. Mar\u0131\u0301n-Jim\u00e9nez"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "This type of biometric approach is considered non-invasive because it is carried out at a distance and does not require the involvement of the subject to be identified, unlike other methods based on fingerprints. Gait recognition can be viewed in the context of video surveillance, from fine-grained features to early detection of persons of interest, such as a computer image that can be considered a specific case of human activity. However, gait recognition requires more subtleties than action detection, as the differences between the different gaits are generally more subtle than between the common categories of action (e.g. \"high jump\" and \"speed-thing\") included in the data collection."}, {"heading": "2. Related work", "text": "Two basic approaches stand out from the rest: silhouette-based and dense motion sequences. Silhouette-based descriptors are the most commonly used descriptions in the state-of-the-art frameworks. In this sense, however, the most popular silhouette-based gait descriptor is the so-called Gait Enery Image (GEI). The key idea is to calculate a temporal averaging of the binary silhouette of the target subject. To improve the performance of gait identification, Liu et al. [22] propose the compilation of HOG descriptors from GEI and the Chrono-Gait Image (CGI). Martin-Felez and Xiang [24] use GEI as the basic gait descriptor, projecting a new ranking model that makes it possible to use training data from different datasets."}, {"heading": "3. CNN overview", "text": "The Convolutionary Neural Network (CNN) model is therefore an important type of feeder neural network with particular success in applications where target information can be represented by a hierarchy of local features (see [2]).A CNN is defined as the composition of several revolutionary layers and several fully interconnected layers. Each revolutionary layer is generally the composition of a nonlinear layer and a pooling or sub-sampling layer to obtain a certain spatial inventory.In images, the nonlinear layer of CNN uses the 2D structure present in the data through local connections and weight partitioning. These two conditions impose a very strong regulation of the total number of weights in the model, which allows successful training of the model through the use of back propagation. In our approach, although we do not feed the model directly with the RGB image pixels, the CNN approach remains relevant, as the fluorescent information is the hierarchical dependency local characteristics."}, {"heading": "4. Proposed approach", "text": "In this section, we describe our proposed framework for solving the problem of gait recognition using CNN. The CNN-based pipeline is illustrated in Figure 2: (i) calculation of the optical flow (OF) along the entire sequence; (ii) construction of a data cube from successive OF cards; (iii) feeding CNN with OF cuboids to extract the gait signature; and (iv) use of a classifier to determine the identity of the subject."}, {"heading": "4.1. Input data", "text": "The use of optical media (OF, 2K, 2K) for action representation in video art has already proven itself in the past."}, {"heading": "4.2. CNN architecture for gait signature extraction", "text": "The CNN architecture we propose for gait recognition is based on the one described in [27] for general action detection in the video. However, in our case, the input has a size of 60 x 60 x 50, obtained from the sequence of 25 OF images with their corresponding two channels, as explained in the previous section. Proposed CNN is composed of the following sequence of layers (fig. 4): \"Convex 1,\" 96 filters of size 7 x 7 applied with step 1 followed by normalization and maximum merging of 2 x 2; \"Convex 2,\" 192 filters of size 5 x 5 applied with step 2 followed by max pooling 2 x 2; \"Convex 3 \u2212 512 filters of size 3 x 3 applied with step 1 followed by maximum summary of 2 x 2;\" Convex 4, \"4096 filters of size 2 x 2 applied with step 1;\" Full5, \"fully attached layer with 4096 units and dropouts.\""}, {"heading": "4.3. Classification strategies", "text": "Although the Softmax layer of CNN is already a classifier (i.e. each unit represents the likelihood of belonging to a class), the fully connected layers can play the role of linear-core binary C-SVM classifiers that can be used as input for a support vector machine (SVM) classifier. Since we are dealing with a multi-class problem, we define an interaction of binary C-SVM classifiers with a linear core in a \"one-on-all\" manner, with C being the number of possible subject identities. Previous work (e.g. [4]) indicates that this configuration of binary classifiers is suitable for achieving top-class results in this problem. Note that we L2 normalize the upper layer before we use it as a feature vector. A classic alternative to discriminatory classifiers is the nearest (NN) classifiers that are placed in our system to allow us to easily assign a certain step identification (1)."}, {"heading": "5. Experiments and results", "text": "Here we present the experiments to validate our approach and the results obtained on the selected gait detection data set."}, {"heading": "5.1. Dataset", "text": "We perform our experiments on the most recent \"TUM Gait from Audio, Image and Depth\" (TUM-GAID) dataset [14] for gait recognition. In TUM-GAID 305 subjects perform two tracks in an interior, the first track is from left to right and the second track from right to left. Therefore, both sides of the subjects are performed with a resolution of 640 x 480 pixels, with the subjects wearing heavy jackets and mostly winter boots, while the subjects wear different clothing. A Microsoft Kinect sensor captures the action, which provides a video stream with a resolution of 640 x 480 pixels."}, {"heading": "5.2. Performance evaluation", "text": "Therefore, we use the following metrics to quantify the performance of the proposed system: rank-1 and rank-5. Metric rank-1 measures the percentage of test samples where the top assigned identity matches the correct one. Rank-5 measures the percentage of test samples where the basic truth identity is included in the first five rankings of the corresponding test sample. Note that rank-5 is less strict than rank-1 and in a real system would allow verification that the target is one of the five most likely top identities."}, {"heading": "5.3. Experimental setup", "text": "Experiment A: Gait recognition with clothing and wearing conditions. This is the central experiment of this paper, in which we focus on evaluating the capacity of the proposed CNN model. In fact, a number of different types of people are used who have opted for the subsequent experiments. Training of the CNN miscellaneous is performed only by using sequences of the standard training and validation of shoes (i.e., 100 + 50 people) of TUMGAID, including the three scenarios in which the CNN model is used. Only sequences of the standard training and validation of shoes (i.e. 100 + 50 people) of TUMGUhren are used to evaluate the performance of the CNN model."}, {"heading": "5.4. Results and discussion", "text": "We conducted our experiments on a computer with 32 cores in 2 GHz, 256 GB of RAM and a GPU number of 48 participants each, most of them relying on Matlab 2014b for Ubuntu 14.04. After dividing the training sequences (of the subjects) into subsequences, we have a training set of 269352 samples, which are used to learn the filter systems, all of which are able to fine tune the last model (see Fig. 4); and a second training set consists of 108522 samples, which were composed of the substance itself. With all of these samples, the entire training process (from the first CNN model to the fine-tuning of the final model) took about 60 hours. Due to the specifications of the datasets, we had to balance the number of samples we needed to balance the number of samples."}, {"heading": "6. Conclusions and future work", "text": "The experimental validation was carried out with the sophisticated TUM-GAID dataset, using a version of the original low-resolution (i.e. eight times lower) video sequences, and the results suggest that the proposed CNN is capable of extracting meaningful gait signatures from pure sequences of optical flow (i.e. L2-normalized, fully bonded top layer) that allow high detection rates on the available scenarios (i.e. different clothing and carrier bags), achieving state-of-the-art results, as opposed to classic gait detection approaches that use handcrafted features, mainly binary silhouettes or dense tracklets. In terms of classification strategies, an interplay of \"one-vs-all\" features is a good choice, although a NN approach to PCA-compressed glances provides a similar accuracy that does not require gender detection before we take a step towards automatic delivery of our data."}], "references": [{"title": "Frontal-view gait recognition by intra- and inter-frame rectangle size distribution", "author": ["O. Barnich", "M.V. Droogenbroeck"], "venue": "Pattern Recognition Letters, 30(10):893 \u2013 901,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep learning", "author": ["Y. Bengio", "I.J. Goodfellow", "A. Courville"], "venue": "Book in preparation for MIT Press,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Empirical study of audio-visual features fusion for gait recognition", "author": ["F.M. Castro", "Mar\u0131\u0301n-Jim\u00e9nez", "N. Guil"], "venue": "In Proc. CAIP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Pyramidal Fisher Motion for multiview gait recognition", "author": ["F.M. Castro", "M. Mar\u0131\u0301n-Jim\u00e9nez", "R. Medina-Carnicer"], "venue": "In Proc. ICPR, pages 1692\u20131697,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "CoRR, abs/1410.0759,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P.P. Kuksa"], "venue": "CoRR, abs/1103.0398,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE PAMI, 35(8):1915\u20131929,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Two-frame motion estimation based on polynomial expansion", "author": ["G. Farneb\u00e4ck"], "venue": "Proc. of Scandinavian Conf. on Image Analysis, volume 2749, pages 363\u2013370,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Fisher tensor decomposition for unconstrained gait recognition", "author": ["W. Gong", "M. Sapienza", "F. Cuzzolin"], "venue": "Proc. of Tensor Methods for Machine Learning, Workshop of the European Conference of Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "On reducing the effect of covariate factors in gait recognition: a classifier ensemble method", "author": ["Y. Guan", "C. Li", "F. Roli"], "venue": "IEEE PAMI, 37(7):1521\u20131528,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A robust speed-invariant gait recognition system for walker and runner identification", "author": ["Y. Guan", "C.-T. Li"], "venue": "Intl. Conf. on Biometrics (ICB), pages 1\u20138,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Individual recognition using gait energy image", "author": ["J. Han", "B. Bhanu"], "venue": "IEEE PAMI, 28(2):316\u2013322,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "The TUM Gait from Audio, Image and Depth (GAID) database: Multimodal recognition of subjects and traits", "author": ["M. Hofmann", "J. Geiger", "S. Bachmann", "B. Schuller", "G. Rigoll"], "venue": "J. of Visual Com. and Image Repres., 25(1):195 \u2013 206,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal feature learning for gait biometric based human identity recognition", "author": ["E. Hossain", "G. Chetty"], "venue": "Neural Information Processing, pages 721\u2013728,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Enhanced gabor feature based classification using a regularized locally tensor discriminant model for multiview gait recognition", "author": ["H. Hu"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on, 23(7):1274\u20131286, July", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiview gait recognition based on patch distribution features and uncorrelated multilinear sparse local discriminant canonical correlation analysis", "author": ["H. Hu"], "venue": "Circuits  and Systems for Video Technology, IEEE Transactions on, 24(4):617\u2013630, April", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey on visual surveillance of object motion and behaviors", "author": ["W. Hu", "T. Tan", "L. Wang", "S. Maybank"], "venue": "Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, 34(3):334\u2013352,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Better exploiting motion for better action recognition", "author": ["M. Jain", "H. Jegou", "P. Bouthemy"], "venue": "CVPR, pages 2555\u20132562,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "An improved adaptive background mixture model for real-time tracking with shadow detection", "author": ["P. KaewTraKulPong", "R. Bowden"], "venue": "Video-Based Surveillance Systems, pages 135\u2013144.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiple HOG templates for gait recognition", "author": ["Y. Liu", "J. Zhang", "C. Wang", "L. Wang"], "venue": "Proc. ICPR, pages 2930\u2013 2933. IEEE,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "On how to improve tracklet-based gait recognition systems", "author": ["M.J. Mar\u0131\u0301n-Jim\u00e9nez", "F.M. Castro", "A. Carmona-Poyato", "N. Guil"], "venue": "Pattern Recognition Letters,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Uncooperative gait recognition by learning to rank", "author": ["R. Mart\u0131\u0301n-F\u00e9lez", "T. Xiang"], "venue": "Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Fisher kernels on visual vocabularies for image categorization", "author": ["F. Perronnin", "C. Dance"], "venue": "CVPR, pages 1\u20138. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Fisher vectors meet neural networks: A hybrid classification architecture", "author": ["F. Perronnin", "D. Larlus"], "venue": "CVPR, pages 3743\u20133752,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "NIPS, pages 568\u2013576,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "UCF101: A dataset of 101 human action classes from videos in the wild", "author": ["K. Soomro", "A.R. Zamir", "M. Shah"], "venue": "CRCV-TR-12-01, November", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "MatConvNet \u2013 Convolutional Neural Networks for MATLAB", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceeding of the ACM Int. Conf. on Multimedia,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Action Recognition by Dense Trajectories", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C.-L. Liu"], "venue": "CVPR, pages 3169\u2013 3176,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Action recognition with trajectory-pooled deep-convolutional descriptors", "author": ["L. Wang", "Y. Qiao", "X. Tang"], "venue": "CVPR, pages 4305\u20134314,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic distance-based shape features for gait recognition", "author": ["T. Whytock", "A. Belyaev", "N. Robertson"], "venue": "Journal of Mathematical Imaging and Vision, 50(3):314\u2013326,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "CoRR, abs/1311.2901,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Silhouette-based gait recognition via deterministic learning", "author": ["W. Zeng", "C. Wang", "F. Yang"], "venue": "Pattern Recognition, 47(11):3568 \u2013 3584,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "CoRR, abs/1509.01626,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "\u2018javelin throw\u2019) included in state-of-the-art datasets [29].", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "In last years, great effort has been put into the problem of people identification based on gait recognition [18].", "startOffset": 109, "endOffset": 113}, {"referenceID": 12, "context": "In this sense, the most popular silhouette-based gait descriptor is the called Gait Enery Image (GEI) [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 21, "context": "[22] propose the computation of HOG descriptors from GEI and the Chrono-Gait Image (CGI).", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Martin-Felez and Xiang [24], using GEI as the basic gait descriptor, propose a new ranking model that allows to leverage training data from different datasets.", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "Hu proposes in [16] the use of a regularized local tensor discriminant analysis method with the Enhanced Gabor representation of the GEI.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "In addition, the same author defines in [17] a method to identify camera viewpoints at test time from patch distribution features.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "[11] proposed a novel approach to deal with covariate factors (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Dense trajectories are described with the concatenation of different histograms, like Histograms of Oriented Gradients (HOG), Histograms of Optical Flow (HOF) and Motion Boundary Histograms (MBH) [31].", "startOffset": 196, "endOffset": 200}, {"referenceID": 18, "context": "[19] where instead of using HOG, HOF and MBH, they use a new kind of descriptor (DivergenceCurl-Shear) based on partial derivatives of the optical flow.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Finally all these trajectories are summarized at video level by using Fisher Vectors [25] as in [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "Finally all these trajectories are summarized at video level by using Fisher Vectors [25] as in [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 3, "context": "A successful gait descriptor based on this approach is the called \u2018Pyramidal Fisher Motion\u2019 [4], which has reported state-of-the-art results on several gait datasets [3, 23].", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "A successful gait descriptor based on this approach is the called \u2018Pyramidal Fisher Motion\u2019 [4], which has reported state-of-the-art results on several gait datasets [3, 23].", "startOffset": 166, "endOffset": 173}, {"referenceID": 22, "context": "A successful gait descriptor based on this approach is the called \u2018Pyramidal Fisher Motion\u2019 [4], which has reported state-of-the-art results on several gait datasets [3, 23].", "startOffset": 166, "endOffset": 173}, {"referenceID": 20, "context": "Traditionally, deep learning approaches based in Convolutional Neural Networks (CNN) have been used in image-based tasks with great success [21, 28, 34].", "startOffset": 140, "endOffset": 152}, {"referenceID": 27, "context": "Traditionally, deep learning approaches based in Convolutional Neural Networks (CNN) have been used in image-based tasks with great success [21, 28, 34].", "startOffset": 140, "endOffset": 152}, {"referenceID": 33, "context": "Traditionally, deep learning approaches based in Convolutional Neural Networks (CNN) have been used in image-based tasks with great success [21, 28, 34].", "startOffset": 140, "endOffset": 152}, {"referenceID": 26, "context": "In [27], Simonyan and Zisserman proposed to use as input to a CNN a volume obtained as the concatenation of frames with two channels that contain the optical flow in the x-axis and y-axis respectively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "[7] propose another point of view in deep learning using a novel architecture called \u201cLong-term Recurrent Convolutional Networks\u201d.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "[32] combined dense trajectories with deep learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] propose a more traditional approach using Fisher Vectors as input to a Deep Neural Network instead of using other classifiers like SVM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In [15], Hossain and Chetty propose the use of Restricted Boltzmann Machines to extract gait features from binary silhouettes, but a very small probe set (i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "Our approach takes the idea of Simonyan and Zisserman [27] and uses a spatio-temporal volume of optical flow as input to a CNN specially designed for gait recognition.", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "The convolutional neural network (CNN) model is an important type of feed-forward neural network with special success on applications where the target information can be represented by a hierarchy of local features (see [2]).", "startOffset": 220, "endOffset": 223}, {"referenceID": 5, "context": ") [6, 21, 8, 36].", "startOffset": 2, "endOffset": 16}, {"referenceID": 20, "context": ") [6, 21, 8, 36].", "startOffset": 2, "endOffset": 16}, {"referenceID": 7, "context": ") [6, 21, 8, 36].", "startOffset": 2, "endOffset": 16}, {"referenceID": 35, "context": ") [6, 21, 8, 36].", "startOffset": 2, "endOffset": 16}, {"referenceID": 26, "context": "The use of optical flow (OF) as input data for action representation in video with CNN has already shown excellent results [27].", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "Then, we compute dense OF on pairs of frames by using the method of Farneback [9] implemented in OpenCV library.", "startOffset": 78, "endOffset": 81}, {"referenceID": 19, "context": "In parallel, people are located in a rough manner along the video sequences by background substraction [20].", "startOffset": 103, "endOffset": 107}, {"referenceID": 0, "context": "For most state-of-the-start datasets, 25 frames cover almost one complete gait cycle, as stated by other authors [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 26, "context": "The CNN architecture we propose for gait recognition is based on the one described in [27] for general action recognition in video.", "startOffset": 86, "endOffset": 90}, {"referenceID": 29, "context": "Implementation details We use the implementation of CNN provided in MatConvNet library [30].", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "In addition, it takes advantage of CUDA and cuDNN [5] to improve the performance of the algorithms.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "[4]) indicate that this configuration of binary classifiers is suitable to obtain top-tier results in this problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "We run our experiments on the recent \u2018TUM Gait from Audio, Image and Depth\u2019 (TUM-GAID) dataset [14] for gait recognition.", "startOffset": 95, "endOffset": 99}, {"referenceID": 3, "context": "Moreover, for comparison purposes, we have implemented the \u2018Pyramidal Fisher Motion\u2019 (PFM) descriptor, as described in [4], since it does not need binary silhouettes as input for its computation and has previously reported stateof-the-art results for the problem of gait recognition [3].", "startOffset": 119, "endOffset": 122}, {"referenceID": 2, "context": "Moreover, for comparison purposes, we have implemented the \u2018Pyramidal Fisher Motion\u2019 (PFM) descriptor, as described in [4], since it does not need binary silhouettes as input for its computation and has previously reported stateof-the-art results for the problem of gait recognition [3].", "startOffset": 283, "endOffset": 286}, {"referenceID": 3, "context": "For \u2018PFM@640 \u00d7 480\u2019, we have used the whole video sequence to compute a single descriptor, as in the original paper [4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 13, "context": "For comparison purposes, bottom row contains the accuracy reported for this task in paper [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 26, "context": "ones made by Simonyan and Zisserman in [27] applied to action datasets.", "startOffset": 39, "endOffset": 43}, {"referenceID": 34, "context": "6 4 0 \u00d7 4 8 0 SDL [35] - - - - 96.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "9 - - GEI [14] 99.", "startOffset": 10, "endOffset": 14}, {"referenceID": 32, "context": "SEIM [33] 99.", "startOffset": 5, "endOffset": 9}, {"referenceID": 32, "context": "6 GVI [33] 99.", "startOffset": 6, "endOffset": 10}, {"referenceID": 32, "context": "9 SVIM [33] 98.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "0 RSM [12] 100.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "3 PFM [4] 99.", "startOffset": 6, "endOffset": 9}, {"referenceID": 13, "context": "[14] (640 \u00d7 480) 95.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "in [14], the average on the three scenarios (N, B, S) for our method is 88.", "startOffset": 3, "endOffset": 7}], "year": 2017, "abstractText": "This work targets people identification in video based on the way they walk (i.e. gait). While classical methods typically derive gait signatures from sequences of binary silhouettes, in this work we explore the use of convolutional neural networks (CNN) for learning high-level descriptors from low-level motion features (i.e. optical flow components). We carry out a thorough experimental evaluation of the proposed CNN architecture on the challenging TUM-GAID dataset. The experimental results indicate that using spatio-temporal cuboids of optical flow as input data for CNN allows to obtain state-of-the-art results on the gait task with an image resolution eight times lower than the previously reported results (i.e. 80\u00d7 60 pixels).", "creator": "LaTeX with hyperref package"}}}