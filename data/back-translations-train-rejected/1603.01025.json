{"id": "1603.01025", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Convolutional Neural Networks using Logarithmic Data Representation", "abstract": "Recent advances in convolutional neural networks have considered model complexity and hardware efficiency to enable deployment onto embedded systems and mobile devices. For example, it is now well-known that the arithmetic operations of deep networks can be encoded down to 8-bit fixed-point without significant deterioration in performance. However, further reduction in precision down to as low as 3-bit fixed-point results in significant losses in performance. In this paper we propose a new data representation that enables state-of-the-art networks to be encoded to 3 bits with negligible loss in classification performance. To perform this, we take advantage of the fact that the weights and activations in a trained network naturally have non-uniform distributions. Using non-uniform, base-2 logarithmic representation to encode weights, communicate activations, and perform dot-products enables networks to 1) achieve higher classification accuracies than fixed-point at the same resolution and 2) eliminate bulky digital multipliers. Finally, we propose an end-to-end training procedure that uses log representation at 5-bits, which achieves higher final test accuracy than linear at 5-bits.", "histories": [["v1", "Thu, 3 Mar 2016 08:51:52 GMT  (2526kb,D)", "http://arxiv.org/abs/1603.01025v1", "10 pages, 7 figures"], ["v2", "Thu, 17 Mar 2016 03:32:30 GMT  (2526kb,D)", "http://arxiv.org/abs/1603.01025v2", "10 pages, 7 figures"]], "COMMENTS": "10 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["daisuke miyashita", "edward h lee", "boris murmann"], "accepted": false, "id": "1603.01025"}, "pdf": {"name": "1603.01025.pdf", "metadata": {"source": "META", "title": "Convolutional Neural Networks using Logarithmic Data Representation", "authors": ["Daisuke Miyashita", "Edward H. Lee", "Boris Murmann"], "emails": ["DAISUKEM@STANFORD.EDU", "EDHLEE@STANFORD.EDU", "MURMANN@STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is a way in which people are able to surpass themselves by putting themselves at the centre of attention. (...) In fact, it is a way in which people are able to surpass themselves. (...) It is not as if people are able to surpass themselves. (...) It is as if people are able to surpass themselves. \"(...)\" It is not as if people are able to surpass themselves. \"(...) It is as if they are able to surpass themselves.\" (...), (...) It is as if they are able to surpass themselves. \"(...), (...) It is as if people are able to surpass themselves.\""}, {"heading": "2. Related work", "text": "However, they do not encourage this technique to complete network updates. (Shin et al., 2016; Sung et al., 2015; Vanhoucke et al., 2011; Han et al., 2015a) analyzed the effects of quantifying trained weights for conclusions. (Han et al., 2015b) shows that revolutionary layers in AlexNet (Krizhevsky et al., 2012) can be encoded to less than 5 bits without significant penalty of precision. There has also been recent work in training using low precision arithmetic. (Gupta et al., 2015) suggests a stochastic rounding scheme to support networks with 16-bit fixed points. (Lin et al., 2015) suggest quantified reverse propagation and ternary connection, which reduces the number of floating multiplications by throwing them into the power of two multiplies."}, {"heading": "3. Concept and Motivation", "text": "Each revolutionary and fully connected layer of a network performs matrix operations, which are distilled into point products y = wTx, where x-Rn is the input, w-Rn is the weights, and y is the activation before being transformed by nonlinearity (e.g. ReLU). Using conventional digital hardware, this process is performed with n multiplication and addition operations using floating or fixed point representations, as shown in Figure 1 (a), but this point product can also be calculated in the log domain, as shown in Figure 1 (b, c)."}, {"heading": "3.1. Proposed Method 1.", "text": "The first proposed method, as shown in Figure 1 (b), is to transform an operand into its log representation, convert the resulting transformation back to the linear range and multiply it by the other operand. This is simplewTx'n \u2211 i = 1wi \u00d7 2x \u0445i = n \u2211 i = 1 bit shift (wi, x-i), (1) where x-i = quantize (log2 (xi), quantize (\u2022) quantifies \u2022 to an integer, and Bitshift (a, b) is the function that shifts a value by an integer b in fixed point arithmetic. In floating point, this operation is simply an addition of b with the exponent part of a. Using the bit shift (a, b) operator to perform multiplications, it eliminates the need for expensive digital multiplications b in fixed point arithmetic."}, {"heading": "3.2. Proposed Method 2.", "text": "The second proposed method, as illustrated in Figure 1 (c), is the extension of the first method of calculating dot products in the log domain for both operands. Additions in the linear domain map to sums of exponentials in the log domain and multiplications in the log domain become log addition. The resulting dot product iswTx'n \u00b2 i = 12quantize (log2 (wi) + quantize (log2 (xi)) = n \u00b2 bitshift (1, w \u00b2 i + x \u00b2), (2) where the log domain weights w \u00b2 i = quantize (wi) and log domain inputs x \u00b2 i = quantize (log2)."}, {"heading": "4. Experiments of Proposed Methods", "text": "At this point, we evaluate our methods as described in sections 3.1 and 3.2 on the classification task of ILSVRC-2012 (Deng et al., 2009) using chainer (Tokui et al., 2015); we evaluate method 1 (Section 3.1) on forward pass in section 4.1; and we evaluate method 2 (Section 3.2) on inference in sections 4.2 and 4.3. For these experiments, we use published models (AlexNet (Krizhevsky et al., 2012), VGG16 (Simonyan & Zisserman, 2014) from the Caffe model zoo (((Jia et al., 2014)) without fine-tuning (or additional retraining)."}, {"heading": "4.1. Logarithmic Representation of Activations", "text": "It is not a matter of whether or not there is a connection between the two, but whether or not there is a connection between the two, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not there is a connection, whether or not, whether or not there is a connection, whether or not, whether or not there is a connection, whether or not, whether or not."}, {"heading": "4.2. Logarithmic Representation of Weights of Fully Connected Layers", "text": "The FC weights are quantified using the same strategies as those in Section 4.1, except that they have a sign bit. We evaluate the classification performance using the protocol data representation for both FC weights and activations together with Method 2 in Section 3.2. For comparison, we use linear protocols for FC weights and protocols for activations for reference. For both methods, we use optimal 4b protocols for activations calculated in Section 4.1. Table 4 compares the above approaches to floating point. We observe a small gain of 0.4% for protocols using linear protocols for AlexNet, but a 0.2% reduction for VGG16. Nevertheless, the protocol calculation is done without the use of multipliers. An additional benefit of quantification is a reduction in the model size. By quantifying to 4b including sign bit, we compress the FC weights significantly from 1.9 Gb to 0.27 Gb total GFC for Alex8b and 4.4 Gb respectively for Alex8b and G4."}, {"heading": "4.3. Logarithmic Representation of Weights of Convolutional Layers", "text": "We consider the representation of the activations at 4b log and the representation of the weights of the FC layers at 4b log, and compare our log method with the linear reference and the ideal floating point. We also perform the Dot products using two different bases: 2, \u221a 2. Note that there is no additional expense for the log base \u221a 2, since it is calculated using the same equation as shown in Equation 4.Table 5. Results show a decrease in the performance of floating point to 5b base-2 of about 6%, but a relatively small decrease of 1.7% for 5b base \u221a 2. They include character bit. There are also some important observations here.We observe first that the weights of the Convolutionary layers for AlexNet and VGG16 are more sensitive to quantification than the FC weights. Each FC weight is used only once per image (batch size of 1), whereas Convolutionary weights when entering the layer for AlexNet and VG16 are more sensitive to quantification than the quantification of 5b and 5b."}, {"heading": "4.4. Training with Logarithmic Representation", "text": "The proposed protocol and linear networks are trained at the same resolution using 4-bit unsigned activations and 5-bit signed weights and gradients using algorithm 1 on the CIFAR10 dataset with simple data augmentation (He et al., 2015). Note: Unlike BinaryNet (Courbariaux & Bengio, 2016), we quantify the backpropagated gradients to form lognet networks, enabling end-to-end training with logarithmic representation at the 5-bit level. However, for linear quantization, we considered it necessary to maintain the gradients in their unquantified floating point accuracy in order to achieve good convergence."}, {"heading": "5. Conclusion", "text": "In this paper, we describe a method for displaying the weights and low-resolution activations in the log domain that eliminates bulky digital multipliers, which is also motivated by the unequal distribution of weights and activations, making log representation more robust than quantification. We evaluate our methods for the classification task of ILSVRC-2012 using pre-trained models (AlexNet and VGG16)."}], "references": [{"title": "Noise benefits in backpropagation and deep bidirectional pre-training", "author": ["Audhkhasi", "Kartik", "Osoba", "Osonde", "Kosko", "Bart"], "venue": "In Proceedings of The 2013 International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "Audhkhasi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Audhkhasi et al\\.", "year": 2013}, {"title": "Training with noise is equivalent to tikhonov regularization", "author": ["Bishop", "Christopher M"], "venue": "In Neural Computation,", "citeRegEx": "Bishop and M.,? \\Q1995\\E", "shortCiteRegEx": "Bishop and M.", "year": 1995}, {"title": "The tradeoffs of large scale learning", "author": ["Bottou", "L\u00e9on", "Bousquet", "Olivier"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Bottou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2007}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Chetlur", "Sharan", "Woolley", "Cliff", "Vandermersch", "Philippe", "Cohen", "Jonathan", "Tran", "John", "Catanzaro", "Bryan", "Shelhamer", "Evan"], "venue": "In Proceedings of Deep Learning and Representation Learning Workshop: NIPS 2014,", "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. FeiFei"], "venue": "In CVPR09,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Hardware accelerated convolutional neural networks for synthetic vision systems", "author": ["Farabet", "Cl\u00e9ment", "Martini", "Berin", "Akselrod", "Polina", "Talay", "Sel\u00e7uk", "LeCun", "Yann", "Culurciello", "Eugenio"], "venue": "In Proceedings of 2010 IEEE International Symposium on Circuits and Systems (IS-", "citeRegEx": "Farabet et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2010}, {"title": "Deep learning with limited numerical precision", "author": ["Gupta", "Suyog", "Agrawal", "Ankur", "Gopalakrishnan", "Kailash", "Narayanan", "Pritish"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Neural networks with few multiplications", "author": ["Lin", "Zhouhan", "Courbariaux", "Matthieu", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1510.03009,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Neelakantan", "Arvind", "Vilnis", "Luke", "Le", "Quoc V", "Sutskever", "Ilya", "Kaiser", "Lukasz", "Karol Kurach", "James Martens"], "venue": "arXiv preprint arXiv:1511.06807,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Tensorizing neural networks", "author": ["Novikov", "Alexander", "Podoprikhin", "Dmitry", "Osokin", "Anton", "Vetrov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Novikov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novikov et al\\.", "year": 2015}, {"title": "Fixed point performance analysis of recurrent neural networks", "author": ["Shin", "Sungho", "Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Proceedings of The 41st IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP2016)", "citeRegEx": "Shin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:11409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Resiliency of deep neural networks under quantization", "author": ["Sung", "Wonyong", "Shin", "Sungho", "Hwang", "Kyuyeon"], "venue": "arXiv preprint arXiv:1511.06488,", "citeRegEx": "Sung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sung et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": null, "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vanhoucke", "Vincent", "Senior", "Andrew", "Mao", "Mark Z"], "venue": "In Proceedings of Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Optimizing FPGA-based accelerator design for deep convolutional neural networks", "author": ["Zhang", "Chen", "Li", "Peng", "Sun", "Guangyu", "Guan", "Yijin", "Xiao", "Bingjun", "Cong", "Jason"], "venue": "In Proceedings of 23rd International Symposium on Field-Programmable Gate Arrays (FPGA2015),", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Deep convolutional neural networks (CNN) have demonstrated state-of-the-art performance in image classification (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2015) but have steadily grown in computational complexity.", "startOffset": 112, "endOffset": 182}, {"referenceID": 11, "context": "Deep convolutional neural networks (CNN) have demonstrated state-of-the-art performance in image classification (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2015) but have steadily grown in computational complexity.", "startOffset": 112, "endOffset": 182}, {"referenceID": 11, "context": "For example, the Deep Residual Learning (He et al., 2015) set a new record in image classification accuracy at the expense of 11.", "startOffset": 40, "endOffset": 57}, {"referenceID": 20, "context": "Recently, many researchers have successfully deployed networks that compute using 8-bit fixed-point representation (Vanhoucke et al., 2011; Abadi et al., 2015) and have successfully trained networks with 16-bit fixed point (Gupta et al.", "startOffset": 115, "endOffset": 159}, {"referenceID": 8, "context": ", 2015) and have successfully trained networks with 16-bit fixed point (Gupta et al., 2015).", "startOffset": 71, "endOffset": 91}, {"referenceID": 6, "context": "Interesting directions point towards matrix factorization (Denton et al., 2014) and tensorification (Novikov et al.", "startOffset": 58, "endOffset": 79}, {"referenceID": 15, "context": ", 2014) and tensorification (Novikov et al., 2015) by leveraging structure of the fully-connected (FC) layers.", "startOffset": 28, "endOffset": 50}, {"referenceID": 11, "context": "And with network architectures currently pushing towards increasing the depth of convolutional layers by settling for fewer dense FC layers (He et al., 2015; Szegedy et al., 2015), there are potential problems in motivating a one-size-fits-all solution to handle these computational and memory demands.", "startOffset": 140, "endOffset": 179}, {"referenceID": 19, "context": "And with network architectures currently pushing towards increasing the depth of convolutional layers by settling for fewer dense FC layers (He et al., 2015; Szegedy et al., 2015), there are potential problems in motivating a one-size-fits-all solution to handle these computational and memory demands.", "startOffset": 140, "endOffset": 179}, {"referenceID": 16, "context": "(Shin et al., 2016; Sung et al., 2015; Vanhoucke et al., 2011; Han et al., 2015a) analyzed the effects of quantizing the trained weights for inference.", "startOffset": 0, "endOffset": 81}, {"referenceID": 18, "context": "(Shin et al., 2016; Sung et al., 2015; Vanhoucke et al., 2011; Han et al., 2015a) analyzed the effects of quantizing the trained weights for inference.", "startOffset": 0, "endOffset": 81}, {"referenceID": 20, "context": "(Shin et al., 2016; Sung et al., 2015; Vanhoucke et al., 2011; Han et al., 2015a) analyzed the effects of quantizing the trained weights for inference.", "startOffset": 0, "endOffset": 81}, {"referenceID": 12, "context": ", 2015b) shows that convolutional layers in AlexNet (Krizhevsky et al., 2012) can be encoded to as little as 5 bits without a significant accuracy penalty.", "startOffset": 52, "endOffset": 77}, {"referenceID": 8, "context": "(Gupta et al., 2015) propose a stochastic rounding scheme to help train networks using 16-bit fixed-point.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "(Lin et al., 2015) propose quantized back-propagation and ternary connect.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "Training with reduced precision is motivated by the idea that high-precision gradient updates is unnecessary for the stochastic optimization of networks (Bottou & Bousquet, 2007; Bishop, 1995; Audhkhasi et al., 2013).", "startOffset": 153, "endOffset": 216}, {"referenceID": 14, "context": "For example, (Neelakantan et al., 2015) empirically finds that gradient noise can also encourage faster exploration and annealing of optimization space, which can help network generalization performance.", "startOffset": 13, "endOffset": 39}, {"referenceID": 7, "context": "For example (Farabet et al., 2010) developed Field-Programmable Gate Arrays (FPGA) to perform real-time forward propagation.", "startOffset": 12, "endOffset": 34}, {"referenceID": 21, "context": "(Zhang et al., 2015) have also explored the design of convolutions in the context of memory versus compute management under the RoofLine model.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Other works focus on specialized, optimized kernels for general purpose GPUs (Chetlur et al., 2014).", "startOffset": 77, "endOffset": 99}, {"referenceID": 12, "context": "Structure of AlexNet(Krizhevsky et al., 2012) with quan-", "startOffset": 20, "endOffset": 45}, {"referenceID": 5, "context": "2 on the classification task of ILSVRC-2012 (Deng et al., 2009) using Chainer (Tokui et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 12, "context": "For those experiments, we use published models (AlexNet (Krizhevsky et al., 2012), VGG16 (Simonyan & Zisserman, 2014)) from the caffe model zoo ((Jia et al.", "startOffset": 56, "endOffset": 81}, {"referenceID": 8, "context": "In similar spirit to that of (Gupta et al., 2015), we describe the logarithmic quantization layer LogQuant that performs the element-wise operation as follows:", "startOffset": 29, "endOffset": 49}, {"referenceID": 11, "context": "The proposed log and linear networks are trained at the same resolution using 4-bit unsigned activations and 5-bit signed weights and gradients using Algorithm 1 on the CIFAR10 dataset with simple data augmentation described in (He et al., 2015).", "startOffset": 228, "endOffset": 245}], "year": 2017, "abstractText": "Recent advances in convolutional neural networks have considered model complexity and hardware efficiency to enable deployment onto embedded systems and mobile devices. For example, it is now well-known that the arithmetic operations of deep networks can be encoded down to 8-bit fixed-point without significant deterioration in performance. However, further reduction in precision down to as low as 3-bit fixed-point results in significant losses in performance. In this paper we propose a new data representation that enables state-of-the-art networks to be encoded to 3 bits with negligible loss in classification performance. To perform this, we take advantage of the fact that the weights and activations in a trained network naturally have non-uniform distributions. Using non-uniform, base-2 logarithmic representation to encode weights, communicate activations, and perform dot-products enables networks to 1) achieve higher classification accuracies than fixed-point at the same resolution and 2) eliminate bulky digital multipliers. Finally, we propose an end-to-end training procedure that uses log representation at 5-bits, which achieves higher final test accuracy than linear at 5-bits.", "creator": "LaTeX with hyperref package"}}}