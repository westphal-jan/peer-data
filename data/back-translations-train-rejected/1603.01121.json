{"id": "1603.01121", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Deep Reinforcement Learning from Self-Play in Imperfect-Information Games", "abstract": "Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without any prior knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a competitive strategy that approached the performance of human experts and state-of-the-art methods.", "histories": [["v1", "Thu, 3 Mar 2016 15:01:54 GMT  (305kb,D)", "http://arxiv.org/abs/1603.01121v1", null], ["v2", "Tue, 28 Jun 2016 15:28:30 GMT  (274kb,D)", "http://arxiv.org/abs/1603.01121v2", "updated version, incorporating conference feedback"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.GT", "authors": ["johannes heinrich", "david silver"], "accepted": false, "id": "1603.01121"}, "pdf": {"name": "1603.01121.pdf", "metadata": {"source": "META", "title": "Deep Reinforcement Learning from Self-Play in Imperfect-Information Games", "authors": ["Johannes Heinrich", "David Silver"], "emails": ["J.HEINRICH@CS.UCL.AC.UK", "D.SILVER@CS.UCL.AC.UK"], "sections": [{"heading": "1. Introduction", "text": "This year it is so far that it will only take one year to reach an agreement."}, {"heading": "2. Background", "text": "In this section we give a brief overview of reinforcement learning, extensive form games and fictional self-play. For a more detailed description, we refer the reader to (Sutton & Barto, 1998), (Myerson, 1991), (Fudenberg, 1998) and (Heinrich et al., 2015)."}, {"heading": "2.1. Reinforcement Learning", "text": "Reinforcement learning (Sutton & Barto, 1998) Agents typically learn to maximize their expected future rewards from interacting with an environment.The agent's goal is to improve their policies to maximize their profit.Gt + 1 = tRi + 1, which is a random variable of the agent's cumulative future rewards, starting with time.Many arrangement learning algorithms learn from sequential experiences in the form of transition tuples (st, rt + 1, st + 1), where st is the state in due course t, rt + 1 is the reward that will be received thereafter, and st + 1 is the next state in which the agent implements policy.A common goal is to learn the action value function, Q (s, a) = Riedel's learning process."}, {"heading": "2.2. Extensive-Form Games", "text": "Based on rationality, the goal of each player is to maximize their profit in the game. In games with imperfect information, each player only observes their respective information states, e.g. in a poker game, a player only knows his own private cards, but not those of other players. Each player chooses a behavioral strategy that maps information states to probability distributions of available actions. We assume that games with perfect recall, i.e. the current information state of each player implies knowledge of the sequence of information states and actions, si1, a i 1, s i 2, a i 2,... that have led to this information state. Realization probability (Von Stengel, 1996), x\u03c0i (sit) = 1 k = 1 \u03c0 i (sik, a i k), determines the probability that players i \u2212 s are more strategically brilliant than an approximate balance profile that an equilibrium test is that all strategies are an optimal balance fix."}, {"heading": "2.3. Fictitious Self-Play", "text": "Fictional Game (Brown, 1951) is a game theoretical model of self-play learning. Fictional players select the best responses to the average behavior of their opponents. Average strategies of fictional players converge to Nash balances in certain game classes. It has similar convergence to common fictional games, but allows approximate best answers and disruptive average strategy updates, making it particularly suitable for machine learning. Fictional play is generally defined in normal form, which is exponentially less efficient for extensive form play. Heinrich et al. (2015) introduce Full-Width ExtensiveForm ExtensiveForm ExtensiveForm Play (XFP), which enables fictional players to update their strategies in behavioral, extensive form."}, {"heading": "3. Neural Fictitious Self-Play", "text": "The NFSP agent is an agent who interacts with the other players in a game and stores his experiences of the game transitions and his own behavior. The resulting network defines the approximate learning strategy of the agent, \u03b2 = -greedy (FQ), which selects a random action with probability and otherwise selects the measures that maximize the predicted action values. The NFSP agent trains a separate neural network to imitate his own past behavior using the data."}, {"heading": "4. Experiments", "text": "We evaluate NFSP and related algorithms in Leduc (Southey et al., 2005) and Limit Texas Hold'em poker games. Most of our experiments measure the recoverability of learned strategy profiles. In a zero-sum game for two players, the recoverability of a strategy profile is defined as the expected average yield that a best reaction profile achieves. A recoverability of 2\u03b4 results in at least a \u03b4-Nash equilibrium."}, {"heading": "4.1. Robustness of XFP", "text": "In order to understand how function approximation interacts with FSP, we start with some simple experiments that emulate approach and sampling errors in the full-width XFP algorithm. First, we examine what happens when the perfect averaging used in XFP is replaced by an incremental mean process that is closer to lowering the gradient. Second, we examine what happens when the exact table check used in XFP is replaced by an approximation with Epsilon errors. Figure 1 shows the performance of XFP with default values, 1 / T and constant step sizes for its sampling strategy updates. We see improved asymptotic, but lower initial performance for smaller step sizes. Because constant step sizes of sampling performance seem to lead to a plateau rather than to deviate from each other when sampling sampling sampling sampling sample values. By sampling reservoirs, we can achieve an effective step size of 1 / T for sampling sampling sampling sampling sampling samples, but lower initial performance for smaller step sizes."}, {"heading": "4.2. Convergence of NFSP", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4.3. Comparison to DQN", "text": "In fact, it is so that most of them will be able to move around, while they are able to move around, most of them are not able to move around, most of them are not able to move around, most of them are not able to move around, most of them are not able to move around, most of them are not able to move around, most of them are not able to move around, most of them are not able to move around, most of them are not able to move around, most of them are not able to move around, most of them are not able to move around."}, {"heading": "4.4. Limit Texas Hold\u2019em", "text": "We applied NFSP to LHE, a game popular with humans. Since 2008 a computer program beat the experienced human LHE players for the first time in a public competition, modern computer agents are widely regarded as superhuman (Newall, 2013). The game was essentially solved by Bowling et al. (2015). We rated our agents against SmooCT, a smooth UCT agent that started at the beginning of a hand.We calibrated NFSP manually by trying out 9 configurations. We achieved the best performance in milli-big blinds won per hand, mbb / h, i.e. a thousandth of a large blind, the players at the beginning of a hand.We calibrated NFSP manually by trying out 9 configurations. We achieved the best performance with the following parameters. The neural networks were completely connected to four hidden levels of 1024, 512, 1024 and 512 neurons with fictitious lines."}, {"heading": "5. Related work", "text": "In fact, it is not as if one sees oneself as being able to surpass oneself, as if one could surpass oneself. (...) It is not as if one is able to surpass oneself. (...) It is not as if one is able to surpass oneself. (...) It is not as if one is able to surpass oneself. (...) It is as if one is able to surpass oneself. (...) It is as if one is able to surpass oneself. (...) It is as if one is able to surpass oneself. (...) It is not as if one can surpass oneself. (...) It is not as if one is surpassing oneself. (...) It is not as if one is surpassing oneself. (...) It is not as if one is surpassing oneself. (...) It is not as if one is surpassing oneself."}, {"heading": "6. Conclusion", "text": "We have introduced NFSP, the first end-to-end approach to learning approximate Nash balances of imperfect information games from self-play. NFSP addresses three problems: First, NFSP agents learn without prior knowledge; second, they do not rely on local search at runtime; and third, they approach Nash balances in self-play. Our empirical results provide the following insights: The performance of a fictional game deteriorates in a straight line with various approximation errors; and third, NFSP reliably approaches Nash balances in a small poker game, while the greedy and average strategies of the DQN do not. NFSP learned a competitive strategy from scratch in an imperfect information game on a real scale, without applying explicit prior knowledge. In this work, we focus on imperfect information in two-tiered zero-sum games."}, {"heading": "Acknowledgements", "text": "We thank Peter Dayan, Marc Lanctot and Marc Bellemare for helpful discussions and feedback. This research was supported by the UK Centre for Doctoral Training in Financial Computing and NVIDIA Corporation."}], "references": [{"title": "Predictably irrational", "author": ["Ariely", "Dan", "Jones", "Simon"], "venue": "HarperCollins New York,", "citeRegEx": "Ariely et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ariely et al\\.", "year": 2008}, {"title": "Opportunities for multiagent systems and multiagent reinforcement learning in traffic control", "author": ["Bazzan", "Ana LC"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Bazzan and LC.,? \\Q2009\\E", "shortCiteRegEx": "Bazzan and LC.", "year": 2009}, {"title": "Rational and convergent learning in stochastic games", "author": ["Bowling", "Michael", "Veloso", "Manuela"], "venue": "In Proceedings of the 17th International Joint Conference on Artifical Intelligence,", "citeRegEx": "Bowling et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bowling et al\\.", "year": 2001}, {"title": "Heads-up limit holdem poker is solved", "author": ["Bowling", "Michael", "Burch", "Neil", "Johanson", "Tammelin", "Oskari"], "venue": null, "citeRegEx": "Bowling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowling et al\\.", "year": 2015}, {"title": "Iterative solution of games by fictitious play", "author": ["Brown", "George W"], "venue": "Activity analysis of production and allocation,", "citeRegEx": "Brown and W.,? \\Q1951\\E", "shortCiteRegEx": "Brown and W.", "year": 1951}, {"title": "Solving imperfect information games using decomposition", "author": ["Burch", "Neil", "Johanson", "Michael", "Bowling"], "venue": "In 28th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Burch et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Burch et al\\.", "year": 2014}, {"title": "Training deep convolutional neural networks to play go", "author": ["Clark", "Christopher", "Storkey", "Amos"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Clark et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2015}, {"title": "Optimal network security hardening using attack graph games", "author": ["Durkota", "Karel", "Lis\u1ef3", "Viliam", "Bo\u0161ansk\u1ef3", "Branislav", "Kiekintveld", "Christopher"], "venue": "In Proceedings of the 24th International Joint Conference on Artifical Intelligence,", "citeRegEx": "Durkota et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Durkota et al\\.", "year": 2015}, {"title": "Treebased batch mode reinforcement learning", "author": ["Ernst", "Damien", "Geurts", "Pierre", "Wehenkel", "Louis"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Ernst et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ernst et al\\.", "year": 2005}, {"title": "The theory of learning in games, volume 2", "author": ["Fudenberg", "Drew"], "venue": "MIT press,", "citeRegEx": "Fudenberg and Drew.,? \\Q1998\\E", "shortCiteRegEx": "Fudenberg and Drew.", "year": 1998}, {"title": "Endgame solving in large imperfect-information games", "author": ["Ganzfried", "Sam", "Sandholm", "Tuomas"], "venue": "In Proceedings of the 14th International Conference on Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Ganzfried et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ganzfried et al\\.", "year": 2015}, {"title": "The grand challenge of computer go: Monte Carlo tree search and extensions", "author": ["Gelly", "Sylvain", "Kocsis", "Levente", "Schoenauer", "Marc", "Sebag", "Mich\u00e8le", "Silver", "David", "Szepesv\u00e1ri", "Csaba", "Teytaud", "Olivier"], "venue": "Communications of the ACM,", "citeRegEx": "Gelly et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gelly et al\\.", "year": 2012}, {"title": "Gradient-based algorithms for finding Nash equilibria in extensive form games", "author": ["Gilpin", "Andrew", "Hoda", "Samid", "Pena", "Javier", "Sandholm", "Tuomas"], "venue": "In Internet and Network Economics,", "citeRegEx": "Gilpin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gilpin et al\\.", "year": 2007}, {"title": "Smooth UCT search in computer poker", "author": ["Heinrich", "Johannes", "Silver", "David"], "venue": "In Proceedings of the 24th International Joint Conference on Artifical Intelligence,", "citeRegEx": "Heinrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heinrich et al\\.", "year": 2015}, {"title": "Fictitious self-play in extensive-form games", "author": ["Heinrich", "Johannes", "Lanctot", "Marc", "Silver", "David"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Heinrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heinrich et al\\.", "year": 2015}, {"title": "Evaluating state-space abstractions in extensive-form games", "author": ["Johanson", "Michael", "Burch", "Neil", "Valenzano", "Richard", "Bowling"], "venue": "In Proceedings of the 12th International Conference on Autonomous Agents and MultiAgent Systems,", "citeRegEx": "Johanson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johanson et al\\.", "year": 2013}, {"title": "A fictitious play approach to large-scale optimization", "author": ["III Lambert", "J Theodore", "Epelman", "A Marina", "Smith", "L. Robert"], "venue": "Operations Research,", "citeRegEx": "Lambert et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lambert et al\\.", "year": 2005}, {"title": "Generalised weakened fictitious play", "author": ["Leslie", "David S", "Collins", "Edmund J"], "venue": "Games and Economic Behavior,", "citeRegEx": "Leslie et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Leslie et al\\.", "year": 2006}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["Lin", "Long-Ji"], "venue": "Machine learning,", "citeRegEx": "Lin and Long.Ji.,? \\Q1992\\E", "shortCiteRegEx": "Lin and Long.Ji.", "year": 1992}, {"title": "Move evaluation in go using deep convolutional neural networks", "author": ["Maddison", "Chris J", "Huang", "Aja", "Sutskever", "Ilya", "Silver", "David"], "venue": "The International Conference on Learning Representations,", "citeRegEx": "Maddison et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2015}, {"title": "Fictitious play property for games with identical interests", "author": ["Monderer", "Dov", "Shapley", "Lloyd S"], "venue": "Journal of economic theory,", "citeRegEx": "Monderer et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Monderer et al\\.", "year": 1996}, {"title": "Game Theory: Analysis of Conflict", "author": ["Myerson", "Roger B"], "venue": null, "citeRegEx": "Myerson and B.,? \\Q1991\\E", "shortCiteRegEx": "Myerson and B.", "year": 1991}, {"title": "Reinforcement learning for optimized trade execution", "author": ["Nevmyvaka", "Yuriy", "Feng", "Yi", "Kearns", "Michael"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Nevmyvaka et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nevmyvaka et al\\.", "year": 2006}, {"title": "Further Limit Hold \u2019em: Exploring the Model Poker Game", "author": ["P. Newall"], "venue": "Two Plus Two Publishing,", "citeRegEx": "Newall,? \\Q2013\\E", "shortCiteRegEx": "Newall", "year": 2013}, {"title": "Exponential reservoir sampling for streaming language models", "author": ["Osborne", "Miles", "Lall", "Ashwin", "Van Durme", "Benjamin"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Osborne et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osborne et al\\.", "year": 2014}, {"title": "Computing approximate Nash equilibria and robust bestresponses using sampling", "author": ["Ponsen", "Marc", "de Jong", "Steven", "Lanctot"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Ponsen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ponsen et al\\.", "year": 2011}, {"title": "Neural fitted q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["Riedmiller", "Martin"], "venue": "In Machine Learning: ECML", "citeRegEx": "Riedmiller and Martin.,? \\Q2005\\E", "shortCiteRegEx": "Riedmiller and Martin.", "year": 2005}, {"title": "Reinforcement learning for robot soccer", "author": ["Riedmiller", "Martin", "Gabel", "Thomas", "Hafner", "Roland", "Lange", "Sascha"], "venue": "Autonomous Robots,", "citeRegEx": "Riedmiller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Riedmiller et al\\.", "year": 2009}, {"title": "An iterative method of solving a game", "author": ["Robinson", "Julia"], "venue": "Annals of Mathematics, pp", "citeRegEx": "Robinson and Julia.,? \\Q1951\\E", "shortCiteRegEx": "Robinson and Julia.", "year": 1951}, {"title": "Some studies in machine learning using the game of checkers", "author": ["Samuel", "Arthur L"], "venue": "IBM Journal of research and development,", "citeRegEx": "Samuel and L.,? \\Q1959\\E", "shortCiteRegEx": "Samuel and L.", "year": 1959}, {"title": "Bounded rationality", "author": ["Selten", "Reinhard"], "venue": "Journal of Institutional and Theoretical Economics, pp", "citeRegEx": "Selten and Reinhard.,? \\Q1990\\E", "shortCiteRegEx": "Selten and Reinhard.", "year": 1990}, {"title": "Dynamic fictitious play, dynamic gradient play, and distributed convergence to Nash equilibria", "author": ["Shamma", "Jeff S", "Arslan", "G\u00fcrdal"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Shamma et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Shamma et al\\.", "year": 2005}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["Sutskever", "Ilya", "Lillicrap", "Timothy", "Leach", "Madeleine", "Kavukcuoglu", "Koray", "Graepel", "Thore", "Hassabis", "Demis"], "venue": "search. Nature,", "citeRegEx": "Sutskever et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2016}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Security and game theory: algorithms, deployed systems, lessons learned", "author": ["Tambe", "Milind"], "venue": null, "citeRegEx": "Tambe and Milind.,? \\Q2011\\E", "shortCiteRegEx": "Tambe and Milind.", "year": 2011}, {"title": "Temporal difference learning and tdgammon", "author": ["Tesauro", "Gerald"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "Tactex\u201913: a champion adaptive power trading agent", "author": ["Urieli", "Daniel", "Stone", "Peter"], "venue": "In Proceedings of the 13th International Conference on Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Urieli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Urieli et al\\.", "year": 2014}, {"title": "Random sampling with a reservoir", "author": ["Vitter", "Jeffrey S"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "citeRegEx": "Vitter and S.,? \\Q1985\\E", "shortCiteRegEx": "Vitter and S.", "year": 1985}, {"title": "Efficient computation of behavior strategies", "author": ["Von Stengel", "Bernhard"], "venue": "Games and Economic Behavior,", "citeRegEx": "Stengel and Bernhard.,? \\Q1996\\E", "shortCiteRegEx": "Stengel and Bernhard.", "year": 1996}, {"title": "Solving games with functional regret estimation", "author": ["Waugh", "Kevin", "Morrill", "Dustin", "Bagnell", "J. Andrew", "Bowling", "Michael"], "venue": "In 29th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Waugh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Waugh et al\\.", "year": 2015}, {"title": "Poker-cnn: A pattern learning strategy for making draws and bets in poker games using convolutional networks", "author": ["Yakovenko", "Nikolai", "Cao", "Liangliang", "Raffel", "Colin", "Fan", "James"], "venue": "In 30th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Yakovenko et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yakovenko et al\\.", "year": 2016}, {"title": "Regret minimization in games with incomplete information", "author": ["Zinkevich", "Martin", "Johanson", "Michael", "Bowling", "Piccione", "Carmelo"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zinkevich et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 28, "context": "Games have a tradition of encouraging advances in artificial intelligence and machine learning (Samuel, 1959; Tesauro, 1995; Campbell et al., 2002; Riedmiller et al., 2009; Gelly et al., 2012; Bowling et al., 2015).", "startOffset": 95, "endOffset": 214}, {"referenceID": 11, "context": "Games have a tradition of encouraging advances in artificial intelligence and machine learning (Samuel, 1959; Tesauro, 1995; Campbell et al., 2002; Riedmiller et al., 2009; Gelly et al., 2012; Bowling et al., 2015).", "startOffset": 95, "endOffset": 214}, {"referenceID": 3, "context": "Games have a tradition of encouraging advances in artificial intelligence and machine learning (Samuel, 1959; Tesauro, 1995; Campbell et al., 2002; Riedmiller et al., 2009; Gelly et al., 2012; Bowling et al., 2015).", "startOffset": 95, "endOffset": 214}, {"referenceID": 23, "context": "develop algorithms that will scale to more complex, realworld games such as airport and network security, financial and energy trading, traffic control and routing (Lambert III et al., 2005; Nevmyvaka et al., 2006; Bazzan, 2009; Tambe, 2011; Urieli & Stone, 2014; Durkota et al., 2015).", "startOffset": 164, "endOffset": 285}, {"referenceID": 7, "context": "develop algorithms that will scale to more complex, realworld games such as airport and network security, financial and energy trading, traffic control and routing (Lambert III et al., 2005; Nevmyvaka et al., 2006; Bazzan, 2009; Tambe, 2011; Urieli & Stone, 2014; Durkota et al., 2015).", "startOffset": 164, "endOffset": 285}, {"referenceID": 13, "context": "Technically, NFSP extends and instantiates Fictitious Self-Play (FSP) (Heinrich et al., 2015) with neural network function approximation.", "startOffset": 70, "endOffset": 93}, {"referenceID": 42, "context": "stract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013).", "startOffset": 36, "endOffset": 104}, {"referenceID": 12, "context": "stract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013).", "startOffset": 36, "endOffset": 104}, {"referenceID": 15, "context": "stract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013).", "startOffset": 36, "endOffset": 104}, {"referenceID": 3, "context": "While Limit Texas Hold\u2019em (LHE), a poker game of real-world scale, has got within reach of being solved with current computational resources (Bowling et al., 2015), most other poker and real-world games remain far out of scope without ab-", "startOffset": 141, "endOffset": 163}, {"referenceID": 13, "context": "For a more detailed exposition we refer the reader to (Sutton & Barto, 1998), (Myerson, 1991), (Fudenberg, 1998) and (Heinrich et al., 2015).", "startOffset": 117, "endOffset": 140}, {"referenceID": 8, "context": "Fitted Q Iteration (FQI) (Ernst et al., 2005) is a batch reinforcement learning method that replays experience with Q-learning.", "startOffset": 25, "endOffset": 45}, {"referenceID": 13, "context": "Heinrich et al. (2015) introduce Full-Width ExtensiveForm Fictitious Play (XFP) that enables fictitious players to update their strategies in behavioural, extensive form, re-", "startOffset": 0, "endOffset": 23}, {"referenceID": 13, "context": "Heinrich et al. (2015) introduce Fictitious Self-Play (FSP), a sample- and machine learning-based class of algorithms that approximate XFP.", "startOffset": 0, "endOffset": 23}, {"referenceID": 25, "context": "NFSP uses reservoir sampling (Vitter, 1985; Osborne et al., 2014) to memorize experience of its average best responses.", "startOffset": 29, "endOffset": 65}, {"referenceID": 13, "context": "Heinrich et al. (2015) propose to use sampling and machine learning to generate data on and learn convex combinations of normal-form strategies in extensive form.", "startOffset": 0, "endOffset": 23}, {"referenceID": 41, "context": "The Poker-CNN algorithm introduced by Yakovenko et al. (2016) stores a small number of past strategies which it iteratively computes new strategies against.", "startOffset": 38, "endOffset": 62}, {"referenceID": 42, "context": "Contrary to other work on computer poker (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013), we do not engineer any higher-level features.", "startOffset": 41, "endOffset": 109}, {"referenceID": 12, "context": "Contrary to other work on computer poker (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013), we do not engineer any higher-level features.", "startOffset": 41, "endOffset": 109}, {"referenceID": 15, "context": "Contrary to other work on computer poker (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013), we do not engineer any higher-level features.", "startOffset": 41, "endOffset": 109}, {"referenceID": 26, "context": "Other common reinforcement learning methods have been shown to exhibit similarly stagnating performance in poker games (Ponsen et al., 2011; Heinrich & Silver, 2015).", "startOffset": 119, "endOffset": 165}, {"referenceID": 24, "context": "Since in 2008 a computer program beat expert human LHE players for the first time in a public competition, modern computer agents are widely considered to have achieved super-human performance (Newall, 2013).", "startOffset": 193, "endOffset": 207}, {"referenceID": 2, "context": "The game was essentially solved by Bowling et al. (2015). We evaluated our agents against SmooCT, a Smooth UCT (Heinrich & Silver, 2015) agent which achieved 3 silver medals in the Annual Computer Poker Competition", "startOffset": 35, "endOffset": 57}, {"referenceID": 25, "context": "MSL was updated with exponentiallyaveraged reservoir sampling (Osborne et al., 2014), replacing entries in MSL with minimum probability 0.", "startOffset": 62, "endOffset": 84}, {"referenceID": 42, "context": "In computer poker, current game-theoretic approaches use heuristics of card strength to abstract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013).", "startOffset": 126, "endOffset": 194}, {"referenceID": 12, "context": "In computer poker, current game-theoretic approaches use heuristics of card strength to abstract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013).", "startOffset": 126, "endOffset": 194}, {"referenceID": 15, "context": "In computer poker, current game-theoretic approaches use heuristics of card strength to abstract the game to a tractable size (Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al., 2013).", "startOffset": 126, "endOffset": 194}, {"referenceID": 18, "context": "In computer Go, Maddison et al. (2015) and Clark & Storkey (2015) trained deep neural networks from data of expert human play.", "startOffset": 16, "endOffset": 39}, {"referenceID": 18, "context": "In computer Go, Maddison et al. (2015) and Clark & Storkey (2015) trained deep neural networks from data of expert human play.", "startOffset": 16, "endOffset": 66}, {"referenceID": 12, "context": ", 2007; Gilpin et al., 2007; Johanson et al., 2013). Waugh et al. (2015) recently combined one of these methods with function approximation.", "startOffset": 8, "endOffset": 73}, {"referenceID": 26, "context": "However, common simulationbased local search algorithms have been shown to diverge when applied to imperfect-information poker games (Ponsen et al., 2011; Heinrich & Silver, 2015).", "startOffset": 133, "endOffset": 179}, {"referenceID": 5, "context": "Furthermore, even game-theoretic methods do not generally achieve unexploitable behaviour when planning locally in imperfectinformation games (Burch et al., 2014; Ganzfried & Sandholm, 2015; Lis\u00fd et al., 2015).", "startOffset": 142, "endOffset": 209}, {"referenceID": 5, "context": "Furthermore, even game-theoretic methods do not generally achieve unexploitable behaviour when planning locally in imperfectinformation games (Burch et al., 2014; Ganzfried & Sandholm, 2015; Lis\u00fd et al., 2015). Another problem of local search is the potentially prohibitive cost at runtime if no prior knowledge is injected to guide the search. This poses the question of how to obtain this prior knowledge. Silver et al. (2016) trained convolutional neural networks on human expert data and then used a self-play reinforcement", "startOffset": 143, "endOffset": 429}, {"referenceID": 41, "context": "Yakovenko et al. (2016) trained deep neural networks in self-play in computer poker, including two poker games that are popular with humans.", "startOffset": 0, "endOffset": 24}, {"referenceID": 18, "context": "more, recent developments in continuous-action reinforcement learning (Lillicrap et al., 2015) could enable NFSP to be applied to continuous-action games, which current game-theoretic methods cannot deal with directly.", "startOffset": 70, "endOffset": 94}], "year": 2016, "abstractText": "Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable endto-end approach to learning approximate Nash equilibria without any prior knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Hold\u2019em, a poker game of realworld scale, NFSP learnt a competitive strategy that approached the performance of human experts and state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}