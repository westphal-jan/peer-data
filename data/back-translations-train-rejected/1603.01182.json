{"id": "1603.01182", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Network Unfolding Map by Edge Dynamics Modeling", "abstract": "The emergence of collective dynamics in neural networks is a mechanism of the animal and human brain for information processing. In this paper, we develop a computational technique of distributed processing elements, which are called particles. We observe the collective dynamics of particles in a complex network for transductive inference on semi-supervised learning problems. Three actions govern the particles' dynamics: walking, absorption, and generation. Labeled vertices generate new particles that compete against rival particles for edge domination. Active particles randomly walk in the network until they are absorbed by either a rival vertex or an edge currently dominated by rival particles. The result from the model simulation consists of sets of edges sorted by the label dominance. Each set tends to form a connected subnetwork to represent a data class. Although the intrinsic dynamics of the model is a stochastic one, we prove there exists a deterministic version with largely reduced computational complexity; specifically, with subquadratic growth. Furthermore, the edge domination process corresponds to an unfolding map. Intuitively, edges \"stretch\" and \"shrink\" according to edge dynamics. Consequently, such effect summarizes the relevant relationships between vertices and uncovered data classes. The proposed model captures important details of connectivity patterns over the edge dynamics evolution, which contrasts with previous approaches focused on vertex dynamics. Computer simulations reveal that our model can identify nonlinear features in both real and artificial data, including boundaries between distinct classes and the overlapping structure of data.", "histories": [["v1", "Thu, 3 Mar 2016 17:11:23 GMT  (1280kb,D)", "http://arxiv.org/abs/1603.01182v1", "14 pages, 7 figures, 1 appendix, submitted to IEEE Transactions on Neural Networks and Learning Systems"]], "COMMENTS": "14 pages, 7 figures, 1 appendix, submitted to IEEE Transactions on Neural Networks and Learning Systems", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["filipe alves neto verri", "paulo roberto urio", "liang zhao"], "accepted": false, "id": "1603.01182"}, "pdf": {"name": "1603.01182.pdf", "metadata": {"source": "CRF", "title": "Network Unfolding Map by Edge Dynamics Modeling", "authors": ["Filipe Alves Neto Verri", "Paulo Roberto Urio", "Liang Zhao"], "emails": ["filipeneto@usp.br.", "urio@usp.br.", "zhao@usp.br."], "sections": [{"heading": null, "text": "This year it has come to the point that it has never come as far as this year."}], "references": [{"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": "Secaucus, NJ: Springer,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Artificial Intelligence: A Modern Approach, 3rd ed", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien", "Eds"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Introduction to Semi-Supervised Learning", "author": ["X. Zhu", "A.B. Goldberg"], "venue": "Morgan and Claypool Publishers,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Text classification from labeled and unlabeled documents using EM", "author": ["K. Nigam", "A.K. McCallum", "S. Thrun", "T. Mitchell"], "venue": "Machine Learning, vol. 39, no. 2-3, pp. 103\u2013134, 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Semi-Supervised Nearest Mean Classification Through a Constrained Log-Likelihood", "author": ["M. Loog", "A.C. Jensen"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 5, pp. 995\u20131006, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Constrained kmeans clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schroedl"], "venue": "Proc. of the 18th International Conference on Machine Learning, 2001, pp. 577\u2013584.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Tri-training: exploiting unlabeled data using three classifiers", "author": ["Z.-H. Zhou", "M. Li"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 17, no. 11, pp. 1529\u20131541, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Statitical learning theory", "author": ["V.N. Vapnik"], "venue": "New York, NY: Wiley,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Semi-supervised learning guided by the modularity measure in complex networks", "author": ["T.C. Silva", "L. Zhao"], "venue": "Neurocomputing, vol. 78, no. 1, pp. 30\u201337, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Semi-supervised Domain Adaptation on Manifolds", "author": ["L. Cheng", "S.J. Pan"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 12, pp. 2240\u20132249, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Scaling Up Graph-Based Semisupervised Learning via Prototype Vector Machines", "author": ["K. Zhang", "L. Lan", "J.T. Kwok", "S. Vucetic", "B. Parvin"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 26, no. 3, pp. 444\u2013457, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2399\u20132434, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Dynamical processes on complex networks", "author": ["A. Barrat", "M. Barth\u00e9lemy", "A. Vespignani"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Some new directions in graph-based semi-supervised learning", "author": ["X. Zhu", "A.B. Goldberg", "T. Khot"], "venue": "Proc. of the EEE International Conference on Multimedia and Expo, pp. 1504\u20131507, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Particle competition for complex network community detection", "author": ["M.G. Quiles", "L. Zhao", "R.L. Alonso", "R.A.F. Romero"], "venue": "Chaos, vol. 18, no. 3, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Network-based stochastic semisupervised learning", "author": ["T.C. Silva", "L. Zhao"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 3, pp. 451\u201366, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Network-based stochastic semisupervised learning", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 3, pp. 451\u2013466, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Detecting and preventing error propagation via competitive learning", "author": ["\u2014\u2014"], "venue": "Neural Networks, vol. 41, pp. 70\u201384, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Sur les fonctions convexes et les in\u00e9galit\u00e9s entre les valeurs moyennes", "author": ["J. Jensen"], "venue": "Acta Mathematica, vol. 30, no. 1, pp. 175\u2013193, 1906.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1906}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "Commun. ACM, vol. 18, no. 9, pp. 509\u2013517, 1975.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1975}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems 16, S. Thrun, L. Saul, and B. Sch\u00f6lkopf, Eds. MIT Press, 2004, pp. 321\u2013328.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Large Scale Transductive SVMs", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou"], "venue": "Journal of Machine Learning Research, vol. 7, no. 7, pp. 1687\u20131712, 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Dynamic label propagation for semi-supervised multi-class multi-label classification", "author": ["B. Wang", "Z. Tu", "J.K. Tsotsos"], "venue": "Proc. of the IEEE International Conference on Computer Vision, pp. 425\u2013432, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["X. Zhu", "Z. Ghahramani"], "venue": "School Comput Sci Carnegie Mellon Univ Pittsburgh PA Tech Rep, vol. 54, no. CMU-CALD-02-107, pp. 1\u201319, 2002.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "MTC: A Fast and Robust Graph-Based Transductive Learning Method", "author": ["Y.M. Zhang", "K. Huang", "G.G. Geng", "C.L. Liu"], "venue": "IEEE Trans Neural Netw Learn Syst, vol. 26, no. 9, pp. 1979\u20131991, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1979}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": "Science, vol. 290, no. 5500, p. 2319, 2000.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2000}, {"title": "PR-Tools4.1, a matlab toolbox for pattern recognition", "author": ["R. Duin", "P. Juszczak", "P. Pacl\u0131\u0301k", "E. Pekalska", "D. de Ridder", "D. Tax", "S. Verzakov"], "venue": "2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "A Public Domain Dataset for Human Activity Recognition Using Smartphones", "author": ["D. Anguita", "A. Ghio", "L. Oneto", "X. Parra", "J.L. Reyes-Ortiz"], "venue": "21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013, 2013.  PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS  14", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Conversely, unsupervised learning techniques try to discover the intrinsic structure of a completely unlabeled data set [1], [2].", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "Conversely, unsupervised learning techniques try to discover the intrinsic structure of a completely unlabeled data set [1], [2].", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "vised learning paradigms where both unlabeled and labeled data are taken into account in class or cluster formation and prediction process [3], [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 3, "context": "vised learning paradigms where both unlabeled and labeled data are taken into account in class or cluster formation and prediction process [3], [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 4, "context": "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]\u2013[12].", "startOffset": 101, "endOffset": 104}, {"referenceID": 5, "context": "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]\u2013[12].", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]\u2013[12].", "startOffset": 168, "endOffset": 171}, {"referenceID": 7, "context": "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]\u2013[12].", "startOffset": 187, "endOffset": 190}, {"referenceID": 8, "context": "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]\u2013[12].", "startOffset": 222, "endOffset": 225}, {"referenceID": 9, "context": "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]\u2013[12].", "startOffset": 250, "endOffset": 254}, {"referenceID": 11, "context": "Up to now, many semi-supervised learning techniques have been developed, including generative models [5], discriminative models [6], clustering and labeling techniques [7], multitraining [8], low-density separation models [9], and graphbased methods [10]\u2013[12].", "startOffset": 255, "endOffset": 259}, {"referenceID": 12, "context": "The labels are propagated to the whole graph using a particular optimization heuristic [13].", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "Complex networks are large-scale graphs with nontrivial topology [14].", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "Such networks introduce a powerful tool to describe the interplay of topology, structure, and dynamics of complex systems [14], [15].", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "Moreover, traditional graph-based techniques have high computational complexity, usually at cubic order [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "A common strategy to overcome this disadvantage is using a set of sparse prototypes derived from the data [12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "The particle competition model was originally proposed in [17] and extended for the data clustering task in [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 16, "context": "The particle competition model was originally proposed in [17] and extended for the data clustering task in [18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "Later, it has been applied to semisupervised learning [19], [20] where the particle competition is formally represented by a nonlinear stochastic dynamical system.", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": "Later, it has been applied to semisupervised learning [19], [20] where the particle competition is formally represented by a nonlinear stochastic dynamical system.", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "If the destination vertex is not a sink, its survival probability is 1\u2212 \u03bb\u03c3\u0303 ij(t) where \u03bb \u2208 [0, 1] is the competition parameter.", "startOffset": 92, "endOffset": 98}, {"referenceID": 19, "context": "Thus, with the Jensen\u2019s inequality [21], we have E [ \u03c3\u0303 ij(t) ] \u2265 1\u2212 E [ \u00f1ij(t) ] + E [ \u00f1ji(t) ] \u2211C q=1 E [ \u00f1qij(t) ] + E [ \u00f1qji(t) ] .", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "The k-NN method, for example, has complexity order of O(D |V| log |V|) using multidimensional binary search tree [22].", "startOffset": 113, "endOffset": 117}, {"referenceID": 8, "context": "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).", "startOffset": 182, "endOffset": 185}, {"referenceID": 21, "context": "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).", "startOffset": 221, "endOffset": 225}, {"referenceID": 22, "context": "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).", "startOffset": 259, "endOffset": 263}, {"referenceID": 23, "context": "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).", "startOffset": 296, "endOffset": 300}, {"referenceID": 24, "context": "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).", "startOffset": 323, "endOffset": 327}, {"referenceID": 16, "context": "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).", "startOffset": 350, "endOffset": 354}, {"referenceID": 25, "context": "Hence, this step has complexity order of TABLE I TIME COMPLEXITY OF COMMON GRAPH-BASED TECHNIQUES DISREGARDING THE GRAPH CONSTRUCTION STEP Algorithm Time Complexity Transductive SVM [9] C |V| Local and Global Consistency [23] |V| Large Scale Transductive SVM [24] C |V| Dynamic Label Propagation [25] |V| Label Propagation [26] |V| Vertex Domination [18] C2 |V|+ C |E| Edge Domination C |V|+ C |E| Minimum Tree Cut [27] |V| O(C |E|).", "startOffset": 415, "endOffset": 419}, {"referenceID": 25, "context": "Only the proposed Edge Domination and Minimum Tree Cut [27] have linear time, though the latter must either receive or construct a spanning tree.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "The networks used for the analysis were generated by the following model: A complex network is constructed given a labeled vector y, a number m > 0 of edges by vertex, and a weight p \u2208 [0, 1] that controls the preferential attachment between vertices of different classes.", "startOffset": 185, "endOffset": 191}, {"referenceID": 26, "context": "In our experiments, we use k-NN graph with Euclidean distance since it was proved to approximate the low-dimensional manifold of a set of points [28].", "startOffset": 145, "endOffset": 149}, {"referenceID": 27, "context": "(The datasets were generated using the PRTools framework [29].", "startOffset": 57, "endOffset": 61}, {"referenceID": 2, "context": "Simulations on Benchmark Datasets We compare our model with 14 semi-supervised techniques tested on Chapelle\u2019s benchmark [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "The datasets are described in [3].", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "), Transductive support vector machines (TSVM), Cluster kernels (ClusterKernel), Low-density separation (LDS), Laplacian regularized least squares (Laplacian RLS), Local and global consistency (LGC), Label propagation (LP), Linear neighborhood propagation (LNP), and Network-Based Stochastic Semisupervised Learning (Vertex Domination), The simulation results were collected from [3], except for LGC, LP, LNP, and Vertex Domination that are found in [18].", "startOffset": 380, "endOffset": 383}, {"referenceID": 16, "context": "), Transductive support vector machines (TSVM), Cluster kernels (ClusterKernel), Low-density separation (LDS), Laplacian regularized least squares (Laplacian RLS), Local and global consistency (LGC), Label propagation (LP), Linear neighborhood propagation (LNP), and Network-Based Stochastic Semisupervised Learning (Vertex Domination), The simulation results were collected from [3], except for LGC, LP, LNP, and Vertex Domination that are found in [18].", "startOffset": 450, "endOffset": 454}], "year": 2016, "abstractText": "Abstract\u2014The emergence of collective dynamics in neural networks is a mechanism of the animal and human brain for information processing. In this paper, we develop a computational technique of distributed processing elements, which are called particles. We observe the collective dynamics of particles in a complex network for transductive inference on semi-supervised learning problems. Three actions govern the particles\u2019 dynamics: walking, absorption, and generation. Labeled vertices generate new particles that compete against rival particles for edge domination. Active particles randomly walk in the network until they are absorbed by either a rival vertex or an edge currently dominated by rival particles. The result from the model simulation consists of sets of edges sorted by the label dominance. Each set tends to form a connected subnetwork to represent a data class. Although the intrinsic dynamics of the model is a stochastic one, we prove there exists a deterministic version with largely reduced computational complexity; specifically, with subquadratic growth. Furthermore, the edge domination process corresponds to an unfolding map. Intuitively, edges \u201cstretch\u201d and \u201cshrink\u201d according to edge dynamics. Consequently, such effect summarizes the relevant relationships between vertices and uncovered data classes. The proposed model captures important details of connectivity patterns over the edge dynamics evolution, which contrasts with previous approaches focused on vertex dynamics. Computer simulations reveal that our model can identify nonlinear features in both real and artificial data, including boundaries between distinct classes and the overlapping structure of data.", "creator": "LaTeX with hyperref package"}}}