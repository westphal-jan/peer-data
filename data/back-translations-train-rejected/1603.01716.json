{"id": "1603.01716", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2016", "title": "Classifier ensemble creation via false labelling", "abstract": "In this paper, a novel approach to classifier ensemble creation is presented. While other ensemble creation techniques are based on careful selection of existing classifiers or preprocessing of the data, the presented approach automatically creates an optimal labelling for a number of classifiers, which are then assigned to the original data instances and fed to classifiers. The approach has been evaluated on high-dimensional biomedical datasets. The results show that the approach outperformed individual approaches in all cases.", "histories": [["v1", "Sat, 5 Mar 2016 12:01:00 GMT  (2526kb,D)", "http://arxiv.org/abs/1603.01716v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["b\\'alint antal"], "accepted": false, "id": "1603.01716"}, "pdf": {"name": "1603.01716.pdf", "metadata": {"source": "CRF", "title": "Classifier ensemble creation via false labelling", "authors": ["B\u00e1lint Antal"], "emails": ["antal.balint@inf.unideb.hu."], "sections": [{"heading": null, "text": "While other ensemble building techniques rely on careful selection of existing classifiers or pre-processing of the data, the presented approach automatically generates optimal labeling for a number of classifiers, which are then assigned to the original data instances and fed to classifiers. The approach was evaluated using high-dimensional biomedical datasets and the results show that in all cases the approach outperformed the individual approaches. Keywords: ensemble learning, diversity, hidden Markov random fields, simulated annealing, bioinformatics."}, {"heading": "1. Introduction", "text": "There is an enormous amount of research to improve classification in such cases. A highly studied approach to this problem is the multiple prediction approach, which provides for a more efficient classification. [3] A fundamental condition for the creation of classifications is the diversification of individual countries. That is, the classification is able to complete the individual countries."}, {"heading": "2. Ensemble creation via false labelling", "text": "This section shows the mathematical background behind the algorithm. In addition, an optimization problem is defined to provide an efficient solution to the wrong labeling problem. Then, a function designation can be called a classifier. (1) Then, a vector. (1) A vector. (2) A vector. (2) A vector. (2)................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "2.1. Ensemble creation", "text": "The proposed formation of an ensemble depends on the output of a classifier Dorig for a given training data set T. First, we divide T into two equal parts T (0) and T (1) at random. We train Dorig on T (1) and classify all ~ \u03c71j-T (0), j = 1,.., k / 2 elements of T (1): C1orig = {\u03c9j | \u03c9j = Dorig (~ \u03c71j), ~ \u03c71j-T 1,.., j = 1,., k / 2}. (4) Then we create a majority voting classifier ensemble of L members: Dmaj = {D1 = Dorig, D2,..., DL}. (5) To train D2,..., DL, we define an incorrect labelling function F: \u043bl\u00fck / 2 \u2192 lt.k / 2 \u00b7 (L \u2212 1). This is F (Corig) {C2j, Ci, CL2b, CL3b."}, {"heading": "2.2. Selection of the false labelling function", "text": "To define an optimal false labeling function F (see Eq.6), we give the decomposition of the majority selection error described in [14]. The majority selection error can be divided into three terms: the individual error of the classifiers, the disagreement of the classifiers if they correctly classified the input (\"good diversity\"), and the disagreement of the classifiers if they misclassified the input (\"bad diversity\"). The decomposition of the majority selection error is the basis for defining the energy function of our method. Let y (~) be the true class name for the feature vector. Then the zero-one loss for dt (~) is defined as follows [14]: et (~) = 12 (~) = 1 \u2212 y (~) dt (~))))) (8) Then the average individual zero-one-one loss for the feature vector."}, {"heading": "3. Optimization via Hidden Markov Random Fields", "text": "To solve the optimization problem, an approach based on HMRF (HMRF) is possible. (HMRF). (HMRF). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR. (HR). (HR). (HR. (HR). (HR). (HR). (HR. (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR. (HR). (HR). (HR). (HR). (HR. (HR). (HR). (HR). (HR). (HR. (HR). (HR. (HR). (HR). (HR. (HR). (HR). (HR. (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR). (HR. (HR). (HR). ("}, {"heading": "4. Methodology", "text": "The proposed approach was evaluated on high-dimensional biomedical datasets containing LB LB expressions or proteomics data downloaded from the Keng Ridge repository [18]. The description of the datasets including the number of instances, the number of characteristics per instance and the status of the patient by disease is summarized in Table 1. As can be seen, the datasets contain a large number of characteristics for a small number of instances, which results in a fair comparison. Thus, the datasets for training the number of instances per class are similar to those for a better comparison of methods. The datasets were divided into two equal partitions 10 times to have a fair comparison. The incorrect identification ensembles are divided with 3, 5, 7, 9, 11, 13, 15 members with Naive Bayes [19] Base srf\u00fc the basic classifiers for each problem."}, {"heading": "5. Results and discussion", "text": "The validity of the optimization technique can be seen in Figures 2 (a) and 2 (b). As can be seen in this example, the accuracy of the ensemble has steadily increased by converting to an accuracy of 1, while the correlation of the labels of the ensemble members has been reduced at the same time. Figure 2 (c) shows the optimization time through iterations. As can be seen, the optimization process increases energy efficiency with fewer changes in the labeling, while most of the combinations need to be tested to increase the energy that requires more time. Mean accuracy and their standard deviations for the ensembles can be found in Table 2. Each column contains the accuracy of the respective ensemble used in the respective ensembles."}, {"heading": "6. Conclusion", "text": "In this context, it should be noted that the objectives that have been achieved in recent years are not only objectives, but also objectives that cannot be achieved."}, {"heading": "Acknowledgments", "text": "The publication was supported by the TA \u0301 MOP-4.2.2.C-11 / 1 / KONV2012-0001 project, co-financed by the European Union."}], "references": [{"title": "An ensemble-based system for microaneurysm detection and diabetic retinopathy grading", "author": ["B. Antal", "A. Hajdu"], "venue": "IEEE Transactions on Biomedical Engineering 59 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "An ensemble-based system for automatic screening of diabetic retinopathy", "author": ["B. Antal", "A. Hajdu"], "venue": "Knowledge-Based Systems 60 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Combining Pattern Classifiers", "author": ["L.I. Kuncheva"], "venue": "Methods and Algorithms, Wiley", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Diversity creation methods: a survey and categorisation", "author": ["G. Brown", "J. Wyatt", "R. Harris", "X. Yao"], "venue": "Information Fusion 6 (1) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning 24 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Nonparametric estimates of standard error: The jackknife", "author": ["B. Efron"], "venue": "the bootstrap and other methods, Biometrika 68 68 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1981}, {"title": "Attribute bagging: improving accuracy of classifier ensembles by using random feature subsets", "author": ["R. Bryll"], "venue": "Pattern Recognition 20 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "The random subspace method for constructing decision forests", "author": ["T. Ho"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 20 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "R", "author": ["Y. Freund"], "venue": "E. Schapire, A decision-theoretic generalization of on-line learning and an application to boosting ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "The boosting approach to machine learning: An overview", "author": ["R.E. Schapire"], "venue": "in: MSRI (Mathematical Sciences Research Institute) Workshop on Nonlinear Estimation and Classification", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Classifier selection for majority voting", "author": ["D. Ruta", "B. Gabrys"], "venue": "Information Fusion 6 (1) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Ensemble based systems in decision making", "author": ["R. Polikar"], "venue": "IEEE Circuits and Systems magazine Third Quarter ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy", "author": ["L. Kuncheva", "C. Whitaker"], "venue": "Machine Learning 29  51 (2) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "GOOD and BAD diversity in majority vote ensembles", "author": ["G. Brown", "L.I. Kuncheva"], "venue": "in: Proc. 9th International Workshop on Multiple Classifier Systems (MCS\u201910), Vol. LNCS 5997 of LNCS, Springer-Verlag, Cairo, Egypt", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian image classification using markov random fields", "author": ["M. Berthod", "Z. Kato", "S. Yu", "J. Zerubia"], "venue": "Image and Vision Computing 14 (4) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1016}, {"title": "P", "author": ["J.M. Hammersley"], "venue": "Clifford, Markov field on finite graphs and lattices ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1971}, {"title": "Optimization by simulated annealing", "author": ["S. Kirkpatrick", "C.D. Gelatt", "M.P. Vecchi"], "venue": "Science 220 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1983}, {"title": "Mean-entropy discretized features are effective for classifying high-dimensional biomedical data", "author": ["J. Li", "H. Liu", "L. Wong"], "venue": "in: Proceedings of the 3rd ACM SIGKDD Workshop on Data Mining in Bioinformatics", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Estimating continuous distributions in bayesian classifiers", "author": ["P.L. George H. John"], "venue": "in: Eleventh Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1995}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["I.H. Witten", "E. Frank"], "venue": "2nd Edition, Morgan Kaufmann, San Francisco", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "The use of ranks to avoid the assumption of normality implicit in the analysis of variance", "author": ["M. Friedman"], "venue": "Journal of the American Statistical Association 32 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1937}, {"title": "Comparing individual means in the analysis of variance", "author": ["J.W. Tukey"], "venue": "Biometrics", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1949}], "referenceMentions": [{"referenceID": 0, "context": "In numerous application fields very complex data needs to be classified which is often a difficult task for a single machine learning classifier [1] [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "In numerous application fields very complex data needs to be classified which is often a difficult task for a single machine learning classifier [1] [2].", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "One highly investigated field for this problem is ensemble learning [3], where multiple prediction are fused the produce a more efficient classifica-", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "One fundamental requirement for the creation of classifier ensembles is diversity among them [4], that is, the classifiers included in the ensemble need to complement each other to provide more generalization capabilities than a single learner.", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "Bagging [5] uses randomly selected training subsets with possible overlap (bootstrapping [6]) to ensure diversity among the member of the ensemble.", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "Bagging [5] uses randomly selected training subsets with possible overlap (bootstrapping [6]) to ensure diversity among the member of the ensemble.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "Other diversity creation techniques may involve disjoint random sampling (random subspace methods [7], for example, some variants of Random Forest algorithms [8]), while Adaboost [9] based techniques aims to increase the accuracy of a weak learner iteratively (boosting [10]) using targeted sampling: each iteration considers the misclassified instances of the training data to be more important, and drives the iteration process to include them in the current training set.", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "Other diversity creation techniques may involve disjoint random sampling (random subspace methods [7], for example, some variants of Random Forest algorithms [8]), while Adaboost [9] based techniques aims to increase the accuracy of a weak learner iteratively (boosting [10]) using targeted sampling: each iteration considers the misclassified instances of the training data to be more important, and drives the iteration process to include them in the current training set.", "startOffset": 158, "endOffset": 161}, {"referenceID": 8, "context": "Other diversity creation techniques may involve disjoint random sampling (random subspace methods [7], for example, some variants of Random Forest algorithms [8]), while Adaboost [9] based techniques aims to increase the accuracy of a weak learner iteratively (boosting [10]) using targeted sampling: each iteration considers the misclassified instances of the training data to be more important, and drives the iteration process to include them in the current training set.", "startOffset": 179, "endOffset": 182}, {"referenceID": 9, "context": "Other diversity creation techniques may involve disjoint random sampling (random subspace methods [7], for example, some variants of Random Forest algorithms [8]), while Adaboost [9] based techniques aims to increase the accuracy of a weak learner iteratively (boosting [10]) using targeted sampling: each iteration considers the misclassified instances of the training data to be more important, and drives the iteration process to include them in the current training set.", "startOffset": 270, "endOffset": 274}, {"referenceID": 10, "context": "Another approach to create diverse ensembles is ensemble selection [11], where diversity of classifiers trained on the same dataset is measured and an optimal subset is selected.", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "A more comprehensive review on the above described techniques can be found in [12].", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "Although the definite connection between diversity measures and ensemble accuracy is an open question [13], a decomposition of majority voting error into good and bad diversity is proposed in [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Although the definite connection between diversity measures and ensemble accuracy is an open question [13], a decomposition of majority voting error into good and bad diversity is proposed in [14].", "startOffset": 192, "endOffset": 196}, {"referenceID": 2, "context": "For the basic machine learning and ensemble definitions, we relied on the classic literature [3] and [14].", "startOffset": 93, "endOffset": 96}, {"referenceID": 13, "context": "For the basic machine learning and ensemble definitions, we relied on the classic literature [3] and [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 13, "context": "Selection of the false labelling function To define an optimal false labelling function F (see equation 6), we recite the decomposition of the majority voting error described in [14].", "startOffset": 178, "endOffset": 182}, {"referenceID": 13, "context": "Then, the zero-one loss for dt (~ \u03c7) is defined as follows [14]: et (~ \u03c7) = 1 2 (1\u2212 y (~ \u03c7) dt (~ \u03c7)) (8) Then, the average individual zero-one loss is [14] eind (~ \u03c7) = 1 L L \u2211", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "Then, the zero-one loss for dt (~ \u03c7) is defined as follows [14]: et (~ \u03c7) = 1 2 (1\u2212 y (~ \u03c7) dt (~ \u03c7)) (8) Then, the average individual zero-one loss is [14] eind (~ \u03c7) = 1 L L \u2211", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": "t=1 et (~ \u03c7) (9) and the ensemble zero-one loss is: emaj (~ \u03c7) = 1 2 (1\u2212 y (~ \u03c7) dmaj (~ \u03c7)) (10) The disagreement between dt and the ensemble is the following [14]: \u03b4t (~ \u03c7) = 1 2 (1\u2212 dt (~ \u03c7) dmaj (~ \u03c7)) .", "startOffset": 160, "endOffset": 164}, {"referenceID": 13, "context": "(11) The classification error of an ensemble is defined [14] as follows: Emaj = \u222b", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "In this section, we briefly summarize the basis for Hidden Markov Random Field (HMRF) optimization based on [15].", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "The optimal labelling for the A variables with the HMRF optimization, one can use the the Hammersley-Clifford Theorem [16] to calculate the global energy for a labelling by summarizing the local energies for each variable.", "startOffset": 118, "endOffset": 122}, {"referenceID": 13, "context": "Thus, we maximize the disagreement for bad diversity and minimize to good diversity [14].", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "Since simulated annealing [17], an efficient algorithm for finding approximate global solutions for large state-spaces.", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": "Methodology The proposed approach has been evaluated on high-dimensional biomedical datasets containing gene expressions or proteomics data downloaded from the the Keng Ridge repository [18].", "startOffset": 186, "endOffset": 190}, {"referenceID": 18, "context": "The false-labelling ensembles are tested with 3, 5, 7, 9, 11, 13, 15 members with Naive Bayes [19] base classifiers for each problem.", "startOffset": 94, "endOffset": 98}, {"referenceID": 19, "context": "The implementation of the classifiers was done using Weka [20].", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "First, Friedmantest [21] was performed to check whether the results of the proposed ensemble based classifiers, the Naive Bayes, Adaboost, Bagging and Random Forest are from the same distribution.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "To recognize these differences, Tukey\u2019s multiple comparison test [22] is also performed.", "startOffset": 65, "endOffset": 69}], "year": 2016, "abstractText": "In this paper, a novel approach to classifier ensemble creation is presented. While other ensemble creation techniques are based on careful selection of existing classifiers or preprocessing of the data, the presented approach automatically creates an optimal labelling for a number of classifiers, which are then assigned to the original data instances and fed to classifiers. The approach has been evaluated on high-dimensional biomedical datasets. The results show that the approach outperformed individual approaches in all cases.", "creator": "LaTeX with hyperref package"}}}