{"id": "1603.01870", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2016", "title": "Personalized Advertisement Recommendation: A Ranking Approach to Address the Ubiquitous Click Sparsity Problem", "abstract": "We study the problem of personalized advertisement recommendation (PAR), which consist of a user visiting a system (website) and the system displaying one of $K$ ads to the user. The system uses an internal ad recommendation policy to map the user's profile (context) to one of the ads. The user either clicks or ignores the ad and correspondingly, the system updates its recommendation policy. PAR problem is usually tackled by scalable \\emph{contextual bandit} algorithms, where the policies are generally based on classifiers. A practical problem in PAR is extreme click sparsity, due to very few users actually clicking on ads. We systematically study the drawback of using contextual bandit algorithms based on classifier-based policies, in face of extreme click sparsity. We then suggest an alternate policy, based on rankers, learnt by optimizing the Area Under the Curve (AUC) ranking loss, which can significantly alleviate the problem of click sparsity. We conduct extensive experiments on public datasets, as well as three industry proprietary datasets, to illustrate the improvement in click-through-rate (CTR) obtained by using the ranker-based policy over classifier-based policies.", "histories": [["v1", "Sun, 6 Mar 2016 20:26:41 GMT  (240kb,D)", "http://arxiv.org/abs/1603.01870v1", "Under review"]], "COMMENTS": "Under review", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["sougata chaudhuri", "georgios theocharous", "mohammad ghavamzadeh"], "accepted": false, "id": "1603.01870"}, "pdf": {"name": "1603.01870.pdf", "metadata": {"source": "CRF", "title": "Personalized Advertisement Recommendation: A Ranking Approach to Address the Ubiquitous Click Sparsity Problem", "authors": ["Sougata Chaudhuri", "Ann Arbor", "Georgios Theocharous", "Mohammad Ghavamzadeh"], "emails": [], "sections": [{"heading": "Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "Contextual Bandit (CB) Approach to PAR", "text": "The main contextual bandit algorithms can largely be divided into two classes: those that make specific parametric assumptions about the reward process, and those that simply assume that context and rewards are generated by some distribution. Both algorithms assume that the reward of each individual work (arm) is a continuous linear function of some unknown parameters, which is not an appropriate premise for click-based binary reward in PAR. In addition, both algorithms assume that there is context information available for each ad, while we assume the availability of user-only context in our setting. So, from now on, we focus on the second class of contextual bandit algorithms. We provide a formal description of the framework of contextual bandits that adapt to the PAR setting, and then discuss the problem that arises from clicking."}, {"heading": "Binary Classifier Based Policies", "text": "In fact, the fact is that most of them are able to move without moving in the direction in which they are moving."}, {"heading": "Cost Sensitive Multi-Class Classifier Based Policies", "text": "Another type of political space consists of cost-sensitive multi-class classifiers [Langford and Zhang, 2008; Dudik et al., 2011; Agarwal et al., 2014]. They can be cost-sensitive multi-class classifiers [Cao, Zhao and Zaiane, 2013], multi-class logisticians or filter trees [Beygelzimer, Langford and Ravikumar, 2007]. Clicking thrift presents slightly different problems in the practical design of a policy space of such classifiers. Cost-sensitive multi-class classifiers work as follows: Suppose that a context-reward-vector pair (x, r) is generated as described in the PAR setting. The classifier will try to select a class (display) so that the reward ra is maximum good among all decisions made by ra \u2032, and the reward class of ra \u2032 (K) is very bad (we consider reward maximization of classifiers, rather than minimizing costs)."}, {"heading": "AUC Optimized Ranker", "text": "rE \"s rf\u00fc ide rf\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die"}, {"heading": "Constructing Policy from Rankers", "text": "Similar to learning one classifier per ad, a separate ranking function fwa (\u00b7) is learned for each ad from the data of the bandit stack a. Subsequently, the following technique is used to convert the individual K-ranking functions into a recommendation policy. First, a threshold sa is learned separately for each action (see details below), and then for a new user x, the combined policy \u03c0 works as follows: \u03c0 (x) = argmax a [K] (fwa (x) \u2212 sa). (2) Thus \u03c0 x maps to show a maximum \"normalized score.\" This normalization negates the inherent scoring bias that might exist for each ranking function. That is, a ranking function for an action a [K] could learn to rate all instances (both positive and negative) higher than a ranking function for an action b [K]. Therefore, the ranking function for a new instance always gives a higher score than the possible ranking function for a misscore, resulting in a missing ranking function."}, {"heading": "Competing Policies and Evaluation Techniques", "text": "To support our hypothesis that rank-based policies solve the problem of click sparseness better than classifying policies, we conducted two types of experiments: We compared deterministic policies (\"exploitation\" only) with complete information datasets (classification) and compared stochastic policies (\"exploitation + exploration\") with bandit datasets using a specific offline evaluation technique. Both of our experiments were designed for setting batch learning processes, with policies constructed from separate classifiers / markers per ad. The classifiers considered were linear and composite randomForest classifiers and markers considered as AUC optimized markers. Deterministic policies: Policies constructed from the trained classifiers were constructed using the \"one-vs-all\" technique, i.e. for a new user, the display was calculated using the maximum number of points and markers."}, {"heading": "Evaluation on Full Information Classification Data", "text": "Therefore, we compared the deterministic guidelines using K-class classification benchmark data, which were converted into K-class bandit data using the technique in [Langford, Li and Dudik, 2011]. In short, the default conversion technique is as follows: A K-class data set is randomly divided into the Xtrain training set and the Xtest test test set (in our experiments we used a 70-30 split). Only the described training set is converted into bandit data according to the procedure. The new bandit instance is an instance and the corresponding class in the training set. A class a \u2032 [K] is randomly selected. If a = a \u2032, a reward of 1 x is assigned; otherwise, a reward of 0 is assigned."}, {"heading": "Evaluation on Bandit Information Data", "text": "Bandit data sets have both training and test sets in bandit form, and data sets that we use are industry-specific. Evaluation techniques: We have compared the stochastic strategies in bandit data sets. Comparing the strategies in bandit test sets comes with the following unique challenge: For a stochastic policy, the expected reward is p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p"}, {"heading": "Empirical Results", "text": "Linear Classifiers and Rankers: For each display a, a linear classifier was learned by optimizing the logistical replacement, while a linear Ranker was learned by optimizing the objective function (1), where \"(\u00b7) was the logistical replacement. Since we did not have the problem of sparse high-dimensional features in our datasets, we added a\" 2 regulator instead of a \"1 regulator.\" We applied SGD with 1 million iterations; varied the parameters of the \"2 regulator in quantity {0.01, 0.1, 1, 10} and recorded the best result. Ensemble Classifiers: We learned a RandomForest classifier for each display a. The RandomForests consisted of 200 trees, both for compositional feasibility and for the more theoretical reason outlined in [Koh and Gupta, 2014]."}, {"heading": "Comparison of Deterministic Policies", "text": "Data sets: Multi-class policy is detailed in Table 1. Evaluation: To compare deterministic strategies, we conducted two sets of tests; one without subsamples of negative classes during training (i.e. without class balance) and one with a strong subsample of negative classes (artificial class balance). Training and tests were repeated ten times for each set of data to take into account the randomness introduced in the conversion of classification data into bandit training data, and the average accuracy of the runs is reported. Figure 1 shows the performance of various strategies learned without and with subsample samples during training. Substints were conducted to achieve positive results: negative rates such as 1: 2 for each class (this essentially means that Avg% was positive). The 1: 2 ratio generally yielded the best results (observations: a) When heavily subsample sampled, the performance of classifying strategies improves significantly during training."}, {"heading": "Comparison of Stochastic Policies", "text": "Our next set of experiments were conducted on three different sets of data owned by a major technology company.Datasets: Two of the records were from campaigns by two major banks and another from campaignTable 1: All records were sourced from the UCI repository (https: / / archive.ics.uci.edu / ml /). Five different sets of data were selected. In the table, the size represents the number of examples in the complete datasets. Properties indicate the dimension of the instances. Avg.% positive indicates the number of positive instances per class divided by the total number of instances for that class, in bandit training, averaged across all classes. The lower the value, the more the imbalance per class is during training. OptDigits Isolet Letter PenDigits Movementlibras Size 5620 7797 20000 10992 360Features 64 617 16 classes."}], "references": [{"title": "Translating relevance scores to probabilities for contextual advertising", "author": ["Agarwal et al", "2009] Agarwal", "E. Gabrilovich", "R. Hall", "V. Josifovski", "R. Khanna"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "D. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "D. et al\\.", "year": 2009}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Agarwal et al", "2014] Agarwal", "D. Hsu", "S. Kale", "J. Langford", "L. Li", "R. Schapire"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "A. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "A. et al\\.", "year": 2014}, {"title": "and Goyal", "author": ["S. Agrawal"], "venue": "N.", "citeRegEx": "Agrawal and Goyal. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiclass classification with filter trees", "author": ["Langford Beygelzimer", "Ravikumar", "2007] Beygelzimer", "J. Langford", "P. Ravikumar"], "venue": null, "citeRegEx": "Beygelzimer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2007}, {"title": "S", "author": ["T. Calders", "Jaroszewicz"], "venue": "2007. Efficient AUC optimization for classification. In Knowledge Discovery in Databases: PKDD", "citeRegEx": "Calders and Jaroszewicz. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "An optimized cost-sensitive svm for imbalanced data learning", "author": ["Zhao Cao", "Zaiane", "2013] Cao", "D. Zhao", "O. Zaiane"], "venue": "In Advances in Knowledge Discovery and Data", "citeRegEx": "Cao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "and Li", "author": ["O. Chapelle"], "venue": "L.", "citeRegEx": "Chapelle and Li. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Editorial: special issue on learning from imbalanced data sets", "author": ["Japkowicz Chawla", "Kotcz", "2004] Chawla", "N. Japkowicz", "A. Kotcz"], "venue": "ACM Sigkdd Explorations Newsletter", "citeRegEx": "Chawla et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chawla et al\\.", "year": 2004}, {"title": "FSelector: a ruby gem for feature selection. Bioinformatics 28(21):2851\u20132852", "author": ["Wang Cheng", "Bryant", "2012] Cheng", "Y. Wang", "S. Bryant"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2012}, {"title": "Contextual bandits with linear payoff functions", "author": ["Chu et al", "2011] Chu", "L. Li", "L. Reyzin", "R. Schapire"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "W. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "W. et al\\.", "year": 2011}, {"title": "and Mohri", "author": ["C. Cortes"], "venue": "M.", "citeRegEx": "Cortes and Mohri. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "T", "author": ["M. Dudik", "D. Hsu", "S. Kale", "N. Karampatziakis", "J. Langford", "L. Reyzin", "Zhang"], "venue": "2011. Efficient optimal learning for contextual bandits. Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Dudik et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Practical lessons from predicting clicks on ads at facebook", "author": ["X He"], "venue": "[He and others,", "citeRegEx": "He,? \\Q2014\\E", "shortCiteRegEx": "He", "year": 2014}, {"title": "and Stephen", "author": ["N. Japkowicz"], "venue": "S.", "citeRegEx": "Japkowicz and Stephen. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "and Gupta", "author": ["E. Koh"], "venue": "N.", "citeRegEx": "Koh and Gupta. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Zhang", "author": ["J. Langford"], "venue": "T.", "citeRegEx": "Langford and Zhang. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Doubly robust policy evaluation and learning", "author": ["Li Langford", "Dudik", "2011] Langford", "L. Li", "M. Dudik"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Langford et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2011}, {"title": "Unbiased offline evaluation of contextual-banditbased news article recommendation algorithms", "author": ["Li et al", "2011] Li", "W. Chu", "J. Langford", "X. Wang"], "venue": "In Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "L. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "L. et al\\.", "year": 2011}, {"title": "Ad click prediction: a view from the trenches", "author": ["H McMahan"], "venue": "[McMahan and others,", "citeRegEx": "McMahan,? \\Q2013\\E", "shortCiteRegEx": "McMahan", "year": 2013}, {"title": "Predicting clicks: estimating the click-through rate for new ads", "author": ["Dominowska Richardson", "Ragno", "2007] Richardson", "E. Dominowska", "R. Ragno"], "venue": "In Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "Richardson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2007}, {"title": "and Klautau", "author": ["R. Rifkin"], "venue": "A.", "citeRegEx": "Rifkin and Klautau. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "and Zhang", "author": ["O. Shamir"], "venue": "T.", "citeRegEx": "Shamir and Zhang. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Ad recommendation systems for life-time value optimization", "author": ["Thomas Theocharous", "Ghavamzadeh", "2015] Theocharous", "P. Thomas", "M. Ghavamzadeh"], "venue": "In Proceedings of the 24th International Conference on World Wide Web Companion,", "citeRegEx": "Theocharous et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theocharous et al\\.", "year": 2015}, {"title": "High confidence off-policy evaluation", "author": ["Theocharous Thomas", "Ghavamzadeh", "2015] Thomas", "G. Theocharous", "M. Ghavamzadeh"], "venue": "In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence", "citeRegEx": "Thomas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2015}, {"title": "S", "author": ["P. Zhao", "R. Jin", "T. Yang", "Hoi"], "venue": "C.", "citeRegEx": "Zhao et al.. 2011", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 15, "context": "While the techniques in different papers differ in their details, the majority of them can be be analyzed under the umbrella framework of contextual bandits [Langford and Zhang, 2008].", "startOffset": 157, "endOffset": 183}, {"referenceID": 14, "context": "Some exploration techniques, like explicit greedy [Koh and Gupta, 2014; Theocharous, Thomas, and Ghavamzadeh, 2015] or implicit Bayesian type sampling from the posterior distribution maintained on classifier parameters [Chapelle and Li, 2011] are sometimes combined with this exploitation strategy.", "startOffset": 50, "endOffset": 115}, {"referenceID": 6, "context": "Some exploration techniques, like explicit greedy [Koh and Gupta, 2014; Theocharous, Thomas, and Ghavamzadeh, 2015] or implicit Bayesian type sampling from the posterior distribution maintained on classifier parameters [Chapelle and Li, 2011] are sometimes combined with this exploitation strategy.", "startOffset": 219, "endOffset": 242}, {"referenceID": 11, "context": "Other, more theoretically sophisticated online bandit algorithms, essentially learn a costsensitive multi-class classifier by updating after every round of user-system interaction [Dudik et al., 2011; Agarwal et al., 2014].", "startOffset": 180, "endOffset": 222}, {"referenceID": 21, "context": "The rankers are learnt by optimizing the Area Under Curve (AUC) ranking loss via stochastic gradient descent (SGD) [Shamir and Zhang, 2013], leading to a highly scalable algorithm.", "startOffset": 115, "endOffset": 139}, {"referenceID": 2, "context": ", 2011] and Thompson sampling [Agrawal and Goyal, 2013].", "startOffset": 30, "endOffset": 55}, {"referenceID": 14, "context": "A number of research publications show that researchers consider binary linear classifiers, that are learnt by optimizing the logistic loss [Richardson, Dominowska, and Ragno, 2007], while ensemble classifiers, like random forests, are also becoming popular [Koh and Gupta, 2014].", "startOffset": 258, "endOffset": 279}, {"referenceID": 13, "context": "This is colloquially referred to as \u201cclass imbalance problem\u201d in binary classification [Japkowicz and Stephen, 2002].", "startOffset": 87, "endOffset": 116}, {"referenceID": 15, "context": "Another type of policy space consist of cost-sensitive multiclass classifiers [Langford and Zhang, 2008; Dudik et al., 2011; Agarwal et al., 2014].", "startOffset": 78, "endOffset": 146}, {"referenceID": 11, "context": "Another type of policy space consist of cost-sensitive multiclass classifiers [Langford and Zhang, 2008; Dudik et al., 2011; Agarwal et al., 2014].", "startOffset": 78, "endOffset": 146}, {"referenceID": 10, "context": ", the positive instances are ranked higher than the negatives when instances are sorted in descending order of their scores [Cortes and Mohri, 2004].", "startOffset": 124, "endOffset": 148}, {"referenceID": 21, "context": "The objective function (1) is a convex function and can be efficiently optimized by stochastic gradient descent (SGD) procedure [Shamir and Zhang, 2013].", "startOffset": 128, "endOffset": 152}, {"referenceID": 24, "context": "Lastly, powerful non-linear kernel ranking functions can be learnt in place of linear ranking functions, but at the cost of memory efficiency, and the rankers can even be learnt online, from streaming data [Zhao et al., 2011].", "startOffset": 206, "endOffset": 225}, {"referenceID": 17, "context": "There exist various such evaluation techniques in the literature, with adequate discussion about the process [Li et al., 2011].", "startOffset": 109, "endOffset": 126}, {"referenceID": 14, "context": "The RandomForests were composed of 200 trees, both for computational feasibility and for the more theoretical reason outlined in [Koh and Gupta, 2014].", "startOffset": 129, "endOffset": 150}], "year": 2016, "abstractText": "We study the problem of personalized advertisement recommendation (PAR), which consist of a user visiting a system (website) and the system displaying one of K ads to the user. The system uses an internal ad recommendation policy to map the user\u2019s profile (context) to one of the ads. The user either clicks or ignores the ad and correspondingly, the system updates its recommendation policy. PAR problem is usually tackled by scalable contextual bandit algorithms, where the policies are generally based on classifiers. A practical problem in PAR is extreme click sparsity, due to very few users actually clicking on ads. We systematically study the drawback of using contextual bandit algorithms based on classifier-based policies, in face of extreme click sparsity. We then suggest an alternate policy, based on rankers, learnt by optimizing the Area Under the Curve (AUC) ranking loss, which can significantly alleviate the problem of click sparsity. We conduct extensive experiments on public datasets, as well as three industry proprietary datasets, to illustrate the improvement in click-through-rate (CTR) obtained by using the ranker-based policy over classifier-based policies.", "creator": "LaTeX with hyperref package"}}}