{"id": "1603.01987", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2016", "title": "A matter of words: NLP for quality evaluation of Wikipedia medical articles", "abstract": "Automatic quality evaluation of Web information is a task with many fields of applications and of great relevance, especially in critical domains like the medical one. We move from the intuition that the quality of content of medical Web documents is affected by features related with the specific domain. First, the usage of a specific vocabulary (Domain Informativeness); then, the adoption of specific codes (like those used in the infoboxes of Wikipedia articles) and the type of document (e.g., historical and technical ones). In this paper, we propose to leverage specific domain features to improve the results of the evaluation of Wikipedia medical articles. In particular, we evaluate the articles adopting an \"actionable\" model, whose features are related to the content of the articles, so that the model can also directly suggest strategies for improving a given article quality. We rely on Natural Language Processing (NLP) and dictionaries-based techniques in order to extract the bio-medical concepts in a text. We prove the effectiveness of our approach by classifying the medical articles of the Wikipedia Medicine Portal, which have been previously manually labeled by the Wiki Project team. The results of our experiments confirm that, by considering domain-oriented features, it is possible to obtain sensible improvements with respect to existing solutions, mainly for those articles that other approaches have less correctly classified. Other than being interesting by their own, the results call for further research in the area of domain specific features suitable for Web data quality assessment.", "histories": [["v1", "Mon, 7 Mar 2016 09:54:11 GMT  (816kb,D)", "http://arxiv.org/abs/1603.01987v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["vittoria cozza", "marinella petrocchi", "angelo spognardi"], "accepted": false, "id": "1603.01987"}, "pdf": {"name": "1603.01987.pdf", "metadata": {"source": "CRF", "title": "A matter of words: NLP for quality evaluation of Wikipedia medical articles", "authors": ["Vittoria Cozza", "Marinella Petrocchi", "Angelo Spognardi"], "emails": ["m.petrocchi}@iit.cnr.it", "angsp@dtu.dk"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2 Dataset", "text": "This year, most of them will be able to go to the United States to stay where they are: to the United States, to Europe, to the United States, to Europe, to Europe, to the United States, to Europe, to Europe, to the United States, to Europe, to the United States, to Europe, to the United States, to Europe, to the United States, to Europe, to Europe, to the United States, to the United States, to Europe, to the United States, to Europe, to the United States, to Europe, to the United States, to Europe and to the United States."}, {"heading": "3 Baseline: the actionable model", "text": "We apply a multi-level classification approach to mark the articles of the sampled dataset into the six mentioned WikiProject quality classes. To have a baseline, we first apply the state-of-the-art model proposed in [28] to the dataset. 7 Implemented and available in the Weka FrameworkThe \"feasible model\" in [28] focuses on five linguistic and structural features and weighs them as follows: 1. Completeness = 0.4 * NumBrokenWikilinks + 0.4 * NumWikilinks 2. Informativities = 0.6 * InfoNoise + 0.3 * NumImages 3. NumHeadings 4. ArticleLength 5. NumReferences / ArticleLengthwhere- NumWikilinks is the number of links pointing to other Wikipedia pages (whereas NumBrokenWikilinks counts the links that are defective); - InfoNoise is the proportion of the text content that remains in the article after text has been removed from wiki markups and basic NP operations."}, {"heading": "4 The medical domain model", "text": "As shown in Figure 3, the characteristics of the medical model, the bio-medical units, were extracted exclusively from the free text, using advanced NLP techniques and using domain dictionaries. In detail, we define and extract the following new features from the data set: 1. InfoBoxNormSize: This feature represents the normalized size of an infobox containing standard medical encodings. 2. Category: The category to which a page belongs. 3. DomainInformativity: the number of bio-medical units that are the domain-dependent terms of the article (such as those that denote symptoms, diseases, treatments, etc.) The idea of considering infoboxes is not new: In [28] the authors noted that the presence of an infobox is a feature characterized by good articles."}, {"heading": "4.1 Infobox-based feature", "text": "We have calculated the size of the infobox as the base 10 log of the data bytes contained in the Mediawiki tags enveloping an infobox, and we have normalized it with respect to the article length introduced in Section 3."}, {"heading": "4.2 Category-based feature", "text": "We have used the categories assigned to articles in Wikipedia, especially in relation to medical topics, which can be found at https: / / en.wikipedia.org / wiki / Portal: Medicine.We have defined 4 upper-level categories of our interest: - A Anatomy: an article is about anatomy; - B Biography: an article is a biography of someone or tells the story of something; - D Disorder: it is a disorder; - F First Aid: it reports information for first aid or emergency contacts; - O Other: nothing of the kind. We have matched the text of the article within the MediaWiki category tag with an approximate list of keywords related to our category of interest, as reported in Table 2."}, {"heading": "5 Bio-medical entities", "text": "In the literature, there are several methods for extracting biomedical units from a text (i.e. from medical notes and / or articles). We refer to [19] to get an overview of valuable existing techniques. In this work, we have taken a dictionary-based approach that utilizes lexical characteristics and domain knowledge gained from the Unified Medical Languages System (UMLS). [7] The approach was proposed in a previous paper for the Italian language [1]. Since the approach combines the use of linguistic analysis and domain resources, we were able to adapt it conveniently to the English language, since it provides both the linguistic pipeline and UMLS for multiple languages (including English and Italian). Dictionary-based approaches have proven to be valid for the task of extracting units, see, for example, another well-known, similar approach for the language used here, i.e. Metamap10."}, {"heading": "5.1 Reference dictionary", "text": "To build a medical dictionary, we have extracted definitions of medical units from the Unified Medical Languages System (UMLS) Metathesaurus [7]. UMLS integrates biomedical resources, such as SNOMED-CT11, which provide the core terminology for electronic health records. In addition, UMLS also provides a semantic network in which each unit in the metathesaurus has an assigned concept Unique Identifier (CUI), and it is semantically typed.From UMLS, we have extracted entries belonging to the following SNOMEDCT semantic groups: Treatment, Sign or Symptom, Disease or Syndrome, Body Parts, Organs or Organ Components, Pathological Functions, and Mental or Behavioral Dysfunction, for a total of more than one million entries."}, {"heading": "5.2 Extraction of bio-medical entities", "text": "We have extracted the biomedical units contained in Wikipedia's medical articles using an n-gram-based technique to remove the match between the two matches. A pre-processing phase takes place in a similar way to the composition of the dictionary. In view of a Wikipedia article in English, we have pre-edited the text through the Tanl pipeline. Similar to Section 5.1 for the Reference Dictionary, we first split the text into sentences and sentences into individual word forms. For each form, we have considered the lemma (where available) and the part of the speech (POS). Starting with an example sentence on the Wikipedia page on Alzheimer's disease: \"Other risk factors include a history of head injuries, depression or high blood pressure.\" We have obtained the comment in Figure 4. As with the dictionary, each word in the text has been colloquially defined to match the two."}, {"heading": "6 Experiments and results", "text": "In this section, we describe the experiments and report on the results of the classification of Wikipedia medical articles into the six classes of the Wikipedia medical portal. We compare the results obtained using four different classifiers: the feasible model in [28] and three classifiers that use the ad hoc characteristics from the medical field discussed in the previous sections. All the experiments were conducted within the Weka framework [14] and validated by 10-fold cross-validation. For each experiment, we relied on the data set presented in Section 2, and in particular the data set obtained after majority class samples and minority sample samples (right column in Table 1). The data set serves as both a training and a test set for the classifiers. To take into account the unbalanced data, we have applied several classification algorithms and, for the sake of precision, reported only the best results we have achieved below."}, {"heading": "6.1 Classifiers\u2019 features", "text": "In Table 4, we report on a summary of the characteristics for each of the models considered: the basic model in [28] and two new models that use the characteristics of the medical domain. In the medical domain model, we add the basic characteristics of the domain informativeness, as described in sections 4 and 5. In addition, the model of the complete medical domain also takes into account the characteristics InfoBoxNormSize and category.For each of the characteristics, the table also indicates the information gain that is evaluated over the entire data set (24,362 articles). Information gain is a known parameter to evaluate the dependence of a class on a single characteristic, see e.g. [11]. We can observe how the characteristic of the domain informativeness has a considerably higher information value compared to informativeness. We expect here that this will lead to more precise classification results for the highest classes, as reported in the next section."}, {"heading": "6.2 Classification results", "text": "Table 5 shows the results of our multi-class classification. For each of the classes, we have calculated the lowest values generally used to assess the performance of companies (i.e., the number of people benefiting from the positive is high)."}, {"heading": "7 Related work", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "8 Conclusions", "text": "We focused on an actionable model, namely the characteristics associated with the content of the articles, so that they can also directly propose strategies to improve a particular article. An important and new aspect of our classifier compared to previous work is the use of characteristics extracted from the specific medical field using natural language processing techniques. As the results of our experiments confirm, taking into account specific domain-based characteristics such as domain informativeness and category can ultimately contribute to improving automatic classification results. As the results are encouraging, our future work will evaluate other characteristics based on the specific medical domain. We also plan to expand our idea to include and compare other non-medical articles (thereby extending the IBM approach to further research results: http: / www.m.org)."}], "references": [{"title": "Adapting linguistic tools for the analysis of italian medical records", "author": ["G. Attardi", "V. Cozza", "D. Sartiano"], "venue": "The First Italian Conference on Computational Linguistics CLiC-it 2014, page 17,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "The TANL pipeline", "author": ["G. Attardi", "S.D. Rossi", "M. Simi"], "venue": "Web Services and Processing Pipelines in HLT: Tool Evaluation, LR Production and Validation (LREC:WSSP),", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Is Wikipedia a reliable learning resource for medical students? Evaluating respiratory topics", "author": ["S.A. Azer"], "venue": "Advances in Physiology Education, 39(1):5\u201314,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Accuracy of cardiuvascular articles on Wikipedia: Are they reliable learning resources for medical students", "author": ["S.A. Azer", "N. AlSwaidan", "L. AlSwairikh", "J. AlShammari"], "venue": "BMJ Open,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Natural Language Processing with Python", "author": ["S. Bird", "E. Klein", "E. Loper"], "venue": "O\u2019Reilly Media,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Size matters: Word count as a measure of quality on Wikipedia", "author": ["J.E. Blumenstock"], "venue": "In Proceedings of the 17th International Conference on World Wide Web, WWW \u201908, pages 1095\u20131096, New York, NY, USA,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Exploring semantic groups through visual approaches", "author": ["O. Bodenreider", "A.T. McCray"], "venue": "Journal of biomedical informatics, 36(6):414\u2013432,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "An information reliability index as a simple consumer-oriented indication of quality of medical web sites", "author": ["F. Cabitza"], "venue": "In Quality Issues in the Management of Web Information, volume 50 of Intelligent Systems Reference Library, pages 159\u2013177. Springer Berlin Heidelberg,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "SMOTE: Synthetic minority over-sampling technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "Journal of Artificial Intelligence Research, 16:321\u2013357,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Editorial: Special issue on learning from imbalanced data sets", "author": ["N.V. Chawla", "N. Japkowicz", "A. Kotcz"], "venue": "SIGKDD Explor. Newsl., 6(1):1\u20136, June", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "Wiley-Interscience,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "On measuring the quality of Wikipedia articles", "author": ["G. De la Calzada", "A. Dekhtyar"], "venue": "In Proceedings of the 4th Workshop on Information Credibility,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Assessing document and sentence readability in less resourced languages and across textual genres", "author": ["F. Dell\u2019Orletta", "S. Montemagni", "G. Venturi"], "venue": "International Journal of Applied Linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "The WEKA data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD explorations newsletter, 11(1):10\u201318,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning from Imbalanced Data", "author": ["H. He", "E. Garcia"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 21(9):1263\u20131284, Sept", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Wikipedians reach out to academics", "author": ["R. Hodson"], "venue": "Nature News, Sept.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Derivation of new readability formulas for navy enlisted personnel", "author": ["J.P. Kincaid", "R.P. Fishburne", "R.L. Rogers", "B.S. Chissom"], "venue": "National Technical Information Service: Research Report, pages 8\u201375,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1975}, {"title": "Improved automatic maturity assessment of Wikipedia medical articles", "author": ["E. Marzini", "A. Spognardi", "I. Matteucci", "P. Mori", "M. Petrocchi", "R. Conti"], "venue": "In On the Move to Meaningful Internet Systems: OTM 2014 Conferences, volume 8841 of Lecture Notes in Computer Science, pages 612\u2013622. Springer,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "editors", "author": ["P. Nakov", "T. Zesch"], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014). Association for Computational Linguistics and Dublin City University, Dublin, Ireland, August", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to quality issues in the management of web information", "author": ["G. Pasi", "G. Bordogna", "L. Jain"], "venue": "In G. Pasi, G. Bordogna, and L. C. Jain, editors, Quality Issues in the Management of Web Information, volume 50 of Intelligent Systems Reference Library, pages 1\u20133. Springer Berlin Heidelberg,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation", "author": ["D.M.W. Powers"], "venue": "International Journal of Machine Learning Technologies, 2(1):37\u201363,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "How to make a decision: The Analytic Hierarchy Process", "author": ["T.L. Saaty"], "venue": "European Journal of Operational Research, 48(1),", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1990}, {"title": "Artificial intelligence aims to make Wikipedia friendlier and better", "author": ["T. Simonite"], "venue": "In MIT Technology Review, November", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Probability as readability: A new machine learning approach to readability assessment for written Swedish", "author": ["J. Sj\u00f6holm"], "venue": "Master\u2019s thesis, Link\u00f6ping University,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "A model for online consumer health information quality", "author": ["B. Stvilia", "L. Mon", "Y.J. Yi"], "venue": "Journal of the American Society for Information Science and Technology, 60(9):1781\u20131791,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Assessing information quality of a community-based encyclopedia", "author": ["B. Stvilia", "M.B. Twidale", "L.C. Smith", "L. Gasser"], "venue": "In Proceedings of the 2005 International Conference on Information Quality, pages 442\u2013454, Cambridge, MA,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Readability assessment for text simplification: From analysing documents to identifying sentential simplifications", "author": ["S. Vajjala", "D. Meurers"], "venue": "International Journal of Applied Linguistics, 165(2):194\u2013222,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Tell me more: An actionable quality model for Wikipedia", "author": ["M. Warncke-Wang", "D. Cosley", "J. Riedl"], "venue": "In Proceedings of the 9th International Symposium on Open Collaboration, WikiSym \u201913, pages 8:1\u20138:10, New York, NY, USA,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Modelling the quality of attributes in Wikipedia infoboxes", "author": ["K. Wecel", "W. Lewoniewski"], "venue": "In Business Information Systems Workshops, volume 228 of Business Information Processing, pages 308\u2013320. Springer International Publishing,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Mining the factors affecting the quality of wikipedia articles", "author": ["K. Wu", "Q. Zhu", "Y. Zhao", "H. Zheng"], "venue": "Information Science and Management Engineering (ISME), 2010 International Conference of, 1:343\u2013346, Aug", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "As observed by a recent article of Nature News [16], \u201cWikipedia is among the most frequently visited websites in the world and one of the most popular places to tap into the world\u2019s scientific and medical information\u201d.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "In an attempt to fix this shortcoming, Wikipedia has recently enlisted the help of scientists to actively support the editing on Wikipedia [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "Fighting vandalism is one of the main goals of the Wikimedia Foundation, the nonprofit organization that supports Wikipedia: machine learning techniques have been considered to offer a service to \u201cjudge whether an edit was made in good faith or not\u201d [23].", "startOffset": 250, "endOffset": 254}, {"referenceID": 16, "context": "Traditionally, the literature has widely adopted well known criteria, as the \u201cFlesch-Kincaid\u201d measure\u201d [17], to automatically assess readability in textual documents.", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": ", [13] for the Italian use case, [24] for the Swedish one, [27] for English).", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": ", [13] for the Italian use case, [24] for the Swedish one, [27] for English).", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": ", [13] for the Italian use case, [24] for the Swedish one, [27] for English).", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": ", [20]), which points out like the quality of Web information is strictly connected to the scope for which one needs such information.", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": "As an example, the work in [28] exploits classification techniques based on structural and linguistic features of an article.", "startOffset": 27, "endOffset": 31}, {"referenceID": 27, "context": "Section 3 briefly presents the actionable model in [28]: we adopt it as the baseline for our analysis.", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "Many studies have been conducted to improve learning algorithms accuracy in presence of imbalanced data [15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "For the current work, we have considered one of the most popular approaches, namely the Synthetic Sampling with Data Generation, detailed in [9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 9, "context": ", [10].", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": "In order to have a baseline, we first apply the state of the art model proposed in [28] to the dataset.", "startOffset": 83, "endOffset": 87}, {"referenceID": 27, "context": "The \u201cactionable model\u201d in [28] focuses on five linguistic and structural features and it weighs them as follows:", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "We have measured ArticleLength, NumWikilinks and NumBrokenWikilinks as suggested in [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 4, "context": "In details, nltk has been used for computing the InfoNoise feature, whose computation includes the stopwords removal, following the Porter Stopwords Corpus available through nltk [5].", "startOffset": 179, "endOffset": 182}, {"referenceID": 27, "context": "The idea of considering infoboxes is not novel: for example, in [28] the authors noticed that the presence of an infobox is a characteristic featured by good articles.", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "We refer to [19] for an overview of valuable existing techniques.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "In this work, we have adopted a dictionary-based approach, which exploits lexical features and domain knowledge extracted from the Unified Medical Languages System (UMLS) Metathesaurus [7].", "startOffset": 185, "endOffset": 188}, {"referenceID": 0, "context": "The approach has been proposed for the Italian language in a past work [1].", "startOffset": 71, "endOffset": 74}, {"referenceID": 18, "context": "It is worth noting like, even though dictionarybased approaches could be less precise than Named Entity Recognition [19], in our context even an approximate solution is enough, since we are not annotating medical records.", "startOffset": 116, "endOffset": 120}, {"referenceID": 6, "context": "To build a medical dictionary, we have extracted definitions of medical entities from the Unified Medical Languages System (UMLS) Metathesaurus [7].", "startOffset": 144, "endOffset": 147}, {"referenceID": 1, "context": "In details, we have pre-processed the entries by mean of the Tanl pipeline [2], a suite of modules for text analytics and NLP, based on machine learning.", "startOffset": 75, "endOffset": 78}, {"referenceID": 27, "context": "We compare the results obtained adopting four different classifiers: the actionable model in [28] and three classifiers that leverage the ad-hoc features from the medical domain discussed in the previous sections.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "All the experiments were realized within the Weka framework [14] and validated through 10 fold cross-validation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "In Table 4, we report a summary of the features for each of the considered models: the baseline model in [28] and two new models that employ the medical domain features.", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": ", [11].", "startOffset": 2, "endOffset": 6}, {"referenceID": 20, "context": "For each of the classes, we have computed the ROC Area and F-Measure metrics [21].", "startOffset": 77, "endOffset": 81}, {"referenceID": 5, "context": "The size of an article, expressed either as the word count, analyzed in [6], or as the article length, as done here, appears a very strong feature, able to discriminate the articles belonging to the highest and lowest quality classes.", "startOffset": 72, "endOffset": 75}, {"referenceID": 27, "context": "This is testified also by the results achieved exploiting the baseline model of [28], which poorly succeeds in discriminating the articles of the intermediate quality classes, while achieving good results for Stub and FA.", "startOffset": 80, "endOffset": 84}, {"referenceID": 25, "context": "In [26], Stvilia et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Blumenstock [6] inspects the relevance of the word-count feature at each quality stage, showing that it can play a very important role in the quality assessment of Wikipedia articles.", "startOffset": 12, "endOffset": 15}, {"referenceID": 29, "context": "In [30], the authors try to analyze the factors affecting the quality of Wikipedia articles, with respect to their quality class.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "In [29], the authors consider the quality of the data in the infoboxes of Wikipedia, finding a correlation between the quality of information in the infobox and the article itself.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "In [28], the authors deal with the problem of discriminating between two large classes, namely NeedWork, GoodEnough (including in GoodEnough both GA and FA), in order to identify which articles need further revisions for being featured.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "Recent studies specifically address the quality of medical information (in Wikipedia as well as in other resources): in [3] and [4], the authors debate if Wikipedia is a reliable learning resource for medical students, evaluating articles on respiratory topics and cardiovascular diseases.", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "Recent studies specifically address the quality of medical information (in Wikipedia as well as in other resources): in [3] and [4], the authors debate if Wikipedia is a reliable learning resource for medical students, evaluating articles on respiratory topics and cardiovascular diseases.", "startOffset": 128, "endOffset": 131}, {"referenceID": 17, "context": "In [18] the authors provide novel solutions for measure the quality of medical information in Wikipedia, by adopting an unsupervised approach based on the Analytic Hierachy Process, a multi-criteria decision making technique [22].", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "In [18] the authors provide novel solutions for measure the quality of medical information in Wikipedia, by adopting an unsupervised approach based on the Analytic Hierachy Process, a multi-criteria decision making technique [22].", "startOffset": 225, "endOffset": 229}, {"referenceID": 7, "context": "The work in [8] aims to provide the web surfers a numerical indication of Quality of Medical Web Sites.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "In particular in [8] the author proposes an index to make IQ judgment of the content and of its reliability, to give the so called \u201csurface markers\u201d and \u201ctrust indicator\u201d.", "startOffset": 17, "endOffset": 20}, {"referenceID": 24, "context": "A similar measurement is considered in [25].", "startOffset": 39, "endOffset": 43}], "year": 2016, "abstractText": "Automatic quality evaluation of Web information is a task with many fields of applications and of great relevance, especially in critical domains like the medical one. We move from the intuition that the quality of content of medical Web documents is affected by features related with the specific domain. First, the usage of a specific vocabulary (Domain Informativeness); then, the adoption of specific codes (like those used in the infoboxes of Wikipedia articles) and the type of document (e.g., historical and technical ones). In this paper, we propose to leverage specific domain features to improve the results of the evaluation of Wikipedia medical articles. In particular, we evaluate the articles adopting an \u201cactionable\u201d model, whose features are related to the content of the articles, so that the model can also directly suggest strategies for improving a given article quality. We rely on Natural Language Processing (NLP) and dictionaries-based techniques in order to extract the biomedical concepts in a text. We prove the effectiveness of our approach by classifying the medical articles of the Wikipedia Medicine Portal, which have been previously manually labeled by the Wiki Project team. The results of our experiments confirm that, by considering domain-oriented features, it is possible to obtain sensible improvements with respect to existing solutions, mainly for those articles that other approaches have less correctly classified. Other than being interesting by their own, the results call for further research in the area of domain specific features suitable for Web data quality assessment.", "creator": "LaTeX with hyperref package"}}}