{"id": "1603.02041", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2016", "title": "Learning Shared Representations in Multi-task Reinforcement Learning", "abstract": "We investigate a paradigm in multi-task reinforcement learning (MT-RL) in which an agent is placed in an environment and needs to learn to perform a series of tasks, within this space. Since the environment does not change, there is potentially a lot of common ground amongst tasks and learning to solve them individually seems extremely wasteful. In this paper, we explicitly model and learn this shared structure as it arises in the state-action value space. We will show how one can jointly learn optimal value-functions by modifying the popular Value-Iteration and Policy-Iteration procedures to accommodate this shared representation assumption and leverage the power of multi-task supervised learning. Finally, we demonstrate that the proposed model and training procedures, are able to infer good value functions, even under low samples regimes. In addition to data efficiency, we will show in our analysis, that learning abstractions of the state space jointly across tasks leads to more robust, transferable representations with the potential for better generalization. this shared representation assumption and leverage the power of multi-task supervised learning. Finally, we demonstrate that the proposed model and training procedures, are able to infer good value functions, even under low samples regimes. In addition to data efficiency, we will show in our analysis, that learning abstractions of the state space jointly across tasks leads to more robust, transferable representations with the potential for better generalization.", "histories": [["v1", "Mon, 7 Mar 2016 13:03:30 GMT  (5333kb,D)", "http://arxiv.org/abs/1603.02041v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["diana borsa", "thore graepel", "john shawe-taylor"], "accepted": false, "id": "1603.02041"}, "pdf": {"name": "1603.02041.pdf", "metadata": {"source": "META", "title": "Learning Shared Representations for Value Functions in Multi-task Reinforcement Learning", "authors": ["Diana Borsa", "Thore Graepel", "John Shawe-Taylor"], "emails": ["DIANA.BORSA@GMAIL.COM", "THORE@GOOGLE.COM", "J.SHAWE-TAYLOR@CS.UCL.AC.UK"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is such that most of them are able to abide by the rules that they have imposed on themselves. (...) In fact, it is such that they are able to outdo themselves. \"(...) Most of them are able to outdo themselves.\" (...) Most of them are able to outdo themselves. \"(...).\" (...) Most of them are able to outdo themselves. \"(...).\" (...). \"(...). (...).\" (...). (...). \"(...). (...).\" (...). (...). \"(...).\" (...). \"(...).\" (...). (...). (...). (...). (...). (...). (.). (...). (...). (. (.). (.). (.). (.). (.). (.). (.). (.). (...). (. (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.). (.).). (.). (.).). (. (.). (.). (.).). (. (.).). (.). (. (.).). (. (.). (.).). (. (.). (. (.). (.).). (.). (.).). (.). (. (.). (.).). (.). (. (.). (. (.).). (.).). (. (.). (.). (.). (.). (.).). (.).). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.).). (.). (.).). (.).).)."}, {"heading": "2. Proposed Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Background and Notation", "text": "We define a Markov decision-making process (MDP) as a tuple M = (S, A, P, R, \u03b3), where S is the set of states, A is the set of actions 1, P: S \u00b7 (S \u00b7 A) \u2192 [0, 1] is the transition dynamics P (s \u00b2 | s, a) that provides a probability of the next state s, R: S \u00b7 A \u2192 R is a reward signal that is assumed to be limited (Rmax, s.t. R (s, a) \u2264 Rmax, s \u00b2 S, a \u00b2 A) and vice versa [0, 1] is a discount factor.In the face of an MDP and any policy measure \u03c0: S \u00d7 A \u2192 [0, 1] we define the (state action) value function, Q\u03c0 (s, a) as the discounted cumulative reward that an agent expects when he proceeds from state s \u00b2 S to take action and then acts accordingly according to the policy: Q\u043c (s) becomes a Qs (we), which is an optimal outcome of this (we)."}, {"heading": "2.2. Problem formulation", "text": "We look at the scenario in which an agent lives (or is placed) in an environment in which he has to perform a series of tasks, the overall goal being to learn how to successfully complete all of these tasks; the environment is described by a state action space S \u00b7 A and a transitional core P (s \u2032 | s, a), and the tasks can be specified by different reward signals Rt (s, a), one for each task t = 1, T. This formally leads to T MDPsMt = (S, A, P, Rt, \u03b3), which have a lot of structure in common. So, if we find a way to use this structure, we expect that this will support the learning process and lead to a better generalization. (Taylor & Stone, 2009a)"}, {"heading": "2.3. A shared (value function) representation", "text": "We propose to model the common structure contained in the MDP-s defined above as a common embedding of the state sphere of action \u03c6: S \u00b7 A \u2192 Rd, on which we can build the individual optimal value functions for all eligible and potentially new tasks. Therefore, in this paper we are interested in learning this common embedding and ultimately the optimal behavior for each of the tasks considered. In the following, we will explain how to expand two of the most popular paradigms of learning value functions, Fitted Q-Iteration and Fitted Policy Iteration, in order to integrate this common structure assumption."}, {"heading": "3. Multi-task (Fitted) Value Iteration", "text": "In this section, we outline a general framework for using approximate iteration to derive the optimal Q values (and optimal strategies) for a number of tasks, in a given environment according to the previously introduced MT-RL setup. The proposed algorithm is an extension of the Fitted Q-Iteration (FQI), which enables joint learning and transfer across tasks. According to FQI's recipe, we calculate the onestep TD target based on our current estimate of the value function and for each sample in our experience group D = {(s, a, r, s \u00b2). P (. | s, a), we calculate the onestep TD target based on our current estimate of the value function. Then, treating these estimates as a down-to-earth truth, we get a regression problem from the state action space to the TD targets (which are really placeholders for the true value function). In the case of MT-RL, we will get such a regression problem for each task."}, {"heading": "4. Multi-task (Fitted) Policy Iteration", "text": "Through an argument similar to the one presented in the last section for MT-FQI, we can expand the framework of general policy iteration to the MT-FL scenario. Political iteration algorithms are based on an alternative procedure between a policy assessment step and a policy improvement step. We can extend this framework to the multi-task case by defining a (current) set of policies by implementing the policy improvement step by acting abominably in relation to our current estimates of the value function, which is done individually for each task. On the other hand, we allow joint learning and sharing of knowledge at the policy evaluation stage, leading to a general procedure we call Multi-Task Policy Evaluation (MT-PE) - see Algorithm 3. In MT-PE, we will enable a policy of shared learning and sharing of knowledge at the policy evaluation stage."}, {"heading": "5. Multi-task and Representation Learning", "text": "In this section, we will look at some methods that we can integrate into the above-mentioned algorithms in the MTL step (= Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q"}, {"heading": "5.1. Multi-task feature learning", "text": "In terms of planning, the common problem we are trying to solve can be formalized as a conclusion on weight vectors {wt} Tt = 1 = arg minW [\u03b1t] + H (W), where H (W) is a regulator on weight vectors that promotes feature sharing. At the same time, we would like to learn a more compact abstraction of the state action space divided among tasks. To make this somewhat more formal, let Qt, w: S \u00b7 A \u2192 R, Qt, w (s, a): = < \u03a6 (s, a), wt >, then our assumption can be expressed as follows: a small set of characteristics {\u0644i} i = 1, N\u0443 so that this problem does not form a basis, Qt (s, a) = a: a) = i (s, a) = < < T (s, a) < < < < < with respect to the relevant i-s."}, {"heading": "5.2. Allowing for task specificity", "text": "The above method can be used to construct very informative common features - as shown in (Calandriello et al., 2014) and in our experimental section. This is definitely the case in many practical applications and has also been observed in purely monitored environments - it is simply too limited to restrict all tasks to a single common structure. Therefore, researchers have found various ways to integrate task-specific components - see (Zhou et al., 2011), (Jalali et al., 2010), (Chen et al., 2012) and refer to them - and have shown that modelling this explicitly can improve both learning (accuracy and speed) and the interpretability of the resulting representations. In this work, we choose only one of these formulations that was introduced in (Ando et al., 2005)."}, {"heading": "6. Experiments", "text": "We evaluate the performance and behavior of our proposed model and the learning processes in the 4-space navigation task (Sutton et al., 1999). State space S is described by all valid positions an agent could occupy - any position in the net, but the wall and the agent have access to four actions A = {\u2192, \u2190, \u2191, \u2193}. We consider a deterministic dynamic in these directions and all walls are considered elastic - occurrence in walls has no effect on your state. Tasks are specified as destinations in the environment to which the agent must navigate. These are randomly evaluated from the valid states in the environment. We do not specify an initial state - agents must learn to navigate the selected target position from any part of the environment. If the agent transitions to the target states, it collects a positive reward. Since all proposed methods can be executed outside of politics, we can gain and learn a modest level of experience."}, {"heading": "6.1. Learnt shared representations", "text": "Probably the most interesting phenomenon that occurs when learning these common representations is the nature of the low-dimensional representations derived from them. We visualize the derived common features (Figure 5) and their respective weights in the value function (Figure 6), which were generated using MT-FQI with ASO, with the caveat that the common sub-space is at most 5-dimensional. And even this seems to be too permissive, since we actually get strong activations only for the top 3 derived features that are represented in Figure 5. Thus, the learned representation is very low dimensional, but at the same time expressive enough to effectively approximate optimal value functions."}, {"heading": "6.2. Transferring knowledge to new tasks", "text": "The learned representations resemble option-like features (Sutton et al., 1999), which essentially inform the agent about tasks, how to efficiently navigate between rooms and negotiate narrow corridors. These are indeed easily transferable \"skills\" that can be used in learning a new task. We test this hypothesis by learning represen-commerce 1 (s, \u2193) 2 4 6 8 102 4 6 10 x 1 (s, \u2192) 2 4 8 102 4 8 10 x 1 (s, \u2191) 2 4 8 102 4 6 8 10 x 1 (s, \u2190) 2 4 8 102 4 6 8 10x 2 (s,) 2 4 8 102 4 6 10x 2 (s,) 2 4 8 102 4 6 10x 2 (s, \u2191) 2 4 102 4 8 10x 2 (s, \u2191) 2 4 102 4 6 10x 2 (s, \u2191) 2 4 4 10x 2 (s, 4 10x 4 10x 4) 4 10x 2 (s) 4 10x 2 (s, 4 10x 4 10x (s) 4 (s, 4 104) 4 104 (s, \"We can (4) (4) 104 (4) 104 104 (104 104 104 104 10x 10x 10x 104 (104, 104, 104 10x 104, 104) 4 (s)."}, {"heading": "6.3. Connection to Options", "text": "As before, the learned common representation seems to be responsible for the general topology and dynamics of the environment in the value functions. (They have divided the environment into relevant regions to facilitate global navigation into a local neighborhood of the destination.) Some of these characteristics are indeed characteristic of options (Sutton et al., 1999), skills (Konidaris & Barto, 2007), macro actions (Dietterich, 2000) and possibilities for drastically improving the efficiency and scalability of RL methods (Barto & Mahadevan, 2003), (Hengst, 2002).In the following, we would like to investigate this connection further (Sutton et al., 1999), an option o = < I, \u03b2 > is a generalization of primitive actions a \u00b2 A into a temporally extended course of action."}, {"heading": "7. Related work", "text": "There is a good collection of methods that address various aspects of multi-task learning (Lazaric & Ghavamzadeh, 2012) and (Taylor & Stone, 2009a) As with our approach, these methods attempt to jointly learn either value functions or guidelines on a set of tasks (Lazaric & Ghavamzadeh, 2010), (Dimitrakakis & Rothkopf, 2011), but among other structural and environmental assumptions. A recent study in (Konidaris et al., 2012) also uses the idea of a common space of features, but both the learning process and the proposed method of transferring this concerted knowledge differs greatly from ours. The latest idea that this work introduces is the modeling of an explicit common abstraction of government action space that can be refined throughout the learning process while optimizing value functions. The ability to alter representation during the learning process in order to model policy improvement is critical."}, {"heading": "8. Conclusion and Future work", "text": "We introduced the Multi-Task RL paradigm and demonstrated how two of the most popular classes of planning algorithms, Customized Q-Iteration and Approximate Policy Iteration, can be expanded to learn from multiple tasks together. By focusing on the linear parameterization of the Q function, we demonstrated at least two ways to harness the power of well-established multi-task learning and transfer algorithms developed in monitored environments to derive a common structure through optimal value functions and implicitly through strategies. As has been argued and demonstrated in these preliminary experiments, RL can greatly benefit from integrating common treatment of goals and exploiting the commonality between tasks, which should lead to more efficient learning and better generalization. Although these are very encouraging results, this paradigm needs moral support to consider convergence behavior, scalability to more complex tasks, use of other multi-task reinforcement or generalization."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We investigate a paradigm in multi-task reinforcement learning (MT-RL) in which an agent is placed in an environment and needs to learn to perform a series of tasks, within this space. Since the environment does not change, there is potentially a lot of common ground amongst tasks and learning to solve them individually seems extremely wasteful. In this paper, we explicitly model and learn this shared structure as it arises in the state-action value space. We will show how one can jointly learn optimal value-functions by modifying the popular valueiteration and policy-iteration procedures to accommodate this shared representation assumption and leverage the power of multi-task supervised learning. Finally, we demonstrate that the proposed model and training procedures, are able to infer good value functions, even under low samples regimes. In addition to data efficiency, we will show in our analysis, that learning abstractions of the state space jointly across tasks leads to more robust, transferable representations with the potential for better generalization.", "creator": "LaTeX with hyperref package"}}}