{"id": "1603.02194", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2016", "title": "Gaussian Process Regression for Out-of-Sample Extension", "abstract": "Manifold learning methods are useful for high dimensional data analysis. Many of the existing methods produce a low dimensional representation that attempts to describe the intrinsic geometric structure of the original data. Typically, this process is computationally expensive and the produced embedding is limited to the training data. In many real life scenarios, the ability to produce embedding of unseen samples is essential. In this paper we propose a Bayesian non-parametric approach for out-of-sample extension. The method is based on Gaussian Process Regression and independent of the manifold learning algorithm. Additionally, the method naturally provides a measure for the degree of abnormality for a newly arrived data point that did not participate in the training process. We derive the mathematical connection between the proposed method and the Nystrom extension and show that the latter is a special case of the former. We present extensive experimental results that demonstrate the performance of the proposed method and compare it to other existing out-of-sample extension methods.", "histories": [["v1", "Mon, 7 Mar 2016 18:35:51 GMT  (1695kb)", "http://arxiv.org/abs/1603.02194v1", null], ["v2", "Sun, 5 Jun 2016 16:56:21 GMT  (1695kb)", "http://arxiv.org/abs/1603.02194v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["oren barkan", "jonathan weill", "amir averbuch"], "accepted": false, "id": "1603.02194"}, "pdf": {"name": "1603.02194.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In this context, it should be noted that the embedding of high-dimensional data points in a low-dimensional space usually only takes place for the embedding of high-dimensional data points. In this space, the Euclidean distance shows the affinity between the original data in terms of the manifold geometric structure. It is typical that the embedding only takes place for the training data that do not show an extension of the sample points. Furthermore, the process of embedding usually extends to expensive computer operations such as the decomposition of individual values (SVD)."}, {"heading": "2. Related work", "text": "In fact, it is such that the greater number of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to move, to fight, to fight, to fight, to fight, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "3. Gaussian Process Regression", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "4. GPR based OOSE", "text": "Considering a manifold learning algorithm M and a training set 1 {} m N i = = q), we apply M to X and calculate the corresponding low-dimensional embedding set 1 {} () m d i d = = q y. Then, for each dimension1 j d \u2264, independently of each other, we form a new training set {(,) |, 1,..., Nj i ij i ijD y i m = x x x x and train a comparable GPR model. Then, considering an invisible test example * x, we predict by equation. (3) its embedding and the degree of uncertainty in the predictions by [] * * * 1 *, T d \u00b5 = = = y \u00b5 and 2 * *, T d \u00b2 1 *, the test example * x. As the variance increases, our confidence in the prediction can be reduced as hyperparameters."}, {"heading": "4.1 The connection between GPR and the Nystrom extension", "text": "Many Manifolds learning methods are cast in the same framework [2], where the calculation of the embedding of the training data is achieved by self-decomposition of a (normalized) kernel matrix. Therefore, the self-decomposition of K for a given training unit 1 {} m i = = = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q Q = Q = Q Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q"}, {"heading": "5. Experimental results", "text": "In this section, we present experimental results demonstrating the performance of our proposed method and comparing it with other existing OOSE methods."}, {"heading": "5.1 The experimental workflow", "text": "The workflow of the experiments is as follows: Based on a varied learning algorithm M, the OOSE method O and a data set X, we apply M to X and derive corresponding embeddings Y. Then we divide according to the random principle (,) X Y on training and test sets (,) pull R X Y = or (,) test test Q X Y =. The division is based on a certain section. Finally, we measure the accuracy of the extension by the Root Mean Squared Error (RMSE) measurement 1 / 2 211 (,) ni iiiRMSE Y y y y y y n \u2212 = = \u2212. We repeat the above procedure ten times (for different random divisions, R and Q) to generate a series of RMSE values and determine the final RMSE measurement series as the average series we use for the previous evaluation."}, {"heading": "5.2 OOSE methods", "text": "We compare our proposed method with the Nystrom Extension Method and several recently developed OOSE methods that have been shown to overcome some of the limitations of the Nystrom Extension: Multiscale Extension [4], Adaptive Laplacian Pyramids (ALP) [6], and PCA-based OOSE (POOS) [5]. All methods offer an OOSE scheme that is independent of the diverse learning algorithm."}, {"heading": "5.3 Manifold learning algorithms", "text": "We evaluate the performance of the OOSE methods for several well-known, diverse learning algorithms: Diffusion maps (DM) [3], ISOMAP [11], Laplacian eigenmaps (LE) [10], Local Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13]. We apply the methods Hyperparameters according to the data set evaluated: For all data sets, we apply ISOMAP, LE, LLE and MDS with the same closest neighboring value 8k =. For the DM method, we adjust the neighborhood value according to the median of the square Euclidean distances between the data points. For three-dimensional data sets, the target dimensionality for the diverse learning methods was set to 2. For high-dimensional data sets, the target dimension was selected separately according to the spectral decay for each manifold learning algorithm."}, {"heading": "5.4 Datasets", "text": "We use different synthetic and real datasets: Swiss Roll [11], Swiss Hole [15], Corner Planes [16], Punctured Sphere [12], Twin Peaks [12], 3D Cluster [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18]. Each dataset presents a different challenge for the diverse learning algorithm. All datasets were fed into the manifold learning algorithms and OOSE schemes without further pre-processing. From the MNIST and USPS datasets, we randomly generated a collection of 4000 and 1000 images of the same digit, respectively. For each synthetic dataset, we generated a set of 1000 data points."}, {"heading": "5.5 Experiment 1", "text": "To this end, we created a Swiss roll [11] with 1000 data points (Figure (4) bottom left) and created a corresponding embedding with each of the manifold learning algorithms, separately from each other. We then created R and Q sets with 0.1\u03c1 = as described in Section 5.1 and trained a GPR model for each of the manifold learning algorithms with R. Figure 2 shows the true embedding test Y and the prediction test created for Test X with the help of GPR. As we can see, even with a small value \u03c1, the predictions succeeded in obtaining a small amount of noise and following the same structure of the true embedding test Y."}, {"heading": "5.6 Experiment 2", "text": "This experiment is designed to evaluate the OOSE methods each time using a specific pair of a multiple learning algorithm and a dataset. To this end, we generate X R and Q for a pair (,) and calculate the average RMSE values for each O (see Section 5.1 for notations and further explanations). We repeat the experiment to increase the values from \u03c1 from 0.05 to 0.8. Then, for each O, we draw a graph of the RMSE protocol as a function of \u03c1. We used the parameters given in Section 5.3. A Gaussian noise was added to all synthetic datasets. The results are presented in Fig. 3. (We have not added any designations to the axis, since y values are measured relative to the competitive methods, instead of their solution values and axis x is a value clearly visible from the context.) Figure 3 is a graph table in which the (, j-input values are measured relative to the competitive methods, because y-values are measured as a value, instead of their solution values and axis x is a value clearly visible from the context.) The graph table in which the (, j-input values are measured as a pair of two points."}, {"heading": "5.7 Experiment 3", "text": "As explained in section 4, the GPR model generates a distribution of the prediction, with high variance 2 * \u03c3 implying that * x is an anomaly. In this experiment, we evaluate the GPR model as an anomaly detector on synthetic datasets. We trained a GPR model for the Swiss Roll and Toroidial Helix datasets generated using the Diffusion Maps method (note that we repeat the same experiment for the other multiple learning methods and achieve the same result), then preserved the 2D view of the first two main dimensions (by fixing the third dimension) and delimited them by a rectangle to form a test set for each dataset. Figure 4, top right and bottom right, show heatmaps created using 2 2 * * 1 () dii H \u03c3 = = \u2212 Circular shape for the toroidal helix or SwissRoll. As we can see, the heatmaps represent the heatmaps for both data sets and geometric H."}, {"heading": "5.8 Experiment 4", "text": "We are experimenting with our proposed models for an anomaly in the DARPA evaluation file [19]. Each instance in this database has 14 attributes based on network traffic. Each instance is associated with a standard network attack and is flagged accordingly."}, {"heading": "6. Conclusion", "text": "In this paper, we proposed a non-parametric Bayesian approach to OOSE, based on GPR. We analyzed the relationship between Nystrom extension and GPR and showed that the former is a special case for the latter. We validated our proposed method in a series of experiments that demonstrated its performance and compared it with other OOSE methods. In addition, we demonstrated how to detect anomalies using a trained GPR model and presented experimental results on both synthetic and real data sets. In the future, we plan to study advanced models such as student-t processes [8] for robust Bayesian regression. We also plan to investigate the performance of Relevance Vector machines (RVMs) [24] for sparse Bayesian regression and to understand whether accurate predictions can be achieved with a minimal subset of the entire training set. This could increase the computational complexity of the training phase, but not least parametric ones."}], "references": [{"title": "Dimensionality reduction: A comparative review.", "author": ["van der Maaten", "Laurens JP", "Eric O. Postma", "H. Jaap van den Herik"], "venue": "Journal of Machine Learning Research", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Plots of the toroidal helix (top) and swiss roll (bottom) views along with their corresponding heatmaps visualizing the negative variance of predictions", "author": ["Bengio", "Yoshua", "Jean-Fran\u00e7ois Paiement", "Pascal Vincent"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Diffusion maps and geometric harmonics.", "author": ["Lafon", "St\u00e9phane S"], "venue": "PhD diss., Yale University,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Multiscale data sampling and function extension.", "author": ["Bermanis", "Amit", "Amir Averbuch", "Ronald R. Coifman"], "venue": "Applied and Computational Harmonic Analysis 34,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "PCA-Based Out-of-Sample Extension for Dimensionality Reduction.", "author": ["Aizenbud", "Yariv", "Amit Bermanis", "Amir Averbuch"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Auto-adaptative Laplacian Pyramids for High-dimensional Data Analysis.", "author": ["Fern\u00e1ndez", "\u00c1ngela", "Neta Rabin", "Dalia Fishelov", "Jos\u00e9 R. Dorronsoro"], "venue": "arXiv preprint arXiv:1311.6594", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A generalised solution to the out-of-sample extension problem in manifold learning.", "author": ["Strange", "Harry", "Reyer Zwiggelaar"], "venue": "In Twenty-Fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Gaussian processes for machine learning.", "author": ["Rasmussen", "Carl Edward"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines.", "author": ["Williams", "Christopher", "Matthias Seeger"], "venue": "In Proceedings of the 14th Annual Conference on Neural Information Processing Systems, no", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering.", "author": ["Belkin", "Mikhail", "Partha Niyogi"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "A global geometric framework for nonlinear dimensionality reduction.", "author": ["Tenenbaum", "Joshua B", "Vin De Silva", "John C. Langford"], "venue": "Science 290,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Nonlinear dimensionality reduction by locally linear embedding.", "author": ["Roweis", "Sam T", "Lawrence K. Saul"], "venue": "Science 290,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Gaussian process kernels for pattern discovery and extrapolation.", "author": ["Wilson", "Andrew Gordon", "Ryan Prescott Adams"], "venue": "arXiv preprint arXiv:1302.4245", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Hessian eigenmaps: Locally linear embedding techniques for highdimensional data.", "author": ["Donoho", "David L", "Carrie Grimes"], "venue": "Proceedings of the National Academy of Sciences 100, no", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Gradient-based learning applied to document recognition.", "author": ["LeCun", "Yann", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE 86, no", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Statistical learning theory", "author": ["Vapnik", "Vladimir Naumovich", "Vlamimir Vapnik"], "venue": "New York: Wiley,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Evaluating intrusion detection systems: The 1998 DARPA off-line intrusion detection evaluation.", "author": ["Lippmann", "Richard P", "David J. Fried", "Isaac Graf", "Joshua W. Haines", "Kristopher R. Kendall", "David McClung", "Dan Weber"], "venue": "In DARPA Information Survivability Conference and Exposition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Local and Global Regressive Mapping for Manifold Learning with Out-of-Sample Extrapolation.", "author": ["Yang", "Yi", "Feiping Nie", "Shiming Xiang", "Yueting Zhuang", "Wenhua Wang"], "venue": "In Twenty-Fourth AAAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Qui\u00f1onero-Candela. \"Local distance preservation in the GP-LVM through back constraints.", "author": ["Lawrence", "Neil D", "Joaquin"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Scalable sparse subspace clustering.", "author": ["Peng", "Xi", "Lei Zhang", "Zhang Yi"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Connecting the out-of-sample and pre-image problems in kernel methods.", "author": ["Arias", "Pablo", "Gregory Randall", "Guillermo Sapiro"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Sparse Bayesian learning and the relevance vector machine.\" The journal of machine learning research", "author": ["Tipping", "Michael E"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "These algorithms attempt to discover the low dimensional manifold that the data points have been sampled from [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 2, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 3, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 4, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 5, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 6, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 19, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 20, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 7, "context": "In this paper, we propose a general framework for OOSE which is based on Gaussian Process Regression (GPR) [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 8, "context": "We analyze the mathematical connection between the proposed method and the Nystrom extension [9] and show that the latter is a special case of the former.", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "Section 4 describes the proposed method and discusses its connection to the Nystrom extension [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "[2] proposes extensions for several well Gaussian Process Regression for Out-of-Sample Extension", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "2 known manifold learning algorithms: Laplacian Eigenmaps (LE) [10], ISOMAP [11], Locally Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "2 known manifold learning algorithms: Laplacian Eigenmaps (LE) [10], ISOMAP [11], Locally Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "2 known manifold learning algorithms: Laplacian Eigenmaps (LE) [10], ISOMAP [11], Locally Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "The extensions are based on the Nystrom extension [9], which has been widely used for manifold learning algorithms.", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "In [3] the authors propose to use the Nystrom extension of eigenfunctions of the kernel, however in order to maintain numerical stability, they used only the significant eigenvalues.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "[4] suggested to alleviate the aforementioned problem by introducing a method for extending functions using a coarse-to-fine hierarchy of the multiscale decomposition of a Gaussian kernel.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] suggested an extension for a new data point which is based on local Principal Component Analysis (PCA).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] proposes an extension of Laplacian Pyramids model that incorporates a modified Leave One Out Cross Validation (LOOCV), but avoids the large computational cost of the standard one.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "In [7], the authors proposed to extend the embedding to unseen samples by finding a rotation and scale transformations of the sample\u2019s nearest neighbors.", "startOffset": 3, "endOffset": 6}, {"referenceID": 17, "context": "[20] introduced a manifold learning technique that enables OOSE by using regularization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] showed how Gaussian Process Latent Variable models can be generalized through back-constraints (GPLVMBC) to preserve local geometries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] introduces a new kernel that can be used with Gaussian Processes in order to discover patterns to enable extrapolation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Contrary to [14], in this paper we stick to the traditional squared exponential covariance function that sometimes referred as Radial Basis Function (RBF) kernel.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "a closed form expression [8] exists for the predictive distribution", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "In the literature, this method is named as marginal likelihood [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "Fortunately, a close form expressions for LOOCV and its gradients exist [8] and the hyperparameters can be optimized with the Conjugate Gradient method.", "startOffset": 72, "endOffset": 75}, {"referenceID": 7, "context": "The main reason we chose this approach is that the marginal likelihood method is more prone to overfitting [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "Many manifolds learning methods are cast in the same framework [2], where the computation of the embedding of the training data points is obtained by eigendecomposition of a (normalized) kernel matrix.", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "Update ( ) j K and 2 \u03c3 (using LOOCV [8]).", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "Note that our evaluation is similar to the other previous OOSE works [4]-[7], except for the fact we add the parameter \u03c1 that challenges the evaluated methods with variable training set sizes.", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "Note that our evaluation is similar to the other previous OOSE works [4]-[7], except for the fact we add the parameter \u03c1 that challenges the evaluated methods with variable training set sizes.", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "The methods are: Multiscale extension [4], Adaptive Laplacian Pyramids (ALP) [6] and PCA based OOSE (POOS) [5].", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "The methods are: Multiscale extension [4], Adaptive Laplacian Pyramids (ALP) [6] and PCA based OOSE (POOS) [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "The methods are: Multiscale extension [4], Adaptive Laplacian Pyramids (ALP) [6] and PCA based OOSE (POOS) [5].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "We evaluate the performance of the OOSE methods for several well-known manifold learning algorithms: Diffusion Maps (DM) [3], ISOMAP [11], Laplacian Eigenmaps (LE) [10], Local Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 121, "endOffset": 124}, {"referenceID": 10, "context": "We evaluate the performance of the OOSE methods for several well-known manifold learning algorithms: Diffusion Maps (DM) [3], ISOMAP [11], Laplacian Eigenmaps (LE) [10], Local Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 9, "context": "We evaluate the performance of the OOSE methods for several well-known manifold learning algorithms: Diffusion Maps (DM) [3], ISOMAP [11], Laplacian Eigenmaps (LE) [10], Local Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 164, "endOffset": 168}, {"referenceID": 11, "context": "We evaluate the performance of the OOSE methods for several well-known manifold learning algorithms: Diffusion Maps (DM) [3], ISOMAP [11], Laplacian Eigenmaps (LE) [10], Local Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 200, "endOffset": 204}, {"referenceID": 10, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 2, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 188, "endOffset": 191}, {"referenceID": 10, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 199, "endOffset": 203}, {"referenceID": 14, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 211, "endOffset": 215}, {"referenceID": 15, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 225, "endOffset": 229}, {"referenceID": 10, "context": "To this end, we created a Swiss roll [11] with 1000 data points (Fig.", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "The ALP seems to perform the worse, we conclude that it is due overfitting (in the ALP algorithm, a parameter is learnt from the training data and then used in test phase [6]).", "startOffset": 171, "endOffset": 174}, {"referenceID": 16, "context": "8 Experiment 4 We experimented with using our proposed model for an anomaly detection task on the DARPA Intrusion Detection Evaluation Data Set [19].", "startOffset": 144, "endOffset": 148}, {"referenceID": 0, "context": "7 First, each dimension in the training set was mapped to [0, 1] using a constant scale factor.", "startOffset": 58, "endOffset": 64}, {"referenceID": 3, "context": "This is on par with previous OOSE works such as [4]-[6].", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "This is on par with previous OOSE works such as [4]-[6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 7, "context": "In future, we plan to investigate advanced models such as Student t processes [8] for robust Bayesian regression.", "startOffset": 78, "endOffset": 81}, {"referenceID": 21, "context": "We also plan to explore the performance of Relevance Vector Machines (RVMs) [24] for sparse Bayesian regression and understand whether accurate predictions can be achieved using a minimal subset of the entire training set.", "startOffset": 76, "endOffset": 80}], "year": 2016, "abstractText": "Manifold learning methods are useful for high dimensional data analysis. Many of the existing methods produce a low dimensional representation that attempts to describe the intrinsic geometric structure of the original data. Typically, this process is computationally expensive and the produced embedding is limited to the training data. In many real life scenarios, the ability to produce embedding of unseen samples is essential. In this paper we propose a Bayesian non-parametric approach for out-of-sample extension. The method is based on Gaussian Process Regression and independent of the manifold learning algorithm. Additionally, the method naturally provides a measure for the degree of abnormality for a newly arrived data point that did not participate in the training process. We derive the mathematical connection between the proposed method and the Nystrom extension and show that the latter is a special case of the former. We present extensive experimental results that demonstrate the performance of the proposed method and compare it to other existing out-of-sample extension methods.", "creator": "PScript5.dll Version 5.2.2"}}}