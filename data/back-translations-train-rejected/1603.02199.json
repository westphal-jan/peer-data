{"id": "1603.02199", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2016", "title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection", "abstract": "We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.", "histories": [["v1", "Mon, 7 Mar 2016 18:53:00 GMT  (5414kb,D)", "http://arxiv.org/abs/1603.02199v1", null], ["v2", "Thu, 24 Mar 2016 23:01:46 GMT  (6485kb,D)", "http://arxiv.org/abs/1603.02199v2", "revised related work, added additional experiments to evaluate data requirements"], ["v3", "Sat, 2 Apr 2016 23:50:24 GMT  (6485kb,D)", "http://arxiv.org/abs/1603.02199v3", "fixing some malformed citations"], ["v4", "Sun, 28 Aug 2016 23:32:37 GMT  (6486kb,D)", "http://arxiv.org/abs/1603.02199v4", "This is an extended version of \"Learning Hand-Eye Coordination for Robotic Grasping with Large-Scale Data Collection,\" ISER 2016. Draft modified to correct typo in Algorithm 1 and add a link to the publicly available dataset"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.RO", "authors": ["sergey levine", "peter pastor", "alex krizhevsky", "deirdre quillen"], "accepted": false, "id": "1603.02199"}, "pdf": {"name": "1603.02199.pdf", "metadata": {"source": "META", "title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection", "authors": ["Sergey Levine", "Peter Pastor", "Deirdre Quillen"], "emails": ["SLEVINE@GOOGLE.COM", "PETERPASTOR@GOOGLE.COM", "AKRIZHEVSKY@GOOGLE.COM", "DEQUILLEN@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that most of them are in a position to go into another world, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in"}, {"heading": "2. Related Work", "text": "It is not only a matter of expression, but also a matter of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of expression, of, of expression, of expression, of expression, of, of, of expression, of, of expression, of, of, of expression, of, of, of expression, of expression, of, of expression, of, of, of expression, of, of, of expression, of, of, of, of expression, of, of, of, of, of, of expression"}, {"heading": "3. Overview", "text": "Our approach to learning hand-eye coordination for capture consists of two parts. Although the first part is a prediction network g (It, vt) that accepts visual input and issues a command via task-spacemotion and issues the predicted probability that executing the command will produce a successful capture, the second part is a prediction function f (It) that uses the prediction network to continuously control the robot to make the gripper a success. We describe each of these components in the following: Section 4.1 formally defines the task solved by the prediction network, and describes the network architecture, Section 4.2 describes how the prediction function can use the prediction network to perform a continuous control. By splitting the hand-eye coordination system into components, we can use the CNN predictor using a standard supervised learning target training to optimize the capture and deployment mechanism."}, {"heading": "4. Grasping with Convolutional Networks and Continuous Servoing", "text": "In this section, we discuss each component of our approach, including a description of the architecture of neural networks and the servo-mechanism, and conclude with an interpretation of the method as a form of reinforcement learning, including the corresponding assumptions about the structure of the decision problem."}, {"heading": "4.1. Grasp Success Prediction with Convolutional Neural Networks", "text": "In fact, most of them will be able to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to not find themselves, to not find themselves, to not find themselves, to not find themselves, to not find themselves, to not find themselves, to not find themselves, to not find themselves, to not find oneself, to not find oneself, to not find oneself, to not find oneself, to not find oneself."}, {"heading": "4.2. Continuous Servoing", "text": "In this section, we describe the servoing mechanism f (Es) that uses the rampant prediction network to select the motor commands for the robot that maximizes the likelihood of success. The simplest way to do this is to detect a series of command and application errors and then evaluate them with the highest probability of success. However, we can get better results by performing a small optimization on vt that we perform using the Cross-Entropy Method (CEM) (CEM). CEM is a simple derivative-free optimization algorithm that matches a batch of N values to each iteration, fits a Gaussian distribution on M < N from these examples, and then a new batch from this batch."}, {"heading": "4.3. Interpretation as Reinforcement Learning", "text": "An interesting conceptual question that our approach raises is the relationship between the training of the grass prediction network and fastening learning. In the case where T = 2 and only one decision is made by the servo mechanism, the grass network can be seen as an approximation of the Q function for the policy defined by the servo mechanism f (It), and a reward function that is 1 if the grass is successful and 0 otherwise. In this case, the use of the newest grass network g (It, vt), gathering additional data and reciting g (It, vt) would then be regarded as an adapted Q iteration (Antos et al., 2008)."}, {"heading": "5. Large-Scale Data Collection", "text": "To collect training data to train the prediction network g (It, vt), we used between 6 and 14 robotic manipulators at a given time. An illustration of our data acquisition is shown in Figure 1. This section describes the robots used in our data acquisition process, as well as details of the data acquisition process."}, {"heading": "5.1. Hardware Setup", "text": "Our robot manipulator platform consists of a lightweight 7-degree freedom arm, a compatible, under-operated two-finger gripper, and a camera mounted behind the arm and looking over the shoulder. Figure 5 shows the imaging of a single robot. The under-operated gripper ensures some consistency in strangely shaped objects, at the expense of a loose, slippery grip. An interesting feature of this gripper was the irregular nature of material and tearing during the data acquisition process, which took several months. Images of the grippers of various robots can be seen in Figure 7 and illustrate the variation in gripper wear and geometry. In addition, the cameras were mounted at slightly different angles, offering a different angle for each robot. Views of the cameras of all 14 robots during data acquisition are shown in Figure 6."}, {"heading": "5.2. Data Collection", "text": "The only human intervention in the data acquisition process was to replace the object in the containers in front of the robots and turn the system on. Data collection began with random motor commands and T = 3,2 When executing completely random motor commands, the robots were successful in 10% - 30% of gripping attempts, depending on the respective objects in front of them. About half of the data set was captured using random grass, and the rest used the latest network suitable for all the data collected so far. In the course of data collection, we updated the network four times and increased the number of steps from T = 3 at the beginning to T = 10. The gripping objects were selected from ordinary household and office objects and moved along the longest axis from a 4 to 20 cm long gripper. Some of these objects are shown in Figure 6."}, {"heading": "6. Experiments", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "7. Discussion and Future Work", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "Acknowledgements", "text": "We would like to thank Kurt Konolige and Mrinal Kalakrishnan for additional engineering and insightful discussions, Jed Hewitt, Don Jordan and Aaron Wei\u00df for helping maintain the robots, Max Bajracharya and Nicolas Hudson for providing a pipeline for basic perception, and Vincent Vanhoucke and Jeff Dean for providing support and organization."}, {"heading": "A. Servoing Implementation Details", "text": "In this appendix, we discuss the details of the follow-up procedure we use to derive the motoric command vt with the highest probability of success, as well as additional details of the servo-mechanism. In our implementation, we performed conclusions based on three iterations of the Cross-Entropy Method (CEM). Each iteration of the CEM consists of scanning 64 detection directions from a Gaussian distribution with average \u00b5 and covariance \u03a3, selecting the 6 best detection directions (i.e. the 90th percentile) and re-assigning these 6 best detection directions. The first iteration samples consist of a zero-mean-Gaussian distribution with the current pose of the gripper. All samples are limited (via repulsion sampling) to hold the final pose of the gripper within the working space, and to avoid rotations of more than 180 \u00b0 around the vertical axis. Generally, these constraints could be used to control the gripper where we are trying to seize the scene with the gripper (before the robot tries to)."}, {"heading": "B. Details of Hand-Engineered Grasping System Baseline", "text": "The basic results of the handcrafted gripper system mentioned in Table 1 were obtained using a perception pipeline using the depth sensor instead of the monocular camera and requiring an extrinsic calibration of the camera to the base of the arm. Gripper configurations were calculated as follows: First, the depth sensor's point clouds were combined into a voxel map; second, the voxel map was converted into a 3D graphic and segmented using standard diagram-based segmentation; then, the individual clusters were further segmented from top to bottom into \"tangible objects\" based on the width and height of the region; and finally, an optimal grip was calculated that centrally aligned the fingers along the longer edges of the delimitation box representing the object. This gripper configuration was then used as the target position for a taskspace controller identical to the open-base-loop controller."}], "references": [{"title": "Fitted Q-Iteration in Continuous Action-Space MDPs", "author": ["A. Antos", "C. Szepesvari", "R. Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Antos et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Antos et al\\.", "year": 2008}, {"title": "Photometric visual servoing for omnidirectional cameras", "author": ["G. Caron", "E. Marchand", "E. Mouaddib"], "venue": "Autonoumous Robots,", "citeRegEx": "Caron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Caron et al\\.", "year": 2013}, {"title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs", "author": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv preprint arXiv:1412.7062,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A New Approach to Visual Servoing in Robotics", "author": ["B. Espiau", "F. Chaumette", "P. Rives"], "venue": "IEEE Transactions on Robotics and Automation,", "citeRegEx": "Espiau et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Espiau et al\\.", "year": 1992}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "The Columbia Grasp Database", "author": ["C. Goldfeder", "M. Ciocarlie", "H. Dang", "P.K. Allen"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Goldfeder et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goldfeder et al\\.", "year": 2009}, {"title": "Data-Driven Grasping with Partial Sensor Data", "author": ["C. Goldfeder", "M. Ciocarlie", "J. Peretzman", "H. Dang", "P.K. Allen"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Goldfeder et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goldfeder et al\\.", "year": 2009}, {"title": "Combined Shape, Appearance and Silhouette for Simultaneous Manipulator and Object Tracking", "author": ["P. Hebert", "N. Hudson", "J. Ma", "T. Howard", "T. Fuchs", "M. Bajracharya", "J. Burdick"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Hebert et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hebert et al\\.", "year": 2012}, {"title": "Learning of Grasp Selection based on Shape-Templates", "author": ["A. Herzog", "P. Pastor", "M. Kalakrishnan", "L. Righetti", "J. Bohg", "T. Asfour", "S. Schaal"], "venue": "Autonomous Robots,", "citeRegEx": "Herzog et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Herzog et al\\.", "year": 2014}, {"title": "End-to-End Dexterous Manipulation with Deliberate Interactive Estimation", "author": ["N. Hudson", "T. Howard", "J. Ma", "A. Jain", "M. Bajracharya", "S. Myint", "C. Kuo", "L. Matthies", "P. Backes", "P. Hebert"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Hudson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hudson et al\\.", "year": 2012}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Experimental Evaluation of Uncalibrated Visual Servoing for Precision Manipulation", "author": ["M. J\u00e4gersand", "O. Fuentes", "R.C. Nelson"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "J\u00e4gersand et al\\.,? \\Q1997\\E", "shortCiteRegEx": "J\u00e4gersand et al\\.", "year": 1997}, {"title": "Leveraging Big Data for Grasp Planning", "author": ["D. Kappler", "B. Bohg", "S. Schaal"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Kappler et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kappler et al\\.", "year": 2015}, {"title": "Survey on Visual Servoing for Manipulation", "author": ["D. Kragic", "H.I. Christensen"], "venue": "Computational Vision and Active Perception Laboratory,", "citeRegEx": "Kragic and Christensen,? \\Q2002\\E", "shortCiteRegEx": "Kragic and Christensen", "year": 2002}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Acquiring Visual Servoing Reaching and Grasping Skills using Neural Reinforcement Learning", "author": ["T. Lampe", "M. Riedmiller"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Lampe and Riedmiller,? \\Q2013\\E", "shortCiteRegEx": "Lampe and Riedmiller", "year": 2013}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The Handbook of Brain Theory and Neural Networks,", "citeRegEx": "LeCun and Bengio,? \\Q1995\\E", "shortCiteRegEx": "LeCun and Bengio", "year": 1995}, {"title": "Using Near-Field Stereo Vision for Robotic Grasping in Cluttered Environments", "author": ["A. Leeper", "K. Hsiao", "E. Chu", "J.K. Salisbury"], "venue": "In Experimental Robotics,", "citeRegEx": "Leeper et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Leeper et al\\.", "year": 2014}, {"title": "Deep Learning for Detecting Robotic Grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Lenz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lenz et al\\.", "year": 2015}, {"title": "End-to-end Training of Deep Visuomotor Policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T. Lillicrap", "J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V Mnih"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih,? \\Q2015\\E", "shortCiteRegEx": "Mnih", "year": 2015}, {"title": "Vision Based Control of a Quadrotor for Perching on Planes and Lines", "author": ["K. Mohta", "V. Kumar", "K. Daniilidis"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Mohta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mohta et al\\.", "year": 2014}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot", "author": ["L. Pinto", "A. Gupta"], "venue": "hours. CoRR,", "citeRegEx": "Pinto and Gupta,? \\Q2015\\E", "shortCiteRegEx": "Pinto and Gupta", "year": 2015}, {"title": "Real-time grasp detection using convolutional neural networks", "author": ["J. Redmon", "A. Angelova"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Redmon and Angelova,? \\Q2015\\E", "shortCiteRegEx": "Redmon and Angelova", "year": 2015}, {"title": "From Caging to Grasping", "author": ["A. Rodriguez", "M.T. Mason", "S. Ferry"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Rodriguez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rodriguez et al\\.", "year": 2012}, {"title": "The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning", "author": ["R. Rubinstein", "D. Kroese"], "venue": null, "citeRegEx": "Rubinstein and Kroese,? \\Q2004\\E", "shortCiteRegEx": "Rubinstein and Kroese", "year": 2004}, {"title": "Visual Servoing for Humanoid Grasping and Manipulation Tasks", "author": ["N. Vahrenkamp", "S. Wieland", "P. Azad", "D. Gonzalez", "T. Asfour", "R. Dillmann"], "venue": "In 8th IEEE-RAS International Conference on Humanoid Robots,", "citeRegEx": "Vahrenkamp et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vahrenkamp et al\\.", "year": 2008}, {"title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images", "author": ["M. Watter", "J. Springenberg", "J. Boedecker", "M. Riedmiller"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Watter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watter et al\\.", "year": 2015}, {"title": "Pose Error Robust Grasping from Contact Wrench Space Metrics", "author": ["J. Weisz", "P.K. Allen"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Weisz and Allen,? \\Q2012\\E", "shortCiteRegEx": "Weisz and Allen", "year": 2012}, {"title": "Relative End-Effector Control Using Cartesian Position Based Visual Servoing", "author": ["W.J. Wilson", "Hulls", "C.W. Williams", "G.S. Bell"], "venue": "IEEE Transactions on Robotics and Automation,", "citeRegEx": "Wilson et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 1996}, {"title": "Learning Descriptors for Object Recognition and 3D Pose Estimation", "author": ["P. Wohlhart", "V. Lepetit"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Wohlhart and Lepetit,? \\Q2015\\E", "shortCiteRegEx": "Wohlhart and Lepetit", "year": 2015}, {"title": "Active, uncalibrated visual servoing", "author": ["B.H. Yoshimi", "P.K. Allen"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Yoshimi and Allen,? \\Q1994\\E", "shortCiteRegEx": "Yoshimi and Allen", "year": 1994}], "referenceMentions": [{"referenceID": 25, "context": "Geometric methods analyze the shape of a target object and plan a suitable grasp pose, based on criteria such as force closure (Weisz & Allen, 2012) or caging (Rodriguez et al., 2012).", "startOffset": 159, "endOffset": 183}, {"referenceID": 8, "context": "Data-driven methods take a variety of different forms, including purely human-supervised methods that predict grasp configurations (Herzog et al., 2014; Lenz et al., 2015) and methods that predict finger placement from geometric criteria computed offline (Goldfeder et al.", "startOffset": 131, "endOffset": 171}, {"referenceID": 18, "context": "Data-driven methods take a variety of different forms, including purely human-supervised methods that predict grasp configurations (Herzog et al., 2014; Lenz et al., 2015) and methods that predict finger placement from geometric criteria computed offline (Goldfeder et al.", "startOffset": 131, "endOffset": 171}, {"referenceID": 12, "context": "Both types of data-driven grasp selection have recently incorporated deep learning (Kappler et al., 2015; Lenz et al., 2015; Redmon & Angelova, 2015).", "startOffset": 83, "endOffset": 149}, {"referenceID": 18, "context": "Both types of data-driven grasp selection have recently incorporated deep learning (Kappler et al., 2015; Lenz et al., 2015; Redmon & Angelova, 2015).", "startOffset": 83, "endOffset": 149}, {"referenceID": 9, "context": "Feedback has been incorporated into grasping primarily as a way to achieve the desired forces for force closure and other dynamic grasping criteria (Hudson et al., 2012), as well as in the form of standard servoing mechanisms, including visual servoing (described below) to servo the gripper to a pre-planned grasp pose (Kragic & Christensen, 2002).", "startOffset": 148, "endOffset": 169}, {"referenceID": 27, "context": "Comparatively little prior work has addressed direct visual feedback for grasping, most of which requires manually designed features to track the endeffector (Vahrenkamp et al., 2008; Hebert et al., 2012).", "startOffset": 158, "endOffset": 204}, {"referenceID": 7, "context": "Comparatively little prior work has addressed direct visual feedback for grasping, most of which requires manually designed features to track the endeffector (Vahrenkamp et al., 2008; Hebert et al., 2012).", "startOffset": 158, "endOffset": 204}, {"referenceID": 12, "context": "objects, which is more than an order of magnitude larger than prior methods based on direct self-supervision (Pinto & Gupta, 2015) and more than double the dataset size of prior methods based on synthetic grasps from 3D scans (Kappler et al., 2015).", "startOffset": 226, "endOffset": 248}, {"referenceID": 3, "context": "In contrast to our approach, visual servoing methods are typically concerned with reaching a target pose relative to objects in the scene, and often (though not always) rely on manually designed or specified features for feedback control (Espiau et al., 1992; Wilson et al., 1996; Vahrenkamp et al., 2008; Hebert et al., 2012; Mohta et al., 2014).", "startOffset": 238, "endOffset": 346}, {"referenceID": 30, "context": "In contrast to our approach, visual servoing methods are typically concerned with reaching a target pose relative to objects in the scene, and often (though not always) rely on manually designed or specified features for feedback control (Espiau et al., 1992; Wilson et al., 1996; Vahrenkamp et al., 2008; Hebert et al., 2012; Mohta et al., 2014).", "startOffset": 238, "endOffset": 346}, {"referenceID": 27, "context": "In contrast to our approach, visual servoing methods are typically concerned with reaching a target pose relative to objects in the scene, and often (though not always) rely on manually designed or specified features for feedback control (Espiau et al., 1992; Wilson et al., 1996; Vahrenkamp et al., 2008; Hebert et al., 2012; Mohta et al., 2014).", "startOffset": 238, "endOffset": 346}, {"referenceID": 7, "context": "In contrast to our approach, visual servoing methods are typically concerned with reaching a target pose relative to objects in the scene, and often (though not always) rely on manually designed or specified features for feedback control (Espiau et al., 1992; Wilson et al., 1996; Vahrenkamp et al., 2008; Hebert et al., 2012; Mohta et al., 2014).", "startOffset": 238, "endOffset": 346}, {"referenceID": 22, "context": "In contrast to our approach, visual servoing methods are typically concerned with reaching a target pose relative to objects in the scene, and often (though not always) rely on manually designed or specified features for feedback control (Espiau et al., 1992; Wilson et al., 1996; Vahrenkamp et al., 2008; Hebert et al., 2012; Mohta et al., 2014).", "startOffset": 238, "endOffset": 346}, {"referenceID": 1, "context": "Photometric visual servoing uses a target image rather than features (Caron et al., 2013), and several visual servoing methods have been proposed that do not directly require prior calibration between the robot and camera (Yoshimi & Allen, 1994; J\u00e4gersand et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 11, "context": ", 2013), and several visual servoing methods have been proposed that do not directly require prior calibration between the robot and camera (Yoshimi & Allen, 1994; J\u00e4gersand et al., 1997; Kragic & Christensen, 2002).", "startOffset": 140, "endOffset": 215}, {"referenceID": 14, "context": "Although the technology behind CNNs has been known for decades (LeCun & Bengio, 1995), they have achieved remarkable success in recent years on a wide range of challenging computer vision benchmarks (Krizhevsky et al., 2012), becoming the de facto standard for computer vision systems.", "startOffset": 199, "endOffset": 224}, {"referenceID": 14, "context": "However, applications of CNNs to robotic control problems has been less prevalent, compared to applications to passive perception tasks such as object recognition (Krizhevsky et al., 2012; Wohlhart & Lepetit, 2015), localization (Girshick et al.", "startOffset": 163, "endOffset": 214}, {"referenceID": 4, "context": ", 2012; Wohlhart & Lepetit, 2015), localization (Girshick et al., 2014), and segmentation (Chen et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 2, "context": ", 2014), and segmentation (Chen et al., 2014).", "startOffset": 26, "endOffset": 45}, {"referenceID": 28, "context": ", 2015), executing simple task-space motions for visual servoing (Lampe & Riedmiller, 2013), controlling simple simulated robotic systems (Watter et al., 2015; Lillicrap et al., 2016), and performing a variety of robotic manipulation tasks (Levine et al.", "startOffset": 138, "endOffset": 183}, {"referenceID": 20, "context": ", 2015), executing simple task-space motions for visual servoing (Lampe & Riedmiller, 2013), controlling simple simulated robotic systems (Watter et al., 2015; Lillicrap et al., 2016), and performing a variety of robotic manipulation tasks (Levine et al.", "startOffset": 138, "endOffset": 183}, {"referenceID": 19, "context": ", 2016), and performing a variety of robotic manipulation tasks (Levine et al., 2015).", "startOffset": 64, "endOffset": 85}, {"referenceID": 0, "context": "Repeatedly deploying the latest grasp network g(It,vt), collecting additional data, and refitting g(It,vt) can then be regarded as fitted Q iteration (Antos et al., 2008).", "startOffset": 150, "endOffset": 170}, {"referenceID": 17, "context": "systems use a wrist-mounted camera to address this difficulty (Leeper et al., 2014).", "startOffset": 62, "endOffset": 83}], "year": 2016, "abstractText": "We describe a learning-based approach to handeye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.", "creator": "LaTeX with hyperref package"}}}