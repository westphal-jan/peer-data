{"id": "1603.02250", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2016", "title": "Online Sparse Linear Regression", "abstract": "We consider the online sparse linear regression problem, which is the problem of sequentially making predictions observing only a limited number of features in each round, to minimize regret with respect to the best sparse linear regressor, where prediction accuracy is measured by square loss. We give an inefficient algorithm that obtains regret bounded by $\\tilde{O}(\\sqrt{T})$ after $T$ prediction rounds. We complement this result by showing that no algorithm running in polynomial time per iteration can achieve regret bounded by $O(T^{1-\\delta})$ for any constant $\\delta &gt; 0$ unless $\\text{NP} \\subseteq \\text{BPP}$. This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013). This hardness result holds even if the algorithm is allowed to access more features than the best sparse linear regressor up to a logarithmic factor in the dimension.", "histories": [["v1", "Mon, 7 Mar 2016 20:49:52 GMT  (12kb)", "http://arxiv.org/abs/1603.02250v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dean foster", "satyen kale", "howard karloff"], "accepted": false, "id": "1603.02250"}, "pdf": {"name": "1603.02250.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Satyen Kale"], "emails": ["dean@foster.net", "satyen@yahoo-inc.com", "howard@cc.gatech.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.02 250v 1 [cs.L G] 7M ar \u221a T) after T prediction rounds. We supplement this result by showing that no algorithm running in polynomial time per iteration can achieve a remorse for any constant \u03b4 > 0 that is limited by O (T 1 \u2212 \u03b4) unless NP BPP. This calculation result solves an open problem presented in COLT 2014 (Kale, 2014) and also provided by Zolghadr et al. (2013). This hardness result applies even if the algorithm is allowed to access more features than the best sparse linear regressor up to a logarithmic factor in dimension."}, {"heading": "1 Introduction", "text": "In various real-world scenarios, characteristics are constructed for examples by executing some mathematically expensive algorithms. With resource constraints, it is essential to make predictions with only a limited number of characteristics calculated per example. An example of this scenario, from (Cesa-Bianchi et al., 2011), is the medical diagnosis of a disease in which each characteristic corresponds to a medical test that the patient can challenge. Obviously, it is undesirable to subject a patient to a battery of medical tests, both for medical and cost reasons. Another example from the same paper is a search engine in which a ranking of websites must be generated for each incoming user query and the limited time allowed to respond to a query imposing limitations on the number of attributes that can be evaluated in the process. In both of these problems, predictions must be made sequentially as patients or searches online."}, {"heading": "2 Related Work and Known Results", "text": "A related setting is attribute-efficient learning (Cesa-Bianchi et al., 2011; Hazan and Koren, 2012; Kukliansky and Shamir, 2015). This is a batch learning problem in which the examples are generated online i.i.d., and the goal is simply to output a linear regressor that uses only a limited number of features per example with limited excess risk, compared to the optimal linear regressor if we get full access to the features in the test period. While the above papers provide efficient, near-optimal algorithms for this problem, these algorithms do not work in the online thrift regression setting in which we are interested, because here we only need to make predictions using a limited number of features. In (Kale, 2014) a simple algorithm based on the execution of a bandit algorithm, in which the actions are performed with the selection of a regular subset of each coordinate is executed within the coordinate."}, {"heading": "3 Notation and Setup", "text": "We use the notation [d] = {1, 2,., d} to refer to the coordinates. (All vectors in this work are in Rd, and all matrices in Rd \u00b7 d.) We also use the IS notation to denote the diagonal matrix, which is indexed in the coordinates of S and zeros. (This is the identity matrix on the subspace of Rd, which is induced by the coordinates in S, as well as the projection matrix for this subspace.) We use the IS notation to denote the diagonals in the coordinates of S and zeros otherwise. (This is the identity matrix on the subspace of Rd, which is induced by the coordinates in S, as the projection matrix for this subspace. (We use the notation from the diagonal matrix, the one in the coordinates index.) We use the prediction of 2 norms in Rd and zero to consider the \"number of zero.\""}, {"heading": "4 Upper bound", "text": "In this section we give an inefficient algorithm for the (k) -on- (k) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) -on- (n) on- on- (n) on- (on- on- on- (n) on- on- (n) on- on- (n) on- on- (n) on- on- (n) on- (n) on- on- (n) on- on- (n) on- (n) on- on- (n) on- on- (n) on- on- (n) on- on- (n) on- on- (n) on- on- (n) on- on- (n) on- on- (n) on- on- (n) on- on- on- (n) on- on- (n) on- on- (n) on- on- (n) on- on- on- (n) on- on- (n) on- on- on- on- (n) on- (n) on- (n) on- on- (n) on- on- on- (n) on- on- (n) on- on- (n) on- on- on- on- (n (n) on- on- on- on- (n (n) on- (n) on- on- (n) on- (n) on- on- on- on- (n (n) on- on- on- on- on- (n (n) on- (n) on- on- on- on- on- on- on"}, {"heading": "5 Computational lower bound", "text": "In this section, we show that there can be no efficient no-regret algorithms for the online regression problem unless we have a constant regression problem. (This hardness results from the hardness with which we approach the set-cover problem.) We give a reduction, of which there is a small coverage, and in the other one each coverage is large. This task is known to be NP-hard for specific parameter values. Specifically, our reduction has the following properties: 1. If there is a small coverage, then there is a reproduced online regression problem in which there is a sparse coverage. (This task is known to come to specific parameter values), our reduction has the following properties: 1. If there is a small coverage, then there is a reproduced online regression problem, then a sparse parameter vector (of 2 norm), which gives 0, and thus the algorithms cause a small loss."}, {"heading": "6 Conclusions", "text": "In this paper, we demonstrate that it is mathematically difficult to minimize remorse in the online regression problem, even if the learner is granted many more traits than the comparator, a sparse linear regressor. We supplement this result with an inefficient no-regret algorithm. The most important open question that remains from this work is what additional assumptions can be made to the examples that arrive online to make the problem comprehensible."}], "references": [{"title": "The Probabilistic Method", "author": ["Noga Alon", "Joel Spencer"], "venue": "John Wiley,", "citeRegEx": "Alon and Spencer.,? \\Q1992\\E", "shortCiteRegEx": "Alon and Spencer.", "year": 1992}, {"title": "The multiplicative weights update method: a meta-algorithm and applications", "author": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": "Theory of Computing,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Efficient learning with partially observed attributes", "author": ["Nicol\u00f2 Cesa-Bianchi", "Shai Shalev-Shwartz", "Ohad Shamir"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2011}, {"title": "Analytical approach to parallel repetition", "author": ["Irit Dinur", "David Steurer"], "venue": "In STOC, pages 624\u2013633,", "citeRegEx": "Dinur and Steurer.,? \\Q2014\\E", "shortCiteRegEx": "Dinur and Steurer.", "year": 2014}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["Abraham Flaxman", "Adam Tauman Kalai", "H. Brendan McMahan"], "venue": "In SODA,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Variable selection is hard", "author": ["Dean Foster", "Howard Karloff", "Justin Thaler"], "venue": "In COLT,", "citeRegEx": "Foster et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2015}, {"title": "Linear regression with limited observation", "author": ["Elad Hazan", "Tomer Koren"], "venue": "In ICML,", "citeRegEx": "Hazan and Koren.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Koren.", "year": 2012}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Open problem: Efficient online sparse regression", "author": ["Satyen Kale"], "venue": "In COLT, pages 1299\u20131301,", "citeRegEx": "Kale.,? \\Q2014\\E", "shortCiteRegEx": "Kale.", "year": 2014}, {"title": "Attribute efficient linear regression with distributiondependent sampling", "author": ["Doron Kukliansky", "Ohad Shamir"], "venue": "In ICML,", "citeRegEx": "Kukliansky and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Kukliansky and Shamir.", "year": 2015}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM J. Computing,", "citeRegEx": "Natarajan.,? \\Q1995\\E", "shortCiteRegEx": "Natarajan.", "year": 1995}, {"title": "Online learning with costly features and labels", "author": ["Navid Zolghadr", "G\u00e1bor Bart\u00f3k", "Russell Greiner", "Andr\u00e1s Gy\u00f6rgy", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "Zolghadr et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zolghadr et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al.", "startOffset": 83, "endOffset": 95}, {"referenceID": 8, "context": "This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013). This hardness result holds even if the algorithm is allowed to access more features than the best sparse linear regressor up to a logarithmic factor in the dimension.", "startOffset": 84, "endOffset": 137}, {"referenceID": 2, "context": "One example of this scenario, from (Cesa-Bianchi et al., 2011), is medical diagnosis of a disease, in which each feature corresponds to a medical test that the patient in question can undergo.", "startOffset": 35, "endOffset": 62}, {"referenceID": 8, "context": "This computational hardness result resolves open problems from (Kale, 2014) and (Zolghadr et al.", "startOffset": 63, "endOffset": 75}, {"referenceID": 11, "context": "This computational hardness result resolves open problems from (Kale, 2014) and (Zolghadr et al., 2013).", "startOffset": 80, "endOffset": 103}, {"referenceID": 2, "context": "A related setting is attribute-efficient learning (Cesa-Bianchi et al., 2011; Hazan and Koren, 2012; Kukliansky and Shamir, 2015).", "startOffset": 50, "endOffset": 129}, {"referenceID": 6, "context": "A related setting is attribute-efficient learning (Cesa-Bianchi et al., 2011; Hazan and Koren, 2012; Kukliansky and Shamir, 2015).", "startOffset": 50, "endOffset": 129}, {"referenceID": 9, "context": "A related setting is attribute-efficient learning (Cesa-Bianchi et al., 2011; Hazan and Koren, 2012; Kukliansky and Shamir, 2015).", "startOffset": 50, "endOffset": 129}, {"referenceID": 8, "context": "In (Kale, 2014), a simple algorithm has been suggested, which is based on running a bandit algorithm in which the actions correspond to selecting one of ( d k )", "startOffset": 3, "endOffset": 15}, {"referenceID": 7, "context": "subsets of coordinates of size k at regular intervals, and within each interval, running an online regression algorithm (such as the Online Newton-Step algorithm of Hazan et al. (2007)) over the k coordinates chosen by the bandit algorithm.", "startOffset": 165, "endOffset": 185}, {"referenceID": 7, "context": "subsets of coordinates of size k at regular intervals, and within each interval, running an online regression algorithm (such as the Online Newton-Step algorithm of Hazan et al. (2007)) over the k coordinates chosen by the bandit algorithm. This algorithm, with the right choice of interval lengths, has a regret bound of O(kdT 2/3 log(T/d)). The algorithm has exponential dependence on k both in running time and the regret. Also, Kale (2014) sketches a different algorithm with performance guarantees similar to the algorithm presented in this paper; our work builds upon that sketch and gives tighter regret bounds.", "startOffset": 165, "endOffset": 444}, {"referenceID": 7, "context": "subsets of coordinates of size k at regular intervals, and within each interval, running an online regression algorithm (such as the Online Newton-Step algorithm of Hazan et al. (2007)) over the k coordinates chosen by the bandit algorithm. This algorithm, with the right choice of interval lengths, has a regret bound of O(kdT 2/3 log(T/d)). The algorithm has exponential dependence on k both in running time and the regret. Also, Kale (2014) sketches a different algorithm with performance guarantees similar to the algorithm presented in this paper; our work builds upon that sketch and gives tighter regret bounds. Zolghadr et al. (2013) consider a very closely related setting (called online probing) in which features and labels may be obtained by the learner at some cost (which may be dif-", "startOffset": 165, "endOffset": 642}, {"referenceID": 5, "context": "On the computational hardness side, it is known that it is NP-hard to compute the optimal sparse linear regressor (Foster et al., 2015; Natarajan, 1995).", "startOffset": 114, "endOffset": 152}, {"referenceID": 10, "context": "On the computational hardness side, it is known that it is NP-hard to compute the optimal sparse linear regressor (Foster et al., 2015; Natarajan, 1995).", "startOffset": 114, "endOffset": 152}, {"referenceID": 5, "context": "On the computational hardness side, it is known that it is NP-hard to compute the optimal sparse linear regressor (Foster et al., 2015; Natarajan, 1995). The hardness result in this paper is in fact inspired by the work of Foster et al. (2015), who proved that it is computationally hard to find even an approximately optimal sparse linear regressor for an ordinary least squares regression problem given a batch of labeled data.", "startOffset": 115, "endOffset": 244}, {"referenceID": 5, "context": "Given the NP-hardness of computing the optimal k-sparse linear regressor (Foster et al., 2015; Natarajan, 1995), we also consider a variant of the problem which gives the learner more flexibility than the comparator: the learner is allowed to choose k\u2032 \u2265 k coordinates to query in each round.", "startOffset": 73, "endOffset": 111}, {"referenceID": 10, "context": "Given the NP-hardness of computing the optimal k-sparse linear regressor (Foster et al., 2015; Natarajan, 1995), we also consider a variant of the problem which gives the learner more flexibility than the comparator: the learner is allowed to choose k\u2032 \u2265 k coordinates to query in each round.", "startOffset": 73, "endOffset": 111}, {"referenceID": 1, "context": "1 in (Arora et al., 2012)) on ( d k )", "startOffset": 5, "endOffset": 25}, {"referenceID": 4, "context": "1 in (Flaxman et al., 2005)) with the specified value of \u03b7SGD, we conclude that for any fixed vector w of l2 norm at most 1, we have,", "startOffset": 5, "endOffset": 27}, {"referenceID": 3, "context": "The starting point for our reduction is the work of Dinur and Steurer (2014) who give a polynomial-time reduction of deciding satisfiability of 3CNF formulas to distinguishing instances of Set Cover with certain useful combinatorial properties.", "startOffset": 52, "endOffset": 77}, {"referenceID": 0, "context": "1 in (Alon and Spencer, 1992)) to the martingale difference sequence Et[(yt \u2212 \u0177t) ] \u2212 (yt \u2212 \u0177t) for t = 1, 2, .", "startOffset": 5, "endOffset": 29}, {"referenceID": 3, "context": "The reduction of Dinur and Steurer (2014) can be \u201ctweaked\u201d so that the cD is arbitrarily close to 1 for any constant D.", "startOffset": 17, "endOffset": 42}, {"referenceID": 3, "context": "The reduction of Dinur and Steurer (2014) can be \u201ctweaked\u201d so that the cD is arbitrarily close to 1 for any constant D. We can now extend the hardness results to the parameter settings k = O(d) for any \u01eb \u2208 (0, 1) and k\u2032 = \u230aD ln(d)k\u230b either by tweaking the reduction of Dinur and Steurer (2014) so it yields cD = \u01eb if \u01eb is close enough to 1, or if \u01eb is small, by adding O(d ) all-zeros columns to the matrix M\u03c6.", "startOffset": 17, "endOffset": 294}], "year": 2016, "abstractText": "We consider the online sparse linear regression problem, which is the problem of sequentially making predictions observing only a limited number of features in each round, to minimize regret with respect to the best sparse linear regressor, where prediction accuracy is measured by square loss. We give an inefficient algorithm that obtains regret bounded by \u00d5( \u221a T ) after T prediction rounds. We complement this result by showing that no algorithm running in polynomial time per iteration can achieve regret bounded by O(T 1\u2212\u03b4) for any constant \u03b4 > 0 unless NP \u2286 BPP. This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013). This hardness result holds even if the algorithm is allowed to access more features than the best sparse linear regressor up to a logarithmic factor in the dimension.", "creator": "LaTeX with hyperref package"}}}