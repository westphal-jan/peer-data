{"id": "1603.02578", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2016", "title": "Batched Lazy Decision Trees", "abstract": "We introduce a batched lazy algorithm for supervised classification using decision trees. It avoids unnecessary visits to irrelevant nodes when it is used to make predictions with either eagerly or lazily trained decision trees. A set of experiments demonstrate that the proposed algorithm can outperform both the conventional and lazy decision tree algorithms in terms of computation time as well as memory consumption, without compromising accuracy.", "histories": [["v1", "Tue, 8 Mar 2016 16:36:31 GMT  (205kb,D)", "http://arxiv.org/abs/1603.02578v1", "7 pages, 2 figures, 3 tables, 3 algorithms"]], "COMMENTS": "7 pages, 2 figures, 3 tables, 3 algorithms", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mathieu guillame-bert", "artur dubrawski"], "accepted": false, "id": "1603.02578"}, "pdf": {"name": "1603.02578.pdf", "metadata": {"source": "CRF", "title": "Batched Lazy Decision Trees", "authors": ["Mathieu Guillame-Bert", "Artur Dubrawski"], "emails": ["mathieug@andrew.cmu.edu,", "awd@cs.cmu.edu"], "sections": [{"heading": null, "text": "We are introducing a stack-by-stack rotten decision tree classification algorithm that avoids unnecessary visits to irrelevant nodes when used to make predictions with eagerly or lazily trained decision trees. A series of experiments show that the proposed algorithm can outperform both conventional and rotten decision tree algorithms in terms of computation time and memory usage without compromising accuracy."}, {"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush. \""}, {"heading": "2 Decision trees and lazy decision trees", "text": "(Bagged) eager decision trees work by growing a series of unpruned decision trees - each of them trained on a bootstrapped set of reference observations. Listing 1 shows a generic Bagged Decision Tree algorithm. A lazy decision tree emulates this operation by independently growing a single tree branch to handle each unlabeled observation. Listing 2 shows a generic Bagged lazy decision tree algorithm. Since lazy decision trees only examine the branches used to predict unlabeled observations, they may end up exploring fewer nodes than in the corresponding conventional, eager decision trees. This can happen when the test set is only a subset of the data used to build the model. Nevertheless, lazy decision trees will repeat the exploration of some nodes. In practice, the number of unlabeled observations is small, lazy decision trees may be slower in prediction mode than the corresponding algorithms will now focus on the complexity in both of these two."}, {"heading": "3 Batched lazy decision trees", "text": "We start with the listing, and then we discuss and compare stacks stacks stacks stacks stacks stacks stacks stacks stacks stacks (m + 1): The training is provided with n observations and m attributes, the last column contains the class labels. b: Number of bootstrastrastraps. c: Minimum number of observations in a knot (e.g. 5). d: Maximum depth (e.g. 20). Output: A series of stackable decisions trees. algorithm R: The last column contains the class labels for i to b: Number bootstrastrastrastrastrass. c: Minimum number of observations in a knot (e.g. 5). d: Maximum depth (e.g. 20)."}, {"heading": "4 Empirical comparison", "text": "In this section, we empirically evaluate the computational costs of diligent, regular rotten, and stacks of rotten algorithms on five publicly available datasets - each with different properties. Each algorithm is configured to calculate b = 100 bootstraps, nodes with the minimum number of training data observations of p = 5 and a maximum exploration depth of d = 20. Note that the computational complexity of all algorithms depends linearly on parameter b. In addition, both p and d limit the exploration capability of the tree. If log2 n p < d, it is the p parameter that mainly restricts exploration behavior. This limitation applies to all tested datasets."}, {"heading": "4.1 Datasets and implementation", "text": "We look at a diverse set of five benchmark datasets: three popular UCI datasets [5] (Breast, Adult and Gamma) and two medical genetic datasets (C2U [3] and ALL [2]). Table 1 reports on different statistics for these datasets. Informal, breast, adult, gamma and C2U are small and average-sized common datasets, while ALL is a typical genetic dataset that contains far fewer observations than attributes. All three algorithms were implemented in C + + and run on the same Intel I7 computer with 16GB of memory. For accuracy of the results, the reported computation times are the sum of user and kernel CPU times. All three algorithms rely on the same function to determine the best division of data at each node according to the information gain provided by each attribute."}, {"heading": "4.2 Results", "text": "Figure 2 shows the calculation times of k-fold cross-validation for each algorithm and for different k-values for each dataset. It also shows the average number of nodes examined by each of the considered algorithms. Table 2 shows the calculation time required by each method to process a fold in a leave-one-out cross-validation scheme, i.e. the time it takes to train the model and predict for a single test observation. Finally, Table 2 shows the numerical values from Figure 2 for 10-fold and 40-fold cross-validation protocols. Table 3 shows the number of memory words needed to train and store (in the case of ardent decision trees) a model built during a 10-fold cross-validation scheme. This table assumes that storing an observation index requires a word and that storing a decision-tree node requires four words (i.e. the attributes used, the address nodes and sub-parameters)."}, {"heading": "Adult", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "ALL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Breast", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C2U", "text": "This year, the number of trees in the vicinity is 10 percent higher than the number of trees in the vicinity, and 40 percent lower than the number of trees in the vicinity. (DT) The number of trees in the vicinity is higher than the number of trees in the vicinity. (DT) The number of trees in the vicinity is higher than the number of trees in the vicinity. (D) The number of trees in the vicinity is higher than the number of trees in the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle. (D) The number of trees in the middle of the middle of the middle of the middle of the middle of the middle of the middle."}, {"heading": "5 Discussion", "text": "We have conducted a series of experiments to empirically evaluate the temporal complexity of the proposed algorithm and its two candidates, and these experiments have shown that the proposed algorithm is faster than conventional (eager, not lazy) decision tree algorithms and lazy decision tree algorithms, regardless of the number of data folds and the number of test observations. Finally, we have analytically and empirically characterized the memory requirements of each algorithm, and we have shown that mixed decision trees require significantly less memory than decision trees trained in the conventional way."}], "references": [{"title": "Random Forrest", "author": ["Leo Breiman"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Gene expression profiles of b-lineage adult acute lymphocytic leukemia reveal genetic patterns that identify lineage derivation and distinct mechanisms of transformation", "author": ["Sabina Chiaretti", "Xiaochun Li", "Robert Gentleman", "Antonella Vitale", "Franco Mandelli", "Jerome Ritz", "Robin Foa"], "venue": "Clinical Cancer Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Simple statistical models predict c-to-u edited sites in plant mitochondrial rna", "author": ["Michael Cummings", "Daniel Myers"], "venue": "BMC Bioinformatics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Lazy Decision Trees", "author": ["J H Friedman", "Ron Kohavi", "Yeogirl Yun", "H Friedman"], "venue": "In 13th National Con- 6  ference on Artificial Intelligence Conference (AAAI),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Boosting Lazy Decision Trees", "author": ["Xiaoli Zhang Fern", "Carla E. Brodley"], "venue": "In Twentieth International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "As an example, ID3 decision tree and Random Forest [1] are eager algorithms, while k-Nearest Neighbours is an example of a lazy algorithm.", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "[4] and later Fern and Brodley [6] experimented with the emulation of decision tree algorithms in lazy frameworks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4] and later Fern and Brodley [6] experimented with the emulation of decision tree algorithms in lazy frameworks.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "[4] and Fern and Brodley [6] have shown how using mutual information between training set and the unlabeled data during the decision tree building can improve the algorithm\u2019s accuracy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4] and Fern and Brodley [6] have shown how using mutual information between training set and the unlabeled data during the decision tree building can improve the algorithm\u2019s accuracy.", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "The authors only report that \u201dit consumes a lot of memory\u201d [4].", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "Three popular UCI datasets [5] (Breast, Adult, and Gamma) and two medical genetic datasets (C2U [3] and ALL [2]).", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "Three popular UCI datasets [5] (Breast, Adult, and Gamma) and two medical genetic datasets (C2U [3] and ALL [2]).", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "have suggested [4], we observed that lazy decision trees can be impractically slow.", "startOffset": 15, "endOffset": 18}], "year": 2016, "abstractText": "We introduce a batched lazy algorithm for supervised classification using decision trees. It avoids unnecessary visits to irrelevant nodes when it is used to make predictions with either eagerly or lazily trained decision trees. A set of experiments demonstrate that the proposed algorithm can outperform both the conventional and lazy decision tree algorithms in terms of computation time as well as memory consumption, without compromising accuracy.", "creator": "LaTeX with hyperref package"}}}