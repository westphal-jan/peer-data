{"id": "1603.02636", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2016", "title": "DROW: Real-Time Deep Learning based Wheelchair Detection in 2D Range Data", "abstract": "We introduce the DROW detector, a deep learning based detector for 2D range data. Laser scanners are lighting invariant, provide accurate range data, and typically cover a large field of view, making them interesting sensors for robotics applications. So far, research on detection in laser range data has been dominated by handcrafted features and boosted classifiers, potentially losing performance due to suboptimal design choices. We propose a Convolutional Neural Network (CNN) based detector for this task. We show how to effectively apply CNNs for detection in 2D range data, and propose a depth preprocessing step and voting scheme that significantly improve CNN performance. We demonstrate our approach on wheelchairs and walkers, obtaining state of the art detection results. Apart from the training data, none of our design choices limits the detector to these two classes, though. We provide a ROS node for our detector and release our dataset containing 464k laser scans, out of which 24k were annotated for training.", "histories": [["v1", "Tue, 8 Mar 2016 19:39:19 GMT  (886kb,D)", "https://arxiv.org/abs/1603.02636v1", "Lucas Beyer and Alexander Hermans contributed equally"], ["v2", "Mon, 5 Dec 2016 18:06:28 GMT  (881kb,D)", "http://arxiv.org/abs/1603.02636v2", "Lucas Beyer and Alexander Hermans contributed equally"]], "COMMENTS": "Lucas Beyer and Alexander Hermans contributed equally", "reviews": [], "SUBJECTS": "cs.RO cs.CV cs.LG cs.NE", "authors": ["lucas beyer", "alexander hermans", "bastian leibe"], "accepted": false, "id": "1603.02636"}, "pdf": {"name": "1603.02636.pdf", "metadata": {"source": "CRF", "title": "DROW: Real-Time Deep Learning based Wheelchair Detection in 2D Range Data", "authors": ["Lucas Beyer", "Alexander Hermans", "Bastian Leibe"], "emails": ["last@vision.rwth-aachen.de"], "sections": [{"heading": null, "text": "In fact, it is the case that most people who stand up for the rights of people must put themselves at the centre of attention. (...) Indeed, it is the case that most people who stand up for the rights of people must also exercise these rights. (...) It is not the case that they can only limit themselves to themselves. (...) It is the case that they must put themselves at the centre. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (). (...). (). (). (...). (). (). (...). (). (). (...). (). (). (...). (). (). (...). (). (...). (...). (...). (...).). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...).). (...). (...).). (...). (...). (...). (...). (...). (...). (...).). (...). (...)."}, {"heading": "II. APPROACH", "text": "Our approach consists of three steps: pre-processing, which cuts a newly detected window around each laser point and calculates detection locations in a local coordinate system; a CNN, which classifies said windows and predicts relative detection locations; and finally a tuning and non-maximum suppression scheme, which turns predictions into detection values. 2D Range Data PreprocessingIn order to use a CNN for detection, the receptive field of the network must cover a large part of the object. The problem with laser scans is that nearby objects cover a large amount of laser beams, while distant objects are only hit by a handful of beams. This means that the receptive field of CNN must cover most of the laser, making it very vulnerable to fitting into the training scenes. To circumvent this problem, while using the real scale information provided by laser data, we suggest evaluating CNN in a depth-guided sliding mode."}, {"heading": "B. Prediction", "text": "For each window, and therefore for each laser point with its context, CNN classifies both whether this window belongs to a particular weight class and, if so, by a regression output, for the center of that object. Since the 2D domain data is inherently rotationally invariant, we do not want to perform a coordination in absolute (x, y) coordinates. Instead, we learn offsets (\u0445 x, \u0445 y) in a coordinate system centered and aligned to the current laser point, as shown in Fig. 1.C. Tuning and non-maximum pressure The predictions for each window must be consolidated in detection centers. This is achieved by encompassing CNN's predictions in regular grids that encompass the laser's field of vision. Let's p (O | w) = relative c-filter p (c-w) show the overall detection probability of the window w in which an object of interest is to be seen, with C being the selected classes."}, {"heading": "III. EXPERIMENTAL EVALUATION", "text": "To evaluate our approach, we will first present our new data set. We will outline the details of our training process and evaluation methods. After the general evaluation of DROW and a comparison with baselines, we will conduct several ablation studies to show how individual parts of our detector contribute to overall performance."}, {"heading": "A. Dataset", "text": "In fact, most of us will be able to play by the rules we have set ourselves."}, {"heading": "B. Training Procedure", "text": "We train a CNN that predicts for each prefabricated window whether it is indicative for a nearby wheelchair or roller = 0.48, and if so, it predicts the displacement of its center. We do this in a single pass using a network with two output levels: a three-way SoftMax that distinguishes between background, wheelchair and roller, and a two-dimensional linear regression output. We optimize the network by adding the sum of a negative log probability criterion on the SoftMax output and a root-mean-square error on the regression output. Regression targets are compressed as (\u2206 x, \u0445 y) in the local coordinate system of each window, as shown in Figure 1. Class identifiers are determined by the type of closest detection to the center of the window, with a maximum euclidean distance of 0.6 m for wheelchairs and 0.4 m for rollers."}, {"heading": "C. Approach Evaluation", "text": "We trained our model on our training plan and calculated hyperparameters on our validation set as described in the previous section. To evaluate the real performance of DROW, we now consider the precision and retrieval curves on our own test set and on the publicly available rehab set of [34] 4, which was recorded using a similar robot. Remember that our test set was recorded in a never-before-seen part of the care facility, i.e. a method that does not take location specifications or background models into account, the result in Fig. 5 shows that DROW generalizes very well and remembers significantly higher than Gandalf [34], both on our test set and on the rehab test scenario, that the actual capture of a mobility aid is more important than its correct classification, and that we have a significant precision."}, {"heading": "D. Baselines", "text": "We compare our proposed method with two baselines in Fig. 5: the publicly available Gandalf detector [34] and a na\u00efve deep learning baseline.None of the accuracy retrieval curves in [34] quantifies Gandalf's actual detection performance, but they are independently focused on parts of the system. In order to obtain comparable detection retrieval values, we evaluate Gandalf5 using the evaluation protocol described above. Assuming that the Gandalf detector does not have a single threshold that can be matched, it leads to a single point in our diagrams. In the classical case, we record the results for cases where Gandalf detections are retained () and discarded (), with a trade-off between precision and Recall.5https: / github.com / neurob / gandalf _ detectorhistogram (y-axis shown on the right)."}, {"heading": "E. Ablation Studies", "text": "In this context, it should be noted that these two are very complex matters, which are very complex, complex and complex."}, {"heading": "IV. RELATED WORK", "text": "This year it is so far that it is only a matter of time before it will be so far, until it is so far."}, {"heading": "V. DISCUSSION", "text": "Some interesting aspects of our detector should be highlighted. First, given the experimental evaluation, it is clear that we are doing well in terms of wheelchairs, but our performance on the roller leaves room for improvement. One reason might be the fact that our training kit tends more towards wheelchairs. Second, the fact that DROW detects a mobility aid of any class that is well located leads us to conclude that the network has \"understood\" the roller, but needs to see more of it. Second, laser-based detection is often dismissed as too sparse or too difficult in a single frame. We hope that our results refute these fears, although we do not ignore the performance gain that could probably be achieved by tracking our detections over time."}, {"heading": "VI. CONCLUSIONS", "text": "In this paper, we introduced the DROW detector, a fast-learning, wheelchair and roller based detector from 2D data. We proposed depth pre-processing and a choice scheme that allows CNNs to significantly exceed the naive CNN detection baselines and achieve state-of-the-art results compared to an earlier method. We conducted a thorough experimental evaluation that justifies all of our important design decisions. To accomplish all of this, we have recorded a large dataset in which we comment on wheelchair and roller centrifuges and believe it will allow further research. We believe that our detector will be generalized to other classes of training data and thus be useful to the community. Once the paper is accepted, we will publish our code, including a ROS node and the annotated dataset."}], "references": [{"title": "Using Boosted Features for the detection of People in 2D Range Data", "author": ["Kai O Arras", "\u00d3scar Mart\u0131\u0301nez Mozos", "Wolfram Burgard"], "venue": "In ICRA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures", "author": ["James Bergstra", "Daniel Yamins", "David Daniel Cox"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Biternion Nets: Continuous Head Pose Regression from Discrete Training Labels", "author": ["Lucas Beyer", "Alexander Hermans", "Bastian Leibe"], "venue": "In GCPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Robotic Assistance: an Automatic Wheelchair Tracking and Following Functionality by Omnidirectional Vision", "author": ["Cyril Cauchois", "Fabrice De Chaumont", "Bruno Marhic", "Laurent Delahoche", "M\u00e9lanie Delafosse"], "venue": "In IROS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Unsupervised Feature Learning for Classification of Outdoor 3D Scans", "author": ["Mark De Deuge", "A Quadros", "C Hung", "B Douillard"], "venue": "ACRA,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Real-time Multisensor People Tracking for Human-Robot Spatial Interaction", "author": ["Christian Dondrup", "Nicola Bellotto", "Ferdian Jovan", "Marc Hanheide"], "venue": "In ICRA,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Deep Sparse Rectifier", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "Neural Networks. In AISTATS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Wheelchair detection using cascaded decision", "author": ["Chun-Rong Huang", "Pau-Choo Chung", "Kuo-Wei Lin", "Sheng-Chieh Tseng"], "venue": "tree. T-ITB,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Matplotlib: A 2D graphics environment", "author": ["J.D. Hunter"], "venue": "CS&E, 9(3):90\u2013 95", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Unsupervised Feature Learning for 3D Scene Labeling", "author": ["Kevin Lai", "Liefeng Bo", "Dieter Fox"], "venue": "In ICRA,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Robust Object Detection with Interleaved Categorization and Segmentation", "author": ["Bastian Leibe", "Ale\u0161 Leonardis", "Bernt Schiele"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "SSD: Single Shot MultiBox Detector", "author": ["Wei Liu", "Dragomir Anguelov", "Dumitru Erhan", "Christian Szegedy", "Scott Reed"], "venue": "arXiv preprint arXiv:1512.02325,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition", "author": ["Daniel Maturana", "Sebastian Scherer"], "venue": "In IROS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "2D Laser Based Road Obstacle Classification for Road Safety Improvement", "author": ["Pierre Merdrignac", "Evangeline Pollard", "Fawzi Nashashibi"], "venue": "In ARSO,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Hough-CNN: Deep Learning for Segmentation of Deep Brain Regions in MRI and Ultrasound", "author": ["Fausto Milletari", "Seyed-Ahmad Ahmadi", "Christine Kroll", "Annika Plate", "Verena Rozanski", "Juliana Maiostre", "Johannes Levin", "Olaf Dietrich", "Birgit Ertl-Wagner", "Kai B\u00f6tzel"], "venue": "arXiv preprint arXiv:1601.07014,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Multi-Part People Detection Using 2D Range Data. IJSR", "author": ["Oscar Martinez Mozos", "Ryo Kurazume", "Tsutomu Hasegawa"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Wheelchair Detection in a Calibrated Environment", "author": ["Ashish Myles", "Niels Da Vitoria Lobo", "Mubarak Shah"], "venue": "In ACCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "End-to-End Tracking and Semantic Segmentation Using Recurrent Neural Networks", "author": ["Peter Ondruska", "Julie Dequaire", "Dominic Zeng Wang", "Ingmar Posner"], "venue": "In RSS, Workshop on Limits and Potentials of Deep Learning in Robotics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Scikit-learn: Machine Learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "JMLR, 12:2825\u20132830", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "You Only Look Once: Unified, Real-Time Object Detection", "author": ["Joseph Redmon", "Santosh Divvala", "Ross Girshick", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1506.02640,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Hough Networks for Head Pose Estimation and Facial Feature Localization", "author": ["Gernot Riegler", "David Ferstl", "Matthias R\u00fcther", "Horst Bischof"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Looking at Vehicles on the Road: A Survey of Vision-Based Vehicle Detection, Tracking, and Behavior Analysis. TITS", "author": ["Sayanan Sivaraman", "Mohan Manubhai Trivedi"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "A Layered Approach to People Detection in 3D Range Data", "author": ["Luciano Spinello", "Kai Oliver Arras", "Rudolph Triebel", "Roland Siegwart"], "venue": "In AAAI,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Human Detection using Multimodal and Multidimensional Features", "author": ["Luciano Spinello", "Roland Siegwart"], "venue": "In ICRA,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1929}, {"title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation", "author": ["Jonathan J Tompson", "Arjun Jain", "Yann LeCun", "Christoph Bregler"], "venue": "In NIPS,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Voting for voting in online point cloud object detection", "author": ["Dominic Zeng Wang", "Ingmar Posner"], "venue": "In Proceedings of Robotics: Science and Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "People Detection and Distinction of their Walking Aids in 2D Laser Range Data based on Generic Distance-Invariant Features", "author": ["Christoph Weinrich", "Tim Wengefeld", "Christof Schroeter", "Horst- Michael Gross"], "venue": "RO-MAN,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Fast Line, Arc/Circle and Leg Detection from Laser Scan Data in a Player Driver", "author": ["Jodo Xavier", "Marquidia Pacheco", "Daniel Castro", "Ant\u00f3nio Ruano", "Urbano Nunes"], "venue": "In ICRA,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2005}, {"title": "Deep Voting: A Robust Approach Toward Nucleus Localization in Microscopy Images", "author": ["Yuanpu Xie", "Xiangfei Kong", "Fuyong Xing", "Fujun Liu", "Hai Su", "Lin Yang"], "venue": "In MICCAI,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Many autonomous robots are equipped with a 2D laser scanner, typically used for navigation-related tasks including the detection of people [1], [34] and objects [18].", "startOffset": 139, "endOffset": 142}, {"referenceID": 33, "context": "Many autonomous robots are equipped with a 2D laser scanner, typically used for navigation-related tasks including the detection of people [1], [34] and objects [18].", "startOffset": 144, "endOffset": 148}, {"referenceID": 17, "context": "Many autonomous robots are equipped with a 2D laser scanner, typically used for navigation-related tasks including the detection of people [1], [34] and objects [18].", "startOffset": 161, "endOffset": 165}, {"referenceID": 34, "context": "While early detection methods used simple heuristics such as fitting lines and circles [35], in the past few years hand crafted features, coupled with learned classifiers, have dominated laser based detection.", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 51, "endOffset": 54}, {"referenceID": 29, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 62, "endOffset": 66}, {"referenceID": 33, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 174, "endOffset": 177}, {"referenceID": 27, "context": "Within this paradigm, a range of successful people [1], [30], [20], mobility aid [34] and road obstacle [18] detectors have been developed to support mobile robot navigation [8] and autonomous driving [28].", "startOffset": 201, "endOffset": 205}, {"referenceID": 29, "context": "seems to be that the information provided by 2D range data is not sufficient to reliably perform detection in a single scan, leading to approaches relying on sensor fusion [30], multilayered sensor setups [20], [29], or temporal integration of information by tracking.", "startOffset": 172, "endOffset": 176}, {"referenceID": 19, "context": "seems to be that the information provided by 2D range data is not sufficient to reliably perform detection in a single scan, leading to approaches relying on sensor fusion [30], multilayered sensor setups [20], [29], or temporal integration of information by tracking.", "startOffset": 205, "endOffset": 209}, {"referenceID": 28, "context": "seems to be that the information provided by 2D range data is not sufficient to reliably perform detection in a single scan, leading to approaches relying on sensor fusion [30], multilayered sensor setups [20], [29], or temporal integration of information by tracking.", "startOffset": 211, "endOffset": 215}, {"referenceID": 33, "context": "[34] face a similar task and propose the Gandalf detector for detecting people, wheelchairs, and walkers in individual laser scans.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In computer vision, deep learning has recently become the new best practice, replacing hand-crafted features by learned ones and overhauling the state of the art in many tasks [13], [32], [5].", "startOffset": 176, "endOffset": 180}, {"referenceID": 31, "context": "In computer vision, deep learning has recently become the new best practice, replacing hand-crafted features by learned ones and overhauling the state of the art in many tasks [13], [32], [5].", "startOffset": 182, "endOffset": 186}, {"referenceID": 4, "context": "In computer vision, deep learning has recently become the new best practice, replacing hand-crafted features by learned ones and overhauling the state of the art in many tasks [13], [32], [5].", "startOffset": 188, "endOffset": 191}, {"referenceID": 15, "context": "While CNN-based image-level detectors like MultiBox [16] and YOLO [24] could in principle be applied to a 2D laser scan, we found that doing so naively is not effective.", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "While CNN-based image-level detectors like MultiBox [16] and YOLO [24] could in principle be applied to a 2D laser scan, we found that doing so naively is not effective.", "startOffset": 66, "endOffset": 70}, {"referenceID": 33, "context": "[34] provide their datasets, their recordings are limited to scenes with a single wheelchair or walker.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "We wrote a custom annotation tool based on matplotlib [11], which", "startOffset": 54, "endOffset": 58}, {"referenceID": 33, "context": "(b) Performance on the Reha test set [34].", "startOffset": 37, "endOffset": 41}, {"referenceID": 33, "context": "Gandalf [34]", "startOffset": 8, "endOffset": 12}, {"referenceID": 33, "context": "5: Performance comparison of DROW, the Gandalf detector [34], and a naive deep learning baseline on two test sets (a) and (b).", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "The architecture of our network, inspired by the popular VGGnet [27]2, is as follows: Conv 5@64, Conv 5@64, Max 2, Conv 5@128, Conv 3@128, Max 2, Conv 5@256, Conv 3@5, where Convn@c represents a convolution with c filters of size n and Max p the maximum operation on a", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "Batch-normalization [12], dropout [31] of 0.", "startOffset": 20, "endOffset": 24}, {"referenceID": 30, "context": "Batch-normalization [12], dropout [31] of 0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "25, and ReLU nonlinearities [9] are applied between all convolutional layers.", "startOffset": 28, "endOffset": 31}, {"referenceID": 25, "context": "All weights are initialized similarly to the scheme proposed in [26], except for those of the output layer, which are initialized to 0.", "startOffset": 64, "endOffset": 68}, {"referenceID": 1, "context": "We implemented our CNN in Theano [2].", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "We then optimize all voting hyper-parameters (class-weights w, grid resolution b and blur-size \u03c3) using hyperopt [4] to maximize maxT fwheelchair(T ) + fwalker(T ) on our validation set, fc(T ) being the F1-score of class c using detection threshold T .", "startOffset": 113, "endOffset": 116}, {"referenceID": 33, "context": "our own test set and on the publicly available Reha test set of [34]4 recorded with a similar robot.", "startOffset": 64, "endOffset": 68}, {"referenceID": 33, "context": "precision and recall than Gandalf [34], both on our test set and on the Reha test set.", "startOffset": 34, "endOffset": 38}, {"referenceID": 33, "context": "5: the publicly available Gandalf detector [34] and a", "startOffset": 43, "endOffset": 47}, {"referenceID": 33, "context": "None of the precision-recall curves in [34] quantify Gandalf\u2019s actual detection performance, they only focus on parts of the system independently.", "startOffset": 39, "endOffset": 43}, {"referenceID": 33, "context": "Note that the detector performance we obtained with the code from [34] is significantly lower than the one reported in the original paper.", "startOffset": 66, "endOffset": 70}, {"referenceID": 33, "context": "The performance could be improved by an additional covariance-based merging step briefly mentioned in [34] which is, however, neither described in detail in the paper, nor included in the provided detector code.", "startOffset": 102, "endOffset": 106}, {"referenceID": 33, "context": "We were thus not able to reproduce the results presented in [34].", "startOffset": 60, "endOffset": 64}, {"referenceID": 23, "context": "As a naive deep learning baseline, we train another CNN, similarly to YOLO [24], that directly predicts up to two detections (sufficient for 95.", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "We used the scikit-learn [23] implementation of the regression forest with all default settings and 50 trees.", "startOffset": 25, "endOffset": 29}, {"referenceID": 33, "context": "[34] who propose a distance invariant feature for laser based detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "A few more publications [7], [14], [17] identify the need for a distance-invariant representation which we highlight in this work.", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "A few more publications [7], [14], [17] identify the need for a distance-invariant representation which we highlight in this work.", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "A few more publications [7], [14], [17] identify the need for a distance-invariant representation which we highlight in this work.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "image from a 3D point-cloud by ray-tracing [7], however, involves multiple significantly more complex operations than our proposed simple pre-processing and no timings were provided.", "startOffset": 43, "endOffset": 46}, {"referenceID": 13, "context": "The other commonly-used solution is the creation of a 3D occupancy-grid [14], [17].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "The other commonly-used solution is the creation of a 3D occupancy-grid [14], [17].", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "Arguably, the latter makes learning easier and predictions more robust [3].", "startOffset": 71, "endOffset": 74}, {"referenceID": 16, "context": "In addition, multiple different heuristics exist for \u201cfilling holes\u201d in those occupancygrids [17], whereas our pre-processing doesn\u2019t produce holes in the first place.", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "[20] in a multi-height laser setup and Spinello et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] in a layered fashion based on 3D range data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Both, related to [15], learn shape models from data and cast votes from the different laser layers to detection centroids.", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "[34], they rely on jump distance segmentation and only the segments can cast votes for detections.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "In [33], Wang et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In line with our findings, voting has recently been shown to work well in combination with various other deep learning approaches [19], [25], [36].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "In line with our findings, voting has recently been shown to work well in combination with various other deep learning approaches [19], [25], [36].", "startOffset": 136, "endOffset": 140}, {"referenceID": 35, "context": "In line with our findings, voting has recently been shown to work well in combination with various other deep learning approaches [19], [25], [36].", "startOffset": 142, "endOffset": 146}, {"referenceID": 32, "context": "It would thus be interesting to establish a relationship between deep, non-linear voting detectors such as DROW and sliding-window detectors on sparse inputs as shown in [33].", "startOffset": 170, "endOffset": 174}, {"referenceID": 21, "context": "[22] shows how to do \u201ctracking\u201d of pedestrians, cyclists, buses, cars, and road obstacles in 2D range data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] also detect cars, bicyclists and static road obstacles in 2D range data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Several approaches have tried this before [21], [10], [6], but none of them is general enough to be applied in all scenarios.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "Several approaches have tried this before [21], [10], [6], but none of them is general enough to be applied in all scenarios.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "Several approaches have tried this before [21], [10], [6], but none of them is general enough to be applied in all scenarios.", "startOffset": 54, "endOffset": 57}, {"referenceID": 20, "context": "They typically rely on geometric primitives [21] or very specific camera setups [6].", "startOffset": 44, "endOffset": 48}, {"referenceID": 5, "context": "They typically rely on geometric primitives [21] or very specific camera setups [6].", "startOffset": 80, "endOffset": 83}], "year": 2016, "abstractText": "We introduce the DROW detector, a deep learning based object detector operating on 2D range data. Laser scanners are lighting invariant, provide accurate 2D range data, and typically cover a large field of view, making them interesting sensors for robotics applications. So far, research on detection in laser 2D range data has been dominated by hand-crafted features and boosted classifiers, potentially losing performance due to suboptimal design choices. We propose a Convolutional Neural Network (CNN) based detector for this task. We show how to effectively apply CNNs for detection in 2D range data, and propose a depth preprocessing step and a voting scheme that significantly improve CNN performance. We demonstrate our approach on wheelchairs and walkers, obtaining state of the art detection results. Apart from the training data, none of our design choices limits the detector to these two classes, though. We provide a ROS node for our detector and release our dataset containing 464k laser scans, out of which 24k were annotated.", "creator": "LaTeX with hyperref package"}}}