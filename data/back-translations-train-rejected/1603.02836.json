{"id": "1603.02836", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2016", "title": "Faster learning of deep stacked autoencoders on multi-core systems using synchronized layer-wise pre-training", "abstract": "Deep neural networks are capable of modelling highly non-linear functions by capturing different levels of abstraction of data hierarchically. While training deep networks, first the system is initialized near a good optimum by greedy layer-wise unsupervised pre-training. However, with burgeoning data and increasing dimensions of the architecture, the time complexity of this approach becomes enormous. Also, greedy pre-training of the layers often turns detrimental by over-training a layer causing it to lose harmony with the rest of the network. In this paper a synchronized parallel algorithm for pre-training deep networks on multi-core machines has been proposed. Different layers are trained by parallel threads running on different cores with regular synchronization. Thus the pre-training process becomes faster and chances of over-training are reduced. This is experimentally validated using a stacked autoencoder for dimensionality reduction of MNIST handwritten digit database. The proposed algorithm achieved 26\\% speed-up compared to greedy layer-wise pre-training for achieving the same reconstruction accuracy substantiating its potential as an alternative.", "histories": [["v1", "Wed, 9 Mar 2016 10:31:00 GMT  (91kb,D)", "http://arxiv.org/abs/1603.02836v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anirban santara", "debapriya maji", "dp tejas", "pabitra mitra", "arobinda gupta"], "accepted": false, "id": "1603.02836"}, "pdf": {"name": "1603.02836.pdf", "metadata": {"source": "CRF", "title": "Faster learning of deep stacked autoencoders on multi-core systems using synchronized layer-wise pre-training", "authors": ["Anirban Santara", "Debapriya Maji", "DP Tejas", "Pabitra Mitra", "Arobinda Gupta"], "emails": ["santara@iitkgp.ac.in"], "sections": [{"heading": null, "text": "Keywords: deep learning, stacked autoencoder, RBM, backpropagation, contrastive divergence, MNIST"}, {"heading": "1 Introduction", "text": "In fact, most of us are able to abide by the rules we have imposed on ourselves, \"he told the German Press Agency.\" But we're not having it easy, \"he said.\" We're not having it easy, \"he said,\" but we're not having it easy. \""}, {"heading": "2 Preliminaries of stacked autoencoder", "text": "In this section, we describe stacked autoencoders and briefly explain the tools and methods used in their training."}, {"heading": "2.1 Autoencoder", "text": "An autoencoder is a type of neural network that is operated in unattended learning mode primarily for learning to visualize and reducing dimensionality [11]. It aims to encode the input x into a representation c (x), so that the input can be reconstructed from the representation with minimal deformation. In the face of a series of unlabeled training examples {x (1), x (2),...} the autoencoder specifies the outputs y (i) = x (i) and tries to learn a function hW, b, so that hW, b (x) = x \u2248 x. A stacked autoencoder [2, 3] has several hidden layers of neurons between the input and output layers. A stacked autoencoder of the depth n has an input layer, 2n \u2212 3 hidden layers and an output layer of neurons. The layer 1, 2, comprises the remaining layer or the encoder."}, {"heading": "2.2 Restricted Boltzmann Machine (RBM)", "text": "An RBM is a Markov random field associated with an undirected graph consisting of m visible units V = (V1,..., Vm) comprising the visible layer and n hidden units H = (H1,.., Hn) comprising the hidden layer [2, 12]. There is no connection between the units of the same layer in the graph. The common probability distribution associated with the model is derived from the Gibbs distribution: p (v, h) = 1Z e \u2212 E (v, h) (1) with the energy function: E (v, h) = \u2212 n \u00b2 i = 1 m \u00b2 j = 1 wi, jhivj \u2212 m \u00b2 j = 1 bjvj \u2212 n \u00b2 i = 1 cihi (2) The absence of connections between the nodes of the same layer implies that the hidden variables are conditionally independent of the state of the visible variables and how the invisible dispersibility (v = 1) can be distributed."}, {"heading": "2.3 Greedy Layer-wise pre-training", "text": "Due to the presence of numerous local optimas, proper initialization of the parameters is crucial for the formation of deep neural networks. Greedy layer pretraining was introduced with the motivation to roughly capture statistical abstractions of the input data in each layer and to initialize the network parameters in a region near a good local optimum [5], [3]. Figure 1 outlines the greedy layer-by-layer pretraining algorithm. Let's consider a deep neural network with K hidden layers of neurons. Let the input dimension be Ni, the output dimension No and the hidden layer dimensions N1, N2,., NK. Let's call the training set of S. Greedy layer-by layer pretraining follows one of the methods described below. Pre-training with RBM: In this approach, an RBM with Ni visible nodes and N1 hidden nodes is first trained."}, {"heading": "3 Synchronized layer-wise pre-training", "text": "This section describes the proposed algorithm for accelerated training of stacked autoencoders. Greedy layer-by-layer pre-training makes a shift wait until all previous layers are finished with learning before it can begin itself, and until all subsequent layers are completed. The guiding philosophy of the proposed algorithm is to reduce the idle time of greedy layer-by-layer pre-training by introducing parallelism with synchronization. This algorithm is suitable for multi-core machines where different layers can learn in parallel on threads running in different cores of the CPU."}, {"heading": "3.1 Methodology", "text": "Ll \"iW\" s \"i\" s \"i, s\" s sasd \"i\" s. \"W\" i \"s, s\" i \"s\" s \"s\" s \"s.\" D \"s\" s \"s\" s \"s.\" D \"s\" s \"s.\" D \"s\" s. \"s\" s. \"s\" s. \"s\" s. \"s\" s. \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s. \"s\" s. \"s.\" s \"s.\" s \"s.\" s \"s.\" s \"s.\" s \"s.\" s \"s.\" s \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s. \"s\" s \"s.\" s \"s.\" s \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s. \"s\" s. \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s \"s.\" s \"s\" s \"s.\" s \"s\" s. \"s\" s \"s\" s \"s.\" s \"s\" s \"s.\" s \"s\" s \"s.\" s \"\" s \"s\" s \"s\" s. \"s\" s \"s.\" s \"s\" s \"s.\" s \"\" s \"s\" s \"s\" s. \"s\" s. \"s\" s \"s\" s \"s\" \"s\" s \"\" \"\" s. \"\" \"s.\" s. \"\" \"s.\" s \"\" s \"s\" s \"s\" s \"s\" \"\" \"\" \"s.\" \"\" s. \"s.\" \"\" s \"s\" \"\" s \"s.\" s. \"s\" s \"s\" \""}, {"heading": "4 Experimental methodology", "text": "The layer-by-layer synchronized pre-training algorithm was tested on the task of reducing the dimensionality of handwritten MNIST digit data sets by means of a stacked autoencoder and the reconstruction accuracy was measured on the basis of a mean square error criterion (MSE)."}, {"heading": "4.1 Dataset", "text": "MNIST3 is a database of (8-bit) 28x28 handwritten grayscale digits, popularly used for training and testing machine vision and machine learning systems. It contains 60,000 training examples and 10,000 test examples. For our experiments, the training set has been further divided into 50,000 training examples and 10,000 validation examples. The validation set contains 1,000 examples from each digit class (0-9), ensuring consistent participation in all courses. The training set has been randomized and divided into size 100 minibatches before each new epoch of training and fine-tuning. Figure 5 (a) shows some sample numbers from the dataset. 3 Free download available at http: / / yann.lecun.com / exdb / mnist /"}, {"heading": "4.2 Parameters of the stacked autoencoder", "text": "Details of the architecture of the stacked autoencoder are shown in Table 1, which corresponds to the architecture used by [3] for the same task, with the sole exception of sigmoid activation functions for all layers."}, {"heading": "4.3 Parameters of the learning algorithms", "text": "The learning rate of both weights and distortions was set at 0.1 for contrastive divergence and 0.001 for back propagation. Additionally, a pulse of 0.5 for the first 5 eras and 0.9 for the remaining eras was used for contrastive divergence learning. Performance of greedy layer-by-layer pre-training (the baseline) [3] and the proposed synchronized layer-by-layer pre-training algorithm were compared on a system with 8 CPU cores (dual-hyperthreaded quad-core) and 8 GB of RAM for the reconstruction error calculated as an average square reconstruction error per digit and total training time. The basic experiment was performed with 20 epochs of RBM pre-training for each shift, followed by 10 epochs of fine-tuning the entire architecture by means of back propagation. Next, the proposed algorithm was performed for the same amount of pre-training and fine-tuning and 20 epochs each, but the 2 and 3 respectively of their previous epothreads."}, {"heading": "5 Results and discussion", "text": "In this section, the results of the comparison are presented and some aspects of the proposed algorithm are discussed."}, {"heading": "5.1 Convergence", "text": "Figure 6 shows the variation of the reconstruction error of the validation set during the execution of the proposed algorithm. Table 2 shows the final reconstruction error for the two experiments. Figure 5 compares the reconstructions of 25 randomly selected samples from the test set of the two methods."}, {"heading": "5.2 Speed-up", "text": "Table 5.1 compares the execution times of the two algorithms. The proposed algorithm achieves the same performance 1h26m49s faster than greedy layer-by-layer pre-training, corresponding to an acceleration of 26.17%. The four threads, T1 to T4, were executed on four different cores of the same CPU. The time for data transmission between the threads was considered to be 5 to 6 orders of magnitude less than the time for an epoch of RBM pre-training. In our experiment, the pre-training of the stacked autoencoder was completed once the RBM for the first shift (784 \u2212 1000) had completed its prescribed 20 epochs. Although it changed DT2 and D V 2 after the 20th epoch, the parameters of L2 or one of its data-dependent shifts were not updated, only to keep the total pre-training time equal to the time required for L1 (the largest and slowest to train the shift) to spend the additional amount of training required for 4 hours."}, {"heading": "5.3 Analysis of the space-time diagram", "text": "Figure 3 shows the space-time diagram for the proposed algorithm. Variation in training reconstruction errors and validation examples was plotted for each layer. Both errors show a decreasing trend for the first three layers indicating the parameters of these layers, with the behavior of the fourth layer being particularly interesting. Figure 4 zooms into its time diagram (marked with a dotted rectangle in Figure 3), indicating that the parameters reach a generalization beyond the values set during the pre-training. The behavior of the fourth layer is particularly interesting. Figure 4 zooms into its time diagram (marked with a dotted rectangle in Figure 3). The validation and training errors differ by a large distance (during the prescribed viewing of the pre-training and beyond)."}, {"heading": "6 Conclusion", "text": "In this paper, the existing methods of parallelization of learning in the deep architecture were examined and a new algorithm for parallelization of the pre-school phase was proposed. Convergence and performance of the algorithm was tested on the task of reducing the dimensionality of the handwritten MNIST digit database using stacked autoencoder and a 26% acceleration to achieve the same performance. The main feature of the proposed algorithm is that it is not greedy, which reduces the likelihood of mismatch between layers, a frequent hazard of greedy learning, and minimizes the idle time for each shift. The proposed algorithm can be seamlessly integrated into the existing methods of parallelization, which mainly focus on fine-tuning."}], "references": [{"title": "Why Does Unsupervised Pre-training Help Deep Learning ?", "author": ["D. Erhan", "A. Courville", "P. Vincent"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "now, 2009,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Reducing the Dimensionality of Data with", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Neural Networks,\u201d Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets,", "author": ["G.E. Hinton", "S. Osindero"], "venue": "Neural computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Greedy Layer-Wise Training of Deep Networks,", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Proceedings of Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Performance analysis of a pipelined backpropagation parallel algorithm,", "author": ["A. Petrowski", "C. Girault"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1993}, {"title": "Pipelined Back-Propagation for Context-Dependent Deep Neural Networks,", "author": ["X. Chen", "A. Eversole", "G. Li", "D. Yu", "F. Seide"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Deep Convex Net : A Scalable Architecture for Speech Pattern Classification,", "author": ["L. Deng", "D. Yu"], "venue": "ISCA, no. August,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Parallelization of Deep Networks,", "author": ["M.D. DeGrazia", "I. Stoianov", "M. Zorzi", "M. De Filippo De Grazia"], "venue": "European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Neural networks: A comprehensive foundation", "author": ["S. Haykin"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "An Introduction to Restricted Boltzmann Machines,", "author": ["A. Fischer", "C. Igel"], "venue": "Progress in Pattern Recognition, Image Analysis2,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Training Products of Experts by Minimizing Contrastive Divergence,", "author": ["G.E. Hinton"], "venue": "Neural computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Artificial neural networks that have more than three hidden layers of neurons are called Deep Neural Networks (DNN) [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "Stacked autoencoder is a class of DNN that is used for unsupervised representation learning and dimensionality reduction [2].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "On the other hand, with small initial weights backpropagated error gradients in the layers close to the input become tiny resulting in no appreciable training of those layers [3].", "startOffset": 175, "endOffset": 178}, {"referenceID": 3, "context": "Greedy layer-wise pre-training was introduced with a motivation to approximately capture statistical abstractions of the input data in every layer and initialize the network parameters in a region near a good local optimum [4, 5].", "startOffset": 223, "endOffset": 229}, {"referenceID": 4, "context": "Greedy layer-wise pre-training was introduced with a motivation to approximately capture statistical abstractions of the input data in every layer and initialize the network parameters in a region near a good local optimum [4, 5].", "startOffset": 223, "endOffset": 229}, {"referenceID": 1, "context": "This approach of training deep architectures has brought great success in many areas of application [2, 3].", "startOffset": 100, "endOffset": 106}, {"referenceID": 2, "context": "This approach of training deep architectures has brought great success in many areas of application [2, 3].", "startOffset": 100, "endOffset": 106}, {"referenceID": 5, "context": "These parallel algorithms fall in two broad categories [6].", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "The neural network can be physically partitioned [7] or logically partitioned [8, 9].", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "The neural network can be physically partitioned [7] or logically partitioned [8, 9].", "startOffset": 78, "endOffset": 84}, {"referenceID": 8, "context": "The second category of algorithms carry out calculations relative to the entire network but specific to subsets of data (single patterns or small batches of data) at different processing nodes [10].", "startOffset": 193, "endOffset": 197}, {"referenceID": 9, "context": "An autoencoder is a type of neural network that is operated in unsupervised learning mode mainly for representation learning and dimensionality reduction [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 1, "context": "A stacked autoencoder [2, 3] has multiple hidden layers of neurons between the input and output layers.", "startOffset": 22, "endOffset": 28}, {"referenceID": 2, "context": "A stacked autoencoder [2, 3] has multiple hidden layers of neurons between the input and output layers.", "startOffset": 22, "endOffset": 28}, {"referenceID": 9, "context": "Autoencoder is usually trained by the backpropagation algorithm with an error criterion like mean squared error or cross entropy error [11].", "startOffset": 135, "endOffset": 139}, {"referenceID": 1, "context": ",Hn) comprising the hidden layer [2, 12].", "startOffset": 33, "endOffset": 40}, {"referenceID": 10, "context": ",Hn) comprising the hidden layer [2, 12].", "startOffset": 33, "endOffset": 40}, {"referenceID": 11, "context": "The parameters are trained by gradient ascent on the log-likelihood of given data using Markov Chain Monte Carlo methods like Contrastive Divergence [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 4, "context": "Greedy layer-wise pre-training was introduced with a motivation to roughly capture statistical abstractions of the input data in every layer and initialize the network parameters in a region near a good local optimum [5], [3].", "startOffset": 217, "endOffset": 220}, {"referenceID": 2, "context": "Greedy layer-wise pre-training was introduced with a motivation to roughly capture statistical abstractions of the input data in every layer and initialize the network parameters in a region near a good local optimum [5], [3].", "startOffset": 222, "endOffset": 225}, {"referenceID": 2, "context": "This is same as the architcture used by [3] for the same task with the only exception of sigmoid activation functions for all the layers.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "The performances of greedy layer-wise pre-training (the baseline) [3] and the proposed synchronized layer-wise pre-training algorithm were compared on a system with 8 CPU cores (dual-hyperthreaded quad-core) and 8GB RAM in terms of reconstruction error calculated as the average squared reconstruction error per digit and the total training time.", "startOffset": 66, "endOffset": 69}], "year": 2016, "abstractText": "Deep neural networks are capable of modelling highly nonlinear functions by capturing different levels of abstraction of data hierarchically. While training deep networks, first the system is initialized near a good optimum by greedy layer-wise unsupervised pre-training. However, with burgeoning data and increasing dimensions of the architecture, the time complexity of this approach becomes enormous. Also, greedy pre-training of the layers often turns detrimental by over-training a layer causing it to lose harmony with the rest of the network. In this paper a synchronized parallel algorithm for pre-training deep networks on multi-core machines has been proposed. Different layers are trained by parallel threads running on different cores with regular synchronization. Thus the pre-training process becomes faster and chances of overtraining are reduced. This is experimentally validated using a stacked autoencoder for dimensionality reduction of MNIST handwritten digit database. The proposed algorithm achieved 26% speed-up compared to greedy layer-wise pre-training for achieving the same reconstruction accuracy substantiating its potential as an alternative.", "creator": "LaTeX with hyperref package"}}}