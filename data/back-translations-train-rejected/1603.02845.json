{"id": "1603.02845", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2016", "title": "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings", "abstract": "In settings where only unlabelled speech data is available, speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. A similar problem is faced when modelling infant language acquisition. In these cases, categorical linguistic structure needs to be discovered directly from speech audio. We present a novel unsupervised Bayesian model that segments unlabelled speech and clusters the segments into hypothesized word groupings. The result is a complete unsupervised tokenization of the input speech in terms of discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional acoustic vector space. The model, implemented as a Gibbs sampler, then builds a whole-word acoustic model in this space while jointly performing segmentation. We report word error rates in a small-vocabulary connected digit recognition task by mapping the unsupervised decoded output to ground truth transcriptions. The model achieves around 20% error rate, outperforming a previous HMM-based system by about 10% absolute. Moreover, in contrast to the baseline, our model does not require a pre-specified vocabulary size.", "histories": [["v1", "Wed, 9 Mar 2016 11:14:23 GMT  (1054kb,D)", "http://arxiv.org/abs/1603.02845v1", "11 pages, 8 figures; Accepted to the IEEE/ACM Transactions on Audio, Speech, and Language Processing"]], "COMMENTS": "11 pages, 8 figures; Accepted to the IEEE/ACM Transactions on Audio, Speech, and Language Processing", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["herman kamper", "aren jansen", "sharon goldwater"], "accepted": false, "id": "1603.02845"}, "pdf": {"name": "1603.02845.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Word Segmentation and Lexicon Discovery Using Acoustic Word Embeddings", "authors": ["Herman Kamper", "Sharon Goldwater"], "emails": ["aren@jhu.edu).", "sgwater@inf.ed.ac.uk)."], "sections": [{"heading": null, "text": "Most of these improvements, however, come from monitored techniques that rely on large corporations of transcribed speech audio data, speech modeling text, and pronunciation. In this environment, untested methods of speech modeling and pronunciation are needed to discover linguistic structures directly from audio speech. Similar techniques are also needed to model how children learn language in their native language. Researchers in the language processing community have recently begun to use completely uncontrolled techniques to build directly from unlabeled speech data."}, {"heading": "II. RELATED WORK", "text": "In the following, we describe relevant studies from both language processing and cognitive modeling communities."}, {"heading": "A. Discovery of words in speech", "text": "Most state-of-the-art UTD systems are based on the groundbreaking work of Park and Glass [5], who proposed a method to find pairs of similar audio segments and then cluster them into hypothetical word types. Pattern Comparison Step uses a variant of dynamic time warping (DTW) called Segmental DTW that makes it possible to identify similar sub-sequences within two vector time series rather than compare whole sequences as in standard DTW. Subsequent work has built on Park and Glass' original method in various ways, for example by improving feature representations [1], [2] or by significantly improving their efficiency [6]. Like our own system, many of these UTD systems operate on full-word representations without subtext levels of representation. However, each word is presented as an isolated time target without requiring a comparison of DW dimensions."}, {"heading": "B. Word segmentation of symbolic input", "text": "Cognitive scientists have long been interested in how infants segment words and discover the lexicon of their mother tongue, with computational models seen as a way to specify and test certain theories (see [7], [8] for reviews).In this community, most word segmentation computational models perform complete segmentation of data into a word sequence. However, these models generally use phonetic or phonetic strings as input and not continuous speech.Early word segmentation approaches using phonmic input include those based on transitional probabilities [15], neural networks [16] and probabilistic models [17].The model presented here is based on the non-parametric Bayesian approach of Goldwater et al al al al al al al al al al al al al al al al al al al al al al al al al al al al. [8], which has been shown to provide more precise word segmentation than previous work. [Their approach learns alangumental model by incorporating the tokens in its small sequences of word segmentation, and predicting the vocabulary."}, {"heading": "C. Full-coverage segmentation of speech", "text": "This year, the time has come for it to be a purely reactionary project, capable of retaliating."}, {"heading": "III. THE SEGMENTAL BAYESIAN MODEL", "text": "In our approach, each potential word segment (of arbitrary length) is mapped to a vector in a fixed dimensional space RD. The goal of this acoustic word embedding process is that word instances of the same type should lie close together; the various hypothetical word types are then modeled in this D-dimensional space using a Gaussian compound model (GMM) with Bayesian priors. Each compound component of the GMM corresponds to a discovered type; the mean of the component can be considered an average embedding for that word. However, since the model is not monitored, we do not know which identities underlie the true word types to which the components are assigned. Assumption for the moment that such an ideal GMM exists; the mean of the component is the core component in our overall approach illustrated in Fig. 1 (a)."}, {"heading": "A. Fixed-dimensional representation of speech segments", "text": "Our model requires that each acoustic language segment in an enunciation be embedded in a fixed-dimensional space. In principle, any approach that is able to map any length vector time series to a fixed-dimensional vector can be used. Based on previous results, we follow that of Levin et al. [29], as summarized below: The notation Y = y1: T is used to denote a vector time series, with each yt being the frame-level method (e.g. MFCCs). We need a mapping function f (Y) that maps time series Y into a space RD in which the proximity between mappings indicates a similar linguistic content, so that words of the same type are closely related. In [29], the mapping f is performed as follows."}, {"heading": "B. Acoustic modelling: discovering word types", "text": "Given a corpus segmentation hypothesis (which indicates where words begin and end), the acoustic model must group the hypothesized word segments (represented as fixed-dimensional vectors) into groups of hypothetical word types. Again, the acoustic model is performed along with the word segmentation (next section), but here we describe the acoustic model under the current segmentation hypothesis. For the acoustic model, we choose a Bayesian GMM with fixed spherical covariance. This model treats its mixture and component means as random variables rather than as punctuation numbers, as is done in a regular GMM. In [32] we showed that Bayesian GMM is significantly better trained in assembling word components than in a regular GMM framework."}, {"heading": "C. Joint segmentation and clustering", "text": "The acoustic model of the previous section can be used to group existing segments. Our joint segmentation and cluster system works by first scanning a segmentation of the current expression (based on the current acoustic model). \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Evaluation setup", "text": "In fact, most of them are able to determine for themselves how they have behaved."}, {"heading": "B. Model implementation and hyperparameters", "text": "The hyperparameters of our model are mainly based on other tasks. [32] However, some parameters were changed manually during development, and these changes are based on the performance of TIDigits1. In the following, we will also point out the changes we made from our own preparatory work [14]. The hyperparameters we applied in [14] resulted in far less consistent results across multiple samples: We use the following hyperparameters, based on [32], [34], [34], [34], [44], which all deviations of less than 1% that we used in Section IV-C. For the acoustic model (Section III-B), we use the following hyperparameters, based on [34], [0, 0, 0, 0, 2, 0.005, 20 =. Based on [29], [32], we use the following parameters for fixed-dimensional extraction (Section III-A)."}, {"heading": "C. Results and analysis", "text": "This year, it has come to the point that there will only be one such process in which there will be such a process."}, {"heading": "V. CHALLENGES IN SCALING TO LARGER VOCABULARIES", "text": "In fact, it is so that most of them are able to survive themselves, and that they feel able to survive themselves if they do not. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "VI. CONCLUSION", "text": "We introduced a novel Bayesian model based on a fixed-dimensional embedding of language, in which segments and clusters without labels embed continuous language into hypothetical word units - an approach very different from any previously presented. We applied our model to a small-word recognition task and compared performance with a more traditional HMM-based approach from a previous study. Our model exceeded the baseline in unattended word error rate (WHO) by more than 10% absolute, without being limited to a small number of word types (such as HMM). Analysis showed that our model is based on full-word fixed segment representation: when subwords are consistently mapped to a similar region in the embedding space, the model suggests these as separate word types. Therefore, most of the model's errors were based on the consistent division of certain digits into separate clusters of sub-clusters, but not on a specific cluster model based on the same."}], "references": [{"title": "Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Resource configurable spoken query detection using deep Boltzmann machines", "author": ["Y. Zhang", "R. Salakhutdinov", "H.-A. Chang", "J.R. Glass"], "venue": "Proc. ICASSP, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "The spoken web search task at MediaEval 2012", "author": ["F. Metze", "X. Anguera", "E. Barnard", "M. Davel", "G. Gravier"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B. Van Durme"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013 197, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions", "author": ["O.J. R\u00e4s\u00e4nen"], "venue": "Speech Commun., vol. 54, pp. 975\u2013997, 2012.  ACCEPTED TO THE IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2016  11", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "A Bayesian framework for word segmentation: Exploring the effects of context", "author": ["S.J. Goldwater", "T.L. Griffiths", "M. Johnson"], "venue": "Cognition, vol. 112, no. 1, pp. 21\u201354, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Joint training of non-negative Tucker decomposition and discrete density hidden Markov models", "author": ["M. Sun", "H. Van hamme"], "venue": "Comput. Speech Lang., vol. 27, no. 4, pp. 969\u2013988, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization", "author": ["C.-T. Chung", "C.-a. Chan", "L.-s. Lee"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "A hierarchical system for word discovery exploiting DTW-based initialization", "author": ["O. Walter", "T. Korthals", "R. Haeb-Umbach", "B. Raj"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Discovering linguistic structures in speech: Models and applications", "author": ["C.-y. Lee"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, Cambridge, MA, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "SCARF: a segmental conditional random field toolkit for speech recognition", "author": ["G. Zweig", "P. Nguyen"], "venue": "Interspeech, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Fully unsupervised smallvocabulary speech recognition using a segmental Bayesian model", "author": ["H. Kamper", "S.J. Goldwater", "A. Jansen"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "An efficient, probabilistically sound algorithm for segmentation and word discovery", "author": ["M.R. Brent"], "venue": "Mach. Learn., vol. 34, no. 1-3, pp. 71\u2013105, 1999.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning to segment speech using multiple cues: A connectionist model", "author": ["M.H. Christiansen", "J. Allen", "M.S. Seidenberg"], "venue": "Lang. Cognitive Proc., vol. 13, no. 2-3, pp. 221\u2013268, 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "A statistical model for word discovery in transcribed speech", "author": ["A. Venkataraman"], "venue": "Comput. Linguist., vol. 27, no. 3, pp. 351\u2013372, 2001.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling", "author": ["D. Mochihashi", "T. Yamada", "N. Ueda"], "venue": "Proc. ACL, 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning a language model from continuous speech", "author": ["G. Neubig", "M. Mimura", "S. Mori", "T. Kawahara"], "venue": "Proc. Interspeech, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "A joint learning model of word segmentation, lexical acquisition and phonetic variability", "author": ["M. Elsner", "S.J. Goldwater", "N. Feldman", "F. Wood"], "venue": "Proc. EMNLP, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised word segmentation from noisy input", "author": ["J. Heymann", "O. Walter", "R. Haeb-Umbach", "B. Raj"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Discovering phone patterns in spoken utterances by non-negative matrix factorization", "author": ["V. Stouten", "K. Demuynck", "H. Van hamme"], "venue": "IEEE Signal Proc. Let., vol. 15, pp. 131\u2013134, 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["C.-y. Lee", "T. O\u2019Donnell", "J.R. Glass"], "venue": "Trans. ACL, vol. 3, pp. 389\u2013403, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A nonparametric Bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J.R. Glass"], "venue": "Proc. ACL, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Weak top-down constraints for unsupervised acoustic model training", "author": ["A. Jansen", "S. Thomas", "H. Hermansky"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Mommy and me: familiar names help launch babies into speech-stream segmentation", "author": ["H. Bortfeld", "J.L. Morgan", "R.M. Golinkoff", "K. Rathbun"], "venue": "Psychol. Sci., vol. 16, no. 4, pp. 298\u2013304, 2005.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning phonetic categories by learning a lexicon", "author": ["N.H. Feldman", "T.L. Griffiths", "J.L. Morgan"], "venue": "Proc. CCSS, 2009.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["H. Kamper", "W. Wang", "K. Livescu"], "venue": "arXiv preprint arXiv:1510.01032, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["K. Levin", "K. Henry", "A. Jansen", "K. Livescu"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Comput., vol. 15, no. 6, pp. 1373\u20131396, 2003.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2399\u20132434, 2006.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised lexical clustering of speech segments using fixed-dimensional acoustic embeddings", "author": ["H. Kamper", "A. Jansen", "S. King", "S.J. Goldwater"], "venue": "Proc. SLT, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian Reasoning and Machine Learning", "author": ["D. Barber"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Conjugate Bayesian analysis of the Gaussian distribution", "author": ["K.P. Murphy"], "venue": "2007. [Online]. Available: http://www.cs.ubc.ca/\u223cmurphyk/mypapers. html", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Gibbs sampling for the uninitiated", "author": ["P. Resnik", "E. Hardisty"], "venue": "University of Maryland, College Park, MD, Tech. Rep., 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "A fully Bayesian approach to unsupervised part-of-speech tagging", "author": ["S.J. Goldwater", "T.L. Griffiths"], "venue": "Proc. ACL, 2007.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian methods for hidden Markov models", "author": ["S.L. Scott"], "venue": "J. Am. Stat. Assoc., vol. 97, no. 457, pp. 337\u2013351, 2002.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2002}, {"title": "A database for speaker-independent digit recognition", "author": ["R.G. Leonard"], "venue": "Proc. ICASSP, 1984.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1984}, {"title": "A computational model for unsupervised word discovery", "author": ["L. ten Bosch", "B. Cranen"], "venue": "Proc. Interspeech, 2007.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "Pattern discovery in continuous speech using block diagonal infinite HMM", "author": ["N. Vanhainen", "G. Salvi"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "A nonparametric Bayesian alternative to spike sorting", "author": ["F. Wood", "M.J. Black"], "venue": "J. Neurosci. Methods, vol. 173, no. 1, pp. 1\u201312, 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Frequency-domain linear prediction for temporal features", "author": ["M. Athineos", "D.P.W. Ellis"], "venue": "Proc. ASRU, 2003.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2003}, {"title": "Unsupervised word discovery from speech using automatic segmentation into syllable-like units", "author": ["O.J. R\u00e4s\u00e4nen", "G. Doyle", "M.C. Frank"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Examples include the query-by-example systems of [1]\u2013[4], and the unsupervised term discovery (UTD) systems of [5], [6], which aim to find repeated words or phrases in a speech collection.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "Examples include the query-by-example systems of [1]\u2013[4], and the unsupervised term discovery (UTD) systems of [5], [6], which aim to find repeated words or phrases in a speech collection.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "Examples include the query-by-example systems of [1]\u2013[4], and the unsupervised term discovery (UTD) systems of [5], [6], which aim to find repeated words or phrases in a speech collection.", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "Examples include the query-by-example systems of [1]\u2013[4], and the unsupervised term discovery (UTD) systems of [5], [6], which aim to find repeated words or phrases in a speech collection.", "startOffset": 116, "endOffset": 119}, {"referenceID": 6, "context": "Here, researchers are interested in the problems faced during early language learning: infants have to learn phonetic categories and a lexicon for their native language using speech audio as input [7].", "startOffset": 197, "endOffset": 200}, {"referenceID": 7, "context": "However, these models take transcribed symbol sequences as input, rather than continuous speech [8].", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "A few recent studies [9]\u2013[12], summarized in detail in Section II-C, share our goal of full-coverage speech segmentation.", "startOffset": 21, "endOffset": 24}, {"referenceID": 11, "context": "A few recent studies [9]\u2013[12], summarized in detail in Section II-C, share our goal of full-coverage speech segmentation.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "[8] (which took symbolic input) to the continuous speech domain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11], without specifying the vocabulary size and without relying on a UTD system for model initialization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "2 \u2018Segmental\u2019 is used here, as in [13], to distinguish approaches operating on whole units of speech from those doing frame-wise modelling.", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "Our preliminary work in this direction was presented in [14].", "startOffset": 56, "endOffset": 60}, {"referenceID": 4, "context": "Most state-of-the-art UTD systems are based on the seminal work of Park and Glass [5], who proposed a method to find pairs of similar audio segments and then cluster them into hypothesized word types.", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "Follow-up work has built on Park and Glass\u2019 original method in various ways, for example through improved feature representations [1], [2] or by greatly improving its efficiency [6].", "startOffset": 130, "endOffset": 133}, {"referenceID": 1, "context": "Follow-up work has built on Park and Glass\u2019 original method in various ways, for example through improved feature representations [1], [2] or by greatly improving its efficiency [6].", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "Follow-up work has built on Park and Glass\u2019 original method in various ways, for example through improved feature representations [1], [2] or by greatly improving its efficiency [6].", "startOffset": 178, "endOffset": 181}, {"referenceID": 6, "context": "language, with computational models seen as one way to specify and test particular theories (see [7], [8] for reviews).", "startOffset": 97, "endOffset": 100}, {"referenceID": 7, "context": "language, with computational models seen as one way to specify and test particular theories (see [7], [8] for reviews).", "startOffset": 102, "endOffset": 105}, {"referenceID": 14, "context": "Early word segmentation approaches using phonemic input include those based on transition probabilities [15], neural networks [16] and probabilistic models [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "Early word segmentation approaches using phonemic input include those based on transition probabilities [15], neural networks [16] and probabilistic models [17].", "startOffset": 126, "endOffset": 130}, {"referenceID": 16, "context": "Early word segmentation approaches using phonemic input include those based on transition probabilities [15], neural networks [16] and probabilistic models [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 7, "context": "[8], which was shown to yield more accurate segmentations than previous work.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] who presented a blocked sampler that uses dynamic programming to resample the segmentation of a full utterance at once.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "\u2019s original model assumed that every instance of a word is represented by the same sequence of phonemes; later studies [19]\u2013[21] proposed noisy-channel extensions in order to deal with variation in word pronunciation.", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "\u2019s original model assumed that every instance of a word is represented by the same sequence of phonemes; later studies [19]\u2013[21] proposed noisy-channel extensions in order to deal with variation in word pronunciation.", "startOffset": 124, "endOffset": 128}, {"referenceID": 18, "context": "In [19]\u2013 [21], variability is modeled symbolically as the conditional probability of an output phone given the true phoneme (so the input to the models is a sequence or lattice of phones), whereas our channel model is a true acoustic model (the input is the speech signal).", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [19]\u2013 [21], variability is modeled symbolically as the conditional probability of an output phone given the true phoneme (so the input to the models is a sequence or lattice of phones), whereas our channel model is a true acoustic model (the input is the speech signal).", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "As in the phonetic noisy channel model of [20], we learn the language model and channel model jointly.", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "Sun and Van hamme [9] developed an approach based on non-negative matrix factorization (NMF).", "startOffset": 18, "endOffset": 21}, {"referenceID": 21, "context": "NMF is a technique which allows fixed-dimensional representations of speech utterances (typically co-occurrence statistics of acoustic events) to be factorized into lower-dimensional parts, corresponding to phones or words [22].", "startOffset": 223, "endOffset": 227}, {"referenceID": 8, "context": "To capture temporal information, Sun and Van hamme [9] incorporated NMF in a maximum likelihood training procedure for discrete-density HMMs.", "startOffset": 51, "endOffset": 54}, {"referenceID": 9, "context": "[10] used an HMM-based approach which", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "3], [23] developed a non-parametric hierarchical Bayesian model for full-coverage speech segmentation.", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "Using adaptor grammars (a generalized framework for defining such Bayesian models), an unsupervised subword acoustic model developed in earlier work [24] was extended with syllable and word layers, as well as a noisy channel model for capturing phonetic variability in word pronunciations.", "startOffset": 149, "endOffset": 153}, {"referenceID": 22, "context": "In [23], although unsupervised WER was not reported, the full-coverage segmentation of the system was evaluated in terms of word boundary F -score.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "As in [10], they followed a two-step iterative approach of subword and word discovery.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "1%; using UTD [5] to provide initial word identities and boundaries, 18.", "startOffset": 14, "endOffset": 17}, {"referenceID": 24, "context": "On the positive side, it is often easier to identify cross-speaker similarities between words than between subwords [25], which is why most UTD systems focus on longer-spanning patterns.", "startOffset": 116, "endOffset": 120}, {"referenceID": 25, "context": "And from a cognitive perspective, there is evidence that infants are able to segment whole words from continuous speech before phonetic contrasts in their native language have been fully learned [26], [27].", "startOffset": 195, "endOffset": 199}, {"referenceID": 26, "context": "And from a cognitive perspective, there is evidence that infants are able to segment whole words from continuous speech before phonetic contrasts in their native language have been fully learned [26], [27].", "startOffset": 201, "endOffset": 205}, {"referenceID": 27, "context": "Improved embedding techniques are the subject of current research [28] and it would be straightforward to replace the current embedding approach with any other (including one that incorporates subword modelling).", "startOffset": 66, "endOffset": 70}, {"referenceID": 28, "context": "[29], as summarized below.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "In [29], the mapping f is performed as follows.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "Dimensionality reduction is performed using Laplacian eigenmaps [30].", "startOffset": 64, "endOffset": 68}, {"referenceID": 30, "context": "To embed an arbitrary segment Y which is not an element of Yref, a kernel-based out-of-sample extension is used [31].", "startOffset": 112, "endOffset": 116}, {"referenceID": 30, "context": "In [31], it was shown that the optimal projection to the jth dimension in the target space is given by", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "We have given only a brief outline of the embedding method here; complete details can be found in [29]\u2013[31].", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "We have given only a brief outline of the embedding method here; complete details can be found in [29]\u2013[31].", "startOffset": 103, "endOffset": 107}, {"referenceID": 31, "context": "In [32] we showed that the Bayesian GMM performs significantly better in clustering word embeddings than a regular GMM trained with expectation-maximization.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "All components share the same fixed covariance matrix \u03c3I; preliminary experiments, based on [32], indicated that it", "startOffset": 92, "endOffset": 96}, {"referenceID": 33, "context": "171], and a sphericalcovariance Gaussian prior in (5) since it is conjugate to the Gaussian distribution in (6) [34].", "startOffset": 112, "endOffset": 116}, {"referenceID": 34, "context": ", zN ) using a collapsed Gibbs sampler [35].", "startOffset": 39, "endOffset": 43}, {"referenceID": 33, "context": "is the posterior predictive of xi for a Gaussian distribution with known spherical covariance and a conjugate prior over its means, which is itself a spherical covariance Gaussian distribution [34].", "startOffset": 193, "endOffset": 197}, {"referenceID": 33, "context": "(11) and xk\\i is component k\u2019s sample mean for this dimension [34].", "startOffset": 62, "endOffset": 66}, {"referenceID": 36, "context": "Although we use a model with a fixed number of components K, Bayesian models that marginalize over their parameters have been shown to prefer sparser solutions than maximumlikelihood models with the same structure [37].", "startOffset": 214, "endOffset": 218}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "This is done using the forward filtering backward sampling dynamic programming algorithm [38].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "Once all \u03b1\u2019s have been calculated, a segmentation can be sampled backwards [18].", "startOffset": 75, "endOffset": 79}, {"referenceID": 28, "context": "In [29], this set was composed of true word segments.", "startOffset": 3, "endOffset": 7}, {"referenceID": 38, "context": "We evaluate using the TIDigits connected digit corpus [39], which has a vocabulary of 11 English digits: \u2018oh\u2019 and \u2018zero\u2019 through \u2018nine\u2019.", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "results on the same corpus as several previous unsupervised studies [9], [11], [40], [41].", "startOffset": 68, "endOffset": 71}, {"referenceID": 10, "context": "results on the same corpus as several previous unsupervised studies [9], [11], [40], [41].", "startOffset": 73, "endOffset": 77}, {"referenceID": 39, "context": "results on the same corpus as several previous unsupervised studies [9], [11], [40], [41].", "startOffset": 79, "endOffset": 83}, {"referenceID": 40, "context": "results on the same corpus as several previous unsupervised studies [9], [11], [40], [41].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "[11] as baselines in our own experiments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "a many-to-one mapping, as in [9]).", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "\u2022 Unsupervised WER: Discovered types are again mapped, but here at most one cluster is mapped to a ground truth digit [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "We consider two system initialization strategies, which were also used in [11]: (i) random initialization; and (ii) initialization from a separate UTD system.", "startOffset": 74, "endOffset": 78}, {"referenceID": 5, "context": "We use the UTD system of [6].", "startOffset": 25, "endOffset": 28}, {"referenceID": 31, "context": "The hyperparameters of our model are set mainly based on previous work on other tasks [32].", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "Below, we also note the changes we made from our own preliminary work [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "The hyperparameters we used in [14] led to far less consistent performance over multiple sampling runs: WER standard deviations were in the order of 9% absolute, compared to the deviations of less than 1% that we obtain in Section IV-C.", "startOffset": 31, "endOffset": 35}, {"referenceID": 31, "context": "For the acoustic model (Section III-B), we use the following hyperparameters, based on [32], [34], [42]: all-zero vector for \u03bc0, a = 1, \u03c3 2 = 0.", "startOffset": 87, "endOffset": 91}, {"referenceID": 33, "context": "For the acoustic model (Section III-B), we use the following hyperparameters, based on [32], [34], [42]: all-zero vector for \u03bc0, a = 1, \u03c3 2 = 0.", "startOffset": 93, "endOffset": 97}, {"referenceID": 41, "context": "For the acoustic model (Section III-B), we use the following hyperparameters, based on [32], [34], [42]: all-zero vector for \u03bc0, a = 1, \u03c3 2 = 0.", "startOffset": 99, "endOffset": 103}, {"referenceID": 28, "context": "Based on [29], [32] we use the following parameters for the fixed-dimensional embedding extraction (Section III-A): dimensionality D = 11, k = 30, \u03c3K = 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": "Based on [29], [32] we use the following parameters for the fixed-dimensional embedding extraction (Section III-A): dimensionality D = 11, k = 30, \u03c3K = 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": "D = 50 in [32]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "In our preliminary work on TIDigits [14], we used D = 15 with Nref = 5000, but here we found that using D = 11 with a bigger reference set Nref = 8000 gave more consistent performance on TIDigits1.", "startOffset": 36, "endOffset": 40}, {"referenceID": 42, "context": "For embedding extraction, speech is parameterized as 15-dimensional frequency-domain linear prediction features [43] at a frame rate of 10 ms, and cosine distance is used as similarity metric in DTW alignments.", "startOffset": 112, "endOffset": 116}, {"referenceID": 31, "context": "As in [32], embeddings are normalized to the unit sphere.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "To improve sampler convergence, we use simulated annealing [8], by raising the boundary probability in (15) to the power 1 \u03b3 before sampling, where \u03b3 is a temperature parameter.", "startOffset": 59, "endOffset": 62}, {"referenceID": 34, "context": "In all cases we run 5 sampling chains in parallel [35],", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "5%, which is higher than the scores of around 85% reported by Sun and Van hamme [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 10, "context": "[11], we use the exemplar set discovered in iteration 3 of Table I (using an unconstrained setup up to this point) and then constrain the Bayesian segmental model to 15 components.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] AND THE SEGMENTAL BAYESIAN MODEL", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Discrete HMM [11] yes 32.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "005 (which we used in the experiments above, based on [32]).", "startOffset": 54, "endOffset": 58}, {"referenceID": 27, "context": "To scale to larger corpora, both the efficiency and accuracy of the embeddings would therefore need to be improved (see [28] for recent supervised efforts in this direction).", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "Many of the previous studies [11], [12], [23], [44] use a first-pass method to find positions of high acoustic change and then only allow word boundaries at these positions.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "Many of the previous studies [11], [12], [23], [44] use a first-pass method to find positions of high acoustic change and then only allow word boundaries at these positions.", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "Many of the previous studies [11], [12], [23], [44] use a first-pass method to find positions of high acoustic change and then only allow word boundaries at these positions.", "startOffset": 41, "endOffset": 45}, {"referenceID": 43, "context": "Many of the previous studies [11], [12], [23], [44] use a first-pass method to find positions of high acoustic change and then only allow word boundaries at these positions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "In particular, [20] showed that for joint segmentation and clustering of noisy phone sequences, a bigram model was needed to improve Word embedding from cluster 33 (\u2192 one)", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "Following [8], [18] it is mathematically straightforward to extend the algorithm of Section III-C to more complex language models.", "startOffset": 10, "endOffset": 13}, {"referenceID": 17, "context": "Following [8], [18] it is mathematically straightforward to extend the algorithm of Section III-C to more complex language models.", "startOffset": 15, "endOffset": 19}], "year": 2016, "abstractText": "In settings where only unlabelled speech data is available, speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. A similar problem is faced when modelling infant language acquisition. In these cases, categorical linguistic structure needs to be discovered directly from speech audio. We present a novel unsupervised Bayesian model that segments unlabelled speech and clusters the segments into hypothesized word groupings. The result is a complete unsupervised tokenization of the input speech in terms of discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional acoustic vector space. The model, implemented as a Gibbs sampler, then builds a whole-word acoustic model in this space while jointly performing segmentation. We report word error rates in a small-vocabulary connected digit recognition task by mapping the unsupervised decoded output to ground truth transcriptions. The model achieves around 20% error rate, outperforming a previous HMM-based system by about 10% absolute. Moreover, in contrast to the baseline, our model does not require a pre-specified vocabulary size.", "creator": "LaTeX with hyperref package"}}}