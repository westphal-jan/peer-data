{"id": "1603.03112", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Mar-2016", "title": "Building a Fine-Grained Entity Typing System Overnight for a New X (X = Language, Domain, Genre)", "abstract": "Recent research has shown great progress on fine-grained entity typing. Most existing methods require pre-defining a set of types and training a multi-class classifier from a large labeled data set based on multi-level linguistic features. They are thus limited to certain domains, genres and languages. In this paper, we propose a novel unsupervised entity typing framework by combining symbolic and distributional semantics. We start from learning general embeddings for each entity mention, compose the embeddings of specific contexts using linguistic structures, link the mention to knowledge bases and learn its related knowledge representations. Then we develop a novel joint hierarchical clustering and linking algorithm to type all mentions using these representations. This framework doesn't rely on any annotated data, predefined typing schema, or hand-crafted features, therefore it can be quickly adapted to a new domain, genre and language. Furthermore, it has great flexibility at incorporating linguistic structures (e.g., Abstract Meaning Representation (AMR), dependency relations) to improve specific context representation. Experiments on genres (news and discussion forum) show comparable performance with state-of-the-art supervised typing systems trained from a large amount of labeled data. Results on various languages (English, Chinese, Japanese, Hausa, and Yoruba) and domains (general and biomedical) demonstrate the portability of our framework.", "histories": [["v1", "Thu, 10 Mar 2016 00:33:28 GMT  (640kb,D)", "http://arxiv.org/abs/1603.03112v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["lifu huang", "jonathan may", "xiaoman pan", "heng ji"], "accepted": false, "id": "1603.03112"}, "pdf": {"name": "1603.03112.pdf", "metadata": {"source": "CRF", "title": "Building a Fine-Grained Entity Typing System Overnight for a New X (X = Language, Domain, Genre)", "authors": ["Lifu Huang", "Jonathan May", "Xiaoman Pan", "Heng Ji"], "emails": ["huangl7@rpi.edu", "jonmay@isi.edu", "panx2@rpi.edu", "jih@rpi.edu", "permissions@acm.org."], "sections": [{"heading": "1. INTRODUCTION", "text": "\"Entity typing\" is an important and challenging task that aims to automatically assign the types to the entities mentioned in unstructured documents. \"Previous entity types mainly work on a small number of predefined types. For example, Mitt-7 [20] defined the three most common types: person, organization, and location. ACE11http: / / www.itl.nist.gov These types carry this notice and the full citation on the first page of this work, which is used by others as Wikipedia. Abstracting with credit is permitted others, or republish, to post on servers or redistributed to lists, requires these notes and the full citation on the first page. Copyrights for components of components of its owned by ACM."}, {"heading": "2. RELATED WORK", "text": "In recent years, we have focused on fine-grained typing. Fleischman et al. [17] Classifications of person entities into eight fine-grained subtypes based on local contexts. Sekine [43] has defined more than 200 types of entities. Abstract Meaning Representation (AMR) [3] has defined more than 100 types of entities. FIGER [51] has derived 112 entity types from the freebase [5] and developed a linear chain CRF model [28] for common entity texts and typing. Gillick et al. [18] and Yogatama et al. [53] have proposed the task of context-dependent typing of entities where acceptable typing is limited to local contexts (e.g. a sentence or a document)."}, {"heading": "3. APPROACH OVERVIEW", "text": "Figure 2 illustrates the overall framework of our system, which takes the boundaries of all entity mentions as input and produces atypical terms for each mention as output; the framework begins by learning three types of representations: (i) a general, global-based, distributed representation of an entity; (ii) a specific context representation to model local context words based on linguistic structures; (iii) a knowledge representation to model domain-specific knowledge for each mention. For example, Figure 1 shows these three representations from which the type \"pHER3\" can be derived. After learning general and context-specific embedding, we apply these three representations as input to a hierarchical X-mean hierarchy. Based on the linkage results, we can determine the knowledge representation and find a pppath for each strongly linkable archical entries to determine the optimal structure of each of these 40]."}, {"heading": "4. REPRESENTATION GENERATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 General Entity Representation", "text": "Based on heuristic 1, we can deduce the types of most entity mentions. For example, \"Mitt Romney\" and \"John McCain\" are both politicians of the same country, and \"HER2\" and \"HER3\" refer to similar \"ERBB (receptor tyrosine protein kinase)\" and therefore have the same entity type \"enzyme.\" We begin by capturing the semantic information of entity mentions based on general lexical embedding, an effective technique for capturing general semantics of words or phrases based on their global context. Several models [33, 32, 55, 35] have been proposed to generate word embedding, using the continuous skip-gram model [32], which is based on a large set of unlabeled in-domain data sets. Most entity mentions are multi-word information units. To generate their phrase embedding, we use the continuous skip-gram model [32], which is based on a large set of unlabeled in-domain data sets."}, {"heading": "4.2 Specific-Context Representation", "text": "As a matter of fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate, to agitate"}, {"heading": "4.3 Knowledge Representation", "text": "The types of some entities rely heavily on domain-specific knowledge and generate knowledge. Existing broad knowledge bases such as DBpedia, Freebase or YAGO, as well as domain-specific ontologies such as BioPortal and NCBO can provide useful knowledge for specific fine-grained types (e.g. \"person,\" \"governor\" for \"Mitt Romney,\" \"place,\" \"place\" for \"Michigan\"), which can be used for typing entities. For the biomedical domain, we can consult BioPortals for domain-specific properties and type designations (e.g. \"oncogenes\" for \"HER2\"). These properties can be used to measure the similarity between mentions."}, {"heading": "5. JOINT LINKING, HIERARCHICAL TYPING, AND NAMING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Hierarchical Typing", "text": "For an entity we mentioned m = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.000002 = 0.0002 = 0.00002 = 0.00002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0000,002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0000,002 = 0.0002 = 0.0000,002 = 0.0002 = 0.0000,002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0.0002 = 0,002 = 0,002 0,002 0,002 = 0,000,002 = 0,002 = 0,002 0,002 = 0,002 0,000,002 0,002 = 0,002 0,002 0,002 = 0,002 0,002 = 0,002 0,002 0,002 0,002 0,002 = 0,002 0,002 0,002 0,002 0,002 0,002 0,002 = 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,002 0,"}, {"heading": "5.2 Hierarchical Type Naming", "text": "Considering the examples in Section 1 again, we can link the entity mentioning \"Mitt Romney\" to YAGO and extract a type path from entity to root: \"Mitt Romney \u2192 Governor \u2192 Politician \u2192 Politician \u2192 Person \u2192 Entity.\" Similarly, we can link \"HER2\" to \"ERBB2\" in BioPortal and extract the type path from entity to ontology root as: \"ERBB2 \u2192 ProtoOncogenes \u2192 Type Oncogenes \u2192 Genome \u2192 Genome Components and Processes.\" We first normalize the type paths and remove them to general type paths candidates (e.g. \"Entity,\" \"Topical Describers\")."}, {"heading": "6. EXPERIMENTS AND EVALUATION", "text": "In this section we present an evaluation of the proposed framework on different genres, domains and languages as well as a comparison with modern systems."}, {"heading": "6.1 Data", "text": "We will first present the data sets for our experiments. To compare the performance of our framework with the most advanced name identifiers and evaluate its effectiveness in various areas and genres, we will first conduct experiments on the data sets of the Abstract Meaning Representation (AMR), which include perfect mention boundaries with fine-grained entity types. For the multi-language experiment, we will use data sets from the DARPA LORELEI Program 3 and foreign news agencies. Detailed data statistics are in Table 2. Since our approach is based on word embedding, which needs to be trained from a large corpus of unlabeled in-domain articles, we will collect all English and Japanese articles from the Wikipedia dump of August 11, 2014, to learn English and Japanese word / phrase embedding, and collect all articles from the fourth edition of the Chinese Gigaword Corpus4 to learn Chinese word / phrase embedding."}, {"heading": "6.2 Evaluation Metrics", "text": "Our framework can automatically detect new fine-grained types. Therefore, in addition to the measurements Precision, Recall and Fmeasure, the following standard clustering measurements are also used: 1) Purity: To calculate purity, each system cluster is assigned to the reference class with the largest overlap of mentions; the sum of all clusters mentioned is then assigned to N.purity = 1 max1 \u2264 i \u2264 M | Cj; Ri is the total number of mentions; K is the number of clusters generated; M is the number of clusters in the answer; Cj is the number of mentions in the jth cluster in our system; and Ri is the number of mentions for the ith type in the answer key.2) Precision, Recall, F-measure (F): Here, we use F-measurements to evaluate the mentions in the individual languages associated with the aforementioned C.Ri, Measurands, Measurands i, Measurands i, Measurands i, Measurands i, Measurands i, i)."}, {"heading": "6.3 Comparison with State-of-the-art", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves."}, {"heading": "6.4 Comparison on Genres", "text": "For comparison between news and discussion forum genres, we use perfect entity boundaries and perfect AMR annotation results to model local contexts and link entities to DBpedia [39]. Figure 6 shows the power. We see that our system performs much better in news articles than in discussion forum posts, for two reasons: (1) many entities appear as abbreviations in discussion forums, which presents challenges for entering and linking entities. For example, in the following post: \"The joke will refer to members of the House of Representatives who are promised a bill to\" \"solve\" the problems with the Senate bill. (2) It is difficult to provide precise general semantic and knowledge representations for mentions like \"House\" (which refers to \"House of Representatives\") and \"Section\" (which refers to \"Democratic Party of the United States\")."}, {"heading": "6.5 Comparison on Domains", "text": "To demonstrate the portability of our framework, we take the biomedical domain as a case study. For a fair comparison, we used perfect AMR semanticgraphs and perfect mention limits. Figure 6 compares performance for news and biomedical articles. As shown in Figure 6, our system works much better on biomedical data than on general news data. In an in-depth analysis of the experimental results, we found that most of the entities in the biomedical domain are unique and unique, and mentions of the same type often share the same name strings. For example, \"HER2,\" \"HER3\" and \"HER4\" refer to similar \"ProtoOncogenes\"; \"B-RAF\" and \"C-RAF\" share the same type of \"RAF kinases.\" However, it is always the opposite in the general news domain."}, {"heading": "6.6 Comparison on Languages", "text": "To evaluate the impact of local contexts on unit input, we compare performance based on AMR and the embedding of context words that occur within a limited window. In our experiment, the window size is 6. Figure 8 shows the performance of English, Chinese and Japanese message data. Figure 8 shows that our framework in Chinese and Japanese has also provided performance comparable to English. The main reason for this is that units in Chinese and Japanese have less ambiguity than English. Almost all of the same name strings refer to the same type of units. Based on the ambiguity, we measure in Section 6.5 that ambiguity is below 0.05 for both Chinese and Japanese."}, {"heading": "6.7 Comparison on Linguistic Structures", "text": "To evaluate the impact of contextual representations on unit typing, we compare performance using various linguistic structures, including AMR, dependency relationships, and word bags in a given window. Figure 9 shows performance using English message records. The overall semantic representation and knowledge representation of the units are the same for all three types of experiments. Compared to word bags, both AMR and dependency-based context-specific representations can perform better than word bags, showing how important the inclusion of a broader range of deep knowledge and semantic relationships is for context representation. In particular, AMR and dependency relationships can capture more meaningful context information more effectively than word bags. In the sentence \"The Kuwolsan, which is rumored to carry weapons or ammunition that may be destined for Pakistan,\" Kuwolsan is a term that falls out of the vocabulary. The word-based method generates contextual representation of weapons, as opposed to the contextual representation of weapons, such as the ring."}, {"heading": "6.8 Type Naming Evaluation", "text": "To evaluate type naming performance, we ask 3 human commentators to determine whether the type designation matches the reference class label on the English messaging data set. We adjust the parameter \u03bb used in Section 5.2 to find the optimal threshold for type naming, which is in Figure 10. Based on the comparative results, the naming performance is close to 90%.7. CONCLUSIONS AND FUTURE WORK In this paper, we take a fresh look at the problem of fine-grained unit typing and, for the first time, propose an unattended framework that incorporates general semantics, specific contexts and domain-specific knowledge to discover the fine-grained types. This framework takes people out of the loop and does not require commented data or predefined types. Without the needs of language-specific characteristics and resources, this framework can easily be adapted to other types, genres and languages."}, {"heading": "8. REFERENCES", "text": "[1] C. Baker and H. Sato. Die framenet data and software. InACL, 2003. [2] K. Balog and R. Neumayer. Hierarchical target typeidentification for entity-oriented queries. In CIKM. L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and N. Schneider. Abstract meaning representation for sembanking. In ACL Workshop on Linguistic Annotation and Interoperability with Discourse, 2013. [4] B. Bazzanella, H. Stoermer, and P. Bouquet. Searching for individual entities. In IRI. [5] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Freebase."}], "references": [{"title": "The framenet data and software", "author": ["C. Baker", "H. Sato"], "venue": "In ACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Abstract meaning representation for sembanking", "author": ["L. Banarescu", "C. Bonial", "S. Cai", "M. Georgescu", "K. Griffitt", "U. Hermjakob", "K. Knight", "P. Koehn", "M. Palmer", "N. Schneider"], "venue": "In ACL Workshop on Linguistic Annotation and Interoperability with Discourse,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Recursive neural networks for learning logical semantics", "author": ["S. Bowman", "C. Potts", "C. Manning"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Using encyclopedic knowledge for named entity disambiguation", "author": ["R. Bunescu", "M. Pasca"], "venue": "In EACL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Finet: Context-aware fine-grained named entity typing", "author": ["L.D. Corro", "A. Abujabal", "R. Gemulla", "G. Weikum"], "venue": "In EMNLP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Large-scale named entity disambiguation based on wikipedia data", "author": ["S. Cucerzan"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "A discriminative graph-based parser for the abstract meaning", "author": ["J. Flanigan", "S. Thomson", "J. Carbonell", "C. Dyer", "N.A. Smith"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Context-dependent fine-grained entity type tagging", "author": ["D. Gillick", "N. Lazic", "K. Ganchev", "J. Kirchner", "D. Huynh"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Message understanding conference-6: A brief history", "author": ["R. Grishman", "B. Sundheim"], "venue": "In COLING,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1996}, {"title": "Named entity disambiguation by leveraging wikipedia semantic knowledge", "author": ["X. Han", "J. Zhao"], "venue": "In CIKM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Discovering relations among named entities from large corpora", "author": ["T. Hasegawa", "S. Sekine", "R. Grishman"], "venue": "In ACL,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["K. Hermann", "P. Blunsom"], "venue": "In ACL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Overview of the tac 2010 knowledge base population", "author": ["H. Ji", "R. Grishman", "H.T. Dang", "K. Griffitt", "J. Ellis"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Extending verbnet with novel verb classes", "author": ["K. Kipper", "A. Korhonen", "N. Ryant", "M. Palmer"], "venue": "In LREC,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F.C. Pereira"], "venue": "In ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive Science,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Named entity recognition for question answering", "author": ["D. Moll\u00e1", "M. Van Zaanen", "D. Smith"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "Sparse autoencoder", "author": ["A. Ng"], "venue": "CS294A Lecture notes,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Extended named entity ontology with attribute information", "author": ["S. Sekine"], "venue": "In LREC,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2008}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "Q. Le", "C. Manning", "A. Ng"], "venue": "TACL,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": "In EMNLP,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}, {"title": "Fine-grained entity recognition", "author": ["L. Xiao", "D.S. Weld"], "venue": "In  AAAI,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "An exploration of embeddings for generalized phrases", "author": ["W. Yin", "H. Sch\u00fctze"], "venue": "In ACL Workshop on Student Research,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}, {"title": "Hyena: Hierarchical type classification for entity names", "author": ["A. Yosef", "S. Bauer", "J. Hoffart", "M. Spaniol", "G. Weikum"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2012}, {"title": "Entity linking for biomedical literature", "author": ["J.G. Zheng", "D. Howsmon", "B. Zhang", "J. Hahn", "D. McGuinness", "J. Hendler", "H. Ji"], "venue": "BMC Medical Informatics and Decision Making,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "For example, MUC-7 [20] defined the three most common types: Person, Organization, and Location.", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "These entity types are very useful for many downstream Natural Language Processing (NLP) and Information Retrieval (IR) tasks such as entity disambiguation [10, 8, 21], entity linking [42, 30, 13], relation extraction [22, 11, 7], knowledge base completion [25, 12, 46], question answering [36, 47], and query analysis [2, 4].", "startOffset": 156, "endOffset": 167}, {"referenceID": 3, "context": "These entity types are very useful for many downstream Natural Language Processing (NLP) and Information Retrieval (IR) tasks such as entity disambiguation [10, 8, 21], entity linking [42, 30, 13], relation extraction [22, 11, 7], knowledge base completion [25, 12, 46], question answering [36, 47], and query analysis [2, 4].", "startOffset": 156, "endOffset": 167}, {"referenceID": 9, "context": "These entity types are very useful for many downstream Natural Language Processing (NLP) and Information Retrieval (IR) tasks such as entity disambiguation [10, 8, 21], entity linking [42, 30, 13], relation extraction [22, 11, 7], knowledge base completion [25, 12, 46], question answering [36, 47], and query analysis [2, 4].", "startOffset": 156, "endOffset": 167}, {"referenceID": 10, "context": "These entity types are very useful for many downstream Natural Language Processing (NLP) and Information Retrieval (IR) tasks such as entity disambiguation [10, 8, 21], entity linking [42, 30, 13], relation extraction [22, 11, 7], knowledge base completion [25, 12, 46], question answering [36, 47], and query analysis [2, 4].", "startOffset": 218, "endOffset": 229}, {"referenceID": 18, "context": "These entity types are very useful for many downstream Natural Language Processing (NLP) and Information Retrieval (IR) tasks such as entity disambiguation [10, 8, 21], entity linking [42, 30, 13], relation extraction [22, 11, 7], knowledge base completion [25, 12, 46], question answering [36, 47], and query analysis [2, 4].", "startOffset": 290, "endOffset": 298}, {"referenceID": 23, "context": "Recent work [51, 29] suggests that using a larger set of finegrained types can lead to substantial improvement for these downstream NLP applications.", "startOffset": 12, "endOffset": 20}, {"referenceID": 23, "context": "[51] described three remaining challenges for the fine-grained entity typing task: selection of a set of fine-grained types, creation of annotated data and linguistic features, and disambiguation of fine-grained types.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Most existing approaches [17, 19, 14, 51, 54, 37, 18, 53, 9] addressed these problems in a \u201cdistant supervision\u201d fashion, by pre-defining a set of fine-grained types based on existing knowledge bases (KBs) (e.", "startOffset": 25, "endOffset": 60}, {"referenceID": 25, "context": "Most existing approaches [17, 19, 14, 51, 54, 37, 18, 53, 9] addressed these problems in a \u201cdistant supervision\u201d fashion, by pre-defining a set of fine-grained types based on existing knowledge bases (KBs) (e.", "startOffset": 25, "endOffset": 60}, {"referenceID": 7, "context": "Most existing approaches [17, 19, 14, 51, 54, 37, 18, 53, 9] addressed these problems in a \u201cdistant supervision\u201d fashion, by pre-defining a set of fine-grained types based on existing knowledge bases (KBs) (e.", "startOffset": 25, "endOffset": 60}, {"referenceID": 4, "context": "Most existing approaches [17, 19, 14, 51, 54, 37, 18, 53, 9] addressed these problems in a \u201cdistant supervision\u201d fashion, by pre-defining a set of fine-grained types based on existing knowledge bases (KBs) (e.", "startOffset": 25, "endOffset": 60}, {"referenceID": 20, "context": "Sekine [43] defined more than 200 types of entities.", "startOffset": 7, "endOffset": 11}, {"referenceID": 1, "context": "The Abstract Meaning Representation (AMR) [3] defined more than 100 types of entities.", "startOffset": 42, "endOffset": 45}, {"referenceID": 23, "context": "FIGER [51] derived 112 entity types from Freebase [5] and trained a linearchain CRF model [28] for joint entity identification and typing.", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "FIGER [51] derived 112 entity types from Freebase [5] and trained a linearchain CRF model [28] for joint entity identification and typing.", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "[18] and Yogatama et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "HYENA [54] derived a very fine-grained type taxonomy from YAGO [24] based on a mapping between Wikipedia categories and WordNet synsets.", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": "[52] computed embeddings for generalized phrases, including both conventional linguistic phrases and skipbigrams.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[35] proposed an additive model and a multiplicative model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Linguistic structures have been proven useful to capture the semantics of basic language units [45, 23, 44, 6].", "startOffset": 95, "endOffset": 110}, {"referenceID": 11, "context": "Linguistic structures have been proven useful to capture the semantics of basic language units [45, 23, 44, 6].", "startOffset": 95, "endOffset": 110}, {"referenceID": 21, "context": "Linguistic structures have been proven useful to capture the semantics of basic language units [45, 23, 44, 6].", "startOffset": 95, "endOffset": 110}, {"referenceID": 2, "context": "Linguistic structures have been proven useful to capture the semantics of basic language units [45, 23, 44, 6].", "startOffset": 95, "endOffset": 110}, {"referenceID": 21, "context": "[44] designed a DT-RNN model to map sentences into compositional vector representations based on dependency trees.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[23] explored a novel class of Combinatory Categorial Autoencoders to utilize the role of syntax in Combinatory Categorial Grammar to model compositional semantics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[45] designed a Recursive Neural Tensor Network (RNTN) to compute sentiment compositionality based on the Sentiment Treebank.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[6] designed three sets of experiments to encode semantic inference based on compositional semantic representations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "Several models [33, 32, 55, 35] have been proposed to generate word embeddings.", "startOffset": 15, "endOffset": 31}, {"referenceID": 15, "context": "Several models [33, 32, 55, 35] have been proposed to generate word embeddings.", "startOffset": 15, "endOffset": 31}, {"referenceID": 17, "context": "Several models [33, 32, 55, 35] have been proposed to generate word embeddings.", "startOffset": 15, "endOffset": 31}, {"referenceID": 15, "context": "Here, we utilize the Continuous Skip-gram model [32] based on a large amount of unlabeled in-domain data set.", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "[52], which learned phrase embeddings directly by considering a phrase as a basic language unit, and (2) a simple element-based additive model (z = x1 + x2 + .", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "+ xi) [32], where z represents a phrase embedding and x1, x2, .", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "Linguistic structures have been proven useful for capturing the semantics of basic language units [45, 23, 44, 6].", "startOffset": 98, "endOffset": 113}, {"referenceID": 11, "context": "Linguistic structures have been proven useful for capturing the semantics of basic language units [45, 23, 44, 6].", "startOffset": 98, "endOffset": 113}, {"referenceID": 21, "context": "Linguistic structures have been proven useful for capturing the semantics of basic language units [45, 23, 44, 6].", "startOffset": 98, "endOffset": 113}, {"referenceID": 2, "context": "Linguistic structures have been proven useful for capturing the semantics of basic language units [45, 23, 44, 6].", "startOffset": 98, "endOffset": 113}, {"referenceID": 1, "context": "Many linguistic knowledge representations and resources, including AMR (Abstract Meaning Representation [3]), dependency parsing, semantic role labeling (SRL), VerbNet [27] and FrameNet [1], can be exploited to capture linguistic properties.", "startOffset": 104, "endOffset": 107}, {"referenceID": 13, "context": "Many linguistic knowledge representations and resources, including AMR (Abstract Meaning Representation [3]), dependency parsing, semantic role labeling (SRL), VerbNet [27] and FrameNet [1], can be exploited to capture linguistic properties.", "startOffset": 168, "endOffset": 172}, {"referenceID": 0, "context": "Many linguistic knowledge representations and resources, including AMR (Abstract Meaning Representation [3]), dependency parsing, semantic role labeling (SRL), VerbNet [27] and FrameNet [1], can be exploited to capture linguistic properties.", "startOffset": 186, "endOffset": 189}, {"referenceID": 19, "context": "To reduce the dimensions and generate a high quality embedding for the specific context, we utilize the sparse auto-encoder framework [38] to learn more low-dimensional representations.", "startOffset": 134, "endOffset": 138}, {"referenceID": 26, "context": ", labels, names, aliases), to locate a list of candidate entities e \u2208 E and compute the importance score by an entropy based approach [56].", "startOffset": 134, "endOffset": 138}, {"referenceID": 26, "context": "We also collect all entities and their properties and type labels from DBpedia and 300+ biomedical domain specific ontologies crawled from BioPortal [56] to learn knowledge embeddings.", "startOffset": 149, "endOffset": 153}, {"referenceID": 23, "context": "We compare with two high-performing name taggers, Stanford NER [15] and FIGER [51], on both coarse-grained types (Person, Location, and Organization), and fine-grained types.", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "We utilize the AMR parser developed by [16] and manually map AMR types and system generated types to three coarse-grained types.", "startOffset": 39, "endOffset": 43}, {"referenceID": 12, "context": "We utilize the ambiguity measure defined in [26] as the criteria to demonstrate the ambiguity degree of news and biomedical domains.", "startOffset": 44, "endOffset": 48}], "year": 2016, "abstractText": "Recent research has shown great progress on fine-grained entity typing. Most existing methods require pre-defining a set of types and training a multi-class classifier from a large labeled data set based on multi-level linguistic features. They are thus limited to certain domains, genres and languages. In this paper, we propose a novel unsupervised entity typing framework by combining symbolic and distributional semantics. We start from learning general embeddings for each entity mention, compose the embeddings of specific contexts using linguistic structures, link the mention to knowledge bases and learn its related knowledge representations. Then we develop a novel joint hierarchical clustering and linking algorithm to type all mentions using these representations. This framework doesn\u2019t rely on any annotated data, predefined typing schema, or hand-crafted features, therefore it can be quickly adapted to a new domain, genre and language. Furthermore, it has great flexibility at incorporating linguistic structures (e.g., Abstract Meaning Representation (AMR), dependency relations) to improve specific context representation. Experiments on genres (news and discussion forum) show comparable performance with state-of-the-art supervised typing systems trained from a large amount of labeled data. Results on various languages (English, Chinese, Japanese, Hausa, and Yoruba) and domains (general and biomedical) demonstrate the portability of our framework.", "creator": "LaTeX with hyperref package"}}}