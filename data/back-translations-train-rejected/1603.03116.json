{"id": "1603.03116", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Mar-2016", "title": "Low-rank passthrough neural networks", "abstract": "Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. Effective learning in this setting is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem of naive deep networks. Many of these architectures, such as LSTMs, GRUs, Highway Networks and Deep Residual Network, are based on a single structural principle: the state passthrough.", "histories": [["v1", "Thu, 10 Mar 2016 01:04:07 GMT  (244kb,D)", "https://arxiv.org/abs/1603.03116v1", "16 pages, 7 figures"], ["v2", "Thu, 19 May 2016 19:38:30 GMT  (333kb,D)", "http://arxiv.org/abs/1603.03116v2", "17 pages, 8 figures"]], "COMMENTS": "16 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["antonio valerio miceli barone"], "accepted": false, "id": "1603.03116"}, "pdf": {"name": "1603.03116.pdf", "metadata": {"source": "CRF", "title": "LOW-RANK PASSTHROUGH NEURAL NETWORKS", "authors": ["Antonio Valerio Miceli Barone"], "emails": ["amiceli@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "Deep learning consists in the formation of neural networks to perform calculations that unfold sequentially in many steps over a time dimension or an intrinsic depth dimension. Effective learning in this environment is usually achieved by specialized network architectures designed to mitigate the problem of the dwindling gradient of na\u00efve deep networks. Many of these architectures, such as LSTMs, GRUs, Highway Networks and Deep Residual Network, are based on a single structural principle: the state passthrough. We observe that these architectures, which are hereby characterized as passthrough networks, in addition to mitigating the problem of the disappearing gradient, allow the decoupling of network state size from the number of network parameters, a possibility that is exploited in some recent work but not thoroughly explored. In this work, we propose simple but effective, low and diagonal matrix parameterization through network pass capacity, which reduces the proximity of the coupling of the IST and the resulting from the IST."}, {"heading": "1 OVERVIEW", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live."}, {"heading": "2 MODEL", "text": "In this section, we will introduce a notation to describe different neural network architectures, then we will formally describe passthrough architectures, and finally our low-dimensional parameterizations for these architectures.A neural network can be described as a dynamic system that transforms an input u into output y in several time steps. In each step, the network has an n-dimensional state vector x (t).Rn defines asx (t) = {in (u, \u03b8), if t = 0 f (x (t \u2212 1), t (t, u), if t (1), in which it is a state initialization function, f is a state transition function \u2212 Rk is a vector of traable parameters. The output = out (x (0: T), \u03b8) (2) is generated by an output function, where x (0: T) is the entire sequence of states visited during execution."}, {"heading": "2.1 PASSTHROUGH NETWORKS", "text": "We define a network to have a state in which an x-grid goes through x-grid when x-grid (t) = fair Rn (t) and x-grid (t). We define a network to have a state in which x-grid is used when x-grid (t) = fair Rn (t) and x-grid (t). We define a network to have a state in which x-grid is used when x-grid (t) = fair Rn (t) and x-grid (t). We define a network to have a state in which x-grid is used when x-grid (t) asx-grid (t) and x-grid (t)."}, {"heading": "2.2 LOW-RANK PASSTHROUGH NETWORKS", "text": "In the fully connected architectures, there are n \u00b7 n matrices that act on the state vector that represents these physical interactions."}, {"heading": "2.3 LOW-RANK PLUS DIAGONAL PASSTHROUGH NETWORKS", "text": "As we show in the experimental section, in some tasks the small-scale constraint may prove excessively restrictive if the goal is to learn a model with fewer parameters than one with arbitrary matrices. A simple extension is to add a diagonal parameter matrix to each small-scale parameter matrix, creating a matrix that is full-fledged but nevertheless parameterized in a low-dimensional space. For example, for the architecture of the road network we modify eq. 7 toctus (W\u03c0) t = \u03b8 (L\u03c0) t \u00b7 ctuces (D\u03c0) tctues (W\u0442) t\u03b8 (L\u0442) t \u00b7 ctues (R\u0442) t (8), where the incorporation of the state into the state is not mandatory."}, {"heading": "3 EXPERIMENTS", "text": "In this section we report on a preliminary experiment on Low-Rank Highway Networks on the MNIST dataset and several experiments on Low-Rank GRUs."}, {"heading": "3.1 LOW-RANK HIGHWAY NETWORKS", "text": "We applied the low-rank and low-rank plus diagonal highway network architecture to the classic benchmark task of the handwritten number classification on the MNIST database. We used the low-rank architecture described by equations 6 and 7, with T = 5 hidden layers, ReLU activation function, state dimension n = 1024 and maximum rank (internal dimension) d = 256. The input-to-state layer is a dense 784 x 1024 matrix followed by a (approximate) ReLU activation and the state-to-output layer is a dense 1024 x 10 matrix followed by a (biased) identity activation. We did not use a convolution layer, pooling layer or data augmentation technique. We used drop-outs (Srivastava et al., 2014) to achieve a regulation. We applied standard drop-out layers with failure probability state = 0.5 state-to-input layer before we applied the inp-2 layer."}, {"heading": "3.2 LOW-RANK GRUS", "text": "We applied the low-rank and low-rank plus diagonal GRU architectures to a subset of sequential benchmarks included in the Unitary Evolution Recurrent Neural Networks article by Arjovsky et al. (2015), in particular the memory task and the sequential random recovery of the MNIST task. For the storage tasks, we also considered two different variants, those of Danihelka et al. (2016) and Henaff et al. (2016), which are difficult for the uRNN architecture, to compare against the uRNN architecture because it sets the state of the art in terms of both data complexity and accuracy, and because it is an architecture with similar design goals to low-rank architectures, namely a low-dimensional parametrization and the mitigation of the vanished gradients, but it is not based on entirely different principles (it uses no state as defined in this work, and relies instead on the GRU)."}, {"heading": "3.2.1 MEMORY TASK", "text": "This year it has come to the point that it will be able to retaliate, \"he said.\" We've never waited so long to be able to unite, \"he said.\" We've never waited so long to be able to unite, \"he said.\" We've never waited so long to be able to unite, \"he said."}, {"heading": "3.2.2 ADDITION TASK", "text": "For each instance of this task, the input sequence has the length T and consists of two real components, with each step, the first component is sampled with a uniform probability regardless of the interval [0, 1], the second component is zero everywhere except for two randomly selected time steps, one in each half of the sequence where it is one. The result is a single real value calculated from the final state we want to be equal to the sum of the two elements of the first component of the sequence at the places where the second component is set to one. In our experiment, we set T = 750. The training set consists of 100,000 training examples and 10,000 validation / test examples. We use a low-level GRU with 2 \u00d7 n input matrix, n \u00d7 1 output matrix and (biased) identity testing."}, {"heading": "3.2.3 SEQUENTIAL MNIST TASK", "text": "This task consists of the handwritten classification of the digits on the MNIST dataset with the proviso that the input is submitted to the model with a pixel value in due course, via T = 784 time steps. To further increase the difficulty of the task, the inputs are rearranged according to a random permutation (fixed for all tasks).We use a low GRU with 1 \u00d7 n input matrix, n \u00b7 10 output matrix and (biased) Softmax output was set at 5 \u00d7 10 \u2212 4, mini-batch size 20, initial bias of carry functions (the \"update\" gates) was set to 5.We consider two hyperparameter configurations: 1. State size n = 24.2. State size n = 512, maximum state size n = 4.Configuration 1, initial bias of carry functions (the \"update\" gates) was put to the test."}, {"heading": "4 CONCLUSIONS AND FUTURE WORK", "text": "We have proposed low-dimensional parameterizations for continuous neural networks based on random decomposition of the n \u00b7 n matrices occurring in the hidden layers. We have experimentally compared our models with state-of-the-art models and achieved competitive results, including a state of the art for the randomly permutated sequential MNIST task. We note that the two approaches can be combined in at least two ways: \u2022 Our parameterizations are an alternative to the revolutionary parameterizations investigated by Srivastava et al. (2015); He et al. (2015); Kaiser & Sutskever (2015)."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Giuseppe Attardi and the Faculty of Computer Science at the University of Pisa for providing us with their machines to carry out the experiments presented in this paper."}], "references": [{"title": "Unitary evolution recurrent neural networks", "author": ["Arjovsky", "Martin", "Shah", "Amar", "Bengio", "Yoshua"], "venue": "CoRR, abs/1511.06464,", "citeRegEx": "Arjovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "Gulcehre", "Caglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Associative Long Short-Term Memory", "author": ["I. Danihelka", "G. Wayne", "B. Uria", "N. Kalchbrenner", "A. Graves"], "venue": "ArXiv e-prints,", "citeRegEx": "Danihelka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Danihelka et al\\.", "year": 2016}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey E"], "venue": "CoRR, abs/1303.5778,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Lstm: A search space odyssey", "author": ["Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Orthogonal RNNs and Long-Memory Tasks", "author": ["M. Henaff", "A. Szlam", "Y. LeCun"], "venue": "ArXiv e-prints,", "citeRegEx": "Henaff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["Hochreiter", "Sepp"], "venue": "Diploma, Technische Universita\u0308t Mu\u0308nchen,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1991}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Iyyer", "Mohit", "Boyd-Graber", "Jordan", "Claudino", "Leonardo", "Socher", "Richard", "Daum\u00e9 III", "Hal"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "An empirical exploration of recurrent network architectures", "author": ["J\u00f3zefowicz", "Rafal", "Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["J\u00f3zefowicz", "Rafal", "Vinyals", "Oriol", "Schuster", "Mike", "Shazeer", "Noam", "Wu", "Yonghui"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "Neural gpus learn algorithms", "author": ["Kaiser", "Lukasz", "Sutskever", "Ilya"], "venue": "CoRR, abs/1511.08228,", "citeRegEx": "Kaiser et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Kalchbrenner", "Nal", "Danihelka", "Ivo", "Graves", "Alex"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "System Identification from Noisy Data", "author": ["Kalman", "R.E"], "venue": "Defense Technical Information Center,", "citeRegEx": "Kalman and R.E,? \\Q1982\\E", "shortCiteRegEx": "Kalman and R.E", "year": 1982}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fastfood-approximating kernel expansions in loglinear time", "author": ["Le", "Quoc", "Sarl\u00f3s", "Tam\u00e1s", "Smola", "Alex"], "venue": "In Proceedings of the international conference on machine learning,", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Le", "Quoc V", "Jaitly", "Navdeep", "Hinton", "Geoffrey E"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["LeCun", "Yann", "Huang", "Fu Jie", "Bottou", "Leon"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "LeCun et al\\.,? \\Q2004\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2004}, {"title": "ACDC: A structured efficient linear layer", "author": ["Moczulski", "Marcin", "Denil", "Misha", "Appleyard", "Jeremy", "de Freitas", "Nando"], "venue": "CoRR, abs/1511.05946,", "citeRegEx": "Moczulski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moczulski et al\\.", "year": 2015}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Neelakantan", "Arvind", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "CoRR, abs/1511.04834,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Linear models based on noisy data and the frisch scheme", "author": ["Ning", "Lipeng", "Georgiou", "Tryphon T", "Tannenbaum", "Allen", "Boyd", "Stephen P"], "venue": "SIAM Review,", "citeRegEx": "Ning et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ning et al\\.", "year": 2015}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Sak", "Hasim", "Senior", "Andrew W", "Beaufays", "Fran\u00e7oise"], "venue": "In INTERSPEECH,", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Diagonal and low-rank matrix decompositions, correlation matrices, and ellipsoid fitting", "author": ["Saunderson", "James", "Chandrasekaran", "Venkat", "Parrilo", "Pablo A", "Willsky", "Alan S"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Saunderson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Saunderson et al\\.", "year": 2012}, {"title": "intelligence,\u201d objectively determined and measured", "author": ["Spearman", "Charles"], "venue": "The American Journal of Psychology,", "citeRegEx": "Spearman and Charles.,? \\Q1904\\E", "shortCiteRegEx": "Spearman and Charles.", "year": 1904}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Temporal localization of finegrained actions in videos by domain transfer from web images", "author": ["Sun", "Chen", "Shetty", "Sanketh", "Sukthankar", "Rahul", "Nevatia", "Ram"], "venue": "In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference,", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "Deep learning using linear support vector machines", "author": ["Tang", "Yichuan"], "venue": "arXiv preprint arXiv:1306.0239,", "citeRegEx": "Tang and Yichuan.,? \\Q2013\\E", "shortCiteRegEx": "Tang and Yichuan.", "year": 2013}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "Lukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Architectural complexity measures of recurrent neural networks", "author": ["Zhang", "Saizheng", "Wu", "Yuhuai", "Che", "Tong", "Lin", "Zhouhan", "Memisevic", "Roland", "Salakhutdinov", "Ruslan", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1602.08210,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Training these deep models is complicated by the exploding and vanishing gradient problems (Hochreiter, 1991; Bengio et al., 1994).", "startOffset": 91, "endOffset": 130}, {"referenceID": 10, "context": ", 2014b) and other variants (Greff et al., 2015; J\u00f3zefowicz et al., 2015).", "startOffset": 28, "endOffset": 73}, {"referenceID": 18, "context": ", 2014b) and other variants (Greff et al., 2015; J\u00f3zefowicz et al., 2015).", "startOffset": 28, "endOffset": 73}, {"referenceID": 8, "context": "These architectures led to a number of breakthroughs in different tasks such as speech recognition (Graves et al., 2013), machine translation (Cho et al.", "startOffset": 99, "endOffset": 120}, {"referenceID": 1, "context": ", 2013), machine translation (Cho et al., 2014a; Bahdanau et al., 2014), natural language parsing (Vinyals et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 37, "context": ", 2014), natural language parsing (Vinyals et al., 2014), question answering (Iyyer et al.", "startOffset": 34, "endOffset": 56}, {"referenceID": 17, "context": ", 2014), question answering (Iyyer et al., 2014) and many others.", "startOffset": 28, "endOffset": 48}, {"referenceID": 12, "context": ", 2015), Deep Residual Networks (He et al., 2015) and Grid LSTM1 (Kalchbrenner et al.", "startOffset": 32, "endOffset": 49}, {"referenceID": 21, "context": ", 2015) and Grid LSTM1 (Kalchbrenner et al., 2015).", "startOffset": 23, "endOffset": 50}, {"referenceID": 1, "context": "Training these deep models is complicated by the exploding and vanishing gradient problems (Hochreiter, 1991; Bengio et al., 1994). Starting from the original LSTM of Hochreiter & Schmidhuber (1997), various network architectures have been proposed to ameliorate the vanishing gradient problem in the recurrent neural network setting, such as the modern LSTM (Graves & Schmidhuber, 2005), the GRU (Cho et al.", "startOffset": 110, "endOffset": 199}, {"referenceID": 27, "context": "For this reason, a number of neural low-dimensional layer parametrization have been proposed, such as convolutional layers (LeCun et al., 2004; Krizhevsky et al., 2012) which impose a sparse, local, periodic structure on the parameter matrices, or multiplicative matrix decompositions, notably the Unitary Evolution RNNs (Arjovsky et al.", "startOffset": 123, "endOffset": 168}, {"referenceID": 24, "context": "For this reason, a number of neural low-dimensional layer parametrization have been proposed, such as convolutional layers (LeCun et al., 2004; Krizhevsky et al., 2012) which impose a sparse, local, periodic structure on the parameter matrices, or multiplicative matrix decompositions, notably the Unitary Evolution RNNs (Arjovsky et al.", "startOffset": 123, "endOffset": 168}, {"referenceID": 0, "context": ", 2012) which impose a sparse, local, periodic structure on the parameter matrices, or multiplicative matrix decompositions, notably the Unitary Evolution RNNs (Arjovsky et al., 2015) (which also addresses the vanishing gradient problem) and others (Le et al.", "startOffset": 160, "endOffset": 183}, {"referenceID": 25, "context": ", 2015) (which also addresses the vanishing gradient problem) and others (Le et al., 2013; Moczulski et al., 2015).", "startOffset": 73, "endOffset": 114}, {"referenceID": 28, "context": ", 2015) (which also addresses the vanishing gradient problem) and others (Le et al., 2013; Moczulski et al., 2015).", "startOffset": 73, "endOffset": 114}, {"referenceID": 12, "context": "To the best of our knowledge, this systematic decoupling has not been described in a systematic way, although it has been exploited by some convolutional passthrough architectures for image recognition (Srivastava et al., 2015; He et al., 2015) or algorithmic tasks (Kaiser & Sutskever, 2015), or architectures with addressable read-write memory (Graves et al.", "startOffset": 202, "endOffset": 244}, {"referenceID": 9, "context": ", 2015) or algorithmic tasks (Kaiser & Sutskever, 2015), or architectures with addressable read-write memory (Graves et al., 2014; Gregor et al., 2015; Neelakantan et al., 2015; Kurach et al., 2015; Danihelka et al., 2016).", "startOffset": 109, "endOffset": 222}, {"referenceID": 11, "context": ", 2015) or algorithmic tasks (Kaiser & Sutskever, 2015), or architectures with addressable read-write memory (Graves et al., 2014; Gregor et al., 2015; Neelakantan et al., 2015; Kurach et al., 2015; Danihelka et al., 2016).", "startOffset": 109, "endOffset": 222}, {"referenceID": 29, "context": ", 2015) or algorithmic tasks (Kaiser & Sutskever, 2015), or architectures with addressable read-write memory (Graves et al., 2014; Gregor et al., 2015; Neelakantan et al., 2015; Kurach et al., 2015; Danihelka et al., 2016).", "startOffset": 109, "endOffset": 222}, {"referenceID": 5, "context": ", 2015) or algorithmic tasks (Kaiser & Sutskever, 2015), or architectures with addressable read-write memory (Graves et al., 2014; Gregor et al., 2015; Neelakantan et al., 2015; Kurach et al., 2015; Danihelka et al., 2016).", "startOffset": 109, "endOffset": 222}, {"referenceID": 19, "context": "(2014) which has been applied to speech recognition, natural language modeling (J\u00f3zefowicz et al., 2016), video analysis (Sun et al.", "startOffset": 79, "endOffset": 104}, {"referenceID": 35, "context": ", 2016), video analysis (Sun et al., 2015) et cetera.", "startOffset": 24, "endOffset": 42}, {"referenceID": 26, "context": "We provide experimental evaluation of our approach on GRU and Highway Network architectures on various machine learning tasks, including a near state of the art result for the hard task of sequential randomly-permuted MNIST image recognition (Le et al., 2015).", "startOffset": 242, "endOffset": 259}, {"referenceID": 0, "context": ", 2012) which impose a sparse, local, periodic structure on the parameter matrices, or multiplicative matrix decompositions, notably the Unitary Evolution RNNs (Arjovsky et al., 2015) (which also addresses the vanishing gradient problem) and others (Le et al., 2013; Moczulski et al., 2015). In this work we observe that the state passthrough allows for a systematic decoupling of the network state size from the number of parameters: since by default the state vector passes mostly unaltered through the layers, each layer can be made simple enough to be described only by a small number of parameters without affecting the overall memory capacity of the network. This effectively spreads the computation over the depth or time dimension of the network, but without making the network \u201dthin\u201d (as proposed, for instance, by Srivastava et al. (2015)).", "startOffset": 161, "endOffset": 849}, {"referenceID": 0, "context": ", 2012) which impose a sparse, local, periodic structure on the parameter matrices, or multiplicative matrix decompositions, notably the Unitary Evolution RNNs (Arjovsky et al., 2015) (which also addresses the vanishing gradient problem) and others (Le et al., 2013; Moczulski et al., 2015). In this work we observe that the state passthrough allows for a systematic decoupling of the network state size from the number of parameters: since by default the state vector passes mostly unaltered through the layers, each layer can be made simple enough to be described only by a small number of parameters without affecting the overall memory capacity of the network. This effectively spreads the computation over the depth or time dimension of the network, but without making the network \u201dthin\u201d (as proposed, for instance, by Srivastava et al. (2015)). To the best of our knowledge, this systematic decoupling has not been described in a systematic way, although it has been exploited by some convolutional passthrough architectures for image recognition (Srivastava et al., 2015; He et al., 2015) or algorithmic tasks (Kaiser & Sutskever, 2015), or architectures with addressable read-write memory (Graves et al., 2014; Gregor et al., 2015; Neelakantan et al., 2015; Kurach et al., 2015; Danihelka et al., 2016). In this work we introduce an unified view of passthrough architectures, describe their state sizeparameter size decoupling property, propose simple but effective low-dimensional parametrizations that exploit this decoupling based on low-rank or low-rank plus diagonal matrix decompositions. Our approach extends the LSTM architecture with a single projection layer by Sak et al. (2014) which has been applied to speech recognition, natural language modeling (J\u00f3zefowicz et al.", "startOffset": 161, "endOffset": 1698}, {"referenceID": 10, "context": "Modern LSTM variants (Greff et al., 2015) typically use a transform function (\u201dforget gate\u201d) f\u03c4 and carry function (\u201dinput gate\u201d) f\u03c4 independent of each other.", "startOffset": 21, "endOffset": 41}, {"referenceID": 9, "context": "This choice is used in the original LSTM of Hochreiter & Schmidhuber (1997) and in the Deep Residual Network2 of He et al. (2015). We denote the state passthrough as convex if f\u03c4 (x(t \u2212 1), t, u, \u03b8) = 1\u2297n\u0302 \u2212 f\u03b3(x(t \u2212 1), t, u, \u03b8).", "startOffset": 113, "endOffset": 130}, {"referenceID": 6, "context": "where g is an element-wise activation function, usually the ReLU (Glorot et al., 2011) or the hyperbolic tangent, \u03c3 is the element-wise logistic sigmoid, and \u2200t \u2208 1, .", "startOffset": 65, "endOffset": 86}, {"referenceID": 31, "context": "A similar approach has been proposed for the LSTM architecture by Sak et al. (2014), although they force the the R matrices to be the same for all the functions of the state transition, while we allow each parameter matrix to be parametrized independently by a pair of R and L matrices.", "startOffset": 66, "endOffset": 84}, {"referenceID": 31, "context": "Refer to Saunderson et al. (2012) and Ning et al.", "startOffset": 9, "endOffset": 34}, {"referenceID": 30, "context": "(2012) and Ning et al. (2015) for a review.", "startOffset": 11, "endOffset": 30}, {"referenceID": 34, "context": "We used dropout (Srivastava et al., 2014) in order to achieve regularization. We applied standard dropout layers with dropout probability p = 0.2 just before the input-to-state layer and p = 0.5 just before the state-to-output layer. We also applied dropout inside each hidden layer in the following way: we inserted dropout layers with p = 0.3 inside both the proposal function and the transform function, immediately before both the R matrices and the L matrices, totaling to four dropout layers per hidden layer, although the random dropout matrices are shared between proposal and transform functions. Dropout applied this way does not disrupt the state passthrough, thus it does not cause a reduction of memory capacity during training. We further applied L2-regularization with coefficient \u03bb = 1\u00d7 10\u22123 per example on the hidden-to-output parameter matrix. We also used batch normalization (Ioffe & Szegedy, 2015) after the input-to-state matrix and after each parameter matrix in the hidden layers. Parameter matrices are randomly initialized using an uniform distribution with scale equal to \u221a 6/a where a is the input dimension. Initial bias vectors are all initialized at zero except for those of the transform functions in the hidden layers, which are initialized at \u22121.0. We trained to minimize the sum of the per-class L2-hinge loss plus the L2-regularization cost (Tang, 2013). Optimization was performed using Adam (Kingma & Ba, 2014) with standard hyperparameters, learning rate starting at 3\u00d7 10\u22123 halving every three epochs without validation improvements. Mini-batch size was equal to 100. Code is available online3. We ran our experiments on a machine with a 24 core Intel(R) Xeon(R) CPU X5670 2.93GHz, 24 GB of RAM. We did not use a GPU. Training took approximately 4 hours . We obtained perfect training accuracy and 98.83% test accuracy. While this result does not reach the state of the art for this task (99.13% test accuracy with unsupervised dimensionality reduction reported by Tang (2013)), it is still relatively close.", "startOffset": 17, "endOffset": 2017}, {"referenceID": 0, "context": "We applied the Low-rank and Low-rank plus diagonal GRU architectures to a subset of sequential benchmarks described in the Unitary Evolution Recurrent Neural Networks article by Arjovsky et al. (2015), specifically the memory task, the addition task and the sequential randomly permuted MNIST task.", "startOffset": 178, "endOffset": 201}, {"referenceID": 0, "context": "We applied the Low-rank and Low-rank plus diagonal GRU architectures to a subset of sequential benchmarks described in the Unitary Evolution Recurrent Neural Networks article by Arjovsky et al. (2015), specifically the memory task, the addition task and the sequential randomly permuted MNIST task. For the memory tasks, we also considered two different variants proposed by Danihelka et al. (2016) and Henaff et al.", "startOffset": 178, "endOffset": 399}, {"referenceID": 0, "context": "We applied the Low-rank and Low-rank plus diagonal GRU architectures to a subset of sequential benchmarks described in the Unitary Evolution Recurrent Neural Networks article by Arjovsky et al. (2015), specifically the memory task, the addition task and the sequential randomly permuted MNIST task. For the memory tasks, we also considered two different variants proposed by Danihelka et al. (2016) and Henaff et al. (2016) which are hard for the uRNN architecture.", "startOffset": 178, "endOffset": 424}, {"referenceID": 5, "context": "In the variant of Danihelka et al. (2016), the length of the sequence to be remembered is randomly sampled between 1 and 10 for each sequence.", "startOffset": 18, "endOffset": 42}, {"referenceID": 13, "context": "In the variant of Henaff et al. (2016), the length of the sequence to be remembered is fixed at 10 but the model is expected to copy it after a variable number of time steps randomly chosen, for each sequence, between 1 and N = 100.", "startOffset": 18, "endOffset": 39}, {"referenceID": 38, "context": "These results surpass the uRNN and are on par with more complex architectures with time-skip connections (Zhang et al., 2016) (reported test set accuracy 94.", "startOffset": 105, "endOffset": 125}, {"referenceID": 38, "context": "These results surpass the uRNN and are on par with more complex architectures with time-skip connections (Zhang et al., 2016) (reported test set accuracy 94.0%). To our knowledge, at the time of this writing, the best result on this task is the LSTM with recurrent batch normalization by Cooijmans et al. (2016) (reported test set accuracy 95.", "startOffset": 106, "endOffset": 312}, {"referenceID": 33, "context": "Our parametrizations are alternative to convolutional parametrizations explored by Srivastava et al. (2015); He et al.", "startOffset": 83, "endOffset": 108}, {"referenceID": 12, "context": "(2015); He et al. (2015); Kaiser & Sutskever (2015).", "startOffset": 8, "endOffset": 25}, {"referenceID": 12, "context": "(2015); He et al. (2015); Kaiser & Sutskever (2015). We note that the two approaches can be combined in at least two ways:", "startOffset": 8, "endOffset": 52}], "year": 2016, "abstractText": "Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. Effective learning in this setting is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem of naive deep networks. Many of these architectures, such as LSTMs, GRUs, Highway Networks and Deep Residual Network, are based on a single structural principle: the state passthrough. We observe that these architectures, hereby characterized as Passthrough Networks, in addition to the mitigation of the vanishing gradient problem, enable the decoupling of the network state size from the number of parameters of the network, a possibility that is exploited in some recent works but not thoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. We present competitive experimental results on synthetic tasks and a near state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data.", "creator": "LaTeX with hyperref package"}}}