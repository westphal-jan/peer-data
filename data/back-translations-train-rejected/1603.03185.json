{"id": "1603.03185", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Mar-2016", "title": "Personalized Speech recognition on mobile devices", "abstract": "We describe a large vocabulary speech recognition system that is accurate, has low latency, and yet has a small enough memory and computational footprint to run faster than real-time on a Nexus 5 Android smartphone. We employ a quantized Long Short-Term Memory (LSTM) acoustic model trained with connectionist temporal classification (CTC) to directly predict phoneme targets, and further reduce its memory footprint using an SVD-based compression scheme. Additionally, we minimize our memory footprint by using a single language model for both dictation and voice command domains, constructed using Bayesian interpolation. Finally, in order to properly handle device-specific information, such as proper names and other context-dependent information, we inject vocabulary items into the decoder graph and bias the language model on-the-fly. Our system achieves 13.5\\% word error rate on an open-ended dictation task, running with a median speed that is seven times faster than real-time.", "histories": [["v1", "Thu, 10 Mar 2016 08:51:51 GMT  (61kb,D)", "http://arxiv.org/abs/1603.03185v1", null], ["v2", "Fri, 11 Mar 2016 22:25:39 GMT  (61kb,D)", "http://arxiv.org/abs/1603.03185v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["ian mcgraw", "rohit prabhavalkar", "raziel alvarez", "montse gonzalez arenas", "kanishka rao", "david rybach", "ouais alsharif", "hasim sak", "alexander gruenstein", "francoise beaufays", "carolina parada"], "accepted": false, "id": "1603.03185"}, "pdf": {"name": "1603.03185.pdf", "metadata": {"source": "CRF", "title": "PERSONALIZED SPEECH RECOGNITION ON MOBILE DEVICES", "authors": ["Ian McGraw", "Rohit Prabhavalkar", "Raziel Alvarez", "Montse Gonzalez Arenas", "Kanishka Rao", "David Rybach", "Ouais Alsharif", "Ha\u015fim Sak", "Alexander Gruenstein", "Fran\u00e7oise Beaufays", "Carolina Parada"], "emails": ["imcgraw@google.com", "prabhavalkar@google.com", "raziel@google.com", "montse@google.com", "kanishkarao@google.com", "rybach@google.com", "oalsha@google.com", "hasim@google.com", "alexgru@google.com", "fsb@google.com", "carolinap@google.com"], "sections": [{"heading": null, "text": "In fact, it is the case that most of them will be able to move into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live."}, {"heading": "3.1. AM Experiments", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "3.2. Efficient Representation and Fast Execution", "text": "In fact, most of us are able to surpass ourselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we will be able to change the world, and that we will be able to change the world. \"He added:\" I don't think we will be able to change the world, and that we will be able to change the world. \""}], "references": [{"title": "Accurate and compact large vocabulary speech recognition on mobile devices", "author": ["Xin Lei", "Andrew Senior", "Alexander Gruenstein", "Jeffrey Sorensen"], "venue": "INTERSPEECH. 2013, pp. 662\u2013665, ISCA.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "ICML, 2006, pp. 369\u2013376.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["Brian Kingsbury"], "venue": "ICASSP. 2009, pp. 3761\u20133764, IEEE.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Jian Xue", "Jinyu Li", "Yifan Gong"], "venue": "INTERSPEECH, 2013, pp. 2365\u20132369.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "On the compression of recurrent neural networks with an application to LVCSR acoustic modeling for embedded speech recognition", "author": ["Rohit Prabhavalkar", "Ouais Alsharif", "Antoine Bruguier", "Ian McGraw"], "venue": "ICASSP. 2016, IEEE.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Locallyconnected and convolutional neural networks for small footprint speaker recognition", "author": ["Yu-hsin Chen", "Ignacio Lopez-Moreno", "Tara N. Sainath", "Mirk\u00f3 Visontai", "Raziel Alvarez", "Carolina Parada"], "venue": "INTERSPEECH. 2015, pp. 1136\u20131140, ISCA.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing deep neural networks using a rank-constrained topology", "author": ["Preetum Nakkiran", "Raziel Alvarez", "Rohit Prabhavalkar", "Carolina Parada"], "venue": "INTERSPEECH. 2015, pp. 1473\u20131477, ISCA.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Structured transforms for small-footprint deep learning", "author": ["Vikas Sindhwani", "Tara N. Sainath", "Sanjiv Kumar"], "venue": "NIPS (to appear), 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Tara N. Sainath", "Brian Kingsbury", "Vikas Sindhwani", "Ebru Arisoy", "Bhuvana Ramabhadran"], "venue": "ICASSP. 2013, pp. 6655\u20136659, IEEE.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Small-footprint high-performance deep neural network-based speech recognition using split-VQ", "author": ["Yongqiang Wang", "Jinyu Li", "Yifan Gong"], "venue": "ICASSP. 2015, pp. 4984\u20134988, IEEE.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Transferring knowledge from a RNN to a DNN", "author": ["William Chan", "Nan Rosemary Ke", "Ian Lane"], "venue": "INTERSPEECH. 2015, ISCA.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved recognition of contact names in voice commands", "author": ["Petar Aleksic", "Cyril Allauzen", "David Elson", "Aleksandar Kracun", "Diego Melendo Casado", "Pedro J. Moreno"], "venue": "ICASSP, 2015, pp. 5172\u20135175.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Composition-based on-the-fly rescoring for salient n-gram biasing", "author": ["Keith Hall", "Eunjoon Cho", "Cyril Allauzen", "Fran\u00e7oise Beaufays", "Noah Coccaro", "Kaisuke Nakajima", "Michael Riley", "Brian Roark", "David Rybach", "Linda Zhang"], "venue": "INTER- SPEECH. 2015, ISCA.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Ha\u015fim Sak", "Andrew Senior", "Fran\u00e7oise Beaufays"], "venue": "INTERSPEECH. 2014, pp. 338\u2013342, ISCA.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Ozan \u0130rsoy", "Alex Graves", "Fran\u00e7oise Beaufays", "Johan Schalkwyk"], "venue": "ICASSP, 2015, pp. 4280\u20134284.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Fran\u00e7oise Beaufays"], "venue": "INTERSPEECH. 2015, pp. 1468\u20131472, ISCA.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Unary data structures for language models", "author": ["Jeffrey Sorensen", "Cyril Allauzen"], "venue": "INTERSPEECH. 2011, ISCA.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg S. Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc V. Le", "Mark Z. Mao", "Marc\u2019Aurelio Ranzato", "Andrew Senior", "Paul Tucker", "Ke Yang", "Andrew Y. Ng"], "venue": "NIPS, 2012, pp. 1223\u20131231.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence discriminative distributed training of long short-term memory recurrent neural networks", "author": ["Ha\u015fim Sak", "Oriol Vinyals", "Georg Heigold", "Andrew Senior", "Erik McDermott", "Rajat Monga", "Mark Mao"], "venue": "INTERSPEECH, 2014, pp. 1209\u2013 1213.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Handbook of Image and Video Processing (Communications, Networking and Multimedia)", "author": ["Alan C. Bovik"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Mao"], "venue": "Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian language model interpolation for mobile speech input", "author": ["Cyril Allauzen", "Michael Riley"], "venue": "INTERSPEECH, 2011, pp. 1429\u20131432.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech recognition with weighted finite-state transducers", "author": ["Mehryar Mohri", "Fernando Pereira", "Michael Riley"], "venue": "Handbook of Speech Processing, Jacob Benesty, M. Sondhi, and Yiteng Huang, Eds., chapter 28, pp. 559\u2013582. Springer, 2008.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Grapheme-to-phoneme conversion using long shortterm memory recurrent neural networks", "author": ["Kanishka Rao", "Fuchun Peng", "Ha\u015fim Sak", "Fran\u00e7oise Beaufays"], "venue": "ICASSP, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In this paper we extend previous work that used quantized deep neural networks (DNNs) and on-the-fly language model rescoring to achieve real-time performance on modern smartphones [1].", "startOffset": 181, "endOffset": 184}, {"referenceID": 1, "context": "We demonstrate that given similar size and computation constraints, we achieve large improvements in word error rate (WER) performance and latency by employing Long Short-Term Memory (LSTM) recurrent neural networks (RNNs), trained with connectionist temporal classification (CTC) [2] and state-level minimum Bayes risk (sMBR) [3] techniques.", "startOffset": 281, "endOffset": 284}, {"referenceID": 2, "context": "We demonstrate that given similar size and computation constraints, we achieve large improvements in word error rate (WER) performance and latency by employing Long Short-Term Memory (LSTM) recurrent neural networks (RNNs), trained with connectionist temporal classification (CTC) [2] and state-level minimum Bayes risk (sMBR) [3] techniques.", "startOffset": 327, "endOffset": 330}, {"referenceID": 3, "context": "LSTMs are made small and fast enough for embedded speech recognition by quantizing parameters to 8 bits, by using context independent (CI) phone outputs instead of more numerous context dependent (CD) phone outputs, and by using Singular Value Decomposition (SVD) compression [4, 5].", "startOffset": 276, "endOffset": 282}, {"referenceID": 4, "context": "LSTMs are made small and fast enough for embedded speech recognition by quantizing parameters to 8 bits, by using context independent (CI) phone outputs instead of more numerous context dependent (CD) phone outputs, and by using Singular Value Decomposition (SVD) compression [4, 5].", "startOffset": 276, "endOffset": 282}, {"referenceID": 3, "context": "SVD has elsewhere been shown to be effective for speech processing tasks [4, 6, 7] as have structured transforms [8] and low-rank matrix factorizations [9].", "startOffset": 73, "endOffset": 82}, {"referenceID": 5, "context": "SVD has elsewhere been shown to be effective for speech processing tasks [4, 6, 7] as have structured transforms [8] and low-rank matrix factorizations [9].", "startOffset": 73, "endOffset": 82}, {"referenceID": 6, "context": "SVD has elsewhere been shown to be effective for speech processing tasks [4, 6, 7] as have structured transforms [8] and low-rank matrix factorizations [9].", "startOffset": 73, "endOffset": 82}, {"referenceID": 7, "context": "SVD has elsewhere been shown to be effective for speech processing tasks [4, 6, 7] as have structured transforms [8] and low-rank matrix factorizations [9].", "startOffset": 113, "endOffset": 116}, {"referenceID": 8, "context": "SVD has elsewhere been shown to be effective for speech processing tasks [4, 6, 7] as have structured transforms [8] and low-rank matrix factorizations [9].", "startOffset": 152, "endOffset": 155}, {"referenceID": 9, "context": "Vector quantization has also been shown to significantly reduce model size with only small accuracy losses [10], however it is unclear whether this algorithm can be implemented in a computationally efficient manner while minimizing runtime memory footprint.", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "For embedded speech recognition, some authors have avoided RNNs citing increased computational costs and instead evaluated methods for transferring knowledge from RNNs to DNNs [11].", "startOffset": 176, "endOffset": 180}, {"referenceID": 11, "context": "We demonstrate that the vocabulary injection and on-the-fly language model biasing techniques from [12, 13] can significantly improve accuracy without significant adverse computational overhead.", "startOffset": 99, "endOffset": 107}, {"referenceID": 12, "context": "We demonstrate that the vocabulary injection and on-the-fly language model biasing techniques from [12, 13] can significantly improve accuracy without significant adverse computational overhead.", "startOffset": 99, "endOffset": 107}, {"referenceID": 0, "context": "We model our baseline system after the embedded speech recognition system presented in [1].", "startOffset": 87, "endOffset": 90}, {"referenceID": 13, "context": "Instead of using a standard feedforward DNN, however, we use deep LSTM models which have been shown to achieve state-of-the-art results on large-scale speech recognition tasks [14, 15, 16].", "startOffset": 176, "endOffset": 188}, {"referenceID": 14, "context": "Instead of using a standard feedforward DNN, however, we use deep LSTM models which have been shown to achieve state-of-the-art results on large-scale speech recognition tasks [14, 15, 16].", "startOffset": 176, "endOffset": 188}, {"referenceID": 15, "context": "Instead of using a standard feedforward DNN, however, we use deep LSTM models which have been shown to achieve state-of-the-art results on large-scale speech recognition tasks [14, 15, 16].", "startOffset": 176, "endOffset": 188}, {"referenceID": 13, "context": "make use of a recurrent projection layer as described in [14] of size 450 for each of hidden layers.", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "This LSTM is trained to predict 2,000 CD states, analogous to the system described in [1].", "startOffset": 86, "endOffset": 89}, {"referenceID": 13, "context": "This system is also trained to optimize the standard (CE) criterion on the training set, with the output labels delayed by 5 frames [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 0, "context": "Unlike in [1], where frames are stacked to provide right and left context to the net, we rely on the LSTM\u2019s memory capabilities and supply only one frame every 10ms as input.", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "The language model presented in this work also follows along the lines of [1].", "startOffset": 74, "endOffset": 77}, {"referenceID": 16, "context": "This rescoring LM is made extremely compact using the LOUDS [17] compression mechanism.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "In particular, the LSTM architecture that we investigate is a CTC model [15, 16]: the system consists of five hidden layers with 500 LSTM cells in each, that predict 41 context independent (CI) phoneme targets plus an additional \u201cblank\u201d target that can be hypothesized if the system is unsure of the identity of the phoneme at the current frame.", "startOffset": 72, "endOffset": 80}, {"referenceID": 15, "context": "In particular, the LSTM architecture that we investigate is a CTC model [15, 16]: the system consists of five hidden layers with 500 LSTM cells in each, that predict 41 context independent (CI) phoneme targets plus an additional \u201cblank\u201d target that can be hypothesized if the system is unsure of the identity of the phoneme at the current frame.", "startOffset": 72, "endOffset": 80}, {"referenceID": 1, "context": "The system is trained to optimize the connectionist temporal classification (CTC) criterion [2] as described in [15, 16].", "startOffset": 92, "endOffset": 95}, {"referenceID": 14, "context": "The system is trained to optimize the connectionist temporal classification (CTC) criterion [2] as described in [15, 16].", "startOffset": 112, "endOffset": 120}, {"referenceID": 15, "context": "The system is trained to optimize the connectionist temporal classification (CTC) criterion [2] as described in [15, 16].", "startOffset": 112, "endOffset": 120}, {"referenceID": 15, "context": "In order to stabilize CTC training, our CTC models use the strategy proposed in [16]: we stack together 8 consecutive frames (7 frames of right context) and only present every third stacked frame as input to the network.", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "All models in our work are trained using distributed asynchronous stochastic gradient descent (ASGD) [18].", "startOffset": 101, "endOffset": 105}, {"referenceID": 14, "context": "As can be seen in Table 1, and consistent with previous work [15], the CTC-trained LSTM model that predicts CI phones outperforms the CE-trained LSTM that predicts 2,000 CD states.", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "Sequence discriminative training with the sMBR criterion [3, 19] further improves system performance by 20% relative to the CTC-trained sytem.", "startOffset": 57, "endOffset": 64}, {"referenceID": 18, "context": "Sequence discriminative training with the sMBR criterion [3, 19] further improves system performance by 20% relative to the CTC-trained sytem.", "startOffset": 57, "endOffset": 64}, {"referenceID": 13, "context": "In order to reduce memory consumption further, we compress our acoustic models using projection layers that sit between the outputs of an LSTM layer and both the recurrent and non-recurrent inputs to same and subsequent layers [14].", "startOffset": 227, "endOffset": 231}, {"referenceID": 4, "context": "This process yields an initialization that results in stable convergence as described in detail in [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "For completeness, we also trained a DNN system with topology described in [1].", "startOffset": 74, "endOffset": 77}, {"referenceID": 19, "context": "Although we could have applied a number of compression schemes [20, 21], with simplicity and performance in mind, and validated by previous work [22], we adopt a uniform linear quantizer that assumes a uniform distribution of the values within a given range.", "startOffset": 63, "endOffset": 71}, {"referenceID": 20, "context": "Although we could have applied a number of compression schemes [20, 21], with simplicity and performance in mind, and validated by previous work [22], we adopt a uniform linear quantizer that assumes a uniform distribution of the values within a given range.", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "We compare performance obtained from the baseline system to a Bayesian interpolated LM [23], where voice commands and dictation are each represented as a unique task and the corresponding task priors are determined by sweeping parameters on a held-out development set to minimize word error rates rather than setting these based on the log counts.", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "8 MB rescoring LM (with LOUDS compression [17]).", "startOffset": 42, "endOffset": 46}, {"referenceID": 22, "context": "An FST-based decoder graph for the CTC model is created by the usual construction and composition of lexicon and LM transducers [24].", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": "In either context, we can perform the additional step of using on-thefly rescoring as in [13] to bias the language model towards recognizing only these contact names.", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "[12] we annotate our training data with a special $CONTACTS symbol in place of contact names and train a language model that includes this placeholder token.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "The G2P problem is treated as a sequence transcription task as described in [25].", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "The LSTM-G2P performs better in terms of word accuracy compared to traditional joint-sequence models represented as finite state transducers (FSTs) (a detailed comparison can be found in [25]).", "startOffset": 187, "endOffset": 191}, {"referenceID": 12, "context": "For the set containing only contact names, we additionaly evaluate performance obtained using on-thefly biasing [13] towards contact names.", "startOffset": 112, "endOffset": 116}, {"referenceID": 3, "context": "This is achieved by using a CTC-based LSTM acoustic model which predicts context-independent phones and is compressed to a tenth of its original size using a combination of SVD-based compression [4, 5] and quantization.", "startOffset": 195, "endOffset": 201}, {"referenceID": 4, "context": "This is achieved by using a CTC-based LSTM acoustic model which predicts context-independent phones and is compressed to a tenth of its original size using a combination of SVD-based compression [4, 5] and quantization.", "startOffset": 195, "endOffset": 201}, {"referenceID": 11, "context": "Language model personalization is achieved through a combination of vocabulary injection and on-the-fly language model biasing [12, 13].", "startOffset": 127, "endOffset": 135}, {"referenceID": 12, "context": "Language model personalization is achieved through a combination of vocabulary injection and on-the-fly language model biasing [12, 13].", "startOffset": 127, "endOffset": 135}, {"referenceID": 0, "context": "For efficient decoding, we use a on-the-fly rescoring strategy following [1] with additional optimizations for CTC models which reduce computation and memory usage.", "startOffset": 73, "endOffset": 76}], "year": 2017, "abstractText": "We describe a large vocabulary speech recognition system that is accurate, has low latency, and yet has a small enough memory and computational footprint to run faster than real-time on a Nexus 5 Android smartphone. We employ a quantized Long Short-Term Memory (LSTM) acoustic model trained with connectionist temporal classification (CTC) to directly predict phoneme targets, and further reduce its memory footprint using an SVD-based compression scheme. Additionally, we minimize our memory footprint by using a single language model for both dictation and voice command domains, constructed using Bayesian interpolation. Finally, in order to properly handle device-specific information, such as proper names and other context-dependent information, we inject vocabulary items into the decoder graph and bias the language model on-the-fly. Our system achieves 13.5% word error rate on an openended dictation task, running with a median speed that is seven times faster than real-time.", "creator": "LaTeX with hyperref package"}}}