{"id": "1603.03267", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Mar-2016", "title": "Hierarchical Linearly-Solvable Markov Decision Problems", "abstract": "We present a hierarchical reinforcement learning framework that formulates each task in the hierarchy as a special type of Markov decision process for which the Bellman equation is linear and has analytical solution. Problems of this type, called linearly-solvable MDPs (LMDPs) have interesting properties that can be exploited in a hierarchical setting, such as efficient learning of the optimal value function or task compositionality. The proposed hierarchical approach can also be seen as a novel alternative to solving LMDPs with large state spaces. We derive a hierarchical version of the so-called Z-learning algorithm that learns different tasks simultaneously and show empirically that it significantly outperforms the state-of-the-art learning methods in two classical hierarchical reinforcement learning domains: the taxi domain and an autonomous guided vehicle task.", "histories": [["v1", "Thu, 10 Mar 2016 13:50:31 GMT  (119kb,D)", "http://arxiv.org/abs/1603.03267v1", "11 pages, 6 figures, 26th International Conference on Automated Planning and Scheduling"]], "COMMENTS": "11 pages, 6 figures, 26th International Conference on Automated Planning and Scheduling", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["anders jonsson", "vicen\\c{c} g\\'omez"], "accepted": false, "id": "1603.03267"}, "pdf": {"name": "1603.03267.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Linearly-Solvable Markov Decision Problems (Extended Version with Supplementary Material)", "authors": ["Anders Jonsson", "Vicen\u00e7 G\u00f3mez"], "emails": ["anders.jonsson@upf.edu", "vicen.gomez@upf.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most people are able to survive themselves if they do not put themselves in a position to survive themselves. In fact, it is so that they are not able to survive themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are not able to survive themselves. In fact, it is so that they are not able to survive themselves. In fact, it is so that they are not able to survive themselves and to survive themselves. In fact, it is so that they are not able to survive themselves. In fact, it is so that they are not able to survive themselves."}, {"heading": "2 Preliminaries", "text": "In this section, we introduce preliminaries and notation, first defining MDPs and semi-MDPs, then explaining the idea behind MAXQ decomposition, and finally describing linearly solvable MDPs."}, {"heading": "2.1 MDPs and Semi-MDPs", "text": "An MDP M = < S, A, P, R > consists of a series of states Q = Q = P \u00b7 P \u00b7 P \u00b7 P (s \u00b2 s, a) = 1 for each state action pair (s, a). MDPs are usually solved by solving a value function V: S \u00b7 A, which estimates the expected future reward in each state. In the undiscounted case, the optimal value is achieved by solving the Bellman optimality equation: V (s) = a maximum future reward. MDPs are usually solved by defining a value function V: S \u2192 R, which estimates the expected future reward in each state. In the undiscounted case, the optimal value is achieved by solving the Bellman optimality equation: V (s) = a maximum future reward."}, {"heading": "2.2 MAXQ Decomposition", "text": "MAXQ decomposition (Dietterich 2000) splits an MDP M = < S, A, P, R > into a finite series of tasks M = {M0,..., Mn} with the root taskM0, i.e. the reward M0 is equivalent to the solution of M. Each task Mi = < Ti, R, I >, 0 \u2264 i \u2264 n, consists of a finite task containing Ti, an action containing Ai, M and a pseudo-reward R. Each task Mi \u2192 R.A task Mi is primitive if it has no sub-tasks, i.e. if a primitive task Mi corresponds to an action, an action of the original MDP M, defining Mi is always applicable, terminates after a time step and has pseudo-reward 0 everywhere. A non-primitive task Mi can only be applied in non-finite states."}, {"heading": "2.3 Linearly-Solvable MDPs", "text": "Linear solvable MDPs (LMDPs) were first introduced by Todorov (2006; 2009b). The original formulation has no explicit actions, and control consists in changing a predefined uncontrolled probability distribution over the next states. An alternative interpretation is to regard the resulting probability distribution as a stochastic policy on deterministic actions. Todorov's idea was to turn the discrete optimization problem on actions into an ongoing optimization problem on transition probabilities that is convex and analytically tractable. Formally, an LMDP L = < S, R > deviating s consists of a number of states S, an uncontrolled transition probability distribution P: S \u00b7 S \u2192 [1] satisfactory \"P (s) = 1 for each state s.\""}, {"heading": "2.4 LMDPs With Transition-Dependent Rewards", "text": "To develop a hierarchical framework based on LMDPs, we need to take into account the fact that each task can accumulate different amounts of reward. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "3.1 Task Compositionality for Non-Deterministic Tasks", "text": "In this subsection, we expand the definition of hierarchical LMDPs to include non-deterministic tasks. As before, we associate Li to LMDP < S, Pi, Ri > with each task, with the important difference that Li (and it subtasks) can have more than one terminal state. Primitive subtasks (or transitions Ns) are addressed as before, and we disregard them for clarity. We define the counterparts of the equations (5) and (6) for multiple terminal states that have more than one terminal state.Denote tj, k (s), k (s), k (s), k (s), k (s), k (s), k (s), k (s), k (s), k (s), k (k), k (s)."}, {"heading": "3.2 Hierarchical Learning Algorithms", "text": "The goal of a hierarchical LMDP is to learn an estimated hierarchical control policy. Similar to the decomposition of the MAXQ, there are two main alternatives to learning a hierarchical policy: 1. Learn each policy individually in a bottom-up method. 2. Learn all strategies simultaneously using a hierarchical execution in a top-down manner. Implementing an algorithm of the first kind is simple: Since each individual Li task is an LMDP, we can use the power iteration method or Z-learning. Since all subtasks are solved by Li himself, the rewards of Li are known and fixed in solving the first type. To implement a second type algorithm, similar to MAXQ-Q-Q-Q-Learning, we must finish the main L0 task and perform a subtask by Li himself."}, {"heading": "3.3 Intra-Task Z-Learning", "text": "In hierarchical MDPs, the goal is to learn a separate policy for each task. Since Q-Learning is a non-political algorithm, it is possible to use transitions recorded during a task to learn the politics of another task. Such intra-task learning is known to lead to faster convergence (Sutton and Precup 1998). In this section, we describe an algorithm for intra-task learning. As described in Section 2.3, we can use meaning sampling to improve exploration. Let (st, rt, st + 1) be a transition that maps the task Lj based on the estimated policy, and consider an update of the estimated value Z-i of another taskLi. Even if the sample distribution is different from the estimated policy i of Li, we consider the update in Equation (3) Z-i (st)."}, {"heading": "3.4 State Abstraction", "text": "In hierarchical LMDPs, we can apply the same forms of state abstraction as in MAXQ decomposition (Dietterich 1999). The most common form of state abstraction is projection or maximum irrelevance of the node. This form of state abstraction assumes that the state is included, i.e. S = S1 \u00b7 \u00b7 \u00b7 \u00b7 Sk, where S1,..., Sk are the domains of the k state variables. Max Node Irrelevance identifies state variables that are irrelevant to a given task, which implies that the values of these state variables remain the same until the task is completed. Irrelevant state variables can be ignored while learning the value function. Dietterich (1999) identified other conditions under which it is safe to perform state abstraction. A condition, the irrelevance, does not apply to these states, since actions are no more than sheets of the task diagram. A different condition, the irrelevance, the result, applies to one DPs only."}, {"heading": "4 Experiments", "text": "We evaluate the proposed framework in two tasks commonly used in hierarchical MDPs: the taxi domain (Dietterich 2000) and an autonomous vehicle-driven task (AGV) (Ghavamzadeh and Mahadevan 2007). We compare the following methods: Z: Z learning with naive samples (i.e. Random Walk) without corrective terms, as in Equation (2).Z-IS: Z learning with importance tests, but without intra-task learning, as in Equation (3), Z-IS-IL: Z learning with importance tests and intratask learning, as in Equation (10).Q-G: -greedy Q learning without intra-task learning. Q-G-IL: -greedy Q learning with intra-task learning, where Z learning variants with importance tests and intratask learning are best evaluated, as in Equation (10) with intra-task learning."}, {"heading": "4.1 The Taxi Domain", "text": "There is a passenger in one of the four places and the passenger wishes to be taken to one of the three places. There is also a taxi that has to navigate to the position of the passenger. We disassemble the taxi as shown in Figure 2 and apply the state abstraction in the form of navigation tasks. It ignores the position and destination of the passenger."}, {"heading": "4.2 The Autonomous Guided Vehicle (AGV) Domain", "text": "In fact, most of us are able to surpass ourselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we will be able to change the world. \"He added:\" I don't think we will be able to change the world. \"He added:\" I don't think we will be able to change the world. \"He added:\" I don't think we will be able to change the world. \""}, {"heading": "A Experimental Setup", "text": "In fact, it is so that it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way"}], "references": [{"title": "P", "author": ["Abbasi-Yadkori, Y.", "Bartlett"], "venue": "L.; Chen, X.; and Malek, A.", "citeRegEx": "Abbasi.Yadkori et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A", "author": ["M.M. Botvinick", "Y. Niv", "Barto"], "venue": "C.", "citeRegEx": "Botvinick. Niv. and Barto 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "1999", "author": ["Dietterich", "T. G"], "venue": "State abstraction in MAXQ hierarchical reinforcement learning. In Advances in Neural Information Processing Systems 12 (NIPS), 994\u2013", "citeRegEx": "Dietterich 1999", "shortCiteRegEx": null, "year": 1000}, {"title": "T", "author": ["Dietterich"], "venue": "G.", "citeRegEx": "Dietterich 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "R", "author": ["K. Friston", "P. Schwartenbeck", "T. FitzGerald", "M. Moutoussis", "T. Behrens", "Dolan"], "venue": "J.", "citeRegEx": "Friston et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Mahadevan", "author": ["M. Ghavamzadeh"], "venue": "S.", "citeRegEx": "Ghavamzadeh and Mahadevan 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "H", "author": ["Kappen"], "venue": "J.; G\u00f3mez, V.; and Opper, M.", "citeRegEx": "Kappen. G\u00f3mez. and Opper 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "H", "author": ["Kappen"], "venue": "J.", "citeRegEx": "Kappen 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Evaluation of linearly solvable Markov decision process with dynamic model learning in a mobile robot navigation task. Frontiers in Neurorobotics 7:1\u201313", "author": ["Uchibe Kinjo", "K. Doya 2013] Kinjo", "E. Uchibe", "K. Doya"], "venue": null, "citeRegEx": "Kinjo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kinjo et al\\.", "year": 2013}, {"title": "H", "author": ["T. Matsubara", "V. G\u00f3mez", "Kappen"], "venue": "J.", "citeRegEx": "Matsubara. G\u00f3mez. and Kappen 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "D", "author": ["P.A. Ortega", "Braun"], "venue": "A.", "citeRegEx": "Ortega and Braun 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Precup", "author": ["R.S. Sutton"], "venue": "D.", "citeRegEx": "Sutton and Precup 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "A generalized path integral control approach to reinforcement learning", "author": ["Buchli Theodorou", "E. Schaal 2010] Theodorou", "J. Buchli", "S. Schaal"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Theodorou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Theodorou et al\\.", "year": 2010}, {"title": "C", "author": ["Watkins", "C. J"], "venue": "H.", "citeRegEx": "Watkins 1989", "shortCiteRegEx": null, "year": 1989}], "referenceMentions": [], "year": 2016, "abstractText": "We present a hierarchical reinforcement learning framework that formulates each task in the hierarchy as a special type of Markov decision process for which the Bellman equation is linear and has analytical solution. Problems of this type, called linearly-solvable MDPs (LMDPs) have interesting properties that can be exploited in a hierarchical setting, such as efficient learning of the optimal value function or task compositionality. The proposed hierarchical approach can also be seen as a novel alternative to solving LMDPs with large state spaces. We derive a hierarchical version of the socalled Z-learning algorithm that learns different tasks simultaneously and show empirically that it significantly outperforms the state-of-the-art learning methods in two classical hierarchical reinforcement learning domains: the taxi domain and an autonomous guided vehicle task.", "creator": "LaTeX with hyperref package"}}}