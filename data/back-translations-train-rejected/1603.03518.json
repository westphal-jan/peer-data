{"id": "1603.03518", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2016", "title": "High-dimensional Black-box Optimization via Divide and Approximate Conquer", "abstract": "Divide and Conquer (DC) is conceptually well suited to high-dimensional optimization by decomposing a problem into multiple small-scale sub-problems. However, appealing performance can be seldom observed when the sub-problems are interdependent. This paper suggests that the major difficulty of tackling interdependent sub-problems lies in the precise evaluation of a partial solution (to a sub-problem), which can be overwhelmingly costly and thus makes sub-problems non-trivial to conquer. Thus, we propose an approximation approach, named Divide and Approximate Conquer (DAC), which reduces the cost of partial solution evaluation from exponential time to polynomial time. Meanwhile, the convergence to the global optimum (of the original problem) is still guaranteed. The effectiveness of DAC is demonstrated empirically on two sets of non-separable high-dimensional problems.", "histories": [["v1", "Fri, 11 Mar 2016 04:50:59 GMT  (2890kb,D)", "http://arxiv.org/abs/1603.03518v1", "7 pages, 2 figures, conference"], ["v2", "Mon, 21 Mar 2016 02:06:09 GMT  (2890kb,D)", "http://arxiv.org/abs/1603.03518v2", "7 pages, 2 figures, conference"]], "COMMENTS": "7 pages, 2 figures, conference", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["peng yang", "ke tang", "xin yao"], "accepted": false, "id": "1603.03518"}, "pdf": {"name": "1603.03518.pdf", "metadata": {"source": "CRF", "title": "A Novel Divide and Conquer Approach for Large-scale Optimization Problems", "authors": ["Peng Yang", "Ke Tang", "Xin Yao"], "emails": ["trevor@mail.ustc.edu.cn;", "ketang@ustc.edu.cn;", "x.yao@cs.bham.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that it is a matter of a way in which people are able to determine themselves as they wish to behave. (...) It is also such that people are able to determine themselves as they behave. (...) It is as if people were able to determine themselves. (...) It is such as if they were able to determine themselves. (...) It is such as if they were able to determine themselves. (...) It is such as if they were able to determine themselves. (...) It is such as if they were able to determine themselves. (...) \"(...)\" (...) \"(...)\" ((...) \"(...)\" (...) \"(...)\" (() ((()) ((()) ((()) (()) (()) (()) (()) (() () () () () (()) (()) (() () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () ()) () () () () ()) () () () () () ()) () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () () (() () () (() (() () (() () (() () () (() (() () () () () () () () ((() (() (() () (() ((() () () ((() () (() () (() () () ("}, {"heading": "2 Major challenge of dealing with interdependent sub-problems", "text": "The DC strategy consists of three steps: 1) (Divide) Decompile a problem into M di-dimensional partial solutions, with i = 1,..., M and di < D; 2) (Conquer) Search for the best partial solution in each sub-problem using an existing search approach; 3) Merge the best partial solutions received on all sub-problems as the final output. We limit our discussions here to the Conquer phase. Usually, a derived partial solution in each sub-problem must be guided by the solutions with better functional values, despite some exceptions [Kirkpatrick et al., 1983; Tang et al., 2016] Before evaluating a di-dimensional partial solution, it must be supplemented by specifying the values for the variables in other sub-problems. In particular, we mention a vector of such fixed D \u2212 di values as complement a partial solution."}, {"heading": "3 Divide and Approximate Conquer", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The Accurate Complement", "text": "According to Lemma 1, only the brute force method is applicable to precisely supplement partial solutions to interdependent sub-problems. However, the required computational costs are not acceptable. On the other hand, it is an effective method for deriving the mathematical formulation of a problem taking into account the corresponding brute force method, since such a method usually has to scan the entire solution space and thus reflects the problem characteristics in a natural way. The basic idea of the brute force method is described mathematically as follows (the maximization case is taken into account):"}, {"heading": "3.2 DAC: an approximate approach to DC", "text": "In fact, Definition 3 allows us to approximate the exact additions of partial solutions with only a limited number of candidate supplements; Definition 3 allows us to approximate the exact additions of partial solutions with only a limited number of candidates; Definition 3 allows us to approximate the exact additions of partial solutions with only a limited number of candidates; Definition 3 allows us to approximate the partial solutions; Definition 3 allows us to approximate the exact additions of partial solutions with only a limited number of candidates; Definition 4) Definition N allows us to approximate the partial solutions; Definition 3 allows us to approximate the partial solutions; Definition 3 allows us to approximate the partial solutions; Definition 3 allows us to approximate the partial solutions with only a limited number of candidates. (Definition 4) Definition N means that x \u00b2 n \u00b2 n complex problems are more accurate than x \u00b2 r complex problems, since max \u00b2 (Sxi) is reasonable (Sxi) x \u00b2 r."}, {"heading": "3.3 DAC-HC: an instantiation of DAC", "text": "An instance of DAC is presented to illustrate the detailed steps of a DAC algorithm; HC = = new HC operators; HC = = further empirical studies. To establish DAC, both the decomposition strategy and the sub-problem optimizers should be kept to a minimum. To further emphasize the advantages of the DAC framework in empirical studies, the improvement of performance by these two specified components should be minimized. On this basis, we first dissect a problem for random grouping [Yang et al., 2008a], which is, at the beginning of each iteration, M equally large sub-problems are randomly generated. For the sub-problem optimizer, a Parallel Hill Climbing (PHC) method is applied, thus producing the DAC Hill Climbing (DAC-HC-HC). DAC-HC processes perform N RLS processes on each sub-problem."}, {"heading": "4 Empirical Studies", "text": "DAC is proposed to solve inseparable high-dimensional optimization problems. Empirical studies should focus on this in order to verify the effectiveness of DAC-HC. For this purpose, two inseparable high-dimensional optimization problems are used."}, {"heading": "4.1 Varied numbers of interacting variables tests", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"in which he played the role of the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\""}, {"heading": "4.2 Hyper-parameter tuning for multi-class SVMs", "text": "Considering a set of described data {xi, yi} li = 1, where xi-Rn, the classification task is then to train a classifier in terms of {xi, yi} li = 1 to predict the terms of incoming data. Support Vector Machines (SVMs) [Vapnik, 1998] is often considered as a family of powerful tools for classification. Here, the SVM is considered using the linear kernel. Let's consider both terms, the two terms, the two terms, the two terms, the two terms, terms and concepts, the two terms, both terms and concepts, the two terms. Let's consider both terms, the two terms, concepts and concepts, the two concepts, both concepts, both concepts, both concepts, both concepts, both concepts, both concepts, both concepts. \""}, {"heading": "5 Conclusions and Future Directions", "text": "This paper explored the idea of Divide and Conquer on high-dimensional black box optimization problems. We found that the interdependent sub-problems after decomposition could not actually be accurately addressed. Instead, we proposed the method Divide and Approximate Conquer (DAC) to approximately solve each sub-problem. DAC convergence was proven and empirically substantiated. Empirical studies also suggested a simple instantiation of DAC, i.e. DAC-HC. The advantages of DAC-HC over existing representative approaches were verified using two sets of non-separable high-dimensional problems. For future work, we are interested in: \u2022 Promoting the ability of DAC by introducing more ad-funded optimizers of sub-problems. \u2022 Theoretical analysis of the convergence rate of DAC."}], "references": [{"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "Lin", "2011] Chih-Chung Chang", "Chih-Jen Lin"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "pages 300\u2013309", "author": ["Wenxiang Chen", "Thomas Weise", "Zhenyu Yang", "Ke Tang. Large-scale global optimization using cooperative coevolution with variable interaction learning. In Parallel Problem Solving from Nature", "PPSN XI"], "venue": "Springer,", "citeRegEx": "Chen et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Recursive decomposition for nonconvex optimization", "author": ["Abram L Friesen", "Pedro Domingos"], "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Friesen and Domingos. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Evolutionary computation", "author": ["Nikolaus Hansen", "Andreas Ostermeier. Completely derandomized selfadaptation in evolution strategies"], "venue": "9(2):159\u2013195,", "citeRegEx": "Hansen and Ostermeier. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "IEEE Transactions on Neural Networks", "author": ["Chih-Wei Hsu", "Chih-Jen Lin. A comparison of methods for multiclass support vector machines"], "venue": "13(2):415\u2013425,", "citeRegEx": "Hsu and Lin. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "author": ["Jonathan J Hull. A database for handwritten text recognition research"], "venue": "16(5):550\u2013554,", "citeRegEx": "Hull. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Log-linear convergence and optimal bounds for the (1+ 1)-ES", "author": ["Mohamed Jebalia", "Anne Auger", "Pierre Liardet"], "venue": "Artificial Evolution, pages 207\u2013218. Springer,", "citeRegEx": "Jebalia et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Toward large-scale continuous eda: A random matrix theory perspective", "author": ["Ata Kab\u00e1n", "Jakramate Bootkrajang", "Robert John Durrant"], "venue": "Evolutionary computation,", "citeRegEx": "Kab\u00e1n et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning probability distributions in continuous evolutionary algorithms\u2013a comparative review", "author": ["Stefan Kern", "Sibylle D M\u00fcller", "Nikolaus Hansen", "Dirk B\u00fcche", "Jiri Ocenasek", "Petros Koumoutsakos"], "venue": "Natural Computing, 3(1):77\u2013112,", "citeRegEx": "Kern et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimization by simulated annealing", "author": ["S. Kirkpatrick", "C.D. Gelatt", "M.P. Vecchi"], "venue": "Science, 220(4598):671\u2013680", "citeRegEx": "Kirkpatrick et al.. 1983", "shortCiteRegEx": null, "year": 1983}, {"title": "Newsweeder: Learning to filter netnews", "author": ["Ken Lang"], "venue": "Proceedings of the 12th international conference on machine learning, pages 331\u2013339,", "citeRegEx": "Lang. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Metaheuristics in large-scale global continues optimization: A survey", "author": ["Sedigheh Mahdavi", "Mohammad Ebrahim Shiri", "Shahryar Rahnamayan"], "venue": "Information Sciences, 295:407\u2013428,", "citeRegEx": "Mahdavi et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on Evolutionary Computation", "author": ["Mohammad Nabi Omidvar", "Xiaodong Li", "Yi Mei", "Xin Yao. Cooperative co-evolution with differential grouping for large scale optimization"], "venue": "18(3):378\u2013393,", "citeRegEx": "Omidvar et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI 2016)", "author": ["Hong Qian", "Yang Yu. Scaling simultaneous optimistic optimization for high-dimensional non-convex functions with low effective dimensions"], "venue": "Phoenix, AZ,", "citeRegEx": "Qian and Yu. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Benchmark functions for the cec2010 special session and competition on largescale global optimization", "author": ["Ke Tang", "Xiaodong Li", "P.N. Suganthan", "Zhenyu Yang", "Thomas Weise"], "venue": "Technical report, Nature Inspired Computation and Applications Laboratory,", "citeRegEx": "Tang et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "34(3):542\u2013550", "author": ["Ke Tang", "Peng Yang", "Xin Yao. Negatively correlated search. IEEE Journal on Selected Areas in Communications"], "venue": "March", "citeRegEx": "Tang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "volume 1", "author": ["Vladimir Vapnik. Statistical learning theory"], "venue": "Wiley New York,", "citeRegEx": "Vapnik. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Bayesian optimization in high dimensions via random embeddings", "author": ["Wang et al", "2013] Ziyu Wang", "Masrour Zoghi", "Frank Hutter", "David Matheson", "Nando De Freitas"], "venue": "In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence (IJCAI 2013),", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Information Sciences", "author": ["Zhenyu Yang", "Ke Tang", "Xin Yao. Large scale evolutionary optimization using cooperative coevolution"], "venue": "178(15):2985\u20132999,", "citeRegEx": "Yang et al.. 2008a", "shortCiteRegEx": null, "year": 2008}, {"title": "2008 (IEEE World Congress on Computational Intelligence)", "author": ["Zhenyu Yang", "Ke Tang", "Xin Yao. Self-adaptive differential evolution with neighborhood search. In IEEE Congress on Evolutionary Computation"], "venue": "pages 1110\u20131116. IEEE,", "citeRegEx": "Yang et al.. 2008b", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 7, "context": "An intuitive idea to handle a high-dimensional optimization problem is to project its solution space onto lower dimensions, where traditional approaches perform well [Kab\u00e1n et al., 2015].", "startOffset": 166, "endOffset": 186}, {"referenceID": 13, "context": ", Random Embedding techniques [Wang et al., 2013; Qian and Yu, 2016], consider the high-dimensional problem having low effective dimensionality, for which a random projection would suffice to find the global optimal solution of a high-dimensional problem in a low-dimensional space.", "startOffset": 30, "endOffset": 68}, {"referenceID": 18, "context": ",M , and di < D) [Yang et al., 2008a].", "startOffset": 17, "endOffset": 37}, {"referenceID": 1, "context": "Given an appropriate sub-problems optimizer, the abovedescribed DC strategy works well on the so-called separable problems, for which the global optimal optimum can be found by optimizing one dimension at a time regardless of the values taken on the other dimensions [Chen et al., 2010].", "startOffset": 267, "endOffset": 286}, {"referenceID": 12, "context": "If this condition does not hold, the performance of DC heavily relies on the decomposition method [Omidvar et al., 2014], which aims to divide a black-box high-dimensional problem in such a way that the global optimum can still be obtained by solving the sub-problems in a fully independent manner.", "startOffset": 98, "endOffset": 120}, {"referenceID": 11, "context": "In the past few years, a large variety of decomposition methods have been proposed [Mahdavi et al., 2015].", "startOffset": 83, "endOffset": 105}, {"referenceID": 9, "context": "Usually, a derivative-free search process in each sub-problem is guided by the solutions with better function values, despite a few exceptions [Kirkpatrick et al., 1983; Tang et al., 2016].", "startOffset": 143, "endOffset": 188}, {"referenceID": 15, "context": "Usually, a derivative-free search process in each sub-problem is guided by the solutions with better function values, despite a few exceptions [Kirkpatrick et al., 1983; Tang et al., 2016].", "startOffset": 143, "endOffset": 188}, {"referenceID": 1, "context": "(Interacting Variables) [Chen et al., 2010] Given a D-dimensional problem, its i-th and j-th variables are said to be interacting, if the rank of two partial solutions xi and xi in the i-th dimension may change with different complements, e.", "startOffset": 24, "endOffset": 43}, {"referenceID": 18, "context": "On this basis, we first decompose a problem f via random grouping [Yang et al., 2008a].", "startOffset": 66, "endOffset": 86}, {"referenceID": 8, "context": "After that, each search step-size is adapted at every iteration in terms of the 1/5 successful rule [Kern et al., 2004], using Eq.", "startOffset": 100, "endOffset": 119}, {"referenceID": 2, "context": "Meanwhile, it has also been observed that, in many real-world problems, only parts of variables are interacting [Friesen and Domingos, 2015].", "startOffset": 112, "endOffset": 140}, {"referenceID": 3, "context": "In the first group of comparison, DAC-HC is compared with CMA-ES [Hansen and Ostermeier, 2001], RESOO [Qian and Yu, 2016], and DECC-I [Omidvar et al.", "startOffset": 65, "endOffset": 94}, {"referenceID": 13, "context": "In the first group of comparison, DAC-HC is compared with CMA-ES [Hansen and Ostermeier, 2001], RESOO [Qian and Yu, 2016], and DECC-I [Omidvar et al.", "startOffset": 102, "endOffset": 121}, {"referenceID": 12, "context": "In the first group of comparison, DAC-HC is compared with CMA-ES [Hansen and Ostermeier, 2001], RESOO [Qian and Yu, 2016], and DECC-I [Omidvar et al., 2014], which are representatives of three basic ideas for high-dimensional optimization: the straightforward method, dimensionality reduction, and DC, respectively.", "startOffset": 134, "endOffset": 156}, {"referenceID": 19, "context": "Besides, the sub-problems of DECC-I are optimized by a variant of Differential Evolution [Yang et al., 2008b], which has been empirically observed more advanced than the employed RLSs [Tang et al.", "startOffset": 89, "endOffset": 109}, {"referenceID": 15, "context": ", 2008b], which has been empirically observed more advanced than the employed RLSs [Tang et al., 2016].", "startOffset": 83, "endOffset": 102}, {"referenceID": 6, "context": "It should be noted that, the employed RLS has also been theoretically proved to converge log-linearly on the sphere function [Jebalia et al., 2008].", "startOffset": 125, "endOffset": 147}, {"referenceID": 16, "context": "Support Vector Machines (SVMs) [Vapnik, 1998] is often considered as a family of powerful tools for classification.", "startOffset": 31, "endOffset": 45}, {"referenceID": 5, "context": ", usps [Hull, 1994], news20 [Lang, 1995], and letter [Hsu and Lin, 2002], are used for comparison, which contain 10, 20, 26 classes in each and yield three problems with 45, 190, 325 dimensions, respectively.", "startOffset": 7, "endOffset": 19}, {"referenceID": 10, "context": ", usps [Hull, 1994], news20 [Lang, 1995], and letter [Hsu and Lin, 2002], are used for comparison, which contain 10, 20, 26 classes in each and yield three problems with 45, 190, 325 dimensions, respectively.", "startOffset": 28, "endOffset": 40}, {"referenceID": 4, "context": ", usps [Hull, 1994], news20 [Lang, 1995], and letter [Hsu and Lin, 2002], are used for comparison, which contain 10, 20, 26 classes in each and yield three problems with 45, 190, 325 dimensions, respectively.", "startOffset": 53, "endOffset": 72}], "year": 2017, "abstractText": "Divide and Conquer (DC) is conceptually well suited to high-dimensional optimization by decomposing a problem into multiple small-scale subproblems. However, appealing performance can be seldom observed when the sub-problems are interdependent. This paper suggests that the major difficulty of tackling interdependent sub-problems lies in the precise evaluation of a partial solution (to a sub-problem), which can be overwhelmingly costly and thus makes sub-problems nontrivial to conquer. Thus, we propose an approximation approach, named Divide and Approximate Conquer (DAC), which reduces the cost of partial solution evaluation from exponential time to polynomial time. Meanwhile, the convergence to the global optimum (of the original problem) is still guaranteed. The effectiveness of DAC is demonstrated empirically on two sets of non-separable high-dimensional problems.", "creator": "LaTeX with hyperref package"}}}