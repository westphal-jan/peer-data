{"id": "1603.03541", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2016", "title": "Watch-n-Patch: Unsupervised Learning of Actions and Relations", "abstract": "There is a large variation in the activities that humans perform in their everyday lives. We consider modeling these composite human activities which comprises multiple basic level actions in a completely unsupervised setting. Our model learns high-level co-occurrence and temporal relations between the actions. We consider the video as a sequence of short-term action clips, which contains human-words and object-words. An activity is about a set of action-topics and object-topics indicating which actions are present and which objects are interacting with. We then propose a new probabilistic model relating the words and the topics. It allows us to model long-range action relations that commonly exist in the composite activities, which is challenging in previous works. We apply our model to the unsupervised action segmentation and clustering, and to a novel application that detects forgotten actions, which we call action patching. For evaluation, we contribute a new challenging RGB-D activity video dataset recorded by the new Kinect v2, which contains several human daily activities as compositions of multiple actions interacting with different objects. Moreover, we develop a robotic system that watches people and reminds people by applying our action patching algorithm. Our robotic setup can be easily deployed on any assistive robot.", "histories": [["v1", "Fri, 11 Mar 2016 07:13:59 GMT  (8276kb,D)", "http://arxiv.org/abs/1603.03541v1", "arXiv admin note: text overlap witharXiv:1512.04208"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1512.04208", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.RO", "authors": ["chenxia wu", "jiemi zhang", "ozan sener", "bart selman", "silvio savarese", "ashutosh saxena"], "accepted": false, "id": "1603.03541"}, "pdf": {"name": "1603.03541.pdf", "metadata": {"source": "CRF", "title": "Watch-n-Patch: Unsupervised Learning of Actions and Relations", "authors": ["Chenxia Wu", "Jiemi Zhang", "Ozan Sener", "Bart Selman", "Silvio Savarese", "Ashutosh Saxena"], "emails": ["awu@cs.cornell.edu", "ozan@cs.cornell.edu", "selman@cs.cornell.edu", "jmzhang10@gmail.com", "ssilvio@cs.stanford.edu"], "sections": [{"heading": null, "text": "Index terms - unsupervised learning, discovery of activities, robotic applications.F"}, {"heading": "1 INTRODUCTION", "text": "The average adult forgets three key factors or events that often occur in a compound activity, but not in a second activity. Therefore, it is important for a vision system to recognize not only what a person is doing, but also what he forgot to do. For example, in Figure 1, someone takes milk from the refrigerator, pours the milk into the cup, takes the cup and leaves it without putting the milk back, then the milk would go bad. In this paper, we focus on modeling these compound human activities, which then recognize the forgotten actions of a robot learning from a completely unlabeled set of RGB-D videos. A human activity is composed, that is, it consists of several basic actions."}, {"heading": "2 RELATED WORK", "text": "This year, it has come to the point where there can only be a noteworthy aversion to such aversion."}, {"heading": "3 OVERVIEW", "text": "In fact, it is such that most people are able to determine for themselves what they want and what they do not want. (...) It is not so that people are able to decide what they want and what they do not want. (...) It is not so that they do it. (...) It is not so that they do it. (...) It is not so that they do it. (...) It is so that they do it. (...) It is as if they cannot. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (.). (...). (...). (.). (.). (...). (.). (.). (). (...). (. (). (). (). (.). (). (.). (...). (). (...). (.). (...). (.). (). (.). (...). (). (). (.). (.). ().). (.).). (...). (...). (. (.).). (.). (...). (...). (.). (...). (.). (.). (. ().). (.).). (...). (. (...). (. ().). (.).). (. (.). (.). (. (). (). ().). (. (). (.). (). (). (). (). (). (). (). (). (). (). (). (). (. ().). (). (). (). ().). (). (). (). (). (). (). (). ().). (). (). (). ().). (). ("}, {"heading": "4 VISUAL FEATURES", "text": "We describe how to extract the visual characteristics of a clip in this section. We extract both the characteristics of the human skeleton and the characteristics of the interacting object from the output of the Kinect v2 [1], which has an improved body tracker and the higher resolution of the RGB-D frame than the Kinect v1. The tracked human skeleton has a total of 25 joints. Let us leave Xu = {x (1) u, x (2) u, \u00b7 \u00b7, x (25) u} the 3D coordinates of 25 joints of a skeleton in the current frame and first calculate the cosine of the angles between the connected body parts in each frame: \u03b1 \u2212 pq) = (p (q)))) / (p (p) | | p (q), where the vector p (p) = x (i) \u2212 x (j) represents the body part."}, {"heading": "5 LEARNING MODEL", "text": "The novelty of our model is the ability to capture both short-term and long-term relationships between actions in the composed activity videos in an unattended manner. By using these relationships we can simultaneously segment the video and assign the action themes, each of which consists of a human word associated with the human dictionary. Consider a collection of D-videos (documents in the theme model). Each video as document d consists of continuous clips {cnd}, each of which consists of a human word associated with the human dictionary and an object-word dictionary associated with the object-dictionary."}, {"heading": "5.1 Learning and Inference", "text": "There is a video in which the word whnd, w o and the relative time of the distributions are observed. (1) We can observe the distributions (1) k, p (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) and the temporal distribution (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp (kp) kp (12 kp) kp (12 kp (kp) kp (12 kp (12 kp) kp (12 kp) kp (12 kp) kp (12 kp (kp) kp (12 kp) kp (12 kp (kp) kp (12 kp) kp (12 kp) kp (kp (12 kp) kp (12 kp"}, {"heading": "6 WATCH-BOT TO REMINDING OF FORGOTTEN ACTIONS", "text": "The average adult forgets three key factors, or events, every day. Thus, it is important that a forgotten robot is able to recognize not only what a person is doing, but also what he has forgotten to do. In this section, we describe a new robotic system (see Figure 6) 7 to recognize forgotten actions and remind people of what we have called action clusters and relationships. Instead, it learns about action clich\u00e9s and relationships from the unlabeled action videos and uses them to recognize forgotten actions and remind people of them. Therefore, it is important to prove rich relationships from videos to prove forgotten actions. Our model models are paired and protracted temporary relationships of actions / topics."}, {"heading": "7 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Watch-n-Patch Dataset", "text": "We are collecting a new challenging RGB-D activity record recorded by the new Kinect v2 camera. In addition, each video in the record contains 2-7 actions that interact with various objects (see examples in Fig. 12). The new Kinect v2 features higher resolution RGB-D images (RGB: 1920 x 1080, Depth: 512 x 424) and improved body tracking of human skeletons (25 body joints). We are recording 458 videos with a total length of about 230 minutes. We are asking 7 subjects to perform human daily activities in 8 offices and 5 kitchens with complex backgrounds. And, in each environment, the activities are recorded in different views. They consist of fully commented 21 types of actions (10 in the office, 11 in the kitchen) that interact with 23 types of objects. We are also recording the audio, although it is not used in this paper. To get a variation in the activities, we are asking the participants to complete the task with different combinations of actions to be placed back on the refrigerator and some actions, such as performing some actions during some actions."}, {"heading": "7.2 Experimental Setting and Compared Baselines", "text": "We evaluate \"office\" and \"kitchen\" in two environments. In each environment, we share the data in one move with the most complete videos (office: 87, kitchen 119) and a few forgotten videos (office: 10, kitchen 10), and a test set with a few complete videos (office: 10, kitchen 20) and most forgotten videos (office: 89, kitchen 113). In our experiments, we compare seven unattended approaches with only action themes. They are Hidden Markov Model (HMM), Theme Model LDA (CTM), Theme Model Absolute Time (TM-AT), Correlate Theme Model Absolute Time (CTM-AT), Theme Model Absolute Time (CTM-AT), Relative Time Theme Model (TM-RT) and our Causal Theme Model (CaTM-A)."}, {"heading": "7.3 Evaluation Metrics", "text": "We want to evaluate whether the unattended learned action topics and states of HMM are semantically meaningful. In the unattended environment, we need to assign the assigned topics to the soil truth labels for evaluation, which could be done by counting the assigned frame5. We train a codebook the size of 2000 and encode the extracted DTF features in each clip as the bag of characteristics with the codebook. 9 between topics and soil truth classes. Let ki, ci consider the assigned topic and the soil truth class of Frame5. Counting a mapping is: mkc = charged i-characteristics (ki, k), where we consider the assigned topic and the soil truth class of the frame.We are the assigned topic and the assigned truth class of the frame."}, {"heading": "7.4 Results", "text": "In this context, it should be noted that the solution to problems that have arisen in the past is not a solution, but a solution that is capable of bringing about a solution."}, {"heading": "7.5 Robotic Experiments", "text": "In this section, we show how our Watch-Bot reminds people of the forgotten actions in the real scenarios. We test two forgotten scenarios in \"office\" and \"kitchen,\" respectively. We use a subset of the data set to train the model separately in each activity type. In each scenario, we ask 3 people to perform the activity twice. Therefore, we test a total of 24 attempts. We evaluate three aspects, one of which is objective, the success rate (Succ rate): the laser spot inside the object is correct; the other two are subjective, the average subjective accuracy value (Subj AccScore): We ask the participant whether he or she believes the object shown is correct; and the average subjective auxiliary value (Subj-HelpScore): We ask the participant if the performance of the robot is helpful. Both of them give the best results of our experiments on the current scale of 1-3%."}, {"heading": "8 CONCLUSION AND FUTURE WORK", "text": "In this work, we presented an algorithm that models human activities in a completely unattended environment. We demonstrated that it is important to model the long-term relationships between actions. To achieve this, we considered the video as a sequence of human words / object words and an activity as a set of action themes / object themes. Then, we modeled the word-theme distributions, the thematic correlations, and the theme relative time distributions. We then demonstrated the effectiveness of our model in unattended action segmentation and clustering, as well as action patching. In addition, we demonstrated that our proposed robotic system was capable of effectively reminding people of forgotten actions in the real world using the action patching algorithm. For evaluation, we also presented a new challenging RGB-D activity video dataset.Although we do not see the promising results and interesting applications of the purely supervised work, we have shown that the percentage of unsupervised performance is greater than the one that we can see in the models."}], "references": [{"title": "Human activity analysis: A review", "author": ["J. Aggarwal", "M. Ryoo"], "venue": "ACM Comput. Surv., 43(3):16:1\u201316:43,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Video classification using semantic concept co-occurrences", "author": ["S.M. Assari", "A.R. Zamir", "M. Shah"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognition of complex events: Exploiting temporal dynamics between underlying concepts", "author": ["S. Bhattacharya", "M.M. Kalayeh", "R. Sukthankar", "M. Shah"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "A correlated topic model of science", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "The Annals of Applied Statistics, 1(1):17\u201335,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "Text mining: classification, clustering, and applications, 10:71,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., 3:993\u20131022,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Weakly supervised action labeling in videos under ordering constraints", "author": ["P. Bojanowski", "R. Lajugie", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid", "J. Sivic"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Action recognition using ensemble weighted multi-instance learning", "author": ["G. Chen", "M. Giuliani", "D.S. Clarke", "A.K. Gaschler", "A. Knoll"], "venue": "International Conference on Robotics and Automation (ICRA),", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Activity recognition for natural human robot interaction", "author": ["A. Chrungoo", "S. Manimaran", "B. Ravindran"], "venue": "Social Robotics, volume 8755, pages 84\u201394.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Structured forests for fast edge detection", "author": ["P. Doll\u00e1r", "C.L. Zitnick"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic annotation of human actions in video", "author": ["O. Duchenne", "I. Laptev", "J. Sivic", "F. Bach", "J. Ponce"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Time based activity inference using latent dirichlet allocation", "author": ["T.A. Faruquie", "P.K. Kalra", "S. Banerjee"], "venue": "British Machine Vision Conference (BMVC),", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayesian data analysis", "author": ["A. Gelman", "J.B. Carlin", "H.S. Stern", "D.B. Dunson", "A. Vehtari", "D.B. Rubin"], "venue": "CRC press,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint segmentation and classification of human actions in video", "author": ["M. Hoai", "Z. zhong Lan", "F. De la Torre"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Jointly learning heterogeneous features for rgb-d activity recognition", "author": ["J.-F. Hu", "W.-S. Zheng", "J. Lai", "J. Zhang"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to recognize human activities from soft labeled data", "author": ["N. Hu", "Z. Lou", "G. Englebienne", "B. Krse"], "venue": "Proceedings of Robotics: Science and Systems (RSS),", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Representing videos using mid-level discriminative patches", "author": ["A. Jain", "A. Gupta", "M. Rodriguez", "L. Davis"], "venue": "The IEEE conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised spectral dual assignment clustering of human actions in context", "author": ["S. Jones", "L. Shao"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient feature extraction, encoding and classification for action recognition", "author": ["V. Kantorov", "I. Laptev"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Event detection in crowded videos", "author": ["Y. Ke", "R. Sukthankar", "M. Hebert"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "The doubly correlated nonparametric topic model", "author": ["D.I. Kim", "E.B. Sudderth"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Human focused action localization in video", "author": ["A. Kl\u00e4ser", "M. Marsza\u0142ek", "C. Schmid", "A. Zisserman"], "venue": "International Workshop on Sign, Gesture, and Activity (SGA) in Conjunction with ECCV,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning human activities and object affordances from RGB-D videos", "author": ["H.S. Koppula", "R. Gupta", "A. Saxena"], "venue": "I. J. Robotic Res., 32(8):951\u2013 970,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Anticipating human activities using object affordances for reactive robotic response", "author": ["H.S. Koppula", "A. Saxena"], "venue": "Robotics: Science and Systems (RSS),", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning spatio-temporal structure from RGB-D videos for human activity detection and anticipation", "author": ["H.S. Koppula", "A. Saxena"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "The language of actions: Recovering the syntax and semantics of goal-directed human activities", "author": ["H. Kuehne", "A. Arslan", "T. Serre"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Retrieving actions in movies", "author": ["I. Laptev", "P. Perez"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Depth and skeleton associated action recognition without online accessible rgb-d cameras", "author": ["Y.-Y. Lin", "J.-H. Hua", "N.C. Tang", "M.-H. Chen", "H.-Y. Mark Liao"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognizing human actions by attributes", "author": ["J. Liu", "B. Kuipers", "S. Savarese"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Feature set selection and optimal classifier for human activity recognition", "author": ["M. Losch", "S. Schmidt-Rohr", "S. Knoop", "S. Vacek", "R. Dillmann"], "venue": "Robot and Human interactive Communication,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Space-time tree ensemble for action  recognition", "author": ["S. Ma", "L. Sigal", "S. Sclaroff"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition", "author": ["S. Mathe", "C. Sminchisescu"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "A cause and effect analysis of motion trajectories for modeling actions", "author": ["S. Narayan", "K.R. Ramakrishnan"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "A clickable world: Behavior selection through pointing and context for mobile manipulation", "author": ["H. Nguyen", "A. Jain", "C.D. Anderson", "C.C. Kemp"], "venue": "International Conference on Intelligent Robots and Systems,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiple granularity analysis for fine-grained action detection", "author": ["B. Ni", "V.R. Paramathayalan", "P. Moulin"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling temporal structure of decomposable motion segments for activity classification", "author": ["J.C. Niebles", "C.-W. Chen", "L. Fei-Fei"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Parsing videos of actions with segmental grammars", "author": ["H. Pirsiavash", "D. Ramanan"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Human activity recognition for domestic robots", "author": ["L. Piyathilaka", "S. Kodagoda"], "venue": "Field and Service Robotics, volume 105, pages 395\u2013408,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Rgb-(d) scene labeling: Features and algorithms", "author": ["X. Ren", "L. Bo", "D. Fox"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Action bank: A high-level representation of activity in video", "author": ["S. Sadanand", "J.J. Corso"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "A database for fine grained activity detection of cooking activities", "author": ["B. Schiele"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "A constructive definition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica, 4:639\u2013650,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1994}, {"title": "Human action segmentation and recognition using discriminative semi-markov models", "author": ["Q. Shi", "L. Cheng", "L. Wang", "A. Smola"], "venue": "International Journal of Computer Vision (IJCV), 93(1):22\u201332,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Temporally coherent interpretations for long videos using pattern theory", "author": ["F. Souza", "S. Sarkar", "A. Srivastava", "J. Su"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive background mixture models for real-time tracking", "author": ["C. Stauffer", "W. Grimson"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1999}, {"title": "Unstructured human activity detection from rgbd images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "International Conference on Robotics and Automation (ICRA),", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning latent temporal structure for complex event detection", "author": ["K. Tang", "L. Fei-Fei", "D. Koller"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Spatiotemporal deformable part models for action detection", "author": ["Y. Tian", "R. Sukthankar", "M. Shah"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Human action recognition  14 by representing 3d skeletons as points in a lie group", "author": ["R. Vemulapalli", "F. Arrate", "R. Chellappa"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "From stochastic grammar to bayes network: Probabilistic parsing of complex activity", "author": ["N.N. Vo", "A.F. Bobick"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}, {"title": "Action Recognition by Dense Trajectories", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C.-L. Liu"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}, {"title": "A hierarchical context model for event recognition in surveillance video", "author": ["X. Wang", "Q. Ji"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Topics over time: A non-markov continuoustime model of topical trends", "author": ["X. Wang", "A. McCallum"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2006}, {"title": "Hierarchical semantic labeling for taskrelevant rgb-d perception", "author": ["C. Wu", "I. Lenz", "A. Saxena"], "venue": "Robotics: Science and Systems (RSS),", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2014}, {"title": "Watch-n-patch: Unsupervised understanding of actions and relations", "author": ["C. Wu", "J. Zhang", "S. Savarese", "A. Saxena"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2015}, {"title": "Watch-bot: Unsupervised learning for reminding humans of forgotten actions", "author": ["C. Wu", "J. Zhang", "B. Selman", "S. Savarese", "A. Saxena"], "venue": "International Conference on Robotics and Automation (ICRA),", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition", "author": ["D. Wu", "L. Shao"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-feature maxmargin hierarchical bayesian model for action recognition", "author": ["S. Yang", "C. Yuan", "B. Wu", "W. Hu", "F. Wang"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2015}, {"title": "Robot learning manipulation action plans by watching unconstrained videos from the world wide web", "author": ["Y. Yang", "Y. Li", "C. Fermuller", "Y. Aloimonos"], "venue": "AAAI,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2015}, {"title": "Discovering motion primitives for unsupervised grouping and one-shot learning of human actions, gestures, and expressions", "author": ["Y. Yang", "I. Saleemi", "M. Shah"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI), 35(7):1635\u20131648,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 54, "context": "Parts of this work have been published in [57], [58] as the conference version.", "startOffset": 42, "endOffset": 46}, {"referenceID": 55, "context": "Parts of this work have been published in [57], [58] as the conference version.", "startOffset": 48, "endOffset": 52}, {"referenceID": 28, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 70, "endOffset": 74}, {"referenceID": 40, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 117, "endOffset": 120}, {"referenceID": 10, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 122, "endOffset": 126}, {"referenceID": 26, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 176, "endOffset": 180}, {"referenceID": 34, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 182, "endOffset": 186}, {"referenceID": 47, "context": "In the training, they are given fully labeled actions in videos [31], [42], [43], or weakly supervised action labels [9], [13], or locations of human/their interacting objects [29], [37], [50].", "startOffset": 188, "endOffset": 192}, {"referenceID": 46, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 138, "endOffset": 142}, {"referenceID": 42, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 144, "endOffset": 148}, {"referenceID": 2, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 181, "endOffset": 184}, {"referenceID": 1, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 214, "endOffset": 217}, {"referenceID": 25, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 219, "endOffset": 223}, {"referenceID": 36, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 225, "endOffset": 229}, {"referenceID": 49, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 231, "endOffset": 235}, {"referenceID": 51, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 237, "endOffset": 241}, {"referenceID": 19, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 287, "endOffset": 291}, {"referenceID": 21, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 293, "endOffset": 297}, {"referenceID": 24, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 299, "endOffset": 303}, {"referenceID": 35, "context": "Among them, the temporal structure of actions is often discovered by Markov models such as Hidden Markov Model (HMM) [49] and semi-Markov [16], [45], or by linear dynamical systems [5], or by hierarchical grammars [4], [28], [39], [52], [54], or by other spatio-temporal representations [22], [24], [27], [38].", "startOffset": 305, "endOffset": 309}, {"referenceID": 23, "context": "Object-in-use contextual information has also been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 89, "endOffset": 93}, {"referenceID": 24, "context": "Object-in-use contextual information has also been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 95, "endOffset": 99}, {"referenceID": 34, "context": "Object-in-use contextual information has also been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 101, "endOffset": 105}, {"referenceID": 51, "context": "Object-in-use contextual information has also been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 107, "endOffset": 111}, {"referenceID": 23, "context": "We also use the more informative human skeleton features and RGB-D object features, which have shown higher performance over RGB only features for action recognition [26], [30], [59].", "startOffset": 166, "endOffset": 170}, {"referenceID": 27, "context": "We also use the more informative human skeleton features and RGB-D object features, which have shown higher performance over RGB only features for action recognition [26], [30], [59].", "startOffset": 172, "endOffset": 176}, {"referenceID": 56, "context": "We also use the more informative human skeleton features and RGB-D object features, which have shown higher performance over RGB only features for action recognition [26], [30], [59].", "startOffset": 178, "endOffset": 182}, {"referenceID": 5, "context": ", [8]) and proposed a Casual Topic Model (CaTM).", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "There is a large number of works on action recognition, which can be referred in recent surveys [3].", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 57, "endOffset": 60}, {"referenceID": 10, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 62, "endOffset": 66}, {"referenceID": 26, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 68, "endOffset": 72}, {"referenceID": 28, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 80, "endOffset": 84}, {"referenceID": 35, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 86, "endOffset": 90}, {"referenceID": 39, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 92, "endOffset": 96}, {"referenceID": 47, "context": "Most previous works on action recognition are supervised [9], [13], [29], [31], [34], [38], [42], [50].", "startOffset": 98, "endOffset": 102}, {"referenceID": 46, "context": "Among them, the most popular are linear-chain models such as hidden markov model (HMM) [49], semi-Markov [16], [45] and the linear dynamic system [5].", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "Among them, the most popular are linear-chain models such as hidden markov model (HMM) [49], semi-Markov [16], [45] and the linear dynamic system [5].", "startOffset": 105, "endOffset": 109}, {"referenceID": 42, "context": "Among them, the most popular are linear-chain models such as hidden markov model (HMM) [49], semi-Markov [16], [45] and the linear dynamic system [5].", "startOffset": 111, "endOffset": 115}, {"referenceID": 2, "context": "Among them, the most popular are linear-chain models such as hidden markov model (HMM) [49], semi-Markov [16], [45] and the linear dynamic system [5].", "startOffset": 146, "endOffset": 149}, {"referenceID": 25, "context": "More complex hierarchical relations [28], [39], [52], [54] or graph relations [4], [46] are considered in modeling actions in the complex activity.", "startOffset": 36, "endOffset": 40}, {"referenceID": 36, "context": "More complex hierarchical relations [28], [39], [52], [54] or graph relations [4], [46] are considered in modeling actions in the complex activity.", "startOffset": 42, "endOffset": 46}, {"referenceID": 49, "context": "More complex hierarchical relations [28], [39], [52], [54] or graph relations [4], [46] are considered in modeling actions in the complex activity.", "startOffset": 48, "endOffset": 52}, {"referenceID": 51, "context": "More complex hierarchical relations [28], [39], [52], [54] or graph relations [4], [46] are considered in modeling actions in the complex activity.", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "More complex hierarchical relations [28], [39], [52], [54] or graph relations [4], [46] are considered in modeling actions in the complex activity.", "startOffset": 78, "endOffset": 81}, {"referenceID": 43, "context": "More complex hierarchical relations [28], [39], [52], [54] or graph relations [4], [46] are considered in modeling actions in the complex activity.", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "There are also some works focusing on detecting local action patches, primitives, trajectories or spatio-temporal features [19], [33], [35], [62] without considering the high-level action relations.", "startOffset": 123, "endOffset": 127}, {"referenceID": 30, "context": "There are also some works focusing on detecting local action patches, primitives, trajectories or spatio-temporal features [19], [33], [35], [62] without considering the high-level action relations.", "startOffset": 129, "endOffset": 133}, {"referenceID": 32, "context": "There are also some works focusing on detecting local action patches, primitives, trajectories or spatio-temporal features [19], [33], [35], [62] without considering the high-level action relations.", "startOffset": 135, "endOffset": 139}, {"referenceID": 59, "context": "There are also some works focusing on detecting local action patches, primitives, trajectories or spatio-temporal features [19], [33], [35], [62] without considering the high-level action relations.", "startOffset": 141, "endOffset": 145}, {"referenceID": 59, "context": "[62] develop a meaningful representation by discovering local motion primitives in an unsupervised way, then a HMM is learned over these primitives.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] propose an", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Unlike these approaches, we use the richer human skeleton and RGB-D features rather than the RGB action features [21], [53].", "startOffset": 113, "endOffset": 117}, {"referenceID": 50, "context": "Unlike these approaches, we use the richer human skeleton and RGB-D features rather than the RGB action features [21], [53].", "startOffset": 119, "endOffset": 123}, {"referenceID": 27, "context": "Skeleton-based approach focus on proposing good skeletal representations [30], [43], [48], [51], [59].", "startOffset": 73, "endOffset": 77}, {"referenceID": 40, "context": "Skeleton-based approach focus on proposing good skeletal representations [30], [43], [48], [51], [59].", "startOffset": 79, "endOffset": 83}, {"referenceID": 45, "context": "Skeleton-based approach focus on proposing good skeletal representations [30], [43], [48], [51], [59].", "startOffset": 85, "endOffset": 89}, {"referenceID": 48, "context": "Skeleton-based approach focus on proposing good skeletal representations [30], [43], [48], [51], [59].", "startOffset": 91, "endOffset": 95}, {"referenceID": 56, "context": "Skeleton-based approach focus on proposing good skeletal representations [30], [43], [48], [51], [59].", "startOffset": 97, "endOffset": 101}, {"referenceID": 23, "context": "Object-in-use contextual information has been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 84, "endOffset": 88}, {"referenceID": 24, "context": "Object-in-use contextual information has been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 90, "endOffset": 94}, {"referenceID": 34, "context": "Object-in-use contextual information has been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 96, "endOffset": 100}, {"referenceID": 51, "context": "Object-in-use contextual information has been commonly used for recognizing actions [26], [27], [37], [54].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "[17] propose a joint learning model to simultaneously learn heterogenous features from RGB-D activity videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "LDA [8] was the first hierarchical Bayesian topic model and widely used in different applications.", "startOffset": 4, "endOffset": 7}, {"referenceID": 3, "context": "The correlated topic models [6], [23] add the priors over topics to capture topic correlations.", "startOffset": 28, "endOffset": 31}, {"referenceID": 20, "context": "The correlated topic models [6], [23] add the priors over topics to capture topic correlations.", "startOffset": 33, "endOffset": 37}, {"referenceID": 52, "context": "A topic model over absolute timestamps of words is proposed in [55] and has been applied to action recognition [14].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "A topic model over absolute timestamps of words is proposed in [55] and has been applied to action recognition [14].", "startOffset": 111, "endOffset": 115}, {"referenceID": 57, "context": "Recently, a multifeature max-margin hierarchical Bayesian model [60] is proposed to jointly learn a high-level representation by combining a hierarchical generative model and discriminative maxmargin classifiers in a unified Bayesian framework.", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "Our work is also related to the works on recognizing human actions for robotics [10], [25], [32].", "startOffset": 80, "endOffset": 84}, {"referenceID": 22, "context": "Our work is also related to the works on recognizing human actions for robotics [10], [25], [32].", "startOffset": 86, "endOffset": 90}, {"referenceID": 29, "context": "Our work is also related to the works on recognizing human actions for robotics [10], [25], [32].", "startOffset": 92, "endOffset": 96}, {"referenceID": 58, "context": "[61] presented a system that learns manipulation action plans for robot from unconstrained youtube videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] proposed an activity recognition system trained from soft labeled data for the assistant robot.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] introduced a human-like stylized gestures for better human-robot interaction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[40] used 3D skeleton features and trained dynamic bayesian networks for domestic service robots.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "The output laser spot on object is also related to the work \u2018a clickable world\u2019 [36], which selects the appropriate behavior to execute for an assistive object-fetching robot using the 3D location of the click by the laser pointer.", "startOffset": 80, "endOffset": 84}, {"referenceID": 23, "context": "The human skeleton features and RGB-D object features have shown higher performance over RGB only features for the human action modeling [26], [30], [59].", "startOffset": 137, "endOffset": 141}, {"referenceID": 27, "context": "The human skeleton features and RGB-D object features have shown higher performance over RGB only features for the human action modeling [26], [30], [59].", "startOffset": 143, "endOffset": 147}, {"referenceID": 56, "context": "The human skeleton features and RGB-D object features have shown higher performance over RGB only features for the human action modeling [26], [30], [59].", "startOffset": 149, "endOffset": 153}, {"referenceID": 5, "context": "In order to build a compact representation of the action video, we draw parallels to document modeling in the natural language [8] to represent a video as a sequence of words.", "startOffset": 127, "endOffset": 130}, {"referenceID": 56, "context": "So we extract the motion features and off-set features [59] by computing their Euclidean distances D(, ) to previous frame f u,u\u22121, f u,u\u22121 and the first frame f u,1, f \u03b1 u,1 in the clip: f u,u\u22121 = {D(x u , x (i) u\u22121)} i=1, f u,u\u22121 = {D(\u03b1 u , \u03b1 (pq) u\u22121)}pq; f u,1 = {D(x u , x (i) 1 )} i=1, f u,1 = {D(\u03b1 u , \u03b1 (pq) 1 )}pq.", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "To detect the interacting objects, first we segment each frame into super-pixels using a fast edge detection approach [12] on both RGB and depth images.", "startOffset": 118, "endOffset": 122}, {"referenceID": 44, "context": "We then apply the moving foreground mask [47] to remove the unnecessary steady backgrounds and select those super-pixels within a distance to the human hands in both 3D points and 2D pixels.", "startOffset": 41, "endOffset": 45}, {"referenceID": 38, "context": "Finally, we extract six kernel descriptors [41] from the bounding box of each frame in the trajectory: gradient, color, local binary pattern, depth gradient, spin, surface normals, and KPCA/self-similarity, which have been proven to be useful features for RGB-D data [56].", "startOffset": 43, "endOffset": 47}, {"referenceID": 53, "context": "Finally, we extract six kernel descriptors [41] from the bounding box of each frame in the trajectory: gradient, color, local binary pattern, depth gradient, spin, surface normals, and KPCA/self-similarity, which have been proven to be useful features for RGB-D data [56].", "startOffset": 267, "endOffset": 271}, {"referenceID": 5, "context": "The topic model such as LDA [8] has been very common for document modeling from language.", "startOffset": 28, "endOffset": 31}, {"referenceID": 20, "context": "The stick-breaking notion has been widely used for constructing random weights [23], [44].", "startOffset": 79, "endOffset": 83}, {"referenceID": 41, "context": "The stick-breaking notion has been widely used for constructing random weights [23], [44].", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "Gibbs sampling is commonly used as a means of statistical inference to approximate the distributions of variables when direct sampling is difficult [7], [23].", "startOffset": 148, "endOffset": 151}, {"referenceID": 20, "context": "Gibbs sampling is commonly used as a means of statistical inference to approximate the distributions of variables when direct sampling is difficult [7], [23].", "startOffset": 153, "endOffset": 157}, {"referenceID": 12, "context": "So we instead use a Metropolis-Hastings independence sampler [15].", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "Segment the current frame to super-pixels using edge detection [12] as in Section 3; 9.", "startOffset": 63, "endOffset": 67}, {"referenceID": 54, "context": "They are Hidden Markov Model (HMM), topic model LDA (TM), correlated topic model (CTM), topic model over absolute time (TM-AT), correlated topic model over absolute time (CTM-AT), topic model over relative time (TM-RT) and our causal topic model with only action-topics (CaTM-A) [57].", "startOffset": 279, "endOffset": 283}, {"referenceID": 50, "context": "We also evaluate HMM and our model CaTM using the popular features for action recognition, dense trajectories feature (DTF) [53], extracted only in RGB videos5, named as HMM-DTF and CaTM-A-DTF, CaTMAO-DTF.", "startOffset": 124, "endOffset": 128}, {"referenceID": 36, "context": "Segmentation: we consider a true positive if the overlap (union/intersection) between the detected and the groundtruth segments is more than a default threshold 40% as in [39].", "startOffset": 171, "endOffset": 175}], "year": 2016, "abstractText": "There is a large variation in the activities that humans perform in their everyday lives. We consider modeling these composite human activities which comprises multiple basic level actions in a completely unsupervised setting. Our model learns high-level co-occurrence and temporal relations between the actions. We consider the video as a sequence of short-term action clips, which contains human-words and object-words. An activity is about a set of action-topics and object-topics indicating which actions are present and which objects are interacting with. We then propose a new probabilistic model relating the words and the topics. It allows us to model long-range action relations that commonly exist in the composite activities, which is challenging in previous works. We apply our model to the unsupervised action segmentation and clustering, and to a novel application that detects forgotten actions, which we call action patching. For evaluation, we contribute a new challenging RGB-D activity video dataset recorded by the new Kinect v2, which contains several human daily activities as compositions of multiple actions interacting with different objects. Moreover, we develop a robotic system that watches people and reminds people by applying our action patching algorithm. Our robotic setup can be easily deployed on any assistive robot.", "creator": "LaTeX with hyperref package"}}}