{"id": "1603.03713", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2016", "title": "Cost-sensitive Learning for Utility Optimization in Online Advertising Auctions", "abstract": "One of the most challenging problems in computational advertising is the prediction of ad click and conversion rates for bidding in online advertising auctions. State-of- the-art prediction methods include using the maximum entropy framework (also known as logistic regression) and log linear models. However, one unaddressed problem in the previous approaches is the existence of highly non-uniform misprediction costs. In this paper, we present our approach for making cost-sensitive predictions for bidding in online advertising auctions. We show that one can get significant lifts in offline and online performance by using a simple modification of the logistic loss function.", "histories": [["v1", "Fri, 11 Mar 2016 18:21:55 GMT  (121kb,D)", "http://arxiv.org/abs/1603.03713v1", "Presented at NIPS 2015 Workshop on E-Commerce:this https URL"], ["v2", "Mon, 19 Sep 2016 14:37:00 GMT  (52kb,D)", "http://arxiv.org/abs/1603.03713v2", "First version of the paper was presented at NIPS 2015 Workshop on E-Commerce:this https URL"], ["v3", "Wed, 12 Jul 2017 09:30:53 GMT  (866kb,D)", "http://arxiv.org/abs/1603.03713v3", "First version of the paper was presented at NIPS 2015 Workshop on E-Commerce:this https URLThird version of the paper will be presented at AdKDD 2017 Workshop: adkdd17.wixsite.com/adkddtargetad2017"]], "COMMENTS": "Presented at NIPS 2015 Workshop on E-Commerce:this https URL", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["flavian vasile", "damien lefortier", "olivier chapelle"], "accepted": false, "id": "1603.03713"}, "pdf": {"name": "1603.03713.pdf", "metadata": {"source": "CRF", "title": "Cost-sensitive Learning for Bidding in Online Advertising Auctions", "authors": ["Flavian Vasile", "Damien Lefortier"], "emails": ["d.lefortier}@criteo.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, the way we present ourselves in public, the way we present ourselves in public, the way we present ourselves in public, the way we are perceived in public, the way we are perceived in public, the way we are perceived in public, the way we are perceived in public, the way we are perceived in public, the way we are perceived in public, the way we are perceived in public, the way we are perceived in public, the way we are perceived in public, the way we are perceived in public. \""}, {"heading": "2 Cost-sensitive Logistic Regression", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Cost-sensitive learning and Risk Minimization", "text": "This year, the number of new arrivals has fallen by a third compared to previous years, and the number of new arrivals has increased many times over."}, {"heading": "2.2 Impact of weighting on learning with SGD and L-BFGS", "text": "Let us now discuss how to learn in the case of a cost-sensitive logistical regression, i.e. using WNLLCPA as a loss function, using two well-known learning algorithms: Limited Memory BFGS [13] (L-BFGS) and Stochastic Gradient Descent [14] (SGD). These two algorithms are often used to learn click and conversion models related to display advertising (see e.g. [9, 8] for SGD and [5, 15] for L-BFGS initialized with SGD). Using an advertiser constant revenue weighting scheme corresponds to the definition of an importance weight for each display (see above). In our case, the weight is the average CPA of the advertiser associated with each display."}, {"heading": "2.2.1 Learning with importance weights", "text": "Batch case: L-BFGSL-BFGS is a batch algorithm (such as, Gradient Descent), which means that a pass over the data is required before updating the weights. In this case, using a weight for each example is straightforward while learning [16]. In fact, in batch algorithms we can first calculate the sum of the gradients of all examples before updating the weights once, and then add x times the gradients of an example or add x times the gradients once when this sum is calculated. So we can simply do this, i.e. multiply the gradients by x - which is both simple and inexpensive by adding the L-BFGS code. Note that this is not an approximation. Online case: SGDOn the other hand, SGD is an online algorithm, or streaming algorithm, which means that updating the weights is performed according to each example."}, {"heading": "2.2.2 Impact on the regularization parameter", "text": "In most applications, the function of logistic regression is objectively accompanied by a regularization term for weights w (typically L1 or L2 regularization terms) to prevent overadjustment of training data. We obtain the following loss function for the regularized WNLLCPA: L = 1N N N \u2211 i (\u2212 yi log (pi) \u2212 (1 \u2212 yi) log (1 \u2212 pi))). In this thesis, we use the L2 regularization when performing a cost-sensitive logistic regression, i.e. this hyperparameter needs to be adjusted (e.g. when adding new features). To do this, we use the following simple heuristics to adjust ourselves depending on the value of the weights of importance used, i.e. the average CPA of each advertiser (such as when adding new features)."}, {"heading": "3 Experiments", "text": "In this section, we present our experimental results using our methodology to improve a state-of-the-art conversion rate prediction model, which is used as a sub-model to predict the expected number of sales or expected revenue generated by a user after viewing bids for online advertising auctions. First, we present offline experiments using a data set of more than 3 billion examples of binary (sale vs. no-sale) labels and hundreds of millions of unique attribute values from Criteo's March 2015 production click logs. Then, we present live online traffic experiments."}, {"heading": "3.1 Offline metrics", "text": "For the offline evaluation of our WNLLCPA model, we use offline metrics that approximate the business impact of the model change, namely Normalized Weighted MSE (NMSEW) and Utility (discussed in Section 1). Normalized Weighted MSE indicates the relative improvement in the MSEW of the model to be evaluated compared to a baseline predictor, in our case the average empirical CR rate of the data set, similar to the normalization in [8, 15]. We refer to it as a binary result variable that indicates whether or not a sale has occurred, xi the input display indicates a vector and ci the display costs. In order to model the potential profit change due to a forecast model change offline, the utility measurement was recently proposed in [4]. Since the observed gain is fixed in historical data, the metric assumes that the display costs are derived from a second price auction and that they are calculated according to a Gamma."}, {"heading": "3.2 Offline results", "text": "We compare the performance differences between the standard logistic regression NLL and the weighted version WNLLCPA with L-BFGS initialized with SGD [5, 15]. Considering that our cost weighting scheme reweights the negative and positive examples of each advertiser by a constant, the resulting conversion probabilities remain calibrated and we can only change the loss function in the conversion rate model while the other systems remain unchanged. For the regulation parameter \u03bb, our experiments show that we can achieve results with our heuristics (Section 2.2.2) that are comparable to the best (manually tuned) results, such as Figure 2. We see that the optimal lambda value increases performance in high CPA traffic, but reduces performance in other traffic areas (overall, the heuristic lambda is within the noise of the optimal LPA model). In our experiments, therefore, we use the most up-dated parameter to achieve a significant benefit compared to the one currently used by the regulator."}, {"heading": "3.3 Online experiments", "text": "We conducted an A / B test of the change in the loss function to WNLLCPA in the conversion rate model for both the number of sales forecasts and the total value of the sales forecast and found significant savings in display costs, coupled with an increase in sales performance (number of sales, total sales) for advertisers. Our change therefore had a significant positive impact on projected long-term revenues. In terms of development and operating costs, the change in the loss function took only a few weeks to get production up and running as the code change is minimal, as in Section 2. In addition, the observed training time of the model did not change."}, {"heading": "4 Conclusion", "text": "In this paper, we examined the impact of applying the cost-sensitive learning framework in the context of bidding in online advertising auctions. We introduced a problem-specific weighting scheme for what we call the WNLLCPA, which weights differently the expected number of sales (and associated revenues) of different advertisers based on their expected value, namely their CPA. We applied this change to a state-of-the-art predictive model for conversion rates, which is used as a sub-model to predict the expected number of sales (and expected revenues) generated by a user after an ad. Compared with the vanilla version of logistics loss, we showed major improvements in offline metrics (Normalized Weighted MSE, Utility), followed by significant impacts on business metrics in the live bidding system. In the next steps, we will continue our research in two different directions: First, we plan our current learning methodology with the other cost-sensitive approach, such as calculation."}], "references": [{"title": "Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords,", "author": ["B. Edelman", "M. Ostrovsky", "M. Schwarz"], "venue": "tech. rep., National Bureau of Economic Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Loss functions for predicted click through rates in auctions for online advertising,", "author": ["P. Hummel", "R.P. McAfee"], "venue": "Preprint, Google Inc,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Offline evaluation of response prediction in online advertising auctions,", "author": ["O. Chapelle"], "venue": "Proceedings of the 24th International Conference on World Wide Web Companion,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Simple and scalable response prediction for display advertising,", "author": ["O. Chapelle", "E. Manavoglu", "R. Rosales"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Ad click prediction: a view from the trenches,", "author": ["H.B. McMahan", "G. Holt", "D. Sculley", "M. Young", "D. Ebner", "J. Grady", "L. Nie", "T. Phillips", "E. Davydov", "D. Golovin"], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Estimating rates of rare events with multiple hierarchies through scalable log-linear models,", "author": ["D. Agarwal", "R. Agrawal", "R. Khanna", "N. Kota"], "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Practical lessons from predicting clicks on ads at facebook,", "author": ["X. He", "J. Pan", "O. Jin", "T. Xu", "B. Liu", "Y. Shi", "A. Atallah", "R. Herbrich", "S. Bowers"], "venue": "Proceedings of 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Click-through prediction for advertising in twitter timeline,", "author": ["C. Li", "Y. Lu", "Q. Mei", "D. Wang", "S. Pandey"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Cost-sensitive learning by cost-proportionate example weighting,", "author": ["B. Zadrozny", "J. Langford", "N. Abe"], "venue": "Data Mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Efficient noise-tolerant learning from statistical queries,", "author": ["M. Kearns"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "The foundations of cost-sensitive learning,", "author": ["C. Elkan"], "venue": "in International joint conference on artificial intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "On the limited memory bfgs method for large scale optimization,", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical programming,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "Sources of variability in large-scale machine learning systems,", "author": ["D. Lefortier", "A. Truchet", "M. de Rijke"], "venue": "Machine Learning Systems (NIPS 2015 Workshop),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Online importance weight aware updates,", "author": ["N. Karampatziakis", "J. Langford"], "venue": "arXiv preprint arXiv:1011.1576,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Most of the RTB platforms function using a 2nd price model [2], where advertisers or agents representing the advertisers bid for display opportunities, and the winner pays the maximum between the bid of the second participant in the auction and the reserve price.", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "To take this into account during evaluation, recently proposed metrics on bidders performance are making use of the advertisers\u2019 CPAs [3, 4].", "startOffset": 134, "endOffset": 140}, {"referenceID": 2, "context": "To take this into account during evaluation, recently proposed metrics on bidders performance are making use of the advertisers\u2019 CPAs [3, 4].", "startOffset": 134, "endOffset": 140}, {"referenceID": 2, "context": "This metric can be extended to weight the display-level squared error with the CPA of the corresponding advertiser and to therefore penalize the model proportionally with the unexplained revenue (mentioned as a special case in [4]) \u2014 thus yielding a Weighted MSE (MSEW).", "startOffset": 227, "endOffset": 230}, {"referenceID": 2, "context": "Another example is the Utility metric, recently proposed in [4], which takes into account both the potential upside of the display represented by the CPA, but also the associated display cost (modeled as a Gamma distribution over potential display costs given the observed cost).", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "Shortcomings of current approaches Current state-of-the-art response rate prediction methods range from logistic regression [5, 6], to log-linear models [7], to a combination of log-linear models with decision tress [8], and to combining pure response rate prediction with ad ranking [9].", "startOffset": 124, "endOffset": 130}, {"referenceID": 4, "context": "Shortcomings of current approaches Current state-of-the-art response rate prediction methods range from logistic regression [5, 6], to log-linear models [7], to a combination of log-linear models with decision tress [8], and to combining pure response rate prediction with ad ranking [9].", "startOffset": 124, "endOffset": 130}, {"referenceID": 5, "context": "Shortcomings of current approaches Current state-of-the-art response rate prediction methods range from logistic regression [5, 6], to log-linear models [7], to a combination of log-linear models with decision tress [8], and to combining pure response rate prediction with ad ranking [9].", "startOffset": 153, "endOffset": 156}, {"referenceID": 6, "context": "Shortcomings of current approaches Current state-of-the-art response rate prediction methods range from logistic regression [5, 6], to log-linear models [7], to a combination of log-linear models with decision tress [8], and to combining pure response rate prediction with ad ranking [9].", "startOffset": 216, "endOffset": 219}, {"referenceID": 7, "context": "Shortcomings of current approaches Current state-of-the-art response rate prediction methods range from logistic regression [5, 6], to log-linear models [7], to a combination of log-linear models with decision tress [8], and to combining pure response rate prediction with ad ranking [9].", "startOffset": 284, "endOffset": 287}, {"referenceID": 1, "context": "As a result, we focus on the following questions: Can our predictive models benefit from taking into account the sales\u2019 CPAs? How to better add this information during learning in order to improve the performance of our models? To the best of our knowledge, the only solution to this question was proposed recently in [3], where the authors design specific loss functions that take into account the bidder economic performance and which inspired the work on the Utility metric in [4].", "startOffset": 318, "endOffset": 321}, {"referenceID": 2, "context": "As a result, we focus on the following questions: Can our predictive models benefit from taking into account the sales\u2019 CPAs? How to better add this information during learning in order to improve the performance of our models? To the best of our knowledge, the only solution to this question was proposed recently in [3], where the authors design specific loss functions that take into account the bidder economic performance and which inspired the work on the Utility metric in [4].", "startOffset": 480, "endOffset": 483}, {"referenceID": 8, "context": "In [10], the authors classify the cost-sensitive learning approaches in three main classes as follows.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "The first approach makes learning cost-sensitive by adding costs to the learner \u2014 this works for any model that falls under the statistical query model [11].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "The second approach changes the final decision function and assigns each example to its lowest cost-sensitive risk [12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 8, "context": "Finally, the third approach changes directly the training dataset by duplicating each example by a factor proportional to its relative cost, which has the advantage that it works with any error minimizing classifier [10].", "startOffset": 216, "endOffset": 220}, {"referenceID": 1, "context": "1 Cost-sensitive learning and Risk Minimization As discussed in [3], current state-of-the-art models for online bidding suffer from \u201dmisspecification\u201d2, both due to omitted variables bias and due to functional form misspecification.", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "Utility) when display costs are uniform (see [4, 3]).", "startOffset": 45, "endOffset": 51}, {"referenceID": 1, "context": "Utility) when display costs are uniform (see [4, 3]).", "startOffset": 45, "endOffset": 51}, {"referenceID": 11, "context": "using WNLLCPA as loss function, using two well-known learning algorithms: limited memory BFGS [13] (L-BFGS) and stochastic gradient descent [14] (SGD).", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": ", [9, 8] for SGD, and [5, 15] for L-BFGS initialized with SGD).", "startOffset": 2, "endOffset": 8}, {"referenceID": 6, "context": ", [9, 8] for SGD, and [5, 15] for L-BFGS initialized with SGD).", "startOffset": 2, "endOffset": 8}, {"referenceID": 3, "context": ", [9, 8] for SGD, and [5, 15] for L-BFGS initialized with SGD).", "startOffset": 22, "endOffset": 29}, {"referenceID": 12, "context": ", [9, 8] for SGD, and [5, 15] for L-BFGS initialized with SGD).", "startOffset": 22, "endOffset": 29}, {"referenceID": 13, "context": "In this case, adding an importance weight of x is not equivalent to multiplying the gradient by x when doing the update as in the batch case covered above [17].", "startOffset": 155, "endOffset": 159}, {"referenceID": 13, "context": "Indeed, if we would have an example duplicated x times in the training set, we would do x independent updates, which is not equivalent with doing a single update that is x times larger [17].", "startOffset": 185, "endOffset": 189}, {"referenceID": 13, "context": "[17] also proposes an approximated way to perform importance weight aware updates without doing x updates, which is time-consuming, and better than multiplying by x.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Since this method showed no additional value, there was no benefit in exploring the method from [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "The Normalized Weighted MSE shows the relative improvement in MSEW of the model to be evaluated versus a baseline predictor, in our case the average empirical CR rate of the dataset, similar to the normalization in [8, 15].", "startOffset": 215, "endOffset": 222}, {"referenceID": 12, "context": "The Normalized Weighted MSE shows the relative improvement in MSEW of the model to be evaluated versus a baseline predictor, in our case the average empirical CR rate of the dataset, similar to the normalization in [8, 15].", "startOffset": 215, "endOffset": 222}, {"referenceID": 2, "context": "In order to model offline the potential change in profit due to a prediction model change, the Utility measure has been proposed recently in [4].", "startOffset": 141, "endOffset": 144}, {"referenceID": 3, "context": "We compare the difference in performance between the standard logistic regression NLL and the weighted version WNLLCPA using L-BFGS initialized with SGD [5, 15].", "startOffset": 153, "endOffset": 160}, {"referenceID": 12, "context": "We compare the difference in performance between the standard logistic regression NLL and the weighted version WNLLCPA using L-BFGS initialized with SGD [5, 15].", "startOffset": 153, "endOffset": 160}, {"referenceID": 8, "context": "In the next steps, we will continue our research in two different directions: first, we plan to compare our current approach with other cost-sensitive learning methods, such as the \u201ccosting\u201d approach [10].", "startOffset": 200, "endOffset": 204}], "year": 2017, "abstractText": "One of the most challenging problems in computational advertising is the prediction of ad click and conversion rates for bidding in online advertising auctions. State-ofthe-art prediction methods include using the maximum entropy framework (also known as logistic regression) and log linear models. However, one unaddressed problem in the previous approaches is the existence of highly non-uniform misprediction costs. In this paper, we present our approach for making cost-sensitive predictions for bidding in online advertising auctions. We show that one can get significant lifts in offline and online performance by using a simple modification of the logistic loss function.", "creator": "LaTeX with hyperref package"}}}