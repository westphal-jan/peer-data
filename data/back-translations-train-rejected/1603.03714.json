{"id": "1603.03714", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2016", "title": "Distribution Free Learning with Local Queries", "abstract": "The model of learning with \\emph{local membership queries} interpolates between the PAC model and the membership queries model by allowing the learner to query the label of any example that is similar to an example in the training set. This model, recently proposed and studied by Awasthi, Feldman and Kanade, aims to facilitate practical use of membership queries.", "histories": [["v1", "Fri, 11 Mar 2016 18:23:44 GMT  (20kb,D)", "http://arxiv.org/abs/1603.03714v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["galit bary-weisberg", "amit daniely", "shai shalev-shwartz"], "accepted": false, "id": "1603.03714"}, "pdf": {"name": "1603.03714.pdf", "metadata": {"source": "CRF", "title": "Distribution Free Learning with Local Queries\u2217", "authors": ["Galit Bary-Weisberg", "Amit Daniely", "Shai Shalev-Shwartz"], "emails": [], "sections": [{"heading": null, "text": "We limit ourselves to the Boolean cube {\u2212 1, 1} n and say that a query is local if it has a hammering distance \u2264 q from a training example. On the positive side, we show that 1-local queries already give an additional strength and make it possible to learn a certain type of DNF formula. On the negative side, we show that even (n0.99) local queries cannot help to learn different classes, including automatons, DNFs and more. Similarly, q-local queries for any constant q cannot help to learn juntas, decision trees, sparse polynomials and much more. Furthermore, an algorithm that uses (log0.99 (n) local queries would lead to a breakthrough in the most known runtimes for these classes."}, {"heading": "1 Introduction", "text": "A child typically learns to recognize a cat based on two types of input, the first is given by their parents by pointing at a cat and saying, \"Look, a cat!\" The second is given in response to the child's frequent question, \"What is that?\" These two types of input were the basis for the learning model originally proposed by Valiant [21]. In fact, the acronym PAC stands for the restricted model in which MQ is banned, while the full model is called PAC + MQ. Much work has been done to examine the limitations and strengths of MQ. In particular, member queries have been shown to be stronger than the vanilla PAC model [1, 10, 12, 16]. However, MQ is rarely used in practice."}, {"heading": "2 Previous Work", "text": "Several concept classes are known to be learnable only when membership requests are allowed: Deterministic finite automata [1], k-term DNF for k = log (n) log (n)))) [10], decision trees and k-almost monotonous DNF formulas [12], intersections of half-spaces [7] and DNF formulas under uniform distribution [16]. The last result is based on Freund's boosting algorithm [15] and the Fourier-based technique for learning by member requests based [18]. We note that there are cases where MQ does not help, e.g. in the case of learning DNF and CNF formulas [2], provided that one-way functions exist, and in the case of distributionless agnostic learning [14]. Local Membership Queries Awasthi et al. focus on learning with O (log (n) local queries."}, {"heading": "3 Setting", "text": "We consider a binary classification where the instance space X = Xn = {\u2212 1} n and the label space Y = {0, 1} is. A learning problem is defined by a hypothesis class H, {0, 1} X. The learner receives a training setS = {(x1, h? (x1)), (x2, h? (x2)),..., (xm, h? (xm)), where the xi's from some unknown distribution D to X and h?: X \u2192 Y is some unknown hypothesis. The learner is also allowed to ask membership questions."}, {"heading": "4 Learning DNFs with Evident Examples", "text": "Intuitively, when evaluating a DNF formula using a given example, we check a few conditions that match the conditions of the formula, and consider the example positive if one of them applies. We consider the case that for each of these conditions there is a chance of seeing a \"prototype example\" that meets these conditions in a strong or obvious way. Subsequently, we designate a DNF formula: {\u2212 1, 1} n \u2192 {0, 1} the function caused by a DNF formula F over n variables. Definition 4.1 Let F = T1, T2 do so in different ways."}, {"heading": "4.1 An algorithm", "text": "\"We are not able to find a solution.\" \"We are not able.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"\" \"\" We. \"\" \"\" \"\" \"We.\" \"\" \"\" \"We.\" \"\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"We.\" \"We..\" We. \"\" We.. \"We.\" \"We...\" We. \"\" We. \"\" We.. \"We.\" \"We..\" We. \"\" We. \"\" We. \"We.\" \"We.\" We. \"\" We. \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" We. \"\" \"We.\" \"\" We.. \"\" We.. \"We...\" We.. \"\" \"We.\" \"\" \"We..\" \"\" We.. \"\" \"We..\" \"We...\" \"\" We.. \"\" \"We..\" \"\" We.. \"\" \"We..\" \"\" We.. \"\" \"\" We.. \"We...\" We.. \"We..\" \"\" \"We.....\" We. \"We....\" \"We.\" We. \"\" \"\" We.. \"\" \"We....\" We. \"We.\" We. \"We..\" We..... \"\" We. \"We.\" \"\" \"\" \"We.\" We. \"\" We. \"\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" \"We..\" We.. \"\" We. \"We...\" We... \"We.\" We.. \"\" \"\" \"We..\" We."}, {"heading": "4.2 A matching Lower Bound", "text": "We will show that the problem of learning in this range cannot be reduced to the problem of learning in this range. (D) We will show that learning decision trees can even be reduced to the case that all positive examples evident.Theorem 4.4 students in this range with obvious examples are as hard as learning decisions in this range. (D) We denote from hT the function induced by a decision tree T. (D) The proof will use the following assertion: There is a figure (a reduction). (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D n) n (D n) n (D n) n (D n (D n) n (D n (D n) n (D n) n (D n) n (D n (D n) n (D n) n (D n (D n) n (D n n) n (D n (D n) n (D n (D n) n (D n) n (D n (D n) n (D n (D n) n (D n (D n) n (D n (D n) n (D n) n (D n (D n) n (D n) n (D n (n) n (D n n) n (D n) n (D n (D n) n (D n) n) n (n n (n (D n) n) n (n (n) n (D n) n n (n (n) n) n (D n) n (n n) n (n (D n) n (n (D n) n (n n) n (n (n n) n (D n (D n n) n (n (D n (n n n n) n (D n) n n (n) n n (D n (D n n n (D n) n n (D n) n) n n (D n n (D n (D n) n) n (D n) n (n n n n n n (D n n n n n) n) n (D n n n) n (D n"}, {"heading": "5 Lower Bounds", "text": "For A-Xn and q > 0, the reduction (A, q) = {x-xn | a-xn-xn-xn-x. We say that a mapping number: {\u2212 1, 1} n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n"}, {"heading": "6 Conclusion and Future Work", "text": "As our evidence shows, this stems from the fact that learning these classes without queries can be reduced to a case where local queries are meaningless, in the sense that the answer to them is either always 1 or the label of the nearest training example. On the other hand, the learning problem of DNF's with obvious examples bypasses this property. Indeed, the underlying assumption that local changes can change the label in an analogous way. Although this assumption may be intuitive in some cases, it is certainly very restrictive. Therefore, a natural future direction of research is to seek less restrictive assumptions that still have this property. More concrete assumptions arising from our work relate to classes for which we have shown that (log0.99 (n)) local queries are unlikely to lead to efficient algorithms."}], "references": [{"title": "Learning regular sets from queries and counterexamples", "author": ["Dana Angluin"], "venue": "Information and computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1987}, {"title": "When won t membership queries help", "author": ["Dana Angluin", "Michael Kharitonov"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Randomly fallible teachers: Learning monotone dnf with an incomplete membership oracle", "author": ["Dana Angluin", "Donna K Slonim"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Malicious omissions and errors in answers to membership queries", "author": ["Dana Angluin", "M\u0101rti\u0146\u0161 Kri\u0137is", "Robert H Sloan", "Gy\u00f6rgy Tur\u00e1n"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Learning using local membership queries", "author": ["Pranjal Awasthi", "Vitaly Feldman", "Varun Kanade"], "venue": "arXiv preprint arXiv:1211.0996,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Learning using 1-local membership queries", "author": ["Galit Bary"], "venue": "arXiv preprint arXiv:1512.00165,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Neural net algorithms that learn in polynomial time from examples and queries", "author": ["Eric B Baum"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "Query learning can work poorly when a human oracle is used", "author": ["Eric B Baum", "Kenneth Lang"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "Learning with errors in answers to membership queries", "author": ["Laurence Bisht", "Nader H Bshouty", "Lawrance Khoury"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Fast learning of k-term dnf formulas with queries", "author": ["Avrim Blum", "Steven Rudich"], "venue": "In Proceedings of the twenty-fourth annual ACM symposium on Theory of computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1992}, {"title": "Learning with unreliable boundary queries", "author": ["Avrim Blum", "Prasad Chalasani", "Sally A Goldman", "Donna K Slonim"], "venue": "In Proceedings of the eighth annual conference on Computational learning theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Exact learning boolean functions via the monotone theory", "author": ["Nader H Bshouty"], "venue": "Information and Computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "Complexity theoretic limitations on learning dnf\u2019s", "author": ["Amit Daniely", "Shai Shalev-Shwatz"], "venue": "arXiv preprint arXiv:1404.3378,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "On the power of membership queries in agnostic learning", "author": ["Vitaly Feldman"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Yoav Freund"], "venue": "Information and computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "An efficient membership-query algorithm for learning dnf with respect to the uniform distribution", "author": ["Jeffrey Jackson"], "venue": "In Foundations of Computer Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}, {"title": "Cryptographic limitations on learning boolean formulae and finite automata", "author": ["Michael Kearns", "Leslie Valiant"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Learning decision trees using the fourier spectrum", "author": ["Eyal Kushilevitz", "Yishay Mansour"], "venue": "SIAM Journal on Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "Learning with queries but incomplete information", "author": ["Robert H Sloan", "Gy\u00f6rgy Tur\u00e1n"], "venue": "In Proceedings of the seventh annual conference on Computational learning theory,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "A theory of the learnable", "author": ["Leslie G Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1984}], "referenceMentions": [{"referenceID": 4, "context": "[5], aims to facilitate practical use of membership queries.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "thesis [6] of the first author.", "startOffset": 7, "endOffset": 10}, {"referenceID": 19, "context": "These two types of input were the basis for the learning model originally suggested by Valiant [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16].", "startOffset": 82, "endOffset": 97}, {"referenceID": 9, "context": "In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16].", "startOffset": 82, "endOffset": 97}, {"referenceID": 11, "context": "In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16].", "startOffset": 82, "endOffset": 97}, {"referenceID": 15, "context": "In particular, membership queries were proven stronger than the vanilla PAC model [1, 10, 12, 16].", "startOffset": 82, "endOffset": 97}, {"referenceID": 7, "context": ", [8]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": "[5] suggested a solution to the problem of unnatural examples.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "We prove both positive and negative results in this context: \u2022 One of the strongest and most beautiful results in the MQ model shows that automata are learnable with membership queries [1].", "startOffset": 185, "endOffset": 188}, {"referenceID": 16, "context": "As learning automata is hard under several assumptions [17, 13], our result suggests that it is hard to learn automata even with (n)-local queries.", "startOffset": 55, "endOffset": 63}, {"referenceID": 12, "context": "As learning automata is hard under several assumptions [17, 13], our result suggests that it is hard to learn automata even with (n)-local queries.", "startOffset": 55, "endOffset": 63}, {"referenceID": 0, "context": "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].", "startOffset": 141, "endOffset": 144}, {"referenceID": 9, "context": "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].", "startOffset": 240, "endOffset": 244}, {"referenceID": 6, "context": "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].", "startOffset": 275, "endOffset": 278}, {"referenceID": 15, "context": "Membership Queries Several concept classes are known to be learnable only if membership queries are allowed: Deterministic Finite Automatons [1], k-term DNF for k = log(n) log(log(n)) [10], decision trees and k-almost monotone-DNF formulas [12], intersections of khalfspaces [7] and DNF formulas under the uniform distribution [16].", "startOffset": 327, "endOffset": 331}, {"referenceID": 14, "context": "The last result builds on Freund\u2019s boosting algorithm [15] and the Fourier-based technique for learning using membership queries due to [18].", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "The last result builds on Freund\u2019s boosting algorithm [15] and the Fourier-based technique for learning using membership queries due to [18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 1, "context": ", in the case of learning DNF and CNF formulas [2], assuming that one way functions exist, and in the case of distribution free agnostic learning [14].", "startOffset": 47, "endOffset": 50}, {"referenceID": 13, "context": ", in the case of learning DNF and CNF formulas [2], assuming that one way functions exist, and in the case of distribution free agnostic learning [14].", "startOffset": 146, "endOffset": 150}, {"referenceID": 2, "context": "For example, to allow \u201cI don\u2019t know\u201d answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].", "startOffset": 90, "endOffset": 107}, {"referenceID": 3, "context": "For example, to allow \u201cI don\u2019t know\u201d answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].", "startOffset": 90, "endOffset": 107}, {"referenceID": 10, "context": "For example, to allow \u201cI don\u2019t know\u201d answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].", "startOffset": 90, "endOffset": 107}, {"referenceID": 18, "context": "For example, to allow \u201cI don\u2019t know\u201d answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].", "startOffset": 90, "endOffset": 107}, {"referenceID": 8, "context": "For example, to allow \u201cI don\u2019t know\u201d answers, or to be tolerant to some incorrect answers [3, 4, 11, 20, 9].", "startOffset": 90, "endOffset": 107}], "year": 2016, "abstractText": "The model of learning with local membership queries interpolates between the PAC model and the membership queries model by allowing the learner to query the label of any example that is similar to an example in the training set. This model, recently proposed and studied by Awasthi et al. [5], aims to facilitate practical use of membership queries. We continue this line of work, proving both positive and negative results in the distribution free setting. We restrict to the boolean cube {\u22121, 1}n, and say that a query is q-local if it is of a hamming distance \u2264 q from some training example. On the positive side, we show that 1-local queries already give an additional strength, and allow to learn a certain type of DNF formulas. On the negative side, we show that even ( n0.99 ) -local queries cannot help to learn various classes including Automata, DNFs and more. Likewise, q-local queries for any constant q cannot help to learn Juntas, Decision Trees, Sparse Polynomials and more. Moreover, for these classes, an algorithm that uses ( log(n) ) -local queries would lead to a breakthrough in the best known running times. \u2217This paper is based on the M.Sc. thesis [6] of the first author. The thesis offers a more elaborated discussion, as well as experiments. \u2020Matific inc. Most work was done while the author was an M.Sc. student at the Hebrew University, Jerusalem, Israel \u2021Google inc. Most work was done while the author was a Ph.D. student at the Hebrew University, Jerusalem, Israel \u00a7School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel ar X iv :1 60 3. 03 71 4v 1 [ cs .L G ] 1 1 M ar 2 01 6", "creator": "LaTeX with hyperref package"}}}