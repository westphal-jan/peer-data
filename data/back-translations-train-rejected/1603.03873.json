{"id": "1603.03873", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2016", "title": "Neural Discourse Relation Recognition with Semantic Memory", "abstract": "Humans comprehend the meanings and relations of discourses heavily relying on their semantic memory that encodes general knowledge about concepts and facts. Inspired by this, we propose a neural recognizer for implicit discourse relation analysis, which builds upon a semantic memory that stores knowledge in a distributed fashion. We refer to this recognizer as SeMDER. Starting from word embeddings of discourse arguments, SeMDER employs a shallow encoder to generate a distributed surface representation for a discourse. A semantic encoder with attention to the semantic memory matrix is further established over surface representations. It is able to retrieve a deep semantic meaning representation for the discourse from the memory. Using the surface and semantic representations as input, SeMDER finally predicts implicit discourse relations via a neural recognizer. Experiments on the benchmark data set show that SeMDER benefits from the semantic memory and achieves substantial improvements of 2.56\\% on average over current state-of-the-art baselines in terms of F1-score.", "histories": [["v1", "Sat, 12 Mar 2016 08:54:16 GMT  (191kb,D)", "http://arxiv.org/abs/1603.03873v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["biao zhang", "deyi xiong", "jinsong su"], "accepted": false, "id": "1603.03873"}, "pdf": {"name": "1603.03873.pdf", "metadata": {"source": "CRF", "title": "Neural Discourse Relation Recognition with Semantic Memory", "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su"], "emails": ["zb@stu.xmu.edu.cn,", "jssu@xmu.edu.cn", "dyxiong@suda.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year, the time has come for us to be able to live in a country where people are able to flourish and where they are able to flourish in order to flourish."}, {"heading": "2 Related Work", "text": "A variety of strategies in the field of machine learning have been presented in the past, including feature engineering, connectivist predictions, data selection, and discourse representation using neural networks. In this sense, it is also possible to deal with other issues, such as whether and in what context people are able to identify themselves. In this context, contextual words, word pairs, and parseur trees are used for feature engineering. Later, several more powerful features were developed: aggregated word pairs that identify with others, such as polarity, verb classes, contexts, contexts, and lessons."}, {"heading": "3 The SeMDER Model", "text": "In this section we will explain the proposed SeMDER model in more detail. We will first introduce the flat encoder, which transforms a discourse into a distributed embedding, then describe the semantic encoder, in which the semantic memory is incorporated via an attention model, and then explain how the neural recognition mechanism classifies discourse relationships. In this section we will also discuss the objective function and method of parameter learning."}, {"heading": "3.1 Shallow Encoder", "text": "To obtain surface representations for discourses, we use a flat Convolutionary Neural Network (SCNN) = three major discourse operations (Zhang et al., 2015) as our flat encoder. SCNN is specifically tailored to the PDTB corpus, where implicit discourse relationships between two adjacent arguments are commented on, namely Arg1 and Arg2 (see the example in Figure 1). Faced with an argument consisting of n words, SCNN represents each word as a d-dimensional dense, real vector xi-Rd1 and concatenates them into a word embedding matrix: X = (x1, x2,.., xn), where X-Rd1 \u00b7 n forms the input layer of SCNN. All word vectors in vocabulary V are represented in a parameter matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix."}, {"heading": "3.2 Semantic Encoder", "text": "We assume that the semantic and syntactic attributes of words have already been encoded in this matrix. Therefore, the inclusion of this memory information in discourse representations is beneficial for implicit DRR task.Figure 2 gives an illustration of the procedure for the inclusion of semantic memory. Specifically, given the surface representation p for a discourse and the semantic memory matrix M, we stack an attention layer to project it onto the same space."}, {"heading": "3.3 Neural Recognizer", "text": "To detect the discourse relationship, we put a softmax layer on these two representations: yp = h (Wr, pp + Wr, kpk + br) (13), where h (\u00b7) is the softmax function, Wr, p-Rl-6d1, Wr, k-Rl-d2, and br-Rl are the parameter matrices and the bias term, respectively, and l specifies the number of discourse relations."}, {"heading": "3.4 Objective Function and Parameter Learning", "text": "In view of a training corpus containing T instances {(x, y)} Tt = 1, we use the following cross-entropy error to access how well the predicted relationship yp represents the gold relationship y, E (yp, y) = \u2212 l-Jyj \u00b7 log (yp, j) (14) Therefore, the common training objective of SeMDER is defined as follows: J (\u03b8) = 1T T-Jyp = 1 E (y (t) p, y (t)))) + R (15), where R-Jyp is the regularization term in relation to inequality. In terms of parameters, we will divide them into three different sentences: \u2022 JavL: word embedding matrix L; \u2022 Java: discourse relationship recognition parameter Wr, p, Wr, k and br is the regularization term in relation to inequality."}, {"heading": "4 Experiments", "text": "In this section, we conducted a series of experiments on the implicit DRR task in English. We begin with a brief review of the PDTB dataset and then describe our experimental setup. Finally, we present the experimental results and provide an in-depth analysis of attention.1Bias terms b are not regulated in practice.2https: / / code.google.com / p / word2vec / 3http: / / www.chokkan.org / software / liblbfgs /"}, {"heading": "4.1 Dataset", "text": "We used PDTB 2.0 corpus4 [Prasad et al., 2008] (PDTB thereafter), which is the largest hand-annotated discourse corpus. Discourse relations are commented on in a predictor view in PDTB, where each discourse connector is treated as a predicate that uses two passages of text as arguments. Relation labels in PDTB are arranged in a three-tiered hierarchy, with the top level consisting of four major semantic classes: TEMPORAL (TEM), CONTINGENCY (CON), EXPANSION (EXP), and COMPARISON (COM). Since the relationships at the top level are general enough to be commented on with a high intercommentary agreement, and are common in most discourse theories, we only use this level of annotations in our experiments."}, {"heading": "4.2 Setup", "text": "We chose the GoogleNews vectors-negative3005 as our external semantic memory, which contains 300-dimensional vectors (i.e. d2 = 300) for 3 million words and phrases. They are trained on a part of the Google News dataset (approximately 100 billion words). The large reach and newswire domain of its training corpus, as well as the syntactic property of word2vec models, make this vector a good choice for semantic memory. We remove all datasets with Stanford NLP Toolkit6 and employed a large-scale unlabeled data7 including 1.02M4http: / / www.seas.upenn.upenndata data data _ edb / 5https: / drive.google.com / file / 0B7XkCwpI5KDYNlNlSS 21pQmM / edit? pref = 6http: / / nl.English / lable.data data _ implicit.implicitly.4QmM / edit _ implicitly _ implicit.domain 4QmM / edit _ implicitly _ implicitly _ implicitly.domain _ implicitly _ implicitly 4QmM / edit _ implicitly _ implicitly _ implicitly.domain _ implicitly _ implicitly _.domain _ implicitly _ we _ implied 4QmM / edit _ implicitly _ implicitly 4QmL _ implicitly _ implicitly.domain 6http: / / / / nl.english _ implicitly _ implicitly _ implicitly _ implicitly _ implicitly _.domain _ implicitly _ implicitly _ we _ implicitly 4QmM / e."}, {"heading": "4.3 Classification Results", "text": "The performance of the various models is shown in Table 2, which overall shows that SeMDER exceeds the two baselines and achieves improvements in the F1 score from 1.14% to COM, 1.66% to CON, 1.36% to EXP, and 5.62% to TEM over the best baselines. We also note that the improvements are mainly due to high precision in COM, CON, and TEM, while there is a high level of recall at EXP. This is reasonable as the EXP relationship has the largest number of instances in our data. As a neural baseline, SCNN outperforms SVM at CON, EXP, and TEM, but fails at COM. However, the semantic memory SeMDER consistently outperforms SVM and SCNN in all discourse relationships, suggesting that the included semantic memory is helpful in detecting the correct discourse relationships."}, {"heading": "4.4 Attention Analysis", "text": "We would like to know more about the role of semantic memory in our model, especially what the model learns from this semantic memory. Analysis of semantic representations is relatively meaningless, so we turn to words with high attention weights for the answer. We present an example per discourse relationship from the test in Table 3, in which words with the top 5 attention weights are listed separately. Let's take the example of COM: Our model retrieves the words \"wrong, people, dead, think, chimney,\" which roughly reflect the meaning of the discourse that people think chimney is dead, wrong. Obviously, these words are crucial for understanding discourse. These examples show that SeMDER prefers to retrieve from the semantic memory relationship-relevant words that strongly refer to the corresponding relationships that we believe are the main reason for the success of SeMDER."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we have introduced a neural memory with a distributed semantic memory for implicit DRR. Semantic memory encodes semantic knowledge of words in discourse arguments and helps to decipher and understand them. We use an attention model to retrieve information relevant to discourse into semantic representations of discourses that to some extent simulate the human cognitive process. Experimental results show that our model outperforms several strong baselines, and further analyses show that our model can actually recognize some relevant words. In the future, we would like to use different types of semantic memory, such as distributed memory to ontological concepts and relationships. We also want to examine different attention architectures, such as the concept and point in [Luong et al., 2015]. Furthermore, we are interested in adapting our model to other similar classification tasks, such as the classification of film feelings, film ratings, and language inference."}], "references": [{"title": "and Yoshua Bengio", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho"], "venue": "Neural machine translation by jointly learning to align and translate.", "citeRegEx": "Bahdanau et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 1694\u20131705", "author": ["Chlo\u00e9 Braud", "Pascal Denis. Combining natural", "artificial examples to improve implicit discourse relation identification. In Proc. of COLING"], "venue": "August", "citeRegEx": "Braud and Denis. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Data & Knowledge Engineering", "author": ["Philipp Cimiano", "Uwe Reyle", "Jasmin \u0160ari\u0107. Ontology-driven discourse analysis for information extraction"], "venue": "55:59\u201383,", "citeRegEx": "Cimiano et al.. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "pages 89\u201393", "author": ["Robert Fisher", "Reid Simmons. Spectral semi-supervised discourse relation classification. In Proc. of ACL-IJCNLP"], "venue": "July", "citeRegEx": "Fisher and Simmons. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Llu\u0131\u0301s M\u00e0rquez", "author": ["Francisco Guzm\u00e1n", "Shafiq Joty"], "venue": "and Preslav Nakov. Using discourse structure improves machine translation evaluation. In Proc. of ACL, pages 687\u2013698, June", "citeRegEx": "Guzm\u00e1n et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proc", "author": ["Hugo Hernault", "Danushka Bollegala", "Mitsuru Ishizuka. A Semi-Supervised Approach to Improve Classification of Infrequent Discourse Relations Using Feature Vector Extension"], "venue": "of EMNLP,", "citeRegEx": "Hernault et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "of CIKM", "author": ["Yu Hong", "Xiaopei Zhou", "Tingting Che", "Jianmin Yao", "Qiaoming Zhu", "Guodong Zhou. Cross-argument inference for implicit discourse relation recognition. In Proc"], "venue": "pages 295\u2013304,", "citeRegEx": "Hong et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "pages 977\u2013986", "author": ["Peter Jansen", "Mihai Surdeanu", "Peter Clark. Discourse complements lexical semantics for non-factoid answer reranking. In Proc. of ACL"], "venue": "June", "citeRegEx": "Jansen et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "One vector is not enough: Entity-augmented distributed semantics for discourse relations", "author": ["Yangfeng Ji", "Jacob Eisenstein"], "venue": "TACL, pages 329\u2013344,", "citeRegEx": "Ji and Eisenstein. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Closing the gap: Domain adaptation from explicit to implicit discourse relations", "author": ["Yangfeng Ji", "Gongbo Zhang", "Jacob Eisenstein"], "venue": "Proc. of EMNLP, pages 2219\u20132224, September", "citeRegEx": "Ji et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "CoRR,", "citeRegEx": "Kumar et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 476\u2013 485", "author": ["Man Lan", "Yu Xu", "Zhengyu Niu. Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition. In Proc. of ACL"], "venue": "Sofia, Bulgaria, August", "citeRegEx": "Lan et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "of EMNLP", "author": ["Ziheng Lin", "Min-Yen Kan", "Hwee Tou Ng. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proc"], "venue": "pages 343\u2013351,", "citeRegEx": "Lin et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Proc", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning. Effective approaches to attention-based neural machine translation"], "venue": "of EMNLP,", "citeRegEx": "Luong et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "of ACL", "author": ["Kathleen McKeown", "Or Biran. Aggregated word pair features for implicit discourse relation disambiguation. In Proc"], "venue": "pages 69\u201373,", "citeRegEx": "McKeown and Biran. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proc", "author": ["Eleni Miltsakaki", "Nikhil Dinesh", "Rashmi Prasad", "Aravind Joshi", "Bonnie Webber. Experiments on sense annotations", "sense disambiguation of discourse connectives"], "venue": "of TLT2005,", "citeRegEx": "Miltsakaki et al.. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "and koray kavukcuoglu", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "Recurrent models of visual attention. In Proc. of NIPS, pages 2204\u20132212.", "citeRegEx": "Mnih et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 108\u2013112", "author": ["Joonsuk Park", "Claire Cardie. Improving Implicit Discourse Relation Recognition Through Feature Set Optimization. In Proc. of SIGDIAL"], "venue": "Seoul, South Korea, July", "citeRegEx": "Park and Cardie. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "of EMNLP", "author": ["Gary Patterson", "Andrew Kehler. Predicting the presence of discourse connectives. In Proc"], "venue": "pages 914\u2013923,", "citeRegEx": "Patterson and Kehler. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Technical Reports (CIS)", "author": ["Emily Pitler", "Mridhula Raghupathy", "Hena Mehta", "Ani Nenkova", "Alan Lee", "Aravind K Joshi. Easily identifiable discourse relations"], "venue": "page 884,", "citeRegEx": "Pitler et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "pages 683\u2013691", "author": ["Emily Pitler", "Annie Louis", "Ani Nenkova. Automatic sense prediction for implicit discourse relations in text. In Proc. of ACL-AFNLP"], "venue": "August", "citeRegEx": "Pitler et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Bonnie L Webber", "author": ["Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind K Joshi"], "venue": "The penn discourse treebank 2.0. In LREC. Citeseer,", "citeRegEx": "Prasad et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "pages 645\u2013654", "author": ["Attapol Rutherford", "Nianwen Xue. Discovering implicit discourse relations through brown cluster pair representation", "coreference patterns. In Proc. of EACL"], "venue": "April", "citeRegEx": "Rutherford and Xue. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 799\u2013808", "author": ["Attapol Rutherford", "Nianwen Xue. Improving the inference of implicit discourse relations via classifying explicit discourse connectives. In Proc. of NAACL-HLT"], "venue": "May\u2013June", "citeRegEx": "Rutherford and Xue. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "arthur szlam", "author": ["Sainbayar Sukhbaatar"], "venue": "Jason Weston, and Rob Fergus. End-to-end memory networks. In Proc. of NIPS, pages 2431\u20132439.", "citeRegEx": "Sukhbaatar et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "editors", "author": ["Endel Tulving. Episodic", "semantic memory. In Endel Tulving", "W. Donaldson"], "venue": "Organization of Memory, pages 381\u2013403. Academic Press, New York,", "citeRegEx": "Tulving. 1972", "shortCiteRegEx": null, "year": 1972}, {"title": "of COLING", "author": ["Xun Wang", "Sujian Li", "Jiwei Li", "Wenjie Li. Implicit discourse relation recognition by selecting typical training examples. In Proc"], "venue": "pages 2757\u20132772,", "citeRegEx": "Wang et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "CoRR", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes. Memory networks"], "venue": "abs/1410.3916,", "citeRegEx": "Weston et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio. Show"], "venue": "Proc. of ICML, pages 2048\u20132057,", "citeRegEx": "Xu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Sharon L Thompson-Schill", "author": ["Eiling Yee", "Evangelia G Chrysikou"], "venue": "The cognitive neuroscience of semantic memory,", "citeRegEx": "Yee et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 1834\u20131839", "author": ["Yasuhisa Yoshida", "Jun Suzuki", "Tsutomu Hirao", "Masaaki Nagata. Dependency-based discourse parser for single-document summarization. In Proc. of EMNLP"], "venue": "October", "citeRegEx": "Yoshida et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "of EMNLP", "author": ["Biao Zhang", "Jinsong Su", "Deyi Xiong", "Yaojie Lu", "Hong Duan", "Junfeng Yao. Shallow convolutional neural network for implicit discourse relation recognition. In Proc"], "venue": "September", "citeRegEx": "Zhang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "of COLING", "author": ["Zhi-Min Zhou", "Yu Xu", "Zheng-Yu Niu", "Man Lan", "Jian Su", "Chew Lim Tan. Predicting discourse connectives for implicit discourse relation recognition. In Proc"], "venue": "pages 1507\u20131514,", "citeRegEx": "Zhou et al.. 2010", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 30, "context": "It is relevant to a variety of nature language processing tasks such as summarization [Yoshida et al., 2014], machine translation [Guzm\u00e1n et al.", "startOffset": 86, "endOffset": 108}, {"referenceID": 4, "context": ", 2014], machine translation [Guzm\u00e1n et al., 2014], question answering [Jansen et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 7, "context": ", 2014], question answering [Jansen et al., 2014] and information extraction [Cimiano et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 2, "context": ", 2014] and information extraction [Cimiano et al., 2005].", "startOffset": 35, "endOffset": 57}, {"referenceID": 15, "context": "Although explicit DRR has recently achieved remarkable success [Miltsakaki et al., 2005; Pitler et al., 2008], implicit DRR still remains a serious challenge due to the absence of discourse connectives.", "startOffset": 63, "endOffset": 109}, {"referenceID": 19, "context": "Although explicit DRR has recently achieved remarkable success [Miltsakaki et al., 2005; Pitler et al., 2008], implicit DRR still remains a serious challenge due to the absence of discourse connectives.", "startOffset": 63, "endOffset": 109}, {"referenceID": 29, "context": "Inspired by the semantic memory in cognitive neuroscience [Yee et al., 2014] as well as memory network [Weston et al.", "startOffset": 58, "endOffset": 76}, {"referenceID": 27, "context": ", 2014] as well as memory network [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] and attentional mechanisms [Mnih et al.", "startOffset": 34, "endOffset": 100}, {"referenceID": 24, "context": ", 2014] as well as memory network [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] and attentional mechanisms [Mnih et al.", "startOffset": 34, "endOffset": 100}, {"referenceID": 10, "context": ", 2014] as well as memory network [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] and attentional mechanisms [Mnih et al.", "startOffset": 34, "endOffset": 100}, {"referenceID": 16, "context": ", 2015] and attentional mechanisms [Mnih et al., 2014; Bahdanau et al., 2014; Xu et al., 2015], we propose a neural network with sear X iv :1 60 3.", "startOffset": 35, "endOffset": 94}, {"referenceID": 0, "context": ", 2015] and attentional mechanisms [Mnih et al., 2014; Bahdanau et al., 2014; Xu et al., 2015], we propose a neural network with sear X iv :1 60 3.", "startOffset": 35, "endOffset": 94}, {"referenceID": 28, "context": ", 2015] and attentional mechanisms [Mnih et al., 2014; Bahdanau et al., 2014; Xu et al., 2015], we propose a neural network with sear X iv :1 60 3.", "startOffset": 35, "endOffset": 94}, {"referenceID": 31, "context": "\u2022 Shallow encoder: we feed word embeddings of discourse arguments into a shallow encoder [Zhang et al., 2015] to obtain shallow representations of arguments.", "startOffset": 89, "endOffset": 109}, {"referenceID": 21, "context": "The release of Penn Discourse Treebank (PDTB) [Prasad et al., 2008] opens the door to machine learning based implicit DRR.", "startOffset": 46, "endOffset": 67}, {"referenceID": 14, "context": "Later, several more powerful features have been developed: aggregated word pairs [McKeown and Biran, 2013], Brown clusters and coreference patterns [Rutherford and Xue, 2014].", "startOffset": 81, "endOffset": 106}, {"referenceID": 22, "context": "Later, several more powerful features have been developed: aggregated word pairs [McKeown and Biran, 2013], Brown clusters and coreference patterns [Rutherford and Xue, 2014].", "startOffset": 148, "endOffset": 174}, {"referenceID": 26, "context": "Different data selection methods for implicit DRR can be classified into the following categories: instance typicality [Wang et al., 2012], multi-task learning [Lan et al.", "startOffset": 119, "endOffset": 138}, {"referenceID": 11, "context": ", 2012], multi-task learning [Lan et al., 2013], domain adaptation [Braud and Denis, 2014; Ji et al.", "startOffset": 29, "endOffset": 47}, {"referenceID": 1, "context": ", 2013], domain adaptation [Braud and Denis, 2014; Ji et al., 2015], semi-supervised learning [Hernault et al.", "startOffset": 27, "endOffset": 67}, {"referenceID": 9, "context": ", 2013], domain adaptation [Braud and Denis, 2014; Ji et al., 2015], semi-supervised learning [Hernault et al.", "startOffset": 27, "endOffset": 67}, {"referenceID": 5, "context": ", 2015], semi-supervised learning [Hernault et al., 2010; Fisher and Simmons, 2015] and explicit discourse connective classification [Rutherford and Xue, 2015].", "startOffset": 34, "endOffset": 83}, {"referenceID": 3, "context": ", 2015], semi-supervised learning [Hernault et al., 2010; Fisher and Simmons, 2015] and explicit discourse connective classification [Rutherford and Xue, 2015].", "startOffset": 34, "endOffset": 83}, {"referenceID": 23, "context": ", 2010; Fisher and Simmons, 2015] and explicit discourse connective classification [Rutherford and Xue, 2015].", "startOffset": 83, "endOffset": 109}, {"referenceID": 8, "context": "Specifically, two different neural network models have been developed for implicit DRR: recursive neural network for entity-augmented distributed semantics [Ji and Eisenstein, 2015] and shallow convolutional neural network for discourse representation [Zhang et al.", "startOffset": 156, "endOffset": 181}, {"referenceID": 31, "context": "Specifically, two different neural network models have been developed for implicit DRR: recursive neural network for entity-augmented distributed semantics [Ji and Eisenstein, 2015] and shallow convolutional neural network for discourse representation [Zhang et al., 2015].", "startOffset": 252, "endOffset": 272}, {"referenceID": 31, "context": "Instead, our proposed model does not rely on any linguistic resources and incorporates a semantic memory to obtain deep semantic representations over shallow representations in [Zhang et al., 2015].", "startOffset": 177, "endOffset": 197}, {"referenceID": 27, "context": "In order to explore semantic memory in neural networks, we borrow ideas from recently introduced memory networks [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] to organize semantic memory as a distributed matrix and use an attention model to retrieve this distributed memory.", "startOffset": 113, "endOffset": 179}, {"referenceID": 24, "context": "In order to explore semantic memory in neural networks, we borrow ideas from recently introduced memory networks [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] to organize semantic memory as a distributed matrix and use an attention model to retrieve this distributed memory.", "startOffset": 113, "endOffset": 179}, {"referenceID": 10, "context": "In order to explore semantic memory in neural networks, we borrow ideas from recently introduced memory networks [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] to organize semantic memory as a distributed matrix and use an attention model to retrieve this distributed memory.", "startOffset": 113, "endOffset": 179}, {"referenceID": 31, "context": "To obtain surface representations for discourses, we employ a shallow convolutional neural network (SCNN) [Zhang et al., 2015] as our shallow encoder.", "startOffset": 106, "endOffset": 126}, {"referenceID": 13, "context": "As discussed in [Luong et al., 2015], a general scoring function is much better for the local attention.", "startOffset": 16, "endOffset": 36}, {"referenceID": 21, "context": "0 corpus4 [Prasad et al., 2008] (PDTB thereafter), which is the largest hand-annotated discourse corpus.", "startOffset": 10, "endOffset": 31}, {"referenceID": 20, "context": "Following previous work [Pitler et al., 2009; Zhou et al., 2010; Lan et al., 2013; Zhang et al., 2015], we used sections 2-20 as our training set, sections 21-22 as the test set.", "startOffset": 24, "endOffset": 102}, {"referenceID": 32, "context": "Following previous work [Pitler et al., 2009; Zhou et al., 2010; Lan et al., 2013; Zhang et al., 2015], we used sections 2-20 as our training set, sections 21-22 as the test set.", "startOffset": 24, "endOffset": 102}, {"referenceID": 11, "context": "Following previous work [Pitler et al., 2009; Zhou et al., 2010; Lan et al., 2013; Zhang et al., 2015], we used sections 2-20 as our training set, sections 21-22 as the test set.", "startOffset": 24, "endOffset": 102}, {"referenceID": 31, "context": "Following previous work [Pitler et al., 2009; Zhou et al., 2010; Lan et al., 2013; Zhang et al., 2015], we used sections 2-20 as our training set, sections 21-22 as the test set.", "startOffset": 24, "endOffset": 102}, {"referenceID": 31, "context": "We optimized the hyperparameters d1, \u03bbL, \u03bbR, \u03bbM according to previous work [Zhang et al., 2015] and preliminary experiments on the development set.", "startOffset": 75, "endOffset": 95}, {"referenceID": 22, "context": "Features used in SVM experiments are taken from the stateof-the-art implicit discourse relation recognition model, including Bag of Words, Cross-Argument Word Pairs, Polarity, First-Last, First3, Production Rules, Dependency Rules and Brown cluster pair [Rutherford and Xue, 2014].", "startOffset": 254, "endOffset": 280}, {"referenceID": 13, "context": "the concat and dot in [Luong et al., 2015].", "startOffset": 22, "endOffset": 42}], "year": 2016, "abstractText": "Humans comprehend the meanings and relations of discourses heavily relying on their semantic memory that encodes general knowledge about concepts and facts. Inspired by this, we propose a neural recognizer for implicit discourse relation analysis, which builds upon a semantic memory that stores knowledge in a distributed fashion. We refer to this recognizer as SeMDER. Starting from word embeddings of discourse arguments, SeMDER employs a shallow encoder to generate a distributed surface representation for a discourse. A semantic encoder with attention to the semantic memory matrix is further established over surface representations. It is able to retrieve a deep semantic meaning representation for the discourse from the memory. Using the surface and semantic representations as input, SeMDER finally predicts implicit discourse relations via a neural recognizer. Experiments on the benchmark data set show that SeMDER benefits from the semantic memory and achieves substantial improvements of 2.56% on average over current state-of-the-art baselines in terms of F1-score.", "creator": "LaTeX with hyperref package"}}}