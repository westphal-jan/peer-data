{"id": "1603.04186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2016", "title": "Visual Concept Recognition and Localization via Iterative Introspection", "abstract": "Convolutional neural networks have been shown to develop internal representations, which correspond closely to semantically meaningful objects and parts, although trained solely on class labels. Class Activation Mapping (CAM) is a recent method that makes it possible to easily highlight the image regions contributing to a network's classification decision. We build upon these two developments to enable a network to re-examine informative image regions, which we term introspection. We propose a weakly-supervised iterative scheme, which shifts its center of attention to increasingly discriminative regions as it progresses, by alternating stages of classification and introspection. We evaluate our method and show its effectiveness over a range of several datasets, obtaining a top-1 accuracy 84.48% CUB-200-2011, which is the highest to-date without using external data or stronger supervision. On Stanford-40 Actions, we set a new state-of the art of 87.89%, and on FGVC-Aircraft and the Stanford Dogs dataset, we show consistent improvements over baselines, some of which include significantly more supervision.", "histories": [["v1", "Mon, 14 Mar 2016 10:18:03 GMT  (669kb,D)", "http://arxiv.org/abs/1603.04186v1", null], ["v2", "Wed, 25 May 2016 13:27:37 GMT  (3273kb,D)", "http://arxiv.org/abs/1603.04186v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["amir rosenfeld", "shimon ullman"], "accepted": false, "id": "1603.04186"}, "pdf": {"name": "1603.04186.pdf", "metadata": {"source": "META", "title": "Visual Concept Recognition and Localization via Iterative Introspection", "authors": ["Amir Rosenfeld", "Shimon Ullman"], "emails": ["amir.rosenfeld@weizmann.ac.il", "shimon.ullman@weizmann.ac.il"], "sections": [{"heading": null, "text": "Keywords: Classification, Introspection, Attention, Semi-Supervised Learning"}, {"heading": "1 Introduction", "text": "With the emergence of deep revolutionary neural networks as the leading method in computer vision, several attempts have been made to understand their inner workings. Examples of pioneering work in this direction are [1,2]; they provide insights into the representations learned from the intermediate levels in the network. Specifically, the recent work of Zhou et al. [3] provides an elegant mechanism to highlight the discriminatory image regions that served CNN for a specific task, which can be considered a form of introspection that highlights the source of the network's conclusions. A useful feature we have observed in experiments is that even if the final classification is incorrect, the highlighted image regions are still informative in terms of the correct target class, probably due to the similar occurrence of confused classes."}, {"heading": "2 Related Work", "text": "Controlled methods consistently outperform unsupervised or semi-supervised methods because they allow for the incorporation of prior knowledge into the learning process. There is a trade-off between more accurate classification results and structured results on the one hand and the cost of labor-intensive manual annotations on the other. Some examples are [8,9] where boundary fields and partial annotations on migration time are given. Apart from the resources required for large-scale annotations, such methods elude the question of learning from poorly supervised data (and largely unsupervised data), as is known to be the case with human infants who can learn from limited examples [10]. Below are some lines of work related to the proposed method."}, {"heading": "2.1 Neural Net Visualization and Inversion", "text": "Zeiler et al. [1] have found patterns that activate hidden units via deconvolutionary neural networks, and they also investigate the localization capability of a CNN by observing the change in classification when different image areas are hidden. [2] Solves an optimization problem that aims to produce an image whose properties resemble a target image previously regulated by a natural image. Zhou et al. [11] aims to explicitly identify which image fields activate hidden network units, and finds that many of them actually correspond to semantic concepts and object parts. These visualizations suggest that, despite the training, there is much to be used exclusively with image labels within the internal representations learned by the network, and that the resulting representations can be used for weakly supervised localization and other fine-grained tasks."}, {"heading": "2.2 Semi-Supervised class Localization", "text": "Some recent work attempts to achieve object localization through weak labels, i.e. the network is trained on image-level class labels, but it also learns localization. [12] Locates image regions that belong to the target class by hiding subimages and checking changes when activating the network. Oquab et al. [13] use global max pooling to obtain points on the target objects. Recently, Zhou et al. [3] used global average pooling (GAP) to generate class-activation mapping (CAM) to visualize discriminatory image regions and enable the localization of recognized concepts. Our introspection mechanism uses their CAMs to iteratively identify discriminating regions and uses them to improve classification without additional monitoring."}, {"heading": "2.3 Attention Based Mechanisms", "text": "Recently, some attention-based mechanisms have been proposed that allow to focus on relevant image regions, either for the task of better classification [14] or efficient object localization [15]. Such methods benefit from the recent fusion between the areas of deep learning and enhanced learning [16]. Another interesting method is the space-transformer networks in [17]: they have designed a network that learns spatial distortions and applies them to the characteristic maps, effectively aligning the inputs, resulting in increased robustness to geometric transformations. This enables a fine-grained categorization on the CUB-200-2011 birds [4] datasets, altering the image so that only discriminating parts are taken into account (bird's head, body). Further work appears in [18] that detects and groups discriminatory spots to produce partial identifiers, the detection of which is combined with the detected spots for a definitive classification of two networks [19], with the use of which is then combined by two networks [19]."}, {"heading": "3 Approach", "text": "Our approach consists of alternating between two main steps: classification and introspection. In the classification step, we apply a trained network to the animation region (possibly the entire image); in the introspection step, we use the output of a hidden layer in the network whose values were specified during the classification step; this highlights image regions that are fed into the next step of iteration; this process is iterated a few times (typically 4, see Section 4, Fig. 5); and finally, the results of all stages are combined; the training proceeds by learning a specialized classifier for each iteration as different iterations capture different contexts and levels of detail (but without additional monitoring); both classification / introspection steps utilize the most recent class activation mapping method of [3]; we briefly review the CAM method and then describe how we build on it."}, {"heading": "3.1 Iterative Classification-Introspection", "text": "The proposed method alternates classification and introspection. Here, we provide the outline of the method, with specific details such as the values of the parameters discussed in Section 4.6. For a particular image I and window w (initially the entire image), a learned classifier is applied to the GAP characteristics extracted from the window Iw, resulting in C classification values Sc, c and the corresponding CAMs Mwc (x, y). The introspection phase uses a strategy to select a sub-window for the next step, applying a bar search to a series of purported sub-windows. The order of the windows visited by the method is a route on an exploration tree, from the root to one of the leaves. Each node represents an image window and the root is the entire image. Next, we explain how the sub-windows are created and how the search is appended. We arrange the current classification Sategc by the ascending subclasses w and K (we keep the corresponding order of the subclasses w and K)."}, {"heading": "3.2 Feature Aggregation", "text": "Let k be the number of windows generated by the iteration t > 0. We denote the window set by: Wt = (w t i) k i = 1 (7) and the entire window set as: R = (Wt) Tt = 0 (8), where W0 is the entire image. For each window w t t t t t t i we extract the characteristics f t w-RK, e.g. K = 1024, the dimension of the CAP characteristics, and the classification values Swti + R C. The window set R for an image I is arranged as a node in the exploration tree. The final prediction is the result of aggregating evidence from selected partial windows along a path from the root to a tree leaf. We evaluate variants of both early fusion (combination of characteristics from different iterations) and later fusion (combination of predictions from different iterations)."}, {"heading": "3.3 Training", "text": "The training consists of two main phases: the first is to train a sequence of classifiers that generate an exploration tree for each training / test sample; the second is to train feature aggregations along different routes in the exploration trees to produce a final model; during the training, we train a classifier for each iteration (for a predefined number of iterations, a total of 5) of the introspection / classification sequence; the automatic training of multiple classifiers on different scales directly contributes to the success of the method, as the same classifier for all iterations does not provide improvement over baseline results (Section 4.1); for the first iteration, we train entire images with the soil truth class labels; for each iteration t > 1, we place the training samples on the sub-windows of the sub-windows."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup:", "text": "In all our experiments, we start with a variant of the VGG-16 network [22], which was tuned to ILSVRC by [3]. We chose it via GoogLeNet-GAP because it got slightly higher classification results on the ILSVRC validation set. In this network, all layers were removed according to conven5-3, including the subsequent pooling layer, so the spatial resolution of the resulting feature maps / CAM was added by 14 \u00d7 14 for an input of size 224 \u00d7 224 (leaving the pooling layer would reduce the resolution to 7 \u00d7 7). A layer of 1024 filters was added, followed by a fully connected layer to predict classes. This is our basic GAPnetwork, called VGG-GAP. Each image window, including the entire original image, is so large that its smaller dimension is reduced to U24 pixels, resulting in functionality of 14 \u00d7 n 1024."}, {"heading": "4.2 Correlation of Class and Localization", "text": "In this section, we show some examples to confirm our observation that CAMs tend to emphasize informative image positions in the target class, despite the fact that the image was misclassified at the first iteration (i.e. before zooming in on partial windows).To do this, we applied a classifier to the Stanford 40 action dataset test set that learned from VGG-GAP's GAP characteristics. For each category, in turn, we ranked all images in the test set according to the classifier's rating for that category. We then chose the five best true positive images and the five best false positive images. See Figure 4 for some representative images. We can see that the CAMs are consistent for a target class both in positive images and in the ranking of non-classifier images. Intuitively, this is the case that giving more weight to patterns that look similar."}, {"heading": "4.3 Early and Late Fusion", "text": "In all our experiments, we found that using characteristics extracted from a window in some iteration alone can produce worse results than those extracted from previous iterations (including this window). However, performance tends to improve when we combine results from multiple iterations, namely, on late fusion. If we train the combined (averaged) characteristics of windows from multiple iterations, the results (early fusion) continue to improve. If we add the values of early fused characteristics for different track lengths, accuracy continues to improve: If Si is the value of the classifier that is on a route of length i. Then we create a final value from S1 + St +..... ST tends to improve when T grows, and typically stabilizes at T = 5. See Fig. 5 for an illustration of this effect. Importantly, we have tried to use the classifier from the first iteration (i.e., trained on whole images) for all iterations."}, {"heading": "4.4 Fine-Grained vs General Categories", "text": "The Standford 40 Action dataset [7] is a benchmark dataset consisting of 9532 images from 40 different action classes, with 4000 images for training and the rest for testing. It contains a variety of action classes, including transitive with small objects (smoking, drinking) and large objects (horses), as well as uncompromising actions (running, jumping). As a starting point, we used the GAP network of [3] as a feature extractor and trained a multi-class SVM [26] based on the resulting characteristics. Using 5 iterations, we obtained a final precision of 77.08% and 80.06% for fine tuning. It is particularly interesting to examine the classes for which our method is most advantageous. We calculated and compared the F measure for each class based on the classification values from the fourth and first iteration. Fig. 6 shows this; the greatest absolute improvements are for relatively demanding classes such as the 38% (9.89%) of a message (9.89%) and the other objects (9.89%)."}, {"heading": "4.5 Top-Down vs. Bottom-Up attention", "text": "However, in order to further verify that our introspection mechanism highlights regions worth exploring, we evaluated an alternative to the introspection level using a generic highlighting measure [30]. On the Stanford 40 dataset, instead of using the CAM aspect after the first step of the classification, we chose the most prominent pixel as the center of the next sub-window. Then, as usual, we continued with training and testing, which resulted in a sharp decline in results: in the first iteration step, performance dropped from 74.47% when using the CAM to 62.31% when using the highlighting map. Corresponding performance declines were measured in the late fusion and early fusion steps, which improved the results in the proposed scheme, but worsened when using highlighting as a guide."}, {"heading": "4.6 Various Parameters & Fine-Tuning", "text": "Our method includes several parameters, including the number of iterations, the width of the beam search used to explore the routes of the windows on the image, and the ratio between the size of the current window and the next one. As for the number of iterations, we have consistently observed that the performance around the iteration is saturated and even deteriorates a little. An example of this can be seen in Figure 5, which shows the performance iteration number on the Standford 40 dataset. Similar behavior has been observed on all the datasets on which we evaluated the method, probably due to the increasingly small image regions that are taken into account in each iteration. In terms of the number of windows that need to be taken into account at each stage, we have tried to choose between 1 and 3 of the windows that relate to the highest rankings on a validation set. At best, such strategies that have been implemented, as well as the greedy strategy that has only chosen the highest scoring window in each iteration."}, {"heading": "5 Conclusions", "text": "The method is based on the observation that a trained CNN can be used to highlight relevant image areas even if its final classification is incorrect. This is the result of training in multiple visual categories using a common representation of characteristics. We have built on Class Activation Maps [3] for their simplicity and elegance, although other methods of identifying the source of the classification decision (e.g. [1]) could also be used. The proposed method integrates several features extracted at different locations and scales. It continuously improves fine-grained classification tasks and tasks where the classification depends on finely localized details. It obtains state-of-the-art results on CUB-200-2011 [4], among other methods that avoid strong monitoring, such as Bounding Boxes or Keypoint Annotations. The improvements are shown although the method is trained only on class labels to avoid the need for the results of monitoring to be examined automatically in the form of (33 annotations)."}], "references": [{"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Computer vision\u2013ECCV 2014. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding deep image representations by inverting them", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Deep Features for Discriminative Localization", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "arXiv preprint arXiv:1512.04150", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Novel Dataset for FineGrained Image Categorization", "author": ["A. Khosla", "N. Jayadevaprakash", "B. Yao", "L. Fei-Fei"], "venue": "First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Fine-grained visual classification of aircraft", "author": ["S. Maji", "E. Rahtu", "J. Kannala", "M. Blaschko", "A. Vedaldi"], "venue": "arXiv preprint arXiv:1306.5151", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Human action recognition by learning bases of action attributes and parts", "author": ["B. Yao", "X. Jiang", "A. Khosla", "A.L. Lin", "L. Guibas", "L. Fei-Fei"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Part-based R-CNNs for finegrained category detection", "author": ["N. Zhang", "J. Donahue", "R. Girshick", "T. Darrell"], "venue": "Computer Vision\u2013ECCV 2014. Springer", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Fine-grained pose prediction, normalization, and recognition", "author": ["N. Zhang", "E. Shelhamer", "Y. Gao", "T. Darrell"], "venue": "arXiv preprint arXiv:1511.07063", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "Science 350(6266)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Object detectors emerge in deep scene cnns", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "arXiv preprint arXiv:1412.6856", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Self-taught object localization with deep networks", "author": ["A. Bergamo", "L. Bazzani", "D. Anguelov", "L. Torresani"], "venue": "arXiv preprint arXiv:1409.3964", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Is object localization for free?-weaklysupervised learning with convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A Graves"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Active object localization with deep reinforcement learning", "author": ["J.C. Caicedo", "S. Lazebnik"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A Zisserman"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "The application of two-level attention models in deep convolutional neural network for fine-grained image classification", "author": ["T. Xiao", "Y. Xu", "K. Yang", "J. Zhang", "Y. Peng", "Z. Zhang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Bilinear CNN models for fine-grained visual recognition", "author": ["T.Y. Lin", "A. RoyChowdhury", "S. Maji"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Fine-grained recognition without part annotations", "author": ["J. Krause", "H. Jin", "J. Yang", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep spatial pyramid: The devil is once again in the details", "author": ["B.B. Gao", "X.S. Wei", "J. Wu", "W. Lin"], "venue": "arXiv preprint arXiv:1504.05277", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Weakly Supervised Fine-Grained Image Categorization", "author": ["Y. Zhang", "Wei", "X.s.", "J. Wu", "J. Cai", "J. Lu", "V.A. Nguyen", "M.N. Do"], "venue": "arXiv preprint arXiv:1504.04943", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural activation constellations: Unsupervised part model discovery with convolutional networks", "author": ["M. Simon", "E. Rodner"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "LIBLINEAR: A Library for Large Linear Classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "Journal of Machine Learning Research 9", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "MatConvNet: Convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference, ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Vlfeat: an open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "In Bimbo, A.D., Chang, S.F., Smeulders, A.W.M., eds.: ACM Multimedia, ACM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Saliency optimization from robust background detection", "author": ["W. Zhu", "S. Liang", "Y. Wei", "J. Sun"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition", "author": ["J. Krause", "B. Sapp", "A. Howard", "H. Zhou", "A. Toshev", "T. Duerig", "J. Philbin", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1511.06789", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Augmenting strong supervision using web data for fine-grained categorization", "author": ["Z. Xu", "S. Huang", "Y. Zhang", "D. Tao"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Long Short Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Examples of pioneering work in this direction include [1,2]; providing glimpses into the representations learned by intermediate levels in the network.", "startOffset": 54, "endOffset": 59}, {"referenceID": 1, "context": "Examples of pioneering work in this direction include [1,2]; providing glimpses into the representations learned by intermediate levels in the network.", "startOffset": 54, "endOffset": 59}, {"referenceID": 2, "context": "[3] provides an elegant mechanism to highlight the discriminative image regions that served the CNN for a given task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "1: (Top) Class Activation Maps [3] show the source of a network\u2019s classification.", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "An SVM trained on features extracted from VGG-GAP [3] misclassified all of these images, while highlighting the discriminative regions.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "As shown, our method is particularly beneficial for fine-grained tasks such as species [4,5] or model [6] identification and to challenging cases in e.", "startOffset": 87, "endOffset": 92}, {"referenceID": 4, "context": "As shown, our method is particularly beneficial for fine-grained tasks such as species [4,5] or model [6] identification and to challenging cases in e.", "startOffset": 102, "endOffset": 105}, {"referenceID": 5, "context": ", action recognition [7], which requires attention to small and localized details.", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "Some examples are [8,9], where bounding boxes and part annotations are given at train time.", "startOffset": 18, "endOffset": 23}, {"referenceID": 7, "context": "Some examples are [8,9], where bounding boxes and part annotations are given at train time.", "startOffset": 18, "endOffset": 23}, {"referenceID": 8, "context": "Aside from the resources required for large-scale annotations, such methods elude the question of learning from weakly supervised data (and mostly unsupervised data), as is known to happen in human infants, who can learn from limited examples [10].", "startOffset": 243, "endOffset": 247}, {"referenceID": 0, "context": "[1] found patterns that activate hidden units via deconvolutional neural networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Solves an optimization problem, aiming to generate an image whose features are similar to a target image, regularized by a natural image prior.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[11] aims to explicitly find what image patches activate hidden network units, finding that indeed many of them correspond to semantic concepts and object parts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Localizes image regions pertaining to the target class by masking out subimages and inspecting change in activations of the network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] use global max-pooling to obtain points on the target objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] used global average pooling (GAP) to generate a Class-Activation Mapping (CAM), visualizing discriminative image regions and enabling the localization of detected concepts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[14] or efficient object localization [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] or efficient object localization [15].", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "Such methods benefit from the recent fusion between the fields of deep learning and reinforcement learning [16].", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "Another method of interest is the spatial-transformer networks in [17]: they designed a network that learns and applies spatial warping to the feature maps, effectively aligning inputs, which results in increased robustness to geometric transformations.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "Additional works appear in [18], who discovers discriminative patches and groups them to generate part detectors, whose detections are combined with the discovered patches for a final classification.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "In [19], the outputs of two networks are combined via an outer-product, creating a strong feature representation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "[20] discovers and uses parts by using co-segmentation on ground-truth bounding boxes followed by alignment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Both classification/introspection steps utilize the recent Class-Activation Mapping method of [3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "In [3], a deep neural network is modified so that post-classification, it is possible to visualize the varying contribution of image regions, via a so-called Class Activation Mapping (CAM).", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "Using the notation in [3]: let fk(x, y) be the k\u2019th output of the last convolutional layer at grid location (x, y).", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "Indeed, [3] has shown this method to highlight informative image", "startOffset": 8, "endOffset": 11}, {"referenceID": 19, "context": "regions (with respect to the task at hand), while being on par with the classification performance obtained by the unmodified network (GoogLeNet [21] in their case).", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "Each classifier is an SVM trained on the features the output of the GAP layer of the network (as was done in [3]).", "startOffset": 109, "endOffset": 112}, {"referenceID": 20, "context": "This reduces the training time at no significant change to the final performance; such an effect has also been noted by [22].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "3: Exploration routes on images: Each row shows the original image and 3 iterations of the algorithm, including the resulting Class-Activation Maps [3] used to guide the next iteration.", "startOffset": 148, "endOffset": 151}, {"referenceID": 20, "context": "In all our experiments, we start with a variant of the VGG-16 network [22] which was fined tuned on ILSVRC by [3].", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "In all our experiments, we start with a variant of the VGG-16 network [22] which was fined tuned on ILSVRC by [3].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "VGG-GAP* is our improved baseline using the VGG-GAP network [3].", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "GoogLeNet-GAP[3] 72.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "03[3],81 [23] 79.", "startOffset": 2, "endOffset": 5}, {"referenceID": 21, "context": "03[3],81 [23] 79.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "92[24] 77.", "startOffset": 2, "endOffset": 6}, {"referenceID": 16, "context": "9 [18], 81.", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": "01 [25], 82 [20], 84.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "01 [25], 82 [20], 84.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "1 [17] 84.", "startOffset": 2, "endOffset": 6}, {"referenceID": 17, "context": "1[19]", "startOffset": 1, "endOffset": 5}, {"referenceID": 24, "context": "All classifiers are trained with a linear SVM [26] on `2 normalized feature vectors.", "startOffset": 46, "endOffset": 50}, {"referenceID": 25, "context": "Our experiments were carried out using the MatConvNet framework [27], as well as [28,29].", "startOffset": 64, "endOffset": 68}, {"referenceID": 26, "context": "Our experiments were carried out using the MatConvNet framework [27], as well as [28,29].", "startOffset": 81, "endOffset": 88}, {"referenceID": 5, "context": "We evaluated our approach on several datasets, including Stanford-40 Actions [7], the Caltech-USCD Birds-200-2011 [4] (a.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "a CUB-200-2011), the Stanford-Dogs dataset, [5] and the FGVC-Aircraft dataset [6].", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": "a CUB-200-2011), the Stanford-Dogs dataset, [5] and the FGVC-Aircraft dataset [6].", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "638; This is complementary evidence to [3], who shows that the CAMs have good localization capabilities for the correct class.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "The Standford-40 Action dataset [7] is a benchmark dataset made of 9532 images of 40 different action classes, with 4000 images for training and the rest for testing.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "As a baseline, we used the GAP-network of [3] as a feature extractor and trained a multi-class SVM [26] using the resulting features.", "startOffset": 42, "endOffset": 45}, {"referenceID": 24, "context": "As a baseline, we used the GAP-network of [3] as a feature extractor and trained a multi-class SVM [26] using the resulting features.", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "Performance is shown on the Stanford-40 [7] dataset", "startOffset": 40, "endOffset": 43}, {"referenceID": 5, "context": "The figure shows absolute difference in terms of F-measure over the baseline approach on all categories of the Stanford-40 Actions [7] dataset", "startOffset": 131, "endOffset": 134}, {"referenceID": 27, "context": "To further verify that our introspection mechanism highlights regions whose exploration is worthwhile, we evaluated an alternative to the introspection stage by using a generic saliency measure [30].", "startOffset": 194, "endOffset": 198}, {"referenceID": 20, "context": "We used the VGG-16 [22] network to extract fc6 features along the GAP features for all considered windows.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "89%, compared to the previous best 81% of [23].", "startOffset": 42, "endOffset": 46}, {"referenceID": 2, "context": "03% of [3].", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": ", Stanford-40 [7], CUB-200-2011 [4], but did not improve results significantly for others (Dogs [5], Aircraft [6]).", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": ", Stanford-40 [7], CUB-200-2011 [4], but did not improve results significantly for others (Dogs [5], Aircraft [6]).", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": ", Stanford-40 [7], CUB-200-2011 [4], but did not improve results significantly for others (Dogs [5], Aircraft [6]).", "startOffset": 110, "endOffset": 113}, {"referenceID": 28, "context": "48%, which to our knowledge the best result to-date, second only to works which added a massive amount of external data mined from the web [31] (91.", "startOffset": 139, "endOffset": 143}, {"referenceID": 29, "context": "9%) and/or strong supervision [32] (84.", "startOffset": 30, "endOffset": 34}, {"referenceID": 2, "context": "We have built upon Class Activation Maps [3] due to their simplicity and elegance, though other methods for identifying the source of the classification decision (e.", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": ", [1]) could probably be employed as well.", "startOffset": 2, "endOffset": 5}, {"referenceID": 30, "context": "In future work, it would be interesting to examine the use of recurrent nets (RNN, LSTM [33]) to automatically learn sequential processes, which incrementally improve classification results, extending the approach described in the current work.", "startOffset": 88, "endOffset": 92}], "year": 2017, "abstractText": "Convolutional neural networks have been shown to develop internal representations, which correspond closely to semantically meaningful objects and parts, although trained solely on class labels. Class Activation Mapping (CAM) is a recent method that makes it possible to easily highlight the image regions contributing to a network\u2019s classification decision. We build upon these two developments to enable a network to re-examine informative image regions, which we term introspection. We propose a weakly-supervised iterative scheme, which shifts its center of attention to increasingly discriminative regions as it progresses, by alternating stages of classification and introspection. We evaluate our method and show its effectiveness over a range of several datasets, obtaining a top-1 accuracy 84.48% CUB-200-2011, which is the highest to-date without using external data or stronger supervision. On Stanford-40 Actions, we set a new state-of the art of 87.89%, and on FGVC-Aircraft and the Stanford Dogs dataset, we show consistent improvements over baselines, some of which include significantly more supervision.", "creator": "LaTeX with hyperref package"}}}