{"id": "1603.04416", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2016", "title": "Criteria of efficiency for conformal prediction", "abstract": "We study optimal conformity measures for various criteria of efficiency in an idealized setting. This leads to an important class of criteria of efficiency that we call probabilistic; it turns out that the most standard criteria of efficiency used in literature on conformal prediction are not probabilistic.", "histories": [["v1", "Mon, 14 Mar 2016 19:49:07 GMT  (21kb,D)", "https://arxiv.org/abs/1603.04416v1", "16 pages"], ["v2", "Wed, 14 Sep 2016 12:57:51 GMT  (38kb,D)", "http://arxiv.org/abs/1603.04416v2", "31 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vladimir vovk", "ilia nouretdinov", "valentina fedorova", "ivan petej", "alex gammerman"], "accepted": false, "id": "1603.04416"}, "pdf": {"name": "1603.04416.pdf", "metadata": {"source": "CRF", "title": "Criteria of efficiency for conformal prediction\u2217", "authors": ["Vladimir Vovk", "Ilia Nouretdinov", "Valentina Fedorova", "Ivan Petej", "Alex Gammerman"], "emails": ["volodya.vovk@gmail.com", "alushaf@gmail.com", "ivan.petej@gmail.com", "ilia@cs.rhul.ac.uk", "alex@cs.rhul.ac.uk"], "sections": [{"heading": null, "text": "The conference version of this paper was published in the Proceedings of COPA 2016."}, {"heading": "1 Introduction", "text": "In this sense, conventional predictors are well able to differ in their efficiency, in the way they are used in the literature."}, {"heading": "2 Criteria of Efficiency for Conformal Predic-", "text": "This is an example of how a conformity measurement is a measurable function A associated with each finite sequence (z1,......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "2.1 Basic criteria", "text": "The simplest criteria for efficiency are: \u2022 The S criterion (where \"S\" stands for \"sum\") measures efficiency based on the average sum1k l + k \u00b2 i = l + 1 \u00b2 y pyi (4) of the p values; small values are preferable for this criterion. It is -free. \u2022 The N criterion uses the average size1k l + k \u00b2 i = l + 1 \u00b2 i \u00b2 of the forecast sets (\"N\" stands for \"number\": the size of a forecast set is the number of designations therein). Small values are preferable. Under this criterion, efficiency is a function of the significance level. Both criteria are preceding. The S criterion was introduced in [3] and the N criterion was introduced independently in [5] and [3] in relation to the size of the forecast (although the N criterion was published in 2012)."}, {"heading": "2.2 Other prior criteria", "text": "The question that has arisen in recent years in the USA and Europe is whether such a constellation will occur in the USA, that the USA and the EU will be able to establish themselves in the EU, and whether such a constellation will occur in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in"}, {"heading": "2.3 Observed criteria", "text": "The criteria discussed in the previous subsection treat the largest p value or predictive values of size 1 in a special way. The corresponding criteria of this subsection attempt to achieve the same goal by using the observed label. These are the observed counterparts to the non-basic previous -free criteria: \u2022 The criterion OU (\"observed distrust\") uses the average observed distrust 1k l + k \u2211 i = l + 1 max y 6 = yi pyiover of the test sequence, where the observed distrust for a test example (xi, yi) is the largest p value p y i for the wrong labels y 6 = yi. Smaller values are preferable for this test. \u2022 The criterion OF (\"observed blur\") uses the average sum of Pvalues for the wrong labels, i.e., 1k l + k = l + k = i pyi; (10) smaller prediction values are preferable."}, {"heading": "3 Idealised Setting", "text": "Starting from this section, we will consider the limit case of the infinitely long training and test sequences Q = y (and we will return to the realistic end case only in Section 7, where we describe our empirical studies. To formalize the intuition of the infinitely long training sequence, we will assume that the prediction algorithm directly specifies the data-generating probability distribution Q to Z, rather than receiving a training sequence. Instead of the conformity measures, we will use idealized conformity measures: Functions A (Q, z) of Q-P (Z) (where P (Z) is the set of all probability measures to Z) and z (Z). We will fix the data-generalizing distribution Q for the rest of the paper, writing the corresponding conformity values as A (z). The idealized conformity assessments correspond to the A value."}, {"heading": "4 Probabilistic Criteria of Efficiency", "text": "The goal in this section is to characterize the optimal idealized conformity measures for the four efficiency criteria set out in italics in Table 1. (We assume that the entire sample range Z is finite. (We also assume that the data-generating probability distributions QX (x) > 0 for all x-X (we often omit curly expressions such as QX ({x}): We can always omit the xs for which QX (x) idealized probability (CP) measures idealized conformity isA (x, y) = QY (y | x) = QY (y | x): = Q (x, y) QX (x) QX (x))). (In this paper we will measure idealized conformity."}, {"heading": "5 Criteria of Efficiency that are not Probabilis-", "text": "ticNow we define the idealized analogies of the six criteria, which are not italic in Table 1. An idealized conformity variable A is: \u2022 U-optimal if an idealized conformity variable B, we have eitherEx, bar min y \u2032 6 = ypA (x, y \u2032) < Ex, bar max y \u2032 (21) and Ex, bar maxy pA (x, y) (20) or both Ex, bar max y \u2032 6 = ypA (x, y \u2032) = Ex, Miny max y \u2032 6 = ypA (x, y \u2032) (21) and Ex, bar maxy pA (x, y \u2032)."}, {"heading": "S: CP (Theorem 1) N: CP (Theorem 1)", "text": "An MCP idealized conformity scale is an idealized conformity unit that corresponds to any elective function; R (MCP) is defined analogously to R (CP), but using MCP idealized conformity measures instead of the CP idealized conformity measurement. Theorem 3. O (F) = O (E) = 0 (E) = 2 (MCP).Theorems 2 and 3 are of course equivalent when the modified significant conformity measurement (MSP) is defined by A (x, y): = f (x), if f (x) > 1 / 2 and y (x) 0, if f (x) \u2264 1 / 2 \u2212 f (x) > 1 / 2 \u2212 f (y), if f (x) > 1 / 2 and y = y (y), where f is the predictability function (32); note that this definition is not affected by the choice function."}, {"heading": "6 Proofs of Theorems 2\u20134", "text": "The evidence in this section will be somewhat less formal than the evidence for Theorem 1; in particular, all references to the Neyman-Pearson dilemma will be implicit."}, {"heading": "6.1 Proof of Theorem 2", "text": "It is clear that an M-optimum conformity measurement will assign the lowest conformity to the group of examples (x, y) with f (x) = f1 and y (x) for any choice function (x), unless the conformity of such examples is removed (x) for the group of examples x (x, x) for all objects x (x, x) for all x (x) and the remaining predictabilities are sorted in decreasing order. It is clear that an M-optimum conformity measurement will assign the lowest conformity to the group of examples (x, y) with f (x) = f1 and y (x) for any choice function y (see 33). Conformity of such examples may vary, provided it is not included in the group of examples."}, {"heading": "6.2 Proof of Theorem 3", "text": "Our reasoning for O (E) = R (MCP) will be similar to the reasoning for O (M) = R (SP) in the previous subsection; we will again analyze the requirements imposed on us by starting from small values of A (0, 1). Let us imply \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "6.3 Proof of Theorem 4", "text": "The proof is similar to the proofs in the previous two subsections. First, we check that O (OM) = Q = Q (R) = R \u2032 (MSP) contains all possible examples, with the requirement for OM optimality starting from small values (0, 1). Let f1 > f2 > \u00b7 \u00b7 fn > 0.5 sort the list of predictions of all objects x \u00b2 X whose predictability exceeds 0.5, removing all duplicates and sorting the remaining predictions in decreasing order. All examples are divided into 2n + 1 groups (perhaps some of them blank) so that each element of the ith group x comes before each element of the jth group when 1 < j \u2264 2n + 1. The ith group, i = 1, contains all examples (x, y) with predictability fi and Q (y) < x; 1 / 2, the group (n + 1)."}, {"heading": "7 Empirical Study", "text": "In this section we show differences between two of our -free criteria, OF (probabilistic) and U (default, but not probabilistic) on the USPS dataset of handwritten digits ([7]; examples of such digits are given in Figure 1, which is a subset of Figure 2 in [7]. We use the original division of the data into training and test sets. Our programs are written in R, and the results shown in the figures below are for the seed 0 of the R random number generator; however, we observe similar results in experiments with other digits. The problem is to classify handwritten digits, the labels are elements of {0,., 9} and the objects are elements of R256, where the 256 numbers represent the brightness of the pixels in 16 x 16 images. We standardize each object by applying the same affine transformation (depending on the object) to each of its pixels that form the mean brightness of the image in the pixel."}, {"heading": "8 Efficiency of Label-conditional Conformal", "text": "Predictors and TransducersCompliant predictors, as defined in Section 2, only guarantee the overall coverage probability averaged across all labels. Sometimes we would like to have a guarantee for the coverage probability for each label and each Y separately, and in this case one should use brand-related compliant predictors examined in this section."}, {"heading": "8.1 Label-conditional conformal predictors and transducers", "text": "The label-related conformity predictor determined by a conformity measure A is defined by (1), defining the label-related p-values py bypy: = (,,,.., l | yi = y & \u03b1yi < \u03b1yl + 1,.., l | yi = y & \u03b1yi = \u03b1yl + 1,..) / (, l | yi = y} | + 1) (42) (instead of (2); as before, it is a random number evenly distributed over the interval [0, 1] (conditioned on all examples), and the conformity values are defined by (3); the label-related conformity values determined by A represent the system of p-values (py | y, Y), defined by (42)."}, {"heading": "8.2 Idealised setting", "text": "As before, we assume that object space X is limited and QX (x) > 0 for all x-X. We also assume that QY (y) > 0 is for all y-Y, where QY is the boundary distribution of Q in label space Y. Let A be an idealized measure of conformity. For each potential label y-Y for an object, we define the corresponding label-conditioned p-value aspy = p (x, y): = Q {(x, y) \u2022 Z | A (x, y) < A (x, y)} QY (y) + Q {(x, y)."}, {"heading": "8.3 Probabilistic criteria of efficiency", "text": "Label-conditioned S-optimal, N-optimal, OF-optimal and OE-optimal idealized conformity measures are exactly defined as S-optimal, N-optimal, OF-optimal idealized conformity measures at the end of Section 3, but with the label-conditioned definitions of p values and predictive sectors. Let us say that an idealized conformity measure A is a label-conditioned refinement of an idealized conformity measure B ifB (x1, y) = Olma (Olma, y) shows that an idealized conformity measure A (x2, y) for all x1, x2, X and all y, y. Note that the notion of label-conditioned refinement is weaker than that of refinement (as defined by (15): if A is an optimal constraint of B, then A is a label-conditioned constraint of B, but not vice versa)."}, {"heading": "8.4 Other criteria of efficiency", "text": "Using the Label-conditioned definitions of the p values and the prediction quantities, we define Q = Q = Q = Q = Q Q = Q Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q ="}, {"heading": "9 Conclusion", "text": "It would be interesting to apply the results of this paper as far as possible to the cases of: \u2022 Regression. The sum of p-values (as used in the S criterion) now becomes an integral part of the p-value as a function of the label y of the test example, and the size of a predictive set becomes its Lebesgue measurement (as already considered in [11] in the non-idealized case), whereas the latter is typically finite, ensuring the convergence of the former is less simple. \u2022 Detection of anomalies. A first step in this direction is taken in [17], where the average p-value is considered a criterion of efficiency. \u2022 Infinity, including non-discrete object spaces, are the object rooms X. \u2022 Non-idealized conform predictors. \u2022 Significance level = y, which depend on the predisposition Y in the parental case."}, {"heading": "Acknowledgments", "text": "We thank the reviewers of the conference version of this paper for their helpful comments. This work was partially supported by the EPSRC (Grant EP / K033344 / 1), the Air Force Office of Scientific Research (Grant \"Semantic Completions\") and the EU research and innovation programme Horizon 2020 (Grant 671555)."}], "references": [{"title": "Conformal Prediction for Reliable Machine Learning: Theory, Adaptations, and Applications", "author": ["Vineeth N. Balasubramanian", "Shen-Shyang Ho", "Vladimir Vovk", "editors"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Probability forecasting", "author": ["A. Philip Dawid"], "venue": "Encyclopedia of Statistical Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Conformal prediction under hypergraphical models", "author": ["Valentina Fedorova", "Alex Gammerman", "Ilia Nouretdinov", "Vladimir Vovk"], "venue": "Artificial Intelligence Applications and Innovations. Second Workshop on Conformal Prediction and Its Applications (COPA", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Strictly proper scoring rules, prediction, and estimation", "author": ["Tilmann Gneiting", "Adrian E. Raftery"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Evolved decision trees as conformal predictors", "author": ["Ulf Johansson", "Rikard K\u00f6nig", "Tuve L\u00f6fstr\u00f6m", "Henrik Bostr\u00f6m"], "venue": "Proceedings of the 2013 IEEE Conference on Evolutionary Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Reducibility among combinatorial problems", "author": ["Richard M. Karp"], "venue": "Complexity of Computer Computations,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1972}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Yann Le Cun", "Bernhard E. Boser", "John S. Denker", "Donnie Henderson", "R.E. Howard", "Wayne E. Hubbard", "Lawrence D. Jackel"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1990}, {"title": "Testing Statistical Hypotheses", "author": ["Erich L. Lehmann"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1986}, {"title": "Classification with confidence", "author": ["Jing Lei"], "venue": "Biometrika, 101:755\u2013769,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Distribution free prediction sets", "author": ["Jing Lei", "James Robins", "Larry Wasserman"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Distribution free prediction bands for nonparametric regression", "author": ["Jing Lei", "Larry Wasserman"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Knapsack Problems: Algorithms and Computer Implementations", "author": ["Silvano Martello", "Paolo Toth"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1990}, {"title": "Comparing the Bayes and typicalness frameworks", "author": ["Thomas Melluish", "Craig Saunders", "Ilia Nouretdinov", "Vladimir Vovk"], "venue": "Proceedings of the Twelfth European Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Special Issue of the Annals of Mathematics and Artificial Intelligence on Conformal Prediction and its Applications, volume 74(1\u20132)", "author": ["Harris Papadopoulos", "Alex Gammerman", "Vladimir Vovk", "editors"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Least ambiguous set-valued classifiers with bounded error levels", "author": ["Mauricio Sadinle", "Jing Lei", "Larry Wasserman"], "venue": "Technical Report arXiv:1609.00451v1 [stat.ME],", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Transduction with confidence and credibility", "author": ["Craig Saunders", "Alex Gammerman", "Vladimir Vovk"], "venue": "Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Anomaly detection of trajectories with kernel density estimation by conformal prediction", "author": ["James Smith", "Ilia Nouretdinov", "Rachel Craddock", "Charles Offer", "Alexander Gammerman"], "venue": "AIAI Workshops, COPA 2014,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Criteria of efficiency for conformal prediction", "author": ["Vladimir Vovk", "Valentina Fedorova", "Ilia Nouretdinov", "Alex Gammerman"], "venue": "Proceedings of the Fifth International Symposium on Conformal and Probabilistic Prediction with Applications (COPA 2016),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Algorithmic Learning in a Random World", "author": ["Vladimir Vovk", "Alex Gammerman", "Glenn Shafer"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": ", [1, 14] (and the COPA Proceedings, 2012\u20132016).", "startOffset": 2, "endOffset": 9}, {"referenceID": 13, "context": ", [1, 14] (and the COPA Proceedings, 2012\u20132016).", "startOffset": 2, "endOffset": 9}, {"referenceID": 2, "context": "In two recent papers [3, 5] two probabilistic criteria have been introduced, and in this paper we introduce two more and argue that probabilistic criteria should be used in place of more standard ones.", "startOffset": 21, "endOffset": 27}, {"referenceID": 4, "context": "In two recent papers [3, 5] two probabilistic criteria have been introduced, and in this paper we introduce two more and argue that probabilistic criteria should be used in place of more standard ones.", "startOffset": 21, "endOffset": 27}, {"referenceID": 18, "context": ", [19], p.", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "96; introduced in [16]).", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "This criterion was introduced in [13], Section 7.", "startOffset": 33, "endOffset": 37}, {"referenceID": 18, "context": "2, and used extensively in [19].", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "The other two criteria that had been used before the publication of the conference version [18] of this paper are the sum of the p-values for all potential labels (this does not depend on the significance level) and the size of the prediction set at a given significance level: see the papers [3] and [5].", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "The other two criteria that had been used before the publication of the conference version [18] of this paper are the sum of the p-values for all potential labels (this does not depend on the significance level) and the size of the prediction set at a given significance level: see the papers [3] and [5].", "startOffset": 293, "endOffset": 296}, {"referenceID": 4, "context": "The other two criteria that had been used before the publication of the conference version [18] of this paper are the sum of the p-values for all potential labels (this does not depend on the significance level) and the size of the prediction set at a given significance level: see the papers [3] and [5].", "startOffset": 301, "endOffset": 304}, {"referenceID": 1, "context": "As we point out in Section 5, probabilistic criteria of efficiency are conceptually similar to \u201cproper scoring rules\u201d in probability forecasting [2, 4], and this is our main motivation for their detailed study in this paper.", "startOffset": 145, "endOffset": 151}, {"referenceID": 3, "context": "As we point out in Section 5, probabilistic criteria of efficiency are conceptually similar to \u201cproper scoring rules\u201d in probability forecasting [2, 4], and this is our main motivation for their detailed study in this paper.", "startOffset": 145, "endOffset": 151}, {"referenceID": 14, "context": "A version (with a different treatment of empty observations) of one of the new non-probabilistic criteria of efficiency that we discuss in this paper (the one that we call the E criterion) has been introduced independently in [15].", "startOffset": 226, "endOffset": 230}, {"referenceID": 7, "context": "[8], Section 3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "\u03c4 is a random number distributed uniformly on the interval [0, 1] (even conditionally on all the examples), and the corresponding sequence of conformity scores is defined by (\u03b1 1 , .", "startOffset": 59, "endOffset": 65}, {"referenceID": 0, "context": "The standard property of validity for conformal transducers is that the pvalues p are distributed uniformly on [0, 1] when the examples z1, .", "startOffset": 111, "endOffset": 117}, {"referenceID": 0, "context": "are generated independently from the same probability distribution Q on Z and \u03c4 is generated independently from the uniform probability distribution on [0, 1] (see, e.", "startOffset": 152, "endOffset": 158}, {"referenceID": 18, "context": ", [19], Proposition 2.", "startOffset": 2, "endOffset": 6}, {"referenceID": 2, "context": "The S criterion was introduced in [3] and the N criterion was introduced independently in [5] and [3], although the analogue of the N criterion for regression (where the size of a prediction set is defined to be its Lebesgue measure) had been used earlier in [11] (whose arXiv version was published in 2012).", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "The S criterion was introduced in [3] and the N criterion was introduced independently in [5] and [3], although the analogue of the N criterion for regression (where the size of a prediction set is defined to be its Lebesgue measure) had been used earlier in [11] (whose arXiv version was published in 2012).", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "The S criterion was introduced in [3] and the N criterion was introduced independently in [5] and [3], although the analogue of the N criterion for regression (where the size of a prediction set is defined to be its Lebesgue measure) had been used earlier in [11] (whose arXiv version was published in 2012).", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "The S criterion was introduced in [3] and the N criterion was introduced independently in [5] and [3], although the analogue of the N criterion for regression (where the size of a prediction set is defined to be its Lebesgue measure) had been used earlier in [11] (whose arXiv version was published in 2012).", "startOffset": 259, "endOffset": 263}, {"referenceID": 2, "context": "The U criterion in this form was introduced in [3], but it is equivalent to using the average confidence (one minus unconfidence), which is very common.", "startOffset": 47, "endOffset": 50}, {"referenceID": 18, "context": "This is a widely used criterion; in particular, it was used in [19] and papers preceding it.", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "A criterion that is very similar to the M and E criteria is used by Lei in [9] (Section 2.", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "The difference of the criterion used in [9] is that it prohibits empty predictions (an intermediate approach would be to prefer smaller values for the number (9) of empty predictions).", "startOffset": 40, "endOffset": 43}, {"referenceID": 14, "context": "Lei\u2019s criterion is extended to the multi-class case in [15], which proposes a modification of the E criterion with a different treatment of empty predictions.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "}), but we often omit pairs of parentheses when there is no danger of ambiguity), where \u03c4 is a random number distributed uniformly on [0, 1].", "startOffset": 134, "endOffset": 140}, {"referenceID": 0, "context": "The standard properties of validity for conformal transducers and predictors mentioned in the previous section simplify in this idealised case as follows: \u2022 If (x, y) is generated from Q and \u03c4 \u2208 [0, 1] is generated from the uniform", "startOffset": 195, "endOffset": 201}, {"referenceID": 0, "context": "distribution independently of (x, y), p(x, y) is distributed uniformly on [0, 1].", "startOffset": 74, "endOffset": 80}, {"referenceID": 0, "context": "where the notation Ex,\u03c4 refers to the expected value when x and \u03c4 are independent, x \u223c QX, and \u03c4 \u223c U ; QX is the marginal distribution of Q on X, and U is the uniform distribution on [0, 1]; \u2022 N-optimal if, for any idealised conformity measure B and any significance level , Ex,\u03c4 |\u0393 A(x)| \u2264 Ex,\u03c4 |\u0393 B(x)| ; \u2022 OF-optimal if, for any idealised conformity measure B, E(x,y),\u03c4 \u2211", "startOffset": 183, "endOffset": 189}, {"referenceID": 2, "context": ") This idealised conformity measure was introduced by an anonymous referee of the conference version of [3], but its non-idealised analogue in the case of regression had been used in [11] (following [10] and literature on minimum volume prediction).", "startOffset": 104, "endOffset": 107}, {"referenceID": 10, "context": ") This idealised conformity measure was introduced by an anonymous referee of the conference version of [3], but its non-idealised analogue in the case of regression had been used in [11] (following [10] and literature on minimum volume prediction).", "startOffset": 183, "endOffset": 187}, {"referenceID": 9, "context": ") This idealised conformity measure was introduced by an anonymous referee of the conference version of [3], but its non-idealised analogue in the case of regression had been used in [11] (following [10] and literature on minimum volume prediction).", "startOffset": 199, "endOffset": 203}, {"referenceID": 7, "context": "Let us apply the Neyman\u2013Pearson fundamental lemma ([8], Sect.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "where we have used the fact that p(x, y) is distributed uniformly on [0, 1] when ((x, y), \u03c4) \u223c Q\u00d7 U (see [19]).", "startOffset": 69, "endOffset": 75}, {"referenceID": 18, "context": "where we have used the fact that p(x, y) is distributed uniformly on [0, 1] when ((x, y), \u03c4) \u223c Q\u00d7 U (see [19]).", "startOffset": 105, "endOffset": 109}, {"referenceID": 0, "context": "again using the fact that p(x, y) is distributed uniformly on [0, 1] and so P(x,y),\u03c4 (y \u2208 \u0393 (x)) = 1 \u2212 , where P(x,y),\u03c4 refers to the probability when (x, y) \u223c Q and \u03c4 \u223c U are independent.", "startOffset": 62, "endOffset": 68}, {"referenceID": 0, "context": "(instead of (4) or (13), respectively), where \u03c6 : [0, 1]\u2192 R is a fixed continuously differentiable strictly increasing function, not necessarily the identity function.", "startOffset": 50, "endOffset": 56}, {"referenceID": 18, "context": "In the following three definitions we follow [19], Chapter 3.", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": ", [2] and [4]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": ", [2] and [4]).", "startOffset": 10, "endOffset": 13}, {"referenceID": 18, "context": "4 in [19]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "In this section we demonstrate differences between two of our -free criteria, OF (probabilistic) and U (standard but not probabilistic) on the USPS data set of hand-written digits ([7]; examples of such digits are given in Figure 1, which is a subset of Figure 2 in [7]).", "startOffset": 181, "endOffset": 184}, {"referenceID": 6, "context": "In this section we demonstrate differences between two of our -free criteria, OF (probabilistic) and U (standard but not probabilistic) on the USPS data set of hand-written digits ([7]; examples of such digits are given in Figure 1, which is a subset of Figure 2 in [7]).", "startOffset": 266, "endOffset": 269}, {"referenceID": 0, "context": "(instead of (2)); as before, \u03c4 is a random number distributed uniformly on the interval [0, 1] (conditionally on all the examples), and the conformity scores are defined by (3).", "startOffset": 88, "endOffset": 94}, {"referenceID": 0, "context": "The property of validity for label-conditional conformal predictors and transducers is that the p-values p are distributed uniformly on [0, 1] given y when the examples z1, .", "startOffset": 136, "endOffset": 142}, {"referenceID": 18, "context": ", [19], Proposition 4.", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "analogously to (11), with the same random number \u03c4 \u2208 [0, 1] used for all (x, y).", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "The properties of validity now become conditional: \u2022 If (x, y) is generated from Q and \u03c4 is generated independently from the uniform probability distribution on [0, 1], p(x, y) is distributed uniformly on [0, 1] even if we condition on y.", "startOffset": 161, "endOffset": 167}, {"referenceID": 0, "context": "The properties of validity now become conditional: \u2022 If (x, y) is generated from Q and \u03c4 is generated independently from the uniform probability distribution on [0, 1], p(x, y) is distributed uniformly on [0, 1] even if we condition on y.", "startOffset": 205, "endOffset": 211}, {"referenceID": 14, "context": "The label-conditional U and M criteria are standard, and the label conditional E criterion (with a different treatment of empty observations) has been introduced and explored in [15].", "startOffset": 178, "endOffset": 182}, {"referenceID": 10, "context": "The sum of p-values (as used in the S criterion) now becomes the integral of the p-value as function of the label y of the test example, and the size of a prediction set becomes its Lebesgue measure (considered, as already mentioned, in [11] in the non-idealised case).", "startOffset": 237, "endOffset": 241}, {"referenceID": 16, "context": "A first step in this direction is made in [17], which considers the average p-value as its criterion of efficiency.", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": ", [12], Chapter 4 (a special case of this problem, Partition, was already one of Karp\u2019s original 21 NP-complete problems [6]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 5, "context": ", [12], Chapter 4 (a special case of this problem, Partition, was already one of Karp\u2019s original 21 NP-complete problems [6]).", "startOffset": 121, "endOffset": 124}], "year": 2016, "abstractText": "We study optimal conformity measures for various criteria of efficiency of classification in an idealised setting. This leads to an important class of criteria of efficiency that we call probabilistic; it turns out that the most standard criteria of efficiency used in literature on conformal prediction are not probabilistic unless the problem of classification is binary. We consider both unconditional and label-conditional conformal prediction. The conference version of this paper has been published in the Proceedings of COPA 2016.", "creator": "LaTeX with hyperref package"}}}