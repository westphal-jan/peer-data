{"id": "1603.04513", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2016", "title": "Multichannel Variable-Size Convolution for Sentence Classification", "abstract": "We propose MVCNN, a convolution neural network (CNN) architecture for sentence classification. It (i) combines diverse versions of pretrained word embeddings and (ii) extracts features of multigranular phrases with variable-size convolution filters. We also show that pretraining MVCNN is critical for good performance. MVCNN achieves state-of-the-art performance on four tasks: on small-scale binary, small-scale multi-class and largescale Twitter sentiment prediction and on subjectivity classification.", "histories": [["v1", "Tue, 15 Mar 2016 00:25:02 GMT  (516kb,D)", "http://arxiv.org/abs/1603.04513v1", "in Proceeding of CoNLL2015"]], "COMMENTS": "in Proceeding of CoNLL2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "hinrich sch\\\"utze"], "accepted": false, "id": "1603.04513"}, "pdf": {"name": "1603.04513.pdf", "metadata": {"source": "CRF", "title": "Multichannel Variable-Size Convolution for Sentence Classification", "authors": ["Wenpeng Yin"], "emails": ["wenpeng@cis.uni-muenchen.de"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most people are able to determine for themselves what they want and what they want. (...) It is not so that people are able to decide whether they want it or not. (...) It is not so that they want it. (...) It is not so that they do not want it. (...) It is as if they do not want it. (...) \"It is not so that they want it.\" (...). (...) \"It is not so that they want it.\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...\" It is. \"(...).\" (...). \"(...\" It is. \"(...).\" (... \"It is.\" (...). (...). \"It.\" (... \"It.\" (...). \"(.\" It. \"(...).\" It. \"(...\" It is. \"(...).\" (. \"It is.\" (... \"It.\" (...). \"It is.\" (... \"(...).\" It is. \"(.\" It. \"(...).\" (. \"It.\" (...). \"It is. (. (...).\" It is. \"(. (.\" It. (...). \"(.\" It is. (. \"It.\"). (. \"It is. (.\"). (. \"It is. (. (...).\" It. (. (. \"). (it.\"). (it. \"It is. (.\"). (. (it.). (it. \"). (it. (It is.\"). (It is. (. (it. (it.). (it. \"). (It is. (it.\"). (It is. (. (it.). \"). (It is. (It is. (it.\"). (It is. (it. (. (. (.). \"). (It is. (it. (.). (it.). (It is."}, {"heading": "2 Related Work", "text": "In fact, it is not as if we have been able to hide ourselves as we have done in the past. (...) It is not as if we have been able to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a situation, in which we have been able to survive ourselves. (...) It is not as if we have been able to survive ourselves. (...) It is not as if we have been able to survive ourselves. (...) It is not as if we are able to survive ourselves. (...) It is not as if we are able to survive ourselves. (...) It is not as if we are able to survive ourselves. (...) It is not as if we are able to survive ourselves. (...) It is not as if we are able to survive ourselves."}, {"heading": "3 Model Description", "text": "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "4 Model Enhancements", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in fact, in which they are able to"}, {"heading": "5 Experiments", "text": "We test the network on the basis of four classification tasks. First, we specify aspects of the network implementation and training. Afterwards, we report on the results of the experiments."}, {"heading": "5.1 Hyperparameters and Training", "text": "In each of the experiments, the top of the network is a logistic regression that predicts the probability distribution over classes, since the input set is given; the network is trained to minimize the transverse entropy of predicted and true distributions; the goal includes an L2 regression term over the parameters; the parameter set includes word embedding, all filter weights, and weights in fully interconnected layers; a suspension operation (Hinton et al., 2012) is placed before the logistic regression layer; the network is trained by backpropagation in mini-batches; and gradient-based optimization is performed using the AdaGrad updating rule (Duchi et al., 2011). In all datasets, the initial learning rate is 0.01, the failure probability 0.8, the L2 weight is 5 \u00b7 10 \u2212 3, the batch size 50. In each folding layer, the filter sizes {3, 5, 7, 9, and each filter size (five) are independent of the cores."}, {"heading": "5.2 Datasets and Experimental Setup", "text": "The output variable is binary in one experiment and can lead to five possible outcomes in the other: {negative, somewhat negative, neutral, reasonably positive, positive}. In the binary case, we use the given split of 6920 training, 872 development and 1821 test sets. Similarly, in the fine-grained case, we use the standard 8544 / 1101 / 2210 split. Socher et al. (2013) used the Stanford parsers (Klein and Manning, 2003) to split each sentence into subphrases. Subphrases were then labeled by human commentators as the sentences were labeled. Labeled sentences that occur as subparts of the training sets are treated as independent training instances, as in (Le and Mikolov, 2014; Kalchbrenner et al., 2014).Sentiment1402 (Go et al., 2009) are all treated as subparts of the training sets."}, {"heading": "5.2.1 Pretrained Word Vectors", "text": "In this paper, we use five embedding versions, as shown in Table 1, to initialize embedding words, four of which are downloaded directly from the Internet.2http: / / help.sentiment140.com / for-students 3http: / / www.cs.cornell.edu / people / pabo / movie-review-data / (i) HLBL. Hierarchical log-bilinear model presented by Mnih and Hinton (2009) and released by Turian et al. (2010); 4 size: 246,122 word embeddings; training corpus: RCV1 corpus, one year of Reuters English newswire from August 1996 to August 1997. (ii) Huang.5 Huang et al. (2012) integrated global context to address challenges raised by words with multiple meanings; size: 100,232 word embeddings of us; training corpus: April 2010 snapshot of Wikipedia."}, {"heading": "5.2.2 Results and Analysis", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "6 Conclusion", "text": "This paper introduced MVCNN, a novel CNN architecture for sentence classification that combines multi-channel initialization - different versions of pre-formed word embeddings are used - and variable-sized filters - features of multicranular phrases are extracted with variable-sized folding filters. We demonstrated that multi-channel initialization and variable-sized filters improve system performance in mood classification and subjectivity classification."}, {"heading": "7 Future Work", "text": "As pointed out by the critics, the success of the multi-channel approach is probably due to a combination of several very different effects.First, there is the effect of the embedded learning algorithm. These algorithms differ in many aspects, including sensitivity to the word order (e.g. SENNA: yes, word2vec: no), in objective function and in their treatment of ambiguity (explicitly modelled only by Huang et al. (2012)).Second, there is the effect of the corpus. We would expect that the size and genre of the corpus will have a large effect, even if we do not analyze this effect in this paper.Third, the complementarity of word embedding is probably more useful for some tasks than for others. Feeling is a good application for complementary word embedding, as solving this task requires heterogeneous information sources, including syntax, semantics, and genre, as well as the core polarity of a word embedding is likely to be more useful than for others."}, {"heading": "Acknowledgments", "text": "Thanks to CIS members and anonymous reviewers for constructive comments. This work was supported by Baidu (with a Baidu scholarship to Wenpeng Yin) and the German Research Foundation (DFG SCHU 2246 / 8-2, SPP 1335)."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "The expressive power of word embeddings", "author": ["Yanqing Chen", "Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena."], "venue": "ICML Workshop on Deep Learning for Audio, Speech, and Language Processing.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Modelling, visualising and summarising documents with a single convolutional neural network", "author": ["Misha Denil", "Alban Demiraj", "Nal Kalchbrenner", "Phil Blunsom", "Nando de Freitas."], "venue": "arXiv preprint arXiv:1406.3830.", "citeRegEx": "Denil et al\\.,? 2014", "shortCiteRegEx": "Denil et al\\.", "year": 2014}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["C\u0131cero Nogueira Dos Santos", "Ma\u0131ra Gatti."], "venue": "Proceedings of the 25th International Conference on Computational Linguistics.", "citeRegEx": "Santos and Gatti.,? 2014", "shortCiteRegEx": "Santos and Gatti.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang."], "venue": "CS224N Project Report, Stanford, pages 1\u201312.", "citeRegEx": "Go et al\\.,? 2009", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "A-R Mohamed", "Geoffrey Hinton."], "venue": "Acoustics, Speech and Signal Processing, 2013 IEEE International Conference on, pages 6645\u20136649. IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Not all neural embeddings are born equal", "author": ["Felix Hill", "KyungHyun Cho", "Sebastien Jean", "Coline Devin", "Yoshua Bengio."], "venue": "NIPS Workshop on Learning Semantics.", "citeRegEx": "Hill et al\\.,? 2014", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Association for Computational", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, October.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D Manning."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423\u2013430. Association for Computational Linguistics.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "Proceedings of the 31st international conference on Machine learning.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Pre-trained multi-view word embedding using two-side neural network", "author": ["Yong Luo", "Jian Tang", "Jun Yan", "Chao Xu", "Zheng Chen."], "venue": "TwentyEighth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Luo et al\\.,? 2014", "shortCiteRegEx": "Luo et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of Workshop at ICLR.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Statistical language models based on neural networks", "author": ["Tomas Mikolov."], "venue": "Presentation at Google, Mountain View, 2nd April.", "citeRegEx": "Mikolov.,? 2012", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1081\u20131088.", "citeRegEx": "Mnih and Hinton.,? 2009", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh."], "venue": "Proceedings of the 29th International Conference on Machine Learning, pages 1751\u20131758.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Bo Pang", "Lillian Lee."], "venue": "Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Com-", "citeRegEx": "Pang and Lee.,? 2004", "shortCiteRegEx": "Pang and Lee.", "year": 2004}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing, 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng."], "venue": "Advances in Neural Information Processing Systems, pages 801\u2013809.", "citeRegEx": "Socher et al\\.,? 2011a", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Socher et al\\.,? 2011b", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the conference on em-", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394. Association for", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D Manning."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 90\u201394. As-", "citeRegEx": "Wang and Manning.,? 2012", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Fast dropout training", "author": ["Sida Wang", "Christopher Manning."], "venue": "Proceedings of the 30th International Conference on Machine Learning, pages 118\u2013126.", "citeRegEx": "Wang and Manning.,? 2013", "shortCiteRegEx": "Wang and Manning.", "year": 2013}, {"title": "An exploration of embeddings for generalized phrases", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 52nd annual meeting of the association for computational linguistics, student research workshop, pages 41\u201347.", "citeRegEx": "Yin and Sch\u00fctze.,? 2014", "shortCiteRegEx": "Yin and Sch\u00fctze.", "year": 2014}, {"title": "Deep learning for answer sentence selection", "author": ["Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman."], "venue": "NIPS deep learning workshop.", "citeRegEx": "Yu et al\\.,? 2014", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "In recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al.", "startOffset": 90, "endOffset": 115}, {"referenceID": 9, "context": ", 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008).", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": ", 2013) and NLP (Collobert and Weston, 2008).", "startOffset": 16, "endOffset": 44}, {"referenceID": 13, "context": "Hence, convolution neural networks (CNN) are getting increasing attention, for they are able to model long-range dependencies in sentences via hierarchical structures (Dos Santos and Gatti, 2014; Kim, 2014; Denil et al., 2014).", "startOffset": 167, "endOffset": 226}, {"referenceID": 5, "context": "Hence, convolution neural networks (CNN) are getting increasing attention, for they are able to model long-range dependencies in sentences via hierarchical structures (Dos Santos and Gatti, 2014; Kim, 2014; Denil et al., 2014).", "startOffset": 167, "endOffset": 226}, {"referenceID": 10, "context": ", 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees.", "startOffset": 8, "endOffset": 69}, {"referenceID": 0, "context": "guage models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a).", "startOffset": 13, "endOffset": 117}, {"referenceID": 22, "context": "guage models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a).", "startOffset": 13, "endOffset": 117}, {"referenceID": 18, "context": "guage models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a).", "startOffset": 13, "endOffset": 117}, {"referenceID": 21, "context": "guage models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a).", "startOffset": 13, "endOffset": 117}, {"referenceID": 19, "context": "guage models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a).", "startOffset": 13, "endOffset": 117}, {"referenceID": 22, "context": "(2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al.", "startOffset": 21, "endOffset": 44}, {"referenceID": 3, "context": "(2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al.", "startOffset": 52, "endOffset": 80}, {"referenceID": 30, "context": "(2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al.", "startOffset": 89, "endOffset": 110}, {"referenceID": 11, "context": ", 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions.", "startOffset": 18, "endOffset": 38}, {"referenceID": 2, "context": "For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 2, "context": "For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. Hill et al. (2014) showed that embeddings learned by neural machine translation models outperform three reprear X iv :1 60 3.", "startOffset": 13, "endOffset": 301}, {"referenceID": 20, "context": "gram (Mikolov et al., 2013b), GloVe (Pennington et al.", "startOffset": 5, "endOffset": 28}, {"referenceID": 25, "context": ", 2013b), GloVe (Pennington et al., 2014) and C&W (Collobert et al.", "startOffset": 16, "endOffset": 41}, {"referenceID": 4, "context": ", 2014) and C&W (Collobert et al., 2011) in some cases.", "startOffset": 16, "endOffset": 40}, {"referenceID": 33, "context": "Yin and Sch\u00fctze (2014) extended this approach by composing on words and phrases instead of only single words.", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "(2014) adapted CBOW (Mikolov et al., 2013a) to train word embeddings on different datasets: free text documents from Wikipedia, search click-through data and user query data, showing that combining them gets stronger results than using individual word embeddings in web search ranking and word similarity task.", "startOffset": 20, "endOffset": 43}, {"referenceID": 30, "context": "However, these two papers either learned word representations on the same corpus (Turian et al., 2010) or enhanced the embedding quality by extending training corpora, not learning algorithms (Luo et", "startOffset": 81, "endOffset": 102}, {"referenceID": 25, "context": "For example, Turian et al. (2010) compared Brown clusters, C&W embeddings and HLBL embeddings in NER and chunking tasks.", "startOffset": 13, "endOffset": 34}, {"referenceID": 17, "context": "Luo et al. (2014) adapted CBOW (Mikolov et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "In each layer, there are usually multiple filters of the same size, but with different weights (Kalchbrenner et al., 2014).", "startOffset": 95, "endOffset": 122}, {"referenceID": 12, "context": "DCNN in (Kalchbrenner et al., 2014) used onedimensional convolution: each higher-order feature is produced from values of a single dimension in the lower-layer feature map.", "startOffset": 8, "endOffset": 35}, {"referenceID": 3, "context": "(2014) pool the k most active features compared with simple max (1-max) pooling (Collobert and Weston, 2008).", "startOffset": 80, "endOffset": 108}, {"referenceID": 12, "context": "L} is the order of convolution layer from bottom to top in Figure 1; L is the total numbers of convolution layers; ktop is a constant determined empirically, we set it to 4 as (Kalchbrenner et al., 2014).", "startOffset": 176, "endOffset": 203}, {"referenceID": 12, "context": "(Kalchbrenner et al., 2014) to use dynamic kmax pooling to stack multiple convolution layers, and gets insight from (Kim, 2014) to investigate variable-size filters in a convolution layer.", "startOffset": 0, "endOffset": 27}, {"referenceID": 13, "context": ", 2014) to use dynamic kmax pooling to stack multiple convolution layers, and gets insight from (Kim, 2014) to investigate variable-size filters in a convolution layer.", "startOffset": 96, "endOffset": 107}, {"referenceID": 12, "context": "Compared to (Kalchbrenner et al., 2014), MVCNN has rich feature maps as input and as output of each convolution layer.", "startOffset": 12, "endOffset": 39}, {"referenceID": 13, "context": "MVCNN extends the network in (Kim, 2014) by hierarchical convolution architecture and", "startOffset": 29, "endOffset": 40}, {"referenceID": 23, "context": "Then, this resulting vector is treated as a predicted representation of the middle word and is used to find the true middle word by means of noise-contrastive estimation (NCE) (Mnih and Teh, 2012).", "startOffset": 176, "endOffset": 196}, {"referenceID": 7, "context": "ing the AdaGrad update rule (Duchi et al., 2011)", "startOffset": 28, "endOffset": 48}, {"referenceID": 29, "context": "Standard Sentiment Treebank (Socher et al., 2013).", "startOffset": 28, "endOffset": 49}, {"referenceID": 14, "context": "(2013) used the Stanford Parser (Klein and Manning, 2003) to parse each sentence into subphrases.", "startOffset": 32, "endOffset": 57}, {"referenceID": 25, "context": "Socher et al. (2013) used the Stanford Parser (Klein and Manning, 2003) to parse each sentence into subphrases.", "startOffset": 0, "endOffset": 21}, {"referenceID": 16, "context": "Labeled phrases that occur as subparts of the training sentences are treated as independent training instances as in (Le and Mikolov, 2014; Kalchbrenner et al., 2014).", "startOffset": 117, "endOffset": 166}, {"referenceID": 12, "context": "Labeled phrases that occur as subparts of the training sentences are treated as independent training instances as in (Le and Mikolov, 2014; Kalchbrenner et al., 2014).", "startOffset": 117, "endOffset": 166}, {"referenceID": 8, "context": "Sentiment1402 (Go et al., 2009).", "startOffset": 14, "endOffset": 31}, {"referenceID": 24, "context": "Subjectivity classification dataset3 released by (Pang and Lee, 2004) has 5000 subjective sentences and 5000 objective sentences.", "startOffset": 49, "endOffset": 69}, {"referenceID": 22, "context": "Hierarchical log-bilinear model presented by Mnih and Hinton (2009) and released by Turian et al.", "startOffset": 45, "endOffset": 68}, {"referenceID": 22, "context": "Hierarchical log-bilinear model presented by Mnih and Hinton (2009) and released by Turian et al. (2010);4 size: 246,122 word em-", "startOffset": 45, "endOffset": 105}, {"referenceID": 11, "context": "5 Huang et al. (2012) incorporated global context to deal with challenges raised by words with multiple meanings; size: 100,232 word embeddings; training corpus: April 2010 snapshot of Wikipedia.", "startOffset": 2, "endOffset": 22}, {"referenceID": 27, "context": "baselines 1 RAE (Socher et al., 2011b) 82.", "startOffset": 16, "endOffset": 38}, {"referenceID": 28, "context": "2 \u2013 \u2013 2 MV-RNN (Socher et al., 2012) 82.", "startOffset": 15, "endOffset": 36}, {"referenceID": 29, "context": "4 \u2013 \u2013 3 RNTN (Socher et al., 2013) 85.", "startOffset": 13, "endOffset": 34}, {"referenceID": 12, "context": "7 \u2013 \u2013 4 DCNN (Kalchbrenner et al., 2014) 86.", "startOffset": 13, "endOffset": 40}, {"referenceID": 16, "context": "4 \u2013 5 Paragraph-Vec (Le and Mikolov, 2014) 87.", "startOffset": 20, "endOffset": 42}, {"referenceID": 13, "context": "7 \u2013 \u2013 6 CNN-rand (Kim, 2014) 82.", "startOffset": 17, "endOffset": 28}, {"referenceID": 13, "context": "6 7 CNN-static (Kim, 2014) 86.", "startOffset": 15, "endOffset": 26}, {"referenceID": 13, "context": "0 8 CNN-non-static (Kim, 2014) 87.", "startOffset": 19, "endOffset": 30}, {"referenceID": 13, "context": "4 9 CNN-multichannel (Kim, 2014) 88.", "startOffset": 21, "endOffset": 32}, {"referenceID": 31, "context": "2 10 NBSVM (Wang and Manning, 2012) \u2013 \u2013 \u2013 93.", "startOffset": 11, "endOffset": 35}, {"referenceID": 31, "context": "2 11 MNB (Wang and Manning, 2012) \u2013 \u2013 \u2013 93.", "startOffset": 9, "endOffset": 33}, {"referenceID": 32, "context": "6 12 G-Dropout (Wang and Manning, 2013) \u2013 \u2013 \u2013 93.", "startOffset": 15, "endOffset": 39}, {"referenceID": 32, "context": "4 13 F-Dropout (Wang and Manning, 2013) \u2013 \u2013 \u2013 93.", "startOffset": 15, "endOffset": 39}, {"referenceID": 8, "context": "6 14 SVM (Go et al., 2009) \u2013 \u2013 81.", "startOffset": 9, "endOffset": 26}, {"referenceID": 8, "context": "6 \u2013 15 BINB (Go et al., 2009) \u2013 \u2013 82.", "startOffset": 12, "endOffset": 29}, {"referenceID": 12, "context": "7 \u2013 16 MAX-TDNN (Kalchbrenner et al., 2014) \u2013 \u2013 78.", "startOffset": 16, "endOffset": 43}, {"referenceID": 12, "context": "8 \u2013 17 NBOW (Kalchbrenner et al., 2014) \u2013 \u2013 80.", "startOffset": 12, "endOffset": 39}, {"referenceID": 8, "context": "18 MAXENT (Go et al., 2009) \u2013 \u2013 83.", "startOffset": 10, "endOffset": 27}, {"referenceID": 27, "context": "RAE: Recursive Autoencoders with pretrained word embeddings from Wikipedia (Socher et al., 2011b).", "startOffset": 75, "endOffset": 97}, {"referenceID": 28, "context": "MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012).", "startOffset": 64, "endOffset": 85}, {"referenceID": 29, "context": "RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013).", "startOffset": 89, "endOffset": 110}, {"referenceID": 3, "context": "DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al.", "startOffset": 121, "endOffset": 149}, {"referenceID": 12, "context": "DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014).", "startOffset": 178, "endOffset": 205}, {"referenceID": 16, "context": "Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014).", "startOffset": 63, "endOffset": 85}, {"referenceID": 8, "context": "SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009).", "startOffset": 115, "endOffset": 132}, {"referenceID": 13, "context": "CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a \u201cchannel\u201d) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training (Kim, 2014).", "startOffset": 305, "endOffset": 316}, {"referenceID": 3, "context": "DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a \u201cchannel\u201d) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training (Kim, 2014).", "startOffset": 122, "endOffset": 530}, {"referenceID": 3, "context": "DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a \u201cchannel\u201d) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training (Kim, 2014). G-Dropout, F-Dropout: Gaussian Dropout and Fast Dropout from Wang and Manning (2013). Minus sign \u201c-\u201d in MVCNN (-Huang) etc.", "startOffset": 122, "endOffset": 934}, {"referenceID": 11, "context": "SENNA: yes, word2vec: no), in objective function and in their treatment of ambiguity (explicitly modeled only by Huang et al. (2012).", "startOffset": 113, "endOffset": 133}], "year": 2016, "abstractText": "We propose MVCNN, a convolution neural network (CNN) architecture for sentence classification. It (i) combines diverse versions of pretrained word embeddings and (ii) extracts features of multigranular phrases with variable-size convolution filters. We also show that pretraining MVCNN is critical for good performance. MVCNN achieves state-of-the-art performance on four tasks: on small-scale binary, small-scale multi-class and largescale Twitter sentiment prediction and on subjectivity classification.", "creator": "TeX"}}}