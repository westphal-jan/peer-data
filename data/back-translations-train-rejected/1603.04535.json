{"id": "1603.04535", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2016", "title": "Learning Domain-Invariant Subspace using Domain Features and Independence Maximization", "abstract": "When the distributions of the source and the target domains are different, domain adaptation techniques are needed. For example, in the field of sensors and measurement, discrete and continuous distributional change often exist in data because of instrumental variation and time-varying sensor drift. In this paper, we propose maximum independence domain adaptation (MIDA) to address this problem. Domain features are first defined to describe the background information of a sample, such as the device label and acquisition time. Then, MIDA learns features which have maximal independence with the domain features, so as to reduce the inter-domain discrepancy in distributions. A feature augmentation strategy is designed so that the learned projection is background-specific. Semi-supervised MIDA (SMIDA) extends MIDA by exploiting the label information. The proposed methods can handle not only discrete domains in traditional domain adaptation problems but also continuous distributional change such as the time-varying drift. In addition, they are naturally applicable in supervised/semi-supervised/unsupervised classification or regression problems with multiple domains. This flexibility brings potential for a wide range of applications. The effectiveness of our approaches is verified by experiments on synthetic datasets and four real-world ones on sensors, measurement, and computer vision.", "histories": [["v1", "Tue, 15 Mar 2016 02:56:22 GMT  (717kb,D)", "http://arxiv.org/abs/1603.04535v1", "13 pages, 9 figures, 6 tables"], ["v2", "Thu, 22 Jun 2017 01:39:22 GMT  (2544kb,D)", "http://arxiv.org/abs/1603.04535v2", "Accepted"]], "COMMENTS": "13 pages, 9 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["ke yan", "lu kou", "david zhang"], "accepted": false, "id": "1603.04535"}, "pdf": {"name": "1603.04535.pdf", "metadata": {"source": "CRF", "title": "Domain Adaptation via Maximum Independence of Domain Features", "authors": ["Ke Yan", "Lu Kou"], "emails": ["yank10@mails.tsinghua.edu.cn).", "cslkou@comp.polyu.edu.hk).", "csdzhang@comp.polyu.edu.hk)."], "sections": [{"heading": null, "text": "It is the time in which the people in the USA, in Europe, in the USA, in Europe, in Europe, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "II. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Unsupervised domain adaptation", "text": "In this section, we will focus on typical methods of extracting domain invariant traits. In comparison, researchers have developed many strategies: Some algorithms project all samples onto a common latent space [5], [16] to reduce the difference in distributions between domains while obtaining useful information at the same time; some algorithms project all samples onto a common latent space [16], [19]; and Distribution Component Analysis (TCA) [5] attempts to capture the transfer components in a reproducing kernel."}, {"heading": "B. Hilbert-Schmidt independence criterion (HSIC)", "text": "HSIC is used as a practical method to measure the dependence between two samples X and Y. Let kx and ky be two core functions associated with the RKHSs F and G, respectively.. pxy is the common distribution. HSIC is defined as the square of the Hilbert Schmidt norm of the cross covariance operator Cxy [20]: HSIC (pxy, F, G) = \u0441 Cxy-2HS = Exx \u2032 yy \u2032 [kx (x, x \u2032) ky (y, y \u2032)] + Exx \u2032 [kx (x, x \u2032)]] Eyy \u2032 (kx (y, y \u2032)] \u2212 2Exy [kx (x, x \u2212)] Ey \u2032 [ky (y, y \u2032)]]. Here is Exx \u2032 yy \u2032 the expectation towards independent pairs (x, y \u2032) and (x \u2032, y \u2032) the dependence on Kxy."}, {"heading": "III. PROPOSED METHOD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Domain Feature", "text": "We want to reduce the dependence between the extracted characteristics and the background information; (1) the background information of a sample should be present, so it can be easily obtained; (2) has different distributions in training and test samples; (3) correlates with the distribution of the original characteristics; minimizing the aforementioned dependence is therefore desirable; and first, a group of new characteristics needs to be designed to describe the background information; the characteristics are referred to as \"domain characteristics.\" From the perspective of calibration transfer and drift correction, there are two main types of background information: device identification (with which device the sample was collected) and acquisition time (when the sample was collected)."}, {"heading": "B. Feature Augmentation", "text": "In fact, the mentioned seeds of the target domain in which they are located are as if they were able to make a prediction. In this way, the learned model can be considered as two different models for the two domains. In the meantime, the two models share a common component. The strategy requires that the data lie in discrete domains and cannot handle time-changing drifts. We present a more general and efficient prediction strategy."}, {"heading": "C. Maximum Independence Domain Adaptation (MIDA)", "text": "In this section we will present the formulation of the MIDA in detail. Suppose X-Rm \u00b7 n is the matrix of the n samples (1). The training and the test samples are merged. More importantly, we do not need to explicitly distinguish from which domain a sample comes from. Based on the kernel trick, we do not need to know the exact form of knowledge, but the inner product of the Kx can be represented by the kernel matrix Kx. (X) Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx-Kx. Then a projection matrix-W is applied to a new space. (X) We are applied to a subspace with the dimension h, which leads to the projected samples."}, {"heading": "D. Semi-supervised MIDA (SMIDA)", "text": "The biggest advantage of this strategy is that all kinds of labels can be exploited, such as the discrete labels in classification and the continuous ones in regression. The label matrix Y is defined as follows: The label matrix Y is equivalent. For c class classification problems, the one-hot coding scheme can be used, i.e. Y-Rn-c, j = 1, if the problems are labeled with class Y."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we will first conduct experiments on some synthetic datasets to verify the effectiveness of the proposed methods, and then perform calibration transfer and drift correction experiments on two E-nose datasets and a spectroscopy dataset. To demonstrate the universality of the proposed methods, we will further evaluate them on a dataset for visual object recognition, comparing them with newer, unattended adaptation algorithms at the feature level."}, {"heading": "A. Synthetic Dataset", "text": "Figure 1 compares TCA [5] and MIDA on a 2D dataset with two separate domains, thus playing a good role. In both methods, the linear kernel was used on the original characteristics and the hyper parameter \u00b5 was set to 1. However, in order to quantify the impact of domain matching, logistic regression models were trained on the first projected dimension and tested on the target data. Accuracy is shown in the caption, which shows that the order of performance is MIDA > TCA > original characteristics. However, TCA only aligns the two domains on the first projected dimension. However, we can find that the two classes have large overlaps in this dimension, because the direction of alignment is different from that of discrimination. Including the identification information of the source domain (SSTCA) does not help. On the contrary, MIDA can align the two domains well in both projected dimensions that are pre-domain specific to those taken in the domains."}, {"heading": "B. Gas Sensor Array Drift Dataset", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "C. Breath Analysis Dataset", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "D. Corn Dataset", "text": "In fact, the fact is that most of them are able to hold their own, and that they are able to hold their own."}, {"heading": "V. CONCLUSION", "text": "The main idea of MIDA is to reduce the discrepancy between domains by maximizing the independence between the learned characteristics and the domain characteristics of the samples. Domain characteristics describe the background information of each sample, such as the domain label in traditional domain fitting problems. In the field of sensors and measurements, the device label and the acquisition time of each captured sample can be expressed by the domain characteristics, so that uncontrolled calibration transfer and drift correction can be achieved through the use of MIDA. Finally, the feature suction strategy proposed in this paper adds domain-specific distortions to the learned characteristics, helping MIDA to align domains. It is also useful when there is a change in conditional probability. Finally, to include the identification information, the semi-visioned MIDA (SMIDA) is more widespread."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank the providers of the data sets used in this article."}], "references": [{"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual domain adaptation: A survey of recent advances", "author": ["V.M. Patel", "R. Gopalan", "R. Li", "R. Chellappa"], "venue": "Signal Processing Magazine, IEEE, vol. 32, no. 3, pp. 53\u201369, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Flowing on riemannian manifold: Domain adaptation by shifting covariance", "author": ["Z. Cui", "W. Li", "D. Xu", "S. Shan", "X. Chen", "X. Li"], "venue": "IEEE Trans. Cybern., vol. 44, no. 12, pp. 2264\u20132273, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Cross-domain human action recognition", "author": ["W. Bian", "D. Tao", "Y. Rui"], "venue": "Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, vol. 42, no. 2, pp. 298\u2013307, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "Neural Networks, IEEE Transactions on, vol. 22, no. 2, pp. 199\u2013210, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Combating negative transfer from predictive distribution differences", "author": ["C.-W. Seah", "Y.-S. Ong", "I.W. Tsang"], "venue": "IEEE Trans. Cybern., vol. 43, no. 4, pp. 1153\u20131165, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A brief history of electronic noses", "author": ["J.W. Gardner", "P.N. Bartlett"], "venue": "Sens. Actuators B: Chem., vol. 18, no. 1, pp. 210\u2013211, 1994.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1994}, {"title": "Electronic nose: current status and future trends", "author": ["F. R\u00f6ck", "N. Barsan", "U. Weimar"], "venue": "Chem. Rev., vol. 108, no. 2, pp. 705\u2013725, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Gas distribution mapping of multiple odour sources using a mobile robot", "author": ["A. Loutfi", "S. Coradeschi", "A.J. Lilienthal", "J. Gonzalez"], "venue": "Robotica, vol. 27, no. 02, pp. 311\u2013319, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "On-line sensor calibration transfer among electronic nose instruments for monitoring volatile organic chemicals in indoor air quality", "author": ["L. Zhang", "F. Tian", "C. Kadri", "B. Xiao", "H. Li", "L. Pan", "H. Zhou"], "venue": "Sens. Actuators: B. Chem., vol. 160, no. 1, pp. 899\u2013909, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Design of a breath analysis system for diabetes screening and blood glucose level prediction", "author": ["K. Yan", "D. Zhang", "D. Wu", "H. Wei", "G. Lu"], "venue": "IEEE Trans. Biomed. Eng., vol. 61, no. 11, pp. 2787\u20132795, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Signal and data processing for machine olfaction and chemical sensing: a review", "author": ["S. Marco", "A. Guti\u00e9rrez-G\u00e1lvez"], "venue": "IEEE Sens. J., vol. 12, no. 11, pp. 3189\u20133214, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Drift correction methods for gas chemical sensors in artificial olfaction systems: techniques and challenges", "author": ["S. Di Carlo", "M. Falasconi"], "venue": "InTech,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Improving the transfer ability of prediction models for electronic noses", "author": ["K. Yan", "D. Zhang"], "venue": "Sens. Actuators B: Chem., vol. 220, pp. 115\u2013124, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Calibration transfer and drift compensation of e-noses via coupled task learning", "author": ["\u2014\u2014"], "venue": "Sens. Actuators B: Chem., vol. 225, pp. 288\u2013297, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Information-theoretical learning of discriminative clusters for unsupervised domain adaptation", "author": ["Y. Shi", "F. Sha"], "venue": "Proceedings of the Intl. Conf. on Machine Learning (ICML), 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["B. Fernando", "A. Habrard", "M. Sebban", "T. Tuytelaars"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 2960\u2013 2967.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning kernels for unsupervised  SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS  13 domain adaptation with applications to visual object recognition", "author": ["B. Gong", "K. Grauman", "F. Sha"], "venue": "International Journal of Computer Vision, vol. 109, no. 1-2, pp. 3\u201327, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Generalized transfer subspace learning through low-rank constraint", "author": ["M. Shao", "D. Kit", "Y. Fu"], "venue": "International Journal of Computer Vision, vol. 109, no. 1-2, pp. 74\u201393, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Measuring statistical dependence with hilbert-schmidt norms", "author": ["A. Gretton", "O. Bousquet", "A. Smola", "B. Schlkopf"], "venue": "Algorithmic learning theory. Springer, 2005, pp. 63\u201377.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Transfer of multivariate calibration models: a review", "author": ["R.N. Feudale", "N.A. Woody", "H. Tan", "A.J. Myles", "S.D. Brown", "J. Ferr\u00e9"], "venue": "Chemometr. Intell. Lab., vol. 64, no. 2, pp. 181\u2013192, 2002.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 2066\u2013 2073.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Drift compensation for electronic nose by semi-supervised domain adaption", "author": ["Q. Liu", "X. Li", "M. Ye", "S.S. Ge", "X. Du"], "venue": "IEEE Sens. J., vol. 14, no. 3, pp. 657\u2013665, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature selection via dependence maximization", "author": ["L. Song", "A. Smola", "A. Gretton", "J. Bedo", "K. Borgwardt"], "venue": "J. Mach. Learn. Res., vol. 13, no. 1, pp. 1393\u20131434, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Colored maximum variance unfolding", "author": ["L. Song", "A. Gretton", "K.M. Borgwardt", "A.J. Smola"], "venue": "Advances in neural information processing systems, 2007, pp. 1385\u20131392.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Supervised principal component analysis: Visualization, classification and regression on subspaces and submanifolds", "author": ["E. Barshan", "A. Ghodsi", "Z. Azimifar", "M.Z. Jahromi"], "venue": "Pattern Recogn., vol. 44, no. 7, pp. 1357\u20131371, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Frustratingly easy domain adaptation", "author": ["III H. Daum"], "venue": "Proc. 45th Ann. Meeting of the Assoc. for Computational Linguistics, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Schlkopf", "A. Smola", "K.-R. Mller"], "venue": "Neural computation, vol. 10, no. 5, pp. 1299\u20131319, 1998.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Fisher discriminant analysis with kernels", "author": ["B. Scholkopft", "K.-R. Mullert"], "venue": "Neural networks for signal processing, vol. IX, pp. 41\u201348, 1999.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Finding stationary subspaces in multivariate time series", "author": ["P. Von B\u00fcnau", "F.C. Meinecke", "F.C. Kir\u00e1ly", "K.-R. M\u00fcller"], "venue": "Physical review letters, vol. 103, no. 21, p. 214101, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey on concept drift adaptation", "author": ["J. a. Gama", "I. \u017dliobait\u0117", "A. Bifet", "M. Pechenizkiy", "A. Bouchachia"], "venue": "ACM Computing Surveys (CSUR), vol. 46, no. 4, p. 44, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2399\u20132434, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Chemical gas sensor drift compensation using classifier ensembles", "author": ["A. Vergara", "S. Vembu", "T. Ayhan", "M.A. Ryan", "M.L. Homer", "R. Huerta"], "venue": "Sens. Actuators B: Chem., vol. 166, pp. 320\u2013329, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Therefore, domain adaptation or transfer learning is needed to improve the performance in the target domain by leveraging unlabeled (and maybe a few labeled) target samples [1].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "This topic is receiving increasing attention in recent years due to its broad applications such as computer vision [2], [3], [4] and text classification [5], [6].", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "This topic is receiving increasing attention in recent years due to its broad applications such as computer vision [2], [3], [4] and text classification [5], [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "This topic is receiving increasing attention in recent years due to its broad applications such as computer vision [2], [3], [4] and text classification [5], [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "This topic is receiving increasing attention in recent years due to its broad applications such as computer vision [2], [3], [4] and text classification [5], [6].", "startOffset": 153, "endOffset": 156}, {"referenceID": 5, "context": "This topic is receiving increasing attention in recent years due to its broad applications such as computer vision [2], [3], [4] and text classification [5], [6].", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "A typical application plagued by this problem is machine olfaction, which uses electronic noses (e-noses) and pattern recognition algorithms to predict the type and concentration of odors [7].", "startOffset": 188, "endOffset": 191}, {"referenceID": 7, "context": "The applications of machine olfaction range from agriculture and food to environmental monitoring, robotics, biometrics, and disease analysis [8], [9], [10], [11].", "startOffset": 142, "endOffset": 145}, {"referenceID": 8, "context": "The applications of machine olfaction range from agriculture and food to environmental monitoring, robotics, biometrics, and disease analysis [8], [9], [10], [11].", "startOffset": 147, "endOffset": 150}, {"referenceID": 9, "context": "The applications of machine olfaction range from agriculture and food to environmental monitoring, robotics, biometrics, and disease analysis [8], [9], [10], [11].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "The applications of machine olfaction range from agriculture and food to environmental monitoring, robotics, biometrics, and disease analysis [8], [9], [10], [11].", "startOffset": 158, "endOffset": 162}, {"referenceID": 11, "context": "However, owing to the nature of chemical sensors, many e-noses are prone to instrumental variation and time-varying drift mentioned above [12], [13], which greatly hamper their usage in real-world applications.", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "However, owing to the nature of chemical sensors, many e-noses are prone to instrumental variation and time-varying drift mentioned above [12], [13], which greatly hamper their usage in real-world applications.", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "They should be collected with each device and in each time period so as to provide mapping information between the source and the target domains [12], [10], [14], [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 9, "context": "They should be collected with each device and in each time period so as to provide mapping information between the source and the target domains [12], [10], [14], [15].", "startOffset": 151, "endOffset": 155}, {"referenceID": 13, "context": "They should be collected with each device and in each time period so as to provide mapping information between the source and the target domains [12], [10], [14], [15].", "startOffset": 157, "endOffset": 161}, {"referenceID": 14, "context": "They should be collected with each device and in each time period so as to provide mapping information between the source and the target domains [12], [10], [14], [15].", "startOffset": 163, "endOffset": 167}, {"referenceID": 9, "context": "Then, a widely-used method is to map the features in the target domain to the source domain with regression algorithms [10], [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 13, "context": "Then, a widely-used method is to map the features in the target domain to the source domain with regression algorithms [10], [14].", "startOffset": 125, "endOffset": 129}, {"referenceID": 4, "context": "to learn domain-invariant feature representation [5], [16], [17], [3], [18], [19].", "startOffset": 49, "endOffset": 52}, {"referenceID": 15, "context": "to learn domain-invariant feature representation [5], [16], [17], [3], [18], [19].", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "to learn domain-invariant feature representation [5], [16], [17], [3], [18], [19].", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": "to learn domain-invariant feature representation [5], [16], [17], [3], [18], [19].", "startOffset": 66, "endOffset": 69}, {"referenceID": 17, "context": "to learn domain-invariant feature representation [5], [16], [17], [3], [18], [19].", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "to learn domain-invariant feature representation [5], [16], [17], [3], [18], [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 4, "context": "[5] proposed transfer component analysis (TCA), which finds a latent feature space that minimizes the difference of distributions between two domains in the sense of maximum mean discrepancy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Then, it finds a latent feature space in which the samples and their domain features are maximally independent in the sense of Hilbert-Schmidt independence criterion (HSIC) [20].", "startOffset": 173, "endOffset": 177}, {"referenceID": 20, "context": "Note that spectrometers suffer the same instrumental variation problem as e-noses [21].", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "Finally, a domain adaptation experiment is conducted on a well-known object recognition benchmark [22].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "Two good surveys on domain adaptation can be found in [1] and [2].", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Two good surveys on domain adaptation can be found in [1] and [2].", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "Some algorithms project all samples to a common latent space [5], [16], [19].", "startOffset": 61, "endOffset": 64}, {"referenceID": 15, "context": "Some algorithms project all samples to a common latent space [5], [16], [19].", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "Some algorithms project all samples to a common latent space [5], [16], [19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 4, "context": "Transfer component analysis (TCA) [5] tries to learn transfer components across domains in a reproducing kernel Hilbert space (RKHS) using maximum mean discrepancy.", "startOffset": 34, "endOffset": 37}, {"referenceID": 15, "context": "[16] measured domain difference by the mutual information between all samples and their binary domain labels, which can be viewed as a primitive version of the domain features used in this paper.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The low-rank transfer subspace learning (LTSL) algorithm presented in [19] is a reconstruction guided knowledge transfer method.", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "Another class of methods first project the source and the target data into separate subspaces, and then build connections between them [17], [22], [23], [3].", "startOffset": 135, "endOffset": 139}, {"referenceID": 21, "context": "Another class of methods first project the source and the target data into separate subspaces, and then build connections between them [17], [22], [23], [3].", "startOffset": 141, "endOffset": 145}, {"referenceID": 22, "context": "Another class of methods first project the source and the target data into separate subspaces, and then build connections between them [17], [22], [23], [3].", "startOffset": 147, "endOffset": 151}, {"referenceID": 2, "context": "Another class of methods first project the source and the target data into separate subspaces, and then build connections between them [17], [22], [23], [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 16, "context": "[17] utilized a transformation matrix to map the source subspace to the target one, where a subspace was represented by eigenvectors of PCA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The geodesic flow kernel (GFK) method [22] measures the geometric distance between two different domains in a Grassmann manifold by constructing a geodesic flow.", "startOffset": 38, "endOffset": 42}, {"referenceID": 22, "context": "[23] adapted GFK for drift correction of enoses.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Another improvement of GFK is domain adaptation by shifting covariance (DASC) [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 19, "context": "HSIC is defined as the square of the Hilbert-Schmidt norm of the cross-covariance operator Cxy [20]:", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "It can be proved that with characteristic kernels kx and ky , HSIC(pxy,F ,G) is zero if and only if x and y are independent [24].", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": ", (xn, yn)}, Kx,Ky \u2208 Rn\u00d7n are the kernel matrices of X and Y , respectively, then [20]:", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "Due to its simplicity and power, HSIC has been adopted for feature extraction [25], [5], [26] and feature selection [24].", "startOffset": 78, "endOffset": 82}, {"referenceID": 4, "context": "Due to its simplicity and power, HSIC has been adopted for feature extraction [25], [5], [26] and feature selection [24].", "startOffset": 84, "endOffset": 87}, {"referenceID": 25, "context": "Due to its simplicity and power, HSIC has been adopted for feature extraction [25], [5], [26] and feature selection [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "Due to its simplicity and power, HSIC has been adopted for feature extraction [25], [5], [26] and feature selection [24].", "startOffset": 116, "endOffset": 120}, {"referenceID": 26, "context": "In [27], the author proposed a feature augmentation strategy for domain adaptation: if a sample x \u2208 R is from the source domain, then its augmented feature vector is x\u0302 = \uf8ee\uf8f0 xx", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "Meanwhile, the two models share a common component [27].", "startOffset": 51, "endOffset": 55}, {"referenceID": 27, "context": "Similar to other kernel dimensionality reduction algorithms [28], [29], the key idea is to express each projection direction as a linear combination of all samples in the space, namely W\u0303 = \u03a6(X)W .", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "Similar to other kernel dimensionality reduction algorithms [28], [29], the key idea is to express each projection direction as a linear combination of all samples in the space, namely W\u0303 = \u03a6(X)W .", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "In domain adaptation, the goal is not only minimizing the difference of distributions, but also preserving important properties of data, such as the variance [5].", "startOffset": 158, "endOffset": 161}, {"referenceID": 25, "context": "Note that a conventional constraint is requiring W\u0303 to be orthonormal as in [26], which will lead to a generalized eigenvector problem.", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "Different kernels indicate different assumptions on the type of dependence in using HSIC [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "According to [24], the polynomial and RBF kernels map the original features to a higher or infinite dimensional space, thus are able to detect more types of dependence.", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "However, choosing a suitable kernel width parameter (\u03c3) is also important for these more powerful kernels [24].", "startOffset": 106, "endOffset": 110}, {"referenceID": 4, "context": "The maximum mean discrepancy (MMD) criterion is used in TCA [5] to measure the difference between two distributions.", "startOffset": 60, "endOffset": 63}, {"referenceID": 23, "context": "[24] showed that when HSIC and MMD are both applied to measure the dependence of features and labels in a binary-class classification problem, they are identical up to a constant factor if the label kernel matrix in HSIC is properly designed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The stationary subspace analysis (SSA) algorithm [30] is able to identify temporally stationary components in multivariate time series.", "startOffset": 49, "endOffset": 53}, {"referenceID": 30, "context": "Concept drift adaptation algorithms [31] are able to correct continuous time-varying drift.", "startOffset": 36, "endOffset": 40}, {"referenceID": 24, "context": "Similar to [25], [5], [26], [24], HSIC is adopted to maximize the dependence between the projected features and the labels.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "Similar to [25], [5], [26], [24], HSIC is adopted to maximize the dependence between the projected features and the labels.", "startOffset": 17, "endOffset": 20}, {"referenceID": 25, "context": "Similar to [25], [5], [26], [24], HSIC is adopted to maximize the dependence between the projected features and the labels.", "startOffset": 22, "endOffset": 26}, {"referenceID": 23, "context": "Similar to [25], [5], [26], [24], HSIC is adopted to maximize the dependence between the projected features and the labels.", "startOffset": 28, "endOffset": 32}, {"referenceID": 31, "context": "Besides variance and label dependence, another useful property of data is the geometry structure, which can be preserved by manifold regularization (MR) [32].", "startOffset": 153, "endOffset": 157}, {"referenceID": 31, "context": "MR can be conveniently incorporated into SMIDA by adding a regularizer \u2212\u03bb tr(WKxLKxW ) into (12), where L is the graph Laplacian matrix [32], \u03bb > 0 is a trade-off hyperparameter.", "startOffset": 136, "endOffset": 140}, {"referenceID": 4, "context": "1, TCA [5] and MIDA are compared on a 2D dataset with two discrete domains.", "startOffset": 7, "endOffset": 10}, {"referenceID": 29, "context": "2, SSA [30] and MIDA are compared on a 2D dataset with continuous distributional change, which resembles time-varying drift in machine olfaction.", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "[33] is dedicated to research in drift correction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "TABLE I PERIOD OF COLLECTION AND NUMBER OF SAMPLES IN THE GAS SENSOR ARRAY DRIFT DATASET [33].", "startOffset": 89, "endOffset": 93}, {"referenceID": 32, "context": "A strategy used in previous literatures [33], [23] was also adopted in this paper so that the performance of the domain adaptation algorithms can be evaluated.", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": "A strategy used in previous literatures [33], [23] was also adopted in this paper so that the performance of the domain adaptation algorithms can be evaluated.", "startOffset": 46, "endOffset": 50}, {"referenceID": 32, "context": "In the dataset, each sample is represented by 128 features extracted from the sensors\u2019 response curves [33].", "startOffset": 103, "endOffset": 107}, {"referenceID": 4, "context": "As displayed in Table II, the compared methods include kernel PCA (KPCA), transfer component analysis (TCA), semi-supervised TCA (SSTCA) [5], subspace alignment (SA) [17], geodesic flow kernel (GFK) [22], manifold regularization with combination GFK (ML-comGFK) [23], and informationtheoretical learning (ITL) [16].", "startOffset": 137, "endOffset": 140}, {"referenceID": 16, "context": "As displayed in Table II, the compared methods include kernel PCA (KPCA), transfer component analysis (TCA), semi-supervised TCA (SSTCA) [5], subspace alignment (SA) [17], geodesic flow kernel (GFK) [22], manifold regularization with combination GFK (ML-comGFK) [23], and informationtheoretical learning (ITL) [16].", "startOffset": 166, "endOffset": 170}, {"referenceID": 21, "context": "As displayed in Table II, the compared methods include kernel PCA (KPCA), transfer component analysis (TCA), semi-supervised TCA (SSTCA) [5], subspace alignment (SA) [17], geodesic flow kernel (GFK) [22], manifold regularization with combination GFK (ML-comGFK) [23], and informationtheoretical learning (ITL) [16].", "startOffset": 199, "endOffset": 203}, {"referenceID": 22, "context": "As displayed in Table II, the compared methods include kernel PCA (KPCA), transfer component analysis (TCA), semi-supervised TCA (SSTCA) [5], subspace alignment (SA) [17], geodesic flow kernel (GFK) [22], manifold regularization with combination GFK (ML-comGFK) [23], and informationtheoretical learning (ITL) [16].", "startOffset": 262, "endOffset": 266}, {"referenceID": 15, "context": "As displayed in Table II, the compared methods include kernel PCA (KPCA), transfer component analysis (TCA), semi-supervised TCA (SSTCA) [5], subspace alignment (SA) [17], geodesic flow kernel (GFK) [22], manifold regularization with combination GFK (ML-comGFK) [23], and informationtheoretical learning (ITL) [16].", "startOffset": 310, "endOffset": 314}, {"referenceID": 21, "context": "The subspace dimension of GFK was determined according to the subspace disagreement measure in [22].", "startOffset": 95, "endOffset": 99}, {"referenceID": 22, "context": "The results of ML-comGFK are copied from [23].", "startOffset": 41, "endOffset": 45}, {"referenceID": 0, "context": "The domain feature vector of a sample was thus [1, 0] if it is from the source domain and [0, 1] if it is from the target.", "startOffset": 47, "endOffset": 53}, {"referenceID": 0, "context": "The domain feature vector of a sample was thus [1, 0] if it is from the source domain and [0, 1] if it is from the target.", "startOffset": 90, "endOffset": 96}, {"referenceID": 7, "context": "As a noninvasive approach, disease screening and monitoring with e-noses is attracting more and more attention [8], [11].", "startOffset": 111, "endOffset": 114}, {"referenceID": 10, "context": "As a noninvasive approach, disease screening and monitoring with e-noses is attracting more and more attention [8], [11].", "startOffset": 116, "endOffset": 120}, {"referenceID": 10, "context": "For example, the concentration of acetone in diabetics\u2019 breath is often higher than that in healthy people [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "We have collected a breath analysis dataset in years 2014\u2013 2015 using two e-noses of the same model [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "TCA [5] 82.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "SSTCA [5] 84.", "startOffset": 6, "endOffset": 9}, {"referenceID": 16, "context": "SA [17] 80.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "GFK [22] 77.", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "ML-comGFK [23] 80.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "ITL [16] 76.", "startOffset": 4, "endOffset": 8}, {"referenceID": 29, "context": "Because methods other than stationary subspace analysis (SSA) [30], MIDA, and SMIDA are not capable of handling the chronological information, we simply regarded each device as a discrete domain and learned device-invariant features with them.", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "TCA [5] 67.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "SSTCA [5] 67.", "startOffset": 6, "endOffset": 9}, {"referenceID": 16, "context": "SA [17] 29.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "GFK [22] 41.", "startOffset": 4, "endOffset": 8}, {"referenceID": 15, "context": "ITL [16] 68.", "startOffset": 4, "endOffset": 8}, {"referenceID": 29, "context": "SSA [30] 49.", "startOffset": 4, "endOffset": 8}, {"referenceID": 4, "context": "For example, the noise in data can also be stationary [5].", "startOffset": 54, "endOffset": 57}, {"referenceID": 20, "context": "Instrumental variation is also a problem for them [21].", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "TCA [5] 0.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "SSTCA [5] 0.", "startOffset": 6, "endOffset": 9}, {"referenceID": 16, "context": "SA [17] 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "GFK [22] 0.", "startOffset": 4, "endOffset": 8}, {"referenceID": 21, "context": "In [22], Gong et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "Following the experimental setting provided in the sample code from the authors of [22], experiments were conducted in 20 random trials for each pair of domains.", "startOffset": 83, "endOffset": 87}, {"referenceID": 17, "context": "For GFK, low-rank transfer subspace learning (LTSL), and domain adaptation by shifting covariance (DASC), we copied the best results reported in the original papers [18], [19], [3].", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "For GFK, low-rank transfer subspace learning (LTSL), and domain adaptation by shifting covariance (DASC), we copied the best results reported in the original papers [18], [19], [3].", "startOffset": 171, "endOffset": 175}, {"referenceID": 2, "context": "For GFK, low-rank transfer subspace learning (LTSL), and domain adaptation by shifting covariance (DASC), we copied the best results reported in the original papers [18], [19], [3].", "startOffset": 177, "endOffset": 180}, {"referenceID": 23, "context": "We observe that TCA and SSTCA have comparable performance with MIDA and SMIDA, which may be explained by the fact that the HSIC criterion used in MIDA and MMD used in TCA are identical under certain conditions when there are one source and one target domain [24].", "startOffset": 258, "endOffset": 262}, {"referenceID": 4, "context": "TCA [5] 49.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "SSTCA [5] 50.", "startOffset": 6, "endOffset": 9}, {"referenceID": 15, "context": "ITL [16] 41.", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "SA [17] 48.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "GFK [18] 40.", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "LTSL [19] 50.", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "DASC [3] 39.", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "TCA [5] 55.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "SSTCA [5] 55.", "startOffset": 6, "endOffset": 9}, {"referenceID": 15, "context": "ITL [16] 51.", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "SA [17] 51.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "GFK [18] 46.", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "LTSL [19] 50.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "Besides, this paper mainly focus on reducing the difference of the marginal distribution P (X) among domains, while it may be better if the conditional distribution P (Y |X) among domains can also be approximately aligned [16].", "startOffset": 222, "endOffset": 226}], "year": 2016, "abstractText": "When the distributions of the source and the target domains are different, domain adaptation techniques are needed. For example, in the field of sensors and measurement, discrete and continuous distributional change often exist in data because of instrumental variation and time-varying sensor drift. In this paper, we propose maximum independence domain adaptation (MIDA) to address this problem. Domain features are first defined to describe the background information of a sample, such as the device label and acquisition time. Then, MIDA learns features which have maximal independence with the domain features, so as to reduce the inter-domain discrepancy in distributions. A feature augmentation strategy is designed so that the learned projection is background-specific. Semi-supervised MIDA (SMIDA) extends MIDA by exploiting the label information. The proposed methods can handle not only discrete domains in traditional domain adaptation problems but also continuous distributional change such as the time-varying drift. In addition, they are naturally applicable in supervised/semi-supervised/unsupervised classification or regression problems with multiple domains. This flexibility brings potential for a wide range of applications. The effectiveness of our approaches is verified by experiments on synthetic datasets and four real-world ones on sensors, measurement, and computer vision.", "creator": "LaTeX with hyperref package"}}}