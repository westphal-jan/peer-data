{"id": "1603.04747", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2016", "title": "Topic Modeling Using Distributed Word Embeddings", "abstract": "We propose a new algorithm for topic modeling, Vec2Topic, that identifies the main topics in a corpus using semantic information captured via high-dimensional distributed word embeddings. Our technique is unsupervised and generates a list of topics ranked with respect to importance. We find that it works better than existing topic modeling techniques such as Latent Dirichlet Allocation for identifying key topics in user-generated content, such as emails, chats, etc., where topics are diffused across the corpus. We also find that Vec2Topic works equally well for non-user generated content, such as papers, reports, etc., and for small corpora such as a single-document.", "histories": [["v1", "Tue, 15 Mar 2016 16:21:58 GMT  (3675kb)", "http://arxiv.org/abs/1603.04747v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ramandeep s randhawa", "parag jain", "gagan madan"], "accepted": false, "id": "1603.04747"}, "pdf": {"name": "1603.04747.pdf", "metadata": {"source": "CRF", "title": "Topic Modeling Using Distributed Word Embeddings", "authors": ["Ramandeep S. Randhawa", "Gagan Madan"], "emails": ["ramandeep.randhawa@marshall.usc.edu", "paragjain78@gmail.com", "gagan.madan1@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.04 747v 1 [cs.C L] 15 Mar 2"}, {"heading": "1 Introduction", "text": "In fact, it is about a way in which it is about a way in which people put themselves and themselves at the centre, in which they put themselves at the centre of public attention. (...) It is about the way in which people are put at the centre. (...) It is about the way in which people are put at the centre. (...) It is about the way in which people are put at the centre. (...) It is about the way in which people are put at the centre. (...) It is about the way in which people are put at the centre. (...) It is about the way in which people are put at the centre. (...) It is about the way in which people are put at the centre. (...) It is about the way in which people are put at the centre, the way in which people are put. (...) It is about the way in which people are put at the centre."}, {"heading": "2 Background", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution that meets the needs of the people."}, {"heading": "3 Methodology", "text": "In this section, we describe our algorithm for identifying the key topics underlying a corpus. Our approach is threefold: First, we build distributed word embeddings for the vocabulary of the corpus; second, we group the word embeddings using K-means to obtain K-clusters of semantically related words; third, we implement K-means using the Euclidean standard method, but with all word vectors standardized so that their norm is unity; and third, we identify each of the K clusters obtained by K-means as a topic; and third, we evaluate the importance of each topic and name the keywords that best describe the topic. Vec2Topic's core lies in the third step, i.e., we evaluate the importance of topics; therefore, we focus on describing this next in Section 3.1. To facilitate exposure, we assume that the first and second steps are completed."}, {"heading": "3.1 Scoring importance of topics", "text": "This year it is more than ever before."}, {"heading": "3.2 Building distributed word embeddings", "text": "This approach, however, assumes that the vocabulary of the words is large enough, and the words show up in several different contexts to develop semantically accurate word representations; trained models are built on corpses that have billions of symbols and millions of words in the vocabulary; nevertheless, we understand how these words are used in users \"contexts. To overcome these constraints, we tend to have much smaller corpus and vocabulary that do not allow understanding the relative meaning of words; nevertheless, we grasp how these words are used in users\" contexts. To overcome these constraints, we use a two-tiered word: we learn \"global\" word labels using skip-gram technology on a knowledge basis - these word labels tend to grasp the generic meaning of words in widespread contexts."}, {"heading": "4 Discussion", "text": "In this section, we will discuss various aspects of Vec2Topic. First, we will discuss its speed in Section 4.1, then in Section 4.2 we will discuss its robustness in relation to the number of topics to be extracted. In Section 4.3, we will demonstrate its implementation performance on: a non-user generated data set (NIPS 2015 papers) and a small data set (a single document, Apple's 10K financial report). Finally, in Section 4.4, we will discuss some additional considerations in implementing the algorithm."}, {"heading": "4.1 Algorithm complexity", "text": "To understand the complexity of the algorithm, we look at each of its key components. We use V-shaped terms to denote the entire vocabulary of the corpus (remember that V-shaped terms are the vocabulary of nouns and nouns).1 Hierarchical (agglomerative) clustering: We use the Quick Cluster method, which has a complexity of the algorithm. In all our experiments, the algorithm takes no more than a few minutes to run. Table 5 provides details on the size of all the data sets considered in this paper and the runtime of the algorithm. The slowest component of Vec2Topic is the construction of word vectors, which take up the majority of the runtime. However, this method is quite scalable and has been used to create word vectors on a Google server."}, {"heading": "4.2 Effect of changing K", "text": "Similar to other topic modeling methods, our algorithm assumes as input the number of topics K to be extracted. For the results described so far, we have K = 10. Table 6 indicates the top 4 topics for Mr. Kaminski, if the number of topics K to be extracted is 5 and 50. Note that in both cases the total number of points of the word is not affected by the K change, the only change being the clustering of topics. Note that the top topic is identical for these two cases, and for case K = 10 of Table 2. Even the second-placed topic is quite similar. This illustrates the robustness of our approach to collecting the key topics from a corpus. Note that the case K = 5 is quite coarse in the sense that all words are clustered in 5 clusters, and therefore we observe a diffuse topic such as topic 4. On the other hand, K = 50 is much more concise and picks up a large number of small clusters, which leads us to find that 20 topics are well specialized in our experiments, many of which we find K = extensive."}, {"heading": "4.3 Other datasets", "text": "In fact, the table provides a good insight into how Vec2Topic differs from LDA. Our algorithm focuses on concepts in the documents in the dataset, whereas LDA does a very good job in the concepts within the datasets. In fact, the table provides a good insight into how Vec2Topic differs from LDA. Our algorithm focuses on concepts in the documents in the dataset, whereas LDA focuses on concepts within the datasets."}, {"heading": "4.4 Other implementation considerations", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We are able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "5 Conclusions", "text": "In this paper, we propose a novel technique of topic modeling that harnesses the understanding of word semantics by means of high-dimensional word vectors. We have shown that Vec2Topic works well to extract a user's key topics across his or her own generated content - it also classifies these topics and identifies keywords that best describe them. We have compared it to the state-of-the-art theme modeling algorithm LDA and found that it works much better when the topic keywords are distributed across the various documents and surrounded by several contiguous words that are of a general nature. Furthermore, we observe that the technique is not limited to user-generated content; it works equally well on more structured documents such as scientific essays, news articles, blogs, websites, etc. It is also relatively robust compared to the body size - it can scale from a single document to a large collection. \"One of our ongoing efforts focuses on expanding the algorithm to compose the user's keywords to identify the phrases of interest - it can scale from a single document to a large collection."}, {"heading": "6 Acknowledgments", "text": "The authors thank Achal Bassamboo for many useful discussions."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Commun. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "The nested Chinese restaurant process and bayesian nonparametric inference of topic hierarchies", "author": ["D.M. Blei", "T.L. Griffiths", "M.I. Jordan"], "venue": "Journal of the ACM (JACM), 57(2):7", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "Proceedings of the 23rd international conference on Machine learning, pages 113\u2013120. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "A correlated topic model of science", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "The Annals of Applied Statistics, pages 17\u201335", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Hierarchical relational models for document networks", "author": ["J. Chang", "D.M. Blei"], "venue": "The Annals of Applied Statistics, pages 124\u2013150", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th International Conference on Machine Learning, ICML \u201908, pages 160\u2013167, New York, NY, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL \u201912, pages 873\u2013882, Stroudsburg, PA, USA", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": "arXiv preprint arXiv:1506.07285", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, abs/1301.3781", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111\u20133119. Curran Associates, Inc.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-T. Yih", "G. Zweig"], "venue": "HLT-NAACL, pages 746\u2013751", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1081\u20131088", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "fastcluster: Fast hierarchical", "author": ["D. M\u00fcllner"], "venue": "agglomerative clustering routines for r and python. Journal of Statistical Software, 53(9):1\u201318", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic pooling and unfolding recursive auto-encoders for paraphrase detection", "author": ["R. Socher", "E.H. Huang", "J. Pennin", "C.D. Manning", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, pages 801\u2013809", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "C. Manning", "A.Y. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), pages 129\u2013136", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised recursive auto-encoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151\u2013161. Association for Computational Linguistics", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "Proceedings of the conference on empirical methods in natural language processing (EMNLP), volume 1631, page 1642. Citeseer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Hierarchical dirichlet processes. Journal of the American Statistical Association, 101:1566\u20131581", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394. Association for Computational Linguistics", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Visualizing data using t-SNE", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Topic modeling: beyond bag-of-words", "author": ["H.M. Wallach"], "venue": "Proceedings of the 23rd international conference on Machine learning, pages 977\u2013984. ACM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint arXiv:1502.05698", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "State-of-the-art topic modeling algorithms, such as Latent Dirichlet Allocation (LDA, [6, 2]), have been successfully applied to discover the main topics across a large collection of documents for some time now, and are a natural candidate for solving the problem at hand.", "startOffset": 86, "endOffset": 92}, {"referenceID": 1, "context": "State-of-the-art topic modeling algorithms, such as Latent Dirichlet Allocation (LDA, [6, 2]), have been successfully applied to discover the main topics across a large collection of documents for some time now, and are a natural candidate for solving the problem at hand.", "startOffset": 86, "endOffset": 92}, {"referenceID": 5, "context": "In 2003, LDA was introduced as a generative probabilistic topic modeling model to manage large document archives, [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 24, "context": "Since then, a huge body of work has been done to relax and extend the statistical assumptions made in LDA to uncover more sophisticated structure in the text: Topic models that assume that topics generate words conditional on the previous word [25]; dynamic topic models that respect the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5]; Relational topic models that capture both the topic model and a network model for the documents [7].", "startOffset": 244, "endOffset": 248}, {"referenceID": 3, "context": "Since then, a huge body of work has been done to relax and extend the statistical assumptions made in LDA to uncover more sophisticated structure in the text: Topic models that assume that topics generate words conditional on the previous word [25]; dynamic topic models that respect the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5]; Relational topic models that capture both the topic model and a network model for the documents [7].", "startOffset": 310, "endOffset": 313}, {"referenceID": 2, "context": "Since then, a huge body of work has been done to relax and extend the statistical assumptions made in LDA to uncover more sophisticated structure in the text: Topic models that assume that topics generate words conditional on the previous word [25]; dynamic topic models that respect the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5]; Relational topic models that capture both the topic model and a network model for the documents [7].", "startOffset": 402, "endOffset": 409}, {"referenceID": 21, "context": "Since then, a huge body of work has been done to relax and extend the statistical assumptions made in LDA to uncover more sophisticated structure in the text: Topic models that assume that topics generate words conditional on the previous word [25]; dynamic topic models that respect the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5]; Relational topic models that capture both the topic model and a network model for the documents [7].", "startOffset": 402, "endOffset": 409}, {"referenceID": 4, "context": "Since then, a huge body of work has been done to relax and extend the statistical assumptions made in LDA to uncover more sophisticated structure in the text: Topic models that assume that topics generate words conditional on the previous word [25]; dynamic topic models that respect the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5]; Relational topic models that capture both the topic model and a network model for the documents [7].", "startOffset": 476, "endOffset": 479}, {"referenceID": 6, "context": "Since then, a huge body of work has been done to relax and extend the statistical assumptions made in LDA to uncover more sophisticated structure in the text: Topic models that assume that topics generate words conditional on the previous word [25]; dynamic topic models that respect the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5]; Relational topic models that capture both the topic model and a network model for the documents [7].", "startOffset": 577, "endOffset": 580}, {"referenceID": 0, "context": "Our algorithm is inspired by the recent work in learning word vector representations using neural networks [1, 8, 15, 23, 9].", "startOffset": 107, "endOffset": 124}, {"referenceID": 7, "context": "Our algorithm is inspired by the recent work in learning word vector representations using neural networks [1, 8, 15, 23, 9].", "startOffset": 107, "endOffset": 124}, {"referenceID": 14, "context": "Our algorithm is inspired by the recent work in learning word vector representations using neural networks [1, 8, 15, 23, 9].", "startOffset": 107, "endOffset": 124}, {"referenceID": 22, "context": "Our algorithm is inspired by the recent work in learning word vector representations using neural networks [1, 8, 15, 23, 9].", "startOffset": 107, "endOffset": 124}, {"referenceID": 8, "context": "Our algorithm is inspired by the recent work in learning word vector representations using neural networks [1, 8, 15, 23, 9].", "startOffset": 107, "endOffset": 124}, {"referenceID": 0, "context": "[1] used a feedforward neural network with a linear projection layer and a non-linear hidden layer to jointly learn the word vector representations and a statistical language model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] and [9] leveraged distributed word vectors to show that neural network based models match or outperform feature-engineered systems for standard Natural Language Processing (NLP) tasks that include part-of-speech tagging, chunking, named entity recognition, and semantic role labeling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[8] and [9] leveraged distributed word vectors to show that neural network based models match or outperform feature-engineered systems for standard Natural Language Processing (NLP) tasks that include part-of-speech tagging, chunking, named entity recognition, and semantic role labeling.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "[10] introduced a technique to learn better word embeddings by incorporating both local and global document context, and account for homonymy and polysemy by learning multiple embeddings per word.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12, 13, 14] introduced Word2Vec and the Skip-gram model, a very simple method for learning word vectors from large amounts of unstructured text data.", "startOffset": 0, "endOffset": 12}, {"referenceID": 12, "context": "[12, 13, 14] introduced Word2Vec and the Skip-gram model, a very simple method for learning word vectors from large amounts of unstructured text data.", "startOffset": 0, "endOffset": 12}, {"referenceID": 13, "context": "[12, 13, 14] introduced Word2Vec and the Skip-gram model, a very simple method for learning word vectors from large amounts of unstructured text data.", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "[17] later introduced GloVe, Global Vectors for Word Representation, which combines word-word global co-occurrence statistics from a corpus, and context based learning similar to Word2Vec to deliver an improved word vector representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Word vectors are an attractive building block and are being used as input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21], question and answer systems, [11, 26], and others.", "startOffset": 150, "endOffset": 166}, {"referenceID": 18, "context": "Word vectors are an attractive building block and are being used as input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21], question and answer systems, [11, 26], and others.", "startOffset": 150, "endOffset": 166}, {"referenceID": 19, "context": "Word vectors are an attractive building block and are being used as input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21], question and answer systems, [11, 26], and others.", "startOffset": 150, "endOffset": 166}, {"referenceID": 20, "context": "Word vectors are an attractive building block and are being used as input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21], question and answer systems, [11, 26], and others.", "startOffset": 150, "endOffset": 166}, {"referenceID": 10, "context": "Word vectors are an attractive building block and are being used as input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21], question and answer systems, [11, 26], and others.", "startOffset": 197, "endOffset": 205}, {"referenceID": 25, "context": "Word vectors are an attractive building block and are being used as input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21], question and answer systems, [11, 26], and others.", "startOffset": 197, "endOffset": 205}, {"referenceID": 23, "context": "we use the t-SNE technique [24], which does a remarkable job of dimension-reduction by trying to ensure that the distances in two dimensions are reflective of those in the higher-dimensional space.", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "A natural way to build distributed word embeddings is to apply a standard technique such as skip-gram [12] on the given corpus.", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "This approach however assumes that the training corpus is large enough, and words show up in several different contexts to develop semantically accurate word representations; trained models have been built on corpuses that have billions of tokens and millions of words in the vocabulary [12, 13].", "startOffset": 287, "endOffset": 295}, {"referenceID": 12, "context": "This approach however assumes that the training corpus is large enough, and words show up in several different contexts to develop semantically accurate word representations; trained models have been built on corpuses that have billions of tokens and millions of words in the vocabulary [12, 13].", "startOffset": 287, "endOffset": 295}, {"referenceID": 15, "context": "Hierarchical (agglomerative) clustering: we use the fastcluster method, [16], which has a complexity of \u0398(|V |2).", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "Building word vectors: we use the skip-gram model [12], which has a running complexity of E \u00d7 T \u00d7Q, where E is the number of iterations, which is typically 5 \u2212 50, T is the total number of words or tokens in the corpus, and Q = c\u00d7 (x+x log2(|V\u0304 |)), where c is the context size (in all our experiments we set c = 5) and x \u2208 {\u03ba, \u03bb} denotes the dimensionality of the word vectors used in training.", "startOffset": 50, "endOffset": 54}], "year": 2016, "abstractText": "We propose a new algorithm for topic modeling, Vec2Topic, that identifies the main topics in a corpus using semantic information captured via high-dimensional distributed word embeddings. Our technique is unsupervised and generates a list of topics ranked with respect to importance. We find that it works better than existing topic modeling techniques such as Latent Dirichlet Allocation for identifying key topics in user-generated content, such as emails, chats, etc., where topics are diffused across the corpus. We also find that Vec2Topic works equally well for non-user generated content, such as papers, reports, etc., and for small corpora such as a single-document.", "creator": "LaTeX with hyperref package"}}}