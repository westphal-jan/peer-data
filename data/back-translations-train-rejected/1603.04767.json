{"id": "1603.04767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2016", "title": "Evaluating the word-expert approach for Named-Entity Disambiguation", "abstract": "Named Entity Disambiguation (NED) is the task of linking a named-entity mention to an instance in a knowledge-base, typically Wikipedia. This task is closely related to word-sense disambiguation (WSD), where the supervised word-expert approach has prevailed. In this work we present the results of the word-expert approach to NED, where one classifier is built for each target entity mention string. The resources necessary to build the system, a dictionary and a set of training instances, have been automatically derived from Wikipedia. We provide empirical evidence of the value of this approach, as well as a study of the differences between WSD and NED, including ambiguity and synonymy statistics.", "histories": [["v1", "Tue, 15 Mar 2016 17:16:02 GMT  (776kb,D)", "http://arxiv.org/abs/1603.04767v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["angel x chang", "valentin i spitkovsky", "christopher d manning", "eneko agirre"], "accepted": false, "id": "1603.04767"}, "pdf": {"name": "1603.04767.pdf", "metadata": {"source": "CRF", "title": "Evaluating the word-expert approach for Named-Entity Disambiguation", "authors": ["Angel X. Chang", "Valentin I. Spitkovsky", "Christopher D. Manning", "Eneko Agirre"], "emails": ["angelx@cs.stanford.edu,", "vals@cs.stanford.edu,", "manning@cs.stanford.edu,", "e.agirre@ehu.eus"], "sections": [{"heading": "1 Introduction", "text": "Building the formal representations of snippets of free form text is a long-soughtafter goal in natural language processing (NLP).It is possible that grounding written language in terms of background knowledge of real entities and world events is already important for building such representations. There are also many applications in its own right: text mining, information retrieval, and the semantic web [Weikum and Theobald, 2010]. Wikipedia and related repositories of structured data (e.g. WikiData or DBpedia) already provide extensive inventories of named entities, including people, organizations, and geo-political entities.A single named entity could refer to several entities, and the process of dissolving the corresponding meaning in context is called entity linking (EL) or named entity disambiguation (NED).The earlier terminology (EL) emphasizes the importance of linking an actual instance in the given knowledge base (McNamee, 2009 and 2009)."}, {"heading": "2 Related Work", "text": "We will now examine several NLP issues closely related to NED, including Wikification and WSD, as well as previous work on NED. To get a clearer picture, we will group the previous work into three sections: first the earlier and more influential contributions, followed by work on Wikification, and finally NED systems. We will briefly discuss the latest techniques used in NED, in particular the handling of candidate generation (i.e. our dictionary) and discambiguity. For a full comparison of our results with those of the latest state-of-the-art systems, see Section 8.7."}, {"heading": "2.1 Related Problems", "text": "In fact, it is as if most of them will be able to follow the rules that they have imposed on themselves, and that they will be able to follow the rules that they have imposed on themselves. (...) In fact, it is as if they are able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves what they want. (...) It is as if they were able to do it. (...) They want to do it, as if they want to do it. (...) (...) They want to do it, as if they want to. (...) They want to do it. (...) They want to do it, as if they want to do it. (...)"}, {"heading": "2.2 Word Sense Disambiguation", "text": "In WSD, the task of determining the meaning of an open class term - i.e., an adverb, an adjective, or a term that is not designated as an entity - applies to a particular occurrence of that word [Agirre and Edmonds, 2006]. Typically, the meaning of the topic is taken from a dictionary, such as WordNet [Fellbaum, 1998], and it is fixed in advance. Dictionaries are comprehensive and cover almost all uses of a word. Consequently, WSD systems tend to render a sense for each occurrence. For example, the Senseval 3 lexical dataset, which is usually used. In 2004, it contains a remark of the word, 98.3% of which a dictionary has been assigned: only 1.7% are problematic and have found no meaning in the dictionary."}, {"heading": "2.3 Early Work on NED", "text": "The earliest work on NED using Wikipedia was by Bunescu and Pasca [2006], who used article titles, redirects and clarification pages to generate candidate units, and the similarity between the context of a mention and the article text provided the tf-idf ranking and cosine similarity, adding words from other articles in the same category to the vector of each article. Cucerzan [2007] followed an overall similar design, but used context vectors consisting of keywords and short phrases extracted from Wikipedia."}, {"heading": "2.4 Wikification", "text": "Most Wikifixing research excludes candidate generation and focuses on ambiguities. In groundbreaking work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u00efve Bayes) classifier, which is the work most similar to ours. However, they did not address the problems of building a dictionary or other methods of collecting training data (see Section 5.2). Wikifixing work continued with Milne and Witten [2008], which combined popularity and kinship (calculated as the number of links between the context and the target articles), using several machine learning algorithms, and they were the first to use the Wikipedia linkage structure. Kulkarni et al. [2009] later proposed a method that collectively selected a complete document by including a ceramic context in both the wireless and the wireless environment, but also incorporating it into the wireless environment in 2008."}, {"heading": "2.5 Current NED Systems", "text": "This year it is so far that it will only take a few weeks to reach an agreement."}, {"heading": "3 System Architecture", "text": "Figure 6 summarizes our approach to NED, which is inspired by the word-expert technique that is popular in mainstream WSD systems. We take a text form (mention) with the context of its occurrence and determine a suitable unit (meaning) to which the input line can refer. In this example, our system clarifies the mention of \"Abbott\" in the sentence \"The voice was provided by Candy Candido, who briefly became Abbott's partner after Costello's death.\" First, a context-independent component (the dictionary) expands the string to a series of potentially referential candidate units, ordered by popularity; second, a context-dependent component (supervised classifier) selects a candidate who seems most suitable for the context. The context-independent dictionary component (the dictionary) includes lists of possible Wikipedia titles; it also provides scores - indicative of conditional probabilities that determine the individual articles."}, {"heading": "4 The Dictionary", "text": "The dictionary is a cornerstone component of our system that serves two objectives. Its primary goal is to provide a short list of candidates who can point to Wikipedia articles that could name an entity. In addition, it provides a score that quantifies the affinity of the string for each candidate. However, if the dictionary does not recognize that a particular string could point to a particular entity, then our system will not be able to return that entity. Thus, the dictionary presents a performance bottleneck. We will use this fact to measure a cap on the performance of our system by which decisions are limited to only the entities proposed by the dictionary. The dictionary represents a set of weighted pairs - a complete enumeration of all possible string entity entity entity entity pages."}, {"heading": "4.1 Redirects and Canonical Pages", "text": "One of the many difficulties in building a Wikipedia-based dictionary stems from redirects, because it is important to separate articles that are actual entries in Wikipedia from other placeholder pages that refer to them. We will use the term in this context to refer randomly to each article page (redirect or not), for example, both http: / / de. wikipedia.org / wiki / Stanford and http: / / en.wikipedia.org / wiki / Stanford _ University, where it is redirected. To collect all titles and URLs that refer to the same article, we first assign all such strings to URLs using Wikipedia's canonization algorithm. 11 We then join any two URLs that either appear together as an official redirect (in a Wikipedia dump) or are redirected at Crawl-Time (using HTTP status codes 3xx)."}, {"heading": "4.2 The Core and Other Dictionaries", "text": "We have created a core dictionary by extracting strings from the titles of canonical articles, redirects, and clarification pages, as well as the referenced anchor text that occurs in both Wikipedia and the Google crawl. In all cases, we pair a string with the canonical article of the respective cluster. Our core dictionary assigns strings to sorted lists of Wikipedia articles, with associated scores. These scores are calculated from the frequencies of anchor texts. For a particular pair of string articles, where the string has been observed as the anchor text of a total of y-inter-Wikipedia and v-external - links, of which x (or u) pointed to a page represented by the canonical article in the pair, we set the pair's score (x + u) / (y + v). We call this dictionary exactly (EXCT) because it matches exactly with the strings found above."}, {"heading": "4.2.1 Aggregating String Variants", "text": "In addition to the exact string searches, we also adopt two less strict views of the core dictionary. They capture string variants whose low-threshold normalized forms are either the same (the LNRM dictionary) or close (the FUZZ dictionary) of Levenshtein editdistance to that of the queried string. In both cases, an incoming string now corresponds to a set of keys (strings) in the dictionary, whose lists of rated articles are then merged as follows: specified n articles, with scores ai / bi, whose total score is also a ratio (\u2211 n = 1 ai) / (\u2211 n i = 1 bi). We form the low-threshold variant l (s) of a string by canonizing Unicode characters, eliminating dictionary, low-threshold keys, and confused Uk characters that are not alpha-numeric."}, {"heading": "4.2.2 Querying Search Engines", "text": "In addition to creating our own custom dictionaries, we also experimented with imitating a search by querying the Google search engine (twice), 13, which we call the GOOG dictionary. Our query pairs consisted of the raw target string < str > - and also a separately issued phrase query \"< str >\" - combined with a constraint, site: en.wikipedia.org. We retained only the returned URLs starting with http: / en.wikipedia.org / wiki /, using their (sums) reverse order as ratings. Although this method cannot be used effectively to find all strings that can name an entity, the dictionary section for that particular string can be dynamically filled in as needed."}, {"heading": "4.3 Disambiguating with Dictionaries", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country."}, {"heading": "5 Supervised Disambiguation", "text": "The large number of naturally occurring links pointing to entities in Wikipedia allows for the automatic collection of richly annotated data to improve context-free discambiguity through the raw frequency information of the dictionaries. In this respect, NED differs from many other applications in natural language processing and information extraction, where most of the available input data is not pre-marked and considerable resources are devoted to manual annotation."}, {"heading": "5.1 Core Method", "text": "We took a mainstream WSD (Word Expert) approach, training the classifiers for all target strings as follows: \u2022 For each string in the dictionary, we first identify the units to which it can relate. We then collect all sample texts from Wikipedia articles that contain links to those units. \u2022 To ensure that our training data is natural language (not lists or tables, for example), we only insert text labeled as paragraphs (i.e., the text is an acronym for the title < P > and < / P >). The relevant training subset for a target string then consists of sample contexts with anchor text labeled as the string.1714I.e. Either (a) the string is an acronym for the title < P > and < / P >. The relevant training subset for a target string then consists of sample anchor text containing the string.14.e."}, {"heading": "5.2 Variations", "text": "In the course of the development of our system, we tested several variations of the core algorithm: classifier: we tried maximum entropy models (MAXENT) and supported vector machines (SVM). Dictionary: A dictionary influences the supervised classification in two places. First, we compared the sample voltages selected for training when building the training data. Second, as a backup marker, in cases where a classifier is not trained due to a lack of examples. Both in the filter phase and in the back-off phase, we compared the use of the HEUR dictionary instead of the LNRM cascade. Span: In addition to training with contexts of (up to) 100 characters left and right of a string, we also tried use and full-paragraph spans (the 100, SENT and PARA variants). Match: When collecting examples for a target string, we made sure that the anchor string does not allow this additional PARA variant (the PART)."}, {"heading": "6 Datasets for NED", "text": "The evaluation of NED systems requires manually annotated data. Although many corporas have been introduced in various papers, we decided to focus on the earlier datasets developed for the KBP task of linking the Population Knowledge Base (KBP) to Text Analysis Conferences (TAC), 18, which have been running annually since 2009 [McNamee and Dang, 2009, Ji et al., 2010, Ji and Grishman, 2011].The TAC-KBP evaluation focuses on three main types of designated entities: Persons (PER), Organizations (ORG), and Geo-political entities (GPE). Given a number of hand-selected mentions of entities - and documents containing these strings - the task is to determine which Knowledge Base instance, if any, matches each designated entity chain. The Knowledge Base (KB) is derived from a subset of Wikipedia."}, {"heading": "6.1 The TAC-KBP Dataset", "text": "The TAC-KBP exercise provides an inventory of targets called entities, based on a subset of Wikipedia articles included in October 2008; this KB contains 818,741 entities (a.k.a. instances), each identified by (i) their name (a string); (ii) the assigned entity (a PER, ORG, or GPE); (iii) a KB instance ID (a unique identifier, such as E001); (iv) the set of slot names and values from the assigned Wikipedia page; and (v) the text of this Wikipedia page shows a sample KB input for a person whose entity is identified from the Wikipedia article."}, {"heading": "6.2 Ambiguity", "text": "In our dictionary, the ICNC string is divided into four different entities in Wikipedia. We rate the ambiguity as follows: For each target string in a dataset, we have counted the number of different KB instances associated with it by human announcers. Table 3 reports on ambiguities in development and evaluation. The section of the definition includes 1,162 unique target strings (types), of which 462 were considered appropriate."}, {"heading": "6.3 Synonymy", "text": "Another quantity that illuminates the complexity of a disambiguation task is the number of different strings that can be used to name a particular entity: its synonymy, in WSD terms. As before, the matching of all unique strings that can point to a particular entity is an open problem. Table 5 tabulates the synonymy of the gold standard (i.e. the number of strings found by the annotators in the document collection) for each entity in the KB. Most units are associated with only a single string, especially in the valuation sets. This could be an artifact of how the organizers construct the test data, as their procedures first select ambiguous strings and then find documents that contain different meanings (entities) for that mention - unlike the first selection of entities and then the query of string variants of names."}, {"heading": "6.4 Ambiguity and Synonymy in Dictionaries", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "7 A NED System for TAC-KBP", "text": "The evaluation of our system in the TAC-KBP exercise required several adjustments. Figure 14 shows the updated architecture, in which a new module searches for the best-placed articles in the KB. If a top scorer has a corresponding entry in the KB, the module returns its KB ID. Since all KB instances are from Wikipedia, there is a direct assignment of titles to KB IDs. To ensure consistency, we changed our dictionary structure slightly, ensuring that all Wikipedia titles were explicitly referenced by the official KB (see Figure 12). When deciding which pages are canonical (see Section 4.1), we preferred KB entries over all others (from a superset of Wikipedia data that included the October 2008 TAC-KBP dump)."}, {"heading": "8 Experimental Results and Performance Analyses", "text": "We now evaluate various variants of dictionaries and monitored classifiers based on development data. These experiments allow us to adjust settings that can be used with the blind test (see Section 8.7)."}, {"heading": "8.1 Evaluation Setup for TAC-KBP", "text": "We follow a standard evaluation procedure using scripts from the 2009-10 TAC-KBP exercise. The measurement is (micro-averaged) accuracy: Given a reference set of N queries - and a corresponding set of assumptions where C is correctly ambiguous (i.e., the output label of a system corresponds to the entity ID string of the gold standard) - the score is simply C / N. Since our main goal is to be unique between entities, we will focus on evaluating dictionaries and monitored classifiers limited to entities present in the KB (i.e., ignoring gold standard examples marked with NILs)."}, {"heading": "8.2 Performance of Top Dictionary Entries", "text": "Table 10 shows the performance of the news subset of development data for the EXCT and GOOG dictionaries (69.4 and 69.6%), the LNMR and FUZZ cascades (69.5 and 71.3%), and the heuristic combination HEUR (72.1%). It confirms our intuition (see Section 4): HEUR improves on the FUZZ cascade and provides cleaner suggestions (as it produces far fewer candidates). GOOG's results compete with cascades and can be a good alternative in situations where full dictionaries are not available.19 The table also shows that the use of counts from Wikipedia alone is worse than the use of counts from the rest of the Internet - and that merging the two counts (see Section 4) works best - for the LNRM cascade; we use summarized counts in all remaining experiments."}, {"heading": "8.3 Precision/Recall Curves for Dictionaries", "text": "Although the performance of the top candidates is an indication of the overall quality of a dictionary, it says little about the less popular choices. We use dictionaries to expand queries into pools of potential candidates, which can be clearly determined by a monitored system. Therefore, it is important to understand how close a dictionary could come to capturing all units, even low-ranking ones, that may be relevant to a gold standard. Figure 15 shows precision / recall curves that rate our dictionaries beyond pure top entries (the most left-leaning points of each curve corresponding to Table 10).20 These curves show that the FUZZ cascade generates more units than other dictionaries, with a higher recall rate and similar precision; unfortunately, it uses far too many candidates. LNRM and EXCT dictionaries perform similarly well at all recall levels; HEUR performs better at high recall rates, at high recall rates, and at low recall rates, and worse in between."}, {"heading": "8.4 Performance of Supervised Classifiers", "text": "The first line corresponds to the standard parameters; the rest is a greedy exploration of the space of alternatives; each additional line sets a setting different from the first line; dashes (-) indicate that all other parameters are the same. Classifiers: The second and third lines correspond to the accuracy of a multi-class classifier based on SVMmulticlass [Tsochantaridis et al., 2004], and a one-sided approach using the binary classifiers SVMlight [Joachims, 1999]. Both alternatives perform worse than our standard classification algorithm (maximum entropy with '2 regularization), MAXENT [Manning and Klein, 2003]."}, {"heading": "8.5 Extending Analyses to Web Data", "text": "The best results for the news section of the development data were obtained with the MAXENT classifier, the HEURO dictionary for the two training examples and the hedging, the 100 token span, and the SENSE match strategy. For web data, most of them are consistent with the conclusions from the news data. The HEURO study shows a subset of variants with qualitatively different results. For example, the LNO dictionary is the better option with the web data."}, {"heading": "8.6 Upper Bounds for Supervised Classification with Our Dictionaries", "text": "An important function of the filter dictionary is to provide sets of plausible units that determine the construction of the (remotely) monitored training data. Considering an ideal21Table 12, we disregard results for variations that were worse than the default in the news, as they also perform worse with web data.classifier, we would prefer to use comprehensive dictionaries that could include the correct unit for all strings in the assessment set, even if this would mean introducing many false candidates as well. Table 13 shows the skyline results that could be achieved by an oracle by selecting the best possible entity available for each system. Gold standard units for LNRM and HEUR cascades are among the dictionaries \"suggestions over 92% of the time (98% for web data). Monitored classification with the LEX strategy slightly lowers these limits because in some cases there are no training examples available for the standard gold units, which prevents our classifiers from returning the result more correctly."}, {"heading": "8.7 Final Results and Comparison to the State-of-the-Art", "text": "Following the development phase, we tested our best dictionary and monitored the classifier on the hidden dataset (2010 test) 79. Table 14 shows both results, also broken down by data type. This final review confirmed that the heuristic dictionary as a whole is already doing quite well (at 75%), and that trained classifiers can tap into further improvements (scoring almost 85%), which is greater in news, which is more difficult than the web dataset. Results are similar to those of development, except on the Web, where results have declined by about 5 points. Table 15 shows the performance of the most recent NED systems delivering results for each of the 2009 / 2010 TAC-KBP test sets (see section 2 for their descriptions), including our dictionary and classifier. The dictionary performs very well on the KB-only subset of the 2009 test set and beats some of the more complex systems."}, {"heading": "9 Discussion", "text": "This year, it has become one of the largest nodes in the world, with the largest number of nodes in the world."}, {"heading": "10 Conclusions and Future Work", "text": "Our system consists of two components: (1) a context-independent module, based on the frequencies of entities, that returns the most popular candidates; and (2) a context-sensitive classifier for each named entity chain, that selects the entities that are most suitable for a mentioning text. We show that such a word expert also delivers state-of-the-art results, as measured on the data sets of 2009 and 2010. In the future, we would like to extend our work to other data sets that provide further benchmarks for the state of art. We would also like to include our classifiers in more complex NED systems, where complementary information such as link structures [Moro et al., 2014], similarities between article texts and contexts [Hoffart et al., 2012], and global optimization techniques [Moro et al.]."}, {"heading": "Acknowledgements", "text": "We thank Oier Lopez de Lacalle and David Martinez for the script to extract features, and Daniel Jurafsky and Eric Yeh for their contributions to our earliest participation in TAC-KBP. Some of this work was done during a visit by Eneko Agirre to Stanford University with a scholarship from the Ministry of Science; Angel X. Chang was supported by an SAP Stanford Graduate Fellowship; Valentin I. Spitkovsky was partially supported by NSF scholarships IIS-0811974 and IIS-1216875, and by the Fellowship of the Fannie & John Hertz Foundation."}], "references": [{"title": "Word Sense Disambiguation: Algorithms and Applications, volume 33 of Text, Speech and Language", "author": ["Eneko Agirre", "Philip Edmonds", "editors"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2006}, {"title": "UBC-ALM: Combining k-NN with SVD for WSD", "author": ["Eneko Agirre", "Oier Lopez de Lacalle"], "venue": "In SemEval,", "citeRegEx": "Agirre and Lacalle.,? \\Q2007\\E", "shortCiteRegEx": "Agirre and Lacalle.", "year": 2007}, {"title": "Stanford-UBC at TAC-KBP", "author": ["Eneko Agirre", "Angel X. Chang", "Dan S. Jurafsky", "Christopher D. Manning", "Valentin I. Spitkovsky", "Eric Yeh"], "venue": "In Proceedings of the Text Analysis Conference,", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Web people search: results of the first evaluation and the plan for the second", "author": ["Javier Artiles", "Satoshi Sekine", "Julio Gonzalo"], "venue": "In WWW,", "citeRegEx": "Artiles et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Artiles et al\\.", "year": 2008}, {"title": "WePS 2 evaluation campaign: overview of the web people search clustering task", "author": ["Javier Artiles", "Julio Gonzalo", "Satoshi Sekine"], "venue": "In WePS,", "citeRegEx": "Artiles et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Artiles et al\\.", "year": 2009}, {"title": "Entity-based cross-document coreferencing using the vector space model", "author": ["Amit Bagga", "Breck Baldwin"], "venue": "In COLING-ACL,", "citeRegEx": "Bagga and Baldwin.,? \\Q1998\\E", "shortCiteRegEx": "Bagga and Baldwin.", "year": 1998}, {"title": "Collective entity resolution in relational data", "author": ["Indrajit Bhattacharya", "Lise Getoor"], "venue": "ACM TKDD,", "citeRegEx": "Bhattacharya and Getoor.,? \\Q2007\\E", "shortCiteRegEx": "Bhattacharya and Getoor.", "year": 2007}, {"title": "Using encyclopedic knowledge for named entity disambiguation", "author": ["Razvan C. Bunescu", "Marius Pasca"], "venue": "In EACL,", "citeRegEx": "Bunescu and Pasca.,? \\Q2006\\E", "shortCiteRegEx": "Bunescu and Pasca.", "year": 2006}, {"title": "Stanford-UBC entity linking at TAC-KBP", "author": ["Angel X. Chang", "Valentin I. Spitkovsky", "Christopher D. Manning", "Eneko Agirre"], "venue": "In Proceedings of the Text Analysis Conference,", "citeRegEx": "Chang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2010}, {"title": "Stanford-UBC entity linking at TAC-KBP, again", "author": ["Angel X. Chang", "Valentin I. Spitkovsky", "Christopher D. Manning", "Eneko Agirre"], "venue": "In Proceedings of the Text Analysis Conference,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Collaborative ranking: A case study on entity linking", "author": ["Zheng Chen", "Heng Ji"], "venue": "In EMNLP,", "citeRegEx": "Chen and Ji.,? \\Q2011\\E", "shortCiteRegEx": "Chen and Ji.", "year": 2011}, {"title": "Linking documents to encyclopedic knowledge", "author": ["Andras Csomai", "Rada Mihalcea"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "Csomai and Mihalcea.,? \\Q2008\\E", "shortCiteRegEx": "Csomai and Mihalcea.", "year": 2008}, {"title": "Large-scale named entity disambiguation based on Wikipedia data", "author": ["Silviu Cucerzan"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Cucerzan.,? \\Q2007\\E", "shortCiteRegEx": "Cucerzan.", "year": 2007}, {"title": "Entity disambiguation for knowledge base population", "author": ["Mark Dredze", "Paul McNamee", "Delip Rao", "Adam Gerber", "Tim Finin"], "venue": "In COLING,", "citeRegEx": "Dredze et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dredze et al\\.", "year": 2010}, {"title": "A joint model for entity analysis: Coreference, typing, and linking", "author": ["Greg Durrett", "Dan Klein"], "venue": "In Proceedings of the Transactions of the Association for Computational Linguistics,", "citeRegEx": "Durrett and Klein.,? \\Q2014\\E", "shortCiteRegEx": "Durrett and Klein.", "year": 2014}, {"title": "Resolving personal names in email using context expansion", "author": ["Tamer Elsayed", "Doug Oard", "Galileo Mark Namata"], "venue": "In ACL,", "citeRegEx": "Elsayed et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Elsayed et al\\.", "year": 2008}, {"title": "Scaling Wikipedia-based named entity disambiguation to arbitrary web text", "author": ["Anthony Fader", "Stephen Soderland", "Oren Etzioni"], "venue": "In WikiAI,", "citeRegEx": "Fader et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2009}, {"title": "WordNet: An Electronic Database", "author": ["Christiane Fellbaum", "editor"], "venue": null, "citeRegEx": "Fellbaum and editor.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum and editor.", "year": 1998}, {"title": "Cross-document coreference on a large scale corpus", "author": ["Chung Heong Gooi", "James Allan"], "venue": null, "citeRegEx": "Gooi and Allan.,? \\Q2004\\E", "shortCiteRegEx": "Gooi and Allan.", "year": 2004}, {"title": "Linking entities to a knowledge base with query expansion", "author": ["Swapna Gottipati", "Jing Jiang"], "venue": "In EMNLP,", "citeRegEx": "Gottipati and Jiang.,? \\Q2011\\E", "shortCiteRegEx": "Gottipati and Jiang.", "year": 2011}, {"title": "A graph-based method for entity linking", "author": ["Y. Guo", "W. Che", "T. Liu", "S. Li"], "venue": "In Proceedings of 5th International Joint Conference on Natural Language Processing,", "citeRegEx": "Guo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2011}, {"title": "Evaluating Entity Linking with Wikipedia", "author": ["B. Hachey", "W. Radford", "J. Nothman", "M. Honnibal", "J.R. Curran"], "venue": "Artificial Intelligence,", "citeRegEx": "Hachey et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hachey et al\\.", "year": 2012}, {"title": "A generative entity-mention model for linking entities with knowledge base", "author": ["Xianpei Han", "Le Sun"], "venue": "In ACL HLT,", "citeRegEx": "Han and Sun.,? \\Q2011\\E", "shortCiteRegEx": "Han and Sun.", "year": 2011}, {"title": "An entity-topic model for entity linking", "author": ["Xianpei Han", "Le Sun"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Han and Sun.,? \\Q2012\\E", "shortCiteRegEx": "Han and Sun.", "year": 2012}, {"title": "Robust disambiguation of named entities in text", "author": ["Johannes Hoffart", "Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum"], "venue": "In EMNLP,", "citeRegEx": "Hoffart et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2011}, {"title": "Kore: Keyphrase overlap relatedness for entity disambiguation", "author": ["Johannes Hoffart", "Stephan Seufert", "Dat Ba Nguyen", "Martin Theobald", "Gerhard Weikum"], "venue": "In Proceedings of the 21st ACM international conference on Information and knowledge management,", "citeRegEx": "Hoffart et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2012}, {"title": "Knowledge base population: Successful approaches and challenges", "author": ["Heng Ji", "Ralph Grishman"], "venue": "In ACL HLT,", "citeRegEx": "Ji and Grishman.,? \\Q2011\\E", "shortCiteRegEx": "Ji and Grishman.", "year": 2011}, {"title": "Overview of the TAC 2010 Knowledge Base Population track", "author": ["Heng Ji", "Ralph Grishman", "Hoa Trang Dang", "Kira Griffitt", "Joe Ellis"], "venue": "In TAC,", "citeRegEx": "Ji et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2010}, {"title": "Making large-scale SVM learning practical", "author": ["Thorsten Joachims"], "venue": "Advances in Kernel Methods - Support Vector Learning", "citeRegEx": "Joachims.,? \\Q1999\\E", "shortCiteRegEx": "Joachims.", "year": 1999}, {"title": "Collective annotation of Wikipedia entities in web text", "author": ["Sayali Kulkarni", "Amit Singh", "Ganesh Ramakrishnan", "Soumen Chakrabarti"], "venue": "In KDD,", "citeRegEx": "Kulkarni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2009}, {"title": "LCC approaches to knowledge base population at TAC", "author": ["John Lehmann", "Sean Monahan", "Luke Nezda", "Arnold Jung", "Ying Shi"], "venue": "In TAC,", "citeRegEx": "Lehmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 2010}, {"title": "Unsupervised personal name disambiguation", "author": ["Gideon S. Mann", "David Yarowsky"], "venue": "In CoNLL,", "citeRegEx": "Mann and Yarowsky.,? \\Q2003\\E", "shortCiteRegEx": "Mann and Yarowsky.", "year": 2003}, {"title": "Optimization, maxent models, and conditional estimation without magic", "author": ["Christopher D. Manning", "Dan Klein"], "venue": "In HLT-NAACL,", "citeRegEx": "Manning and Klein.,? \\Q2003\\E", "shortCiteRegEx": "Manning and Klein.", "year": 2003}, {"title": "Supervised corpus-based methods for WSD", "author": ["Lluis Marquez", "Gerard Escudero", "David Martinez", "German Rigau"], "venue": "Word Sense Disambiguation: Algorithms and Applications,", "citeRegEx": "Marquez et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marquez et al\\.", "year": 2006}, {"title": "MUC-7 evaluation of IE technology: Overview of results", "author": ["Elaine Marsh", "Dennis Perzanowski"], "venue": "In MUC,", "citeRegEx": "Marsh and Perzanowski.,? \\Q1998\\E", "shortCiteRegEx": "Marsh and Perzanowski.", "year": 1998}, {"title": "HLTCOE efforts in entity linking at TAC KBP", "author": ["Paul McNamee"], "venue": "In TAC,", "citeRegEx": "McNamee.,? \\Q2010\\E", "shortCiteRegEx": "McNamee.", "year": 2010}, {"title": "Overview of the TAC 2009 Knowledge Base Population track", "author": ["Paul McNamee", "Hoa Dang"], "venue": "In TAC,", "citeRegEx": "McNamee and Dang.,? \\Q2009\\E", "shortCiteRegEx": "McNamee and Dang.", "year": 2009}, {"title": "Wikify!: linking documents to encyclopedic knowledge", "author": ["Rada Mihalcea", "Andras Csomai"], "venue": "In CIKM,", "citeRegEx": "Mihalcea and Csomai.,? \\Q2007\\E", "shortCiteRegEx": "Mihalcea and Csomai.", "year": 2007}, {"title": "The Senseval-3 English lexical sample task", "author": ["Rada Mihalcea", "Timothy Chklovski", "Adam Kilgarriff"], "venue": "In Senseval,", "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Learning to link with Wikipedia", "author": ["David Milne", "Ian H. Witten"], "venue": "In CIKM,", "citeRegEx": "Milne and Witten.,? \\Q2008\\E", "shortCiteRegEx": "Milne and Witten.", "year": 2008}, {"title": "Entity linking meets word sense disambiguation: a unied approach", "author": ["Andrea Moro", "Alessandro Raganato", "Roberto Navigli"], "venue": "Transactions of the Association of Computational Linguistics,", "citeRegEx": "Moro et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Moro et al\\.", "year": 2014}, {"title": "Word sense disambiguation: A survey", "author": ["Roberto Navigli"], "venue": "ACM Computing Surveys,", "citeRegEx": "Navigli.,? \\Q2009\\E", "shortCiteRegEx": "Navigli.", "year": 2009}, {"title": "Exploring entity relations for named entity disambiguation", "author": ["Danuta Ploch"], "venue": "In ACL HLT: Student Session,", "citeRegEx": "Ploch.,? \\Q2011\\E", "shortCiteRegEx": "Ploch.", "year": 2011}, {"title": "SemEval2007 Task-17: English lexical sample SRL and all words", "author": ["Sameer S. Pradhan", "Edward Loper", "Dmitriy Dligach", "Martha Palmer"], "venue": "In SemEval,", "citeRegEx": "Pradhan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2007}, {"title": "Local and global algorithms for disambiguation to Wikipedia", "author": ["Lev-Arie Ratinov", "Dan Roth", "Doug Downey", "Mike Anderson"], "venue": "In ACL,", "citeRegEx": "Ratinov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2011}, {"title": "The English all-words task", "author": ["Benjamin Snyder", "Martha Palmer"], "venue": "In Senseval,", "citeRegEx": "Snyder and Palmer.,? \\Q2004\\E", "shortCiteRegEx": "Snyder and Palmer.", "year": 2004}, {"title": "A cross-lingual dictionary for English Wikipedia concepts", "author": ["Valentin I. Spitkovsky", "Angel X. Chang"], "venue": "In LREC,", "citeRegEx": "Spitkovsky and Chang.,? \\Q2012\\E", "shortCiteRegEx": "Spitkovsky and Chang.", "year": 2012}, {"title": "Introduction to the CoNLL-2003 shared task: language-independent named entity recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder"], "venue": "In CoNLL,", "citeRegEx": "Sang and Meulder.,? \\Q2003\\E", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["Ioannis Tsochantaridis", "Thomas Hofmann", "Thorsten Joachims", "Yasemin Altun"], "venue": "In ICML,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "IIIT Hyderabad at TAC", "author": ["Vasudeva Varma", "Vijay Bharath Reddy", "Sudheer Kovelamudi", "Praveen Bysani", "Santhosh Gsk", "Kiran Kumar", "Kranthi Reddy", "Karuna Kumar", "Nithin Maganti"], "venue": "In TAC,", "citeRegEx": "Varma et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Varma et al\\.", "year": 2009}, {"title": "From information to knowledge: Harvesting entities and relationships from web sources", "author": ["Gerhard Weikum", "Martin Theobald"], "venue": "In PODS,", "citeRegEx": "Weikum and Theobald.,? \\Q2010\\E", "shortCiteRegEx": "Weikum and Theobald.", "year": 2010}, {"title": "Entity linking leveraging automatically generated annotation", "author": ["Wei Zhang", "Jian Su", "Chew Lim Tan", "Wen Ting Wang"], "venue": "In COLING,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Learning to link entities with knowledge base", "author": ["Zhicheng Zheng", "Fangtao Li", "Minlie Huang", "Xiaoyan Zhu"], "venue": "In NAACL HLT,", "citeRegEx": "Zheng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2010}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhi Zhong", "Hwee Tou Ng"], "venue": "In ACL: System Demonstrations,", "citeRegEx": "Zhong and Ng.,? \\Q2010\\E", "shortCiteRegEx": "Zhong and Ng.", "year": 2010}, {"title": "Resolving surface forms to Wikipedia topics", "author": ["Yiping Zhou", "Lan Nie", "Omid Rouhani-Kalleh", "Flavian Vasile", "Scott Gaffney"], "venue": "In COLING,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 50, "context": "It also has many applications in its own right: text mining, information retrieval and the semantic web [Weikum and Theobald, 2010].", "startOffset": 104, "endOffset": 131}, {"referenceID": 36, "context": "The former terminology (EL) stresses the importance of linking a mention to an actual instance in the given knowledge-base [McNamee and Dang, 2009].", "startOffset": 123, "endOffset": 147}, {"referenceID": 53, "context": "The extensive WSD literature [Agirre and Edmonds, 2006, Navigli, 2009] has shown that building a supervised classifier for each target lemma \u2014 the so-called word-expert approach \u2014 outperforms other techniques [Zhong and Ng, 2010].", "startOffset": 209, "endOffset": 229}, {"referenceID": 11, "context": "To our knowledge, this is the first time a NED system following the word-expert approach is reported, although early work on Wikification already hinted at its usefulness [Csomai and Mihalcea, 2008].", "startOffset": 171, "endOffset": 198}, {"referenceID": 46, "context": "The dictionary\u2019s release was documented in a short conference paper [Spitkovsky and Chang, 2012], which we extend with additional explanations and analyses here.", "startOffset": 68, "endOffset": 96}, {"referenceID": 26, "context": "We present a detailed analysis of the performance of the components and variations of our NED system, as applied to the entity linking task of the NIST Text Analysis Conference\u2019s (TAC)6 knowledge-base population (KBP) track [Ji and Grishman, 2011].", "startOffset": 224, "endOffset": 247}, {"referenceID": 24, "context": "In the future we would like to explore other datasets which include all mentions in full documents [Hoffart et al., 2011].", "startOffset": 99, "endOffset": 121}, {"referenceID": 14, "context": "We focus on the candidate generation and disambiguation modules, leaving aside mention detection and the task of NIL detection, where mentions which refer to entities not listed in the knowledge-base have to be detected [Durrett and Klein, 2014].", "startOffset": 220, "endOffset": 245}, {"referenceID": 21, "context": "In WSD, an exhaustive dictionary is provided, while in NED, one has to generate all candidate entities for a target string \u2014 a step that has been shown to be critical to success [Hachey et al., 2012].", "startOffset": 178, "endOffset": 199}, {"referenceID": 6, "context": "Similar problems arise in citation databases, where it is necessary to decide which mentions of authors in bibliographic records refer to the same person [Bhattacharya and Getoor, 2007].", "startOffset": 154, "endOffset": 185}, {"referenceID": 15, "context": "The e-mails task can be tackled by assuming that each address corresponds to a distinct person and that people\u2019s identifying information can be deduced from what they write [Elsayed et al., 2008].", "startOffset": 173, "endOffset": 195}, {"referenceID": 29, "context": "For instance, it presupposes that mentions of named entities have already been identified in text, building up from the named entity recognition (NER) task [Marsh and Perzanowski, 1998, Tjong Kim Sang and De Meulder, 2003]. Each mention may be further labeled with a broad semantic category \u2014 such as names of persons, organizations or locations \u2014 via named entity classification (NEC). This last task is often performed by using gazetteers to cover many known entities, in addition to training a single supervised classifier that outputs category types for input mentions given their specific contexts. Overall, we view NED as a specific instantiation of the record linkage problem, in which the task is to find records referring to the same entity across different data sources, such as data files, databases, books or websites. The term record linkage was first used by Dunn Halbert [1946], in reference to resolving person names across official records held by a government.", "startOffset": 157, "endOffset": 893}, {"referenceID": 3, "context": "More recently, Bagga and Baldwin [1998] focused on cross-document coreference of people by first identifying coreference chains within documents and then comparing the found chains\u2019 contexts across documents.", "startOffset": 15, "endOffset": 40}, {"referenceID": 38, "context": "For instance, the Senseval-3 lexical sample dataset [Mihalcea et al., 2004] contains 2,945 manually annotated occurrences, 98.", "startOffset": 52, "endOffset": 75}, {"referenceID": 45, "context": "5% in the Senseval-3 all-words task [Snyder and Palmer, 2004].", "startOffset": 36, "endOffset": 61}, {"referenceID": 33, "context": "Typically, the problem is modeled using multi-class classifiers [Marquez et al., 2006], with one classifier trained for each target word (a.", "startOffset": 64, "endOffset": 86}, {"referenceID": 38, "context": "On the Senseval3 lexical sample task [Mihalcea et al., 2004], accuracies can reach as high as 72.", "startOffset": 37, "endOffset": 60}, {"referenceID": 45, "context": "4% [Snyder and Palmer, 2004].", "startOffset": 3, "endOffset": 28}, {"referenceID": 7, "context": "The earliest work on NED using Wikipedia is by Bunescu and Pasca [2006], who used article titles, redirects and disambiguation pages to generate candidate entities.", "startOffset": 47, "endOffset": 72}, {"referenceID": 7, "context": "The earliest work on NED using Wikipedia is by Bunescu and Pasca [2006], who used article titles, redirects and disambiguation pages to generate candidate entities. Similarity between a mention\u2019s context and article text provided the rankings, according to tf-idf and cosine similarity. Each article\u2019s term vector was further enriched using words from other articles in the same category. Disambiguation of mentions was local, i.e., performed separately for each one. Cucerzan [2007] followed an overall similar design, but using context vectors that consisted of key words and short phrases extracted from Wikipedia.", "startOffset": 47, "endOffset": 484}, {"referenceID": 18, "context": "7) by Hachey et al. [2012], who also reimplemented the earlier system of Bunescu and Pasca [2006].", "startOffset": 6, "endOffset": 27}, {"referenceID": 7, "context": "[2012], who also reimplemented the earlier system of Bunescu and Pasca [2006]. Fader et al.", "startOffset": 53, "endOffset": 78}, {"referenceID": 7, "context": "[2012], who also reimplemented the earlier system of Bunescu and Pasca [2006]. Fader et al. [2009] also generated candidates as did Cucerzan [2007].", "startOffset": 53, "endOffset": 99}, {"referenceID": 7, "context": "[2012], who also reimplemented the earlier system of Bunescu and Pasca [2006]. Fader et al. [2009] also generated candidates as did Cucerzan [2007]. They introduced prior probabilities \u2014 estimated from the numbers of anchors that refer to entities \u2014 in addition to considering the overlap between the context of a mention and the text of the target articles.", "startOffset": 53, "endOffset": 148}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms.", "startOffset": 44, "endOffset": 387}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification.", "startOffset": 44, "endOffset": 638}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification.", "startOffset": 44, "endOffset": 797}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification.", "startOffset": 44, "endOffset": 817}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification. Zhou et al. [2010] also built on ideas of Milne and Witten [2008] and included, for the first time, relatedness between entities (based on search engine log analysis).", "startOffset": 44, "endOffset": 885}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification. Zhou et al. [2010] also built on ideas of Milne and Witten [2008] and included, for the first time, relatedness between entities (based on search engine log analysis).", "startOffset": 44, "endOffset": 932}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification. Zhou et al. [2010] also built on ideas of Milne and Witten [2008] and included, for the first time, relatedness between entities (based on search engine log analysis). They tested two classifiers (binary and learning-to-rank), with mixed results. Ratinov et al. [2011] utilized link structure \u2014 again, following Milne and Witten [2008] \u2014 to arrive at coherent sets of disambiguations for input documents.", "startOffset": 44, "endOffset": 1135}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification. Zhou et al. [2010] also built on ideas of Milne and Witten [2008] and included, for the first time, relatedness between entities (based on search engine log analysis). They tested two classifiers (binary and learning-to-rank), with mixed results. Ratinov et al. [2011] utilized link structure \u2014 again, following Milne and Witten [2008] \u2014 to arrive at coherent sets of disambiguations for input documents.", "startOffset": 44, "endOffset": 1202}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification. Zhou et al. [2010] also built on ideas of Milne and Witten [2008] and included, for the first time, relatedness between entities (based on search engine log analysis). They tested two classifiers (binary and learning-to-rank), with mixed results. Ratinov et al. [2011] utilized link structure \u2014 again, following Milne and Witten [2008] \u2014 to arrive at coherent sets of disambiguations for input documents. They used a ranker to select best-fitting entities. Although anchor context was used to compute similarity between article texts, it was not tapped for features during classification. Evaluation against their own dataset showed that improvements over local disambiguation were small. Guo et al. [2011] use direct hyperlinks between the target entity and the mentions in the context, using directly the number of such links.", "startOffset": 44, "endOffset": 1573}, {"referenceID": 49, "context": "In 2009, Varma et al. [2009] obtained the best results, using anchors from Wikipedia to train one classifier per target string, combined with querying of mention contexts against an online search engine.", "startOffset": 9, "endOffset": 29}, {"referenceID": 29, "context": "In 2010, Lehmann et al. [2010] did best.", "startOffset": 9, "endOffset": 31}, {"referenceID": 29, "context": "In 2010, Lehmann et al. [2010] did best. They used titles, redirects, anchors and disambiguation pages, as well as Google searches, dynamic generation of acronym expansions, longer mentions and inexact matching. Their system pulled in features from both McNamee [2010] and Milne and Witten [2008] to train a binary classifier.", "startOffset": 9, "endOffset": 269}, {"referenceID": 29, "context": "In 2010, Lehmann et al. [2010] did best. They used titles, redirects, anchors and disambiguation pages, as well as Google searches, dynamic generation of acronym expansions, longer mentions and inexact matching. Their system pulled in features from both McNamee [2010] and Milne and Witten [2008] to train a binary classifier.", "startOffset": 9, "endOffset": 297}, {"referenceID": 29, "context": "In 2010, Lehmann et al. [2010] did best. They used titles, redirects, anchors and disambiguation pages, as well as Google searches, dynamic generation of acronym expansions, longer mentions and inexact matching. Their system pulled in features from both McNamee [2010] and Milne and Witten [2008] to train a binary classifier. McNamee [2010] had the highest-scoring submission among systems that did not use the text in target Wikipedia articles, focusing on the provided KB alone for building the list of candidate entities.", "startOffset": 9, "endOffset": 342}, {"referenceID": 13, "context": "A closely related system by Dredze et al. [2010] included finite-state transducers that had been trained to recognize common abbreviations, such as \u201cJ Miller\u201d for \u201cJennifer Miller.", "startOffset": 28, "endOffset": 49}, {"referenceID": 51, "context": "A similar system [Zhang et al., 2010] made use of a synthetic corpus, with unambiguous occurrences of strings replaced by ambiguous synonyms.", "startOffset": 17, "endOffset": 37}, {"referenceID": 35, "context": "[2010] also use a number of features that resemble those of McNamee [2010]. They evaluated several learning-to-rank systems, with ListNet yielding best results.", "startOffset": 60, "endOffset": 75}, {"referenceID": 35, "context": "[2010] also use a number of features that resemble those of McNamee [2010]. They evaluated several learning-to-rank systems, with ListNet yielding best results. Unfortunately their systems were developed, trained and tested on the same corpus, making it unclear whether their results are comparable to those of other systems evaluated in TAC-KBP 2009. A similar system [Zhang et al., 2010] made use of a synthetic corpus, with unambiguous occurrences of strings replaced by ambiguous synonyms. Manufactured training data was then combined with Wikipedia. Unfortunately, since the overall system was developed and evaluated on the same (2009) corpus, it may have been overfitted.", "startOffset": 60, "endOffset": 642}, {"referenceID": 19, "context": "Recently, Hachey et al. [2012] reimplemented three well-known NED systems [Bunescu and Pasca, 2006, Cucerzan, 2007, Varma et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 11, "context": "None of the published NED systems, except Csomai and Mihalcea [2008], uses the context of anchors in Wikipedia to train a classifier for each mention, as we do.", "startOffset": 42, "endOffset": 69}, {"referenceID": 21, "context": "9Again, with the notable exception of work by Hachey et al. [2012], which we will discuss in detail (see Section 8.", "startOffset": 46, "endOffset": 67}, {"referenceID": 21, "context": "One could view these two components as performing (i) candidate generation, listing all possible entities for a mention; and (ii) candidate selection [Hachey et al., 2012], although our generation module also scores and ranks entities (by popularity).", "startOffset": 150, "endOffset": 171}, {"referenceID": 24, "context": "In the future we would like to extend our work to other datasets which include all mentions in full documents [Hoffart et al., 2011].", "startOffset": 110, "endOffset": 132}, {"referenceID": 48, "context": "Classifier: The second and third rows correspond to the accuracies of a multi-class classifier, based on SVMmulticlass [Tsochantaridis et al., 2004], and a one-versusall approach, using SVMlight binary classifiers [Joachims, 1999].", "startOffset": 119, "endOffset": 148}, {"referenceID": 28, "context": ", 2004], and a one-versusall approach, using SVMlight binary classifiers [Joachims, 1999].", "startOffset": 73, "endOffset": 89}, {"referenceID": 32, "context": "Both alternatives perform worse than our default classification algorithm (maximum entropy with `2regularization), MAXENT [Manning and Klein, 2003].", "startOffset": 122, "endOffset": 147}, {"referenceID": 13, "context": "System KB-only [Dredze et al., 2010] 0.", "startOffset": 15, "endOffset": 36}, {"referenceID": 21, "context": "7063 [Hachey et al., 2012] \u22170.", "startOffset": 5, "endOffset": 26}, {"referenceID": 49, "context": "723 [Varma et al., 2009] 0.", "startOffset": 4, "endOffset": 24}, {"referenceID": 51, "context": "[Zhang et al., 2010] \u22170.", "startOffset": 0, "endOffset": 20}, {"referenceID": 22, "context": "792 [Han and Sun, 2011] \u22170.", "startOffset": 4, "endOffset": 23}, {"referenceID": 20, "context": "79 [Guo et al., 2011] 0.", "startOffset": 3, "endOffset": 21}, {"referenceID": 35, "context": "2010 [McNamee, 2010] 0.", "startOffset": 5, "endOffset": 20}, {"referenceID": 49, "context": "[Varma et al., 2009] 0.", "startOffset": 0, "endOffset": 20}, {"referenceID": 20, "context": "[Guo et al., 2011] 0.", "startOffset": 0, "endOffset": 18}, {"referenceID": 21, "context": "741 [Hachey et al., 2012] \u22170.", "startOffset": 4, "endOffset": 25}, {"referenceID": 30, "context": "784 [Lehmann et al., 2010] 0.", "startOffset": 4, "endOffset": 26}, {"referenceID": 20, "context": "705 Reported by Hachey et al. [2012]. [Guo et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 35, "context": "Here the dictionary performs better than the next best \u201cno context\u201d system [McNamee, 2010] by a large margin: 75% vs.", "startOffset": 75, "endOffset": 90}, {"referenceID": 30, "context": "81% for the next best system [Lehmann et al., 2010].", "startOffset": 29, "endOffset": 51}, {"referenceID": 21, "context": "The entry for Hachey et al. [2012] represents the best of several systems chosen on the test set; their best variant according to development data was, in fact, the system of Varma et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 21, "context": "The entry for Hachey et al. [2012] represents the best of several systems chosen on the test set; their best variant according to development data was, in fact, the system of Varma et al. [2009], which scores 7% lower22", "startOffset": 14, "endOffset": 195}, {"referenceID": 19, "context": "22Note that, since Hachey et al. [2012] reimplemented three well-known systems [Bunescu and Pasca, 2006, Cucerzan, 2007, Varma et al.", "startOffset": 19, "endOffset": 40}, {"referenceID": 49, "context": ", 2010], metaphones [Varma et al., 2009], as well as bolded words in first paragraphs and hatnote templates [Hachey et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 21, "context": ", 2009], as well as bolded words in first paragraphs and hatnote templates [Hachey et al., 2012].", "startOffset": 75, "endOffset": 96}, {"referenceID": 13, "context": ", 2010], finite-state transducers [Dredze et al., 2010], matches with longer mentions [Lehmann et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 30, "context": ", 2010], matches with longer mentions [Lehmann et al., 2010], automatic spelling correction [Zheng et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 52, "context": ", 2010], automatic spelling correction [Zheng et al., 2010], Wikipedia search engines and the \u201cDid you mean.", "startOffset": 39, "endOffset": 59}, {"referenceID": 51, "context": "\u201d functionality [Zhang et al., 2010].", "startOffset": 16, "endOffset": 36}, {"referenceID": 20, "context": "This fact should not be overlooked, since Hachey et al. [2012] found that candidate generation accounted for most of the performance variation in the systems that they reimplemented; in particular, acronym handling, using coreference resolution to find longer mentions, led to substantial improvements.", "startOffset": 42, "endOffset": 63}, {"referenceID": 21, "context": ", Han and Sun [2011] report end-to-end performance of their popularity and name model (roughly equivalent to our dictionary), combined with a NIL detection system, excluding KB- or NIL-only results.", "startOffset": 2, "endOffset": 21}, {"referenceID": 21, "context": "24An exception, Hachey et al. [2012] provide precision and recall of their candidate generation component for KB queries (56.", "startOffset": 16, "endOffset": 37}, {"referenceID": 40, "context": "We would also like to include our classifiers in more complex NED systems, where complementary information like link structure [Moro et al., 2014], similarities between article texts and mention contexts [Hoffart et al.", "startOffset": 127, "endOffset": 146}, {"referenceID": 25, "context": ", 2014], similarities between article texts and mention contexts [Hoffart et al., 2012], as well as global optimization techniques [Ratinov et al.", "startOffset": 65, "endOffset": 87}, {"referenceID": 44, "context": ", 2012], as well as global optimization techniques [Ratinov et al., 2011] could further improve the results.", "startOffset": 51, "endOffset": 73}, {"referenceID": 24, "context": "In the future, we would like to extend our study using datasets which include all mentions in full documents [Hoffart et al., 2011].", "startOffset": 109, "endOffset": 131}, {"referenceID": 21, "context": "In WSD settings, dictionaries are provided, but NED involves constructing possible mappings from strings to entities \u2014 a step that Hachey et al. [2012] showed to be key to success, which we also confirmed experimentally.", "startOffset": 131, "endOffset": 152}], "year": 2016, "abstractText": "Named Entity Disambiguation (NED) is the task of linking a named-entity mention to an instance in a knowledge-base, typically Wikipedia. This task is closely related to word-sense disambiguation (WSD), where the supervised word-expert approach has prevailed. In this work we present, for the first time, the results of the word-expert approach to NED, where one classifier is built for each target entity mention string. The resources necessary to build the system, a dictionary and a set of training instances, have been automatically derived from Wikipedia. We provide empirical evidence of the value of this approach, as well as a study of the differences between WSD and NED, including ambiguity and synonymy statistics.", "creator": "LaTeX with hyperref package"}}}