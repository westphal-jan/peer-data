{"id": "1603.04930", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "Deep Fully-Connected Networks for Video Compressive Sensing", "abstract": "In this work we present a deep learning framework for video compressive sensing. The proposed formulation enables recovery of video frames in a few seconds at significantly improved reconstruction quality compared to previous approaches. Our investigation starts by learning a linear mapping between video sequences and corresponding measured frames which turns out to provide promising results. We then extend the linear formulation to deep fully-connected networks and explore the performance gains using deeper architectures. Our analysis is always driven by the applicability of the proposed framework on existing compressive video architectures. Extensive simulations on several video sequences document the superiority of our approach both quantitatively and qualitatively. Finally, our analysis offers insights into understanding how dataset sizes and number of layers affect reconstruction performance while raising a few points for future investigation.", "histories": [["v1", "Wed, 16 Mar 2016 01:15:35 GMT  (10328kb,D)", "http://arxiv.org/abs/1603.04930v1", "13 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["michael iliadis", "leonidas spinoulas", "aggelos k katsaggelos"], "accepted": false, "id": "1603.04930"}, "pdf": {"name": "1603.04930.pdf", "metadata": {"source": "CRF", "title": "Deep Fully-Connected Networks for Video Compressive Sensing", "authors": ["Michael Iliadis", "Leonidas Spinoulas", "Aggelos K. Katsaggelos"], "emails": ["miliad@u.northwestern.edu", "leonisp@u.northwestern.edu", "aggk@eecs.northwestern.edu"], "sections": [{"heading": "1 Introduction", "text": "It occurs in two incarnations, namely spatial CS and temporal CS architectures, which perform one spatial multiplexing method per measurement and enable video retrieval by accelerating the capture process. In this thesis, we either focus on fast read-out circuits to capture information at video rates [4] or parallelise the single-pixel architecture using multiple sensors, each responsible for capturing a separate spatial area of the scene. [3, 37] In this thesis, we focus on temporal CS multiplexing circuits across the time dimension. Figure 1 shows this process in which a spatial-time volume of the size Wf \u00d7 Hf \u00d7 t = Nf is modulated by binary masks."}, {"heading": "2 Motivation and Related Work", "text": "In simple terms, deep learning attempts to mimic the human brain by forming large, multi-layered neural networks with huge amounts of training samples describing a particular task. Such networks have proved very successful in problems where analytical modeling is not easy or straightforward (e.g. a variety of computer vision tasks [16, 20]. The popularity of neural networks in recent years has led researchers to examine the capabilities of deep architectures even in cases where analytical models often exist and are well understood (e.g. restoration problems [2, 32, 39])."}, {"heading": "3 Deep Networks for Compressed Video", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Linear mapping", "text": "We started our investigation by asking the question: Can training data be used to find a linear mapping W, so that x = Wy? Essentially, the question arises about the inversion of \u03a6 into Equation (1), which of course does not exist. Obviously, such a matrix would be huge to store, but instead, the same logic can be applied to video blocks [22]. We collect a series of training video blocks, which are denoted by xi, i \u00b2 N of size wp \u00b7 hp \u00b7 t = Np. Therefore, the measurement model per block is now yi = \u03a6pxi of size Mp \u00d7 1, with Mp = wp \u00b7 hp and \u03a6p references to the corresponding measurement matrix per block. \u2212 If we collect a series of N video blocks, we get the matrix equation Y = \u03a6pX, (2) where Y = [y1, yN], X = [x1,]."}, {"heading": "3.2 Measurement Matrix Construction", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3.3 Multi-layer Network Architecture", "text": "In this section, we will expand the linear formulation to MLPs and examine performance in deeper structures. We will consider an MLP architecture to learn a nonlinear function f (\u00b7) that maps a measured frame patch yi over several hidden layers onto a video block xi, as shown in Figure 4.Each hidden layer Lk, k = 1,.., K is defined ashk (y) = \u03c3 (bk + Wky), (4) where bk-RNp is the bias vector and Wk is the output weight matrix containing linear filters.The W1-RNp \u00b7 Mp connects yi to the first hidden layer, while for the remaining hidden layers W2 \u2212 K-RNp \u00b7 Np. The last hidden layer is connected to the output layer via bo-RNp and wo-RNp-Np-Se layers without nonlinearity."}, {"heading": "4 Experiments", "text": "We compare our proposed depth architecture both quantitatively and qualitatively with modern approaches. The proposed approaches are evaluated under the assumption of silent measurements or in the presence of measurement noise. Finally, we examine the performance of our methods under various network parameters (e.g. number of layers) and the size of training samples. The measurements PSNR and SSIM were evaluated."}, {"heading": "4.1 Training Data Collection", "text": "For deep neural networks, increasing the number of training samples usually means improved performance. We collected a variety of training samples using 400 high-definition videos from Youtube that depict nature scenes, and the video sequences contain more than 105 images that were converted to grayscale, all of which have nothing to do with the test set. We randomly extracted 10 million video blocks of wp \u00d7 hp \u00b7 t size, with the amount of blocks extracted per video being proportional to their duration. This data was used as output, while the corresponding input was obtained by multiplying each sample by measuring matrix \u03a6p (see Section 3.2 for details). Sample images of the video sequences used for training are shown in Figure 5."}, {"heading": "4.2 Implementation Details", "text": "Our networks were trained for up to 4 x 106 iterations with a mini-batch size of 200. We normalized the input per feature to zero mean and standard deviation one. Weights of each layer were initialized to random values that were evenly distributed (\u2212 1 / \u221a s, 1 / \u221a s), with s being the size of the previous layer [10]. We used stochastic gradient descent (SGD) with an initial crop rate of 0.01 divided by 10 after 3 x 106 iterations. Momentum was set to 0.9 and we continued to use \"2-standard gradient clipping to keep the gradients within a certain range. Gradient clipping is a widely used technique in recurring neural networks to avoid the use of gradients. [27] The threshold of gradient clipping was set to 10."}, {"heading": "4.3 Comparison with Previous Methods", "text": "We compare our methods with the state of the art in order to put them into practice. (D) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S \"S\" S (S) S (S) S (S) S (S) S \"S (S) S (S) S (S) S (S) S (S) S (S) S (S) S\" S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S \"S (S (S) S (S) S (S (S) S (S (S) S (S) S\" S (S (S (S) S (S (S) S (S (S) S \"S (S (S (S) S (S (S (S) S (S\" S (S) S (S (S (S) S (S (S) S (S (S) S (S (S) S (S (S) S \"S (S (S (S\" S (S) S (S (S) S \"S (S (S) S (S (S) S (S) S (S (S) S (S (S) S (S) S (S) S (S (S (S) S) S (S (S (S) S (S) S) S (S (S) S (S (S) S (S) S (S) S (S (S) S (S (S) S (S) S (S) S (S"}, {"heading": "4.4 Reconstruction Results", "text": "The displayed metrics refer to the average performance for reconstructing the first 32 frames of each video sequence, with the second comparing the performance of the best candidate with a subsequent MMLE step. Tables 1 and 7 split the results into two parts. The first part lists the reconstruction performance of the tested approaches without the MMLE step, while the second compares the performance of the best candidate with a subsequent MMLE step. Table 1 highlights the best performative algorithms for each part."}, {"heading": "4.5 Reconstruction Results with Noise", "text": "Previously, we evaluated the proposed algorithms under the assumption of silent measurements. In this subsection, we examine the performance of the presented deep architectures under the presence of measurement noise. In particular, the measurement model of the equation (1) is now modified toy = \u03a6x + n, (6) where n: Mf \u00d7 1 is the additive measurement noise vector. We deal with our best architecture using K = 7 hidden layers and follow two different training programs. In the first, the network is trained on the 10 \u00d7 106 samples as discussed in Section 4.3 (i.e., the same FC7-10M network as before), while in the second, the network is trained using the same data pairs {yi, after adding random smoke noise to each vector yi. Each vector yi was corrupted with a level of noise, so that signal-to-noise ratio (SNR) is uniformly selected."}, {"heading": "4.6 Run Time", "text": "All previous approaches are implemented in MATLAB. Our deep learning methods are implemented in the Caffe package [14] and all algorithms are executed by the same machine. We observe that the deep learning approaches significantly exceed previous approaches in the order of several orders of magnitude. Note that a direct comparison between the methods is not trivial due to the different implementations. However, earlier methods solve an optimization problem during reconstruction, while our MLP is a feed-forward network that requires only a few matrix vector multiplications."}, {"heading": "4.7 Number of Layers and Dataset Size", "text": "However, the improvement achieved by increasing the number of layers (from 4 to 7) for architectures built on small datasets (e.g. 1M) is not significant; this is perhaps expected because it can be argued that more training data would be needed to achieve higher performance with additional layers (i.e. more parameters to train). Intuitively, adding hidden layers allows the network to learn more complex functions. In fact, the reconstruction performance in our 10 million dataset is higher in FC7-10M than in FC4-10M. Increasing the number of hidden layers did not help in our experiments as we did not observe any additional performance improvement."}, {"heading": "5 Conclusions", "text": "To the best of our knowledge, this work represents the first in-depth learning architecture for the reconstruction of temporally compressed video recordings. We demonstrated superior performance over existing algorithms while reducing the reconstruction time to a few seconds. At the same time, we focused on the applicability of our framework to existing compressive camera architectures, indicating that their commercial use might be feasible. We believe that this work can be expanded in three directions: 1) studying the performance of varying architectures such as RNNs, 2) studying the formation of deeper architectures, and 3) finally investigating the reconstruction performance in real video sequences recorded by a temporally compressing camera."}], "references": [{"title": "Adaptive multi-column deep neural networks with application to robust image denoising", "author": ["F. Agostinelli", "M.R. Anderson", "H. Lee"], "venue": "Adv. Neural Inf. Process. Syst. 26, pages 1493\u20131501.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Image denoising: Can plain neural networks compete with BM3D? In Proc", "author": ["H.C. Burger", "C.J. Schuler", "S. Harmeling"], "venue": "IEEE Conf. Comp. Vision Pattern Recognition, pages 2392\u20132399, June", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "FPA-CS: Focal plane array-based compressive imaging in short-wave infrared", "author": ["H. Chen", "M.S. Asif", "A.C. Sankaranarayanan", "A. Veeraraghavan"], "venue": "Proc. IEEE Conf. Comp. Vision Pattern Recognition, pages 2358\u20132366, June", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "High speed single-pixel imaging via time domain compressive sampling", "author": ["H. Chen", "Z. Weng", "Y. Liang", "C. Lei", "F. Xing", "M. Chen", "S. Xie"], "venue": "CLEO: 2014, page JTh2A.132. Optical Society of America,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep network cascade for image super-resolution", "author": ["Z. Cui", "H. Chang", "S. Shan", "B. Zhong", "X. Chen"], "venue": "Computer Vision \u2013 ECCV 2014, volume 8693 of Lecture Notes in Computer Science, pages 49\u201364. Springer International Publishing,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Image super-resolution using deep convolutional networks", "author": ["C. Dong", "C. Loy", "K. He", "X. Tang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 38(2):295\u2013307, Feb.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Single-Pixel imaging via compressive sampling", "author": ["M.F. Duarte", "M.A. Davenport", "D. Takhar", "J.N. Laska", "T. Sun", "K.F. Kelly", "R.G. Baraniuk"], "venue": "IEEE Signal Process. Mag., 25(2):83\u201391, Mar.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Smart pixel imaging with computational-imaging arrays", "author": ["C. Fernandez-Cull", "B.M. Tyrrell", "R. D\u2019Onofrio", "A. Bolstad", "J. Lin", "J.W. Little", "M. Blackwell", "M. Renzi", "M. Kelly"], "venue": "In Proc. SPIE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Single-Shot compressed ultrafast photography at one hundred billion frames per second", "author": ["L. Gao", "J. Liang", "C. Li", "L.V. Wang"], "venue": "Nature, 516:74\u201377,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proc. Int. Conf. Artificial Intelligence and Statistics,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Flutter shutter video camera for compressive sensing of videos", "author": ["J. Holloway", "A.C. Sankaranarayanan", "A. Veeraraghavan", "S. Tambe"], "venue": "IEEE Int. Conf. Computational Photography, pages 1\u20139, April", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Bidirectional recurrent convolutional networks for multi-frame super-resolution", "author": ["Y. Huang", "W. Wang", "L. Wang"], "venue": "Adv. Neural Inf. Process. Syst. 28, pages 235\u2013243.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proc. ACM Int. Conf. Multimedia, MM \u201914, pages 675\u2013678, New York, NY, USA,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "High spatio-temporal resolution video with compressed sensing", "author": ["R. Koller", "L. Schmid", "N. Matsuda", "T. Niederberger", "L. Spinoulas", "O. Cossairt", "G. Schuster", "A.K. Katsaggelos"], "venue": "Opt. Express, 23(12):15992\u2013 16007, June", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Adv. Neural Inf. Process. Syst. 25, pages 1097\u20131105.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "ReconNet: Non-iterative reconstruction of images from compressively sensed random measurements", "author": ["K. Kulkarni", "S. Lohit", "P.K. Turaga", "R. Kerviche", "A. Ashok"], "venue": "CoRR, abs/1601.06892,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444, May", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J. Denker", "D. Henderson", "R. Howard", "W. Hubbard", "L. Jackel"], "venue": "Neural Computation, 1(4):541\u2013551, Dec.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE, 86(11):2278\u20132324, Nov.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient space-time sampling with pixel-wise coded exposure for high speed imaging", "author": ["D. Liu", "J. Gu", "Y. Hitomi", "M. Gupta", "T. Mitsunaga", "S.K. Nayar"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 99:1,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient space-time sampling with pixel-wise coded exposure for high-speed imaging", "author": ["D. Liu", "J. Gu", "Y. Hitomi", "M. Gupta", "T. Mitsunaga", "S.K. Nayar"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 36(2):248\u2013260, Feb", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Coded aperture compressive temporal imaging", "author": ["P. Llull", "X. Liao", "X. Yuan", "J. Yang", "D. Kittle", "L. Carin", "G. Sapiro", "D.J. Brady"], "venue": "Opt. Express, 21(9):10526\u201310545, May", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "A deep learning approach to structured signal recovery", "author": ["A. Mousavi", "A.B. Patel", "R.G. Baraniuk"], "venue": "CoRR, abs/1508.04065,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proc. Int. Conf. Machine Learning, pages 807\u2013814,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Real time compressive sensing video reconstruction in hardware", "author": ["G. Orchard", "J. Zhang", "Y. Suo", "M. Dao", "D.T. Nguyen", "S. Chin", "C. Posch", "T.D. Tran", "R. Etienne- Cummings"], "venue": "IEEE Trans. Emerg. Sel. Topics Circuits Syst., 2(3):604\u2013615, Sept.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML (3), volume 28 of JMLR Proceedings, pages 1310\u20131318. JMLR.org,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to segment object candidates", "author": ["P.O. Pinheiro", "R. Collobert", "P. Doll\u00e1r"], "venue": "CoRR, abs/1506.06204,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "P2C2: Programmable pixel compressive camera for high speed imaging", "author": ["D. Reddy", "A. Veeraraghavan", "R. Chellappa"], "venue": "Proc. IEEE Conf. Comp. Vision Pattern Recognition, pages 329\u2013336, June", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Shepard convolutional neural networks", "author": ["J.S. Ren", "L. Xu", "Q. Yan", "W. Sun"], "venue": "Adv. Neural Inf. Process. Syst. 28, pages 901\u2013909.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Neurocomputing: Foundations of research", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "chapter Learning Representations by Back-propagating Errors, pages 696\u2013699. MIT Press, Cambridge, MA, USA,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1988}, {"title": "A machine learning approach for non-blind image deconvolution", "author": ["C. Schuler", "H. Burger", "S. Harmeling", "B. Scholkopf"], "venue": "Proc. IEEE Conf. Comp. Vision Pattern Recognition, pages 1067\u20131074, June", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Video compressive sensing with on-chip programmable subsampling", "author": ["L. Spinoulas", "K. He", "O. Cossairt", "A. Katsaggelos"], "venue": "Proc. IEEE Conf. Comp. Vision Pattern Recognition Workshops, pages 49\u201357, June", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a convolutional neural network for non-uniform motion blur removal", "author": ["J. Sun", "W. Cao", "Z. Xu", "J. Ponce"], "venue": "Proc. IEEE Conf. Comp. Vision Pattern Recognition, pages 769\u2013777, June", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "IEEE Int. Conf. Computer Vision, pages 4489\u20134497, Dec", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "J. Mach. Learn. Res., 11:3371\u20133408, Dec.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Lisens- a scalable architecture for video compressive sensing", "author": ["J. Wang", "M. Gupta", "A.C. Sankaranarayanan"], "venue": "Proc. IEEE Conf. Comp. Photography, pages 1\u20139, April", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Trans. Image Process., 13(4):600\u2013612, April", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2004}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["J. Xie", "L. Xu", "E. Chen"], "venue": "Adv. Neural Inf. Process. Syst. 25, pages 341\u2013349.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep convolutional neural network for image deconvolution", "author": ["L. Xu", "J.S. Ren", "C. Liu", "J. Jia"], "venue": "Adv. Neural Inf. Process. Syst. 27, pages 1790\u20131798.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Compressive sensing by learning a gaussian mixture model from measurements", "author": ["J. Yang", "X. Liao", "X. Yuan", "P. Llull", "D.J. Brady", "G. Sapiro", "L. Carin"], "venue": "IEEE Trans. Image Processing, 24(1):106\u2013119, Jan.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Video compressive sensing using gaussian mixture models", "author": ["J. Yang", "X. Yuan", "X. Liao", "P. Llull", "D.J. Brady", "G. Sapiro", "L. Carin"], "venue": "IEEE Trans. Image Processing, 23(11):4863\u20134878, Nov.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "Spatial video CS architectures stem from the well-known single-pixel-camera [7], which performs spatial multiplexing per measurement, and enable video recovery by expediting the capturing process.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "They either employ fast readout circuitry to capture information at video rates [4] or parallelize the single-pixel architecture using multiple sensors, each one responsible for sampling a separate spatial area of the scene [3, 37].", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": "They either employ fast readout circuitry to capture information at video rates [4] or parallelize the single-pixel architecture using multiple sensors, each one responsible for sampling a separate spatial area of the scene [3, 37].", "startOffset": 224, "endOffset": 231}, {"referenceID": 36, "context": "They either employ fast readout circuitry to capture information at video rates [4] or parallelize the single-pixel architecture using multiple sensors, each one responsible for sampling a separate spatial area of the scene [3, 37].", "startOffset": 224, "endOffset": 231}, {"referenceID": 2, "context": "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].", "startOffset": 116, "endOffset": 134}, {"referenceID": 36, "context": "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].", "startOffset": 116, "endOffset": 134}, {"referenceID": 8, "context": "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].", "startOffset": 116, "endOffset": 134}, {"referenceID": 20, "context": "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].", "startOffset": 116, "endOffset": 134}, {"referenceID": 28, "context": "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].", "startOffset": 116, "endOffset": 134}, {"referenceID": 14, "context": "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].", "startOffset": 184, "endOffset": 192}, {"referenceID": 22, "context": "Digital micromirror devices (DMD), spatial light modulators (SLM) and liquid crystal on silicon (LCoS) were used in [3, 37, 9, 21, 29] while translating printed masks were employed in [15, 23].", "startOffset": 184, "endOffset": 192}, {"referenceID": 7, "context": "Moreover, a few architectures have eliminated additional optical elements by directly programming the chip\u2019s readout mode through hardware circuitry modifications [8, 26, 33].", "startOffset": 163, "endOffset": 174}, {"referenceID": 25, "context": "Moreover, a few architectures have eliminated additional optical elements by directly programming the chip\u2019s readout mode through hardware circuitry modifications [8, 26, 33].", "startOffset": 163, "endOffset": 174}, {"referenceID": 32, "context": "Moreover, a few architectures have eliminated additional optical elements by directly programming the chip\u2019s readout mode through hardware circuitry modifications [8, 26, 33].", "startOffset": 163, "endOffset": 174}, {"referenceID": 2, "context": ", using sparsity models [3, 12], combining sparsity and dictionary learning [22] or using Gaussian mixture models [41, 42]) are often too computationally intensive, rendering the reconstruction process painfully slow.", "startOffset": 24, "endOffset": 31}, {"referenceID": 11, "context": ", using sparsity models [3, 12], combining sparsity and dictionary learning [22] or using Gaussian mixture models [41, 42]) are often too computationally intensive, rendering the reconstruction process painfully slow.", "startOffset": 24, "endOffset": 31}, {"referenceID": 21, "context": ", using sparsity models [3, 12], combining sparsity and dictionary learning [22] or using Gaussian mixture models [41, 42]) are often too computationally intensive, rendering the reconstruction process painfully slow.", "startOffset": 76, "endOffset": 80}, {"referenceID": 40, "context": ", using sparsity models [3, 12], combining sparsity and dictionary learning [22] or using Gaussian mixture models [41, 42]) are often too computationally intensive, rendering the reconstruction process painfully slow.", "startOffset": 114, "endOffset": 122}, {"referenceID": 41, "context": ", using sparsity models [3, 12], combining sparsity and dictionary learning [22] or using Gaussian mixture models [41, 42]) are often too computationally intensive, rendering the reconstruction process painfully slow.", "startOffset": 114, "endOffset": 122}, {"referenceID": 17, "context": "Deep learning [18] is a burgeoning research field which has demonstrated state-of-the-art performance in a multitude of machine learning and computer vision tasks, such as image recognition [11] or object detection [28].", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "Deep learning [18] is a burgeoning research field which has demonstrated state-of-the-art performance in a multitude of machine learning and computer vision tasks, such as image recognition [11] or object detection [28].", "startOffset": 190, "endOffset": 194}, {"referenceID": 27, "context": "Deep learning [18] is a burgeoning research field which has demonstrated state-of-the-art performance in a multitude of machine learning and computer vision tasks, such as image recognition [11] or object detection [28].", "startOffset": 215, "endOffset": 219}, {"referenceID": 15, "context": ", a variety of computer vision tasks [16, 20]).", "startOffset": 37, "endOffset": 45}, {"referenceID": 19, "context": ", a variety of computer vision tasks [16, 20]).", "startOffset": 37, "endOffset": 45}, {"referenceID": 1, "context": ", restoration problems [2, 32, 39]).", "startOffset": 23, "endOffset": 34}, {"referenceID": 31, "context": ", restoration problems [2, 32, 39]).", "startOffset": 23, "endOffset": 34}, {"referenceID": 38, "context": ", restoration problems [2, 32, 39]).", "startOffset": 23, "endOffset": 34}, {"referenceID": 1, "context": "More specifically, investigators have employed a variety of architectures: deep fully-connected networks or multi-layer perceptrons (MLPs) [2, 32]; stacked denoising auto-encoders (SDAEs) [1, 5, 36, 39], which are MLPs whose layers are pre-trained to provide improved weight initializa-", "startOffset": 139, "endOffset": 146}, {"referenceID": 31, "context": "More specifically, investigators have employed a variety of architectures: deep fully-connected networks or multi-layer perceptrons (MLPs) [2, 32]; stacked denoising auto-encoders (SDAEs) [1, 5, 36, 39], which are MLPs whose layers are pre-trained to provide improved weight initializa-", "startOffset": 139, "endOffset": 146}, {"referenceID": 0, "context": "More specifically, investigators have employed a variety of architectures: deep fully-connected networks or multi-layer perceptrons (MLPs) [2, 32]; stacked denoising auto-encoders (SDAEs) [1, 5, 36, 39], which are MLPs whose layers are pre-trained to provide improved weight initializa-", "startOffset": 188, "endOffset": 202}, {"referenceID": 4, "context": "More specifically, investigators have employed a variety of architectures: deep fully-connected networks or multi-layer perceptrons (MLPs) [2, 32]; stacked denoising auto-encoders (SDAEs) [1, 5, 36, 39], which are MLPs whose layers are pre-trained to provide improved weight initializa-", "startOffset": 188, "endOffset": 202}, {"referenceID": 35, "context": "More specifically, investigators have employed a variety of architectures: deep fully-connected networks or multi-layer perceptrons (MLPs) [2, 32]; stacked denoising auto-encoders (SDAEs) [1, 5, 36, 39], which are MLPs whose layers are pre-trained to provide improved weight initializa-", "startOffset": 188, "endOffset": 202}, {"referenceID": 38, "context": "More specifically, investigators have employed a variety of architectures: deep fully-connected networks or multi-layer perceptrons (MLPs) [2, 32]; stacked denoising auto-encoders (SDAEs) [1, 5, 36, 39], which are MLPs whose layers are pre-trained to provide improved weight initializa-", "startOffset": 188, "endOffset": 202}, {"referenceID": 5, "context": "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].", "startOffset": 43, "endOffset": 66}, {"referenceID": 18, "context": "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].", "startOffset": 43, "endOffset": 66}, {"referenceID": 29, "context": "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].", "startOffset": 43, "endOffset": 66}, {"referenceID": 33, "context": "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].", "startOffset": 43, "endOffset": 66}, {"referenceID": 36, "context": "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].", "startOffset": 43, "endOffset": 66}, {"referenceID": 39, "context": "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].", "startOffset": 43, "endOffset": 66}, {"referenceID": 12, "context": "tion; convolutional neural networks (CNNs) [6, 19, 30, 34, 37, 40] and recurrent neural networks (RNNs) [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 23, "context": "Two recent studies, utilizing SDAEs [24] or CNNs [17], have been presented on spatial CS for still images exhibiting promising performance.", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "Two recent studies, utilizing SDAEs [24] or CNNs [17], have been presented on spatial CS for still images exhibiting promising performance.", "startOffset": 49, "endOffset": 53}, {"referenceID": 1, "context": "Our approach differs from prior 2D image restoration architectures [2, 32] since we are recovering a 3D volume from 2D measurements.", "startOffset": 67, "endOffset": 74}, {"referenceID": 31, "context": "Our approach differs from prior 2D image restoration architectures [2, 32] since we are recovering a 3D volume from 2D measurements.", "startOffset": 67, "endOffset": 74}, {"referenceID": 21, "context": "Clearly, such a matrix would be huge to store but, instead, one can apply the same logic on video blocks [22].", "startOffset": 105, "endOffset": 109}, {"referenceID": 37, "context": "Figure 2 depicts the average peak signalto-noise ratio (PSNR) and structural similarity metric (SSIM) [38] for the reconstruction of 14 video", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "Such a matrix can be straightforwardly implemented on existing systems employing DMDs, SLMs or LCoS [3, 9, 21, 29, 37].", "startOffset": 100, "endOffset": 118}, {"referenceID": 8, "context": "Such a matrix can be straightforwardly implemented on existing systems employing DMDs, SLMs or LCoS [3, 9, 21, 29, 37].", "startOffset": 100, "endOffset": 118}, {"referenceID": 20, "context": "Such a matrix can be straightforwardly implemented on existing systems employing DMDs, SLMs or LCoS [3, 9, 21, 29, 37].", "startOffset": 100, "endOffset": 118}, {"referenceID": 28, "context": "Such a matrix can be straightforwardly implemented on existing systems employing DMDs, SLMs or LCoS [3, 9, 21, 29, 37].", "startOffset": 100, "endOffset": 118}, {"referenceID": 36, "context": "Such a matrix can be straightforwardly implemented on existing systems employing DMDs, SLMs or LCoS [3, 9, 21, 29, 37].", "startOffset": 100, "endOffset": 118}, {"referenceID": 14, "context": "At the same time, in systems utilizing translating masks [15, 23], a repeated mask can be printed and shifted appropriately to produce the same effect.", "startOffset": 57, "endOffset": 65}, {"referenceID": 22, "context": "At the same time, in systems utilizing translating masks [15, 23], a repeated mask can be printed and shifted appropriately to produce the same effect.", "startOffset": 57, "endOffset": 65}, {"referenceID": 24, "context": "The non-linear function \u03c3(\u00b7) is the rectified linear unit (ReLU) [25] defined as, \u03c3(y) = max(0, y).", "startOffset": 65, "endOffset": 69}, {"referenceID": 34, "context": "Following that, one could argue that the subsequent layers could be 3D Convolutional layers [35].", "startOffset": 92, "endOffset": 96}, {"referenceID": 21, "context": "Such small block sizes have provided good reconstruction quality in dictionary learning approaches used for CS video reconstruction [22].", "startOffset": 132, "endOffset": 136}, {"referenceID": 30, "context": "The set of parameters is denoted as \u03b8 = {b1\u2212K ,bo,W1\u2212K ,Wo} and is updated by the backpropagation algorithm [31] minimizing the quadratic error between the set of training mapped measurements f(yi; \u03b8) and the corresponding video blocks xi.", "startOffset": 108, "endOffset": 112}, {"referenceID": 9, "context": "The weights of each layer were initialized to random values uniformly distributed in (\u22121/\u221as, 1/\u221as), where s is the size of the previous layer [10].", "startOffset": 142, "endOffset": 146}, {"referenceID": 26, "context": "Gradient clipping is a widely used technique in recurrent neural networks to avoid exploding gradients [27].", "startOffset": 103, "endOffset": 107}, {"referenceID": 41, "context": "We compare our method with the state-of-the-art video compressive sensing methods: \u2022 GMM-TP, a Gaussian mixture model (GMM)-based algorithm [42].", "startOffset": 140, "endOffset": 144}, {"referenceID": 40, "context": "\u2022 MMLE-GMM, a maximum marginal likelihood estimator (MMLE), that maximizes the likelihood of the GMM of the underlying signals given only their linear compressive measurements [41].", "startOffset": 176, "endOffset": 180}, {"referenceID": 40, "context": "For temporal CS reconstruction, data driven models usually perform better than standard sparsitybased schemes [41, 42].", "startOffset": 110, "endOffset": 118}, {"referenceID": 41, "context": "For temporal CS reconstruction, data driven models usually perform better than standard sparsitybased schemes [41, 42].", "startOffset": 110, "endOffset": 118}, {"referenceID": 21, "context": "Indeed, both GMM-TP and MMLE-GMM have demonstrated superior performance compared to existing approaches in the literature such as Total-Variation (TV) or dictionary learning [22, 41, 42], hence we did not include experiments with the latter methods.", "startOffset": 174, "endOffset": 186}, {"referenceID": 40, "context": "Indeed, both GMM-TP and MMLE-GMM have demonstrated superior performance compared to existing approaches in the literature such as Total-Variation (TV) or dictionary learning [22, 41, 42], hence we did not include experiments with the latter methods.", "startOffset": 174, "endOffset": 186}, {"referenceID": 41, "context": "Indeed, both GMM-TP and MMLE-GMM have demonstrated superior performance compared to existing approaches in the literature such as Total-Variation (TV) or dictionary learning [22, 41, 42], hence we did not include experiments with the latter methods.", "startOffset": 174, "endOffset": 186}, {"referenceID": 41, "context": "In GMM-TP [42] we followed the settings proposed by the authors and used our training data (randomly selecting 20, 000 samples) to train the underlying GMM parameters.", "startOffset": 10, "endOffset": 14}, {"referenceID": 40, "context": "MMLE [41] is a self-training method but it is sensitive to initialization.", "startOffset": 5, "endOffset": 9}, {"referenceID": 40, "context": "In [41], the GMM-TP [42] with full overlapping patches (denoted in our experiments as GMM-1) was used to initialize the MMLE.", "startOffset": 3, "endOffset": 7}, {"referenceID": 41, "context": "In [41], the GMM-TP [42] with full overlapping patches (denoted in our experiments as GMM-1) was used to initialize the MMLE.", "startOffset": 20, "endOffset": 24}, {"referenceID": 40, "context": "\u2022 FC7-10M+MMLE, a K = 7 MLP trained on 10 \u00d7 10 samples which is used as an initialization to the MMLE [41] method.", "startOffset": 102, "endOffset": 106}, {"referenceID": 40, "context": "Figure 6: Qualitative reconstruction comparison of frames from two video sequences between our methods and GMM-1 [41], GMM-1+MMLE [41].", "startOffset": 113, "endOffset": 117}, {"referenceID": 40, "context": "Figure 6: Qualitative reconstruction comparison of frames from two video sequences between our methods and GMM-1 [41], GMM-1+MMLE [41].", "startOffset": 130, "endOffset": 134}, {"referenceID": 21, "context": "They involve a set of videos that were used for dictionary training in [22], provided by the authors, as well as the \u201cBasketball\u201d video sequence used by [41].", "startOffset": 71, "endOffset": 75}, {"referenceID": 40, "context": "They involve a set of videos that were used for dictionary training in [22], provided by the authors, as well as the \u201cBasketball\u201d video sequence used by [41].", "startOffset": 153, "endOffset": 157}, {"referenceID": 40, "context": "The first part lists reconstruction performance of the tested approaches without the MMLE step, while the second compares the performance of the best candidate proposed and previous methods with a subsequent MMLE step [41].", "startOffset": 218, "endOffset": 222}, {"referenceID": 41, "context": "Reconstruction Method Video Sequence Metric W-10M FC7-10M GMM-4 [42] GMM-1 [41] FC7-10M +MMLE GMM-1 +MMLE [41] Electric Ball PSNR 40.", "startOffset": 64, "endOffset": 68}, {"referenceID": 40, "context": "Reconstruction Method Video Sequence Metric W-10M FC7-10M GMM-4 [42] GMM-1 [41] FC7-10M +MMLE GMM-1 +MMLE [41] Electric Ball PSNR 40.", "startOffset": 75, "endOffset": 79}, {"referenceID": 40, "context": "Reconstruction Method Video Sequence Metric W-10M FC7-10M GMM-4 [42] GMM-1 [41] FC7-10M +MMLE GMM-1 +MMLE [41] Electric Ball PSNR 40.", "startOffset": 106, "endOffset": 110}, {"referenceID": 40, "context": "Specifically, the average PSNR improvement of FC7-10M over the GMM-1 [41] is 2.", "startOffset": 69, "endOffset": 73}, {"referenceID": 40, "context": "When these two methods are used to initialize the MMLE [41] algorithm, the average PSNR gain of FC7-10M+MMLE over the GMM-1+MMLE [41] is 1.", "startOffset": 55, "endOffset": 59}, {"referenceID": 40, "context": "When these two methods are used to initialize the MMLE [41] algorithm, the average PSNR gain of FC7-10M+MMLE over the GMM-1+MMLE [41] is 1.", "startOffset": 129, "endOffset": 133}, {"referenceID": 41, "context": "Figure 8: Qualitative reconstruction performance of video frames between the proposed method FC7-10M and the previous method GMM-4 [42].", "startOffset": 131, "endOffset": 135}, {"referenceID": 41, "context": "Figure 9: PSNR comparison for all the frames of 3 video sequences between the proposed method FC7-10M and the previous method GGM-4 [42].", "startOffset": 132, "endOffset": 136}, {"referenceID": 41, "context": "Figure 9 compares the PSNR for all the frames of 3 video sequences using our FC7-10M algorithm and the fastest previous method GMM-4 [42], while Figure 8 depicts representative snapshots for some of them.", "startOffset": 133, "endOffset": 137}, {"referenceID": 14, "context": "The varying PSNR performance across the frames of a 16 frame block is consistent for both algorithms and is reminiscent of the reconstruction tendency observed in other video CS papers in the literature [15, 23, 41, 42].", "startOffset": 203, "endOffset": 219}, {"referenceID": 22, "context": "The varying PSNR performance across the frames of a 16 frame block is consistent for both algorithms and is reminiscent of the reconstruction tendency observed in other video CS papers in the literature [15, 23, 41, 42].", "startOffset": 203, "endOffset": 219}, {"referenceID": 40, "context": "The varying PSNR performance across the frames of a 16 frame block is consistent for both algorithms and is reminiscent of the reconstruction tendency observed in other video CS papers in the literature [15, 23, 41, 42].", "startOffset": 203, "endOffset": 219}, {"referenceID": 41, "context": "The varying PSNR performance across the frames of a 16 frame block is consistent for both algorithms and is reminiscent of the reconstruction tendency observed in other video CS papers in the literature [15, 23, 41, 42].", "startOffset": 203, "endOffset": 219}, {"referenceID": 1, "context": ", [2]) or denoising algorithm to remove the noise artifacts.", "startOffset": 2, "endOffset": 5}, {"referenceID": 13, "context": "Our deep learning methods are implemented in Caffe package [14] and all algorithms were executed by the same machine.", "startOffset": 59, "endOffset": 63}, {"referenceID": 41, "context": "Figure 11: Qualitative reconstruction comparison between our methods and GMM-4 [42], GMM1 [41] under different levels of measurement noise.", "startOffset": 79, "endOffset": 83}, {"referenceID": 40, "context": "Figure 11: Qualitative reconstruction comparison between our methods and GMM-4 [42], GMM1 [41] under different levels of measurement noise.", "startOffset": 90, "endOffset": 94}], "year": 2016, "abstractText": "In this work we present a deep learning framework for video compressive sensing. The proposed formulation enables recovery of video frames in a few seconds at significantly improved reconstruction quality compared to previous approaches. Our investigation starts by learning a linear mapping between video sequences and corresponding measured frames which turns out to provide promising results. We then extend the linear formulation to deep fully-connected networks and explore the performance gains using deeper architectures. Our analysis is always driven by the applicability of the proposed framework on existing compressive video architectures. Extensive simulations on several video sequences document the superiority of our approach both quantitatively and qualitatively. Finally, our analysis offers insights into understanding how dataset sizes and number of layers affect reconstruction performance while raising a few points for future investigation.", "creator": "LaTeX with hyperref package"}}}