{"id": "1603.04947", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "On the Complexity of One-class SVM for Multiple Instance Learning", "abstract": "In traditional multiple instance learning (MIL), both positive and negative bags are required to learn a prediction function. However, a high human cost is needed to know the label of each bag---positive or negative. Only positive bags contain our focus (positive instances) while negative bags consist of noise or background (negative instances). So we do not expect to spend too much to label the negative bags. Contrary to our expectation, nearly all existing MIL methods require enough negative bags besides positive ones. In this paper we propose an algorithm called \"Positive Multiple Instance\" (PMI), which learns a classifier given only a set of positive bags. So the annotation of negative bags becomes unnecessary in our method. PMI is constructed based on the assumption that the unknown positive instances in positive bags be similar each other and constitute one compact cluster in feature space and the negative instances locate outside this cluster. The experimental results demonstrate that PMI achieves the performances close to or a little worse than those of the traditional MIL algorithms on benchmark and real data sets. However, the number of training bags in PMI is reduced significantly compared with traditional MIL algorithms.", "histories": [["v1", "Wed, 16 Mar 2016 03:30:59 GMT  (236kb,D)", "http://arxiv.org/abs/1603.04947v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhen hu", "zhuyin xue"], "accepted": false, "id": "1603.04947"}, "pdf": {"name": "1603.04947.pdf", "metadata": {"source": "CRF", "title": "On the Complexity of One-class SVM for Multiple Instance Learning", "authors": ["Zhen Hu", "Zhuyin Xue"], "emails": ["49859211@qq.com"], "sections": [{"heading": null, "text": "In traditional Multiple Instance Learning (MIL), both positive and negative pockets are required to learn a predictive function. However, high human costs are required to know the label of each bag - positive or negative. Only positive pockets contain our focus (positive instances), while negative pockets consist of noise or background (negative instances). Therefore, we do not expect to spend too much money on labeling the negative pockets. Contrary to our expectations, almost all existing MIL methods require both positive pockets and enough negative pockets. In this paper, we propose an algorithm called Positive Multiple Instance (PMI), which learns a classifier that only indicates a set of positive pockets. Therefore, the negative pocket comment is unnecessary in our methodology. PMI is based on the assumption that the unknown positive instances in positive pockets are similar to each other and form a compact cluster in feature space and the negative instances outside of this cluster. Experimental results show that MIL achieves that the benchmark is close to or lower than the PMI."}, {"heading": "1 Introduction", "text": "In fact, most people who are in a position to decide for themselves what they want and what they want are also able to move, \"he told the German Press Agency in an interview.\" I don't think they want that, \"he said.\" But I don't think they want that. \"He added,\" I don't think they want that. \"He added,\" I don't think they want that, but I don't think they want that. \""}, {"heading": "2 Our Proposed Algorithm", "text": "In this section we propose a PMI algorithm. A class SVM [18] is involved in the PMI, so here is a brief overview of a class SVM."}, {"heading": "2.1 Review on One-Class SVM", "text": "The formal definition of the single-class function SVM is described as follows: N d-dimensional instances x1, x2,..., xN in the attribute space Rd are given, where R denotes the real number field. The task is to learn a predictive function, whereby most predefined instances are captured in a small area of the attribute space and otherwise. A class SVM maximizes the margin between the training instances and the origin in the attribute space in order to obtain a decision hyper level by the following formulation: min w, \u03c1, 0, 1) is a parameter for balancing the regularization w \u00b2 2 + 1 \u03bdi \u2212 \u03bdi (xi (xi))))) lies within a range within which the attribute boundary lies."}, {"heading": "2.2 Our Proposed Algorithm", "text": "Our proposed PMI algorithm is presented in detail in this subsection. At the beginning, we give the formal definition of the problem that PMI solves. As a training set, a collection of bags {B1, B2,..., BN} is given. The i-th bag in the training set Bi contains nid-dimensional instances Bi = [Bi1,..., BiNi], and Bij-Rd is a d-dimensional instance, j = 1,..., Ni. We define a d-n matrix that collects all instances B = [B1,..., BN], where n indicates the total number of all instances in N-pockets. At least one positive instance is in each bag of the training set, but it is not known which instance is positive. Similarly, like the label rule in previous MIL approaches, a bag is positive if it contains at least one positive instance and is otherwise negative. Our goal is to learn a function to predict the label unpredictability."}, {"heading": "2.2.1 Training Step", "text": "We explain this assumption in Figure 1. Let's take face recognition as an example: The task is to learn a function to predict whether an image contains a face or not. So the visual fields that contain a face are positive instances, which we focus on in Figure 1, and the other visual fields are negative instances. It is easy to see that visual fields (red rectangles in Figure 1) look similar to each other, while visual fields are unequal. In summary, positive instances point to a concept that leads to the high similarity between positive and negative instances, which are usually outside the region occupied by positive instances in attribute spaces. According to the previous explanation and figure in Figure 1, we assume that most positive instances take place outside this cluster."}, {"heading": "2.2.2 Query Step", "text": "As described above, the positive instances should be captured compactly in the attribute space. On the other hand, the distribution of negative instances is unknown. If the negative instances are scattered and dissimilar to each other (Figure 2), we can get the prediction function to enclose most positive instances in the attribute space, but negative instances are sometimes (Figure 3) and even share a higher similarity than positive ones. Let's take facial recognition as an example, some facial images are used to learn a prediction function and each image contains the same background (for example, tree, sky and so on). If there is a higher similarity between the background patches than face patches, the variance of negative instances is smaller than that of the positive ones. The result of the training is to enclose the negative instances with a higher similarity than the desired positive ones. To avoid this result, our method must confirm whether the prediction function consists of positive or negative instances."}, {"heading": "2.3 Time Complexity Analysis", "text": "The time complexity analysis of PMI includes Equivalent (9), Equivalent (11), Equivalent (16) and the number of instances queried. These three objective functions Equivalent (9), Equivalent (11) and Equivalent (16) are three quadratic programming problems related to the time complexities O (n3), O (N3) and O (N3), respectively. Since n > N generally dominates the time complexity of PMI, the number of instances queried according to Theorem 3 is related to nN. Therefore, the time complexity O (n4 N) is. However, in real applications the number of instances queried is much smaller than the theoretical result in Theorem 3 and can be approximated to a constant. The time complexity of PMI is close to O (n3)."}, {"heading": "3 Experiments", "text": "In this section, the experimental results are presented on the basis of five benchmark data sets and one face image data set. We evaluate PMI compared to conventional MIL algorithms. To solve the quadratic programming problem in Equivalent (2), the optimization toolbox \"Mosek\" [16] is used."}, {"heading": "3.1 Benchmark Data Sets", "text": "Five benchmark datasets are used in our experiment: Musk1, Musk2, Elephant, Fox and Tiger, which have been widely used to test new MIL algorithms in previous studies."}, {"heading": "3.2 Face Identification", "text": "The task of facial recognition is to predict whether an image contains face (s) or not. Each of these facial images contains a person's face and a similar green background. These 37 people comprise different types: male and female, young and old. Each image collects the faces of 37 people from different views in different light conditions. Each image is 640 x 480 in JPEG file format. No facial image is provided in this set of images. Figure 4 illustrates some examples of this set of images. Many more facial images than face are required: male and female, young and old. Each image is 640 x 480 in JPEG file format. No facial image is provided in this set of images. Figure 4 illustrates some examples of this set. Many more facial images than face are required in previous MIL studies. However, no facial image is provided in this data set. The extraction method function in [5] is used in our experiments. Only the brief introduction is provided. Please read [more details]."}, {"heading": "4 Conclusion", "text": "This paper proposes a new PMI algorithm that learns a predictive function only with positive bags. Our proposed algorithm is to significantly reduce the cost of labels compared to previous MIL algorithms without much loss of accuracy. Detailed theoretical analysis is also provided. In most real-world applications, the number of instances queried is much smaller than the theoretical result in Theorem 3 and can be estimated at a constant. PMI's time complexity is estimated as O (n3). Experimental results show that PMI generally comes close to the performance of traditional MIL methods, especially when traditional MIL algorithms can predict bag marking with high accuracy. In future work, we will try to apply some dimensional reduction approaches [12, 13, 11] to our proposed method to improve efficiency."}, {"heading": "Acknowledgement", "text": "Confirm here.Attachment here."}], "references": [{"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Robust object tracking with online multiple instance learning", "author": ["B. Babenko", "M.-H. Yang", "S. Belongie"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Miles: Multiple-instance learning via embedded instance selection", "author": ["Y. Chen", "J. Bi", "J.Z. Wang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1931}, {"title": "Image categorization by learning and reasoning with regions", "author": ["Y. Chen", "J.Z. Wang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Unsupervised segmentation of color-texture regions in images and video", "author": ["Y. Deng", "B.S. Manjunath"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on (PAMI),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T. Dietterich", "R. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Multiple instance learning for computer aided diagnosis", "author": ["G. Fung", "M. Dundar", "B. Krishnapuram", "R.B. Rao"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Multiple rank multi-linear svm for matrix data classification", "author": ["C. Hou", "F. Nie", "C. Zhang", "D. Yi", "Y. Wu"], "venue": "Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Stable local dimensionality reduction approaches", "author": ["C. Hou", "C. Zhang", "Y. Wu", "Y. Jiao"], "venue": "Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Multiple view semi-supervised dimensionality reduction", "author": ["C. Hou", "C. Zhang", "Y. Wu", "F. Nie"], "venue": "Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["R.F.L. Fei-Fei", "P. Perona"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Drosophila gene expression pattern annotation through multi-instance multi-label learning", "author": ["Y.-X. Li", "S. Ji", "S. Kumar", "J. Ye", "Z.-H. Zhou"], "venue": "Computational Biology and Bioinformatics, IEEE/ACM Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["B. Sch\u00f6lkopf", "J. Platt", "J. Shawe-Taylor", "A. Smola", "R. Williamson"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Support vector method for novelty detection", "author": ["B. Sch\u00f6lkopf", "R. Williamson", "A. Smola", "J. Shawe-Taylor", "J. Platt"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Image annotation by graph-based inference with integrated multiple/single instance representations", "author": ["J. Tang", "H. Li", "G.-J. Qi", "T.-S. Chua"], "venue": "Multimedia, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Multiple instance learning with multiple objective genetic programming for web mining", "author": ["A. Zafra", "E.L. Gibaja", "S. Ventura"], "venue": "Applied Soft Computing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "A multiple instance learning approach for content based image retrieval using one-class support vector machine", "author": ["C. Zhang", "X. Chen", "M. Chen", "S. Chen", "M. Shyu"], "venue": "In Multimedia and Expo,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "EM-DD: An improved multiple-instance learning technique", "author": ["Q. Zhang", "S. Goldman"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Content-Based Image Retrieval Using Multiple- Instance Learning", "author": ["Q. Zhang", "S.A. Goldman", "W. Yu", "J.E. Fritts"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}], "referenceMentions": [{"referenceID": 5, "context": "Multiple instance learning (MIL) is introduced by [9] to solve the drug activity prediction problem.", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 124, "endOffset": 134}, {"referenceID": 3, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 124, "endOffset": 134}, {"referenceID": 14, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 124, "endOffset": 134}, {"referenceID": 18, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 152, "endOffset": 160}, {"referenceID": 16, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 152, "endOffset": 160}, {"referenceID": 1, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 171, "endOffset": 174}, {"referenceID": 15, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 209, "endOffset": 213}, {"referenceID": 6, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 236, "endOffset": 240}, {"referenceID": 16, "context": "One related work is [22].", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "So [22] can be applied to a limited range of settings.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "[22] cannot select multiple positive instances accurately in one bag.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "It causes the prediction accuracy of [22] is too sensitive to the provided positive instance.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "The training step is to convert the problem of learning with only positive bags into a one-class classification problem [18] to get a classifier f .", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": "One-class SVM [18] is involved in PMI, so a brief review on one-class SVM is presented here.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "As is stated in [2, 10], the selection of instances will be formulated as a combinatorial optimization problem, which is non-convex and difficult to solve especially when a global optimal solution", "startOffset": 16, "endOffset": 23}, {"referenceID": 6, "context": "As is stated in [2, 10], the selection of instances will be formulated as a combinatorial optimization problem, which is non-convex and difficult to solve especially when a global optimal solution", "startOffset": 16, "endOffset": 23}, {"referenceID": 13, "context": "Theorem 1 is easy to be validated from proposition 4 in [18][17].", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "Theorem 1 is easy to be validated from proposition 4 in [18][17].", "startOffset": 60, "endOffset": 64}, {"referenceID": 5, "context": "They are Musk1, Musk2, Elephant, Fox and Tiger, which were used frequently to test new MIL algorithms in previous studies[9, 10, 2].", "startOffset": 121, "endOffset": 131}, {"referenceID": 6, "context": "They are Musk1, Musk2, Elephant, Fox and Tiger, which were used frequently to test new MIL algorithms in previous studies[9, 10, 2].", "startOffset": 121, "endOffset": 131}, {"referenceID": 0, "context": "They are Musk1, Musk2, Elephant, Fox and Tiger, which were used frequently to test new MIL algorithms in previous studies[9, 10, 2].", "startOffset": 121, "endOffset": 131}, {"referenceID": 6, "context": "To compare with PMI, we also provide the performances [10, 2] of several other traditional MIL algorithms .", "startOffset": 54, "endOffset": 61}, {"referenceID": 0, "context": "To compare with PMI, we also provide the performances [10, 2] of several other traditional MIL algorithms .", "startOffset": 54, "endOffset": 61}, {"referenceID": 0, "context": "These traditional MIL algorithms are mi-SVM [2], MI-SVM[2], EM-DD[23] and MICA[10].", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "These traditional MIL algorithms are mi-SVM [2], MI-SVM[2], EM-DD[23] and MICA[10].", "startOffset": 55, "endOffset": 58}, {"referenceID": 17, "context": "These traditional MIL algorithms are mi-SVM [2], MI-SVM[2], EM-DD[23] and MICA[10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "These traditional MIL algorithms are mi-SVM [2], MI-SVM[2], EM-DD[23] and MICA[10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "The feature extraction method in [5] is used in our experiments.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "Please read [6] for more details.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "We use the toolkit \u201cJSEG\u201d[8] to segment each image into regions.", "startOffset": 25, "endOffset": 28}, {"referenceID": 10, "context": "We select 100 background images(for the balance between positive and negative bags) randomly from [4, 14] as negative bags for testing and training in other traditional MIL methods.", "startOffset": 98, "endOffset": 105}, {"referenceID": 8, "context": "In the future work, we try to apply some dimensional reduction approaches [12, 13, 11] on our proposed method to improve the efficiency.", "startOffset": 74, "endOffset": 86}, {"referenceID": 9, "context": "In the future work, we try to apply some dimensional reduction approaches [12, 13, 11] on our proposed method to improve the efficiency.", "startOffset": 74, "endOffset": 86}, {"referenceID": 7, "context": "In the future work, we try to apply some dimensional reduction approaches [12, 13, 11] on our proposed method to improve the efficiency.", "startOffset": 74, "endOffset": 86}], "year": 2016, "abstractText": "In traditional multiple instance learning (MIL), both positive and negative bags are required to learn a prediction function. However, a high human cost is needed to know the label of each bag\u2014positive or negative. Only positive bags contain our focus (positive instances) while negative bags consist of noise or background (negative instances). So we do not expect to spend too much to label the negative bags. Contrary to our expectation, nearly all existing MIL methods require enough negative bags besides positive ones. In this paper we propose an algorithm called \u201cPositive Multiple Instance\u201d (PMI), which learns a classifier given only a set of positive bags. So the annotation of negative bags becomes unnecessary in our method. PMI is constructed based on the assumption that the unknown positive instances in positive bags be similar each other and constitute one compact cluster in feature space and the negative instances locate outside this cluster. The experimental results demonstrate that PMI achieves the performances close to or a little worse than those of the traditional MIL algorithms on benchmark and real data sets. However, the number of training bags in PMI is reduced significantly compared with traditional MIL algorithms.", "creator": "LaTeX with hyperref package"}}}