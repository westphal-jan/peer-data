{"id": "1603.05118", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "Recurrent Dropout without Memory Loss", "abstract": "This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to forward connections of feed-forward architectures or RNNs, we propose to drop neurons directly in recurrent connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for the most popular recurrent networks: vanilla RNNs, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. Our experiments on three NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.", "histories": [["v1", "Wed, 16 Mar 2016 14:33:47 GMT  (47kb)", "http://arxiv.org/abs/1603.05118v1", null], ["v2", "Fri, 5 Aug 2016 09:59:25 GMT  (95kb)", "http://arxiv.org/abs/1603.05118v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stanislau semeniuta", "aliaksei severyn", "erhardt barth"], "accepted": false, "id": "1603.05118"}, "pdf": {"name": "1603.05118.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["stas@inb.uni-luebeck.de", "barth@inb.uni-luebeck.de", "severyn@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.05 118v 1 [cs.C L] 16 Mar 201 6"}, {"heading": "1 Introduction", "text": "In fact, it is such that we will be able to move to another world, in which we can move to another world, in which we are able to move to another world, in which we are able to exchange ourselves to another world, in which we are able to change the world, in which we are able to live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we are able to change the world, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we are able to live, in which we live,"}, {"heading": "2 Related Work", "text": "This often leads to overadjustment, especially when the amount of training data is low and has led to a lot of research aimed at improving neural network generalization. (2013) and Zaremba et al. (2014) have shown that the LSTMs can be effectively regulated if they use the dropouts in the front networks. While this already leads to effective regulation of recurrent neural networks, it is intuitive that the introduction of dropouts also leads to a more robust display when hidden. (2015)"}, {"heading": "3 Recurrent Dropout", "text": "This section describes how to apply dropouts to recurring connections of three most common types of recurring networks: vanilla RNNNs, LSTMs, and GRUs."}, {"heading": "3.1 Dropout", "text": "Dropout (Hinton et al., 2012) is today one of the few effective tools for regulating neural networks. It is widely used in feedback-forward architectures that are applied to different layers: y = f (Wd (x)), (1) where x is the input of the layer, W is the matrix of weights, f is an activation function, y is the activation vector, and d is a dropout operation, defined as: d (x) = {mask \u0445 x, if otherwise the tensile phase is (1 \u2212 p) x, (2) where p is the dropout rate and the mask is a vector sampled from the Bernoulli distribution with probability of success 1 \u2212 p."}, {"heading": "3.2 Dropout in vanilla RNNs", "text": "Vanilla RNNs were one of the first ideas for modeling sequential data with neural networks. Formally, RNNs process the input sequences as follows: ht = f (Wh [xt, ht \u2212 1] + bh), (3) where xt is the input to the time steps t; ht and ht \u2212 1 are hidden vectors encoding the current and previous state of the network; Wh is a parameter matrix that models input to hidden and hidden (recurring) connections; b is a vector of defaults, and f is the activation function. Since RNs model sequential data through a fully connected layer, we apply failure strategies in the same way as they are applied to the advanced networks. Specifically, we modify Equation 3 in the following way: ht = f (Wh [xt, d \u2212 1) + bh), where d is the failure strategy from equation 2."}, {"heading": "3.3 Dropout in LSTM and GRU networks", "text": "Despite the simplicity and effectiveness of vanilla RNNs on a number of sequence modeling tasks, we have shown that their formation poses serious problems, primarily caused by so-called disappearing and exploding gradients (Bengio et al., 1994). While the problem of exploding gradients can be effectively addressed by gradient clipping (Pascanu et al., 2013), the disappearing gradients seem to be a more serious problem \u2212 there are two primary instructions to address the latter problem: (i) Using more sophisticated Hessian-free optimization algorithms instead of Stochastic Gradient Descend (SGD), e.g. Martens and Sutskever (2011) make adjustments to the network architecture, e.g. by introducing gated urms storage networks (Hochreiter and Schmidhuber, 1997)."}, {"heading": "3.4 Dropout and memory", "text": "This year it is more than ever before."}, {"heading": "4 Experiments", "text": "First, we demonstrate empirically the problems associated with memory loss when using different dropout techniques in relapsing networks (see Section 3.4). To this end, we experiment with training LSTM networks on one of the synthetic tasks (Hochreiter and Schmidhuber, 1997), in particular the Temporal Order task. Then, we validate the effectiveness of our relapse in vanilla RNNNs, LSTMs, and GRUs using three different public benchmarks: Language Modeling, Named Entity Recognition, and Twitter Sentiment Classification."}, {"heading": "4.1 Synthetic Task", "text": "The task is to classify a sequence into one of four classes ({AA, AB, BA, BB}) based on the sequence of non-intoxicating symbols. We code the symbols with two bits. The networks are trained on 200 minibatches of 32 sequences and tested on 10k sequences. Setup. We use LSTM with a layer containing 256 hidden units and 0.5 strength repetitive dropouts. We use SGD with a learning rate of 0.1 and train for 5k epochs. We generate data so that each sequence is divided into three parts of the same size and sends a meaningful symbol in the first and second layer. The prediction is made after the full sequence has been processed. We use two modes in our experiments: Shorten the results are 15 steps to update the results."}, {"heading": "4.2 Language Modeling", "text": "Following Mikolov et al. (2011), we use the Penn Treebank Corpus to train our Language Modeling (LM) models. The data set contains about 1 million words and comes with predefined training, validation and test splits and a vocabulary of 10k words. Setup. in our LM experiments, we use recurring networks with a single layer of 256 hidden units. Network parameters have been uniformly initiated in [-0.05, 0.05]. For training, we use plain SGD with batch size 32 with the maximum standard gradient clipping (Pascanu et al., 2013). Learning rate, clipping threshold and number of Backpropagation Through Time (BPTT) steps have been reduced to 0.1, 30 and 15 respectively for Vanilla RNNNNs, 1, 10 and 35 for LSTMs."}, {"heading": "4.3 Named Entity Recognition", "text": "To assess our recurrent Named Entity Recognition (NER) taggers when using recurrent dropouts, we use a public benchmark from CONLL 2003 (Tjong Kim Sang and De Meulder, 2003).The dataset contains approximately 300k words divided into train, validation, and test partitions.Each word is labeled with either a designated entity class to which it belongs, such as place or person, or as undesign.The majority of words are labeled as undesignated entities.The vocabulary size is approximately 22k words.Setup. Previous states of art systems have shown the importance of using word contexts around entities. Therefore, we easily modify the architecture of our recurrent networks to consume the context around the target word by simply linking their embedings.The size of the context window is fixed to 5 words (the two words to be labeled in front of each other)."}, {"heading": "4.4 Twitter Sentiment Analysis", "text": "Data. We use Twitter sentiment corpus from SemEval-2015 Task 10 (Subtask B) (Rosenthal et al., 2015). It contains 15k labeled tweets, divided into training and validation parts. The total number of words is about 330k and the vocabulary size is 22k. The task is to classify a tweet into three classes: positive, neutral and negative. The performance of a classifier is measured by the average of F1 values of positive and negative classes. We evaluate our models using a number of data sets that have been used for benchmarking in recent years. Setup. We use recurring networks in the standard sequence label - we enter words one by one and take the label at the last step. Similarly (Severyn and Moschitti, 2015) we use 1 million weakly labeled tweets to prepare our networks."}, {"heading": "5 Conclusions", "text": "Our approach is simple to implement and is even more effective when combined with conventional forward dropout. We have shown that applying dropout to any cell vector leads to sub-optimal performance for LSTMs and GRUs. We discuss the cause of this effect in detail and propose a simple solution to overcome it. The effectiveness of our approach is verified on three different public NLP benchmarks, while applying it to LSTM and GRU networks requires some care. We show that recurring dropout is most effective when applied to hidden state update vectors rather than conventional sequences. (ii) We observe an improvement in network performance when our primal results are associated with other primal primal dropout networks. (We show that recurring dropout effect is most effective when applied to hidden state update vectors and not to primal states when we observe primal results when centred on other primal states."}, {"heading": "Acknowledgments", "text": "This project was funded by the European Union Framework Programme for Research and Innovation HORIZON 2020 (2014-2020) under Marie Skodowska-Curie Agreement No. 641805. Stanislau Semeniuta thanks the support of Pattern Recognition Company GmbH. We thank NVIDIA Corporation for its support by donating the Titan X GPU used for this research."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks. CoRR, abs/1506.03099", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Where to apply dropout in recurrent neural networks for handwriting recognition", "author": ["Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In 13th International Conference on Document Analysis", "citeRegEx": "Bluche et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bluche et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches. CoRR, abs/1409.1259", "author": ["Cho et al.2014] KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Rmsprop and equilibrated adaptive learning rates for non-convex optimization. CoRR, abs/1502.04390", "author": ["Harm de Vries", "Junyoung Chung", "Yoshua Bengio"], "venue": null, "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "A. Noah Smith"], "venue": "In ACL,", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Opinion mining with deep recurrent neural networks", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "Claire Cardie"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "Character-aware neural language models. CoRR, abs/1508.06615", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Ilya Sutskever", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A simple way to initialize recurrent networks of rectified linear units. CoRR, abs/1504.00941", "author": ["Le et al.2015] Quoc V. Le", "Navdeep Jaitly", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Multi-timescale long short-term memory neural network for modelling sentences and documents", "author": ["Liu et al.2015] Pengfei Liu", "Xipeng Qiu", "Xinchi Chen", "Shiyu Wu", "Xuanjing Huang"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["Martens", "Sutskever2011] James Martens", "Ilya Sutskever"], "venue": "Proceedings of the 28th International Conference", "citeRegEx": "Martens et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2011}, {"title": "Extensions of recurrent neural network language model", "author": ["Mikolov et al.2011] T. Mikolov", "S. Kombrink", "L. Burget", "J.H. Cernocky", "Sanjeev Khudanpur"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning longer memory in recurrent neural networks. CoRR, abs/1412.7753", "author": ["Armand Joulin", "Sumit Chopra", "Micha\u00ebl Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "Rnndrop: A novel dropout for rnns in asr. Automatic Speech Recognition and Understanding (ASRU)", "author": ["Moon et al.2015] Taesup Moon", "Heeyoul Choi", "Hoshik Lee", "Inchul Song"], "venue": null, "citeRegEx": "Moon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moon et al\\.", "year": 2015}, {"title": "Beyond short snippets: Deep networks for video classification", "author": ["Ng et al.2015] Joe Yue-Hei Ng", "Matthew J. Hausknecht", "Sudheendra Vijayanarasimhan", "Oriol Vinyals", "Rajat Monga", "George Toderici"], "venue": null, "citeRegEx": "Ng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Dropout improves recurrent neural networks for handwriting recognition. CoRR, abs/1312.4569", "author": ["Pham et al.2013] Vu Pham", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": null, "citeRegEx": "Pham et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2013}, {"title": "Semeval-2015 task 10: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Svetlana Kiritchenko", "Saif Mohammad", "Alan Ritter", "Veselin Stoyanov"], "venue": "In Proceedings of the 9th International", "citeRegEx": "Rosenthal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2015}, {"title": "Twitter sentiment analysis with deep convolutional neural networks", "author": ["Severyn", "Moschitti2015] Aliaksei Severyn", "Alessandro Moschitti"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference", "citeRegEx": "Severyn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of the Seventh Conference", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Predicting polarities of tweets by composing word embeddings with long short-term memory", "author": ["Wang et al.2015] Xin Wang", "Yuanchao Liu", "Chengjie SUN", "Baoxun Wang", "Xiaolong Wang"], "venue": "In ACL,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization. CoRR, abs/1409.2329", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Zhang", "Lapata2014] Xingxing Zhang", "Mirella Lapata"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Tree recurrent neural networks with application to language modeling. CoRR, abs/1511.00060", "author": ["Zhang et al.2015] Xingxing Zhang", "Liang Lu", "Mirella Lapata"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "Recurrent Neural Networks have recently gained increased attention in the NLP community for their superior ability to model and learn from sequential data, and for showing state-of-the-art results on various public benchmarks ranging from sentence classification (Wang et al., 2015; Irsoy and Cardie, 2014; Liu et al., 2015) and various tagging problems (Dyer et al.", "startOffset": 263, "endOffset": 324}, {"referenceID": 12, "context": "Recurrent Neural Networks have recently gained increased attention in the NLP community for their superior ability to model and learn from sequential data, and for showing state-of-the-art results on various public benchmarks ranging from sentence classification (Wang et al., 2015; Irsoy and Cardie, 2014; Liu et al., 2015) and various tagging problems (Dyer et al.", "startOffset": 263, "endOffset": 324}, {"referenceID": 5, "context": ", 2015) and various tagging problems (Dyer et al., 2015) to language modelling (Kim et al.", "startOffset": 37, "endOffset": 56}, {"referenceID": 9, "context": ", 2015) to language modelling (Kim et al., 2015; Zhang et al., 2015), text generation (Zhang and Lapata, 2014) and sequence-to-sequence prediction tasks (Sutskever et al.", "startOffset": 30, "endOffset": 68}, {"referenceID": 28, "context": ", 2015) to language modelling (Kim et al., 2015; Zhang et al., 2015), text generation (Zhang and Lapata, 2014) and sequence-to-sequence prediction tasks (Sutskever et al.", "startOffset": 30, "endOffset": 68}, {"referenceID": 23, "context": ", 2015), text generation (Zhang and Lapata, 2014) and sequence-to-sequence prediction tasks (Sutskever et al., 2014).", "startOffset": 92, "endOffset": 116}, {"referenceID": 6, "context": "Among the first and most widely used techniques to avoid overfitting and increase generalizability of neural networks is the dropout regularization (Hinton et al., 2012).", "startOffset": 148, "endOffset": 169}, {"referenceID": 10, "context": ", Convolutional Neural Networks (Krizhevsky et al., 2012), the application of dropout in RNNs has been somewhat limited with unclear benefits.", "startOffset": 32, "endOffset": 57}, {"referenceID": 14, "context": "Indeed, Moon et al. (2015) have extended the idea of dropping neurons in forward direction and proposed to drop cell states as well.", "startOffset": 8, "endOffset": 27}, {"referenceID": 0, "context": "Bluche et al. (2015) carry out a study to find where dropout is most effective, e.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Bengio et al. (2015) have proposed an algorithm called scheduled sampling to improve performance of recurrent networks on sequenceto-sequence labeling tasks.", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "Dropout (Hinton et al., 2012) is one of the few effective tools today for regularizing neural networks.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "Despite simplicity and effectiveness of vanilla RNNs on a number of sequence modeling tasks, their training have been shown to pose serious problems, primarily caused by so called vanishing and exploding gradients (Bengio et al., 1994).", "startOffset": 214, "endOffset": 235}, {"referenceID": 19, "context": "While the problem of exploding gradients can be effectively addressed by performing gradient clipping (Pascanu et al., 2013), the vanishing gradients appear to be a more serious issue.", "startOffset": 102, "endOffset": 124}, {"referenceID": 0, "context": "Despite simplicity and effectiveness of vanilla RNNs on a number of sequence modeling tasks, their training have been shown to pose serious problems, primarily caused by so called vanishing and exploding gradients (Bengio et al., 1994). While the problem of exploding gradients can be effectively addressed by performing gradient clipping (Pascanu et al., 2013), the vanishing gradients appear to be a more serious issue. There are two primary directions to address the latter issue: (i) use more sophisticated Hessian-free optimization algorithms instead of Stochastic Gradient Descend (SGD), e.g., Martens and Sutskever (2011), and (ii) make adjustments in the network architecture, e.", "startOffset": 215, "endOffset": 629}, {"referenceID": 17, "context": "In contrast, Moon et al. (2015) propose to apply dropout directly to the cell values and use persequence sampling:", "startOffset": 13, "endOffset": 32}, {"referenceID": 3, "context": "Gated Recurrent Unit (GRU) networks are a recently introduced variant of a recurrent network with hidden state protected by gates (Cho et al., 2014).", "startOffset": 130, "endOffset": 148}, {"referenceID": 16, "context": "We will discuss the limitations of the approach of Moon et al. (2015) in Section 3.", "startOffset": 51, "endOffset": 70}, {"referenceID": 17, "context": "Firstly, we found that an intuitive idea to drop previous hidden states directly, as proposed in Moon et al. (2015), produces mixed results.", "startOffset": 97, "endOffset": 116}, {"referenceID": 17, "context": "Firstly, we found that an intuitive idea to drop previous hidden states directly, as proposed in Moon et al. (2015), produces mixed results. We have observed that it helps the network to generalize better when not coupled with the forward dropout, but is no longer beneficial when used together with regular forward dropout. While in some cases applying both forward and hidden state dropout yields an improvement in the network performance, we found this outcome to be rare and hard to achieve. Below we discuss two primary problems with the dropout method of Moon et al. (2015).", "startOffset": 97, "endOffset": 580}, {"referenceID": 17, "context": "Moon et al. (2015) make the same observation and propose to use per-sequence mask sampling as well.", "startOffset": 0, "endOffset": 19}, {"referenceID": 16, "context": "The fact that Moon et al. (2015) have achieved an improvement can be explained by the experimentation domain.", "startOffset": 14, "endOffset": 33}, {"referenceID": 11, "context": "Le et al. (2015) have proposed a simple yet effective way to initialize vanilla RNNs and reported that they have achieved a good result in the Speech Recognition domain while having an effect similar to the one caused by Eq.", "startOffset": 0, "endOffset": 17}, {"referenceID": 14, "context": "Following Mikolov et al. (2011) we use the Penn Treebank Corpus to train our Language Modeling (LM) models.", "startOffset": 10, "endOffset": 32}, {"referenceID": 19, "context": "For training, we use plain SGD with batch size 32 with the maximum norm gradient clipping (Pascanu et al., 2013).", "startOffset": 90, "endOffset": 112}, {"referenceID": 14, "context": "The aforementioned choices were largely guided by the work of Mikolov et al. (2014)1.", "startOffset": 62, "endOffset": 84}, {"referenceID": 14, "context": "We found that to achieve reasonable results with vanilla RNNs on the LM task, it was important to construct training batches similarly to Mikolov et al. (2014), where the authors backpropagate 10 steps back every 5 steps forward.", "startOffset": 138, "endOffset": 160}, {"referenceID": 17, "context": "For LSTM and GRU networks we also present results when the dropout is applied directly to hidden states as in (Moon et al., 2015).", "startOffset": 110, "endOffset": 129}, {"referenceID": 15, "context": "The network inputs include only word embeddings (initialized with pretrained word2vec embeddings (Mikolov et al., 2013) and kept static) and capitalization features.", "startOffset": 97, "endOffset": 119}, {"referenceID": 4, "context": "For training we use the RMSProp algorithm (Dauphin et al., 2015) with \u03c1 fixed at 0.", "startOffset": 42, "endOffset": 64}, {"referenceID": 18, "context": "To speed up the training we use a length expansion approach described in (Ng et al., 2015), where training is performed in two stages: (i) we first sample short 5-words input sequences with their contexts and train for 25 epochs; (ii) we fine tune the network on input 15-words sequences for 10 epochs.", "startOffset": 73, "endOffset": 90}, {"referenceID": 21, "context": "We use Twitter sentiment corpus from SemEval-2015 Task 10 (subtask B) (Rosenthal et al., 2015).", "startOffset": 70, "endOffset": 94}, {"referenceID": 17, "context": "We demonstrate that recurrent dropout is most effective when applied to hidden state update vectors rather than hidden states; (ii) we observe an improvement in the network\u2019s performance when our recurrent dropout is coupled with the standard forward dropout, though the extent of this improvement depends on the values of dropout rates; (iii) contrary to our expectations, networks trained with per-step and per-sequence mask sampling produce similar results when using our recurrent dropout method, both being better than the dropout scheme proposed by Moon et al. (2015).", "startOffset": 555, "endOffset": 574}], "year": 2016, "abstractText": "This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to forward connections of feed-forward architectures or RNNs, we propose to drop neurons directly in recurrent connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for the most popular recurrent networks: vanilla RNNs, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. Our experiments on three NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.", "creator": "LaTeX with hyperref package"}}}