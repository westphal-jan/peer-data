{"id": "1603.05145", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions", "abstract": "Many deep Convolutional Neural Networks (CNN) make incorrect predictions on adversarial samples obtained by imperceptible perturbations of clean samples. We hypothesize that this is caused by a failure to suppress unusual signals within network layers. As remedy we propose the use of Symmetric Activation Functions (SAF) in non-linear signal transducer units. These units suppress signals of exceptional magnitude. We prove that SAF networks can perform classification tasks to arbitrary precision in a simplified situation. In practice, rather than use SAFs alone, we add them into CNNs to improve their robustness. The modified CNNs can be easily trained using popular strategies with the moderate training load. Our experiments on MNIST and CIFAR-10 show that the modified CNNs perform similarly to plain ones on clean samples, and are remarkably more robust against adversarial and nonsense samples.", "histories": [["v1", "Wed, 16 Mar 2016 15:35:07 GMT  (1440kb,D)", "http://arxiv.org/abs/1603.05145v1", "11 pages, 12 figures"]], "COMMENTS": "11 pages, 12 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["qiyang zhao", "lewis d griffin"], "accepted": false, "id": "1603.05145"}, "pdf": {"name": "1603.05145.pdf", "metadata": {"source": "META", "title": "Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions", "authors": ["Qiyang Zhao", "Lewis D Griffin"], "emails": ["ZHAOQY@BUAA.EDU.CN", "L.GRIFFIN@CS.UCL.AC.UK"], "sections": [{"heading": "1. Introduction", "text": "Sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-sample-to-sample-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-to-sample-to-to-sample-to-sample-to-sample-to-to-sample-to-sample-to-sample-to-sample-to-to-sample-to-sample-to-sample-to-sample-to-sample-to-to-sample-to-sample-to-sample-to-sample-to-to-sample-to-sample-to-to-sample-to-sample-to-sample-to-sample-to-sample-to-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-sample-to-to-sample-to-sample-to-sample-to-to-sample-to-sample-to-to-sample-to-to-sample-to-sample-to-to-sample-to-to-sample-to-to-sample-to-to-sample-to-to-to-sample-to-to-sample-to-to-sample-to-sample-to-to-sample-to-to-sample-to-to-sample-to-to-sample-to-to-sample-to-to-to-sample-to-to-sample-to-to-to-sample-to-to-sample-to-to-to-sample-to-to-sample-to-to-to-sample-to-to-to-sample-to-sample-to-to"}, {"heading": "2. Motivation", "text": "In contrast to CNNs and linear classification models, local models (Moody & Darken, 1989) (Jacobs et al., 1991) based on radial basic functions (RBF) (Broomhead & Lowe, 1988) are intrinsically immune to abnormal samples that are far from regions of high density: they will always output low confidence values for them. Furthermore, RBFs are resilient to tiny input disturbances, suggesting the possibility of improving the robustness of CNNs by using RBF units. RBFs have been used in various ways within networks, e.g. as a hidden layer in the mix of experts (Jacobs et al., 1991), and as the top layer of units in LeNet-5 (LeCun et al., 1998). Only a few implementations attempt multiple RBF layers (Craddock & Warwick, 1996)."}, {"heading": "3. Symmetric Activation Functions", "text": "Compared to commonly used activation functions such as Rectified Linear Units (ReLU) (Nair & Hinton, 1996) and Sigmoid, RBFs are known to have a localizing property: 1-D RBFs suppress signals that deviate from normal state in both directions, while their counterparts suppress signals only on one side. By suppressing signals from abnormal samples, we ensure that there is no strong predictive confidence in the top layer. Inspired by the ReLU, we propose the mirrored rectified linear unit (mReLU). In the paper, we refer to both the 1-D RBF and the mReLU symmetric activation functions (SAF) as they have reflective symmetry."}, {"heading": "3.1. 1-D Radial Basis Function", "text": "The 1-D RBF used in the essay is \u03c3 (x) = e \u2212 x 2, and its derivative is \u2212 2x\u03c3 (x) (Fig. 1.a-b). Although it is parameter-free, any required flexibility can be achieved by recalculation and displacement in the preceding shaft layer."}, {"heading": "3.2. Mirrored Rectified Linear Unit (mReLU)", "text": "Inspired by the simplicity of the ReLU, we propose the mReLU, which consists only of basic calculations and logic calculations (fig. 1.c-d): mReLU (x) = 1 + x, 0 > x > \u2212 1,1 \u2212 x, + 1 > x > 0, 0, otherwise (1) or equivalent lymReLU (x) = min (ReLU (1 \u2212 x), ReLU (1 + x))). (2) Your derivative isddx mReLU = + 1, 0 > x > \u2212 1; \u2212 1, + 1 > x > 0; 0, otherwise. (3)"}, {"heading": "3.3. The Capacity of SAF Networks", "text": "In the limit of using an infinite number of RBF units, we classify the problem into a category; otherwise, we assume that it is nonsense; we can approximate any function with any precision (Park & Sandberg, 1991); we can use the products of 1-D RBFs to accurately mimic any high-dimensional RBF, and thus construct a 1- D RBF network for functional approximation; however, the product-of-units scheme is not compatible with popular CNNs, and it does not work for the mReLUs; we establish a property that is weaker than functional approximation, but with closer ties to our problem: We express the problem in geometric form. Suppose we have N template Z1, Z2, ZN in a uniform n-D data space, and a fidelity threshold."}, {"heading": "4. Building Robust CNNs", "text": "We note that the empirical distributions of these characteristics are relatively compact and symmetrical (Fig. 3). Therefore, it is possible to suppress unusual signals with similarly shaped SAFs. Immediately after the winding layers, we add additional SAF layers to suppress unusual signals, as in Fig. 2. Possible ReLU activation layers can be discarded, as the SAF outputs are not negative. We do the same with sigmoid activation layers. We add a 1D-RBF layer as a new last layer after the fully bonded layer, but do not add SAF layers between fully bonded layers. Moreover, we refer to the modified models as robust CNNs and the original as pure CNNs."}, {"heading": "4.1. The Hybrid Loss function", "text": "We are designing a special hybrid loss function for the robust CNN, which represents the negative logarithm of the normalized value and the weighted p-order errors (p > 1) according to: Lh (X, ', \u03b8) = \u2212 \u03b11 \u00b7 ln (y '\u2211 i yi) + \u03b12 \u00b7 (1 \u2212 y') p + \u03b13 \u00b7 \u2211 i 6 = 'ypi (7) where \"the correct designation is Y = {yi} the prediction of X by the robust CNN with the parameter \u03b8, L being the number of categories, and \u03b11, \u03b12, \u03b13 being the weight parameters. Lh is designed so that y\" should be relative and absolutely large. It is necessary to include the p-order errors in order to obtain a sufficiently large y \"to detect nonsense samples that would be small y\" s. We set p = 2 in all our experiments."}, {"heading": "4.2. Robustness Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "X hg gn Yhngf f g", "text": "In the simplified robust CNN in Fig. 7, fi's are revolutionary or bundled modules, and gi's are the SAF modules that follow revolutionary layers. We have Y = gn (gn \u2212 1 (fn \u2212 1 (... g1 (f1 (X)...))))) (8), where Y = [y1, y2,.., yL] T is the confidence vector for length L, the number of categories. The partial derivative of y 'in relation to xk is the number of categories in relation to Xk: \"\u2202 xk = dgn,\" \"dfn\" in \u2212 1,..., i1 (\u2202 fn, gn \u2212 1, \u2212 1)."}, {"heading": "4.3. Training Issues", "text": "In this work, in addition to clean samples, we also use training data, but instead of opposing samples, we use random disturbances in the mean of training data. We find that the middle sample itself can easily be misclassified (it rarely belongs to a meaningful category), and we can create nonsensical samples by easily disturbing them. Therefore, we use it as a typical hard instance and train robust CNNs to generate a high level of trust for nonsense. However, there is only one medium sample and therefore little influence on the training process. So we interfere with it with the Gaussian noise and get a certain number of loud copies. We call this trick training for deficiency. It is suggested in (Goodfellow et al., 2015) that randomly disturbed samples are of little importance. Interestingly, we find them effective for our network, especially when combined with common training."}, {"heading": "5. Experiments", "text": "We use two well-studied datasets, MNIST (LeCun et al., \u03b2, 1998) and CIFAR-10 (CIFAR, retrieved in February 2016), in our experiments. In the following numbers and tables, we use Plain, RBF, and mReLU to refer respectively to simple CNNs, robust CNNs using 1-D RBF, and robust CNNs using mReLUs. We use the flags -a, -r, and -m to indicate the use of opposite, random, and middle samples. Plain CNNs are typical models implemented in (Vedaldi, retrieved in February 2016) stationary models. We use all default settings in (Vedaldi, retrieved in February 2016) to train them, except for brightening and normalization. We plan to release our source code and data according to acceptance. In related literature, CNNit is expected to be against non-explicit types of samples:"}, {"heading": "5.1. MNIST", "text": "We use LeNet-5 (LeCun et al., 1998) as a simple CNN for MNIST, see Table 1 for its structure. There are 70,000 clean samples and we follow (Vedaldi, accessed in February 2016) to use 60,000 as a tensile set and 10,000 as a test set. There are 10,000 groups of nonsense samples. All CNNs are trained with 20 epochs. We use \u03b11 = 1, \u03b12 = 1, \u03b13 = 0 for the hybrid loss functions of robust CNNs. We present accuracies and error rates in Fig. 9 and Table 2. It is clear that CNNs using SAFs are much more robust than the simple CNN. The best, mReLU-r-m, makes slightly more errors than the pure CNN for clean samples, contradictory samples with interferences up to strength \u03b2 = 0.01, and noisy samples up to \u03b2 = 0.10."}, {"heading": "5.2. CIFAR-10", "text": "The CNN level for CIFAR-10 is a deeper LeNet model; see Table 3 for its structure. There are 60,000 clean samples and we follow (Vedaldi, retrieved in February 2016) to use 50,000 as the tension set and 10,000 as the test set. The number of hidden units in fully connected layers is 500 (fc1) and 10 (fc2). The maximum number of hidden units in hidden layers is 500 (fc1) and 10 (fc2)."}, {"heading": "6. Conclusions and Discussion", "text": "In fact, we see ourselves in a position to mobilize the reactionary forces that are able to retaliate, \"he said in an interview with\" Welt am Sonntag. \""}], "references": [{"title": "Scalable training of l1-regularized log-linear models", "author": ["G. Andrew", "J. Gao"], "venue": "In ICML,", "citeRegEx": "Andrew and Gao,? \\Q2007\\E", "shortCiteRegEx": "Andrew and Gao", "year": 2007}, {"title": "Unsupervised clustering with spiking neurons by sparse temporal coding and multilayer rbf networks", "author": ["S.M. Bohte", "H.L. Poutr\u00e9", "J.N. Kok"], "venue": "IEEE TNN,", "citeRegEx": "Bohte et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bohte et al\\.", "year": 2002}, {"title": "Multivariable fuctional interpolation and adaptive networks", "author": ["D.S. Broomhead", "D. Lowe"], "venue": "Complex systems,", "citeRegEx": "Broomhead and Lowe,? \\Q1988\\E", "shortCiteRegEx": "Broomhead and Lowe", "year": 1988}, {"title": "Visual causal feature learning", "author": ["K. Chalupka", "P. Perona", "F. Eberhardt"], "venue": "In UAI,", "citeRegEx": "Chalupka et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chalupka et al\\.", "year": 2015}, {"title": "Multi-layer radial basis function networks. an extension to the radial basis function", "author": ["R.J. Craddock", "K. Warwick"], "venue": "In ICNN,", "citeRegEx": "Craddock and Warwick,? \\Q1996\\E", "shortCiteRegEx": "Craddock and Warwick", "year": 1996}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "In ICLR,", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S. Gu", "L. Rigazio"], "venue": "In ICLR workshop,", "citeRegEx": "Gu and Rigazio,? \\Q2015\\E", "shortCiteRegEx": "Gu and Rigazio", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Batch normalization: accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Adaptive mixtures of local experts", "author": ["R.A. Jacobs", "M.I. Jordan", "S.J. Nowlan", "G.E. Hinton"], "venue": "Neural computations,", "citeRegEx": "Jacobs et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Jacobs et al\\.", "year": 1991}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Distributional smoothing by virtual adversarial examples", "author": ["T. Miyato", "S.I. Maeda", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": "In ICLR,", "citeRegEx": "Miyato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2016}, {"title": "Fast learning in networks of locally-tuned processing units", "author": ["J. Moody", "C. Darken"], "venue": "Neural computation,", "citeRegEx": "Moody and Darken,? \\Q1989\\E", "shortCiteRegEx": "Moody and Darken", "year": 1989}, {"title": "Multi-layer radial basis function networks. an extension to the radial basis function", "author": ["I. Mr\u00e1zov\u00e1", "M. Kuka\u010dka"], "venue": "In ICNN,", "citeRegEx": "Mr\u00e1zov\u00e1 and Kuka\u010dka,? \\Q1996\\E", "shortCiteRegEx": "Mr\u00e1zov\u00e1 and Kuka\u010dka", "year": 1996}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G. Hinton"], "venue": "In ICML,", "citeRegEx": "Nair and Hinton,? \\Q1996\\E", "shortCiteRegEx": "Nair and Hinton", "year": 1996}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "In CVPR,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Universal approximation using radial-basis-function networks", "author": ["J. Park", "I.W. Sandberg"], "venue": "Neural computation,", "citeRegEx": "Park and Sandberg,? \\Q1991\\E", "shortCiteRegEx": "Park and Sandberg", "year": 1991}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "In ICLR,", "citeRegEx": "Radford et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Adversarial manipulation of deep representations", "author": ["S. Sabour", "Y. Cao", "F. Faghri", "D.J. Fleet"], "venue": "In ICLR,", "citeRegEx": "Sabour et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sabour et al\\.", "year": 2016}, {"title": "Unsupervised and semi-supervised learning with categorical generative adversarial networks", "author": ["J.T. Springenberg"], "venue": "In ICLR,", "citeRegEx": "Springenberg,? \\Q2016\\E", "shortCiteRegEx": "Springenberg", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "In ICLR,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Matconvnet. http://www.vlfeat.org/matconvnet, accessed in Feb 2016", "author": ["A. Vedaldi"], "venue": null, "citeRegEx": "Vedaldi,? \\Q2016\\E", "shortCiteRegEx": "Vedaldi", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "Zeiler and Fergus,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "Although deep CNNs have delivered state-of-the-art performance on several major computer vision challenges (Krizhevsky et al., 2012)(He et al.", "startOffset": 107, "endOffset": 132}, {"referenceID": 7, "context": ", 2012)(He et al., 2015), they have some pooly understood aspects.", "startOffset": 7, "endOffset": 24}, {"referenceID": 21, "context": "In particular, it was shown in (Szegedy et al., 2014) that one can easily construct an adversarial sample by imperceptible perturbation of any clean sample using the box-constrained limited-memory BFGS method (Andrew & Gao, 2007).", "startOffset": 31, "endOffset": 53}, {"referenceID": 5, "context": "In (Goodfellow et al., 2015), a simpler method, fast gradient sign (FGS), is proposed to compute adversarial samples effectively.", "startOffset": 3, "endOffset": 28}, {"referenceID": 19, "context": "Recently, a more complicated method is proposed to construct an adversarial sample with both the prediction and internal CNN features being similar to another arbitrary sample (Sabour et al., 2016).", "startOffset": 176, "endOffset": 197}, {"referenceID": 5, "context": "Nonsense samples (called rubbish samples in (Goodfellow et al., 2015)) are another challenge to the robustness of CNNs.", "startOffset": 44, "endOffset": 69}, {"referenceID": 5, "context": "These are generated from noise or arbitrary images using FGS-like methods (Goodfellow et al., 2015)(Nguyen et al.", "startOffset": 74, "endOffset": 99}, {"referenceID": 16, "context": ", 2015)(Nguyen et al., 2015).", "startOffset": 7, "endOffset": 28}, {"referenceID": 21, "context": "All these references argue that adversarial and nonsense samples arise from some unknown flaw of popular deep CNNs, either in their structure or training methods (Szegedy et al., 2014)(Goodfellow et al.", "startOffset": 162, "endOffset": 184}, {"referenceID": 5, "context": ", 2014)(Goodfellow et al., 2015)(Sabour et al.", "startOffset": 7, "endOffset": 32}, {"referenceID": 19, "context": ", 2015)(Sabour et al., 2016).", "startOffset": 7, "endOffset": 28}, {"referenceID": 21, "context": "A proposed solution to the problem is adversarial training (Szegedy et al., 2014)(Goodfellow et al.", "startOffset": 59, "endOffset": 81}, {"referenceID": 5, "context": ", 2014)(Goodfellow et al., 2015): adar X iv :1 60 3.", "startOffset": 7, "endOffset": 32}, {"referenceID": 5, "context": "Unfortunately, the final error rates on the newest adversarial samples, after this scheme, remain large (Goodfellow et al., 2015).", "startOffset": 104, "endOffset": 129}, {"referenceID": 12, "context": "It is interesting that adversarial training can be understood as a new regularization method for training CNNs; for example in (Miyato et al., 2016), virtual adversarial samples are constructed to force the model distributions of a supervised learning process to be smooth.", "startOffset": 127, "endOffset": 148}, {"referenceID": 18, "context": "Adversarial samples can also be used to establish new unsupervised and/or semi-supervised representation learning methods (Radford et al., 2016) (Springenberg, 2016) or autoencoders (Makhzani et al.", "startOffset": 122, "endOffset": 144}, {"referenceID": 20, "context": ", 2016) (Springenberg, 2016) or autoencoders (Makhzani et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 12, "context": "The idea is similar to (Miyato et al., 2016).", "startOffset": 23, "endOffset": 44}, {"referenceID": 3, "context": "Another interesting idea, proposed in (Chalupka et al., 2015), is to identify features that are causally related, rather than merely correlated, with the categories.", "startOffset": 38, "endOffset": 61}, {"referenceID": 9, "context": "Unlike CNNs and linear classification models, local models (Moody & Darken, 1989)(Jacobs et al., 1991) based on radial basis functions (RBF) (Broomhead & Lowe, 1988) are intrinsically immune to abnormal samples which are far from high-density regions: they will always output low confidence scores for such.", "startOffset": 81, "endOffset": 102}, {"referenceID": 9, "context": "as a hidden layer in mixture of experts (Jacobs et al., 1991), and as the top layer units in LeNet-5 (LeCun et al.", "startOffset": 40, "endOffset": 61}, {"referenceID": 11, "context": ", 1991), and as the top layer units in LeNet-5 (LeCun et al., 1998).", "startOffset": 47, "endOffset": 67}, {"referenceID": 1, "context": "Only a few implementations attempt multiple RBF layers (Craddock & Warwick, 1996)(Bohte et al., 2002)(Mr\u00e1zov\u00e1 & Kuka\u010dka, 1996).", "startOffset": 81, "endOffset": 101}, {"referenceID": 5, "context": "This high-dimensionality makes it hard to train deep networks involving RBF units to achieve acceptable accuracies (Goodfellow et al., 2015).", "startOffset": 115, "endOffset": 140}, {"referenceID": 5, "context": "Some have argued that more powerful optimization methods are needed to make this work (Goodfellow et al., 2015).", "startOffset": 86, "endOffset": 111}, {"referenceID": 11, "context": "To assess this, we run a LeNet-5 model (LeCun et al., 1998) (see Sec.", "startOffset": 39, "endOffset": 59}, {"referenceID": 5, "context": "It is suggested in (Goodfellow et al., 2015) that randomly perturbed samples are of little significance.", "startOffset": 19, "endOffset": 44}, {"referenceID": 11, "context": "We use two well-studied datasets, MNIST (LeCun et al., 1998) and CIFAR-10 (CIFAR, accessed in Feb 2016), in our experiments.", "startOffset": 40, "endOffset": 60}, {"referenceID": 5, "context": "We follow (Goodfellow et al., 2015) to generate adversarial samples and nonsense samples by:", "startOffset": 10, "endOffset": 35}, {"referenceID": 11, "context": "MNIST We use LeNet-5 (LeCun et al., 1998) as the plain CNN for MNIST, please see Table 1 for its structure.", "startOffset": 21, "endOffset": 41}, {"referenceID": 5, "context": ", 2013) trained using adversarial training (Goodfellow et al., 2015).", "startOffset": 43, "endOffset": 68}], "year": 2016, "abstractText": "Many deep Convolutional Neural Networks (CNN) make incorrect predictions on adversarial samples obtained by imperceptible perturbations of clean samples. We hypothesize that this is caused by a failure to suppress unusual signals within network layers. As remedy we propose the use of Symmetric Activation Functions (SAF) in non-linear signal transducer units. These units suppress signals of exceptional magnitude. We prove that SAF networks can perform classification tasks to arbitrary precision in a simplified situation. In practice, rather than use SAFs alone, we add them into CNNs to improve their robustness. The modified CNNs can be easily trained using popular strategies with the moderate training load. Our experiments on MNIST and CIFAR-10 show that the modified CNNs perform similarly to plain ones on clean samples, and are remarkably more robust against adversarial and nonsense samples.", "creator": "LaTeX with hyperref package"}}}