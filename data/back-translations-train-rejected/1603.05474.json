{"id": "1603.05474", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2016", "title": "Neural Aggregation Network for Video Face Recognition", "abstract": "In this paper, we present a Neural Aggregation Network (NAN) for video face recognition. The network takes a face video or face image set of a person with variable number of face frames as its input, and produces a compact and fixed-dimension visual representation of that person. The whole network is composed of two modules. The feature embedding module is a CNN which maps each face frame into a feature representation. The neural aggregation module is composed of two content based attention blocks which is driven by a memory storing all the features extracted from the face video through the feature embedding module. The output of the first attention block adapts the second, whose output is adopted as the aggregated representation of the video faces. Due to the attention mechanism, this representation is invariant to the order of the face frames. The experiments show that the proposed NAN consistently outperforms hand-crafted aggregations such as average pooling, and achieves state-of-the-art accuracy on three video face recognition datasets: the YouTube Face, IJB-A and Celebrity-1000 datasets.", "histories": [["v1", "Thu, 17 Mar 2016 13:30:45 GMT  (392kb,D)", "http://arxiv.org/abs/1603.05474v1", "TR"], ["v2", "Fri, 18 Nov 2016 12:38:10 GMT  (2187kb,D)", "http://arxiv.org/abs/1603.05474v2", "TR"], ["v3", "Wed, 12 Apr 2017 06:02:06 GMT  (2274kb,D)", "http://arxiv.org/abs/1603.05474v3", "accepted by CVPR 2017 (IEEE Conference on Computer Vision and Pattern Recognition)"], ["v4", "Wed, 2 Aug 2017 08:08:14 GMT  (1593kb,D)", "http://arxiv.org/abs/1603.05474v4", "Post CVPR2017 version with minor typo fix"]], "COMMENTS": "TR", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["jiaolong yang", "peiran ren", "dongqing zhang", "dong chen", "fang wen", "hongdong li", "gang hua"], "accepted": false, "id": "1603.05474"}, "pdf": {"name": "1603.05474.pdf", "metadata": {"source": "CRF", "title": "Neural Aggregation Network for Video Face Recognition", "authors": ["Jiaolong Yang", "Peiran Ren", "Dong Chen", "Fang Wen", "Hongdong Li", "Gang Hua"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are in a position to understand themselves and to understand what they are doing to change the world, and that they are going to go to another world, in which they are going to go to another world, in which they are going to find themselves in another world, in which they are going to live in another world, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are, in which they are living, in which they are, in which they are living, in which they are, in which they are living, in which they are living, in which they are, in which they are living, in which they are, they are"}, {"heading": "2. Neural aggregation network", "text": "The entire network of our method consists of two modules. The feature embedding part is a CNN model for facial feature extraction from a simple image. The aggregation module merges all feature vectors into a fixed-dimensional representation that can be used in any decision model. For face recognition, the frame of our method is in Fig. 1. For face verification, we use a modified version with a Siamese structure [4]. Our new representation stands on the shoulders of the state-of-the-art, deep CNN model and is more powerful by aggregating all frames in the input video adaptively to a compact 128-dimensional representation. In this section, we will first present the aggregation and then introduce the embedding modules in Section 2.1 and 2.2 respectively."}, {"heading": "2.1. Aggregation module", "text": "The aggregation module is designed to benefit from all the frames in a video that may contain more discriminatory information than a single image, and process any video size in a uniform form, creating a constant representation. Consider the task of video face recognition using n pairs of labeled data (Xi, yi) ni = 1, where Xi is a video of different sizes ti: Xi = {xi1, xi2,..., xiti}, in which x i k, k = 1,... ti is the kest image in the video, and yi is the corresponding subject ID of Xi. Each frame xik has a corresponding representation extracted from the Feature Embedding module. For better readability, we omit the upper index, where appropriate in the following text. Our goal is to use all feature vectors from a video to generate a series of linear weights."}, {"heading": "2.1.1 Attention blocks", "text": "The attention block in the aggregation module takes a series of {fk} characteristics as input, filters them with a kernel q over a dot product and yields a series of corresponding signifiers {ek}. They are then passed on to a Softmax operator to normalize weights {ak}, \u2211 k ak = 1. Both operations are described by the following equations: ek = q T fk (2) ak = exp (ek) \u2211 j exp (ej). (3) The attention block is modulated exclusively by a filter kernel q. Another key advantage of the attention block is that its output is invariant to the input order of fk. It can be seen from Eq. 2, 3 and 1 that the permutation of fk and fk \u00b2 has no effect on the aggregated representation r. Another appealing feature of the attention block is that the number of inputs {fk} does not affect the same size of the output as the one of the attention block."}, {"heading": "2.1.2 Discussion", "text": "The structure of the aggregation module looks like an unfolded recursive neural network (RNN) used in the orderless set network [27], since in this module two attention blocks work one after the other. However, there are at least two differences between the proposed network and the one used in their papers. First, we use a feed forward network, without weight distribution or an evolving internal state. In fact, the first filter core q0 and the coefficients of the transmission layer (W, b) are fixed. Second, adding more attention blocks in the aggregation modules sounds like a reasonable extension at first glance. However, this leads to more nonlinearity, which is doubtful whether they are beneficial or not, but guarantees further difficulties in training."}, {"heading": "2.2. Feature embedding module", "text": "The image embedding module of our NAN is a Deep Convolution Neural Network (CNN) that embeds each image of a video in a face feature representation. To use modern Deep CNN networks with high-end performance, we follow the GoogLeNet [25] network structure in this essay and equip it with the Batch Normalization (BN) technique [12]. GoogLeNet's output image features have 128 dimensions, which are then fed into the aggregation module. As a result, our NAN network produces a 128-d feature representation for each input video. For the rest of this essay, we simply refer to the used GoogLeNet BN network as CNN."}, {"heading": "2.3. Network training", "text": "The proposed network can be trained for both face verification and identification tasks, each described as the following. Face identification attempts to detect the identity of a face video by optimizing the coefficients of our network and a fully connected predictive level by minimizing the average loss of classification: li = \u2212 log pi, yi, where yi is the target label of the i (video) instance, pi, yi = exp (pi, yi) \u2211 z exp (pi, z), and pi, z is the zth output of the FC layer. In the test phase, the identity of a face video is determined either by feeding it into the trained network (for tight tests) or by selecting gallery subobjects based on the closest distances of the aggregated features (for open tests). \u2212 Face verification is given the identity of a face video either by feeding it into the trained network (for tight tests) or by selecting the pairs of the next (i)."}, {"heading": "2.3.1 Module training", "text": "In this paper, we use the latter option to train the NAN network. Specifically, we train CNN first on individual images with the identification task, then we train the aggregation module in addition to the features extracted from CNN. We have chosen this separate training strategy mainly for two reasons: First, we want to focus in this work on analyzing the effectiveness and performance of the aggregation module with the attention mechanism. Despite the tremendous success of Deep CNN in the image-based facial recognition task, CNN feature aggregation has received little attention to our knowledge. Second, training a deep CNN usually requires a large amount of marked data. While millions of stills can be obtained for training nowadays [26, 23], it does not seem practical to collect such a quantity of face videos."}, {"heading": "3. Experiments", "text": "This section evaluates the performance of the proposed Neural Aggregation Network for Video Face Recognition tasks. First, we present our training details and basic methods for the NAN. Then, we report the results for three video face recognition data sets: the YouTube Face Recognition Data Set [29], the IARPA Janus Benchmark A (IJB-A) [15], and the Celebrity 1000 Data Set [19]."}, {"heading": "3.1. Training details", "text": "As mentioned in Section 2.3, in this paper, two networks are trained separately: To train CNN, we use about 3M facial images of 50K identities crawled from the Internet to perform image-based identification; the faces are detected using the JDA method [2] and aligned using the LBR method [22]; the input image size is 224x224; after the training, CNN is fixed in the Feature Embedding module and we focus on analyzing the effectiveness of the neural aggregation module; the aggregation module is trained on the Videoface datasets we use with standard backpropagation and gradient descent; as the network is quite simple and the image functions compact (128-d), the training process is quite efficient: Training on 5K video pairs with a total of 1M images takes only about 3 minutes on the desktop PC CPU."}, {"heading": "3.2. Baseline Methods", "text": "Since our goal is to create a compact fixed-size representation for video surfaces, we mainly compared our results with aggregation strategies such as average pooling. In addition, we also compared them with some fixed similarity measurements that allow for pairwise comparison at the image level.CNN + Mean L2 measures the similarity of two video surfaces by averaging the feature spacing of all image pairs. It requires storing all the image characteristics of a video or sub-object and has O (n2) complexity for calculating similarity. CNN + Min L2 is similar to the above, but uses the smallest pair spacing. CNN + AvePool is an average pooling along each feature dimension for aggregation. CNN + MaxPool is a maximum pooling along each feature dimension for aggregation. CNN + AvePool, CNN + MaxPool and our NAN all generate a 128d representation for each feature or motif for each (1) and (2) for composition."}, {"heading": "3.3. Results on YouTube Face dataset", "text": "We first tested our method on the YouTube Face dataset, which is designed for full face verification in videos. It contains 3,425 videos from 1,595 different people, and an average of 2.15 videos are available for each topic. The length of the video clips varies from 48 to 6,070 frames on YouTube, with an average length of 181.3 frames per video. There are ten folds of 500 video pairs available, and we follow the standard verification protocol to report average accuracy with cross-validation. Since this dataset does not include visual field marks, we first apply the LBR method [22] to locate landmarks and align each image with CNN before image-level feature extraction. The results of our NAN, baselines, and other methods are presented in Table 1, with their ROC curves shown in Fig. 3. It can be observed that our baselines, except for Maxpool data, achieve similar accuracies to CNN."}, {"heading": "3.4. Results on IJB-A dataset", "text": "The IJB-A dataset contains facial images and videos taken from unrestricted environments. There are 500 subjects with 5,397 images and 2,042 videos sampled with a total of 20,412 frames, 11.4 frames and 4.2 videos per subject on average. The IJB-A dataset features complete pose variations and large differences in imaging conditions, making facial recognition very difficult. In this dataset, each training and test instance referred to as a \"template\" includes a mixture of stills and sampled video frames. The number of images in the templates ranges from 1 to 190 with about 10 frames per template. In our experiments, the faces are balanced with the provided facial markings (two eyes and nose base) prior to image evaluation. We tested the proposed method on the \"comparison line\" (1: 1 match with the templates)."}, {"heading": "3.5. Results on Celebrity-1000 dataset", "text": "The Celebrity 1000 datasets are designed to examine the full video-based facial recognition problem (there are 159,726 video sequences of 1,000 human subjects, with a total of 2.4M frames (15 frames per sequence on average).This dataset provides the facial regions and 5 facial features from the Facial Detector. We use these landmarks to align the faces before the image function and we evaluate our NAN results on both sides. In the open protocol L2 and CNN + MaxPool, the subjects are poorly represented. Through our CNN module, two types of protocols - open and close-ups - exist on this dataset and we evaluated our NAN results on both sides. In the open protocol, 200 subjects are used for training, while video sequences of the remaining 800 subjects are used as a gallery set and probe during the test phase. There are 4 different experimental settings with different numbers of subjects, 800 and 400 galleries."}, {"heading": "4. Closing Remarks", "text": "We have introduced a Neural Aggregation Network based on CNN and attention mechanisms for video face representation and recognition. It merges all input images with a set of content adjustment weights, resulting in a compact (128-d) representation that is invariant for the order of input images, which is an important feature in face recognition. The structure of the aggregation module is simple with small computation and memory prints, but can produce a comprehensive face representation after training through supervised learning. Experiments have shown that the proposed NAN network consistently exceeds the baseline of handmade aggregations and sets a new state of the art on the three video face recognition data sets tested. It should be noted that the proposed method can support general face representation and can therefore also be used in applications other than video face recognition."}], "references": [{"title": "Scene aligned pooling for complex video recognition", "author": ["L. Cao", "Y. Mu", "A. Natsev", "S.-F. Chang", "G. Hua", "J.R. Smith"], "venue": "Computer Vision\u2013ECCV 2012, pages 688\u2013701. Springer,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Joint cascade face detection and alignment", "author": ["D. Chen", "S. Ren", "Y. Wei", "X. Cao", "J. Sun"], "venue": "Computer Vision\u2013ECCV 2014, pages 109\u2013122. Springer,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "An end-to-end system for unconstrained face verification with deep convolutional neural networks", "author": ["J.-C. Chen", "R. Ranjan", "A. Kumar", "C.-H. Chen", "V. Patel", "R. Chellappa"], "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 118\u2013126,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["S. Chopra", "R. Hadsell", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 539\u2013546. IEEE,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Fusing robust face region descriptors via multiple metric learning for face recognition in the wild", "author": ["Z. Cui", "W. Li", "D. Xu", "S. Shan", "X. Chen"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Stable hyper-pooling and query expansion for event detection", "author": ["M. Douze", "J. Revaud", "C. Schmid", "H. J\u00e9gou"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1825\u20131832,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "CoRR, abs/1410.5401,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "Computer vision and pattern recognition, 2006 IEEE computer society conference on, volume 2, pages 1735\u20131742. IEEE,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Discriminative deep metric learning for face verification in the wild", "author": ["J. Hu", "J. Lu", "Y.-P. Tan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1875\u20131882,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Large margin multimetric learning for face and kinship verification in the wild", "author": ["J. Hu", "J. Lu", "J. Yuan", "Y.-P. Tan"], "venue": "Computer Vision\u2013ACCV 2014, pages 252\u2013267. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid", "P. P\u00e9rez"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 3304\u20133311. IEEE,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Triangulation embedding and democratic aggregation for image search", "author": ["H. J\u00e9gou", "A. Zisserman"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3310\u20133317,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a", "author": ["B.F. Klare", "B. Klein", "E. Taborsky", "A. Blanton", "J. Cheney", "K. Allen", "P. Grother", "A. Mah", "M. Burge", "A.K. Jain"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 1931\u20131939. IEEE,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Open source biometric recognition", "author": ["J.C. Klontz", "B.F. Klare", "S. Klum", "A.K. Jain", "M.J. Burge"], "venue": "Biometrics: Theory, Applications and Systems (BTAS), 2013 IEEE Sixth International Conference on, pages 1\u20138. IEEE,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic elastic matching for pose variant face verification", "author": ["H. Li", "G. Hua", "Z. Lin", "J. Brandt", "J. Yang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3499\u20133506,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Eigen-pep for video face recognition", "author": ["H. Li", "G. Hua", "X. Shen", "Z. Lin", "J. Brandt"], "venue": "Computer Vision\u2013ACCV 2014, pages 17\u201333. Springer,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward largepopulation face identification in unconstrained videos", "author": ["L. Liu", "L. Zhang", "H. Liu", "S. Yan"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on, 24(11):1874\u20131884,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Volume structured ordinal features with background similarity measure for video face recognition", "author": ["H. Mendez-Vazquez", "Y. Martinez-Diaz", "Z. Chai"], "venue": "International Conf. on Biometrics (ICB),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "A compact and discriminative face track descriptor", "author": ["O.M. Parkhi", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Face alignment at 3000 fps via regressing local binary features", "author": ["S. Ren", "X. Cao", "Y. Wei", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1685\u20131692,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 815\u2013823,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deeply learned face representations are sparse, selective, and robust", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "Proceedings of the 8  IEEE Conference on Computer Vision and Pattern Recognition, pages 2892\u20132900,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1701\u20131708,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Order matters: sequence to sequence for sets", "author": ["O. Vinyals", "S. Bengio", "M. Kudlur"], "venue": "Proc. International Conf. on Learning Representation, San Juan, Puerto Rico, May", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Face search at scale: 80 million gallery", "author": ["D. Wang", "C. Otto", "A.K. Jain"], "venue": "arXiv preprint arXiv:1507.07242,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Face recognition in unconstrained videos with matched background similarity", "author": ["L. Wolf", "T. Hassner", "I. Maoz"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 529\u2013534. IEEE,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "The svm-minus similarity score for video face recognition", "author": ["L. Wolf", "N. Levy"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 28, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 16, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 29, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 4, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 19, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 17, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 18, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 20, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 9, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 25, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 22, "context": "Video face recognition has caught more and more attention from the community in recent years [29, 17, 30, 5, 20, 18, 19, 21, 10, 26, 23].", "startOffset": 93, "endOffset": 136}, {"referenceID": 25, "context": "A naive approach would be representing a video face as a set of frame-level face features [26, 23].", "startOffset": 90, "endOffset": 98}, {"referenceID": 22, "context": "A naive approach would be representing a video face as a set of frame-level face features [26, 23].", "startOffset": 90, "endOffset": 98}, {"referenceID": 17, "context": "For example, the Eigen-PEP representations [18] takes the average of a part-based representation across different video frames and then conducts a PCA dimension reduction.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "Other works such as the Video Fisher Vector Faces (VF) [21] have attempted to use a more general feature encoding schemes, i.", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "The key component of this network is inspired by the Neural Turing Machine [7] and the Orderless Set Network [27], both of which applied an attention mechanism to organize the input through a memory.", "startOffset": 75, "endOffset": 78}, {"referenceID": 26, "context": "The key component of this network is inspired by the Neural Turing Machine [7] and the Orderless Set Network [27], both of which applied an attention mechanism to organize the input through a memory.", "startOffset": 109, "endOffset": 113}, {"referenceID": 26, "context": "Different from the Orderless Set Network [27], we designed a much simpler network structure for our task of video face recognition, instead of the bulky Long Short Term Memory network (LSTM) [9] used in their paper.", "startOffset": 41, "endOffset": 45}, {"referenceID": 8, "context": "Different from the Orderless Set Network [27], we designed a much simpler network structure for our task of video face recognition, instead of the bulky Long Short Term Memory network (LSTM) [9] used in their paper.", "startOffset": 191, "endOffset": 194}, {"referenceID": 28, "context": "We observed consistent margins in three challenging datasets, including the YouTube face dataset [29], the IJB-A dataset [15], and the Celebrity-1000 dataset [19] compared to the baseline solutions using average pooling and other works.", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "We observed consistent margins in three challenging datasets, including the YouTube face dataset [29], the IJB-A dataset [15], and the Celebrity-1000 dataset [19] compared to the baseline solutions using average pooling and other works.", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "We observed consistent margins in three challenging datasets, including the YouTube face dataset [29], the IJB-A dataset [15], and the Celebrity-1000 dataset [19] compared to the baseline solutions using average pooling and other works.", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "Therefore, it may also serve for feature aggregation for other computer vision tasks such as image recognition and video event recognition [13, 6, 14, 1].", "startOffset": 139, "endOffset": 153}, {"referenceID": 5, "context": "Therefore, it may also serve for feature aggregation for other computer vision tasks such as image recognition and video event recognition [13, 6, 14, 1].", "startOffset": 139, "endOffset": 153}, {"referenceID": 13, "context": "Therefore, it may also serve for feature aggregation for other computer vision tasks such as image recognition and video event recognition [13, 6, 14, 1].", "startOffset": 139, "endOffset": 153}, {"referenceID": 0, "context": "Therefore, it may also serve for feature aggregation for other computer vision tasks such as image recognition and video event recognition [13, 6, 14, 1].", "startOffset": 139, "endOffset": 153}, {"referenceID": 12, "context": "We leave it as our future work to evaluate its performance against other hand-crafted pooling schemes on these tasks, such as VLAD [13, 6, 14] and scene aligned pooling [1].", "startOffset": 131, "endOffset": 142}, {"referenceID": 5, "context": "We leave it as our future work to evaluate its performance against other hand-crafted pooling schemes on these tasks, such as VLAD [13, 6, 14] and scene aligned pooling [1].", "startOffset": 131, "endOffset": 142}, {"referenceID": 13, "context": "We leave it as our future work to evaluate its performance against other hand-crafted pooling schemes on these tasks, such as VLAD [13, 6, 14] and scene aligned pooling [1].", "startOffset": 131, "endOffset": 142}, {"referenceID": 0, "context": "We leave it as our future work to evaluate its performance against other hand-crafted pooling schemes on these tasks, such as VLAD [13, 6, 14] and scene aligned pooling [1].", "startOffset": 169, "endOffset": 172}, {"referenceID": 3, "context": "For face verification, we use a modified version employing a Siamese structure [4].", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "For this purpose, we employed a content based attention mechanism [7] in our new network structure for face set representation.", "startOffset": 66, "endOffset": 69}, {"referenceID": 26, "context": "The structure of the aggregation module looks like an unfolded recurrent neural network (RNN) used in the Orderless Set Network [27] due to the coincidence that there are sequentially two attention blocks working in this module.", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "To leverage modern deep CNN networks with high-end performances, in this paper we follow the GoogLeNet [25] network structure, and equip it with the Batch Normalization (BN) technique [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "To leverage modern deep CNN networks with high-end performances, in this paper we follow the GoogLeNet [25] network structure, and equip it with the Batch Normalization (BN) technique [12].", "startOffset": 184, "endOffset": 188}, {"referenceID": 3, "context": "To train our network for this task, we build a siamese neural aggregation network structure [4] with two NANs sharing their weights, and minimize the average of the contrastive loss [8]: li,j = yi,j ||ri \u2212 rj ||2 +(1\u2212yi,j)max(0,m\u2212 ||ri \u2212 rj ||2), where yi,j = 1 if the pair (i, j) is from the same identity and yi,j = 0 otherwise.", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "To train our network for this task, we build a siamese neural aggregation network structure [4] with two NANs sharing their weights, and minimize the average of the contrastive loss [8]: li,j = yi,j ||ri \u2212 rj ||2 +(1\u2212yi,j)max(0,m\u2212 ||ri \u2212 rj ||2), where yi,j = 1 if the pair (i, j) is from the same identity and yi,j = 0 otherwise.", "startOffset": 182, "endOffset": 185}, {"referenceID": 25, "context": "While millions of still images can be obtained for training nowadays [26, 23], it appears not practical to collect such amount of face videos.", "startOffset": 69, "endOffset": 77}, {"referenceID": 22, "context": "While millions of still images can be obtained for training nowadays [26, 23], it appears not practical to collect such amount of face videos.", "startOffset": 69, "endOffset": 77}, {"referenceID": 28, "context": "Then we will report results on three video face recognition datasets: the YouTube Face dataset [29], the IARPA Janus Benchmark A (IJB-A) [15], and the Celebrity-1000 dataset [19].", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "Then we will report results on three video face recognition datasets: the YouTube Face dataset [29], the IARPA Janus Benchmark A (IJB-A) [15], and the Celebrity-1000 dataset [19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 18, "context": "Then we will report results on three video face recognition datasets: the YouTube Face dataset [29], the IARPA Janus Benchmark A (IJB-A) [15], and the Celebrity-1000 dataset [19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 1, "context": "The faces are detected using the JDA method[2], and aligned with the LBR method [22].", "startOffset": 43, "endOffset": 46}, {"referenceID": 21, "context": "The faces are detected using the JDA method[2], and aligned with the LBR method [22].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "LM3L [11] 81.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "3 DDML (combined) [10] 82.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "EigenPEP [18] 84.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "6 DeepFace-single [26] 91.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "DeepID2+ [24] 93.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "2 \u2013 FaceNet [23] 95.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "As facial landmarks are not provided in this dataset, we first apply the LBR method [22] to locate the landmarks and align every image before image-level feature extraction with CNN.", "startOffset": 84, "endOffset": 88}, {"referenceID": 22, "context": "It can be observed that our baselines, except CNN+MaxPool, achieve similar accuracies to the stateof-the-art method FaceNet [23], which has an accuracy of 95.", "startOffset": 124, "endOffset": 128}, {"referenceID": 22, "context": ", 10K pairs) was used [23].", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "OpenBR [16] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "LSFS [28] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "013 DCNNmanual+metric [3] \u2013 0.", "startOffset": 22, "endOffset": 25}, {"referenceID": 27, "context": "THese results show that both NAN and the baselines outperform previous methods such as [28] and [3], and NAN outperforms its baselines especially on the low FAR cases.", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "THese results show that both NAN and the baselines outperform previous methods such as [28] and [3], and NAN outperforms its baselines especially on the low FAR cases.", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "The errors are reduced by about 56%, 51% and 22% respectively compared to the state-of-the-art method [3], or about 51%, 28% and 5% respectively compared to the best results by CNN+Mean L2 and CNN+AvePool1.", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "Note that averaged image features was used by the method presented in [3] for image set representation.", "startOffset": 70, "endOffset": 73}, {"referenceID": 18, "context": "MTJSR [19] 50.", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "Eigen-PEP [18] 50.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "MTJSR [19] 46.", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "Eigen-PEP [18] 51.", "startOffset": 10, "endOffset": 14}], "year": 2016, "abstractText": "In this paper, we present a Neural Aggregation Network (NAN) for video face recognition. The network takes a face video or face image set of a person with variable number of face frames as its input, and produces a compact and fixeddimension visual representation of that person. The whole network is composed of two modules. The feature embedding module is a CNN which maps each face frame into a feature representation. The neural aggregation module is composed of two content based attention blocks which is driven by a memory storing all the features extracted from the face video through the feature embedding module. The output of the first attention block adapts the second, whose output is adopted as the aggregated representation of the video faces. Due to the attention mechanism, this representation is invariant to the order of the face frames. The experiments show that the proposed NAN consistently outperforms hand-crafted aggregations such as average pooling, and achieves state-of-the-art accuracies on three video face recognition datasets: the YouTube Face, IJB-A and Celebrity-1000 datasets.", "creator": "LaTeX with hyperref package"}}}