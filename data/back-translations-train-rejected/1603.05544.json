{"id": "1603.05544", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2016", "title": "Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent", "abstract": "SGD is the widely adopted method to train CNN. Conceptually it approximates the population with a randomly sampled batch; then it evenly trains batches by conducting a gradient update on every batch in an epoch. In this paper, we demonstrate Sampling Bias, Intrinsic Image Difference and Fixed Cycle Pseudo Random Sampling differentiate batches in training, which then affect learning speeds on them. Because of this, the unbiased treatment of batches involved in SGD creates improper load balancing. To address this issue, we present Inconsistent Stochastic Gradient Descent (ISGD) to dynamically vary training effort according to learning statuses on batches. Specifically ISGD leverages techniques in Statistical Process Control to identify a undertrained batch. Once a batch is undertrained, ISGD solves a new subproblem, a chasing logic plus a conservative constraint, to accelerate the training on the batch while avoid drastic parameter changes. Extensive experiments on a variety of datasets demonstrate ISGD converges faster than SGD. In training AlexNet, ISGD is 21.05\\% faster than SGD to reach 56\\% top1 accuracy under the exactly same experiment setup. We also extend ISGD to work on multiGPU or heterogeneous distributed system based on data parallelism, enabling the batch size to be the key to scalability. Then we present the study of ISGD batch size to the learning rate, parallelism, synchronization cost, system saturation and scalability. We conclude the optimal ISGD batch size is machine dependent. Various experiments on a multiGPU system validate our claim. In particular, ISGD trains AlexNet to 56.3% top1 and 80.1% top5 accuracy in 11.5 hours with 4 NVIDIA TITAN X at the batch size of 1536.", "histories": [["v1", "Thu, 17 Mar 2016 15:49:48 GMT  (733kb,D)", "https://arxiv.org/abs/1603.05544v1", "The patent of ISGD belongs to NEC Labs"], ["v2", "Fri, 18 Mar 2016 05:35:22 GMT  (593kb,D)", "http://arxiv.org/abs/1603.05544v2", "The patent of ISGD belongs to NEC Labs"], ["v3", "Tue, 28 Mar 2017 13:56:03 GMT  (1052kb,D)", "http://arxiv.org/abs/1603.05544v3", "The patent of ISGD belongs to NEC Labs"]], "COMMENTS": "The patent of ISGD belongs to NEC Labs", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["linnan wang", "yi yang", "martin renqiang min", "srimat chakradhar"], "accepted": false, "id": "1603.05544"}, "pdf": {"name": "1603.05544.pdf", "metadata": {"source": "CRF", "title": "Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent", "authors": ["Linnan Wanga", "Yi Yangb", "Renqiang Minb", "Srimat Chakradharb"], "emails": ["linnan.wang@gatech.edu"], "sections": [{"heading": null, "text": "Stochastic Gradient Descent (SGD) updates network parameters with a noise gradient calculated from a random stack, and each stack updates the network evenly once in an epoch. This model applies the same training effort to each stack, but overlooks the fact that the gradient variance induced by sampling bias and intrinsic image difference transfers different training dynamics to stacks. In this paper, we develop a new training strategy for SGD, called Inconsistent Stochastic Gradient Descent (ISGD), to solve this problem. ISGD's core concept is inconsistent training that dynamically adjusts training effort."}, {"heading": "1. Introduction", "text": "The underlying strategies and strategies that have developed in the United States and Europe in recent years are not yet known in the United States."}, {"heading": "2. Related Work", "text": "It is only a matter of time before it is as far as it has ever been, until it is so far."}, {"heading": "3. Problem Statement", "text": "In theory, we prove that the contribution of gradient updates varies according to the batch, based on the analysis of the convergence rate of the SGD. We also assume that Intrinsic Image Differences and Sampling Bias are high-grade factors for the phenomenon, and the hypothesis is confirmed by two controlled experiments. Both theories and experiments support our conclusion that the contribution of a gradient update of a batch is different. Then, we show that the pseudo-random sampling applied by the SGD in the fixed cycle is inefficient to solve this problem. In particular, the consistent gradient updates of all batches, regardless of their status, are wasteful, especially at the end of the training, and the gradient updates of the small loss batch could have been used to accelerate large loss batches."}, {"heading": "3.1. A Recap of CNN Training", "text": "The purpose of the CNN training is to find a solution to the following optimization problem. [22] The purpose of the Weight Decay is to punish the large parameters so that static noise and irrelevant components of weight vectors are suppressed. [22] A typical CNN training process consists of a forward and backward gear. The forward pass results in a loss that measures the discrepancy between current predictions and the truth. The backward pass calculates the gradient and the negative dataset indicating the steepest downward direction."}, {"heading": "3.2. Measure Training Status with Cross Entropy Error", "text": "A Convolutionary Neural Network is therefore a function of Rn \u2192 R, whose last layer is a Softmax loss function, which calculates the cross entropy between the true prediction probabilities p (x) and the estimated prediction probabilities p (x). Defining the Softmax loss function of a lot in iteration t is equivalent to the number of images in a lot and regulates the weight drop. As the loss of a lot varies by a small number after it is fully detected, the loss generated by the cross entropy is a reliable indicator of the formation status of a lot."}, {"heading": "3.3. Motivation: Non-uniform Training Dynamics of Batches", "text": "In fact, it is that it is a way in which people see themselves as being able to surpass themselves. (...) In fact, it is that people are able to surpass themselves. (...) It is that people are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if people are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves."}, {"heading": "3.4. Problems of Consistent Training in SGD", "text": "It is simple in math, but not in system implementation."}, {"heading": "4. Inconsistent Stochastic Gradient Descent", "text": "The first question is how to dynamically identify a slow or undertrained stack during training. We model the training as a stochastic process and apply the upper control limit to dynamically identify an undertrained stack. The second question is how to accelerate an undertrained stack. We propose a new optimization to be solved on the stack, the goal of which is to speed up training without drastic parameter changes. For practical reasons, we also study the effects of ISGD stack size on convergence rate, system saturation, and synchronization costs."}, {"heading": "4.1. Identifying Under-trained Batch", "text": "The reasons for this are: 1) SGD requires a small learning rate (lr) [29] to convergence, and lr is usually less than 10 \u2212 1. lr determines the step length, while the normalized gradient determines the step direction. 2) Each batch represents the original data set, and there is a strong correlation between the batches in formation. This implies that the loss of a batch will not be drastically different from the average iteration process. Figure 2 shows the loss distribution of a network on CIFAR-10, in which losses by epochs are sorted."}, {"heading": "4.2. Inconsistent Training", "text": "The core concept of our training model is to reduce more iterations to the large loss parameters than to the small losses. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "4.3. Extend to Other SGD Variants", "text": "It is easy to extend the inconsistent training to other variants of SGD. Momentum [30], for example, updates the weight with the following equations: vt + 1 = \u00b5vt \u2212 \u03b1 \u0432 (wt) wt + 1 = wt + vt + 1 (19), and the Nesterov accelerated gradient follows the update rule of vt + 1 = \u00b5vt \u2212 \u03b1 \u0432 (wt + \u00b5vt) wt + 1 = wt + vt + 1 (20). To introduce inconsistent training for these SGD variants, we just need to change line 21 of Alg.1 according to Equation 19 or Equation 20."}, {"heading": "4.4. Parallel ISGD", "text": "The ISGD intends to scale the distributed or multiGPU system with MPI-style collectives such as broadcast, reduction, and reduction [31]. Alg.1 and Alg.2 are already the parallel versions manifested by the collectives in them. Fig.4 demonstrates the parallelization scheme of data within the ISGD. Suppose there are n computing nodes, each of which is a GPU or server in a cluster. Each node contains a model duplicate. A node fetches an independent segment of the original batch, known as a subbatch. Subsequently, all nodes simultaneously compute subgradients and sublosses with the assigned subbatches. Once the computation is done, the algorithm reduces subgradients and sublosses (lines 10-12 in Alg.1) to a master node to acquire a global gradient and sublosses with the assigned subbatches. Subsequently, the algorithm updates the network algorithm (the 21 in Network Node I) to the algorithm (the algorithm in Network Node 1)."}, {"heading": "4.5. Batch Size and Convergence Speed", "text": "The current convergence rate analysis uses iterations as the only performance variable, but ignores the fact that a faster iteration algorithm can cost more time than its slower counterpart, so it is handy to analyze the time-domain convergence rate. Suppose the maximum processing capacity of a system is C1 frames per second, and the time spent on synchronization is C2 seconds. Network costs are a constant because they depend only on the size of the network parameter. A gradient update essentially yields costs: Titer = tcomp + tcomm = nb C1 + C2 (21), where nb is the batch size."}, {"heading": "5. Experiments", "text": "In this section, we demonstrate the performance of inconsistent training against SGD variants such as Momentum and Nesterov on a variety of widely recognized data sets, including MNIST [34], CIFAR-10, and ImageNet. MNIST has 60,000 handwritten digits from 0 to 9. CIFAR-10 has 60,000 32 x 32 RGB images categorized into 10 classes. ILSVRC 2012 ImageNet has 1431167 256 x 256 RGB images representing 1000 object categories. We use LeNet, Caffe CIFAR-10 Quick, and AlexNet to sort on MNIST, CIFAR-10, and ImageNet. The complexity of the networks is proportional to the size of the data sets. Therefore, our benchmarks cover the small, medium, and large CNN training. We conduct the experiments on a MultiGPU system with 4 NVIA MaxUITCITCDA 4GB, which is the 4GB compression machine."}, {"heading": "5.1. Qualitative Evaluation of Inconsistent Training", "text": "This section aims at qualitative evaluation of the impact of inconsistent training. The purpose of inconsistent training is to distribute the training effort across several parts so that the parts receive more training than the parts of the training. To qualitatively assess the impact of inconsistent training, we examine the history of loss distribution, average loss, standard deviation of loss distribution, and evaluation accuracy. We place training with Caffe CIFAR10 Quick Network on CIFAR-10 dataset. The batch size is set at 2500 units, which are independent of each other. Fig.6a and Fig.6b present the loss distribution of 20 units in the training. We record losses in eras the solver explores a batch only once in an epoch. The inconsistent training has the following merits: 1) ISGD converts faster than SGD due to the improvement of the training model."}, {"heading": "5.2. Performance Evaluations", "text": "This year, it has come to the point where there is only one chance to retaliate."}, {"heading": "5.3. Time Domain Convergence Rate W.R.T Batch Size on MultiGPUs", "text": "Fig.8 shows convergence speeds at different batch sizes on MNIST, CIFAR, and ImageNet datasets. Figures reflect the following conclusions: 1) A sufficiently large stack is required for multi-GPU training; the single GPU training includes only tcompt calculations; while the multi-GPU training includes an additional term tcomm for synchronization; a small stack size for single GPU training is preferred to ensure frequent gradient updates; and in multi-GPU training, the cost of synchronization increases linearly with the number of gradient updates; the increasing stack size improves the convergence rate, resulting in fewer iterations and synchronizations; and it also improves system utilization and saturation; as a result, a moderate stack size compared to the multi-GPU training is specified as unfavored in Fig.2)."}, {"heading": "6. Summary", "text": "In this paper, we propose inconsistent training to dynamically adjust the training status of the training unit. ISGD models training as a stochastic process and uses stochastic process control techniques to immediately identify a loss-prone group. ISGD then solves a new sub-problem to accelerate training of the undertrained group. Extensive experiments using a variety of data sets and models show the promising performance of an inconsistent training."}], "references": [{"title": "Distributed asynchronous online learning for natural language processing", "author": ["K. Gimpel", "D. Das", "N.A. Smith"], "venue": "in: Proceedings of the Fourteenth Conference on Computational Natural Language Learning, Association for Computational Linguistics", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel matrix factorization for recommender systems", "author": ["H.-F. Yu", "C.-J. Hsieh", "S. Si", "I.S. Dhillon"], "venue": "Knowledge and Information Systems 41 (3) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Q", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang"], "venue": "V. Le, et al., Large scale distributed deep networks, in: Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "M\u00e9thode g\u00e9n\u00e9rale pour la r\u00e9solution des systemes d\u00e9quations simultan\u00e9es", "author": ["A. Cauchy"], "venue": "Comp. Rend. Sci. Paris 25 (1847) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1847}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N.L. Roux", "M. Schmidt", "F.R. Bach"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1951}, {"title": "Large scale online learning", "author": ["L.B.Y. Le Cun", "L. Bottou"], "venue": "Advances in neural information processing systems 16 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Online learning and stochastic approximations", "author": ["L. Bottou"], "venue": "On-line learning in neural networks 17 (9) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Blasx: A high performance level-3 blas library for heterogeneous multi-gpu computing", "author": ["L. Wang", "W. Wu", "Z. Xu", "J. Xiao", "Y. Yang"], "venue": "in: Proceedings of the 2016 International Conference on Supercomputing, ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "in: Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative learning of deep convolutional feature point descriptors", "author": ["E. Simo-Serra", "E. Trulls", "L. Ferraz", "I. Kokkinos", "P. Fua", "F. Moreno- Noguer"], "venue": "in: Proceedings of the IEEE International Conference on Computer Vision", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Sample size selection in optimization methods for machine learning", "author": ["R.H. Byrd", "G.M. Chin", "J. Nocedal", "Y. Wu"], "venue": "Mathematical programming 134 (1) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient mini-batch training for stochastic optimization", "author": ["M. Li", "T. Zhang", "Y. Chen", "A.J. Smola"], "venue": "in: Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Variance reduction for stochastic gradient optimization", "author": ["C. Wang", "X. Chen", "A.J. Smola", "E.P. Xing"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "An incremental gradient (-projection) method with momentum term and adaptive stepsize rule", "author": ["P. Tseng"], "venue": "SIAM Journal on Optimization 8 (2) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "G", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl"], "venue": "E. Hinton, On the importance of initialization and momentum in deep learning., ICML (3) 28 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Training recurrent neural networks", "author": ["I. Sutskever"], "venue": "Ph.D. thesis, University of Toronto ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research 12 (Jul) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "A simple weight decay can improve generalization", "author": ["J. Moody", "S. Hanson", "A. Krogh", "J.A. Hertz"], "venue": "Advances in neural information processing systems 4 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "On variance reduction in stochastic gradient descent and its asynchronous variants", "author": ["S.J. Reddi", "A. Hefny", "S. Sra", "B. Poczos", "A.J. Smola"], "venue": "in:  Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "in: Proceedings of the 22nd ACM international conference on Multimedia, ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "in: BigLearn, NIPS Workshop, no. EPFL- CONF-192376", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "in: Proceedings of COMPSTAT\u20192010, Springer", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "On the momentum term in gradient descent learning algorithms", "author": ["N. Qian"], "venue": "Neural networks 12 (1) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1999}, {"title": "A", "author": ["E. Gabriel", "G.E. Fagg", "G. Bosilca", "T. Angskun", "J.J. Dongarra", "J.M. Squyres", "V. Sahay", "P. Kambadur", "B. Barrett"], "venue": "Lumsdaine, et al., Open mpi: Goals, concept, and design of a next generation mpi implementation, in: European Parallel Virtual Machine/Message Passing Interface Users Group Meeting, Springer", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "Journal of Machine Learning Research 13 (Jan) ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "C", "author": ["Y. LeCun", "C. Cortes"], "venue": "J. Burges, The mnist database of handwritten digits ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "In particular, large scale neural networks have drastically improved various systems in natural language processing [1], video motion analysis [2], and recommender systems [3].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "In particular, large scale neural networks have drastically improved various systems in natural language processing [1], video motion analysis [2], and recommender systems [3].", "startOffset": 143, "endOffset": 146}, {"referenceID": 2, "context": "In particular, large scale neural networks have drastically improved various systems in natural language processing [1], video motion analysis [2], and recommender systems [3].", "startOffset": 172, "endOffset": 175}, {"referenceID": 3, "context": "For example, it takes 10000 CPU cores up to days to complete the training of a network with 1 billion parameters [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "The standard first order full Gradient Descent (GD), which dates back to [5], calculates the gradient with the whole dataset.", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": "edu (Linnan Wang) descent (O(\u03c1k), \u03c1 < 1) [6], the computation in an iteration linearly increases with the size of dataset.", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "To address this issue, Stochastic Gradient Descent [7, 8] was proposed by observing a large amount of redundancy among training examples.", "startOffset": 51, "endOffset": 57}, {"referenceID": 7, "context": "To address this issue, Stochastic Gradient Descent [7, 8] was proposed by observing a large amount of redundancy among training examples.", "startOffset": 51, "endOffset": 57}, {"referenceID": 8, "context": "Although the convergence rate of SGD, O(1/ \u221a bk + 1/k) [9] where b is the batch size, is slower than GD, SGD updates the model much faster than GD in a period, i.", "startOffset": 55, "endOffset": 58}, {"referenceID": 9, "context": "SGD hits a sweet spot between the good system utilization [10] and the fast gradient updates.", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "It is simple in math, while none-trivial to be implemented on a large-scale dataset such as ImageNet [11].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "However, it is known that the gradient variances differentiate batches in the training [12], and gradient updates from the large loss batch contribute more than the small", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "loss ones [13].", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "Enlarging the batch size expedites the convergence [14], but it linearly adds computations in an iteration.", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "In this case, a moderate large batch reduces overall communications [15], and it also improves the system saturation and the available parallelism.", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "The stochastic sampling in SGD introduces the gradient variance, which slows down the convergence rate [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 11, "context": "Stochastic Variance Reduced Gradient (SVRG) [12] keeps network historical parameters and gradients to explicitly reduce the variance of update rule, but the authors indicate SVRG only works well for the fine-tuning of non-convex neural network.", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "[16] explore the control variates on SGD, while Zhao and Tong [17] explore the importance sampling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Momentum [18] is a widely recognized heuristic to boost SGD.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "Momentum damps oscillations in directions of high curvature by combining gradients with opposite signs, and it builds up speed toward a direction that is consistent with the previously accumulated gradients [19].", "startOffset": 207, "endOffset": 211}, {"referenceID": 18, "context": "The update rule of Nesterov\u2019s accelerated gradient is similar to Momentum [20], but the minor different update mechanism for building the velocity results in important behavior differences.", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "Adagrad [21] adapts the learning rate to the parameters, performing larger updates for infrequent parameters, and smaller updates for frequent parameters.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "[13] adopt a similar idea in training the Siamese network to learn the deep descriptors by intentionally feeding the network with hard training pairs, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "The second term is Weight Decay [22], and \u03bb is a parameter to adjust its contribution (normally around 10\u22124).", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For example, p(x) = [0, 0, 1, 0, 0] indicates the object belongs the category 2 (index starts from 0).", "startOffset": 20, "endOffset": 35}, {"referenceID": 21, "context": "This direction has been well addressed [23, 24].", "startOffset": 39, "endOffset": 47}, {"referenceID": 22, "context": "First, existing datasets, such as Places [25] or ImageNet, contain uneven number of images in each category.", "startOffset": 41, "endOffset": 45}, {"referenceID": 0, "context": "For example, the chance of sampling 1 from [1, 1, 1, 0, 2, 3] is higher than the rest.", "startOffset": 43, "endOffset": 61}, {"referenceID": 0, "context": "For example, the chance of sampling 1 from [1, 1, 1, 0, 2, 3] is higher than the rest.", "startOffset": 43, "endOffset": 61}, {"referenceID": 0, "context": "For example, the chance of sampling 1 from [1, 1, 1, 0, 2, 3] is higher than the rest.", "startOffset": 43, "endOffset": 61}, {"referenceID": 1, "context": "For example, the chance of sampling 1 from [1, 1, 1, 0, 2, 3] is higher than the rest.", "startOffset": 43, "endOffset": 61}, {"referenceID": 2, "context": "For example, the chance of sampling 1 from [1, 1, 1, 0, 2, 3] is higher than the rest.", "startOffset": 43, "endOffset": 61}, {"referenceID": 23, "context": "Existing deep learning frameworks, such as Caffe [27] or Torch [28], alleviates the issue by pre-permuting the entire dataset before slicing into batches: Permute{d} \u2192 d = {d0,d1, .", "startOffset": 49, "endOffset": 53}, {"referenceID": 24, "context": "Existing deep learning frameworks, such as Caffe [27] or Torch [28], alleviates the issue by pre-permuting the entire dataset before slicing into batches: Permute{d} \u2192 d = {d0,d1, .", "startOffset": 63, "endOffset": 67}, {"referenceID": 25, "context": "The reasons are that: 1) SGD demands a small learning rate (lr) [29] to converge, and lr is usually less than 10\u22121.", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "The neural network training needs gradually decrease the learning rate to ensure the convergence [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 26, "context": "For example, Momentum [30] updates the weight with the following equations", "startOffset": 22, "endOffset": 26}, {"referenceID": 27, "context": "Therefore, ISGD separates the algorithm from the system configurations by employing MPIstyle collectives [32].", "startOffset": 105, "endOffset": 109}, {"referenceID": 28, "context": "After T gradient updates, the loss is bounded by [33]", "startOffset": 49, "endOffset": 53}, {"referenceID": 29, "context": "In this section, we demonstrate the performance of inconsistent training against SGD variants such as Momentum and Nesterov on a variety of widely recognized datasets including MNIST [34], CIFAR-10 and ImageNet.", "startOffset": 183, "endOffset": 187}], "year": 2017, "abstractText": "Stochastic Gradient Descent (SGD) updates network parameters with a noisy gradient computed from a random batch, and each batch evenly updates the network once in an epoch. This model applies the same training effort to each batch, but it overlooks the fact that the gradient variance, induced by Sampling Bias and Intrinsic Image Difference, renders different training dynamics on batches. In this paper, we develop a new training strategy for SGD, referred to as Inconsistent Stochastic Gradient Descent (ISGD) to address this problem. The core concept of ISGD is the inconsistent training, which dynamically adjusts the training effort w.r.t the loss. ISGD models the training as a stochastic process that gradually reduces down the mean of batch\u2019s loss, and it utilizes a dynamic upper control limit to identify a large loss batch on the fly. Then, it solves a new subproblem on the identified batch to accelerate the training while avoiding drastic parameter changes. ISGD is straightforward, computationally efficient and without requiring auxiliary memories. A series of empirical evaluations on real world datasets and networks demonstrate the promising performance of inconsistent training.", "creator": "LaTeX with hyperref package"}}}