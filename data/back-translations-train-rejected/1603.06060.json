{"id": "1603.06060", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "DASA: Domain Adaptation in Stacked Autoencoders using Systematic Dropout", "abstract": "Domain adaptation deals with adapting behaviour of machine learning based systems trained using samples in source domain to their deployment in target domain where the statistics of samples in both domains are dissimilar. The task of directly training or adapting a learner in the target domain is challenged by lack of abundant labeled samples. In this paper we propose a technique for domain adaptation in stacked autoencoder (SAE) based deep neural networks (DNN) performed in two stages: (i) unsupervised weight adaptation using systematic dropouts in mini-batch training, (ii) supervised fine-tuning with limited number of labeled samples in target domain. We experimentally evaluate performance in the problem of retinal vessel segmentation where the SAE-DNN is trained using large number of labeled samples in the source domain (DRIVE dataset) and adapted using less number of labeled samples in target domain (STARE dataset). The performance of SAE-DNN measured using $logloss$ in source domain is $0.19$, without and with adaptation are $0.40$ and $0.18$, and $0.39$ when trained exclusively with limited samples in target domain. The area under ROC curve is observed respectively as $0.90$, $0.86$, $0.92$ and $0.87$. The high efficiency of vessel segmentation with DASA strongly substantiates our claim.", "histories": [["v1", "Sat, 19 Mar 2016 07:27:56 GMT  (3661kb,D)", "http://arxiv.org/abs/1603.06060v1", "Accepted at Asian Conference on Pattern Recognition 2015"]], "COMMENTS": "Accepted at Asian Conference on Pattern Recognition 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["abhijit guha roy", "debdoot sheet"], "accepted": false, "id": "1603.06060"}, "pdf": {"name": "1603.06060.pdf", "metadata": {"source": "CRF", "title": "DASA: Domain Adaptation in Stacked Autoencoders using Systematic Dropout", "authors": ["Abhijit Guha Roy"], "emails": ["abhi4ssj@gmail.com,", "debdoot@ee.iitkgp.ernet.in"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Problem Statement", "text": "Consider a retinal image displayed in the RGB color space as I, so that the pixel position x-I has the color vector c (x) = {r (x), g (x), b (x)}. N (x) is a neighborhood of pixels centered in x. The task of retinal vessel segmentation can be formally defined as assigning a class designation y (vessel, background) based on a hypotheses model H (I, x, N (x); (I) train. If the statistics of the samples in I differ significantly from Itrain, the performance of H (\u00b7) is greatly impaired. Generally, the I train is referred to as source domain, and I or the samples used during use belong to the target domain. Hypothesis H (\u00b7), which optimally defines source and target domain, is also referred to as Hsource and Htarget. DA is formally referred to as TransformationHsourceDA \u2192 Htarget 1."}, {"heading": "3. Exposition to the Solution", "text": "Consider the source domain as a dsource with abundantly labeled samples to train an SAE-DNN (Hsource) for the task of segmenting retinal vessels, and a target domain as a dtarget with a limited number of labeled samples and abundant unlabeled samples that are not sufficient to reliably learn Htarget, as illustrated in Figure 1. Dsource and Dtarget are closely related, but exhibit distribution shifts between samples of the source and target domains, resulting in an underdevelopment of Hsource in Dtarget, as illustrated in Figure 1. The technique of creating Hsource with Dsource and then using systematic dropout to match Htarget is explained in the following sections."}, {"heading": "3.1. SAE-DNN learning in the source domain", "text": "The AE is a single neural network that represents the cardinal representations of a pattern p = \u03b2 = J = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = K = K \u00b7 K = K \u00b7 K = K \u00b7 K and B = B = B = B = B = B = J \u00b7 1. We choose fNL (\u00b7) to be a sigmoid function called fNL (z) = 1 / (z). The AE is characteristic of another associated function commonly referred to as a decoding unit, such as the decoding unit called fNL (fNL)."}, {"heading": "3.2. SAE-DNN adaptation in the target domain", "text": "Opaque adjustment of SAE weights using systematic dropouts: The first stage of the DA uses abundant blank samples available in the target domain to maintain nodes that encode domain-invariant hierarchical embeddings, while the nodes in the source domain are retuned to the specific ones in the target domain. We follow the concept of systematic dropouts during training [8]. However, the number of layers and number of nodes in the SAE DNN remain unchanged during the domain adjustment. Fig. 2 illustrates the concept. The weights linking each of the hidden layers are imported from the SAE DNN formed in the source, using an automatic coding mechanism. If each minibatch in Dtarget of this AE is fed with one of the hidden layers from the SAE DNN, while some of the nodes in the hidden layer exhibit a high reaction."}, {"heading": "4. Experiments", "text": "SAE-DNN Architecture: We have a two-layer architecture with L = 2, where AE1 consists of 400 nodes and AE2 of 100 nodes. The number of nodes at the input corresponds to the input with a patch size of 15 \u00d7 15 in the color retinal images in RGB space. AEs are pre-trained unsupervised at a learning rate of 0.3, over 50 epochs, \u03b2 = 0.1 and \u03c1 = 0.04. Monitored weight refinement of SAE-DNN is performed at a learning rate of 0.1 over 200 epochs. Training configuration of learning rate and epochs was the same in the source and target domains, with \u03c4 = 0.1 source and target domains: The SAE-DNN is trained in Dsource using 4% of the available patches from the 20 images in the training set DRIVE data set."}, {"heading": "5. Results and Discussion", "text": "The results, which compare the performance of the SAE-DNN, are presented in terms of logloss and area under ROC curve as shown in Table 1 and DA aspects are reported in Fig. 3. Hierarchical embedding in representations learned across domains: AEs are typically characteristic of learning hierarchically embedded representations. The first level of embedding is presented as overcomplete in nature with respect to w1 in Fig. 3 (g) and shows significant similarities between several weight groups that promote the rarity in nature of w2 in Fig. 3 (h) Some of these weight cores are domain invariant and as such are preserved according to DA, as observed for w1 in Fig. 3 (i) and for w2 in Fig. 3 (j). Some of the cores that are domain specific exhibit significant differences."}, {"heading": "6. Conclusion", "text": "We have presented DASA, a method for knowledge transfer in an SAE-DNN that has been trained with abundantly labeled samples in the source domain for use in the target domain where the number of labeled samples is insufficient to solve the task directly. DASA is based on systematic droupout for customization that is able to use (i) abundantly unlabeled samples and (ii) a limited number of labeled samples in the target domain. We are experimentally providing its effectiveness to solve the problem of vascular segmentation when we train with DRIVE (source domain) data sets and are adapted to use on STARE (target domain) data sets. DASA is observed to outperform the various baselines and also exhibit accelerated learning due to knowledge transfer. While systematic drouput is demonstrated on an SAE-DNN in DASA, it can also be extended to other deep architectures."}, {"heading": "Acknowledgement", "text": "We thank NVIDIA for the partial support of this work by the GPU Education Center at IIT Kharagpur."}], "references": [{"title": "Retinal imaging and image analysis", "author": ["M.D. Abr\u00e0moff", "M.K. Garvin", "M. Sonka"], "venue": "IEEE Rev. Biomed. Engg.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Found., Trends, Mach. Learn.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K. Weinberger", "F. Sha"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Autoencoderbased unsupervised domain adaptation for speech emotion recognition", "author": ["J. Deng", "Z. Zhang", "F. Eyben", "B. Schuller"], "venue": "IEEE Signal Process. Let.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Neural Networks and Learning Machines", "author": ["S. Haykin"], "venue": "Pearson Education,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. Knowledge., Data Engg.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Marginalized denoising autoencoder via graph regularization for domain adaptation", "author": ["Y. Peng", "S. Wang", "B.-L. Lu"], "venue": "In Proc. Neural Inf. Process. Sys.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1958}, {"title": "Real-world font recognition using deep network and domain adaptation", "author": ["Z. Wang", "J. Yang", "H. Jin", "E. Shechtman", "A. Agarwala", "J. Brandt", "T.S. Huang"], "venue": "In Proc. Int. Conf. Learning Representations, page arXiv:1504.00028,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Fast easy unsupervised domain adaptation with marginalized structured dropout", "author": ["Y. Yang", "J. Eisenstein"], "venue": "Proc. Assoc., Comput. Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Transfer learning is an active field of research which deals with transfer of knowledge between the source and target domains for addressing this challenge and enhancing performance of learning based systems [6], when it is challenging to train a system exclusively in the target domain due to unavailability of sufficient labeled samples.", "startOffset": 208, "endOffset": 211}, {"referenceID": 3, "context": "While domain adaptation (DA) have been primarily developed for simple reasoning and shallow network architectures, there exist few techniques for adapting deep networks with complex reasoning [4].", "startOffset": 192, "endOffset": 195}, {"referenceID": 1, "context": "In this paper we propose a systematic dropout based technique for adapting a stacked autoencoder (SAE) based deep neural network (DNN) [2] for the purpose of vessel segmentation in retinal images [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "In this paper we propose a systematic dropout based technique for adapting a stacked autoencoder (SAE) based deep neural network (DNN) [2] for the purpose of vessel segmentation in retinal images [1].", "startOffset": 196, "endOffset": 199}, {"referenceID": 2, "context": "With increase in demand for DA in SAEDNNs different techniques have been proposed including marginalized training [3], via graph regularization [7] and structured dropouts [10], across applications including recognizing speech emotion [4] to fonts [9].", "startOffset": 114, "endOffset": 117}, {"referenceID": 6, "context": "With increase in demand for DA in SAEDNNs different techniques have been proposed including marginalized training [3], via graph regularization [7] and structured dropouts [10], across applications including recognizing speech emotion [4] to fonts [9].", "startOffset": 144, "endOffset": 147}, {"referenceID": 9, "context": "With increase in demand for DA in SAEDNNs different techniques have been proposed including marginalized training [3], via graph regularization [7] and structured dropouts [10], across applications including recognizing speech emotion [4] to fonts [9].", "startOffset": 172, "endOffset": 176}, {"referenceID": 3, "context": "With increase in demand for DA in SAEDNNs different techniques have been proposed including marginalized training [3], via graph regularization [7] and structured dropouts [10], across applications including recognizing speech emotion [4] to fonts [9].", "startOffset": 235, "endOffset": 238}, {"referenceID": 8, "context": "With increase in demand for DA in SAEDNNs different techniques have been proposed including marginalized training [3], via graph regularization [7] and structured dropouts [10], across applications including recognizing speech emotion [4] to fonts [9].", "startOffset": 248, "endOffset": 251}, {"referenceID": 2, "context": "Earlier methods [3, 7, 10] are primarily challenged by their inability to re-tune nodes specific to the source domain to nodes specific for target domain for achieving desired performance, while they are able to only retain nodes or a thinned network which encode domain invariant hierarchical embeddings.", "startOffset": 16, "endOffset": 26}, {"referenceID": 6, "context": "Earlier methods [3, 7, 10] are primarily challenged by their inability to re-tune nodes specific to the source domain to nodes specific for target domain for achieving desired performance, while they are able to only retain nodes or a thinned network which encode domain invariant hierarchical embeddings.", "startOffset": 16, "endOffset": 26}, {"referenceID": 9, "context": "Earlier methods [3, 7, 10] are primarily challenged by their inability to re-tune nodes specific to the source domain to nodes specific for target domain for achieving desired performance, while they are able to only retain nodes or a thinned network which encode domain invariant hierarchical embeddings.", "startOffset": 16, "endOffset": 26}, {"referenceID": 7, "context": "Error backpropagation and weight updates are however across all nodes and not only restricted to the post dropout activated nodes, contrary to classical randomized dropout approaches [8].", "startOffset": 183, "endOffset": 186}, {"referenceID": 4, "context": "However the values of elements of these weight matrices are achieved through learning, and without the need of having class labels of the patterns p, it follows unsupervised learning using some optimization algorithm [5], viz.", "startOffset": 217, "endOffset": 220}, {"referenceID": 7, "context": "We follow the concept of systematic node drop-outs during training [8].", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "We set a free parameter \u03c4 \u2208 [0, 1] defined as the transfer coefficient used for defining saliency metric ({sj} \u2208 s) for the j node in the l layer as", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "Generally \u03c4 \u2208 [0, 1] with \u03c4 \u2192 0 being associated with large margin transfer between domains when they are not very dissimilar, and \u03c4 \u2192 1 being associated otherwise.", "startOffset": 14, "endOffset": 20}], "year": 2016, "abstractText": "Domain adaptation deals with adapting behaviour of machine learning based systems trained using samples in source domain to their deployment in target domain where the statistics of samples in both domains are dissimilar. The task of directly training or adapting a learner in the target domain is challenged by lack of abundant labeled samples. In this paper we propose a technique for domain adaptation in stacked autoencoder (SAE) based deep neural networks (DNN) performed in two stages: (i) unsupervised weight adaptation using systematic dropouts in mini-batch training, (ii) supervised fine-tuning with limited number of labeled samples in target domain. We experimentally evaluate performance in the problem of retinal vessel segmentation where the SAE-DNN is trained using large number of labeled samples in the source domain (DRIVE dataset) and adapted using less number of labeled samples in target domain (STARE dataset). The performance of SAE-DNN measured using logloss in source domain is 0.19, without and with adaptation are 0.40 and 0.18, and 0.39 when trained exclusively with limited samples in target domain. The area under ROC curve is observed respectively as 0.90, 0.86, 0.92 and 0.87. The high efficiency of vessel segmentation with DASA strongly substantiates our claim.", "creator": "LaTeX with hyperref package"}}}