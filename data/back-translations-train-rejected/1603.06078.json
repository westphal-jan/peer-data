{"id": "1603.06078", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Deep Shading: Convolutional Neural Networks for Screen-Space Shading", "abstract": "In computer vision, Convolutional Neural Networks (CNNs) have recently achieved new levels of performance for several inverse problems where RGB pixel appearance is mapped to attributes such as positions, normals or reflectance. In computer graphics, screen-space shading has recently increased the visual quality in interactive image synthesis, where per-pixel attributes such as positions, normals or reflectance of a virtual 3D scene are converted into RGB pixel appearance, enabling effects like ambient occlusion, indirect light, scattering, depth-of-field, motion blur, or anti-aliasing. In this paper we consider the diagonal problem: synthesizing appearance from given per-pixel attributes using a CNN. The resulting Deep Shading simulates all screen-space effects as well as arbitrary combinations thereof at competitive quality and speed while not being programmed by human experts but learned from example images.", "histories": [["v1", "Sat, 19 Mar 2016 10:29:57 GMT  (8850kb,D)", "https://arxiv.org/abs/1603.06078v1", null], ["v2", "Wed, 3 Aug 2016 11:11:55 GMT  (8629kb,D)", "http://arxiv.org/abs/1603.06078v2", null]], "reviews": [], "SUBJECTS": "cs.GR cs.LG", "authors": ["oliver nalbach", "elena arabadzhiyska", "dushyant mehta", "hans-peter seidel", "tobias ritschel"], "accepted": false, "id": "1603.06078"}, "pdf": {"name": "1603.06078.pdf", "metadata": {"source": "META", "title": "Deep Shading: Convolutional Neural Networks for Screen-Space Shading", "authors": ["Oliver Nalbach", "Elena Arabadzhiyska", "Dushyant Mehta", "Tobias Ritschel"], "emails": [], "sections": [{"heading": null, "text": "In computer vision, Convolutionary Neural Networks (CNNs) have recently reached new levels of performance for several inverse problems where the appearance of RGB pixels is associated with attributes such as positions, norms, or reflections. In computer graphics, screen space shading has recently increased visual quality in interactive image synthesis, which allows per-pixel attributes such as positions, norms, or reflections of a 3D virtual scene to be converted to RGB pixel optics, allowing effects such as ambient occlusion, indirect light, scattering, depth-of-field, motion blur, or antialiasing. In this essay, we look at the diagonal problem: synthesis of appearance from given pro-pixel attributes using a CNN. The resulting deep shading simulates various screen-space effects in competitive quality and speed while it is not programmed by human experts, but by using global example lighting:"}, {"heading": "1 Introduction", "text": "The move to deep architectures in machine learning has led to unprecedented levels of performance in various computer vision tasks, with multiple applications having the reverse problem of assigning the appearance of image pixels to RGB attributes such as positions, norms, or reflection as an intermediate or end goal. Deep architectures have also opened up opportunities for several novel applications. In computer graphics, screen shading has been critical in increasing visual quality in interactive image synthesis by using pro-pixel attributes such as positions, standards, or reflections of a 3D virtual scene to reflect the RGB appearance that captures effects such as ambient shading (AO), indirect light (GI), sub-surface scattering (SSS), depth of field (DOF), motion blur (MB), and anti-aliasing (AA). In this paper, we revolve around the typical flow of information through computer vision in deep learning pipelines, which can ultimately only enhance the appearance of these attributes by allowing for better use."}, {"heading": "2 Previous Work", "text": "In fact, most people who fight for the rights of women and men are able to have the same rights, the same rights as men and women, the same rights and obligations as men, the same rights and obligations as women, the same rights and obligations as men, the same rights and obligations as men, the same rights and obligations as women, the same rights and obligations as men, the same rights and obligations as women, the same rights and obligations as men, the same rights and obligations as men, the same rights and obligations as men, the same rights and obligations as men, the same rights and obligations as men, the same rights and obligations as men."}, {"heading": "3 Background", "text": "Rather, it is a situation in which one is able to live in a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country and a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "4 Deep Shading", "text": "Here we describe in detail the training data we have created for our task, the proposed network architecture and the training process."}, {"heading": "4.1 Data Generation", "text": "The structure of the data sets consists of 61,000 pairs of deferred shadow buffers and corresponding shaded reference images in a resolution of 512 x 512 px for AO and 256 x 256 px for all other effects. Of these 61,000 pairs, we use 54,000 images to train the network, 6,000 for validation and the remaining 1,000 for testing (Sec. 6.2). The motion sequences and validation images share the same amount of 10 scenes, while the test images are taken from 4 different scenes that are not used in training or validation. To generate the 60,000 turn / validation images, we first render 1,000 pairs for each of the ten scenes of different nature (Fig. 3, left parts). These images are then rotated (in steps of 90 scenes filmed) and rotated horizontally and vertically to easily increase the robustness and size of the training set."}, {"heading": "4.2 Network", "text": "In fact, most of them are able to survive on their own, without having to put themselves centre stage."}, {"heading": "5 Results", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "6 Analysis", "text": "In the first part of this section, we will look at some shortcomings in the form of typical artifacts produced by our method, and also discuss how the network reacts when applied to new resolutions and attributes presented with a different field of view value; the rest of the section will examine some of the countless alternative ways of applying CNNs and machine learning in general to the problem of screen shading; we will discuss various choices of the actual network structure (paragraph 6.2), loss function (paragraph 6.3) and training data anatomy (paragraph 6.4), as well as two techniques that compete with deep CNNs, namely artificial neural networks (ANNs) and random forest (RFs) (paragraph 6.5)."}, {"heading": "6.1 Visual Analysis", "text": "Typical Artifacts In networks where light transport becomes too complex and the mapping has not been fully captured, what appears plausible in a static image can look wrong in a way that is difficult to compare with common errors in computer graphics: spatio-temporal patterns that resemble the correct patterns but are incompatible with the laws of optics and with each other by adding a picturesque and surrealistic touch. We show exemplary artifacts in Fig. 7. High frequency capture is a key challenge for deep shaders (Fig. 7, a). If the network does not have enough capacity or has not been trained enough, the results in terms of reference could be blurred. We consider this a graceful degradation compared to typical artifacts such as ringing or Monte Carlo noise, which are unstable and unnatural over time."}, {"heading": "6.2 Network Structure", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "6.3 Choice of Loss Function", "text": "The choice of loss function in optimization has a significant influence on how deep shading is perceived by a human observer. We trained the same network structure using the usual losses L1 and L2, as well as the perception-oriented SSIM metric, and also using combinations of the three values. Fig. 11 shows a visual comparison of the results of the respective networks. We found that L1 and L2 tend to produce halos instead of fading effects smoothly, as can be seen in the first two columns. Combining L2 with SSIM also shows this type of artifact to a lesser extent. SSIM and SSIM + L1 both lead to visually appealing results, with pure SSIM being more faithful to the contrast found in the reference images."}, {"heading": "6.4 Training Data Diversity", "text": "In practice, however, the time budget for generating training data is typically limited, and the question arises as to how best to spend that time. A factor that influences the quality of the trained deep shaders is the variety of scenes in the training set, for example, a CNN that has only seen round objects during training will not accurately reflect its effect on square objects. In our training scenes, we use 1000 views from each of 10 different scenes as a starting point (see paragraph 6.4). To see how well CNNs perform for less different data, we have created DO training sets of the same overall size but for a smaller number of different scenes. DO was chosen because we have observed that it is particularly sensitive to scene diversity. The resulting DSSIM values for (same) test sets are plotted on the left in Fig. 12. While the error for five scenes compared to a single scene is 5% smaller, the increase in the number of scenes amounts to just a 10% advantage, resulting from a mere 10% difference in the number of scenes."}, {"heading": "6.5 Comparison With Other Regression Techniques", "text": "In fact, it is that we are able to assert ourselves, that we are able to comply, that we are able to comply with the rules, and that we are able to comply with the rules that we have set ourselves, \"he said."}, {"heading": "7 Conclusion", "text": "We propose Deep Shading, a system that performs shading with the help of CNNs. In a departure from previous applications in computer vision that suggest attributes, Deep Shading uses deep learning to manifest attributes of 3D virtual scenes. It is also the first example of performing complex shading derived from the first principles of optics. We have shown that CNNs can actually create a screen-space shading effect such as ambient occlusion, scattering, depth-field motion and anti-aliasing effects, as well as arbitrary combinations of them at competing quality and speed. Our main result is a detection of screen shading that is not programmed by human experts."}, {"heading": "CIMPOI, M., MAJI, S., KOKKINOS, I., MOHAMED, S., , AND", "text": "VEDALDI, A. 2014. Description of textures in the wild. In CVPR.CRIMINISI, A., AND SHOTTON, J. 2013. Decision forests for computer vision and medical image analysis. Springer Science & Business Media.DACHSBACHER, C. 2011. Analyzing visibility configurations. IEEE Trans. Vis. and Comp. Graph. 17, 4, 475-86."}, {"heading": "DOSOVITSKIY, A., TOBIAS SPRINGENBERG, J., AND BROX, T.", "text": "2015. Learning to create chairs with Convolutionary Neural Networks. In Proc. CVPR, 1538-1546.EIGEN, D., PUHRSCH, C., AND FERGUS, R. 2014. Prediction of depth diagrams using a single image using a multi-scale deep network. In Proc. NIPS, 2366-74.ELEK, O., RITSCHEL, T., AND SEIDEL, H.-P. 2013. Real-time screen space scattering in homogeneous environments. IEEE Computer Graphics and App., 3, 53-65.FARBMAN, Z., FATTAL, R., AND LISCHINSKI, D. 2011. Folding Pyramids. ACM Trans. Graph. (Proc. SIGGRAPH) 30, 6, 175: 1-175: 8.GATYS, L. A., ECKER, A. S., AND THM., AND BEGE, 2015. A neural algorithm of 15.08."}, {"heading": "GIRSHICK, R., DONAHUE, J., DARRELL, T., AND MALIK, J.", "text": "2014. Rich hierarchies for precise object recognition and semantic segmentation. In Proc. CVPR, 580-7."}, {"heading": "HARIHARAN, B., ARBELA\u0301EZ, P., GIRSHICK, R., AND MALIK, J.", "text": "2015. Hypercolumns for object segmentation and fine-grained localization. In Proc. CVPR.HASTAD, J. 1986. Almost optimal lower limits for small depth circuits. In ACM Theory of Computing, ACM, 6-20.HERTZMANN, A. 2003. Machine learning for computer graphics: manifesto and tutorial. In Proc. Pacific Graphics."}, {"heading": "JIA, Y., SHELHAMER, E., DONAHUE, J., KARAYEV, S., LONG, J.,", "text": "GIRSHICK, R., GUADARRAMA, S., AND DARRELL, T. 2014. Caffe: Convolutional architecture for fast feature embedding. In Proc. ACM Multimedia, 675-8.JIMENEZ, J., SUNDSTEDT, V., AND GUTIERREZ, D. 2009. Screenspace perceptual rendering of human skin. ACM Trans. Applied Perception 6, 4, 23.JOHNSON, M. K., DALE, K., AVIDAN, S., PFISTER, H., FREEMAN, W. T., AND MATUSIK, W. 2011. CG2Real: Improving the realism of computer generated images using a large collection of photos. IEE Trans. Vis. and Comp. Graph. 17, 9, 1273-85.JOHNSON, J., ALAHI, AND LIGI, F. 2016. Perceptual for time loss."}, {"heading": "KRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G. E. 2012.", "text": "Imagenet Classification with Deep Convolutionary Neural Networks. In Proc. NIPS, 1097-105."}, {"heading": "KULKARNI, T. D., WHITNEY, W., KOHLI, P., AND TENENBAUM,", "text": "J. B. 2015. Deep convolutional inverse graphics network. In Proc. NIPS.LONG, J., SHELHAMER, E., AND DARRELL, T. 2015. Fully revolutionary networks for semantic segmentation. In Proc. CVPR.LOTTES, T., 2011. FXAA. Nvidia White Paper.MAAS, A. L., HANNUN, A. Y., AND NG, A. Y. 2013. Rectifier nonlinearities improve acoustic models of neural networks. Proc. ICML 30."}, {"heading": "MCGUIRE, M., HENNESSY, P., BUKOWSKI, M., AND OSMAN, B.", "text": "2012. A reconstruction filter for plausible motion blur. In Proc. ACM i3D, 135-42.MITTRING, M. 2007. Search for the next generation: Cryengine 2. In ACM SIGGRAPH 2007 Courses, 97-121.NARIHIRA, T., MAIRE, M., AND YU, S. X. 2015. Direct Intrinsics: Learning Albedo-Shading Decomposition by Convolutionary Regression. In Proc. CVPR, 2992-3."}, {"heading": "NOWROUZEZAHRAI, D., KALOGERAKIS, E., AND FIUME, E.", "text": "2009. Shading of dynamic scenes with arbitrary BRDFs. In Comp. Graph. Forum, Volume 28, 249-58."}, {"heading": "OWENS, J. D., LUEBKE, D., GOVINDARAJU, N., HARRIS, M.,", "text": "KRU \u00bc GER, J., LEFOHN, A. E., AND PURCELL, T. J. 2007. Overview of general calculation of graphics hardware. In Comp. Graph. Forum, vol. 26, 80-113."}, {"heading": "PEDREGOSA, F., VAROQUAUX, G., GRAMFORT, A., MICHEL, V.,", "text": "THIRION, B., GRISEL, O., BLONDEL, M., PRETTENHOFER, P., WEISS, R., DUBOURG, V., VANDERPLAS, J., PASSOS, A., COURNAPEAU, D., BRUCHER, M., PERROT, M., AND DUCHESNAY, E. 2011. Scikit-Learning: Machine Learning in Python. J Machine Learning Res 12, 2825-20.PHONG, B. T. 1975. Illumination of Computer Generated Images. Communication of ACM 18, 6, 311-317."}, {"heading": "REN, P., WANG, J., GONG, M., LIN, S., TONG, X., AND GUO,", "text": "B. 2013. Global Illumination with Radiation Regression Functions. ACM Trans. Graph. (Proc. SIGGRAPH) 32, 4, 130.REN, P., DONG, Y., LIN, S., TONG, X., AND GUO, B. 2015. Image-based Illumination using Neural Networks. ACM Trans. Graph. (TOG) 34, 4, 111.RITSCHEL, T., GROSCH, T., AND SEIDEL, H.-P. 2009. Approximate Dynamic Global Illumination in Image Space. In Proc. ACM i3D, 75-82.ROKITA, P. 1993. Fast Generation of Depth of Field Effects in Computer Graphics. Computer & Graphics 17, 5, 593-95.RONNEBERGER, O., FISCHER, P., AND BROX, T. 2015. U-Net: Convoltional Networks for Bidial Image Segmentation. A1. MIC4c."}, {"heading": "RUMELHART, D. E., HINTON, G. E., AND WILLIAMS, R. J. 1988.", "text": "In ACM SIGGRAPH Computer Graphics, Volume 24, 197-206.TABELLION, E., AND LAMORLETTE, A. 2004. An approximate global lighting system for computer-generated films. ACM Trans. Graph. (Proc., SIGGRAPH) 23, 3, 469-76.WANG, X., FOUHEY, D. F., AND GUPTA, A. 2015. Designing deep networks for surface normal estimation. Proc. CVPR.ZEILER, M. D. 2012. ADADELTA: an adaptive learning rate method. CoRR abs / 1212.5701.ZHAO, H., GALLO, O., FROSIO, M. D. 2015."}], "references": [{"title": "Deep shading: Convolutional neural networks for screen-space shading", "author": ["ANONYMOUS."], "venue": "arXiv 1603.06078.", "citeRegEx": "ANONYMOUS.,? 2016", "shortCiteRegEx": "ANONYMOUS.", "year": 2016}, {"title": "Image-space horizon-based ambient occlusion", "author": ["L. BAVOIL", "M. SAINZ", "R. DIMITROV"], "venue": "ACM SIGGRAPH 2008 Talks.", "citeRegEx": "BAVOIL et al\\.,? 2008", "shortCiteRegEx": "BAVOIL et al\\.", "year": 2008}, {"title": "Image classification using random forests and ferns", "author": ["A. BOSCH", "A. ZISSERMAN", "X. MUNOZ"], "venue": "Proc. ICCV, 1\u20138.", "citeRegEx": "BOSCH et al\\.,? 2007", "shortCiteRegEx": "BOSCH et al\\.", "year": 2007}, {"title": "Approximate reflectance profiles for efficient subsurface scattering", "author": ["P.H. CHRISTENSEN", "B. BURLEY"], "venue": "Tech. rep.", "citeRegEx": "CHRISTENSEN and BURLEY,? 2015", "shortCiteRegEx": "CHRISTENSEN and BURLEY", "year": 2015}, {"title": "Describing textures in the wild", "author": ["M. CIMPOI", "S. MAJI", "I. KOKKINOS", "S. MOHAMED", "A. VEDALDI"], "venue": null, "citeRegEx": "CIMPOI et al\\.,? \\Q2014\\E", "shortCiteRegEx": "CIMPOI et al\\.", "year": 2014}, {"title": "Decision forests for computer vision and medical image analysis", "author": ["A. CRIMINISI", "J. SHOTTON"], "venue": "Springer Science & Business Media.", "citeRegEx": "CRIMINISI and SHOTTON,? 2013", "shortCiteRegEx": "CRIMINISI and SHOTTON", "year": 2013}, {"title": "Analyzing visibility configurations", "author": ["C. DACHSBACHER"], "venue": "IEEE Trans. Vis. and Comp. Graph. 17, 4, 475\u201386.", "citeRegEx": "DACHSBACHER,? 2011", "shortCiteRegEx": "DACHSBACHER", "year": 2011}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["A. DOSOVITSKIY", "J. TOBIAS SPRINGENBERG", "T. BROX"], "venue": "Proc. CVPR, 1538\u20131546.", "citeRegEx": "DOSOVITSKIY et al\\.,? 2015", "shortCiteRegEx": "DOSOVITSKIY et al\\.", "year": 2015}, {"title": "Depth map prediction from a single image using a multi-scale deep network", "author": ["D. EIGEN", "C. PUHRSCH", "R. FERGUS"], "venue": "Proc. NIPS, 2366\u201374.", "citeRegEx": "EIGEN et al\\.,? 2014", "shortCiteRegEx": "EIGEN et al\\.", "year": 2014}, {"title": "Real-time screen-space scattering in homogeneous environments", "author": ["O. ELEK", "T. RITSCHEL", "SEIDEL", "H.-P."], "venue": "IEEE Computer Graph. and App., 3, 53\u201365.", "citeRegEx": "ELEK et al\\.,? 2013", "shortCiteRegEx": "ELEK et al\\.", "year": 2013}, {"title": "Convolution pyramids", "author": ["Z. FARBMAN", "R. FATTAL", "D. LISCHINSKI"], "venue": "ACM Trans. Graph. (Proc. SIGGRAPH) 30, 6, 175:1\u2013175:8.", "citeRegEx": "FARBMAN et al\\.,? 2011", "shortCiteRegEx": "FARBMAN et al\\.", "year": 2011}, {"title": "A neural algorithm of artistic style", "author": ["L.A. GATYS", "A.S. ECKER", "M. BETHGE"], "venue": "arXiv 1508.06576.", "citeRegEx": "GATYS et al\\.,? 2015", "shortCiteRegEx": "GATYS et al\\.", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. GIRSHICK", "J. DONAHUE", "T. DARRELL", "J. MALIK"], "venue": "Proc. CVPR, 580\u20137.", "citeRegEx": "GIRSHICK et al\\.,? 2014", "shortCiteRegEx": "GIRSHICK et al\\.", "year": 2014}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["B. HARIHARAN", "P. ARBEL\u00c1EZ", "R. GIRSHICK", "J. MALIK"], "venue": "Proc. CVPR.", "citeRegEx": "HARIHARAN et al\\.,? 2015", "shortCiteRegEx": "HARIHARAN et al\\.", "year": 2015}, {"title": "Almost optimal lower bounds for small depth circuits", "author": ["J. HASTAD"], "venue": "ACM Theory of computing, ACM, 6\u201320.", "citeRegEx": "HASTAD,? 1986", "shortCiteRegEx": "HASTAD", "year": 1986}, {"title": "Machine learning for computer graphics: A manifesto and tutorial", "author": ["A. HERTZMANN"], "venue": "Proc. Pacific Graphics.", "citeRegEx": "HERTZMANN,? 2003", "shortCiteRegEx": "HERTZMANN", "year": 2003}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. JIA", "E. SHELHAMER", "J. DONAHUE", "S. KARAYEV", "J. LONG", "R. GIRSHICK", "S. GUADARRAMA", "T. DARRELL"], "venue": "Proc. ACM Multimedia, 675\u20138.", "citeRegEx": "JIA et al\\.,? 2014", "shortCiteRegEx": "JIA et al\\.", "year": 2014}, {"title": "Screenspace perceptual rendering of human skin", "author": ["J. JIMENEZ", "V. SUNDSTEDT", "D. GUTIERREZ"], "venue": "ACM Trans. Applied Perception 6, 4, 23.", "citeRegEx": "JIMENEZ et al\\.,? 2009", "shortCiteRegEx": "JIMENEZ et al\\.", "year": 2009}, {"title": "CG2Real: Improving the realism of computer generated images using a large collection of photographs", "author": ["M.K. JOHNSON", "K. DALE", "S. AVIDAN", "H. PFISTER", "W.T. FREEMAN", "W. MATUSIK"], "venue": "IEE Trans. Vis. and Comp. Graph. 17, 9, 1273\u201385.", "citeRegEx": "JOHNSON et al\\.,? 2011", "shortCiteRegEx": "JOHNSON et al\\.", "year": 2011}, {"title": "Perceptual losses for real-time style transfer and super-resolution", "author": ["J. JOHNSON", "A. ALAHI", "LI", "F."], "venue": "arXiv 1603.08155.", "citeRegEx": "JOHNSON et al\\.,? 2016", "shortCiteRegEx": "JOHNSON et al\\.", "year": 2016}, {"title": "The rendering equation", "author": ["J.T. KAJIYA"], "venue": "ACM SIGGRAPH, vol. 20, 143\u201350.", "citeRegEx": "KAJIYA,? 1986", "shortCiteRegEx": "KAJIYA", "year": 1986}, {"title": "A machine learning approach for filtering Monte Carlo noise", "author": ["N.K. KALANTARI", "S. BAKO", "SEN", "P."], "venue": "ACM Trans. Graph. (Proc. SIGGRAPH).", "citeRegEx": "KALANTARI et al\\.,? 2015", "shortCiteRegEx": "KALANTARI et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. KRIZHEVSKY", "I. SUTSKEVER", "G.E. HINTON"], "venue": "Proc. NIPS, 1097\u2013105.", "citeRegEx": "KRIZHEVSKY et al\\.,? 2012", "shortCiteRegEx": "KRIZHEVSKY et al\\.", "year": 2012}, {"title": "Deep convolutional inverse graphics network", "author": ["T.D. KULKARNI", "W. WHITNEY", "P. KOHLI", "J.B. TENENBAUM"], "venue": "Proc. NIPS.", "citeRegEx": "KULKARNI et al\\.,? 2015", "shortCiteRegEx": "KULKARNI et al\\.", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. LONG", "E. SHELHAMER", "T. DARRELL"], "venue": "Proc. CVPR.", "citeRegEx": "LONG et al\\.,? 2015", "shortCiteRegEx": "LONG et al\\.", "year": 2015}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. MAAS", "A.Y. HANNUN", "NG", "A.Y."], "venue": "Proc. ICML 30.", "citeRegEx": "MAAS et al\\.,? 2013", "shortCiteRegEx": "MAAS et al\\.", "year": 2013}, {"title": "A reconstruction filter for plausible motion blur", "author": ["M. MCGUIRE", "P. HENNESSY", "M. BUKOWSKI", "B. OSMAN"], "venue": "Proc. ACM i3D, 135\u201342.", "citeRegEx": "MCGUIRE et al\\.,? 2012", "shortCiteRegEx": "MCGUIRE et al\\.", "year": 2012}, {"title": "Finding next gen: Cryengine 2", "author": ["M. MITTRING"], "venue": "ACM SIGGRAPH 2007 Courses, 97\u2013121.", "citeRegEx": "MITTRING,? 2007", "shortCiteRegEx": "MITTRING", "year": 2007}, {"title": "Direct intrinsics: Learning albedo-shading decomposition by convolutional regression", "author": ["T. NARIHIRA", "M. MAIRE", "YU", "S.X."], "venue": "Proc. CVPR, 2992\u20133.", "citeRegEx": "NARIHIRA et al\\.,? 2015", "shortCiteRegEx": "NARIHIRA et al\\.", "year": 2015}, {"title": "Shadowing dynamic scenes with arbitrary BRDFs", "author": ["D. NOWROUZEZAHRAI", "E. KALOGERAKIS", "E. FIUME"], "venue": "Comp. Graph. Forum, vol. 28, 249\u201358.", "citeRegEx": "NOWROUZEZAHRAI et al\\.,? 2009", "shortCiteRegEx": "NOWROUZEZAHRAI et al\\.", "year": 2009}, {"title": "A survey of general-purpose computation on graphics hardware", "author": ["J.D. OWENS", "D. LUEBKE", "N. GOVINDARAJU", "M. HARRIS", "J. KR\u00dcGER", "A.E. LEFOHN", "T.J. PURCELL"], "venue": "Comp. Graph. Forum, vol. 26, 80\u2013113.", "citeRegEx": "OWENS et al\\.,? 2007", "shortCiteRegEx": "OWENS et al\\.", "year": 2007}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. PEDREGOSA", "G. VAROQUAUX", "A. GRAMFORT", "V. MICHEL", "B. THIRION", "O. GRISEL", "M. BLONDEL", "P. PRETTENHOFER", "R. WEISS", "V. DUBOURG", "J. VANDERPLAS", "A. PASSOS", "D. COURNAPEAU", "M. BRUCHER", "M. PERROT", "E. DUCHESNAY"], "venue": "J", "citeRegEx": "PEDREGOSA et al\\.,? 2011", "shortCiteRegEx": "PEDREGOSA et al\\.", "year": 2011}, {"title": "Illumination for computer generated pictures", "author": ["B.T. PHONG"], "venue": "Communications of the ACM 18, 6, 311\u2013317.", "citeRegEx": "PHONG,? 1975", "shortCiteRegEx": "PHONG", "year": 1975}, {"title": "Global illumination with radiance regression functions", "author": ["REN P.", "WANG J.", "GONG M.", "LIN S.", "TONG X.", "GUO", "B."], "venue": "ACM Trans. Graph. (Proc. SIGGRAPH) 32, 4, 130.", "citeRegEx": "P. et al\\.,? 2013", "shortCiteRegEx": "P. et al\\.", "year": 2013}, {"title": "Image based relighting using neural networks", "author": ["REN P.", "DONG Y.", "LIN S.", "TONG X.", "GUO", "B."], "venue": "ACM Trans. Graph. (TOG) 34, 4, 111.", "citeRegEx": "P. et al\\.,? 2015", "shortCiteRegEx": "P. et al\\.", "year": 2015}, {"title": "Approximating dynamic global illumination in image space", "author": ["T. RITSCHEL", "T. GROSCH", "SEIDEL", "H.-P."], "venue": "Proc. ACM i3D, 75\u201382.", "citeRegEx": "RITSCHEL et al\\.,? 2009", "shortCiteRegEx": "RITSCHEL et al\\.", "year": 2009}, {"title": "Fast generation of depth of field effects in computer graphics", "author": ["P. ROKITA"], "venue": "Computers & Graphics 17, 5, 593\u201395.", "citeRegEx": "ROKITA,? 1993", "shortCiteRegEx": "ROKITA", "year": 1993}, {"title": "U-Net: Convolutional networks for biomedical image segmentation", "author": ["O. RONNEBERGER", "P. FISCHER", "T. BROX"], "venue": "Proc. MICAI. 234\u201341.", "citeRegEx": "RONNEBERGER et al\\.,? 2015", "shortCiteRegEx": "RONNEBERGER et al\\.", "year": 2015}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. RUMELHART", "G.E. HINTON", "R.J. WILLIAMS"], "venue": "Cognitive modeling 5, 3, 1.", "citeRegEx": "RUMELHART et al\\.,? 1988", "shortCiteRegEx": "RUMELHART et al\\.", "year": 1988}, {"title": "Comprehensible rendering of 3-d shapes", "author": ["T. SAITO", "T. TAKAHASHI"], "venue": "ACM SIGGRAPH Computer Graphics, vol. 24, 197\u2013206.", "citeRegEx": "SAITO and TAKAHASHI,? 1990", "shortCiteRegEx": "SAITO and TAKAHASHI", "year": 1990}, {"title": "An approximate global illumination system for computer generated films", "author": ["E. TABELLION", "A. LAMORLETTE"], "venue": "ACM Trans. Graph. (Proc., SIGGRAPH) 23, 3, 469\u201376.", "citeRegEx": "TABELLION and LAMORLETTE,? 2004", "shortCiteRegEx": "TABELLION and LAMORLETTE", "year": 2004}, {"title": "Designing deep networks for surface normal estimation", "author": ["X. WANG", "D.F. FOUHEY", "A. GUPTA"], "venue": "Proc. CVPR.", "citeRegEx": "WANG et al\\.,? 2015", "shortCiteRegEx": "WANG et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. ZEILER"], "venue": "CoRR abs/1212.5701.", "citeRegEx": "ZEILER,? 2012", "shortCiteRegEx": "ZEILER", "year": 2012}, {"title": "Is L2 a good loss function for neural networks for image processing", "author": ["H. ZHAO", "O. GALLO", "I. FROSIO", "J. KAUTZ"], "venue": null, "citeRegEx": "ZHAO et al\\.,? \\Q2015\\E", "shortCiteRegEx": "ZHAO et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Attributes-to-appearance The rendering equation [Kajiya 1986] is a reliable forward model of appearance in the form of radiance incident at a virtual camera sensor when a three-dimensional description of the scene in form of attributes like positions, normals and reflectance is given.", "startOffset": 48, "endOffset": 61}, {"referenceID": 30, "context": "Interactive performance is only possible through advanced parallel implementations in specific shader languages [Owens et al. 2007], which not only demands a substantial programming effort, but the proficiency as well.", "startOffset": 112, "endOffset": 131}, {"referenceID": 27, "context": "Our approach is based on screen-space shading that has been demonstrated to approximate many visual effects at high performance, such as ambient occlusion (AO) [Mittring 2007], indirect light (GI) [Ritschel et al.", "startOffset": 160, "endOffset": 175}, {"referenceID": 35, "context": "Our approach is based on screen-space shading that has been demonstrated to approximate many visual effects at high performance, such as ambient occlusion (AO) [Mittring 2007], indirect light (GI) [Ritschel et al. 2009], sub-surface scattering (SSS) [Jimenez et al.", "startOffset": 197, "endOffset": 219}, {"referenceID": 17, "context": "2009], sub-surface scattering (SSS) [Jimenez et al. 2009], participating media [Elek et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 9, "context": "2009], participating media [Elek et al. 2013], depth-of-field (DOF) ar X iv :1 60 3.", "startOffset": 27, "endOffset": 45}, {"referenceID": 36, "context": "[Rokita 1993] and motion blur (MB) [McGuire et al.", "startOffset": 0, "endOffset": 13}, {"referenceID": 26, "context": "[Rokita 1993] and motion blur (MB) [McGuire et al. 2012].", "startOffset": 35, "endOffset": 56}, {"referenceID": 39, "context": "All of these approaches proceed by transforming a deferred shading buffer [Saito and Takahashi 1990], i.", "startOffset": 74, "endOffset": 100}, {"referenceID": 18, "context": "The CG2Real system [Johnson et al. 2011] starts from simulated images that are then augmented by patches of natural images.", "startOffset": 19, "endOffset": 40}, {"referenceID": 11, "context": "Recently, CNNs were used to transfer artistic style from a corpus of example images to any new exemplar [Gatys et al. 2015].", "startOffset": 104, "endOffset": 123}, {"referenceID": 13, "context": "A general overview of how computer graphics could benefit from machine learning, combined with a tutorial from a CG perspective, is given by Hertzmann [2003]. The CG2Real system [Johnson et al.", "startOffset": 141, "endOffset": 158}, {"referenceID": 6, "context": "Dachsbacher [2011] has used neural networks to reason about occluder configurations.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "Dachsbacher [2011] has used neural networks to reason about occluder configurations. Neural networks have also been used as a basis of pre-computed radiance transfer [Ren et al. 2013] (PRT) by running them on existing features to fit a function valid for a single scene. In a similar spirit, Ren et al. [2015] have applied machine learning to re-lighting: Here an artificial neural network (ANN) learns how image pixels change color in response to modified lighting.", "startOffset": 0, "endOffset": 310}, {"referenceID": 29, "context": "Earlier, neural networks were used to learn a mapping from character poses to visibility for PRT [Nowrouzezahrai et al. 2009].", "startOffset": 97, "endOffset": 125}, {"referenceID": 21, "context": "Kalantari et al. [2015] have used sample data to learn optimal parameters for filtering Monte Carlo Noise.", "startOffset": 0, "endOffset": 24}, {"referenceID": 21, "context": "Kalantari et al. [2015] have used sample data to learn optimal parameters for filtering Monte Carlo Noise. Our function domain, i. e., screenspace attributes, is similar to Kalantari et al. [2015]. The range however, is very different.", "startOffset": 0, "endOffset": 197}, {"referenceID": 21, "context": "Kalantari et al. [2015] have used sample data to learn optimal parameters for filtering Monte Carlo Noise. Our function domain, i. e., screenspace attributes, is similar to Kalantari et al. [2015]. The range however, is very different. While they learn filter parameters, we learn the entire shading. Not much is known about the complexity of the mapping from attributes to filter settings and what is the effect of sub-optimal learning. In our case, the mapping from attributes to the value is as complex as shading itself. At the same time, the stakes are high: learning a mapping from attributes to shading results in an entirely different form of interactive image synthesis, not building on anything such as Monte-Carlo ray-tracing that can be slow to compute. Typically, no end-to-end performance numbers are given for Monte-Carlo noise filtering work such as Kalantari et al. [2015].", "startOffset": 0, "endOffset": 890}, {"referenceID": 10, "context": "For image processing, convolution pyramids [Farbman et al. 2011] have pursued an approach that optimizes over the space of filters to the end of fast and large convolutions.", "startOffset": 43, "endOffset": 64}, {"referenceID": 22, "context": "Of late, deep neural networks, particularly CNNs, have shown unprecedented advances in typical inverse problems such as detection [Krizhevsky et al. 2012], segmentation and detection [Girshick et al.", "startOffset": 130, "endOffset": 154}, {"referenceID": 12, "context": "2012], segmentation and detection [Girshick et al. 2014], or depth [Eigen et al.", "startOffset": 34, "endOffset": 56}, {"referenceID": 8, "context": "2014], or depth [Eigen et al. 2014], normal [Wang et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 41, "context": "2014], normal [Wang et al. 2015] or reflectance estimation [Narihira et al.", "startOffset": 14, "endOffset": 32}, {"referenceID": 28, "context": "2015] or reflectance estimation [Narihira et al. 2015].", "startOffset": 32, "endOffset": 54}, {"referenceID": 24, "context": "One recent advance would be of importance in the application of CNNs to high-quality shading: The ability to produce dense perpixel output, even for high resolutions, by CNNs that do not only decrease, but also increase resolutions as proposed by [Long et al. 2015; Hariharan et al. 2015], resulting in fine per-pixel solutions.", "startOffset": 247, "endOffset": 288}, {"referenceID": 13, "context": "One recent advance would be of importance in the application of CNNs to high-quality shading: The ability to produce dense perpixel output, even for high resolutions, by CNNs that do not only decrease, but also increase resolutions as proposed by [Long et al. 2015; Hariharan et al. 2015], resulting in fine per-pixel solutions.", "startOffset": 247, "endOffset": 288}, {"referenceID": 13, "context": "2015; Hariharan et al. 2015], resulting in fine per-pixel solutions. For the problem of segmentation, Ronneberger et al. [2015] even apply a fully symmetric U-shaped net where each down-sampling step is matched by a corresponding up-sampling step that may also re-use earlier intermediate results of the same resolution level.", "startOffset": 6, "endOffset": 128}, {"referenceID": 7, "context": "CNNs have also been employed to replace certain graphics pipeline operations such as changing the viewpoint [Dosovitskiy et al. 2015; Kulkarni et al. 2015].", "startOffset": 108, "endOffset": 155}, {"referenceID": 23, "context": "CNNs have also been employed to replace certain graphics pipeline operations such as changing the viewpoint [Dosovitskiy et al. 2015; Kulkarni et al. 2015].", "startOffset": 108, "endOffset": 155}, {"referenceID": 38, "context": ", the error is first computed at the output layer and then propagated backwards through the network [Rumelhart et al. 1988].", "startOffset": 100, "endOffset": 123}, {"referenceID": 24, "context": "However, de-convolutional (or up-sampling) networks allow us to increase the resolution back again [Long et al. 2015], which is critical for our task, where per-pixel appearance i.", "startOffset": 99, "endOffset": 117}, {"referenceID": 31, "context": "For surfaces, we use the set of parameters to the Phong [1975] reflection model, i.", "startOffset": 50, "endOffset": 63}, {"referenceID": 3, "context": "For scattering we use the model by Christensen and Burley [2015] which is parameterized by the length of the mean free path for each color channel (Rscatt).", "startOffset": 35, "endOffset": 65}, {"referenceID": 25, "context": "4) consist of leaky ReLUs as described by Maas et al. [2013], which multiply negative values by a small constant instead of zero.", "startOffset": 42, "endOffset": 61}, {"referenceID": 16, "context": "Training Caffe [Jia et al. 2014], an open-source neural network implementation, is used to implement and train our networks.", "startOffset": 15, "endOffset": 32}, {"referenceID": 42, "context": "To facilitate learning of networks of varying complexity, without the need of hyper-parameter optimization, particularly of learning rates, we use an adaptive learning rate method (ADADELTA [Zeiler 2012]) with a momentum of 0.", "startOffset": 190, "endOffset": 203}, {"referenceID": 1, "context": "6, we show a same-time comparison to Horizon-based Ambient Occlusion (HBAO) [Bavoil et al. 2008] which is an efficient technique used in games.", "startOffset": 76, "endOffset": 96}, {"referenceID": 35, "context": "Directional Occlusion Directional occlusion [Ritschel et al. 2009] is a generalization of AO where each sample direction is associated with a radiance sample taken from an environment map and light is summed only from unblocked directions.", "startOffset": 44, "endOffset": 66}, {"referenceID": 40, "context": "To simplify the problem, the set of relevant light paths is often reduced to a single \u201cindirect bounce\u201d, diffuse reflection [Tabellion and Lamorlette 2004] and restricted to a certain radius of influence.", "startOffset": 124, "endOffset": 155}, {"referenceID": 17, "context": "A popular approximation to this is screen-space sub-surface scattering (SSSS) [Jimenez et al. 2009] which essentially applies a spatially-varying blurring kernel to the different color channels of the image.", "startOffset": 78, "endOffset": 99}, {"referenceID": 3, "context": "We produce training data at every pixel by iterating over all other pixels and applying Pixar\u2019s scattering profile [Christensen and Burley 2015] depending on the distance between the 3D position at the two pixels.", "startOffset": 115, "endOffset": 144}, {"referenceID": 26, "context": "The direction and strength of the blur depends on the speed of the object in the image plane [McGuire et al. 2012].", "startOffset": 93, "endOffset": 114}, {"referenceID": 5, "context": "Aside from deep CNNs, approaches such as shallow artificial neural networks (ANNs) and random forests (RFs) [Criminisi and Shotton 2013] could putatively be used for our objective, having found use in image synthesis related tasks such as estimation of filter parameters [Kalantari et al.", "startOffset": 108, "endOffset": 136}, {"referenceID": 21, "context": "Aside from deep CNNs, approaches such as shallow artificial neural networks (ANNs) and random forests (RFs) [Criminisi and Shotton 2013] could putatively be used for our objective, having found use in image synthesis related tasks such as estimation of filter parameters [Kalantari et al. 2015] or relighting [Ren et al.", "startOffset": 271, "endOffset": 294}, {"referenceID": 31, "context": "For ANNs, speed is measured using the OpenGL implementation of the forward pass of the network (as with CNNs), and for RFs we employ scikit-learn [Pedregosa et al. 2011] on a regular workstation.", "startOffset": 146, "endOffset": 169}, {"referenceID": 2, "context": "Even with more readily prallelizable variants of RFs [Bosch et al. 2007], there would have to be an improvement of more than two orders of magnitude on the run time to be comparable with our Deep Shader.", "startOffset": 53, "endOffset": 72}, {"referenceID": 14, "context": "This is a deciding factor in choosing deeper convolutional networks, to allow for an increase in effective receptive field sizes through stacked convolutions and downsampling, without an exponential increase in the number of parameters, while leveraging the expressive power of deeper representations [Hastad 1986].", "startOffset": 301, "endOffset": 314}, {"referenceID": 19, "context": "We would hope that more and diverse training data, advances in learning methods, and new types of deep representations or losses, such as network losses [Johnson et al. 2016] will allow surpassing human shader programmer performance in a not-so-distant future.", "startOffset": 153, "endOffset": 174}], "year": 2016, "abstractText": "In computer vision, convolutional neural networks (CNNs) have recently achieved new levels of performance for several inverse problems where RGB pixel appearance is mapped to attributes such as positions, normals or reflectance. In computer graphics, screenspace shading has recently increased the visual quality in interactive image synthesis, where per-pixel attributes such as positions, normals or reflectance of a virtual 3D scene are converted into RGB pixel appearance, enabling effects like ambient occlusion, indirect light, scattering, depth-of-field, motion blur, or anti-aliasing. In this paper we consider the diagonal problem: synthesizing appearance from given per-pixel attributes using a CNN. The resulting Deep Shading simulates various screen-space effects at competitive quality and speed while not being programmed by human experts but learned from example images.", "creator": "LaTeX acmsiggraph.cls (11/2015)"}}}