{"id": "1603.06127", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Sentence Pair Scoring: Towards Unified Framework for Text Comprehension", "abstract": "We review the task of Sentence Pair Scoring, popular in the literature in various forms --- slanted as Answer Sentence Selection, Paraphrasing, Semantic Text Scoring, Next Utterance Ranking, Recognizing Textual Entailment or e.g. a component of Memory Networks.", "histories": [["v1", "Sat, 19 Mar 2016 18:35:26 GMT  (29kb)", "http://arxiv.org/abs/1603.06127v1", "submitted as long paper to ACL 2016"], ["v2", "Thu, 28 Apr 2016 03:10:26 GMT  (29kb)", "http://arxiv.org/abs/1603.06127v2", "submitted as long paper to ACL 2016"], ["v3", "Fri, 6 May 2016 22:17:36 GMT  (29kb)", "http://arxiv.org/abs/1603.06127v3", "submitted as paper to CoNLL 2016"], ["v4", "Tue, 17 May 2016 14:08:38 GMT  (30kb)", "http://arxiv.org/abs/1603.06127v4", "submitted as paper to CoNLL 2016"]], "COMMENTS": "submitted as long paper to ACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["petr baudi\\v{s}", "jan pichl", "tom\\'a\\v{s} vysko\\v{c}il", "jan \\v{s}ediv\\'y"], "accepted": false, "id": "1603.06127"}, "pdf": {"name": "1603.06127.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["baudipet@fel.cvut.cz", "sedivjan@fel.cvut.cz"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.06 127v 1 [cs.C L] 19 Mar 2We argue that such tasks are similar from a model perspective (especially in the context of high-capacity deep neural models) and propose new foundations by comparing the performance of popular revolutionary, recurring, and attention-based neural models in many sentence pair scoring tasks and datasets. We discuss the problem of evaluating randomized models in a future-proof manner, propose a statistically sound methodology, and seek to improve comparisons by publishing new datasets that are much more difficult than some of the currently well-researched benchmarks. To address the current fragmentation of research in a future-proof manner, we are introducing a unified open source software framework with easily pluggable models that allow for easy evaluation of a wide range of semantic tasks of natural language. This allows us to demonstrate a path toward a universal machine-learned semantic model for these model reading machine tasks, even by training the very different experiments that we are using."}, {"heading": "1 Introduction", "text": "A typical NLP task is to classify a set of symbols, such as a sentence or a document, i.e. the approximation of a function f1 (s).1 There is a large class of problems associated with the classification of a sentence, f2 (s0, s1).This formulation makes it possible to specify a set of symbols, typically sentences. Function f2 represents a kind of semantic similarity, i.e. the two sequences are semantically related."}, {"heading": "2 Tasks and Datasets", "text": "The tasks we know can be described as f2-like problems. In general, we have chosen to focus primarily on tasks where relatively large and realistically complex data sets are freely available. On the contrary, we have explicitly avoided data sets that have license-limited availability or commercial use."}, {"heading": "2.1 Answer Sentence Selection", "text": "In fact, most people are able to determine for themselves what they want and what they want, \"he told the German Press Agency.\" It's not as if we are able to hide, \"he said.\" But it's not as if we are able to hide, as if we are able to hide. \""}, {"heading": "2.2 Next Utterance Ranking", "text": "(Lowe et al., 2015) proposed a new large-scale and realistic dataset for an f2-like task to classify candidates for the next utterance in a chat dialog. The technical formulation of the task is the same as in Answer Sentence Selection, but semantically, selecting the best sequence involves other concerns than selecting a responsive set. The newly proposed dataset for the Ubuntu dialog is based on IRC chat logs of the Ubuntu community's technical support channels and includes randomly typed interactions on computer-related issues. 4 While the training set consists of individually designated pairs of token sequences, the evaluation is done by ranking 10 sequences of sequences of sequences to given messages that are potentially relatively long (even more than 200 tokens).Our primary motivation for using this dataset is its size. The numerical features of this dataset are presented in Table 1.5. We use the version of this dataset based on the data set published so far (the version of this dataset was based on)."}, {"heading": "2.3 Semantic Textual Similarity", "text": "One of the canonical problems for f2-like tasks is the STS track of the SemEval conferences (Agirrea et al., 2015), an annual contest of scoring pairs from 0 to 5 with the aim of maximizing correlation (Pearson's r) with manually annotated gold standard; the data is composed of various pro-source splits ranging from almost word-to-word paraphrases of newspaper titles to loosely related user forum4In some ways, they resemble Tweet data, but without the length description and with highly technical jargon, command sequences, etc. 5As in previous papers, we use only the first 1M pairs (10%) of the training set.6 https: / / github.com / rkadlec-ranking-dataset-creatorquestions. Each year, the final years are used as a training set.Since 2016, the results are not published at the time of writing this paper."}, {"heading": "2.4 Paraphrase Identification", "text": "After all, a popular, separately considered task is paraphrase identification, which can be considered a special case of semantic textual scoring, but the task objective is binary classification, not the regression of a score on a continuous scale.The canonical dataset is the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005), consisting of 4076 training pairs and 1725 test pairs; 2 / 3 of the samples are labeled as 1 (is a paraphrase).77 http: / / aclweb.org / aclwiki / index.php? title = Paraphrase _ identification _ (State _ of _ the _ art, 2015) yet.77 http: / / aclwiki / index.php? title = Paraphrase _ identification _ (State _ of _ the _ art)"}, {"heading": "2.5 Other", "text": "Due to the very wide range of the field, we leave some popular tasks and datasets as future work, especially the Textual Entailment recognition task (supported by the SNLI dataset) (Bowman et al., 2015) and the problem of memory selection in storage networks (supported by the baBi dataset) (Weston et al., 2015). A more realistic large dataset based on the AskUbuntu Stack Overflow Forum was recently proposed (Lei et al., 2015)."}, {"heading": "3 Models", "text": "Since our goal is a universal text understanding model, we focus architecturally on neural network models. We assume that the sequence is transformed to input by N-dimensional word embedding, and we use models that generate a pair of sentence embedding E0, E1 from the word embedding e0, e1. Unless otherwise noted, a Siamese architecture is used that divides the weights between the two sentences. A goal scoring module that compares the E0, E1 sentence embedding to obtain a scalar result is associated with the model; for specific task model configurations, we use either the dot product module E0 \u00b7 ET1 (which represents a non-normalized vector angle, such as in (Yu et al., 2014) or (Weston et al., 2014)) or the MLP module that divides the elementary product and the sum of the embedding layer into this two-layer formula (e.g.)."}, {"heading": "3.1 Baselines", "text": "To anchor the reported performance, we report on several basic methodologies. The weighted word overlap metrics TF-IDF and BM25 (Robertson et al., 1995) are inspired by IR research and provide strong baselines for many tasks. We treat s08The motivation is to capture both angle and euclide distance in multiple weighted sums. In the previous literature, absolute difference is used instead of sum, but both have been performed equally in our experiments and we have adopted sums for technical reasons, e.g. in the query and s1 as a document, counting the number of common words and weighting them accordingly. IDF is used on the training set.The avg metric represents the basic method when using word embeddings that have proven successful, e.g. in (Yu et al., 2014) or (Weston et al., 2014), simply counting the middle vector of the word and weighing it accordingly."}, {"heading": "3.2 Recurrent Neural Networks", "text": "RNN with storage units are popular models for processing sensors (Tan et al., 2015) (Lowe et al., 2015) (Bowman et al., 2015). We use a bidirectional network with 2N GRU storage units 9 (Cho et al., 2014) in each direction; the final unit states are added up using the two GRU calculated per direction to obtain a 2N vector representation of the set. As with the avg baseline, a projection matrix is applied to this representation and the final vectors are compared by an MLP goal shooter. We have found that the application of massive suspenders p = 4 / 5 on both the input and output of the network helps to avoid overhauling from the very beginning of the training."}, {"heading": "3.3 Convolutional Neural Networks", "text": "CNN with a satin second pool layer are also popular models for processing sets (Yu et al., 2014) (Tan et al., 2015) (Severyn and Moschitti, 2015) (He et al., 2015) (Kadlec et al., 2015). We apply a multi-channel folding (Kim, 2014) with a single token channel of N turns and 2, 3, 4 and 5 token channels of N / 2 turns each, relay transfer function, maximum bundling over the entire set and as above a projection onto the shared space and an MLP goal scorer. Dropout9While the LSTM architecture is more popular, we have found that the GRU results are equivalent while the number of parameters are reduced. p = 1 / 2 is applied to both the input and output of the folding network."}, {"heading": "3.4 RNN-CNN Model", "text": "The RNN-CNN model aims to combine both recursive and revolutionary networks by using the states of the memory unit in each token as a new representation of the token, which is then fed into the revolutionary network. Inspired by (Tan et al., 2015), the aim of this model is to allow the RNN to model long-term dependencies and contextual representations of words, while using the CNN pooling operation to clearly select the core of the set. We use the same parameters as for the individual models, except that we find the use of suspensions disadvantageous and need to reduce the number of parameters by using only N memory units per direction."}, {"heading": "3.5 Attention-Based Models", "text": "The idea of attention models is to preferably consider some parts of the sentence when creating their representation (Hermann et al., 2015) (dos Santos et al., 2016) (Rockta \ufffd schel et al., 2015). There are many ways to model attention, since we adopt the conceptually simple and easy-to-implement model attn1511 as the first proof of the concept (Tan et al., 2015). It asymmetrically expands the RNN-CNN model by adding additional links from the s0 CNN output to the post-recurring representation of each s1 token, where an attention level for each token is determined by the weighed sum of the token vectors and focuses on the relevant s1 segment by transforming the attention levels using softmax and multiplying the token representations by the attention levels before feeding them to the revolutionary data network."}, {"heading": "4.2 Experimental Setting", "text": "We use N = 300 dimensional GloVe embedding matrix preschooled in Wikipedia 2014 + Gigaword 5 (Pennington et al., 2014), which we keep adaptable during training; words in the training set that are not included in the pre-trained model are initialized by random vectors that are uniformly selected from [\u2212 0.25, + 0.25] to match the embedded standard deviation. Word overlaps are an important feature in many f2-like tasks (Yu et al., 2014) (Severyn et al., 2015), especially when the sentences may contain designated units, numerical or other data for which no embedding is available. As a workaround, a combination of world overlap and neural model point count is usually used to achieve the endpoint count. We try to take a more flexible approach by adding several additional dimensions to each individual input character, which carry unenlarged punctuation flags and lrays."}, {"heading": "4.3 Evaluation Methodology", "text": "One consideration we must emphasize is that randomness plays a major role in neural models, both in terms of randomized weight initialization and stochastic dropout. For example, the typical methodology for reporting the results of the Wang dataset is to evaluate and report a single test run after processing the 10Link. The snapshot in submission.dev set, 11 but wang test MRR has an empirical standard deviation of 0.025 over repeated runs of our attn1511 model, which is more than twice as large as the gap between all two consecutive papers advancing the state of the art on this dataset! See example in Fig. 2 for a practical example of this phenomenon."}, {"heading": "4.4 Results", "text": "Figures 2 to 5 show the performance of our models in terms of cross-sectional tasks. We can observe an effect analogous to what was described in (Kadlec et al., 2015) - if the data set is smaller, CNN models are preferable, while a larger data set allows RNN models to capture the text comprehension task confirmed by face-to-face communication with paper authors.12 For a larger number of samples, this estimate converges to the normal confidence level. Note that the confidence interval covers the range of actual expected performance, not the individually measured samples. In addition, IR baseline offers strong competition and the search for new ways to combine it with models should prove beneficial in the future."}, {"heading": "5 Model Reusability", "text": "To confirm the hypothesis that our models are learning a generic task that resembles a form of text comprehension, we tried to train a model on the large Ubuntu Next Utterance Ranking task and then transfer the weights and retrain the model instance to the smaller curatedv2 dataset (Answer Sentence Selection task).We used the RNN model for the experiment in a configuration with the point-product scorer (which works much better on the Ubuntu dataset).The configuration achieved when it is trained from scratch on curatedv2, MRR 0.371 \u00b1 0.023, while it achieves 0.493 \u00b1 0.021 with the Ubuntu Dialogue Pre-Training MRR. This represents a jump of about 0.12 points and the resulting performance is comparable to a much more complex attention model that has been tried exclusively on the curatedv2 datasets, but that we have never tried to use it during our initial training sessions."}, {"heading": "6 Conclusion", "text": "We have unified a variety of tasks into a single scientific framework of sentence pair evaluation, demonstrating a platform for generalized modeling of the problem and aggregated benchmarking of these models across many datasets. Promising early transfer learning results indicate that the search for a generic neural model capable of understanding task-independent text is becoming a meaningful endeavor. The open-source nature of our framework and the choice of a popular and extensible deep learning library allow for high reusability of our research and easy enhancements with more advanced models."}, {"heading": "6.1 Future Work", "text": "In fact, it is not that we are able to cover all the important tasks at once. Developing adapters, models and benchmarks for the task of recognizing text modules remains the most important next step, as does the problem of real events and newspaper snippets, as well as solving school exams based on a number of memory modules. We are in the process of developing real, hard data networks based on yes / no questions and newspaper snippets, as well as solving school exams based on textbooks and encyclopeditists."}, {"heading": "Acknowledgments", "text": "This work was financially supported by the Funding Agency of the Czech Technical University in Prague, grant number SGS16 / 084 / OHK3 / 1T / 13 and the Forecast Foundation. Computer resources were provided by CESNET LM2015042 and CERIT Scientific Cloud LM2015085 as part of the \"Projects of Major Research, Development and Innovation Infrastructures\" program. We would like to thank Toma \ufffd s Tunys, Rudolf Kadlec, Ryanair Lowe, Cicero Nogueira dos Santos and Bowen Zhou for their helpful discussions and insights, and Silvestr Stanko and Jir i Na dvorn\u0131 \u0301 k for their software contributions."}], "references": [{"title": "Modeling of the question answering task in the YodaQA system", "author": ["Petr Baudi\u0161", "Jan \u0160ediv\u00fd."], "venue": "Experimental IR Meets Multilinguality, Multimodality, and Interaction, pages 222\u2013228. Springer.", "citeRegEx": "Baudi\u0161 and \u0160ediv\u00fd.,? 2015", "shortCiteRegEx": "Baudi\u0161 and \u0160ediv\u00fd.", "year": 2015}, {"title": "YodaQA: A Modular Question Answering System Pipeline", "author": ["Petr Baudi\u0161."], "venue": "POSTER 2015 - 19th International Student Conference on Electrical Engineering.", "citeRegEx": "Baudi\u0161.,? 2015", "shortCiteRegEx": "Baudi\u0161.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Learning to rank using gradient descent", "author": ["Chris Burges", "Tal Shaked", "Erin Renshaw", "Ari Lazier", "Matt Deeds", "Nicole Hamilton", "Greg Hullender."], "venue": "Proceedings of the 22nd international conference on Machine learning, pages 89\u201396. ACM.", "citeRegEx": "Burges et al\\.,? 2005", "shortCiteRegEx": "Burges et al\\.", "year": 2005}, {"title": "Syntaxaware multi-sense word embeddings for deep compositional models of meaning", "author": ["Jianpeng Cheng", "Dimitri Kartsaklis."], "venue": "arXiv preprint arXiv:1508.02354.", "citeRegEx": "Cheng and Kartsaklis.,? 2015", "shortCiteRegEx": "Cheng and Kartsaklis.", "year": 2015}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."], "venue": "CoRR, abs/1601.06733.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "CoRR, abs/1409.1259.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Automatically constructing a corpus of sentential paraphrases", "author": ["William B Dolan", "Chris Brockett"], "venue": null, "citeRegEx": "Dolan and Brockett.,? \\Q2005\\E", "shortCiteRegEx": "Dolan and Brockett.", "year": 2005}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "CoRR, abs/1505.08075.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Multiperspective sentence similarity modeling with convolutional neural networks", "author": ["Hua He", "Kevin Gimpel", "Jimmy Lin"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1684\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": null, "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Discriminative improvements to distributional sentence similarity", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "EMNLP.", "citeRegEx": "Ji and Eisenstein.,? 2013", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2013}, {"title": "Improved deep learning baselines for ubuntu corpus dialogs", "author": ["Rudolf Kadlec", "Martin Schmid", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1510.03753.", "citeRegEx": "Kadlec et al\\.,? 2015", "shortCiteRegEx": "Kadlec et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in Neural Information Processing Systems, pages 3276\u20133284.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher."], "venue": "CoRR, abs/1506.07285.", "citeRegEx": "Kumar et al\\.,? 2015", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Denoising bodies to titles: Retrieving similar questions with recurrent convolutional models", "author": ["Tao Lei", "Hrishikesh Joshi", "Regina Barzilay", "Tommi S. Jaakkola", "Kateryna Tymoshenko", "Alessandro Moschitti", "Llu\u0131\u0301s M\u00e0rquez i Villodre"], "venue": null, "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."], "venue": "CoRR, abs/1506.08909.", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Okapi at trec-3", "author": ["Stephen E Robertson", "Steve Walker", "Susan Jones"], "venue": null, "citeRegEx": "Robertson et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Robertson et al\\.", "year": 1995}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Phil Blunsom."], "venue": "CoRR, abs/1509.06664.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2015", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["Aliaksei Severyn", "Alessandro Moschitti."], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages", "citeRegEx": "Severyn and Moschitti.,? 2015", "shortCiteRegEx": "Severyn and Moschitti.", "year": 2015}, {"title": "Dls@ cu: Sentence similarity from word alignment and semantic vector composition", "author": ["Md Arafat Sultan", "Steven Bethard", "Tamara Sumner."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation, pages 148\u2013153.", "citeRegEx": "Sultan et al\\.,? 2015", "shortCiteRegEx": "Sultan et al\\.", "year": 2015}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "CoRR, abs/1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Lstmbased deep learning models for non-factoid answer selection", "author": ["Ming Tan", "Bing Xiang", "Bowen Zhou."], "venue": "CoRR, abs/1511.04108.", "citeRegEx": "Tan et al\\.,? 2015", "shortCiteRegEx": "Tan et al\\.", "year": 2015}, {"title": "What is the jeopardy model? a quasisynchronous grammar for qa", "author": ["Mengqiu Wang", "Noah A Smith", "Teruko Mitamura."], "venue": "EMNLP-CoNLL, volume 7, pages 22\u201332.", "citeRegEx": "Wang et al\\.,? 2007", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "CoRR, abs/1410.3916.", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "CoRR, abs/1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Deep learning for answer sentence selection", "author": ["Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman."], "venue": "CoRR, abs/1412.1632.", "citeRegEx": "Yu et al\\.,? 2014", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "This task is popular in the NLP research community thanks to the dataset introduced in (Wang et al., 2007) which we refer to as wang, with six papers published between February 2015 and 2016 alone and neural models substantially improving over classical approaches based primarily on parse tree edits.", "startOffset": 87, "endOffset": 106}, {"referenceID": 32, "context": "Alternative datasets WikiQA (Yang et al., 2015) and InsuranceQA (Tan et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 28, "context": ", 2015) and InsuranceQA (Tan et al., 2015) were proposed, but are encumbered by licence restrictions.", "startOffset": 24, "endOffset": 42}, {"referenceID": 0, "context": "To alleviate the problems listed above, we are introducing a new dataset yodaqa/curatedv2 based on the curatedv2 question dataset (introduced in (Baudi\u0161 and \u0160ediv\u00fd, 2015), further denoisified by Mechanical Turkers) with candidate sentences as retrieved by the YodaQA question answering system (Baudi\u0161, 2015) from English Wikipedia.", "startOffset": 145, "endOffset": 170}, {"referenceID": 1, "context": "To alleviate the problems listed above, we are introducing a new dataset yodaqa/curatedv2 based on the curatedv2 question dataset (introduced in (Baudi\u0161 and \u0160ediv\u00fd, 2015), further denoisified by Mechanical Turkers) with candidate sentences as retrieved by the YodaQA question answering system (Baudi\u0161, 2015) from English Wikipedia.", "startOffset": 293, "endOffset": 307}, {"referenceID": 20, "context": "(Lowe et al., 2015) proposed a new large-scale and realistic dataset for an f2-style task of ranking candidates for the next utterance in a chat dialog, given the dialog context.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "(Lowe et al., 2015) (Kadlec et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": ", 2015) (Kadlec et al., 2015)", "startOffset": 8, "endOffset": 29}, {"referenceID": 26, "context": "Contrary to Answer Sentence Selection, state-of-art methods are based on parse tree alignments (Sultan et al., 2015) and weren\u2019t beaten by neural models yet.", "startOffset": 95, "endOffset": 116}, {"referenceID": 21, "context": "We also report results for this task on another dataset from the SemEval conferences, SICK2014 (Marelli et al., 2014).", "startOffset": 95, "endOffset": 117}, {"referenceID": 8, "context": "The canonical dataset is the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005), consisting of 4076 training and 1725 test pairs; 2/3 of the samples are labelled as 1 (is-a-paraphrase).", "startOffset": 66, "endOffset": 92}, {"referenceID": 13, "context": "State-of-art model uses an ensemble of handcrafted overlap features (Ji and Eisenstein, 2013) and weren\u2019t beaten by neural models (Cheng and Kartsaklis, 2015) (He et al.", "startOffset": 68, "endOffset": 93}, {"referenceID": 4, "context": "State-of-art model uses an ensemble of handcrafted overlap features (Ji and Eisenstein, 2013) and weren\u2019t beaten by neural models (Cheng and Kartsaklis, 2015) (He et al.", "startOffset": 130, "endOffset": 158}, {"referenceID": 10, "context": "State-of-art model uses an ensemble of handcrafted overlap features (Ji and Eisenstein, 2013) and weren\u2019t beaten by neural models (Cheng and Kartsaklis, 2015) (He et al., 2015) yet.", "startOffset": 159, "endOffset": 176}, {"referenceID": 2, "context": "In particular, this concerns the Recognizing Textual Entailment task (supported by the SNLI dataset) (Bowman et al., 2015) and the problem of memory selection in Memory Networks (supported by the baBi dataset) (Weston et al.", "startOffset": 101, "endOffset": 122}, {"referenceID": 31, "context": ", 2015) and the problem of memory selection in Memory Networks (supported by the baBi dataset) (Weston et al., 2015).", "startOffset": 95, "endOffset": 116}, {"referenceID": 19, "context": "A more realistic large paraphrasing dataset based on the AskUbuntu Stack Overflow forum had been recently proposed (Lei et al., 2015).", "startOffset": 115, "endOffset": 133}, {"referenceID": 33, "context": "(Yu et al., 2014) or (Weston et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 30, "context": ", 2014) or (Weston et al., 2014)) or the MLP module that takes elementwise product and sum of the embeddings and feeds them to a two-layer perceptron with hidden layer of width 2N (as in e.", "startOffset": 11, "endOffset": 32}, {"referenceID": 27, "context": "(Tai et al., 2015)).", "startOffset": 0, "endOffset": 18}, {"referenceID": 27, "context": "8 For the STS task, we follow this by score regression using class interpolation as in (Tai et al., 2015).", "startOffset": 87, "endOffset": 105}, {"referenceID": 3, "context": "When training for a ranking task (Answer Sentence Selection), we use the bipartite ranking version of Ranknet (Burges et al., 2005) as the objective; when training for STS task, we use Pearson\u2019s r formula as the objective; for binary classification tasks, we use the binary crossentropy objective.", "startOffset": 110, "endOffset": 131}, {"referenceID": 23, "context": "Weighed word overlaps metrics TF-IDF and BM25 (Robertson et al., 1995) are inspired by IR research and provide strong baselines for many tasks.", "startOffset": 46, "endOffset": 70}, {"referenceID": 33, "context": "in (Yu et al., 2014) or (Weston et al.", "startOffset": 3, "endOffset": 20}, {"referenceID": 30, "context": ", 2014) or (Weston et al., 2014), simply taking the mean vector of the word embedding sequence and training an U weight matrix N\u00d72N that projects both embeddings to the same vector space, Ei = tanh(U \u00b7 \u0113i), where the MLP scorer compares them.", "startOffset": 11, "endOffset": 32}, {"referenceID": 12, "context": "A simple extension of the above are the DAN Deep Averaging Networks (Iyyer et al., 2015), which were shown to adequately replace much more complex models in some tasks.", "startOffset": 68, "endOffset": 88}, {"referenceID": 28, "context": "RNN with memory units are popular models for processing sentenes (Tan et al., 2015) (Lowe et al.", "startOffset": 65, "endOffset": 83}, {"referenceID": 20, "context": ", 2015) (Lowe et al., 2015) (Bowman et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 2, "context": ", 2015) (Bowman et al., 2015).", "startOffset": 8, "endOffset": 29}, {"referenceID": 6, "context": "We use a bidirectional network with 2N GRU memory units9 (Cho et al., 2014) in each direction; the final unit states are summed across the two per-direction GRUs to yield a 2N vector representation of the sentence.", "startOffset": 57, "endOffset": 75}, {"referenceID": 33, "context": "CNN with sentence-wide pooling layer are also popular models for processing sentences (Yu et al., 2014) (Tan et al.", "startOffset": 86, "endOffset": 103}, {"referenceID": 28, "context": ", 2014) (Tan et al., 2015) (Severyn and Moschitti, 2015) (He et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 25, "context": ", 2015) (Severyn and Moschitti, 2015) (He et al.", "startOffset": 8, "endOffset": 37}, {"referenceID": 10, "context": ", 2015) (Severyn and Moschitti, 2015) (He et al., 2015) (Kadlec et al.", "startOffset": 38, "endOffset": 55}, {"referenceID": 14, "context": ", 2015) (Kadlec et al., 2015).", "startOffset": 8, "endOffset": 29}, {"referenceID": 15, "context": "We apply a multi-channel convolution (Kim, 2014) with single-token channel of N convolutions and 2, 3, 4 and 5-token channels of N/2 convolutions each, relu transfer function, max-pooling over the whole sentence, and as above a projection to shared space and an MLP scorer.", "startOffset": 37, "endOffset": 48}, {"referenceID": 28, "context": "Inspired by (Tan et al., 2015), the aim of this model is to allow the RNN to model long-term dependencies and model contextual representations of words, while taking advantage of the CNN and pooling operation for crisp selection of the gist of the sentence.", "startOffset": 12, "endOffset": 30}, {"referenceID": 11, "context": "The idea of attention models is to attend preferrentially to some parts of the sentence when building its representation (Hermann et al., 2015) (Tan et al.", "startOffset": 121, "endOffset": 143}, {"referenceID": 28, "context": ", 2015) (Tan et al., 2015) (dos Santos et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 24, "context": ", 2016) (Rockt\u00e4schel et al., 2015).", "startOffset": 8, "endOffset": 34}, {"referenceID": 28, "context": "There are many ways to model attention, as the initial proof of concept we adopt the (Tan et al., 2015) model attn1511 which is conceptually simple and easy to implement.", "startOffset": 85, "endOffset": 103}, {"referenceID": 7, "context": "To easily implement models, dataset loaders and task adapters in a modular fashion so that any model can be easily run on any f2-type task, we have created a new software package dataset-sts that integrates a variety of datasets, a Python dataset adapter PySTS and a Python library for easy construction of deep neural NLP models for semantic sentence pair scoring KeraSTS that uses the Keras machine learning library (Chollet, 2015).", "startOffset": 418, "endOffset": 433}, {"referenceID": 22, "context": "We use N = 300 dimensional GloVe embeddings matrix pretrained on Wikipedia 2014 + Gigaword 5 (Pennington et al., 2014) that we keep adaptable during training; words in the training set not included in the pretrained model are initialized by random vectors uniformly sampled from [\u22120.", "startOffset": 93, "endOffset": 118}, {"referenceID": 33, "context": "Word overlap is an important feature in many f2-type tasks (Yu et al., 2014) (Severyn and Moschitti, 2015), especially when the sentences may contain named entities, numeric or other data for which no embedding is available.", "startOffset": 59, "endOffset": 76}, {"referenceID": 25, "context": ", 2014) (Severyn and Moschitti, 2015), especially when the sentences may contain named entities, numeric or other data for which no embedding is available.", "startOffset": 8, "endOffset": 37}, {"referenceID": 16, "context": "We apply 10 L2 regularization and use Adam optimization with standard parameters (Kingma and Ba, 2014).", "startOffset": 81, "endOffset": 102}, {"referenceID": 13, "context": "(Ji and Eisenstein, 2013) 0.", "startOffset": 0, "endOffset": 25}, {"referenceID": 10, "context": "859 (He et al., 2015) 0.", "startOffset": 4, "endOffset": 21}, {"referenceID": 14, "context": "We can observe an effect analogous to what has been described in (Kadlec et al., 2015) \u2014 when the dataset is smaller, CNN models are preferrable, while larger dataset allows RNN models to capture the text comprehension task bet-", "startOffset": 65, "endOffset": 86}, {"referenceID": 14, "context": "We have tried simple averaging of predictions (as per (Kadlec et al., 2015)), but the benefit was small and inconsistent.", "startOffset": 54, "endOffset": 75}, {"referenceID": 28, "context": "(Tan et al., 2015) 0.", "startOffset": 0, "endOffset": 18}, {"referenceID": 28, "context": "5%) scored better than (Tan et al., 2015).", "startOffset": 23, "endOffset": 41}, {"referenceID": 24, "context": "for the RTE task (Rockt\u00e4schel et al., 2015), and the skip-thoughts method of sentence embedding.", "startOffset": 17, "endOffset": 43}, {"referenceID": 17, "context": "(Kiros et al., 2015)", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": ", 2016) (Cheng et al., 2016) (Kumar et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 18, "context": ", 2016) (Kumar et al., 2015) clearly points towards", "startOffset": 8, "endOffset": 28}, {"referenceID": 20, "context": "\u2217 Exact models from (Lowe et al., 2015) reran on the v2 version of the dataset (by personal communication with Ryan Lowe) \u2014 note that the results in (Lowe et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 20, "context": ", 2015) reran on the v2 version of the dataset (by personal communication with Ryan Lowe) \u2014 note that the results in (Lowe et al., 2015) and (Kadlec et al.", "startOffset": 117, "endOffset": 136}, {"referenceID": 14, "context": ", 2015) and (Kadlec et al., 2015) are on v1 and not directly comparable.", "startOffset": 12, "endOffset": 33}, {"referenceID": 27, "context": "841 (Tai et al., 2015) 0.", "startOffset": 4, "endOffset": 22}, {"referenceID": 9, "context": "Another promising approach might be giving the network more flexibility regarding the final representation, for example by allowing it to remember a set of \u201cfacts\u201d derived from each sentence; related work has been done on end-to-end differentiable shift-reduce parsers with LSTM as stack cells (Dyer et al., 2015).", "startOffset": 294, "endOffset": 313}], "year": 2017, "abstractText": "We review the task of Sentence Pair Scoring, popular in the literature in various forms \u2014 slanted as Answer Sentence Selection, Paraphrasing, Semantic Text Scoring, Next Utterance Ranking, Recognizing Textual Entailment or e.g. a component of Memory Networks. We argue that such tasks are similar from the model perspective (especially in the context of high-capacity deep neural models) and propose new baselines by comparing the performance of popular convolutional, recurrent and attentionbased neural models across many Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating randomized models, propose a statistically grounded methodology, and attempt to improve comparisons by releasing new datasets that are much harder than some of the currently used well explored benchmarks. To address the current research fragmentation in a future-proof way, we introduce a unified open source software framework with easily pluggable models, allowing easy evaluation on a wide range of semantic natural language tasks. This allows us to outline a path towards a universal machine learned semantic model for machine reading tasks. We support this plan by experiments that demonstrate reusability of models trained on different tasks, even across corpora of very different nature.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}