{"id": "1603.06129", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Automated Correction for Syntax Errors in Programming Assignments using Recurrent Neural Networks", "abstract": "We present a method for automatically generating repair feedback for syntax errors for introductory programming problems. Syntax errors constitute one of the largest classes of errors (34%) in our dataset of student submissions obtained from a MOOC course on edX. The previous techniques for generating automated feed- back on programming assignments have focused on functional correctness and style considerations of student programs. These techniques analyze the program AST of the program and then perform some dynamic and symbolic analyses to compute repair feedback. Unfortunately, it is not possible to generate ASTs for student pro- grams with syntax errors and therefore the previous feedback techniques are not applicable in repairing syntax errors.", "histories": [["v1", "Sat, 19 Mar 2016 18:43:28 GMT  (328kb,D)", "http://arxiv.org/abs/1603.06129v1", null]], "reviews": [], "SUBJECTS": "cs.PL cs.AI cs.LG cs.SE", "authors": ["sahil bhatia", "rishabh singh"], "accepted": false, "id": "1603.06129"}, "pdf": {"name": "1603.06129.pdf", "metadata": {"source": "CRF", "title": "Automated Correction for Syntax Errors in Programming Assignments using Recurrent Neural Networks", "authors": ["Sahil Bhatia", "Rishabh Singh"], "emails": ["sahilbhatia.nsit@gmail.com", "risin@microsoft.com", "permissions@acm.org."], "sections": [{"heading": null, "text": "We present a syntax error feedback technique that uses relapsing neural networks (RNNs) to model syntactically valid token sequences. Our approach is inspired by recent work on learning speech models from Big Code (Large Code Corpus). For a given programming task, we first learn an RNN to model all valid token sequences using the set of syntactically correct student submissions. We then quiz the learned RNN model with the prefix token sequence to predict token sequences that can correct the error by either replacing or inserting the predicted token sequence at the error point. We evaluate our technique based on over 14,000 student submissions with syntax errors. Our technique can fully repair 31.69% (4501 / 14203) of the submissions with syntax errors and additionally, 6.39% (908 / 143) of the submissions with partial corrections."}, {"heading": "1. INTRODUCTION", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2. MOTIVATING EXAMPLES", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "3. APPROACH", "text": "For a specific programming problem, we first use the set of all syntactically correct student submissions to train a neural network in the training phase to learn a token sequence model for all valid token sequences specific to the problem. Then, we use the SYNFIX algorithm to find small corrections to a submission of students with syntax errors based on the token sequences predicted from the learned model. These corrections are then used to provide feedback on possible syntax error corrections. We now describe the two key phases of our workflow: i) the training phase and ii) the SYNFIX algorithm."}, {"heading": "3.1 Neural Network Model", "text": "The simplest class of denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated dendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendenden"}, {"heading": "3.2 The SYNFIX Algorithm", "text": "The SYNFIX algorithms shown in the algorithm sequence T'k take as their input a program P (with syntax errors) and a token sequence model M ', and specify either a fixed program P' (if possible) or an error that the program cannot fix. First, the algorithm uses a parser to obtain the type of error and the token location where the error occurs, loc. \"We use notation a [i.. j] to name a sequence of a sequence of token sequences that starts with index i (included) and ends with index j (exclusive), and the algorithm then questions the modelM to predict the token sequence T'k of a constant length that is most likely to follow the prefix sequence."}, {"heading": "4. EVALUATION", "text": "The first question we are examining is whether it is possible to learn the RNN models for token sequences that can capture syntactically valid sequences. In how many cases can our system fix the syntax errors with the predicted sequences by using different algorithmic choices in the SYNFIX algorithm. Finally, we are also experimenting with different RNN and LSTM configurations and the vocabulary threshold to evaluate their impact on the final result."}, {"heading": "4.1 Benchmarks", "text": "Our benchmark set consists of student submissions on five programming problems: recurPower, iterPower, oddTuples, evalPoly, and compDeriv from the edX course. The recursive problem asks students to write a recursive function that takes a number base and an integer Exp as input and calculates the value baseexp. The iterPower problem has the same functional specification as the re-curPower problem, but asks students to write an iterative solution instead of a recursive solution. the oddTuples problem asks students to write a function that takes a tuple l as input and returns another tuple that consists of every other element of l. The evalualPoly problem asks students to write a function to calculate the value of a polynomial at a given point, with the coefficients of the polynomial polynomial well represented in a list of duplicates. Finally, the write back function of a polynomial student is required to write back the problem."}, {"heading": "4.2 Training Phase", "text": "During the training phase, we use all student submissions without syntax errors to learn the token sequence model for a particular problem. First, student submissions are aggregated into a series of tokens, which are then fed into the neural network to learn the token sequence model. Overall, the number of tokens obtained from students \"syntactically correct submissions for each problem in Table 2 also includes the initial vocabulary size (the amount of unique tokens in student submissions) and the training vocabulary size achieved by replacing all tokens whose frequency is below a threshold as IDENT. For our experiments, we use a threshold of t = 4. To train the recurring neural network, we used a learning rate of 0.002, a sequence length of 10, and a batch size of 50. We use the batch gradient descend method with rmsprop (decay rate of 0.97), each of which was weighted to 12GB and 12GB respectively."}, {"heading": "4.3 Number of Corrected Submissions", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "4.4 Different Neural Network Baselines", "text": "In this section, we compare different neural baseline networks to learn the token sequence models and their respective effectiveness in correcting syntax errors for the recursive problem. In particular, we look at 6 baselines: (i) RNN- (1128), (ii) RNN- (2128), (iii) LSTM- (1128), (iv) LSTM- (2128), (v) LSTM (1256) and (vii) LSTM- (2256), whereby the NN- (x, y) network denotes a neural network (RNN or LSTM) consisting of x number of hidden layers with one unit each. Results for the 6 baseline networks are presented in Table 5. There is not much difference in the performance of different neural networks (1128), with the RNN (1128) model representing the largest number of student groups with a hidden number of units each."}, {"heading": "4.5 Effect of Different Threshold values", "text": "The results for 3 different thresholds (t = 1, 4, 8) are shown in Table 6. When the threshold rises, a larger number of tokens is referred to as IDENT tokens, reducing the size of the training vocabulary. Without using a threshold (t = 1), the system corrects the least number of syntax errors for the recursive problem. There are several erroneous inputs that cannot be corrected in this case because the learned model performs poorly on prefix token sequences consisting of rarely used tokens (such as rare variable names). We can also observe that the threshold of 4 performs better than the threshold of 8. One hypothesis for this phenomenon is that the neural network overgeneralizes some tokens to IDENT and loses the specific token information needed to correct some syntax errors."}, {"heading": "5. RELATED WORK", "text": "This year it is more than ever before."}, {"heading": "6. LIMITATIONS AND FUTURE WORK", "text": "There are several reasons why we have to wait so long to be able to find a solution."}, {"heading": "7. CONCLUSION", "text": "In this paper, we introduced a technique for using Recurrent Neural Networks (RNNs) to learn token sequence models to repair syntax errors in student programs. For a programming task, our technique first uses the set of all syntactically correct student submissions to train an RNN to learn the token sequence model, and then uses the trained model for predicting token sequences to repair student submissions with syntax errors. Our technique is inspired by two emerging areas of research: 1) learning large-scale language models and 2) efficient learning techniques for recurrent neural networks. For our dataset of student experiments obtained from the edX platform, our technique can generate repairs for approximately 32% of submissions. We believe that this technique provides a basis for providing automated feedback on syntax errors to hundreds of thousands of students who may benefit from online introductory courses, Course X, and Course X."}], "references": [{"title": "Learning natural coding conventions", "author": ["M. Allamanis", "E.T. Barr", "C. Bird", "C.A. Sutton"], "venue": "In FSE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Suggesting accurate method and class names", "author": ["M. Allamanis", "E.T. Barr", "C. Bird", "C.A. Sutton"], "venue": "In FSE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Mining source code repositories at massive scale using language modeling", "author": ["M. Allamanis", "C.A. Sutton"], "venue": "In MSR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Mining idioms from source code", "author": ["M. Allamanis", "C.A. Sutton"], "venue": "In FSE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Powergrading: a clustering approach to amplify human effort for short answer grading", "author": ["S. Basu", "C. Jacobs", "L. Vanderwende"], "venue": "TACL, 1:391\u2013402,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Feedback generation for performance problems in introductory programming assignments", "author": ["S. Gulwani", "I. Radicek", "F. Zuleger"], "venue": "In FSE,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "On the naturalness of software", "author": ["A. Hindle", "E.T. Barr", "Z. Su", "M. Gabel", "P.T. Devanbu"], "venue": "In ICSE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Syntactic and functional variability of a million code submissions in a machine learning MOOC", "author": ["J. Huang", "C. Piech", "A. Nguyen", "L.J. Guibas"], "venue": "In AIED,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Peer and self assessment in massive online classes", "author": ["C.E. Kulkarni", "P.W. Wei", "H. Le", "D.J. hao Chia", "K. Papadopoulos", "J. Cheng", "D. Koller", "S.R. Klemmer"], "venue": "ACM Trans. Comput.-Hum. Interact.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Mathematical language processing: Automatic grading and feedback for open response mathematical questions", "author": ["A.S. Lan", "D. Vats", "A.E. Waters", "R.G. Baraniuk"], "venue": "In Learning@Scale,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Codewebs: scalable homework search for massive open online programming courses", "author": ["A. Nguyen", "C. Piech", "J. Huang", "L.J. Guibas"], "venue": "In WWW,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "A statistical semantic language model for source code", "author": ["T.T. Nguyen", "A.T. Nguyen", "H.A. Nguyen", "T.N. Nguyen"], "venue": "In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Learning program embeddings to propagate feedback on student code", "author": ["C. Piech", "J. Huang", "A. Nguyen", "M. Phulsuksombati", "M. Sahami", "L.J. Guibas"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Predicting program properties from \"big code", "author": ["V. Raychev", "M.T. Vechev", "A. Krause"], "venue": "In POPL,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1. chapter Learning Internal Representations by Error Propagation, pages 318\u2013362", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1986}, {"title": "Automated feedback generation for introductory programming assignments", "author": ["R. Singh", "S. Gulwani", "A. Solar-Lezama"], "venue": "In PLDI,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Combinatorial sketching for finite programs", "author": ["A. Solar-Lezama", "L. Tancau", "R. Bod\u00edk", "S.A. Seshia", "V.A. Saraswat"], "venue": "In ASPLOS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1990}], "referenceMentions": [{"referenceID": 10, "context": "These approaches can be categorized into two broad categories \u2013 peer-grading [11] and automated grading techniques [18, 13].", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "These approaches can be categorized into two broad categories \u2013 peer-grading [11] and automated grading techniques [18, 13].", "startOffset": 115, "endOffset": 123}, {"referenceID": 12, "context": "These approaches can be categorized into two broad categories \u2013 peer-grading [11] and automated grading techniques [18, 13].", "startOffset": 115, "endOffset": 123}, {"referenceID": 17, "context": "AutoProf [18] is a system for providing automated feedback on functional correctness of buggy student solutions.", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "The Codewebs system [13] is a search engine for coding assignments that allows for querying massive dataset of student submissions using \"code phrases\", which are subgraphs of AST in the form of subtrees, subforests, and contexts.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "We use a Recursive Neural Network (RNN) [17] to learn the token sequence model that can learn large contextual dependencies between tokens.", "startOffset": 40, "endOffset": 44}, {"referenceID": 7, "context": "Our approach is inspired from the recent pioneering work on learning probabilistic models of source code from a large repository of code for many different applications [8, 14, 3, 2, 1, 4, 16].", "startOffset": 169, "endOffset": 192}, {"referenceID": 13, "context": "Our approach is inspired from the recent pioneering work on learning probabilistic models of source code from a large repository of code for many different applications [8, 14, 3, 2, 1, 4, 16].", "startOffset": 169, "endOffset": 192}, {"referenceID": 2, "context": "Our approach is inspired from the recent pioneering work on learning probabilistic models of source code from a large repository of code for many different applications [8, 14, 3, 2, 1, 4, 16].", "startOffset": 169, "endOffset": 192}, {"referenceID": 1, "context": "Our approach is inspired from the recent pioneering work on learning probabilistic models of source code from a large repository of code for many different applications [8, 14, 3, 2, 1, 4, 16].", "startOffset": 169, "endOffset": 192}, {"referenceID": 0, "context": "Our approach is inspired from the recent pioneering work on learning probabilistic models of source code from a large repository of code for many different applications [8, 14, 3, 2, 1, 4, 16].", "startOffset": 169, "endOffset": 192}, {"referenceID": 3, "context": "Our approach is inspired from the recent pioneering work on learning probabilistic models of source code from a large repository of code for many different applications [8, 14, 3, 2, 1, 4, 16].", "startOffset": 169, "endOffset": 192}, {"referenceID": 15, "context": "Our approach is inspired from the recent pioneering work on learning probabilistic models of source code from a large repository of code for many different applications [8, 14, 3, 2, 1, 4, 16].", "startOffset": 169, "endOffset": 192}, {"referenceID": 7, "context": "[8] learn an n-gram language model to capture the repetitiveness present in a code corpora and show that n-gram models are effective at capturing the local regularities.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] enhanced this model for code auto-completion to also include semantic knowledge about the tokens (such as types) and the scope and dependencies amongst the tokens to consider global relationships amongst them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The NATURALIZE framework [1] learns an n-gram language model for learning coding conventions and suggesting changes to increase the stylistic consistency of the code.", "startOffset": 25, "endOffset": 28}, {"referenceID": 15, "context": "More recently, some other probabilistic models such as conditional random fields and log bilinear context models have been presented for suggesting names for variables, methods, and classes [16, 2].", "startOffset": 190, "endOffset": 197}, {"referenceID": 1, "context": "More recently, some other probabilistic models such as conditional random fields and log bilinear context models have been presented for suggesting names for variables, methods, and classes [16, 2].", "startOffset": 190, "endOffset": 197}, {"referenceID": 5, "context": "The simplest class of neural networks [6] (also called convolutional networks) are feedforward neural networks and were the first type of artificial neural network devised.", "startOffset": 38, "endOffset": 41}, {"referenceID": 19, "context": "During the training phase, RNN uses backpropagation through time(BPTT) [20] to calculate the gradient and adjust the weights.", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "Long Short Term Memory networks (LSTM): LSTMs [9] are a special kind of RNN that are capable of learning long-term dependencies and have been shown to outperform general RNNs for a variety of tasks.", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "[8] use an n-gram model to capture the regularity of local project-specific contexts in software.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] extended this syntactic n-gram language model to also include semantic token annotations that describe the token type and their semantic role (such as variable, function call etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] applied this technique of learning n-gram models on a much larger code corpus containing over 350 million lines of code, and showed that using a large corpus for training these n-gram model can significantly increase their predictive capabilities.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "NATURALIZE [1] is a language-independent framework that uses the n-gram language model to learn the coding convention and coding style from a project codebase, and suggests revisions to improve stylistic consistency.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] recently presented a technique for suggesting method and class names from its body and methods respectively using a neural probabilistic language model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "JSNice [16] is a scalable prediction engine for predicting identifier names and type annotation of variables in JavaScript programs.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "AutoProf [18] is a system for providing automated feedback on functional correctness of introductory programming assignments.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "AutoProf uses constraint-based synthesis techniques [19] for finding minimum number of changes (guided by an error model) in the incorrect student submission to make it functionally equivalent to the reference implementation.", "startOffset": 52, "endOffset": 56}, {"referenceID": 6, "context": "Another approach [7] based on dynamic program analysis was recently presented for providing feedback on performance problems.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "[10] present an approach to au-", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Codewebs [13] creates a queryable index that allows for fast searches of code phrases into a massive dataset of student submissions.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "The elements of the embedding matrix of a program are then used as code features for automatically propagating instructor feedback at scale [15].", "startOffset": 140, "endOffset": 144}, {"referenceID": 11, "context": "The Mathematical Language Processing (MLP) [12] framework leverages solutions from large number of learners to evaluate correctness of student solutions to open response Mathematical questions.", "startOffset": 43, "endOffset": 47}, {"referenceID": 4, "context": "[5] present an approach to train a similarity metric between short answer responses to United States Citizenship Exam, which is then used to group the responses into clusters and subclusters for powergrading.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "We present a method for automatically generating repair feedback for syntax errors for introductory programming problems. Syntax errors constitute one of the largest classes of errors (34%) in our dataset of student submissions obtained from a MOOC course on edX. The previous techniques for generating automated feedback on programming assignments have focused on functional correctness and style considerations of student programs. These techniques analyze the program AST of the program and then perform some dynamic and symbolic analyses to compute repair feedback. Unfortunately, it is not possible to generate ASTs for student programs with syntax errors and therefore the previous feedback techniques are not applicable in repairing syntax errors. We present a technique for providing feedback on syntax errors that uses Recurrent neural networks (RNNs) to model syntactically valid token sequences. Our approach is inspired from the recent work on learning language models from Big Code (large code corpus). For a given programming assignment, we first learn an RNN to model all valid token sequences using the set of syntactically correct student submissions. Then, for a student submission with syntax errors, we query the learnt RNN model with the prefix token sequence to predict token sequences that can fix the error by either replacing or inserting the predicted token sequence at the error location. We evaluate our technique on over 14, 000 student submissions with syntax errors. Our technique can completely repair 31.69% (4501/14203) of submissions with syntax errors and in addition partially correct 6.39% (908/14203) of the submissions.", "creator": "TeX"}}}