{"id": "1603.06212", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2016", "title": "Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science", "abstract": "As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning---pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.", "histories": [["v1", "Sun, 20 Mar 2016 13:32:27 GMT  (379kb,D)", "http://arxiv.org/abs/1603.06212v1", "8 pages, 5 figures, preprint to appear in GECCO 2016, edits not yet made from reviewer comments"]], "COMMENTS": "8 pages, 5 figures, preprint to appear in GECCO 2016, edits not yet made from reviewer comments", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["randal s olson", "nathan bartley", "ryan j urbanowicz", "jason h moore"], "accepted": false, "id": "1603.06212"}, "pdf": {"name": "1603.06212.pdf", "metadata": {"source": "CRF", "title": "Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science", "authors": ["Randal S. Olson", "Nathan Bartley", "Ryan J. Urbanowicz", "Jason H. Moore"], "emails": ["olsonran@upenn.edu", "bartleyn@uchicago.edu", "ryanurb@upenn.edu", "jhmoore@upenn.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS concepts \u2022 computer methods \u2022 supervised learning by classification; genetic programming; machine learning; \u2022 software and its engineering \u2192 search-based software engineering; keywords pipeline optimization, hyperparameter optimization, data science, machine learning, genetic programming, pareto optimization, python"}, {"heading": "1. INTRODUCTION", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2. RELATED WORK", "text": "Historically, machine learning automation research has focused primarily on optimizing subsets of the pipeline [10]. For example, grid search is the most commonly used form of hyperparameter optimization that uses brute force search to explore a wide range of model parameters to find the parameter set that allows for the best fit of the model. Recent research has shown that random evaluation of model hyperparameters within the grid search often makes the most ideal parameter settings more efficient than exhaustive search [2], showing promising results for intelligent search in the hyperparameter space. Bayean's optimization of model hyperparameters has been effective in this area and has even surpassed manual hyperparameter setting by experienced practitioners. Another focus of machine learning automation research has been design. A recent example of automated machine working construction is the Data Machine Automated Science Machine. \""}, {"heading": "3. METHODS", "text": "In this section, we describe tree-based pipeline optimization in detail, including the tools and concepts underlying the Tree-based Pipeline Optimization Tool (TPOT). We begin this section with a listing of the basic pipeline operators currently implemented in TPOT. Next, we describe how operators are merged into a tree-based pipeline and then illustrate how tree-based pipelines can be developed through genetic programming. Finally, we end this section by giving an overview of the data sets we use to evaluate TPOT."}, {"heading": "3.1 Pipeline Operators", "text": "Here we list the four main types of pipeline operators currently implemented in TPOT. All pipeline operators use existing implementations in scikit-learn [14]. To take a closer look at these operators, refer to the online documentation and [8].Pre-processors. We have implemented a standardized scaling operator that uses the sample mean and variance to scale the features (StandardScaler), a robust scaling operator that uses the sample mean and variance of the features (Polynomial Features).Decomposition. We have implemented RandomizedPCA, a variant of Principal Component Analysis that uses randomized SVD. Feature Selection. We have implemented a recursive feature elimination strategy (RFE), a strategy that selects the top-k features (SelectKBest), a strategy that selects the top-perceptibility."}, {"heading": "3.2 Assembling Tree-based Pipelines", "text": "In order to combine all of these operators into a flexible pipeline structure, we have implemented the pipelines as trees, as shown in Figure 2, with the different operators being nodes in the tree. Each tree-based pipeline begins with one or more copies of the input data set as leaves of the tree, which are then fed into one of the four classes of pipeline operators: pre-processing, decomposition, feature selection or modeling. When the data passes the tree, they are modified by the operator of that node. If there are multiple copies of the data set being processed, it is possible to combine them into a single record using a record combinator. Each time a record is passed through a modeler, the resulting classifications are stored in such a way that the most recent classifier to process the data overwrites previous predictions, and the predictions of the earlier classifier are stored as a new feature, for example, once the data set is fully processed by the forest (once the data set is processed by the final pipeline)."}, {"heading": "3.3 Evolving Tree-based Pipelines", "text": "To automatically generate and optimize these pipelines, we use a well-known evolutionary computation technology called genetic programming (GP), as implemented in the Python package DEAP [6]. Traditionally, GP builds trees of mathematical functions to optimize them toward certain criteria. In TPOT, we use GP to develop the sequence of pipeline operators as well as each operator's parameters (e.g., the number of trees in a random forest or the number of characteristics to be selected in the selection of characteristics) to maximize the classification accuracy of the pipeline. We follow a standard GP procedure with the settings described in Table 1, where changes to the pipeline can modify, remove or insert new sequences of pipeline-based operators into the tree-based pipeline. In this paper, TPOT pipelines are evaluated based on their classification accuracy on the TPOT-3 test set to optimize the drive pipeline."}, {"heading": "3.4 GAMETES Simulated Data Sets", "text": "To evaluate TPOT, we use a diverse, complex simulation study design. We generate a total of 12 genetic models and 360 associated datasets using GAMETES [19], an open source software package for generating a diverse spectrum of pure, rigorous epistatic genetic models. GAMETES generates random, biallelic n-locus models with \"pure\" epistasis, in which all n loci, but no less, predict disease status. We generate these genetic models precisely with specific inheritance abilities, SNP allelic frequencies, and population prevalences. In this paper, all datasets contain 100 SNP attributes: 8 SNPs predicting a binary case / control endpoint, and 92 SNPs randomly generated, using an allergy frequency between 0.05 and 0.5. The 8 predictive SNPs are combined as four separate purely epistatic models, each of which we combine with anew-added characteristic data using the \"ES\" in each case."}, {"heading": "3.5 UCI Benchmark Data Sets", "text": "To further demonstrate the performance of the TPOT, we evaluate it using 9 hand-picked benchmark data from the well-known UC-Irvine Machine Learning Repository [12]. The purpose of these benchmarks is to demonstrate the performance of the TPOT across a wide range of application areas and dataset types, as the TPOT is intended to be a universal, monitored machine learning tool. For more information on these datasets, see their documentation under [12]. Benchmark datasets are: \u2022 Hill-Valley: Two simulated datasets with and without noise. Each dataset represents 100 points on a two-dimensional chart, with the algorithm having to classify the series either as Hill (a \"bump\" in terrain) or Valley (an \"immersion\" in terrain). \u2022 Breast Cancer Wisdoms: datasets with continuous measurements of tumors. \u2022 The algorithm has to classify the tumor as benign or malignant to classify the series as either Hill (a \"bump\" in terrain \") or Valley (an\" immersion \"in terrain)."}, {"heading": "4. RESULTS", "text": "This year it is more than ever before in the history of the city."}, {"heading": "5. DISCUSSION", "text": "In this paper, we have shown that in many cases, the automatic learning of pipelines can lead to a significant improvement in basic machine learning analysis, while there is little to no input. However, it is important to note that the goal of automated pipelines is not to replace the user. From there, the user is free to export the pipelines and integrate their domain knowledge as they see it. To help in this goal, we have discovered new features in the data, and recommend pipelines to the user."}, {"heading": "6. CONCLUSIONS", "text": "Tree-based pipeline optimization is a new technique that holds promise for 1) making machine learning tools more accessible to the layman, and 2) saving practitioners significant amounts of time by automating the most tedious parts of machine learning. In this paper, we demonstrated that TPOT achieves a similar level of performance to basic analysis of machine learning across a wide range of data sets without user input or prior knowledge. In addition, in several cases, TPOT was able to automatically find combinations of pre-processing and modeling operators that significantly exceeded basic analysis of machine learning. Finally, by integrating Pareto optimization into TPOT, we demonstrated that TPOT can design compact, easy-to-interpret pipelines without sacrificing classification accuracy."}, {"heading": "7. REFERENCES", "text": "[1] W. Banzhaf, P. Nordin, R. E. Keller and F. D.Francone. Genetic Programming: An Introduction. Morgan Kaufmann, San Meateo, CA, 1998. [2] J. Bergstra and Y. Bengio. Random Search for Hyper-Parameter Optimization. Journal of Machine Learning Research, 13: 281-305, 2012. [3] K. Deb et al. A fast and elitist multijective genetic algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation, 6: 182-197, 2002. [4] M. Feurer et al. Efficient and robust automated machine learning. In Advances in Neural Information Processing Systems 28, pp. 2944-2952. Curran Associates, Inc., 2015. [5] S. Forrest et al. A genetic programming approach to automated software repair."}], "references": [{"title": "Genetic Programming: An Introduction", "author": ["W. Banzhaf", "P. Nordin", "R.E. Keller", "F.D. Francone"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Random Search for Hyper-Parameter Optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A fast and elitist multiobjective genetic algorithm: NSGA-II", "author": ["K. Deb"], "venue": "IEEE Transactions on Evolutionary Computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Efficient and robust automated machine learning", "author": ["M. Feurer"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "A genetic programming approach to automated software repair", "author": ["S. Forrest"], "venue": "In Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "DEAP: Evolutionary Algorithms Made Easy", "author": ["F.-A. Fortin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Exploring automated software composition with genetic programming", "author": ["E.M. Fredericks", "B.H. Cheng"], "venue": "In Proceedings of the 15th Annual Conference Companion on Genetic and Evolutionary Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T.J. Hastie", "R.J. Tibshirani", "J.H. Friedman"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Computer-automated evolution of an X-band antenna for NASA\u2019s Space Technology 5 mission", "author": ["G.S. Hornby"], "venue": "Evolutionary Computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Beyond Manual Tuning of Hyperparameters", "author": ["F. Hutter", "J. L\u00fccke", "L. Schmidt-Thieme"], "venue": "KI - Ku\u0308nstliche Intelligenz,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Deep Feature Synthesis: Towards Automating Data Science Endeavors", "author": ["J.M. Kanter", "K. Veeramachaneni"], "venue": "In Proceedings of the International Conference on Data Science and Advance Analytics", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Automating biomedical data science through tree-based pipeline optimization", "author": ["R.S. Olson"], "venue": "In Proceedings of the 18th European Conference on the Applications of Evolutionary and Bio-inspired Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Practical Bayesian Optimization of Machine Learning Algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Genetic programming for finite algebras", "author": ["L. Spector"], "venue": "In Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "GAMETES: a fast, direct algorithm for generating pure, strict, epistatic models with random architectures", "author": ["R.J. Urbanowicz"], "venue": "BioData Mining,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Predicting the difficulty of pure, strict, epistatic models: metrics for simulated model selection", "author": ["R.J. Urbanowicz"], "venue": "BioData Mining,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "From space antenna design [9] to software development [7] and debugging [5] to the study of finite algebras [17], evolutionary algorithms have outperformed humans in a variety of domains that were previously considered exclusive to humans.", "startOffset": 26, "endOffset": 29}, {"referenceID": 6, "context": "From space antenna design [9] to software development [7] and debugging [5] to the study of finite algebras [17], evolutionary algorithms have outperformed humans in a variety of domains that were previously considered exclusive to humans.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "From space antenna design [9] to software development [7] and debugging [5] to the study of finite algebras [17], evolutionary algorithms have outperformed humans in a variety of domains that were previously considered exclusive to humans.", "startOffset": 72, "endOffset": 75}, {"referenceID": 14, "context": "From space antenna design [9] to software development [7] and debugging [5] to the study of finite algebras [17], evolutionary algorithms have outperformed humans in a variety of domains that were previously considered exclusive to humans.", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "In this paper, we report on the most recent development of an evolutionary algorithm called the Tree-based Pipeline Optimization Tool (TPOT) that automatically designs and optimizes machine learning pipelines [13].", "startOffset": 209, "endOffset": 213}, {"referenceID": 0, "context": "TPOT uses a version of genetic programming [1] to automatically design and optimize a series of data transformations and machine learning models that maximize the classification accuracy for a given supervised learning data set.", "startOffset": 43, "endOffset": 46}, {"referenceID": 9, "context": "Historically, machine learning automation research has primarily focused on optimizing subsets of the pipeline [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 1, "context": "Recent research has shown that randomly evaluating parameter sets within the grid search often discovers the ideal parameter set more efficiently than exhaustive search [2], which shows promise for intelligent search in the hyperparameter space.", "startOffset": 169, "endOffset": 172}, {"referenceID": 13, "context": "Bayesian optimization of model hyperparameters, in particular, has been effective in this realm and has even outperformed manual hyperparameter tuning by expert practitioners [16].", "startOffset": 175, "endOffset": 179}, {"referenceID": 10, "context": "One recent example of automated feature construction is the \u201cData Science Machine,\u201d which automatically constructs features from relational databases via deep feature synthesis [11].", "startOffset": 177, "endOffset": 181}, {"referenceID": 3, "context": "developed a machine learning pipeline automation system called auto-sklearn [4], which uses Bayesian optimization to discover the ideal combination of feature preprocessors, models, and model hyperparameters to maximize classification accuracy.", "startOffset": 76, "endOffset": 79}, {"referenceID": 12, "context": "All pipeline operators make use of existing implementations in scikit-learn [14].", "startOffset": 76, "endOffset": 80}, {"referenceID": 7, "context": "For further reading on these operators, refer to the scikit-learn online documentation and [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "To automatically generate and optimize these tree-based pipelines, we use a well-known evolutionary computation technique called genetic programming (GP) as implemented in the Python package DEAP [6].", "startOffset": 196, "endOffset": 199}, {"referenceID": 2, "context": "Since there is not a globally optimal solution that maximally optimizes both criteria, we maintain a Pareto front in TPOT-Pareto and select the pipelines for reproduction according to the NSGA-II selection strategy [3] Through consecutive generations of evolution, TPOT\u2019s GP algorithm will tinker with the pipelines\u2014adding new pipeline operators that improve fitness and removing redundant or detrimental operators\u2014in an intelligent, guided search for high-performing pipelines.", "startOffset": 215, "endOffset": 218}, {"referenceID": 16, "context": "We generate a total of 12 genetic models and 360 associated data sets using GAMETES [19], an open source software package designed to generate a diverse spectrum of pure, strict epistatic genetic models.", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "2 in GAMETES and select the model with median difficulty from all those generated [18].", "startOffset": 82, "endOffset": 86}, {"referenceID": 3, "context": "In the near future, we plan to explore the use of auto-sklearn [4] and other heuristics to seed the TPOT population with promising pipelines, which can \u201ckick start\u201d the TPOT population.", "startOffset": 63, "endOffset": 66}], "year": 2016, "abstractText": "As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning\u2014 pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.", "creator": "LaTeX with hyperref package"}}}