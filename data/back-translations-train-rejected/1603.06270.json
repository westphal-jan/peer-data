{"id": "1603.06270", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2016", "title": "Multi-Task Cross-Lingual Sequence Tagging from Scratch", "abstract": "We present a deep hierarchical recurrent neural network for sequence tagging. Given a sequence of words, our model employs deep gated recurrent units on both character and word levels to encode morphology and context information, and applies a conditional random field layer to predict the tags. Our model is task independent, language independent, and feature engineering free. We further extend our model to multi-task and cross-lingual joint training by sharing the architecture and parameters. Our model achieves state-of-the-art results in multiple languages on several benchmark tasks including POS tagging, chunking, and NER. We also demonstrate that multi-task and cross-lingual joint training can improve the performance in various cases.", "histories": [["v1", "Sun, 20 Mar 2016 21:15:56 GMT  (470kb,D)", "http://arxiv.org/abs/1603.06270v1", null], ["v2", "Tue, 9 Aug 2016 15:07:39 GMT  (470kb,D)", "http://arxiv.org/abs/1603.06270v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zhilin yang", "ruslan salakhutdinov", "william cohen"], "accepted": false, "id": "1603.06270"}, "pdf": {"name": "1603.06270.pdf", "metadata": {"source": "CRF", "title": "Multi-Task Cross-Lingual Sequence Tagging from Scratch", "authors": ["Zhilin Yang", "Ruslan Salakhutdinov", "William Cohen"], "emails": ["zhiliny@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "wcohen@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people are able to determine for themselves what they want and what they do not want. (...) Most people in the world are not able to decide. (...) Most people in the world are not able to decide. (...) Most people in the world are not able to assert their interests. (...) Most people in the world are not able to represent their interests. (...) Most people in the world are not able to represent their interests. (...) People in the world. (...) People in the world are not able to represent their interests. (...) The world is not in the world. (...) The world. (...) The world. (...) The world. (...) The world. (...) The world. (...) The world. (...) The world. (...) The world. (...) The world. (...) The world is in the world. (...)"}, {"heading": "2 Related Work", "text": "Ando and Zhang (2005) proposed a common training framework for multiple tasks, sharing structural parameters among multiple tasks, and improving performance on various tasks, including NER. collobert et al. (2011) presented a task-independent Convolutionary Network and engaged in multilingual joint training to improve the performance of chunking. However, there is still a gap between these multi-task approaches and the state of the art on individual tasks. Furthermore, it is unclear whether these approaches can be effective in a multilingual environment. Multilingual resources have been widely used for lingual sequence tagging by various means, such as lingual feature extraction (Darwish, 2013), text categorization (Virga and Khudanpur, 2003), and Bayesian parallel data prediction (Snyder et al., 2008). Parallel companies and word alignments are also used to train transverse word representations."}, {"heading": "3 Model", "text": "In this section, we present our model of sequence marking, which is based on deep hierarchical gated recurrent units and conditional random fields. Our competing networks are hierarchically structured, as we have multiple levels at both the word and character levels in a hierarchy."}, {"heading": "3.1 Gated Recurrent Unit", "text": "A gated recurrent unit (GRU) network is a type of recurrent neural network first introduced for machine translation (Cho et al., 2014). A recursive network can be represented as a sequence of units corresponding to the input sequence (x1, x2, \u00b7 \u00b7, xT), which can be either a word sequence in a sentence or a character sequence in a word. The unit at position t takes xt and resets the previous hidden state ht \u2212 1 as input and returns the current hidden state ht. Model parameters are divided between different units in the sequence. A gated recurrent unit at position t has two gates, an updated gate zt and a reset gate rt. Specifically, each gated recurrent unit can be expressed as a sequence wsrt = solution (Wrxht + Wrhht \u2212 1), which refers to the sequence."}, {"heading": "3.2 Hierarchical GRU", "text": "Our model uses a hierarchical GRU that encodes sequential information at both word and character levels. Input of our model is a sequence of words (x1, x2, \u00b7 \u00b7 \u00b7, xT) of length T, where xt is an embedding of the t-th word. The word at each position t also has a representation at character level that is called a sequence of length St (ct, 1, ct, 2, \u00b7 \u00b7, ct, St), where ct, s is the embedding of the s-th word."}, {"heading": "3.2.1 Character-Level GRU", "text": "Suppose the character level GRU has Lc layers, then we get hidden states \u2190 \u2212 h Lc, s and \u2212 \u2192 h Lc, s at each position s in the string. Since recurring networks usually tend to memorize shorter-term patterns, we link the first hidden state of the backward GRU and the last hidden state of the forward GRU to encode character-level morphology in both prefixes and suffixes. Furthermore, we link character-level representation to the one-of-K word embedding xt to form the final representation hwt for the t-th word. Specifically, we have hwt = [\u2212 \u2192 h Lc, St, \u2190 \u2212 h Lc, 1, xt] where hwt is a representation of the t-th word that represents both the character morphological level and Figure 1."}, {"heading": "3.2.2 Word-Level GRU", "text": "The GRU character level outputs a sequence of word representations hw = (hw1, h w 2, \u00b7 \u00b7, hwT). Word level GRU uses a deep bidirectional GRU with Lw layers over these word representations. Word level GRU takes the sequence hw as input and computes a sequence of hidden states h = (h1, h2, \u00b7 \u00b7 \u00b7, hT).Unlike the GRU character level, the GRU word level aims to extract the context information in the word order, e.g. N-gram patterns and neighboring word dependencies. Such information is usually encoded with handmade characteristics. However, as we show in our test results, the GRU word level can learn the relevant information without being language-specific or task-specific."}, {"heading": "3.3 Conditional Random Field", "text": "To model the dependencies between the tags in a sequence, we apply a conditional random field (Lafferty et al., 2001). However, the conditional log probability of a tag sequence y can be written by the word-level GRU (Huang et al., 2015) due to the hidden state sequence h. Let Y (h) leave the space of the tag sequences for h. The conditional log probability of a tag sequence y, given the hidden state sequence h, can be written as a logbook p (y | h) = f (h, y) \u2212 logbook y \u2212 Exp f (h, y \u2032), (1) where f is a function that defines the function f (h, y) for each pair of h and y. To define the function f (y), we can multiply the function f (y), for each position t, we multiply the hidden state hwt with a parameter vector wyt, which is yt by the tag index."}, {"heading": "3.4 Training", "text": "We use Mini-Batch-AdaGrad (Duchi et al., 2011) to train our neural network in an end-to-end way with back-propagation, fine-tuning both character embedding and word embedding during training, using dynamic programming to calculate the normalizer of the CRF layer in equivalent (2), and re-using dynamic programming in the CRF layer to decipher the most likely tag sequence."}, {"heading": "4 Multi-Task and Cross-Lingual Joint Training", "text": "In this section we will examine the common training of multiple tasks and multiple languages. On the one hand, different sequence tagging tasks in the same language share language-specific regularities. Therefore, it is desirable to use multiple and multiple tasks to increase model performance. As our model is generally applicable to different tasks in different languages, it can of course be extended to multiple and multiple tasks. The basic idea is to divide part of the architecture and parameters between tasks and languages, and to jointly capture multiple target functions in relation to different tasks in different languages. We will discuss the details of our common training algorithms in multiple tasks and multilinguality of common training. The basic parameters are divided between the tasks and languages."}, {"heading": "5 Experiments", "text": "In this section, we will use multiple benchmark records for multiple tasks in multiple languages to evaluate our model and the common training algorithm."}, {"heading": "5.1 Datasets and Settings", "text": "We use the following benchmark datasets in our experiments: Penn Treebank (PTB) POS tagging, CoNLL 2000 chunking, CoNLL 2003 English NER, CoNLL 2002 Dutch NER and CoNLL 2002 Spanish NER. Statistics of the datasets are in Table 1. We construct the POS tagging dataset using the instructions described in Toutanova et al. (2003). Note that the POS tags are extracted from the trees analyzed by default. For the task of CoNLL 2003 English NER, we follow previous work (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015) to add a hot Gazetteer characteristic to the input of the CRF layer in order to achieve fair comparisons. 1We set the hidden state measurements to 300 for the word level GRU. We set the number of GRU layers to Gc = 2 for the GR = 2 (for the English layers) and one for the GR = 2 (for the English layers)."}, {"heading": "5.2 Pre-Trained Word Embeddings", "text": "Because the training corpus for a sequence tagging task is relatively small, it is difficult to run ran-1Although Gazetteers are probably a kind of feature engineering, we note that unlike most feature engineering techniques, they are easy to incorporate into a model. We only use the Gazetteer file provided by the CoNLL 2003 Shared Task, and do not use Gazetteers for other tasks or languages described here. To accurately capture word semantics, we initialize word embeddings. Therefore, we use word embeddings that have been pre-trained in large companies. All pre-trained embeddings we use are publicly available. On the English datasets, we use the 50-dimensional SENNA embeddings that are trained on Wikipedia based on neural networks (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015), after previous work based on neural networks (Collobert et al., 2015; Chiu and Nichols, 2015)."}, {"heading": "5.3 Performance", "text": "In this section, we report on the results of our model on the benchmark data sets and compare it with the aforementioned results on an equal footing. 2http: / / ronan.collobert.com / senna / 3https: / / sites.google.com / site / rmyeid / projects / polyglot 4We note that this number is inadvertently called 95.23, which is actually the score based on NP chunking, instead of CoNLL 2000.For English NER, there are two evaluation methods used in the literature. Some models are trained both with training and development, while others are only trained with training. We report our results in both cases. In the first case, we adjust the hyperparameters and test for development."}, {"heading": "5.4 Joint Training", "text": "In this section, we analyze the effectiveness of multiple and multilingual joint training in detail. To explore possible performance gains of joint training for low-resource languages or tasks, we consider joint training of different task pairs and language pairs, in which different subsets of the actually labeled corpora are provided. Faced with one language task pair, we jointly train one task with complete labels and the other with partial labels. Specifically, we perform a labeling rate r and try out a fraction of the sentences of the actually labeled corpora and discard the rest. The evaluation is based on the partially labeled task. The results will be shown in Table 8. We note that the performance of a specific task with relatively lower labeling rates (0.1 and 0.3) can usually benefit from other tasks with full labels through multiple or multilingual joint training. The performance gain can be up to 1.99 points if the labeling rate of the target task is 0.1. The improvement with an average of 0.1-37 labeling rate in English can be lower than the average of 0.3."}, {"heading": "6 Conclusion", "text": "We presented a new model of sequence marking based on gated recurrent units and conditional random fields. We investigated multitasking and crossslingual joint training by sharing part of the network architecture and model parameters. We achieved state-of-the-art results on various tasks, including POS tagging, chunking and NER, in multiple languages. We also demonstrated that joint training can improve model performance in different cases. In this work, we mainly focus on the use of morphological similarities for cross-lingual joint training. In the future, a major problem will be joint training based on cross-lingual word semantics using parallel data. Furthermore, it will be interesting to apply our common training approach to resource-poor tasks and languages."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": null, "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang"], "venue": null, "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Named entity extraction using adaboost", "author": ["Lluis Marquez", "Llu\u0131\u0301s Padr\u00f3"], "venue": "In CoNLL,", "citeRegEx": "Carreras et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2002}, {"title": "Named entity recognition: a maximum entropy approach using global information", "author": ["Chieu", "Ng2002] Hai Leong Chieu", "Hwee Tou Ng"], "venue": "In COLING,", "citeRegEx": "Chieu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chieu et al\\.", "year": 2002}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Chiu", "Nichols2015] Jason PC Chiu", "Eric Nichols"], "venue": "arXiv preprint arXiv:1511.08308", "citeRegEx": "Chiu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Named entity recognition using cross-lingual resources: Arabic as an example", "author": ["Kareem Darwish"], "venue": "In ACL,", "citeRegEx": "Darwish.,? \\Q2013\\E", "shortCiteRegEx": "Darwish.", "year": 2013}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Victor Guimaraes", "RJ Niter\u00f3i", "Rio de Janeiro"], "venue": "In Proceedings of NEWS 2015 The Fifth Named Entities Workshop,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "In HLT-NAACL,", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Multilingual language processing from bytes", "author": ["Gillick et al.2015] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": "arXiv preprint arXiv:1512.00103", "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Softmax-margin crfs: Training loglinear models with cost functions", "author": ["Gimpel", "Smith2010] Kevin Gimpel", "Noah A Smith"], "venue": "In NAACL,", "citeRegEx": "Gimpel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2010}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2014] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": null, "citeRegEx": "Gouws et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2014}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "A multiplicative model for learning distributed text-based attribute representations", "author": ["Kiros et al.2014] Ryan Kiros", "Richard Zemel", "Ruslan R Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Chunking with support vector machines", "author": ["Kudo", "Matsumoto2001] Taku Kudo", "Yuji Matsumoto"], "venue": "In NAACL,", "citeRegEx": "Kudo et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kudo et al\\.", "year": 2001}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando CN Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "arXiv preprint arXiv:1603.01360", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Phrase clustering for discriminative learning", "author": ["Lin", "Wu2009] Dekang Lin", "Xiaoyun Wu"], "venue": "In ACL,", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Joint named entity recognition and disambiguation", "author": ["Luo et al.2015] Gang Luo", "Xiaojiang Huang", "ChinYew Lin", "Zaiqing Nie"], "venue": null, "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Learning multilingual named entity recognition from wikipedia", "author": ["Nothman et al.2013] Joel Nothman", "Nicky Ringland", "Will Radford", "Tara Murphy", "James R Curran"], "venue": "Artificial Intelligence,", "citeRegEx": "Nothman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nothman et al\\.", "year": 2013}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": null, "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In CoNLL,", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Voting between multiple data representations for text", "author": ["Shen", "Sarkar2005] Hong Shen", "Anoop Sarkar"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2005}, {"title": "Guided learning for bidirectional sequence classification", "author": ["Shen et al.2007] Libin Shen", "Giorgio Satta", "Aravind Joshi"], "venue": "In ACL,", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Unsupervised multilingual learning for pos tagging", "author": ["Tahira Naseem", "Jacob Eisenstein", "Regina Barzilay"], "venue": "In EMNLP,", "citeRegEx": "Snyder et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snyder et al\\.", "year": 2008}, {"title": "Semisupervised condensed nearest neighbor for part-of-speech tagging", "author": ["Anders S\u00f8gaard"], "venue": "In ACL,", "citeRegEx": "S\u00f8gaard.,? \\Q2011\\E", "shortCiteRegEx": "S\u00f8gaard.", "year": 2011}, {"title": "Modeling latent-dynamic in shallow parsing: a latent conditional model with improved inference", "author": ["Sun et al.2008] Xu Sun", "Louis-Philippe Morency", "Daisuke Okanohara", "Jun\u2019ichi Tsujii"], "venue": "In COLING,", "citeRegEx": "Sun et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2008}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In NAACL,", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Transliteration of proper names in cross-lingual information retrieval", "author": ["Virga", "Khudanpur2003] Paola Virga", "Sanjeev Khudanpur"], "venue": "In Proceedings of the ACL 2003 workshop on Multilingual and mixed-language named entity recognition-Volume", "citeRegEx": "Virga et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Virga et al\\.", "year": 2003}, {"title": "Learning bilingual sentiment word embeddings for cross-language sentiment classification", "author": ["Zhou et al.2015] Huiwei Zhou", "Long Chen", "Fulin Shi", "Degen Huang"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Recently progress has been made on neural sequence-tagging models which make only minimal assumptions about the language, task, and feature set (Collobert et al., 2011)", "startOffset": 144, "endOffset": 168}, {"referenceID": 21, "context": ", 2003), chunking (Shen and Sarkar, 2005), or NER (Luo et al., 2015; Passos et al., 2014).", "startOffset": 50, "endOffset": 89}, {"referenceID": 23, "context": ", 2003), chunking (Shen and Sarkar, 2005), or NER (Luo et al., 2015; Passos et al., 2014).", "startOffset": 50, "endOffset": 89}, {"referenceID": 14, "context": "In other work, some recent neural approaches have been proposed to address multiple sequence tagging problems in a unified framework (Huang et al., 2015).", "startOffset": 133, "endOffset": 153}, {"referenceID": 6, "context": "Though gains have been shown using multi-task joint training, the prior models that benefit from multi-task joint training did not achieve state-ofthe-art performance (Collobert et al., 2011); thus the question of whether joint training can improve over strong baseline methods is still unresolved.", "startOffset": 167, "endOffset": 191}, {"referenceID": 15, "context": "Cross-lingual joint training typically uses word alignments or parallel corpora to improve the performance on different languages (Kiros et al., 2014; Gouws et al., 2014).", "startOffset": 130, "endOffset": 170}, {"referenceID": 13, "context": "Cross-lingual joint training typically uses word alignments or parallel corpora to improve the performance on different languages (Kiros et al., 2014; Gouws et al., 2014).", "startOffset": 130, "endOffset": 170}, {"referenceID": 18, "context": "Some multilingual taggers that do not rely on feature engineering have also been presented (Lample et al., 2016; dos Santos et al., 2015), but while these methods are language-independent, they do not study the effect of cross-lingual joint training.", "startOffset": 91, "endOffset": 137}, {"referenceID": 6, "context": "Collobert et al. (2011) presented a task independent convolutional network and employed multitask joint training to improve the performance of chunking.", "startOffset": 0, "endOffset": 24}, {"referenceID": 7, "context": "Multilingual resources were extensively used for cross-lingual sequence tagging through various ways, such as cross-lingual feature extraction (Darwish, 2013), text categorization (Virga and Khudanpur, 2003), and Bayesian parallel data prediction (Snyder et al.", "startOffset": 143, "endOffset": 158}, {"referenceID": 27, "context": "Multilingual resources were extensively used for cross-lingual sequence tagging through various ways, such as cross-lingual feature extraction (Darwish, 2013), text categorization (Virga and Khudanpur, 2003), and Bayesian parallel data prediction (Snyder et al., 2008).", "startOffset": 247, "endOffset": 268}, {"referenceID": 15, "context": "Parallel corpora and word alignments are also used for training crosslingual distributed word representations (Kiros et al., 2014; Gouws et al., 2014; Zhou et al., 2015).", "startOffset": 110, "endOffset": 169}, {"referenceID": 13, "context": "Parallel corpora and word alignments are also used for training crosslingual distributed word representations (Kiros et al., 2014; Gouws et al., 2014; Zhou et al., 2015).", "startOffset": 110, "endOffset": 169}, {"referenceID": 34, "context": "Parallel corpora and word alignments are also used for training crosslingual distributed word representations (Kiros et al., 2014; Gouws et al., 2014; Zhou et al., 2015).", "startOffset": 110, "endOffset": 169}, {"referenceID": 13, "context": "Huang et al. (2015) used word-level Long ShortTerm Memory (LSTM) units based on handcrafted features; dos Santos et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "(2015) used word-level Long ShortTerm Memory (LSTM) units based on handcrafted features; dos Santos et al. (2015) employed convo-", "startOffset": 93, "endOffset": 114}, {"referenceID": 11, "context": "lutional layers on both character and word levels; Chiu and Nichols (2015) applied convolutional layers on the character level and LSTM units on the word level; Gillick et al. (2015) employed a sequence-to-sequence LSTM with a novel tag-", "startOffset": 161, "endOffset": 183}, {"referenceID": 18, "context": "Most similar to our work is the recent approach independently developed by Lample et al. (2016) (published two weeks before our submission), which employs LSTM on both character and word levels.", "startOffset": 75, "endOffset": 96}, {"referenceID": 5, "context": "A gated recurrent unit (GRU) network is a type of recurrent neural networks first introduced for machine translation (Cho et al., 2014).", "startOffset": 117, "endOffset": 135}, {"referenceID": 30, "context": "form a deep recurrent network (Sutskever et al., 2014).", "startOffset": 30, "endOffset": 54}, {"referenceID": 17, "context": "To model the dependencies between tags in a sequence, we apply a conditional random field (Lafferty et al., 2001) layer on top of the hidden states h output by the word-level GRU (Huang et al.", "startOffset": 90, "endOffset": 113}, {"referenceID": 14, "context": ", 2001) layer on top of the hidden states h output by the word-level GRU (Huang et al., 2015).", "startOffset": 73, "endOffset": 93}, {"referenceID": 9, "context": "We employ mini-batch AdaGrad (Duchi et al., 2011) to train our neural network in an end-toend manner with backpropagation.", "startOffset": 29, "endOffset": 49}, {"referenceID": 31, "context": "We construct the POS tagging dataset with the instructions described in Toutanova et al. (2003). Note that as a standard practice, the POS tags are extracted from the parsed trees.", "startOffset": 72, "endOffset": 96}, {"referenceID": 6, "context": "For the task of CoNLL 2003 English NER, we follow previous works (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015) to append one-hot gazetteer features to the input of the CRF layer for fair comparison.", "startOffset": 65, "endOffset": 133}, {"referenceID": 14, "context": "For the task of CoNLL 2003 English NER, we follow previous works (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015) to append one-hot gazetteer features to the input of the CRF layer for fair comparison.", "startOffset": 65, "endOffset": 133}, {"referenceID": 23, "context": "80 Passos et al. (2014)\u2020\u2021 90.", "startOffset": 3, "endOffset": 24}, {"referenceID": 20, "context": "77 Luo et al. (2015)\u2020\u2021 91.", "startOffset": 3, "endOffset": 21}, {"referenceID": 18, "context": "2 Lample et al. (2016)\u2217 90.", "startOffset": 2, "endOffset": 23}, {"referenceID": 6, "context": "On the English datasets, following previous works that are based on neural networks (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015), we use the 50-dimensional SENNA embeddings2 trained on Wikipedia.", "startOffset": 84, "endOffset": 152}, {"referenceID": 14, "context": "On the English datasets, following previous works that are based on neural networks (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015), we use the 50-dimensional SENNA embeddings2 trained on Wikipedia.", "startOffset": 84, "endOffset": 152}, {"referenceID": 0, "context": "For Spanish and Dutch, we use the 64-dimensional Polyglot embeddings3 (Al-Rfou et al., 2013), which are trained on Wikipedia articles of the corresponding languages.", "startOffset": 70, "endOffset": 92}, {"referenceID": 27, "context": "014 Sun et al. (2008)\u2020\u2021 94.", "startOffset": 4, "endOffset": 22}, {"referenceID": 6, "context": "34 Collobert et al. (2011) 94.", "startOffset": 3, "endOffset": 27}, {"referenceID": 6, "context": "34 Collobert et al. (2011) 94.32 Huang et al. (2015)\u2020 94.", "startOffset": 3, "endOffset": 53}, {"referenceID": 20, "context": "78% reported by Ling et al. (2015). However, the embeddings they used are not publicly available.", "startOffset": 16, "endOffset": 35}, {"referenceID": 22, "context": "24 Shen et al. (2007)\u2020\u2021 97.", "startOffset": 3, "endOffset": 22}, {"referenceID": 22, "context": "24 Shen et al. (2007)\u2020\u2021 97.33 S\u00f8gaard et al. (2011)\u2020\u2021 97.", "startOffset": 3, "endOffset": 52}, {"referenceID": 6, "context": "50 Collobert et al. (2011) 97.", "startOffset": 3, "endOffset": 27}, {"referenceID": 6, "context": "50 Collobert et al. (2011) 97.29 Huang et al. (2015)\u2020 97.", "startOffset": 3, "endOffset": 53}, {"referenceID": 6, "context": "50 Collobert et al. (2011) 97.29 Huang et al. (2015)\u2020 97.55 Ling et al. (2015) 97.", "startOffset": 3, "endOffset": 79}, {"referenceID": 6, "context": "50 Collobert et al. (2011) 97.29 Huang et al. (2015)\u2020 97.55 Ling et al. (2015) 97.78 Ling et al. (2015) (SENNA)\u2217 97.", "startOffset": 3, "endOffset": 104}], "year": 2016, "abstractText": "We present a deep hierarchical recurrent neural network for sequence tagging. Given a sequence of words, our model employs deep gated recurrent units on both character and word levels to encode morphology and context information, and applies a conditional random field layer to predict the tags. Our model is task independent, language independent, and feature engineering free. We further extend our model to multi-task and crosslingual joint training by sharing the architecture and parameters. Our model achieves state-of-the-art results in multiple languages on several benchmark tasks including POS tagging, chunking, and NER. We also demonstrate that multi-task and cross-lingual joint training can improve the performance in various cases.", "creator": "LaTeX with hyperref package"}}}