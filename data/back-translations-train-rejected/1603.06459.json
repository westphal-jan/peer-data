{"id": "1603.06459", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2016", "title": "Characterization of neighborhood behaviours in a multi-neighborhood local search algorithm", "abstract": "We consider a multi-neighborhood local search algorithm with a large number of possible neighborhoods. Each neighborhood is accompanied by a weight value which represents the probability of being chosen at each iteration. These weights are fixed before the algorithm runs, and are considered as parameters of the algorithm. Given a set of instances, off-line tuning of the algorithm's parameters can be done by automated algorithm configuration tools (e.g., SMAC). However, the large number of neighborhoods can make the tuning expensive and difficult even when the number of parameters has been reduced by some intuition. In this work, we propose a systematic method to characterize each neighborhood's behaviours, representing them as a feature vector, and using cluster analysis to form similar groups of neighborhoods. The novelty of our characterization method is the ability of reflecting changes of behaviours according to hardness of different solution quality regions. We show that using neighborhood clusters instead of individual neighborhoods helps to reduce the parameter configuration space without misleading the search of the tuning procedure. Moreover, this method is problem-independent and potentially can be applied in similar contexts.", "histories": [["v1", "Sat, 12 Mar 2016 12:38:32 GMT  (95kb)", "http://arxiv.org/abs/1603.06459v1", "13 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nguyen thi thanh dang", "patrick de causmaecker"], "accepted": false, "id": "1603.06459"}, "pdf": {"name": "1603.06459.pdf", "metadata": {"source": "CRF", "title": "Characterization of neighborhood behaviours in a multi-neighborhood local search algorithm", "authors": ["Nguyen Thi Thanh Dang"], "emails": ["nguyenthithanh.dang@kuleuven-kulak.be", "patrick.decausmaecker@kuleuven-kulak.be"], "sections": [{"heading": null, "text": "ar Xiv: 160 3.06 459v 1 [cs.A I] 1 2M ar2 016Keywords: algorithm configuration, clustering, local search in multiple neighborhoods"}, {"heading": "1 Introduction", "text": "Since optimization algorithms are usually highly parameterized, tuning / configuration parameters are an important task. Given the distribution of problem cases, we need to find parameter configurations that optimize predefined performance measurement over distribution, such as the mean of the optimality gap. In the last fifteen years, automated algorithm configuration has been extensively studied [1]. Utility automated algorithm configuration tools such as SMAC [2] and irace [3] have been successfully applied in several studies. In this paper, we look at the parameter tuning problem of a multidimensional local search algorithm [4] that consists of a large number of neighborhoods."}, {"heading": "2 Parameter tuning for a multi-neighborhood local search algorithm", "text": "The algorithm considered in this paper was developed by members of the CODeS group at the University of Leuven, Belgium. [4], deals with the swap-body vehicle routing problem. It is an iterated local search [6] algorithm that uses late acceptance hill climbing [7] as a local search component. At each iteration of late acceptance hill climbing, a neighborhood Nk is randomly selected from a large group of neighborhoods, and a neighboring solution is generated by Nk. The probability that a neighborhood Nk is chosen is proportional to its weight wk. These weights are fixed during each algorithm run and add up to one. In addition, there are two integer parameters that control late acceptance hill climbing: this local search component is stopped after a number of itWI consecutive iterations, without any improvement of the current solution occurring."}, {"heading": "3 Neighborhood characterization and clustering", "text": "Inspired by the idea of OSCAR [8], which is an automated approach to the online selection of the algorithm portfolio, we characterize each neighborhood Nk's behavior on an instance based on the following six observations: - The probability that Nk improves, aggravates, or does nothing on a solution of I called rimprove deteriorates, rnothing, whereas the improvement + rnothing = 1- magnitudes of improvement and deterioration, called aimprove and aworsen - Nk's runtime (used for tie-breaking, as explained in Section 3.3) The novelty of our method is that we represent Nk with the estimated values of these observation regions on different solution quality regions, as it changes the behavior of Nk according to the hardness of the solution it is dealing with."}, {"heading": "3.1 Collect necessary information during algorithm runs", "text": "In this part we describe the procedure of collecting all the necessary information to characterize behavior in the neighborhood. In the face of a problem instance, we assume that an upper limit and a lower limit of the optimal solution quality are available. Since these limits need not be tight, this assumption is not difficult to fulfill. For example, the upper limit could be reached by a random solution or a solution for each customer. We take the value of this solution as an upper limit. A lower limit for each instance is provided by the authors of the algorithm, as the best solution is produced from operating their best algorithm configuration (the multi-thread version) by creating a route for each customer."}, {"heading": "3.2 Identify solution quality regions as frames", "text": "The intervals are grouped into frames based on sum Iters, the sum of all niter values of all neighborhoods on each interval. Figure 2 shows diagrams with sum Iters on each instance. As lower limits of solution quality are not reached, intervals with zero sum Iters are removed at the end. In this figure, there is a high peak value in each diagram representing the interval in which the algorithm remains most of the time. Therefore, we suspect that local Optima or plateaus should be located there. We can interpret the quality regions of the solution with low sum Iters values before this peak as easy to reach and easy to escape, whereas regions around these are considered easy to reach and difficult to escape and regions after that are hard to reach. The smaller peaks of two instances new with and large with two intervals each should specify the second local Optima or high planes. We suggest algorithm 1 for grouping all interframes in such frames (the interpretation attempts to mirror) an interval."}, {"heading": "3.3 Characterize neighborhood behaviours as feature vectors by aggregating collected information into frames", "text": "For the first three observables, rimprove, rworsen, and rnothing, we simply add the three values nI, nW, and nSN for all intervals that belong to the same frame. We then divide them by the sum of niters to get the ratios. For the other two observables, we improve and aggravate the aggregation more complicated. We cannot add the sI or sW values by intervals and get the average because their values are incomparable between different intervals. Therefore, we cannot say that an improvement of 10 in the two intervals [33762, 33621) and [33621, 33482) are the same, because the hardness of the solutions that belong to them is probably not the same. Therefore, we translate them into ranks before we perform the aggregation. For each interval, neighborhoods are sorted based on the averages of their respective sI, sW values. Since bonds can happen, e.g. some neighborhoods never have an improvement in the hard quality regions, which are partially the same in all intervals."}, {"heading": "3.4 Cluster analysis on neighborhoods", "text": "The first three observables, rimprove, rworsen and rnothing, sum up to one. As a result, their corresponding vector components belong to a special class called compositional data. As explained in [10] \"Sample space for compositional vectors is radically different from the real Euclidean space, which is associated with unrestricted data,\" multivariate statistical methodology, which was designed for unrestricted data, could not be applied directly. To convert them back into Euclidean space, we apply the isometric log ratio transformation proposed in [11]. After the transformation, since the three observables are reduced to two, each feature vector is now 120-dimensional. We can start to make cluster analysis on the basis of these vectors (120) is greater than the number of dimensions (42), the cluster method for clustering, which is distributed on two levels."}, {"heading": "4 Experimental results", "text": "In fact, the majority of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "5 Conclusion and future work", "text": "In this paper, we have proposed a systematic method of characterizing neighborhood behavior in a local search framework for multiple neighborhoods, selecting the probability of selecting a neighborhood for each iteration offline, based on the probabilities that a neighborhood improves, aggravates, or does nothing at all, on the orders of magnitude of its improvement and deterioration, and on its runtime. We have observed that these characteristics change depending on the hardness of the different regions in the solution quality space. As a result, we design our method to attempt to recognize these regions based on aggregated information and to represent the neighborhood behavior on them as trait vectors. Cluster analysis is then applied to form groups of similar neighborhoods. A tuning experiment with the automated algorithm configuration tool SMAC [2] shows that the use of these clusters results in a statistically significant improvement in the testing performance of the obtained algorithm configurations over the non-clustered version."}, {"heading": "Acknowledgement", "text": "This work is financed by COMEX (Project P7 / 36), a BELSPO / IAP programme. We thank Tu'lio Toffolo for his great support during this research, Thomas Stuetzle and Jan Verwaeren for their valuable comments. The computing resources and services used for this work were provided by the VSC (Flemish Supercomputing Centre), financed by the Herkules Foundation and the Flemish government department EMI."}], "references": [{"title": "Automated algorithm configuration and parameter tuning", "author": ["Holger H Hoos"], "venue": "In Autonomous search,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Sequential model-based optimization for general algorithm configuration", "author": ["Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown"], "venue": "In Learning and Intelligent Optimization,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "The irace package, iterated race for automatic algorithm configuration", "author": ["Manuel L\u00f3pez-Ib\u00e1nez", "J\u00e9r\u00e9mie Dubois-Lacoste", "Thomas St\u00fctzle", "Mauro Birattari"], "venue": "Technical report, Citeseer,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "The winning approach for the verolog solver challenge 2014: the swap-body vehicle routing problem", "author": ["Tony Wauters", "T\u00falio Toffolo", "Jan Christiaens", "Sam Van Malderen"], "venue": "Proceedings of ORBEL29,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Verolog solver challenge 2014\u2013vsc2014 problem description. VeRoLog (EURO Working Group on Vehicle Routing and Logistics Optimization) and PTV", "author": ["W Heid", "G Hasle", "D Vigo"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Iterated local search: Framework and applications", "author": ["Helena R Louren\u00e7o", "Olivier C Martin", "Thomas St\u00fctzle"], "venue": "In Handbook of Metaheuristics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "A late acceptance strategy in hill-climbing for exam timetabling problems", "author": ["Edmund K Burke", "Yuri Bykov"], "venue": "In PATAT 2008 Conference, Montreal, Canada,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Robust rank aggregation for gene list integration and meta-analysis", "author": ["Raivo Kolde", "Sven Laur", "Priit Adler", "Jaak Vilo"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A concise guide to compositional data analysis", "author": ["John Aitchison"], "venue": "In 2nd Compositional Data Analysis Workshop,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Isometric logratio transformations for compositional data analysis", "author": ["Juan Jos\u00e9 Egozcue", "Vera Pawlowsky-Glahn", "Gl\u00f2ria Mateu-Figueras", "Carles Barcelo-Vidal"], "venue": "Mathematical Geology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "High-dimensional data clustering", "author": ["Charles Bouveyron", "St\u00e9phane Girard", "Cordelia Schmid"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Hdclassif: An r package for model-based clustering and discriminant analysis of high-dimensional data", "author": ["Laurent Berg\u00e9", "Charles Bouveyron", "St\u00e9phane Girard"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Hyper-heuristics: A survey of the state of the art", "author": ["Edmund K Burke", "Michel Gendreau", "Matthew Hyde", "Graham Kendall", "Gabriela Ochoa", "Ender \u00d6zcan", "Rong Qu"], "venue": "Journal of the Operational Research Society,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "An efficient approach for assessing hyperparameter importance", "author": ["Frank Hutter", "Holger Hoos", "Kevin Leyton-Brown"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "For the last fifteen years, automated algorithm configuration has been extensively studied [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "General-purpose automated algorithm configuration tools such as SMAC [2] and irace [3] have been successfully applied in several studies.", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "General-purpose automated algorithm configuration tools such as SMAC [2] and irace [3] have been successfully applied in several studies.", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "In this work, we consider the parameter tuning problem of a multi-neighborhood local search algorithm [4], which consists of a large number of neighborhoods.", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "The algorithm is the winner of the Verolog Solver Challenge 2014 [5].", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "At each iteration, a neighbor solution is generated by a randomly chosen neighborhood with a probability defined by a weight value in the range of [0,1].", "startOffset": 147, "endOffset": 152}, {"referenceID": 3, "context": "The algorithm considered in this work, which was developed by CODeS group\u2019s members of the University of Leuven (Belgium) [4], tackles the Swap-body Vehicle Routing problem.", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "It is an iterated local search [6] algorithm that uses late acceptance hill climbing [7] as the local search component.", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "It is an iterated local search [6] algorithm that uses late acceptance hill climbing [7] as the local search component.", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "Therefore, we aggregate them using the R package RobustRankAggreg - a robust ranking aggregation method [9] specially designed for similar situations in bioinformatics.", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": "As explained in [10] \u201csample space for compositional vectors is radically different from the real Euclidean space associated with unconstrained data\u201d, multivariate statistical methodology designed for unconstrained data could not be applied directly.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "To convert them back to the Euclidean space, we apply the isometric log-ratio transformation proposed in [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "Since the number of dimensions (120) is larger than the number of individuals (42), the clustering method High-Dimensional Data Clustering (HDDC) [12]), which is implemented in the R package HDclassif [13], is used for cluster analysis.", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "Since the number of dimensions (120) is larger than the number of individuals (42), the clustering method High-Dimensional Data Clustering (HDDC) [12]), which is implemented in the R package HDclassif [13], is used for cluster analysis.", "startOffset": 201, "endOffset": 205}, {"referenceID": 1, "context": "To test this hypothesis, we applied the automated tuning tool SMAC [2] to two configuration scenarios: the first one, dubbed basic, uses the 28 groups of neighborhoods described in Table 1, the second one, dubbed clustered uses the 9 clusters of neighborhoods generated from our characterization method.", "startOffset": 67, "endOffset": 70}, {"referenceID": 12, "context": "In the hyper-heuristic community, in particular the Selection Hyper-heuristic class, in which the aim is to manage a set of low-level heuristics during the search by selecting one of them at each iteration, the Simple Random (SR) heuristic selection mechanism is often used as a baseline [14].", "startOffset": 288, "endOffset": 292}, {"referenceID": 1, "context": "A tuning experiment with the automated algorithm configuration tool SMAC [2] shows that using these clusters gives a statistically significant improvement on test performances of the obtained algorithm configurations over the non-clustered version.", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": "Firstly, a postanalysis on the importance of each cluster using fANOVA [15], which is an efficient approach to \u201cquantify the effect of algorithm parameters\u201d, can be applied.", "startOffset": 71, "endOffset": 75}], "year": 2016, "abstractText": "We consider a multi-neighborhood local search algorithm with a large number of possible neighborhoods. Each neighborhood is accompanied by a weight value which represents the probability of being chosen at each iteration. These weights are fixed before the algorithm runs, and are considered as parameters of the algorithm. Given a set of instances, off-line tuning of the algorithm\u2019s parameters can be done by automated algorithm configuration tools (e.g., SMAC). However, the large number of neighborhoods can make the tuning expensive and difficult even when the number of parameters has been reduced by some intuition. In this work, we propose a systematic method to characterize each neighborhood\u2019s behaviours, representing them as a feature vector, and using cluster analysis to form similar groups of neighborhoods. The novelty of our characterization method is the ability of reflecting changes of behaviours according to hardness of different solution quality regions. We show that using neighborhood clusters instead of individual neighborhoods helps to reduce the parameter configuration space without misleading the search of the tuning procedure. Moreover, this method is problem-independent and potentially can be applied in similar contexts.", "creator": "LaTeX with hyperref package"}}}